[
    {
        "title": "How Does Message Passing Improve Collaborative Filtering?"
    },
    {
        "review": {
            "id": "zrJxmTHxmc",
            "forum": "JZC8cEmMWY",
            "replyto": "JZC8cEmMWY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1469/Reviewer_6tQK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1469/Reviewer_6tQK"
            ],
            "content": {
                "summary": {
                    "value": "Collaborative filtering (CF) is a widely used technique in recommender systems, and some researchers have tried to improve it using message passing in graph neural networks. However, the reasons for the improvement remain unclear. This study investigates the effects of message passing on CF from various perspectives and finds that it primarily improves CF performance through information passed from neighbors and typically benefits low-degree nodes more than high-degree ones. Based on these findings, the authors propose a test-time augmentation framework called TAG-CF, which performs message passing only once at inference time and can be used as a plug-and-play module. Tested on five datasets, TAG-CF achieves similar or better results than existing graph-based CF methods with significantly less training time. The study also shows that test-time aggregation in TAG-CF improves recommendation performance similarly to training-time aggregation, validating the findings on why message passing improves CF."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The ablation study of neighbor information and gradients is reasonable and novel.\n2. The experiments are very detailed for reproducing; the authors provided code and configures of the experiments of information passed from neighbors, additional gradients for neighbors, and individual improvement gains of subgroups."
                },
                "weaknesses": {
                    "value": "1. The analysis of chapter 3.2 is not rigorous; it doesn't safe to conclude that message passing in CF helps low-degree users More. The authors used BPR and DirectAU to denote CF methods whereas both losses belong to pair-wise learning. Generally CF also includes pointwise and listing learning, which are not discussed in the paper.\n2. I agree that the Theorem 1 can reveal that for embed-based CF methods with pairwise losses: \"the room for improvement on high-degree users could be limited, since part of these improvements might already have been claimed by the supervision signal itself\". However, it does not mean that low-degree users will benefit more from message passing. Actually it the same conclusion for low-degree users. The difference of experimental evidences may be due to underfitting of low-degree users.\n3. The authors claimed \"TAG-CF cannot update any parameters since it is applied at test time, and hence requires tune-able normalization hyper-parameters\". This disadvantage makes the method unstable and hard to jduge."
                },
                "questions": {
                    "value": "1. Please address the concerns listed in the weakness. \n2. I suggest doing up-sampling for low-degree users' data and retraining a MF or ENMF model. I conjecture this will achieve competitive result against TAG-CF variants."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1469/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698580571230,
            "cdate": 1698580571230,
            "tmdate": 1699636075893,
            "mdate": 1699636075893,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GrBzTLmt2j",
                "forum": "JZC8cEmMWY",
                "replyto": "zrJxmTHxmc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6tQK [1/4]"
                    },
                    "comment": {
                        "value": "Dear Reviewer 6tQK:\n\nThank you for your valuable feedback. We sincerely appreciate your acknowledgment of the novelty, comprehensiveness, as well as reproducibility of our paper. Our detailed response to your concerns is as follows:\n\n## Weakness # 1: Analysis w.r.t. point-wise CF.\n>\n> Thanks for your comments. We would like to point out that most of the existing research that combines graphs and CF focuses on pair-wise learning schemes [1,2,3,4,5,6,7], instead of the point-wise one. For instance, all state-of-the-art graph learning methods that we compare within this work (e.g., SimGCL [1], SGL [4], LightGCL [2], and UltraGCN [3]) utilize pair-wise learning schemes. Since our paper studies graph-based CF methods, we mostly focus on pair-wise learning schemes. Besides, in our experiments, we include ENMF, a pointwise method, as a baseline model and we show that TAG-CF could still effectively and efficiently improve its performance following conclusions derived from pair-wise models. We believe that our conclusions and findings are philosophically extendable to point-wise methods and we will explore this direction as a future work."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334783756,
                "cdate": 1700334783756,
                "tmdate": 1700334783756,
                "mdate": 1700334783756,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nLdZgf00zB",
                "forum": "JZC8cEmMWY",
                "replyto": "zrJxmTHxmc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6tQK [2/4]"
                    },
                    "comment": {
                        "value": "## Weakness # 2: The difference in experimental evidences may be due to the underfitting of low-degree users.\n>\n> Thanks for your comments. In our manuscript, we connect CF objective functions (i.e., BPR and DirectAU) to message passing and show that they inadvertently conduct message passing during the back-propagation. Since this inadvertent message passing happens during the back-propagation, its performance is positively correlated to the amount of training signals a user/item can get. In the case of CF, the amount of training signals for a user is directly proportional to the node degree of this user. High-degree active users naturally benefit more from the inadvertent message passing from objective functions like BPR and DirectAU, because they acquire more training signals from the objective function. Hence, when explicit message passing is applied to CF methods, the performance gain for high-degree users is less significant than that for low-degree users. Because the contribution of the message passing over high-degree nodes has been mostly fulfilled by the inadvertent message passing during the training.\n\n> Following your suggestions, to quantitatively prove this line of theory, we incrementally upsample low-degree training examples and observe the performance improvement that TAG-CF could introduce at each upsampling rate. If our line of theory is correct, then we should expect less performance improvement on low-degree users for a larger upsampling rate. The results are shown in Table Re1.\n\n> From this table, though upsampling low-degree users hurts the overall performance, we can observe that the performance improvement brought by TAG-CF for low-degree users decreases, as the upsampling rate increases. For instance, when we regard users with a degree less than 40 as low-degree users, increasing the upsampling rate from 100% to 300% reduces the improvement margin by 8.1%, with similar trends on other degree cutoffs. \n\n> According to this experiment, we can conclude that the more supervision signals a user receives (no matter for a low-degree or high-degree user), the less performance improvement message passing can bring. This experiment quantitatively shows why the performance improvement of high-degree users could be limited more than low-degree users. Because high-degree users naturally receive more training signals during the training whereas low-degree users receive fewer training signals. However, simply adding additional supervision signals by upsampling low-degree users will hurt the overall performance. Through this series of experiments, we demonstrate the reason why the performance gain for high-degree users is less significant than that for low-degree users, when explicit message passing is applied to non-graph-based CF methods.\n\n| Upsmple Degree/ Upsample Rate |   100%   |   200%   |   300%   |\n|:----------------------------:|:-----:|:-----:|:-----:|\n| |MF|||\n|              40              | 20.62 | 19.93 | 19.30 |\n|              80              | 20.10 | 19.18 | 18.40 |\n|              160             | 19.39 | 18.40 | 17.93 |\n| |MF+TAG-CF|||\n|              40              | 28.87 | 26.90 | 25.01 |\n|              80              | 27.43 | 24.64 | 23.30 |\n|              160             | 26.63 | 24.30 | 23.37 |\n| |TAG-CF Improvement (%)|||\n|              40              | 38.4% | 31.2% | 30.3% |\n|              80              | 35.9% | 28.2% | 26.8% |\n|              160             | 36.6% | 31.8% | 29.8% |\n\nTable Re1: The performance improvement (NDCG@20) brought by TAG-CF at different node degree cutoffs and upsampling rates on Movielens-1M."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334865952,
                "cdate": 1700334865952,
                "tmdate": 1700334865952,
                "mdate": 1700334865952,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lTUSxyjZsH",
                "forum": "JZC8cEmMWY",
                "replyto": "zrJxmTHxmc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6tQK [3/4]"
                    },
                    "comment": {
                        "value": "## Weakness # 3: Hyper-parameters make the methods unstable and hard to judge. \n>\n> TAG-CF only has two hyper-parameters and each of them only has 5 selections (i.e., -2, -1.5, -1, -0.5, 0), resulting in a total number of 25 runs to conduct a grid-search. Compared with TAG-CF, our baselines have a lot more hyper-parameters and each of them has a lot more selections. Moreover, tuning hyper-parameters for TAG-CF is extremely cheap as it does not induce any model retraining. For instance, as shown in Table 3, a single run of TAG-CF roughly takes less than ~1% of the total running time of an MF method. And in this case, 25 searches over the hyper-parameters only cost less than ~25% of the total running time of a single model, because TAG-CF does not affect the model training at all.  However, for all hyper-parameters in our baselines, a change in hyper-parameters would require retraining the whole model and 25 searches would cost 2500% of the total running time of a single model. We believe that the number of hyper-parameters will not void the stability of TAG-CF."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334906765,
                "cdate": 1700334906765,
                "tmdate": 1700335847411,
                "mdate": 1700335847411,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T29vtebvmu",
                "forum": "JZC8cEmMWY",
                "replyto": "zrJxmTHxmc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6tQK [4/4]"
                    },
                    "comment": {
                        "value": "## Questions 2: Up-sampling for low-degree users' data.\n>\n> We sincerely appreciate your suggestions and the suggested upsampling experiment is really helpful for us to improve the quality of our manuscript. In our response to the second weakness that you raise, we quantitatively show that such an upsampling strategy will generally hurt the recommendation performance. However, through this series of experiments, we demonstrate the reason why the performance gain for high-degree users is less significant than that for low-degree users, when explicit message passing is applied to non-graph-based CF methods. Please refer to our response to Weakness #2 for details. \n\n** We hope we have satisfactorily answered your questions. If so, could you please consider increasing your rating? If you have remaining doubts or concerns, please let us know, and we will happily respond.**\n\nReference: \\\n[1] Yu, Junliang, et al. \"Are graph augmentations necessary? simple graph contrastive learning for recommendation.\" SIGIR 2022\\\n[2] Cai, Xuheng, et al. \"LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation.\" ICLR 2023\\\n[3] Kelong Mao, et al., \u201cUltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation\u201d CIKM 2021\\\n[4] Wu, Jiancan, et al. \"Self-supervised graph learning for recommendation.\" SIRIR 2021\\\n[5] He, Xiangnan, et al. \"Lightgcn: Simplifying and powering graph convolution network for recommendation.\" SIGIR 2020 \\\n[6] Wu, Shiwen, et al. \"Graph neural networks in recommender systems: a survey.\" CSUR 2022\\ \n[7] Wang, Xiang, et al. \"Neural graph collaborative filtering.\" SIGIR 2019 \\"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334942259,
                "cdate": 1700334942259,
                "tmdate": 1700334942259,
                "mdate": 1700334942259,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pO8oTTUvU6",
                "forum": "JZC8cEmMWY",
                "replyto": "zrJxmTHxmc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder about Rebuttal."
                    },
                    "comment": {
                        "value": "Dear Reviewer 6tQK:\n\nAs we conclude our rebuttal today, we look forward to the chance to interact with you. If there are any remaining questions or concerns on your end, please feel free to share them, and we'll gladly respond. Thank you.\n\nBest regards,\\\nTAG-CF authors"
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682561618,
                "cdate": 1700682561618,
                "tmdate": 1700682561618,
                "mdate": 1700682561618,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eSNY55kYnc",
            "forum": "JZC8cEmMWY",
            "replyto": "JZC8cEmMWY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1469/Reviewer_gGxd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1469/Reviewer_gGxd"
            ],
            "content": {
                "summary": {
                    "value": "This article analyzes and empirically verifies that message passing improves collaborative filtering mainly by: (1) information passed from neighbors instead of additional gradients; (2) more benefits for low-degree nodes than high-degree nodes. Inspired by these findings, the authors propose a test-time augmentation framework that only use message passing once at inference time, named Test-time Aggregation for Collaborative Filtering (TAG-CF). And extensive experiments are conducted on five open datasets to demonstrate the effectiveness and efficiency of TAG-CF and verify the previous findings again."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The experiments are sufficient in the elaboration of the proposed two findings. The authors provide ablation studies on different parts of the message passing used for collaborative filtering. From the perspective of improvement gains of subgroups, the conclusions are opposite to those in other graph tasks except CF. \n2. From multiple perspectives, the authors demonstrate the usability of the framework TAG-CF on CF models. Experiments are done on five datasets, especially including one large-scale dataset. And as a plug-and-play module, TAG-CF is putted in two MF methods trained by two supervision signals. Experiments are also carried out to compare time efficiency with other models."
                },
                "weaknesses": {
                    "value": "1. The analysis of experiments and theories is inadequate in section 3. For example: (1) the comparison of LightGCN_(w/o both) with other variants is missing; (2) the logic of the analysis of how supervision signals can lead to limited improvement on high-degree users doesn\u2019t make sense. The theoretical analysis that \u201cthese two supervision signals could inadvertently conduct message passing in the backward step\u201d can\u2019t adequately explain why the improvement on high-degree users could be limited more than low-degree users.\n\n2. The experimental results of the proposed framework are not significant and the scope of application is narrow. (1) Although the proposed TAG-CF can improve the CF methods by using messaging in the test phase, it does not work well compared to GNN-based models, such as the mentioned SGL and LightGCL. (2) And from the design philosophy of using message passing only for testing, this framework can only be used on non-GNN-based models. However, as there are many works that modify the message passing for CF with great performance rather than being limited to LightGCN, it is difficult to bridge the gap in the benefits of training with it. (3) In terms of time efficiency, there is no significant advantage over the relatively light GNN-based models recently. You can find some more recent models to compare the efficiency to show the benefits in this regard.\n\n3. The arrangement of Table 2 is not very reasonable. The comparison of results on low-degree users and overall performance is poorly readable. Results in these two sub-tables can be compared longitudinally in a single table."
                },
                "questions": {
                    "value": "As the object you're analyzing is message passing for CF, are there any similar ablation analyses done for other Graph-based CF methods except LightGCN?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1469/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1469/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1469/Reviewer_gGxd"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1469/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698645891961,
            "cdate": 1698645891961,
            "tmdate": 1699636075822,
            "mdate": 1699636075822,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jyoByQfSbU",
                "forum": "JZC8cEmMWY",
                "replyto": "eSNY55kYnc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gGxd [1/4]"
                    },
                    "comment": {
                        "value": "Dear Reviewer gGxd:\n\nThank you for your valuable feedback. We sincerely appreciate your acknowledgment of the comprehensiveness of our experiments as well as the usability of our proposed method. Our detailed response to your concerns is as follows:\n\n## Weakness # 1: The analysis of experiments and theories is inadequate in section 3.\n>\n> Thanks for your comments. In our manuscript, we connect CF objective functions (i.e., BPR and DirectAU) to message passing and show that they inadvertently conduct message passing during the back-propagation. Since this inadvertent message passing happens during the back-propagation, its performance is positively correlated to the amount of training signals a user/item can get. In the case of CF, the amount of training signals for a user is directly proportional to the node degree of this user. High-degree active users naturally benefit more from the inadvertent message passing from objective functions like BPR and DirectAU, because they acquire more training signals from the objective function. Hence, when explicit message passing is applied to CF methods, the performance gain for high-degree users is less significant than that for low-degree users. Because the contribution of the message passing over high-degree nodes has been mostly fulfilled by the inadvertent message passing during the training. \n\n> To quantitatively prove this line of theory, we incrementally upsample low-degree training examples and observe the performance improvement that TAG-CF could introduce at each upsampling rate. If our line of theory is correct, then we should expect less performance improvement on low-degree users for a larger upsampling rate. The results are shown in Table Re1.\n\n>From this table, though upsampling low-degree users hurts the overall performance, we can observe that the performance improvement brought by TAG-CF for low-degree users decreases, as the upsampling rate increases. For instance, when we regard users with a degree less than 40 as low-degree users, increasing the upsampling rate from 100% to 300% reduces the improvement margin by 8.1%, with similar trends on other degree cutoffs. \n\n> According to this experiment, we can conclude that the more supervision signals a user receives (no matter for a low-degree or high-degree user), the less performance improvement message passing can bring. This experiment quantitatively shows why the performance improvement of high-degree users could be limited more than low-degree users. Because high-degree users naturally receive more training signals during the training whereas low-degree users receive fewer training signals. \n\n\n| Upsmple Degree/ Upsample Rate |   100%   |   200%   |   300%   |\n|:----------------------------:|:-----:|:-----:|:-----:|\n| |MF|||\n|              40              | 20.62 | 19.93 | 19.30 |\n|              80              | 20.10 | 19.18 | 18.40 |\n|              160             | 19.39 | 18.40 | 17.93 |\n| |MF+TAG-CF|||\n|              40              | 28.87 | 26.90 | 25.01 |\n|              80              | 27.43 | 24.64 | 23.30 |\n|              160             | 26.63 | 24.30 | 23.37 |\n| |TAG-CF Improvement (%)|||\n|              40              | 38.4% | 31.2% | 30.3% |\n|              80              | 35.9% | 28.2% | 26.8% |\n|              160             | 36.6% | 31.8% | 29.8% |\n\nTable Re1: The performance improvement (NDCG@20) brought by TAG-CF at different node degree cutoffs and upsampling rates on Movielens-1M."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334480747,
                "cdate": 1700334480747,
                "tmdate": 1700334480747,
                "mdate": 1700334480747,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IP6MVpKSVO",
                "forum": "JZC8cEmMWY",
                "replyto": "eSNY55kYnc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gGxd [2/4]"
                    },
                    "comment": {
                        "value": "## Weakness # 2: The experimental results of the proposed framework are not significant and the scope of application is narrow.\n>\n> **[Performance improvement]**\nThe goal of TAG-CF is to match the performance of an end-to-end trained graph-based CF method (e.g., LightGCN). As shown in Table 2 in our manuscript, the performance improvement by TAG-CF is proportional to the performance gain brought by LightGCN. For instance, on Anime and Internal datasets, the performance improvement brought by TAG-CF is around 10% and 20% respectively, which is a significant improvement margin that aligns with the performance of LightGCN. However, on datasets like Gowalla and Yelp-2018, the performance improvement that could possibly be brought by the message passing scheme is incremental. Hence it will be difficult for TAG-CF to achieve further significant improvement. The goal of TAG-CF is to match the performance of graph-based CF frameworks, instead of surpassing their performance and achieving state-of-the-art performance. TAG-CF can match (and sometimes even outperform) the performance of LightGCN with only a fraction of additional computational overhead over much faster CF methods.\n\n> **[Comparison to GNN-based models]**\nSimilar to the previous bullet point, the goal of TAG-CF is to efficiently match the performance of end-to-end trained graph-based CF methods, instead of surpassing their performance. While not delivering the best number in all circumstances when compared with state-of-the-art graph-based CF methods like SimGCL [1] or LightGCL [2], across these four datasets TAG-CF still achieves competitive average performance. For instance, as shown in Table 4, TAG-CF achieves an average rank of 1.2 on NDCG@20, which is the best number among all state-of-the-art models. While achieving a competitive performance, TAG-CF is also extremely efficient and simple to implement. Considering the efficiency and performance together, TAG-CF achieves the best rank among all state-of-the-art models, significantly surpassing the runner-up by 0.9 ranks. With all the aforementioned evidence, we believe that it works well compared with GNN-based models. \n\n> **[Applicability to other methods]**\nWe use the message passing scheme in LightGCN to derive our findings and further propose TAG-CF. Though our conclusions are based on LightGCN, it does not limit the applicability and potential extensions of our proposal. Our research does not aim at further improving the performance of graph-based CF methods. Instead, we focus on enhancing the performance of non-graph-based CF methods in an extremely efficient and simple manner. Hence, our findings and conclusions are applicable to any CF methods without graphs. We leverage our findings and conclusions and further propose TAG-CF, which is simple to implement and does not require any architectural modifications due to its test-time augmentation trait. We believe that the generality and intuitiveness of our findings will be impactful for both academic researchers and industrial practitioners. \n\n> **[TAG-CF further improves graph-based CF very efficiently]**\nWhile it might not be sensible to apply TAG-CF to graph-based CF methods that leverage message passing already, in order to fully verify our findings, we also apply TAG-CF to a graph-based method (i.e., UltraGCN [6]) that utilizes graph structures as supervision signals only. The performance as well as efficiency of UltraGCN and UltraGCN+TAG-CF are shown in the table below. \n>\n> From Table Re1, we observe that TAG-CF can further improve UltraGCN, even when the former utilizes graph structures during the model training. Specifically, TAG-CF can improve the performance of UltraGCN by ~6%, which is significant. This observation indicates that the findings we propose in this manuscript can be applied to other algorithms. While TAG-CF can improve the performance, as shown in Table Re2, we can also notice that MF-DirectAU + TAG-CF in total runs a lot faster than UltraGCN. This is because UltraGCN still requires repetitive querying of the graph structures while calculating its objective functions."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334554569,
                "cdate": 1700334554569,
                "tmdate": 1700334554569,
                "mdate": 1700334554569,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IFN2HsSC4Y",
                "forum": "JZC8cEmMWY",
                "replyto": "eSNY55kYnc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gGxd [3/4]"
                    },
                    "comment": {
                        "value": "## Weakness # 2: The experimental results of the proposed framework are not significant and the scope of application is narrow (Cont.).\n>\n> **[Efficiency]**\nTAG-CF significantly reduces the computational overheads and conducts these operations only once for all nodes. TAG-CF+ further improves and conducts these operations only once for low-degree nodes. Besides, TAG-CF and TAG-CF+ only require 1-hop neighbor information, unlike existing works that usually require neighbors from 2 to 3 neighbors. The price that TAG-CF and TAG-CF+ pay is the minimal price if one wants to incorporate message passing of any sort in a CF system.  Compared with UltraGCN, a recently proposed efficient graph-based GNN that also does not utilize message passing during training, TAG-CF consistently runs faster, as shown in Table Re3. This is because UltraGCN still requires repetitive querying of the graph structures while calculating its objective functions. \n\n>**[Potential production impact]**\nWhile end-to-end trained graph-based CF methods consistently deliver promising performance, they are rarely used in real-world production scenarios, due to training complexities and prohibitively expensive overheads [8,9,10]. TAG-CF offers an extremely simple yet effective and practical approach for industry settings to benefit from the power of message passing while paying minimal cost. We believe that the theoretical and empirical findings we provide in this paper will be valuable to researchers from both academia and industry.\n\n|    Method   |   MF-BPR   | MF-BPR + TAG-CF |    MF-DirectAU    | MF-DirectAU + TAG-CF |  UltraGCN  | UltraGCN + TAG-CF |\n|:-----------:|:----------:|:---------------:|:-----------------:|:--------------------:|:----------:|:-----------------:|\n|             |            |                 |  Overall NDCG@20  |                      |            |                   |\n| Amazon-Book |  4.15\u00b10.13 |    4.32\u00b10.13    |     8.01\u00b10.03     |       8.13\u00b10.03      |  5.77\u00b10.25 |     6.11\u00b10.27     |\n|    Anime    | 29.51\u00b10.21 |    30.23\u00b10.26   |     24.01\u00b10.06    |      27.25\u00b10.03      | 30.30\u00b10.11 |     30.89\u00b10.11    |\n|   Gowalla   |  7.51\u00b10.12 |    7.99\u00b10.14    |     9.77\u00b10.08     |       9.88\u00b10.04      |  8.53\u00b10.14 |     9.02\u00b10.15     |\n|  Yelp-2018  |  3.96\u00b10.14 |    4.26\u00b10.17    |     6.25\u00b10.06     |       6.36\u00b10.03      |  5.01\u00b10.11 |     5.53\u00b10.11     |\n|             |            |                 | Overall Recall@20 |                      |            |                   |\n| Amazon-Book |  7.35\u00b10.22 |    7.64\u00b10.20    |     12.67\u00b10.06    |      12.97\u00b10.06      |  8.01\u00b10.25 |     8.53\u00b10.25     |\n|    Anime    | 34.84\u00b10.30 |    35.23\u00b10.34   |     29.15\u00b10.09    |      31.95\u00b10.05      | 35.87\u00b10.39 |     37.01\u00b10.39    |\n|   Gowalla   | 14.47\u00b10.23 |    14.92\u00b10.25   |     18.30\u00b10.17    |      18.53\u00b10.11      | 15.93\u00b10.21 |     16.36\u00b10.22    |\n|  Yelp-2018  |  7.27\u00b10.27 |    7.62\u00b10.22    |     10.81\u00b10.10    |      11.21\u00b10.09      |  8.41\u00b10.19 |     9.89\u00b10.20     |\n\nTable Re2: The performance (NDCG@20 and Recall@20) of UltraGCN and TAG-CF. \n\n|   Method    | LightGCN |   MF  | +TAG-CF | UltraGCN | +TAG-CF |\n|:-----------:|:--------:|:-----:|:-------:|:--------:|:-------:|\n|    Anime    |  138.85  | 34.12 |  +0.04  |   93.31  |  +0.04  |\n|  Yelp-2018  |   5.81   |  3.17 |  +0.02  |   5.02   |  +0.02  |\n|   Gowalla   |   13.27  |  7.74 |  +0.02  |   12.55  |  +0.02  |\n| Amazon-Book |   11.54  | 29.21 |  +0.03  |   39.26  |  +0.03  |\n\nTable Re3: The running time comparison (1 x 10^3 seconds) between TAG-CF and UltraGCN."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334614876,
                "cdate": 1700334614876,
                "tmdate": 1700334614876,
                "mdate": 1700334614876,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zWz504tOou",
                "forum": "JZC8cEmMWY",
                "replyto": "eSNY55kYnc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gGxd [4/4]"
                    },
                    "comment": {
                        "value": "## Weakness #3: Table 2 Arrangement \n>\n> Thanks for pointing this out. We accordingly added a modified version of Table 2 and put it in the appendix (i.e., Appendix H.4 Table 11). Upon your approval, we will add it to the main manuscript. \n\n## Question 1: Message passing scheme beyond LightGCN\n>\n> LightGCN is broadly explored and researched in the community of recommender systems and it is used as the backbone model for state-of-the-art recommender systems such as LightGCL, SimGCL, SGL, etc. There do not exist too many different message passing schemes in this community. We believe that the message passing scheme that we explore in this manuscript (i.e., linear neighbor aggregation) is predominant and widely accepted [1,2,3,4,5,6]. The other predominant scheme is NGCF which utilizes non-linear parameterized message passing. However, compared with NGCF, LightGCN has shown that linear neighbor aggregation is more efficient and effective. However, compared with NGCF[7], LightGCN has shown that linear neighbor aggregation is more efficient and effective. Hence, the conclusions and findings we have derived in this work can be widely applied to any graph-based CF methods that explore LightGCN as the backbone model, which accounts for the vast majority of graph-based CF methods. \n\n**We hope we have satisfactorily answered your questions. If so, could you please consider increasing your rating? If you have remaining doubts or concerns, please let us know, and we will happily respond.**\n\nReference: \\\n[1] Yu, Junliang, et al. \"Are graph augmentations necessary? simple graph contrastive learning for recommendation.\" SIGIR 2022\\\n[2] Cai, Xuheng, et al. \"LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation.\" ICLR 2023\\\n[3] Kelong Mao, et al., \u201cUltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation\u201d CIKM 2021\\\n[4] Wu, Jiancan, et al. \"Self-supervised graph learning for recommendation.\" SIRIR 2021\\\n[5] He, Xiangnan, et al. \"Lightgcn: Simplifying and powering graph convolution network for recommendation.\" SIGIR 2020\\\n[6] Wu, Shiwen, et al. \"Graph neural networks in recommender systems: a survey.\" CSUR 2022\\\n[7] Wang, Xiang, et al. \"Neural graph collaborative filtering.\" SIGIR 2019\\\n[8] Si, Si, et al. \"Serving Graph Compression for Graph Neural Networks.\" ICLR 2023\\\n[9] Zhang, Shichang, et al. \"Graph-less neural networks: Teaching old mlps new tricks via distillation.\" ICLR 2022\\\n[10] Guo, Zhichun, et al. \"Linkless link prediction via relational distillation.\" ICML 2023"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334685288,
                "cdate": 1700334685288,
                "tmdate": 1700334685288,
                "mdate": 1700334685288,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vR9qrxX9Et",
                "forum": "JZC8cEmMWY",
                "replyto": "eSNY55kYnc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder about Rebuttal."
                    },
                    "comment": {
                        "value": "Dear Reviewer gGxd:\n\nAs we wrap up our discussion today, we eagerly anticipate the opportunity to engage with you. Should you have any remaining questions or concerns, please don't hesitate to share them, and we'll be more than happy to assist. Thank you.\n\nBest regards,\\\nTAG-CF authors"
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682331953,
                "cdate": 1700682331953,
                "tmdate": 1700682331953,
                "mdate": 1700682331953,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5hIDRf3AgD",
            "forum": "JZC8cEmMWY",
            "replyto": "JZC8cEmMWY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1469/Reviewer_uTpx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1469/Reviewer_uTpx"
            ],
            "content": {
                "summary": {
                    "value": "The authors initially conduct experiments to illustrate that the message-passing process contributes more significant benefits to collaborative filtering than those derived from gradient updates. Subsequently, they discover that the message-passing mechanism offers more substantial advantages for nodes with fewer connections. Ultimately, the paper suggests that TAG-CF does not implement message-passing during the training phase. Instead, it exclusively applies this technique during the inference process."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1.\tThe paper attempts to explain the role of message passing in collaborative filtering, which is a promising avenue of investigation.\n2.\tThe article is easy to read, and the tables and illustrations are quite clear and comprehensible."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n\n1. The conclusions drawn from the experiments on LightGCN are well-known and lack a deeper theoretical foundation.\n\nIn Section 3.1 of the paper, the conclusions drawn are not groundbreaking; instead, they represent widely accepted knowledge, lacking innovation. Moreover, the article fails to provide theoretical underpinnings for the hypotheses and conclusions presented, making it challenging to extend these findings to collaborative filtering algorithms beyond LightGCN. The research solely relies on the LightGCN model in experiments conducted on three datasets with relatively high sparsity, which may introduce bias. The experimental results indicate two key points: 1. Message passing and gradient updates have practical significance for collaborative filtering recommendation systems, and 2. Message passing primarily contributes to performance improvements in collaborative filtering. It's worth noting that since the article exclusively investigates the LightGCN model, the conclusion predominantly emphasizes the role of message passing in enhancing LightGCN's performance. While this conclusion holds for LightGCN, it may not necessarily apply to other models, warranting further exploration.\n\nIndeed, further theoretical exploration may be necessary.\n\n2. The setup of the exploratory experiments is problematic, and this experimental design lacks fairness and equity.\n\nFurthermore, in the exploratory experiments of Section 3.1, the experimental design lacks the necessary rigor to effectively validate the author's hypotheses. In the case of (LightGCNw/o neigh. info), a message passing mechanism is employed during training but not during inference. During training, embedding representations are improved through message passing to obtain new user and item embeddings, which are then used to compute BPR loss. The optimization objective of message passing is to enhance the similarity and dissimilarity between embedding features after message passing. However, in the inference phase, the experiments omit message passing and use the original features as embeddings for users and items. This results in inconsistent optimization objectives between training and inference, with inference embeddings notably lacking in similarity and collaborative signals. Consequently, the (LightGCNw/o neigh. info) model's performance unavoidably becomes suboptimal. These experimental results, therefore, cannot effectively prove that message passing is the most critical factor.\n\n3. Applying TAG-CF solely to MF and ENMF is insufficient to demonstrate the effectiveness of TAG-CF, and in some experiments, the results show non-statistically significant improvements.\n\nThe proposed TAG-CF essentially involves deactivating message passing during model training and enabling it during inference. In the primary experiment, only ENMF and MF had an additional message passing step during inference to validate its effectiveness. However, these experiments are considered insufficiently comprehensive. It might be necessary to apply TAG-CF to a broader range of classic Graph-based recommendation models to thoroughly confirm its effectiveness. Limiting the application to the simplest ENMF and MF models alone may not be sufficient to fully demonstrate the effectiveness of TAG-CF. The comparative models solely comprise classic NGCF and LightGCN; thus, it might be necessary to include more recent models in the experiments. Additionally, on some datasets, the improvement is less than 2%, which is not statistically significant.\n\n4. The choice of datasets in this study is limited to a single type.\n\nAdditionally, the choice of datasets in this study is confined to a single type. The paper's model primarily focuses on node degrees, but the datasets used in the experiments all have relatively high sparsity. It would be beneficial to include datasets with lower sparsity, such as Movielens-1M, to ensure a more comprehensive evaluation of the model's performance.\n\n5. Despite the computational complexity, the improvements achieved with TAG-CF+ are not significantly greater than those of TAG-CF.\n\nTAG-CF+ involves exclusively passing messages to nodes with lower degrees. However, the computation of node degrees, selection of low-degree nodes, and the search for neighbors of low-degree nodes represent computationally complex tasks."
                },
                "questions": {
                    "value": "1. In the experiments in Section 3.2, how many layers of LightGCN are used?\n2. Regarding sensitivity to node degrees, are there experiments being conducted on datasets with even higher sparsity levels, which are commonly known to have higher degrees of sparsity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1469/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698647515515,
            "cdate": 1698647515515,
            "tmdate": 1699636075750,
            "mdate": 1699636075750,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SVPAi6t9yO",
                "forum": "JZC8cEmMWY",
                "replyto": "5hIDRf3AgD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uTpx [1/7]"
                    },
                    "comment": {
                        "value": "Dear Reviewer uTpx:\n\nThank you for your valuable feedback. We sincerely appreciate your acknowledgment of the importance of our research as well as the clarity of our manuscript. Our detailed response to your concerns is as follows:\n\n## Weakness # 1: The conclusions drawn from the experiments on LightGCN are well-known and lack a deeper theoretical foundation.\n>\n> **[Extension beyond LightGCN]**\nLightGCN is broadly explored and researched in the community of recommender systems and it is used as the backbone model for state-of-the-art recommender systems such as LightGCL, SimGCL, SGL, etc. There do not exist too many different message passing schemes in this community. We believe that the message passing scheme that we explore in this manuscript (i.e., linear neighbor aggregation) is predominant and widely accepted [1,2,3,4,5,6]. The other predominant scheme is NGCF which utilizes non-linear parameterized message passing. However, compared with NGCF, LightGCN has shown that linear neighbor aggregation is more efficient and effective.\n>\n> Hence, the conclusions and findings we have derived in this work can be widely applied to any graph-based CF methods that explore LightGCN as the backbone model, and CF methods based on LightGCN account for the vast majority of graph-based CF methods. \n\n> **[Applicability to other methods]**\nWe use the message passing scheme in LightGCN to derive our findings and further propose TAG-CF. Though our conclusions are based on LightGCN, it does not limit the applicability and potential extensions of our proposal. Our research does not aim at further improving the performance of graph-based CF methods. Instead, we focus on enhancing the performance of non-graph-based CF methods in an extremely efficient and simple manner such that they can effectively match the performance of graph-based methods. Hence, our findings and conclusions are applicable to any CF methods without graphs. We leverage our findings and conclusions and further propose TAG-CF, which is simple to implement and does not require any training-time architectural modifications due to its test-time augmentation trait. We believe that the generality and intuitiveness of our findings will be impactful for both academic researchers and industrial practitioners. \n\n>**[TAG-CF further improves graph-based CF very efficiently]**\nWhile it might not be sensible to apply TAG-CF to graph-based CF methods that leverage message passing already, in order to fully verify our findings, we also apply TAG-CF to a graph-based method (i.e., UltraGCN [6]) that utilizes graph structures as supervision signals only. The performance as well as efficiency of UltraGCN and UltraGCN+TAG-CF are shown in the table below. \n\n> From Table Re1, we observe that TAG-CF can further improve UltraGCN, even when the former utilizes graph structures during the model training. Specifically, TAG-CF can improve the performance of UltraGCN by ~6%, which is significant. This observation indicates that the findings we propose in this manuscript can be applied to other algorithms. While TAG-CF can improve the performance, as shown in Table Re2, we can also notice that MF-DirectAU + TAG-CF in total runs a lot faster than UltraGCN. This is because UltraGCN still requires repetitive querying of the graph structures while calculating its objective functions. \n\n|    Method   |   MF-BPR   | MF-BPR + TAG-CF |    MF-DirectAU    | MF-DirectAU + TAG-CF |  UltraGCN  | UltraGCN + TAG-CF |\n|:-----------:|:----------:|:---------------:|:-----------------:|:--------------------:|:----------:|:-----------------:|\n|             |            |                 |  Overall NDCG@20  |                      |            |                   |\n| Amazon-Book |  4.15\u00b10.13 |    4.32\u00b10.13    |     8.01\u00b10.03     |       8.13\u00b10.03      |  5.77\u00b10.25 |     6.11\u00b10.27     |\n|    Anime    | 29.51\u00b10.21 |    30.23\u00b10.26   |     24.01\u00b10.06    |      27.25\u00b10.03      | 30.30\u00b10.11 |     30.89\u00b10.11    |\n|   Gowalla   |  7.51\u00b10.12 |    7.99\u00b10.14    |     9.77\u00b10.08     |       9.88\u00b10.04      |  8.53\u00b10.14 |     9.02\u00b10.15     |\n|  Yelp-2018  |  3.96\u00b10.14 |    4.26\u00b10.17    |     6.25\u00b10.06     |       6.36\u00b10.03      |  5.01\u00b10.11 |     5.53\u00b10.11     |\n|             |            |                 | Overall Recall@20 |                      |            |                   |\n| Amazon-Book |  7.35\u00b10.22 |    7.64\u00b10.20    |     12.67\u00b10.06    |      12.97\u00b10.06      |  8.01\u00b10.25 |     8.53\u00b10.25     |\n|    Anime    | 34.84\u00b10.30 |    35.23\u00b10.34   |     29.15\u00b10.09    |      31.95\u00b10.05      | 35.87\u00b10.39 |     37.01\u00b10.39    |\n|   Gowalla   | 14.47\u00b10.23 |    14.92\u00b10.25   |     18.30\u00b10.17    |      18.53\u00b10.11      | 15.93\u00b10.21 |     16.36\u00b10.22    |\n|  Yelp-2018  |  7.27\u00b10.27 |    7.62\u00b10.22    |     10.81\u00b10.10    |      11.21\u00b10.09      |  8.41\u00b10.19 |     9.89\u00b10.20     |\n\nTable Re1: The performance (NDCG@20 and Recall@20) of UltraGCN and TAG-CF."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700333977211,
                "cdate": 1700333977211,
                "tmdate": 1700333977211,
                "mdate": 1700333977211,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WhQFE7s0RZ",
                "forum": "JZC8cEmMWY",
                "replyto": "5hIDRf3AgD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uTpx [2/7]"
                    },
                    "comment": {
                        "value": "## Weakness # 1: The conclusions drawn from the experiments on LightGCN are well-known and lack a deeper theoretical foundation. (Cont.)\n\n|   Method    | LightGCN |   MF  | +TAG-CF | UltraGCN | +TAG-CF |\n|:-----------:|:--------:|:-----:|:-------:|:--------:|:-------:|\n|    Anime    |  138.85  | 34.12 |  +0.04  |   93.31  |  +0.04  |\n|  Yelp-2018  |   5.81   |  3.17 |  +0.02  |   5.02   |  +0.02  |\n|   Gowalla   |   13.27  |  7.74 |  +0.02  |   12.55  |  +0.02  |\n| Amazon-Book |   11.54  | 29.21 |  +0.03  |   39.26  |  +0.03  |\n\nTable Re2: The running time comparison (1 x 10^3 seconds) between TAG-CF and UltraGCN. \n\n>**[Conclusions from the experiments are well-known]** \nThe most significant conclusions we propose in this work are (1) within the message passing scheme for CF, numerical values of neighbor information are more important than the accompanying gradients, and (2) explicit message passing in CF helps low-degree users more than it does for high-degree users. Based on these two findings, we propose TAG-CF, an extremely efficient test-time aggregation framework for CF. It enables CF systems without graphs to easily match the performance of graph-based CF methods. To our knowledge, these conclusions are novel and valuable to both academic researchers and industrial practitioners. If the reviewer has seen other related work with these conclusions, we are more than happy to discuss and accordingly revise our manuscript."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334025637,
                "cdate": 1700334025637,
                "tmdate": 1700334025637,
                "mdate": 1700334025637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1fZZolDzwZ",
                "forum": "JZC8cEmMWY",
                "replyto": "5hIDRf3AgD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uTpx [3/7]"
                    },
                    "comment": {
                        "value": "## Weakness # 2: The setup of the exploratory experiments is problematic, and this experimental design lacks fairness and equity.\n\n>Thanks for your comment -- we do not believe the fairness of the evaluation is a concern in these experiments.   These experiments simply aim to compare two different components brought by the message-passing mechanism (i.e., neighbor information vs. accompanying gradients). Through these two variants, we aim at answering how much contribution each component can bring by ablating the other one. \n\n>Moreover, even if we remove the second variant (i.e., LightGCN without neig. info), the findings and conclusions we draw in this section mostly still hold. This is because we just want to demonstrate that the message passing in CF can still deliver good performance even when it is not involved in the back-propagation."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334077171,
                "cdate": 1700334077171,
                "tmdate": 1700334077171,
                "mdate": 1700334077171,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jWLvtmkBzV",
                "forum": "JZC8cEmMWY",
                "replyto": "5hIDRf3AgD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uTpx [4/7]"
                    },
                    "comment": {
                        "value": "## Weakness # 3: Applying TAG-CF solely to MF and ENMF is insufficient to demonstrate the effectiveness of TAG-CF, and in some experiments, the results show non-statistically significant improvements.\n>\n> **[Experiments insufficient to demonstrate the effectiveness of TAG-CF]**\nThanks for pointing this out. While designing our experiments, we aim to evaluate the effectiveness of TAG-CF for enhancing the performance of non-graph-based CF methods, and the most predominant methods are MF and NMF. To alleviate your concerns,  we apply TAG-CF to an additional graph-based method (i.e., UltraGCN [6]) which utilizes graph structures as supervision signals only. The performance as well as efficiency of UltraGCN and UltraGCN+TAG-CF is shown in Table Re1. \n\n> From Table Re1, we observe that TAG-CF can further improve UltraGCN, even when the former utilizes graph structures during the model training via supervision. Specifically, TAG-CF can improve the performance of UltraGCN by ~6%, which is significant. This observation indicates that the findings we propose in this manuscript can be applied to other algorithms: namely, that explicit message passing information during forward is more valuable than the gradients in backward). While TAG-CF can improve the performance, as shown in Table Re2,  we can also notice that MF-DirectAU + TAG-CF in total runs a lot faster than UltraGCN, demonstrating both the efficiency and effectiveness of TAG-CF. \n\n> **[Performance improvement]**\nThe goal of TAG-CF is to match the performance of an end-to-end trained graph-based CF method (e.g., LightGCN). As shown in Table 2 in our manuscript, the performance improvement by TAG-CF is proportional to the performance gain brought by LightGCN. For instance, on Anime and Internal datasets, the performance improvement brought by TAG-CF is around 10% and 20% respectively, which is a significant improvement margin that aligns with the performance of LightGCN. However, on datasets like Gowalla and Yelp-2018, the performance improvement that could possibly be brought by the message passing scheme is incremental. Hence it will be difficult for TAG-CF to achieve further significant improvement. The goal of TAG-CF is to match the performance of graph-based CF frameworks, instead of surpassing their performance and achieving state-of-the-art performance. TAG-CF can match (and sometimes even outperform) the performance of LightGCN with only a fraction of additional computational overhead over much faster CF methods.\n\n> **[Potential production impact]**\nWhile end-to-end trained graph-based CF methods consistently deliver promising performance, they are rarely used in real-world production scenarios, due to training complexities and prohibitively expensive overheads [7,8,9]. TAG-CF offers an extremely simple yet effective and practical approach for industry settings to benefit from the power of message passing while paying minimal cost. We believe that the theoretical and empirical findings we provide in this paper will be valuable to researchers from both academia and industry."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334135500,
                "cdate": 1700334135500,
                "tmdate": 1700334135500,
                "mdate": 1700334135500,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GPdqrwu0ac",
                "forum": "JZC8cEmMWY",
                "replyto": "5hIDRf3AgD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uTpx [5/7]"
                    },
                    "comment": {
                        "value": "## Weakness # 4: The choice of datasets in this study is limited to a single type.\n>\n> According to another recent work that uses realistic e-commerce datasets [10], the average user degree is usually less than 50, which aligns with the characteristics of the datasets we use in this work. Nevertheless, to verify that this phenomenon is also observable in dense datasets, we apply TAG-CF to Movielens-1M. It is a dense dataset where each user has an average number of 165 interactions, as opposed to ~50 interactions for other datasets that we utilize in our manuscript. The results are shown in the table below and we can notice that our observation regarding the performance improvement brought by message passing still holds. \n\n|  Method   |   MF  | +TAG-CF | TAG-CF's Improvement over MF | LightGCN | TAG-CF's Improvement over LightGCN |\n|:---------:|:-----:|:-------:|:----------------------------:|:--------:|:----------------------------------:|\n|           |       |         |     Low-degree Percentile    |          |                                    |\n|  NDCG@20  | 20.98 |  29.20  |             39.2%            |   25.95  |                12.5%               |\n| Recall@20 | 23.64 |  28.10  |             18.9%            |   25.80  |                8.9%                |\n|           |       |         |            Overall           |          |                                    |\n|  NDCG@20  | 22.51 |  29.65  |             31.7%            |   26.64  |                11.3%               |\n| Recall@20 | 25.79 |  28.40  |             10.1%            |   26.30  |                8.0%                |\n\nTable Re3: The performance comparison between LightGCN and TAG-CF on Movielens-1M."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334171912,
                "cdate": 1700334171912,
                "tmdate": 1700334171912,
                "mdate": 1700334171912,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ArsY8jJjWw",
                "forum": "JZC8cEmMWY",
                "replyto": "5hIDRf3AgD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uTpx [6/7]"
                    },
                    "comment": {
                        "value": "## Weakness #5: Despite the computational complexity, the improvements achieved with TAG-CF+ are not significantly greater than those of TAG-CF.\n>\n> The goal of TAG-CF+ is to maintain the performance of TAG-CF and meanwhile further avoid redundant computational over high-degree users. While not intended, TAG-CF+ still improves the recommendation performance of TAG-CF, because TAG-CF sometimes hurts the performance of high-degree users even with overall performance improvement.\n\n> **[The selection of low-degree nodes]**\nWith respect to your concerns about the additional complex tasks entailed by TAG-CF+, we would like to highlight that the computational bottleneck of TAG-CF+ depends on the one-time message passing (which is very cheap already), instead of the selection of low-degree nodes. Specifically, TAG-CF+ only searches the selection of low-degree nodes over the validation set. We first conduct regular TAG-CF on the validation set and then select the degree threshold as a post-processing operation by re-using the result from TAG-CF on the validation set. This post-processing operation is as simple as a slicing operation, which finishes in milliseconds on commercial CPUs. In our appendix, we also reported the efficiency improvement of TAG-CF+ to TAG-CF. The overall running time improves by ~10%, including the selection of low-degree nodes. \n\n> **[Passing messages to nodes with lower degrees and searching for neighbors of low-degree nodes]**\nThis portion of computational overhead is inevitable for every single CF method that utilizes knowledge from graphs. In order to conduct message passing of any sort, one is required to acquire node neighbors and pass messages between adjacent nodes. Message passing in existing graph-based CF methods repetitively conducts these operations for every node and every training iteration. TAG-CF significantly reduces the computational overheads and conducts these operations only once for all nodes. TAG-CF+ further improves and conducts these operations only once for low-degree nodes. Besides, TAG-CF and TAG-CF+ only require 1-hop neighbor information, unlike existing works that usually require neighbors from 2 to 3 neighbors. The price that TAG-CF and TAG-CF+ pay is literally the minimal price if one wants to incorporate message passing of any sort in a CF system. \n\n> To quantitatively support our claim, the table below shows the running time of each operation that you mention. We can see that the node selection only takes 1% of the additional running time brought by TAG-CF+ or 0.001% of the total running time, which is almost negligible. \n\n| Dataset/Operation | LightGCN | MF+TAG-CF | MF+TAG-CF+ (passing messages to nodes with lower degree searching for neighbors of low-degree nodes) | Node selection in TAG-CF+ (10 thresholds) | Node selection in TAG-CF+ (50 thresholds) |\n|:-----------------:|----------|:---------:|:----------------------------------------------------------------------------------------------------:|:-----------------------------------------:|:-----------------------------------------:|\n|       Anime       | 138.85   |   34.16   |                                                 34.14                                                |                1.70 x 1e-4                |                2.28 x 1e-4                |\n|     Yelp-2018     | 5.81     |    3.19   |                                                 3.18                                                 |                0.92 x 1e-4                |                1.31 x 1e-4                |\n|      Gowalla      | 13.27    |    7.76   |                                                 7.75                                                 |                1.15 x 1e-4                |                1.42 x 1e-4                |\n|    Amazon-Book    | 46.62    |   29.24   |                                                 29.23                                                |                1.38 x 1e-4                |                1.75 x 1e-4                |\n\nTable Re4: The running time(1 x 10^3 seconds) for LightGCN, TAG-CF, and TAG-CF+."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334277175,
                "cdate": 1700334277175,
                "tmdate": 1700334277175,
                "mdate": 1700334277175,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B9WlUAd4ML",
                "forum": "JZC8cEmMWY",
                "replyto": "5hIDRf3AgD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uTpx [7/7]"
                    },
                    "comment": {
                        "value": "## Questions # 1: Number of layers in section 3.2\n\nWe conduct a hyper-parameter tuning over the selection of 2-3 layers and choose the setup with optimal performance. Specifically, on both Gowalla and Yelp-2018, we choose a two-layer LightGCN.\n\n## Questions # 2: Experiments on datasets with even higher sparsity levels.\n\nWe conduct experiments on our private internal dataset with extremely high sparsity (i.e., 99.99%). As for public benchmark datasets, I believe that Amazon-Book with a sparsity of 99.94% is sparse enough. \n\n\n**We hope we have satisfactorily answered your questions. If so, could you please consider increasing your rating? If you have remaining doubts or concerns, please let us know, and we will happily respond.**\n\nReference: \\\n[1] Wu, Jiancan, et al. \"Self-supervised graph learning for recommendation.\" SIRIR 2021\\\n[2] Yu, Junliang, et al. \"Are graph augmentations necessary? simple graph contrastive learning for recommendation.\" SIGIR 2022\\\n[3] Cai, Xuheng, et al. \"LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation.\" ICLR 2023\\\n[4] He, Xiangnan, et al. \"Lightgcn: Simplifying and powering graph convolution network for recommendation.\" SIGIR 2020\\\n[5] Wu, Shiwen, et al. \"Graph neural networks in recommender systems: a survey.\" CSUR 2022\\\n[6] Kelong Mao, et al., \u201cUltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation\u201d CIKM 2021\\\n[7] Si, Si, et al. \"Serving Graph Compression for Graph Neural Networks.\" ICLR 2023\\\n[8] Zhang, Shichang, et al. \"Graph-less neural networks: Teaching old mlps new tricks via distillation.\" ICLR 2022\\\n[9] Guo, Zhichun, et al. \"Linkless link prediction via relational distillation.\" ICML 2023\\\n[10] Zheng, Wenqing, et al. \"Cold brew: Distilling graph node representations with incomplete or missing neighborhoods.\" ICLR 2022"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700334323273,
                "cdate": 1700334323273,
                "tmdate": 1700334323273,
                "mdate": 1700334323273,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fRQLRbdSiH",
                "forum": "JZC8cEmMWY",
                "replyto": "5hIDRf3AgD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder about Rebuttal."
                    },
                    "comment": {
                        "value": "Dear Reviewer uTpx,\n\nAs today is the final day of our discussion, we anticipate the opportunity to engage with you. If you have any remaining questions or concerns, please don't hesitate to share them with us, and we will be happy to respond. Thank you.\n\nBest regards,\\\nTAG-CF authors"
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682197326,
                "cdate": 1700682197326,
                "tmdate": 1700682210812,
                "mdate": 1700682210812,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1QP8Xpu8dL",
            "forum": "JZC8cEmMWY",
            "replyto": "JZC8cEmMWY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1469/Reviewer_bF6q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1469/Reviewer_bF6q"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses the (positive) role of message passing in graph collaborative filtering. The analysis is initially driven by the assumption that, even though message passing in graph collaborative filtering is applied exactly as it appears in other graph learning tasks, evidence (in terms of recommendation performance) demonstrates that message passing in graph collaborative filtering may be working in a different manner. On one side, through a simple reformulation of the message passing (in the case of LightGCN) the authors show that it inherently comes with the usual user-item similarity score (as in MF) plus additional inductive biases accounting for other more refined interactions between users and/or items. This suggests that the message passing could improve MF-like approaches in two ways, namely: 1) neighborhood aggregation and 2) the additional gradient updates. An empirical analysis demonstrates that it is the neighborhood aggregation to provide the highest contribution to the improved performance. On another side, the authors empirically and mathematically prove that, differently from what happens in graph learning, high degree nodes seem to benefit from message passing more than low degree ones. In the light of above, the authors propose TAG-CF, short for Test-time Aggregation for Collaborative Filtering, a simple but effective model agnostic solution which performs message passing on top of any MF-like recommender system only at inference time. Extensive experimental analyses confirm the efficacy of the proposed approach over several baselines and on popular recommendation datasets. The evaluation is complemented through ablation studies and a computational time assessment."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper proposes a pivotal question to assess the actual role of message passing in collaborative filtering.\n+ The empirical and theoretical preliminary analyses are sound and help supporting the proposal of the TAG-CF solution.\n+ The proposed approach is simple and effective from a theoretical and experimental point of view.\n+ The experimental setting is extensive.\n+ The code is released at review time."
                },
                "weaknesses": {
                    "value": "- Some important related work and baselines may be missing. For instance, GFCF [1] is another work questioning the role of graph convolutional network in recommendation; UltraGCN [2] and SVD-GCN [3] discuss the role of additional neighborhood aggregation types (e.g., user-user and item-item) in collaborative filtering.\n- Some clarification needs to be provided regarding the low degree aspect (i.e., section 3.2).\n\n[1] Yifei Shen, Yongji Wu, Yao Zhang, Caihua Shan, Jun Zhang, Khaled B. Letaief, Dongsheng Li: How Powerful is Graph Convolution for Recommendation? CIKM 2021: 1619-1629\n\n[2] Kelong Mao, Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, Xiuqiang He: UltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation. CIKM 2021: 1253-1262\n\n[3] Shaowen Peng, Kazunari Sugiyama, Tsunenori Mine: SVD-GCN: A Simplified Graph Convolution Paradigm for Recommendation. CIKM 2022: 1625-1634\n\n**After the rebuttal.** The rebuttal clarified all weaknesses."
                },
                "questions": {
                    "value": "* Did the authors consider testing the proposed approach against UltraGCN? Indeed, UltraGCN is described as extremely simplified version of LightGCN also in terms of computational time. What is more, it almost removes the message passing from the training phase and proposes an approximation of infinite propagation layers through additional loss components. \n* Reading the discussion about low and high degree nodes in section 3.2, it seems that the observed behaviour (i.e., good performance on high degree nodes) could be ascribed to the fact that, in general, all recommendation approaches built on the collaborative filtering paradigm tend to provide higher-quality recommendations for active users at the detriment of less active ones (i.e., warm/cold users respectively). Thus, maybe this trend is only linked to the specific characteristics of each recommendation dataset, and it is not unique for graph-based recommender systems. Could the authors elaborate on this aspect?\n\n**After the rebuttal.** The rebuttal answered all questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1469/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1469/Reviewer_bF6q",
                        "ICLR.cc/2024/Conference/Submission1469/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1469/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699259801643,
            "cdate": 1699259801643,
            "tmdate": 1700651380699,
            "mdate": 1700651380699,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zLbQ4pYQMj",
                "forum": "JZC8cEmMWY",
                "replyto": "1QP8Xpu8dL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bF6q [1/2]"
                    },
                    "comment": {
                        "value": "Dear Reviewer bF6q:\n\nThank you for your valuable feedback. We sincerely appreciate your acknowledgment of our paper\u2019s theoretical soundness, practicality, and comprehensiveness for experiments. Our detailed response to your concerns is as follows:\n\n## Question #1: Did the authors consider testing the proposed approach against UltraGCN?\n>\n> Thanks for pointing out these related works. We have modified our manuscript (i.e., in the experiment and related work sections with edits in blue) and accordingly added discussions over these methods (GFCF [1], UltraGCN [2], and SVD-GCN [3]). Following your suggestions, we listed UltraGCN as one of our baselines. The performance and efficiency comparison between TAG-CF and UltraGCN is shown in the table below. \n\n> From Table Re1, we observe that UltraGCN outperforms MF-BPR and MF-BPR+TAG-CF. However, compared with MF and TAG-CF from DirectAU, the performance of UltraGCN is not as competitive. Besides, we also apply TAG-CF to user/item representations trained by UltraGCN and we notice that TAG-CF can further enhance its promising performance (i.e., an average 6.1% improvement on NDCG@20 on these four datasets and 13.4% on Recall@20). While UltraGCN implicitly approximates the utilities of message passing through a series of additional regularization terms, these results indicate that TAG-CF can still improve performance by explicitly aggregating neighbor information. \n\n> Moreover, we compare the total running time of TAG-CF and UltraGCN. We notice that UltraGCN runs faster than LightGCN.  However, compared with vanilla MF, UltraGCN introduces a lot of computational overhead by repetitively calculating additional loss terms guided by the graph structure. On the other hand, TAG-CF consistently brings negligible overheads (i.e., less than 1% of the total running time). \n\n|    Method   |   MF-BPR   | MF-BPR + TAG-CF |    MF-DirectAU    | MF-DirectAU + TAG-CF |  UltraGCN  | UltraGCN + TAG-CF |\n|:-----------:|:----------:|:---------------:|:-----------------:|:--------------------:|:----------:|:-----------------:|\n|             |            |                 |  Overall NDCG@20  |                      |            |                   |\n| Amazon-Book |  4.15\u00b10.13 |    4.32\u00b10.13    |     8.01\u00b10.03     |       8.13\u00b10.03      |  5.77\u00b10.25 |     6.11\u00b10.27     |\n|    Anime    | 29.51\u00b10.21 |    30.23\u00b10.26   |     24.01\u00b10.06    |      27.25\u00b10.03      | 30.30\u00b10.11 |     30.89\u00b10.11    |\n|   Gowalla   |  7.51\u00b10.12 |    7.99\u00b10.14    |     9.77\u00b10.08     |       9.88\u00b10.04      |  8.53\u00b10.14 |     9.02\u00b10.15     |\n|  Yelp-2018  |  3.96\u00b10.14 |    4.26\u00b10.17    |     6.25\u00b10.06     |       6.36\u00b10.03      |  5.01\u00b10.11 |     5.53\u00b10.11     |\n|             |            |                 | Overall Recall@20 |                      |            |                   |\n| Amazon-Book |  7.35\u00b10.22 |    7.64\u00b10.20    |     12.67\u00b10.06    |      12.97\u00b10.06      |  8.01\u00b10.25 |     8.53\u00b10.25     |\n|    Anime    | 34.84\u00b10.30 |    35.23\u00b10.34   |     29.15\u00b10.09    |      31.95\u00b10.05      | 35.87\u00b10.39 |     37.01\u00b10.39    |\n|   Gowalla   | 14.47\u00b10.23 |    14.92\u00b10.25   |     18.30\u00b10.17    |      18.53\u00b10.11      | 15.93\u00b10.21 |     16.36\u00b10.22    |\n|  Yelp-2018  |  7.27\u00b10.27 |    7.62\u00b10.22    |     10.81\u00b10.10    |      11.21\u00b10.09      |  8.41\u00b10.19 |     9.89\u00b10.20     |\n\nTable Re1: The performance (NDCG@20 and Recall@20) of UltraGCN and TAG-CF. \n\n|   Method    | LightGCN |   MF  | +TAG-CF | UltraGCN | +TAG-CF |\n|:-----------:|:--------:|:-----:|:-------:|:--------:|:-------:|\n|    Anime    |  138.85  | 34.12 |  +0.04  |   93.31  |  +0.04  |\n|  Yelp-2018  |   5.81   |  3.17 |  +0.02  |   5.02   |  +0.02  |\n|   Gowalla   |   13.27  |  7.74 |  +0.02  |   12.55  |  +0.02  |\n| Amazon-Book |   11.54  | 29.21 |  +0.03  |   39.26  |  +0.03  |\n\nTable Re2: The running time comparison (1 x 10^3 seconds) between TAG-CF and UltraGCN."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700333494791,
                "cdate": 1700333494791,
                "tmdate": 1700333683139,
                "mdate": 1700333683139,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G5A0t2V83z",
                "forum": "JZC8cEmMWY",
                "replyto": "1QP8Xpu8dL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bF6q [2/2]"
                    },
                    "comment": {
                        "value": "## Questions 2:  Reading the discussion about low and high degree nodes in section 3.2.\n> **[Phenomenon w.r.t. the user degree]** Thanks for your insightful comments. We agree with you on the idea that CF methods (graph-based or non-graph-based) give better recommendation results for active (i.e., high-degree) users than less active (i.e., low-degree) ones. We believe that this claim aligns with the argument we present in our manuscript. In our manuscript, we connect CF objective functions (i.e., BPR and DirectAU) to message passing and show that these CF objectives inadvertently conduct message passing during the back-propagation. \n\n> Since this inadvertent message passing happens during the back-propagation, its performance is positively correlated to the amount of training signals a user/item can get. In the case of CF, the amount of training signals for a user is directly proportional to the node degree of this user. High-degree active users naturally benefit more from the inadvertent message passing from objective functions like BPR and DirectAU, because they acquire more training signals from the objective function. Hence, when explicit message passing is applied to CF methods, the performance gain for high-degree users is less significant than that for low-degree users. Because the contribution of the message passing over high-degree nodes has been mostly fulfilled by the inadvertent message passing during the training. \n\n> We would like to emphasize that our study does not only show that graph-based and non-graph-based CF methods deliver better performance for active high-degree nodes; this observation is already well-known in the community, as you have indicated. However, the most significant observation that we would like to highlight is that the explicit incorporation of message passing (e.g., LightGCN vs. MF, or TAG-CF vs. MF) helps low-degree users more, compared with high-degree users. We believe that this novel observation is unique to graph-based CF methods and valuable to the recommender system community. \n\n> **[Dataset-specific]** \nWe believe this is not a dataset-specific phenomenon; instead, it is a phenomenon commonly seen in the recommender systems community. Almost every dataset in the recommender system community has a heavy-tailed distribution w.r.t. the user degree, where both active high-degree users and less active low-degree users co-exist. Hence, even in dense datasets where the average interaction per user is higher, high-degree users still receive substantially more training signals than low-degree users. So the observations and claims we made in this work are broadly applicable to message passing in CF across a broad range of datasets.\n\n> According to another recent work that uses realistic e-commerce datasets [4], the average user degree is usually less than 50, which aligns with the characteristics of the datasets we use in this work. Nevertheless, to verify that this phenomenon is also observable in dense datasets, we apply TAG-CF to Movielens-1M. It is a dense dataset where each user has an average number of 165 interactions, as opposed to ~50 interactions for other datasets that we utilize in our manuscript. The results are shown in the table below and we can notice that our observation regarding the performance improvement brought by message passing still holds. \n\n|  Method   |   MF  | +TAG-CF | TAG-CF's Improvement over MF | LightGCN | TAG-CF's Improvement over LightGCN |\n|:---------:|:-----:|:-------:|:----------------------------:|:--------:|:----------------------------------:|\n|           |       |         |     Low-degree Percentile    |          |                                    |\n|  NDCG@20  | 20.98 |  29.20  |             39.2%            |   25.95  |                12.5%               |\n| Recall@20 | 23.64 |  28.10  |             18.9%            |   25.80  |                8.9%                |\n|           |       |         |            Overall           |          |                                    |\n|  NDCG@20  | 22.51 |  29.65  |             31.7%            |   26.64  |                11.3%               |\n| Recall@20 | 25.79 |  28.40  |             10.1%            |   26.30  |                8.0%                |\n\nTable Re3: The performance comparison between LightGCN and TAG-CF on Movielens-1M.\n\n**In light of our answers to your concerns, we hope you consider raising your score. If you have any more concerns, please do not hesitate to ask and we'll be happy to respond.**\n\nReference:\\\n[1] Yifei Shen, et al., \u201cHow Powerful is Graph Convolution for Recommendation?\u201d CIKM 2021 \\\n[2] Kelong Mao, et al., \u201cUltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation\u201d CIKM 2021\\\n[3] Shaowen Peng, et al., \u201cSVD-GCN: A Simplified Graph Convolution Paradigm for Recommendation\u201d. CIKM 2022\\\n[4] Zheng, Wenqing, et al. \"Cold brew: Distilling graph node representations with incomplete or missing neighborhoods.\" ICLR 22"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700333535454,
                "cdate": 1700333535454,
                "tmdate": 1700333758983,
                "mdate": 1700333758983,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9HsXoAeE1I",
                "forum": "JZC8cEmMWY",
                "replyto": "G5A0t2V83z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Reviewer_bF6q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Reviewer_bF6q"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nthank you for your careful rebuttal. I'll answer to both points you outlined.\n\n**Question 1.** Thank you for providing additional experiments and results with UltraGCN. They further confirm your analysis and the proposal of the TAG-CF approach.\n\n**Question 2.**  Your response is extensive and helped me solving most of the previous doubts regarding the high- and low-degree nodes. \n\nYour rebuttal did answer to all my concerns, and convinced me even more about the initial rating I gave to your work."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501800257,
                "cdate": 1700501800257,
                "tmdate": 1700501800257,
                "mdate": 1700501800257,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ca9DHJdwol",
                "forum": "JZC8cEmMWY",
                "replyto": "TAdxn646B3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1469/Reviewer_bF6q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1469/Reviewer_bF6q"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nbesides the rebuttal to my review, I also read the other reviews-rebuttals. The answers you gave in the rebuttals were, overall, very careful and well-structured. \n\nAs already stated in my rebuttal, this convinces me even more about the initial acceptance rating I gave to your work.\n\nGood luck with the other rebuttals!"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1469/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501966263,
                "cdate": 1700501966263,
                "tmdate": 1700501966263,
                "mdate": 1700501966263,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]