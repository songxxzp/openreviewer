[
    {
        "title": "Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation"
    },
    {
        "review": {
            "id": "UYfBIpuDlP",
            "forum": "fDZumshwym",
            "replyto": "fDZumshwym",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4169/Reviewer_UGPZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4169/Reviewer_UGPZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel dataset condensation method following the data parameterization approach. The key idea is to use the newly designed Hierarchical Memory Network (HMN) to learn a three-tier representation of the condensed dataset. Experimental results demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea proposed is interesting and novel.\n- The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "- It lacks a discussion and comparison to a very relevant SOTA method, i.e., IDM [1], which also addresses the scalability of data condensation and works in a low GPU memory cost scenario. Thus, it might be incorrect to argue that the proposed method is the \"first method to achieve such good performance with a low GPU memory loss\" and such statements should be revised.\n\n[1] Zhao, G., Li, G., Qin, Y. and Yu, Y., 2023. Improved distribution matching for dataset condensation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 7856-7865).\n\n- The technical novelty is a bit thin. Specifically, the second contribution is relatively weak and more similar to a trick. It is interesting to know that over-budget generation and pruning may work, but this is more similar to remedying some of the shortcomings of the main idea rather than an independent contribution."
                },
                "questions": {
                    "value": "Please see the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4169/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698492993627,
            "cdate": 1698492993627,
            "tmdate": 1699636382896,
            "mdate": 1699636382896,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q6rhMEwfWr",
                "forum": "fDZumshwym",
                "replyto": "UYfBIpuDlP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4169/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer #4 (UGPZ)"
                    },
                    "comment": {
                        "value": "We thank Reviewer #4 (UGPZ) for the reviews and pointing out a missing related work. We address the raised questions and concerns below: \n\n**Q1:** It lacks a discussion and comparison to a very relevant SOTA method, i.e., IDM [1], which also addresses the scalability of data condensation and works in a low GPU memory cost scenario. Thus, it might be incorrect to argue that the proposed method is the \"first method to achieve such good performance with a low GPU memory loss\" and such statements should be revised.\n\n**A1:** We thank Reviewer #4 for pointing out a missing related work. As suggested by the reviewer, we have already added IDM to the Related Work section and added IDM as a new baseline in Table. 2 in the revised version. We also revised related statements in the paper. We would like to note that, *after adding IDM, our proposed method still achieves better or comparable performance than other baselines.*\n\n**Q2:** The technical novelty is a bit thin. Specifically, the second contribution is relatively weak and more similar to a trick. It is interesting to know that over-budget generation and pruning may work, but this is more similar to remedying some of the shortcomings of the main idea rather than an independent contribution.\n\n**A2:**  We would like to note that the major novelty in HMN\u2019s design comes from its hierarchical architecture. HMN is the first data container that considers the hierarchical structure of the classification system, and HMN achieves a better empirical performance.  This makes us believe that *HMN provides certain novelty and contribution to the DC area.*\n\nWe agree with the reviewer that the second contribution (HMN pruning part) is more like a favorable property of HMN. However, we note that not only HMN but also other data parameterization methods, like HaBa, also have data redundancy in the condensed data containers (as discussed in Section 3.2.1). Unlike the HMN approach, it is non-trivial to prune redundant generated examples in factorization-based data containers. HMN allows better independence between different generated images, which makes HMN a different data container that allows for further improving efficacy by pruning redundant information. \n\n-------------\n\nWe thank Reviewer #4 again for pointing out an important missing related work and bringing the novelty of pruning for discussion. We hope our response addressed your concerns regarding the related work and novelty and hope you can kindly consider updating the rating once you feel satisfied with the changes. We are happy to have further discussion and any questions that might arise!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700190837844,
                "cdate": 1700190837844,
                "tmdate": 1700190837844,
                "mdate": 1700190837844,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8uB8Ae67fk",
                "forum": "fDZumshwym",
                "replyto": "UYfBIpuDlP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4169/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks to Reviewer #4 (UGPZ) and the Response Summary on Major Concerns."
                    },
                    "comment": {
                        "value": "Dear Reviewer UGPZ,\n\nWe greatly appreciate your time and efforts in reviewing our paper, and **we would like to briefly summarize the response to your major concerns for your convenience.** We also kindly refer you to our previous responses (both separate and general responses) for more detailed discussions.\n\n---\n\n> **Q: Lack of discussion on a related data condensation paper, IDM.**\n\n**A:**  We have already added IDM to the Related Work section and added IDM as a new baseline in Table. 2 in the revised version. We would like to note that, after adding IDM, our proposed method still achieves better or comparable performance than SOTA methods.\n\n> **Q: The second contribution is relatively weak and more similar to a trick.**\n\n**A:** We would like to note that the major novelty in HMN\u2019s design comes from its hierarchical architecture. HMN is the first data container that considers the hierarchical structure of the classification system. We agree with the reviewer that the second contribution (HMN pruning part) is more like a favorable property of HMN, but this property enables HMN to further improve efficacy by pruning redundant information, which is different from previous data parameterization methods.\n\n----\n\nWe have already revised the paper to include these points in the paper. **We also kindly refer you to our previous responses (both separate and general responses) for more detailed discussions.** We would like to check with you if our response and revised paper addressed your concerns and if we can provide more information to better address your concerns and clarify the claim in the paper!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676023894,
                "cdate": 1700676023894,
                "tmdate": 1700676023894,
                "mdate": 1700676023894,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "R9lF0puoUS",
            "forum": "fDZumshwym",
            "replyto": "fDZumshwym",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4169/Reviewer_RRfc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4169/Reviewer_RRfc"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel data parameterization architecture named Hierarchical Memory Network (HMN), which consists of dataset-level, class-level, and instance-level memory and a feature extractor and a decoder to better leverage the hierarchical nature of the image. Furthermore, the paper proposes instance-level pruning to remove redundant images to improve performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper proposes a novel framework for DC with data parameterization, instead of updating input images, they update the memory (i.e, dataset, class, and instance) and decoder.\n2. Identify the redundancy in condensed images using AUM and suggest to first condense images with over-budget (p%) and then perform post-condensation pruning to remove redundant data."
                },
                "weaknesses": {
                    "value": "Although the proposed method (including HMN and condensed over-budged & pruning) is novel, there are several weaknesses:\n\n1. Lack of experiments on higher resolution such as 128x128 (Image Woof/Nette) or 224x224 (ImageNet-10/100, a subset from ImageNet, as done in IDC)\n2. Lack of describing the memory in detail. For example, how does the memory look like?\n3. The evaluation seems to be different from previous works. For example, HMN evaluates condensed images with a cosine annealing learning rate (LR) scheduler, while DSA or IDC uses a multistep LR scheduler. Thus, the comparison may not be fair."
                },
                "questions": {
                    "value": "I have several questions:\n1. What does the memory look like? How to initialize these memories, feature extractor, and decoder?\n2. HMN stores parameters instead of images, this will incur extra resources for evaluation (to generate images). Is it correct?\n3. How many generated images per class  (GIPC) used in Table 2? \n4. In Table 2, the result of IDC on CIFAR-10 with 50 IPC is lower than in the paper (74.5 (IDC) vs 71.6 (this paper)). Is it a mistake?  On CIFAR-100, this paper reports the accuracy of IDC with 10 IPC on CIFAR-100 is 44.8 while on other papers such as DREAM, the accuracy is 45.1. Can the author double-check? The results of IDC (at 10 and 50 IPC) in Table 2, Table 3, and Table 6 are different. Can the author explain?\n5. As shown in Figure 4 (right),  with an instance memory size near 100, around 600 images per class are generated. How many images per class (per epoch) are used for training in the evaluation phase? 600? If this is correct, the training time with condensed images is much longer than the conventional method where the total images per lass are set as the IPC. Can the author clarify this?\n\nAlthough the proposed method is novel and seems promising, there are unclear parts regarding the GIPC, the memory, and fairness in evaluation comparison. Additionally, some reported results of IDC of this paper are different from those in the original paper without explanation. Thus, I give a borderline reject."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4169/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4169/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4169/Reviewer_RRfc"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4169/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754440523,
            "cdate": 1698754440523,
            "tmdate": 1699636382823,
            "mdate": 1699636382823,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F3eSX9KcOU",
                "forum": "fDZumshwym",
                "replyto": "R9lF0puoUS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4169/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer #3 (RRfc) (Part 1)"
                    },
                    "comment": {
                        "value": "We thank Reviewer #3 (RRfc) for the reviews and suggestions to improve the paper. We address the raised questions and concerns below: \n\n**Q1:** Lack of experiments on higher resolution such as 128x128 (Image Woof/Nette) or 224x224 (ImageNet-10/100, a subset from ImageNet, as done in IDC)\n\n**A1:** We thank the reviewers for suggesting additional evaluation on a high-resolution dataset like ImageNet to better demonstrate the effectiveness of HMN. We now include the ImageNet-10 evaluation result in Table. 2 (Section 4.1) in the revised version for a more extensive comparison. We evaluated HMN on ImageNet-10 1 IPC setting and compared it with IDC (other baselines are too resource-intensive to run).  The evaluation results are summarized below:\n\n|      |  IDC |    HMN   |\n|:----:|:----:|:--------:|\n| 1 IPC | 60.4 | **64.6** |\n\nFrom the table, we find that HMN still achieves strong performance on ImageNet-10, outperforming IDC.  \n\n**Q2:** Lack of describing the memory in detail. For example, how does the memory look like? \nWhat does the memory look like? How to initialize these memories, feature extractor, and decoder? (Q2)\n\n**A2:** All memories are 3-D tensors. For CIFAR10, CIFAR100, and SVHN, memories have a shape (4, 4, Channels). For Tiny-imageNet, memories have a shape (8, 8, Channels). All values in the tensor are initialized with standard normal distribution. We included more details about the memory design of HMN in Section 3.1 and Appendix C.1 in the revised version.\n\n**Q3:** The evaluation seems to be different from previous works. For example, HMN evaluates condensed images with a cosine annealing learning rate (LR) scheduler, while DSA or IDC uses a multistep LR scheduler. Thus, the comparison may not be fair.\n\n**A3:** We thank the reviewer for pointing this out and agree with the reviewer that the evaluation with a multistep LR scheduler can still be useful. We report below the performance of HMN with a multistep LR scheduler on the CIFAR10 dataset as follows.\n\n|      IPC       | 1  | 10   | 50  |\n|:----------------:|:----:|:-----:|-------|\n|    Multi-step    | 65.7 | 73.5  | 76.8  |\n| Cosine Annealing | 65.7 | 73.7  | 76.9  |\n\nWe find the difference due to the LR scheduler choice to be marginal, and the results with the multistep LR scheduler do not change the findings and conclusions of our evaluation. Our primary reason for choosing the cosine annealing LR scheduler in our evaluation is that it has fewer hyperparameters compared to the multistep LR scheduler (like milestones to reduce learning rate, and learning rate decay factor). We added more details to Appendix C.2 and D.4 regarding our hyperparameter settings and the rationale for any differences.\n\n**Q4:** HMN stores parameters instead of images, this will incur extra resources for evaluation (to generate images). Is it correct? \n\n**A4:** The reviewer is right that all data parameterization methods need to generate training data before model training. We would like to clarify that this generation overhead is relatively small, because it just needs one forward pass of HMNs. For example, on a 2080TI, the generation time for a 1 IPC, 10 IPC, and 50 IPC CIFAR10 HMN is **0.036s, 0.11s, and 0.52s,** respectively (average number through 100 repeats).\n\nWe thank the reviewer for pointing this out for discussion, and we have already added this to the discussion in Appendix B in the revised version.\n\n**Q5:** How many generated images per class (GIPC) used in Table 2?\n\n**A5:** We listed GIPC for each setting in Table 5 (Appendix C). Since each instance-level memory corresponds to a generated image, the row \u201c#Instance-level memory\u201d indicates the GIPC for each setting. We have also further revised the writing in Section 3.1 and Appendix C.1 to reduce unnecessary confusion about GIPC of HMNs."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700190657924,
                "cdate": 1700190657924,
                "tmdate": 1700190657924,
                "mdate": 1700190657924,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VZpge5Jho9",
                "forum": "fDZumshwym",
                "replyto": "R9lF0puoUS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4169/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks to Reviewer #3 (RRfc) and the Response Summary on Major Concerns."
                    },
                    "comment": {
                        "value": "Dear Reviewer RRfc,\n\nWe greatly appreciate your time and efforts in reviewing our paper, and **we would like to briefly summarize the response to your major concerns for your convenience.** We also kindly refer you to our previous responses (both separate and general responses) for more detailed discussions.\n\n---\n\n> **Q: How does HMN scale to more complex datasets?**\n\n**A:** We now included the evaluation results on ImageNet-10 in Table 2. Our additional evaluation also shows that HMN also scales to complex datasets like ImageNet-10 and achieves better performance.\n\n> **Q: The evaluation of HMN has a different setting. The training with HMN uses the cosine annealing LR scheduler, but the multi-step LR scheduler is more commonly used.**\n\n**A:** We report the numbers of training with HMN with a multi-step LR scheduler. We find the difference due to the LR scheduler choice to be marginal, and **the results with the multistep LR scheduler do not change the findings and conclusions of our evaluation.\n\n> **Q: HMN seems to have a larger number of GIPC than conventional methods using images as data containers, which can increase the training time.**\n\n**A:**  The reviewer's larger point is valid in that a larger GIPC size can impact training time. We would like to point out that this is a common feature of data parameterization methods, including IDC, HaBa, LinBa, and HMN. For example, LinBa generates 115 images per class with the 1 IPC setting for CIFAR10. However, data parameterization methods also achieve better accuracy. More GIPC can be treated as a tradeoff for better accuracy of data parameterization methods. We believe that this observation presents a promising direction for future research. \n We leave the challenge of reducing GPIC for data parameterization methods, while retaining accuracy, for future exploration.\n\n> **Q: The baseline number of IDC is inconsistent with the numbers reported in the original paper.**\n\n**A:**  We thank the reviewer for pointing out this mismatch. There turned out to be some typos in our paper, and we now correct all the numbers. We would like to note that **the correction of those typos does not change the conclusion of our evaluation. HMN still achieves better results.**\n\n----\n\nWe have already revised the paper to include these points in the paper. **We also kindly refer you to our previous responses (both separate and general responses) for more detailed discussions.** We would like to check with you if our response and revised paper addressed your concerns and if we can provide more information to better address your concerns and clarify the claim in the paper!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675866806,
                "cdate": 1700675866806,
                "tmdate": 1700675866806,
                "mdate": 1700675866806,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AvUqORRl3Z",
                "forum": "fDZumshwym",
                "replyto": "R9lF0puoUS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4169/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A gentle reminder that our response is ready."
                    },
                    "comment": {
                        "value": "Dear Reviewer #3 (RRfc),\n\nWe greatly appreciate your time and efforts in reviewing our paper again. Since the discussion period is about to end, we would like to gently double-check if our responses and revised paper have addressed your concerns. We are happy to provide more information if necessary!\n\nThanks,\n\nPaper 4169 Authors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699862277,
                "cdate": 1700699862277,
                "tmdate": 1700699862277,
                "mdate": 1700699862277,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QsZq2Kfi1e",
                "forum": "fDZumshwym",
                "replyto": "AvUqORRl3Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4169/Reviewer_RRfc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4169/Reviewer_RRfc"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply. \n\nHowever, I think the evaluation time of HMN is much longer than DC approaches such as DSA or even IDC.\nLet's use CIFAR-100 with 50 IPC as an example. As shown in Table 5, the GIPC is 673 (#Instance-level memory), meaning that we generate 673 images for one class. However, CIFAR-100 only has 500 images per class. Hence, in the evaluation phase, the training time of HMN is longer than using the original images. Moreover, HMN achieves lower performance than using the original images. The only advantage of HMN I can see is that we can reduce the storage size. Why don't we compare using the same number of optimization steps ? (IDC reduces the number of epochs so that they can fairly compare to DC/DSA when considering the same training time)"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740848654,
                "cdate": 1700740848654,
                "tmdate": 1700740848654,
                "mdate": 1700740848654,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zGZ7q97OrK",
            "forum": "fDZumshwym",
            "replyto": "fDZumshwym",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4169/Reviewer_Y8Kv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4169/Reviewer_Y8Kv"
            ],
            "content": {
                "summary": {
                    "value": "This paper works on the task of dataset condensation, i.e., producing a data generator whose generated data can be used to train a network that retain similar performance with respect to using a full dataset. The authors proposed a novel hierarchical data generation pipeline, which starts from a dataset level feature, then go through class-specific feature processing layers, and instance-level feature processing layers, and a uniform decoder to decode the features into images. Experiments show the proposed method achieves good performance on 3 benchmarks, on-par or out-performing methods with more expensive computes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The task of dataset condensation is an important and interesting task. This paper provide a good summarization of existing literature, and make solid progresses to this field.\n\n- The proposed methods of hierarchical memory decoding is intuitive and makes a lot of sense to me. The followup pruning technique is also clean.\n\n- Experiments show the proposed method work well on different benchmarks with different data budgets, outperforming existing methods which use more computes during training."
                },
                "weaknesses": {
                    "value": "- The authors highlighted that they are using a more efficient training loss \"batch-based\", and claim it is sufficient to outperform a better but more expensive training loss \"trajectory-based\". Can the proposed method also use \"trajectory-based\" loss to further improve the performance? Or is it the proposed method itself is heavier then others, so that it can't be optimized using the better loss? Note experiments are optional in the rebuttal.\n\n- As a researcher not working on this field, I had a hard time understanding what \"IPC\" means in experimental setup. Through searching other papers I get it is \"Images allocated Per Class\". It would be good if the paper can be more self-contained, to introduce this setting in the paper, and provide background knowledge on why this is used as the main setting. A more intuitive setting in my mind (without knowing the literature) is control the number of parameters of the learned data generators. Is this the same as the setup used in the paper?"
                },
                "questions": {
                    "value": "Overall this paper proposed a novel and valid method on an important task, with solid results. My questions are mostly for clarification, as I don't work on this field. I am happy to vote for an accept for now, conditioned on the authors clarifying my confusions in paper weaknesses during the rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4169/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814052641,
            "cdate": 1698814052641,
            "tmdate": 1699636382719,
            "mdate": 1699636382719,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "z1JPteIAH7",
                "forum": "fDZumshwym",
                "replyto": "zGZ7q97OrK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4169/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer #2 (Y8Kv) (Part 1)"
                    },
                    "comment": {
                        "value": "We thank Reviewer #2 (Y8Kv)\u2019s time and effort for the reviews and highlighting some points in our paper that required further clarification. We address the raised questions and concerns below: \n\n**Q1:** The authors highlighted that they are using a more efficient training loss \"batch-based\", and claim it is sufficient to outperform a better but more expensive training loss \"trajectory-based\". Can the proposed method also use \"trajectory-based\" loss to further improve the performance? Or is it the proposed method itself is heavier then others, so that it can't be optimized using the better loss? Note experiments are optional in the rebuttal.\n\n**A1:** We thank the reviewer for bringing the scalability of different losses up for discussion, and we are happy to explain more about this.\n\nThe major reason that we do not choose a trajectory-based loss is that trajectory-based loss is not scalable enough **not only with HMN (ours) but also with other data parameterization methods, like HaBa and LinBa.**\n\nIn Table 7 (Appendix D.2), we discussed the memory usage of different parameterization methods. We find that HaBa with a trajectory-based loss consumes 48GB of memory even on the CIFAR10 dataset, and LinBa always exceeds the memory limitation. LinBa\u2019s official implementation handles the memory issue by offloading GPU memory to CPU memory. However, CPU memory offloading leads to substantial computational overhead. For instance, in the CIFAR10 1 IPC task, LinBa requires 14 days to finish training on a 2080Ti, but training HMN with a batch-based loss takes only 15 hours with the same hardware. We thus choose the batch-based loss to train HMN. \n\nBesides, to have a fair comparison, we compare HMN with other data parameterization methods with the same batch-based loss in Table 3 ( Section 4.2 ), and we find HMN outperforms other methods.\n\nTo sum up, training with a batch-based loss allows us to evaluate our methods on more complex datasets, like Tiny-ImageNet and ImageNet subset (see general response Q2), with reasonable resources and in a reasonable time. We agree with the reviewer that combining trajectory-based loss with HMN could be a line of future investigation if the hardware supported much higher GPU memory levels, much higher running times were acceptable (with CPU memory offloading), or more scalable trajectory-based methods were developed.  We have revised Appendix D.2 to include more discussion on training with a trajectory-based loss."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700190323184,
                "cdate": 1700190323184,
                "tmdate": 1700191559100,
                "mdate": 1700191559100,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6gT2M1ncDv",
                "forum": "fDZumshwym",
                "replyto": "zGZ7q97OrK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4169/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks to Reviewer #2 (Y8Kv) and the Response Summary."
                    },
                    "comment": {
                        "value": "Dear Reviewer Y8Kv,\n\nWe greatly appreciate your time and efforts in reviewing our paper, and **we would like to briefly summarize the response to your major concerns for your convenience.** We also kindly refer you to our previous responses (both separate and general responses) for more detailed discussions.\n\n---\n\n> **Q: Can we combine HMN with a trajectory-based loss? How is the scalability of the proposed method?**\n\n**A:** In Table 7 (Appendix D.2), we show that trajectory-based loss is not scalable enough **not only with HMN (ours) but also with other data parameterization methods, like HaBa and LinBa (See original response for more details).** Training with a batch-based loss allows us to evaluate our methods on more complex datasets, like Tiny-ImageNet and ImageNet subset.  \n\nIn Table 7, we also show that our proposed end-to-end data condensation methods are more scalable than other SOTA data parameterization methods while achieving better performance. Our additional evaluation also shows that our proposed method also scales to complex datasets like ImageNet-10 and achieves better performance.\n\n> **Q: What is the definition of IPC?**\n\n**A:** The abbreviation IPC stands for \u201cImage Per Class\u201d. This metric is used to measure how many tensors the storage budget is equivalent to in terms of the number of images. For example, 1 IPC for CIFAR10 stands for the storage budget: pixels of an image * IPC * class = 3 * 32 * 32 * 1 * 10 = 30720 tensors.\n\n----\n\nWe have already revised the paper to include these points in the paper. **We also kindly refer you to our previous responses (both separate and general responses) for more detailed discussions.** We would like to check with you if our response and revised paper addressed your concerns and if we can provide more information to better address your concerns and clarify the claim in the paper!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675747028,
                "cdate": 1700675747028,
                "tmdate": 1700675881611,
                "mdate": 1700675881611,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pkjKTaIYJL",
                "forum": "fDZumshwym",
                "replyto": "6gT2M1ncDv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4169/Reviewer_Y8Kv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4169/Reviewer_Y8Kv"
                ],
                "content": {
                    "title": {
                        "value": "Thank you."
                    },
                    "comment": {
                        "value": "Thank the authors for providing the rebuttal. Both my questions are now clarified. I keep my positive rating."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700557997,
                "cdate": 1700700557997,
                "tmdate": 1700700557997,
                "mdate": 1700700557997,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Tci11265fN",
            "forum": "fDZumshwym",
            "replyto": "fDZumshwym",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4169/Reviewer_mnZx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4169/Reviewer_mnZx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method called Hierarchical Memory Network (HMN) that stores condensed data in a three-tier structure that reflects the hierarchical nature of image data. The authors exploited that HMN naturally ensures that generated images are independent and proposed a new algorithm to remove redundant images. The authors evaluated the model on four different datasets, and showed that their technique outperformed several SoTA baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1- The paper proposed a new algorithm to store condensed data in a three-tier memory structure: dataset-level, class-level, and instance-level.\n2- The authors proposed a pruning algorithm to prune redundant examples.\n3- The authors demonstrated the effectiveness of their method on 4 different datasets. Their method outperformed several SoTA baselines by convincing margines."
                },
                "weaknesses": {
                    "value": "1- It is difficult to follow some of the ideas presented in the paper. For example, the paper didn't mention the abbreviation for IPC. Also, the paper didn't talk about whether the decoder parameters is part of the budget or not.\n\n2- The paper demonstrated the effectiveness of their method on tiny datasets, and they didn't address the scalability of their technique. For example, the generated images seem very pixelated and abstract, how does their method perform in a more complex settings."
                },
                "questions": {
                    "value": "Table.1 What is the accuracy drop for randomly pruning 10% ?\n\nAlgorithm 1.  are you computing the accuracy in an online fashion, or you train the model for each subset selection? How this would scale for larger datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4169/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698864788753,
            "cdate": 1698864788753,
            "tmdate": 1699636382653,
            "mdate": 1699636382653,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7tzIBEzDxQ",
                "forum": "fDZumshwym",
                "replyto": "Tci11265fN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4169/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer #1 (mnZx)  (Part 1)"
                    },
                    "comment": {
                        "value": "We thank Reviewer #1 (mnZx)\u2019s time and effort for the reviews and suggestions to improve the paper. We address the raised questions and concerns below: \n\n**Q1:** 1- It is difficult to follow some of the ideas presented in the paper. For example, the paper didn't mention the abbreviation for IPC. Also, the paper didn't talk about whether the decoder parameters is part of the budget or not.\n\n**A1:** We are happy to clarify the unclear presentation in the paper. The abbreviation IPC stands for \u201cImage Per Class\u201d. This metric is used to measure how many tensors the storage budget is equivalent to in terms of the number of images(same as SOTA data parameterization work [1,2]).  For example, 1 IPC for CIFAR10 stands for the storage budget:  Pixels of an image * IPC * class = 3 * 32 * 32 * 1 * 10 = 30720 tensors. Similarly, 10 IPC for CIFAR100 stands for the storage budget:  Pixels of an image * IPC * class = 3 * 32 * 32 * 10 * 100 = 30720000 tensors. Same to the previous works, we treat those tensors as float32 tensors. We added the definition of 'IPC' the first time it was mentioned (Section 1) in the revised version.\n\n> Also, the paper didn't talk about whether the decoder parameters is part of the budget or not.\n\nBoth the decoder networks and memory tensors count towards the storage budget calculation. We also mentioned this in Section 3.1 and at the end of Section 4.1. We further highlighted this point in the revised version to eliminate potential confusion for readers.\n\n\n**Q2:** 2- The paper demonstrated the effectiveness of their method on tiny datasets, and they didn't address the scalability of their technique. For example, the generated images seem very pixelated and abstract, how does their method perform in a more complex settings.\n\n**A2:** We thank Reviewer #1 for bringing up scalability for discussion. \n\nIn Table 7 (Appendix D.2), we compare the scalability of end-to-end training algorithms of HMN with LinBa and HaBa, two data parameterization methods that have good performance. We find that HaBa with a trajectory-based loss consumes 48GB of memory even on the CIFAR10 dataset, and LinBa always exceeds the memory limitation. LinBa\u2019s official implementation handles the memory issue by offloading GPU memory to CPU memory. However, CPU memory offloading leads to substantial computational overhead. For instance, in the CIFAR10 1 IPC task, LinBa requires 14 days to finish training on a 2080Ti, but training HMN with a batch-based loss takes only 15 hours with the same hardware. We believe that our end-to-end data condensation method not only achieves better performance than SOTA data parameterization methods but also is more scalable.\n\nMoreover, we also conducted the evaluation on ImageNet-10 (see General Response Q2). The results show that HMN still achieves strong performance on a more complex high-resolution dataset.\n\n> For example, the generated images seem very pixelated and abstract.\n\nWe thank Reviewer #1 for pointing out the quality of the generated images. We think that it is a great question. We would like to clarify that the goal of data condensation is **not** to generate high-quality images, but **to generate images representing the training behavior of the original dataset**. Similar pixelation can also be found in other DC works, like DC, MTT, and HaBa. Pixelated images are fine as long as the resulting accuracy (performance) is still high for the level of compression desired.  We have revised Appendix D.6 to include generated image quality discussion.\n\n**Q3:** Table.1 What is the accuracy drop for randomly pruning 10% ?\n\n**A3:** We present the accuracy of random pruning on the CIFAR10 10 IPC HaBa condensed dataset as an addition to Table 1. We can see that, compared to AUM pruning, random pruning consistently has a larger accuracy drop, but 10% AUM pruning does not cause an accuracy drop, which indicates the existence of redundancy in training data generated by HaBa.\n\n| Pruning Rate |   0  |  10% |  20% |  30% |  40% |\n|:------:|:----:|:----:|:----:|:----:|:----:|\n|   AUM  | 69.5 | 69.5 | 68.9 | 67.6 | 65.6 |\n| Random | 69.5 | 68.7 | 67.5 | 65.9 | 63.8 |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700190059631,
                "cdate": 1700190059631,
                "tmdate": 1700190059631,
                "mdate": 1700190059631,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sHd3KmBnSq",
                "forum": "fDZumshwym",
                "replyto": "Tci11265fN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4169/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks to Reviewer #1 (mnZx) and the Response Summary on Major Concerns."
                    },
                    "comment": {
                        "value": "Dear Reviewer mnZx,\n\nWe greatly appreciate your time and efforts in reviewing our paper, and **We would like to briefly summarize the response to your major concerns for your convenience.** We also kindly refer you to our previous responses (both separate and general responses) for more detailed discussions.\n\n---\n\n> **Q: What is the definition of IPC? Is the decoder parameters part of the budget?**\n\n**A:** The abbreviation IPC stands for \u201cImage Per Class\u201d. This metric is used to measure how many tensors the storage budget is equivalent to in terms of the number of images. For example, 1 IPC for CIFAR10 stands for the storage budget: pixels of an image * IPC * class = 3 * 32 * 32 * 1 * 10 = 30720 tensors.\n\nBoth the decoder networks and memory tensors count towards the storage budget calculation, which guarantees the fairness of the comparison to other DC methods.\n\n> **Q: How is the scalability of the proposed method?**\n\n**A:** In Table 7 (Appendix D.2), we show that our proposed end-to-end data condensation methods are more scalable than other SOTA data parameterization methods while achieving better performance. Our additional evaluation also shows that our proposed method also scales to complex datasets like ImageNet-10 and achieves better performance.\n\n> **Q: Does Algorithm 1. train the model for each subset selection? How this would scale for larger datasets?**\n\n**A:** Algorithm 1 retrains the model for each subset selection for different hard pruning rates. However, Algorithm 1 requires a relatively short computation time in practice. For example, DC with HMNs for CIFAR10 1 IPC needs about 15 hours on a 2080TI GPU, but the entire Algorithm 1 only costs an additional 20 minutes. Moreover, when we use HMNs for future training, we do not need to search for the best hard pruning rate again.\n\n----\n\nWe have already revised the paper to include these points in the paper. **We also kindly refer you to our previous responses (both separate and general responses) for more detailed discussions.** We would like to check with you if our response and revised paper addressed your concerns and if we can provide more information to better address your concerns and clarify the claim in the paper!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675570489,
                "cdate": 1700675570489,
                "tmdate": 1700675570489,
                "mdate": 1700675570489,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]