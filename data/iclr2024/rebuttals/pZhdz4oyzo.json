[
    {
        "title": "SqueezeLLM: Dense and Sparse Quantization"
    },
    {
        "review": {
            "id": "2PYhGVUc3b",
            "forum": "pZhdz4oyzo",
            "replyto": "pZhdz4oyzo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission598/Reviewer_7Q1V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission598/Reviewer_7Q1V"
            ],
            "content": {
                "summary": {
                    "value": "In this article, the authors propose to address the memory footprint of LLMs. They argue that this represents the main challenge for efficient inference of such models. This claim is well-supported by previous literature in the field. To do so, they propose to combine two main elements: sparse encoding of outliers and hessian-based clustering. The presence of outliers is a well-known challenge regarding LLM quantization. These weights are challenging to encode. Instead of occupying values within the non-uniform quantization codebook. The author decompose the weight tensor in two: a small size (less than 1% of the total) is dedicated to said outliers in high precision and the remainder of the weight values are encoded in a low size LUT. Furthermore, in order to improve the performance of the LUT clustering, the authors propose to adapt the k-means algorithm to the specificity of the weight tensors of a trained model: all weight values are not trained equal. The hessian of the weight values is leveraged as an estimate to weight each weights contribution. \nThe resulting method achieves remarkable results on a variety of challenging benchmarks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written, the proposed method performs well and is thoroughly benchmarked on challenging problems. Although the core elements are not completely novel, from the outliers encoding to the use of the Hessian for importance estimation, the combination of the two is well organized and not so trivial. Consequently, this research contains all the necessary element for a paper published at ICLR."
                },
                "weaknesses": {
                    "value": "Overall, I have two remarks for improvement. First, here are some missing references. Regarding sparse encoding of the outliers, [1] propose an approach that bears some similarities and would be worth mentioning. Similarly, regarding the measurement of the importance using the Hessian matrix, pruning techniques have previously used similar estimates [2,3] and in particular [4]. I think the paper would benefit from these elements. Second, I think the results are great, so there is no need to not highlight other methods when they perform on par with the proposed SqueezeLLM.\n\n[1] Yvinec, Edouard, et al. \"REx: Data-Free Residual Quantization Error Expansion.\" arXiv preprint arXiv:2203.14645 (2022).\n[2] Molchanov, Pavlo, et al. \"Importance estimation for neural network pruning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.\n[3] Li, Mingchen, et al. \"Exploring weight importance and hessian bias in model pruning.\" arXiv preprint arXiv:2006.10903 (2020).\n[4] Yu, Shixing, et al. \"Hessian-aware pruning and optimal neural implant.\" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2022."
                },
                "questions": {
                    "value": "I wonder how would the authors fine-tune such compressed model without losing the benefits of the compression technique, i.e. how to fold a LoRA or else."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission598/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698418934919,
            "cdate": 1698418934919,
            "tmdate": 1699635987326,
            "mdate": 1699635987326,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QXbWkRTxwO",
                "forum": "pZhdz4oyzo",
                "replyto": "2PYhGVUc3b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission598/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission598/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## W1. References\n\n\n> First, here are some missing references. Regarding sparse encoding of the outliers, [1] propose an approach that bears some similarities and would be worth mentioning. Similarly, regarding the measurement of the importance using the Hessian matrix, pruning techniques have previously used similar estimates [2,3] and in particular [4]. I think the paper would benefit from these elements. Second, I think the results are great, so there is no need to not highlight other methods when they perform on par with the proposed SqueezeLLM.\n\nWe appreciate the reviewer\u2019s valuable feedback and the key references. We will include these references and comparisons in the final version of our paper to provide a more comprehensive context.\n\n---\n\n## Q1. Ideas on Fine-tuning\n\n\n> I wonder how would the authors fine-tune such compressed model without losing the benefits of the compression technique, i.e. how to fold a LoRA or else.\n\nThis is indeed an interesting question given that finetuning LLMs is emerging as an interesting research field. We envision that SqueezeLLM could be effectively employed as a backbone quantization method in parameter-efficient finetuning scenarios similar to QLoRA [1] \u2013 where the main model would be quantized for the forward pass, and only the smaller adapter modules would undergo weight updates \u2013 without losing the benefits of our compression technique. While it's common in practice [1, 2] to retain the LoRA module after fine-tuning, how to fold them will be an interesting avenue for future research.\n\n[1] Dettmers, Tim, et al. \"Qlora: Efficient finetuning of quantized llms.\" *arXiv preprint arXiv:2305.14314* (2023).\n\n[2] Chai, Yuji, et al. \"INT2. 1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation.\" *arXiv preprint arXiv:2306.08162* (2023)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission598/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700164899052,
                "cdate": 1700164899052,
                "tmdate": 1700164938269,
                "mdate": 1700164938269,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q75Xvmf2Yv",
                "forum": "pZhdz4oyzo",
                "replyto": "PHkt0AiruY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission598/Reviewer_7Q1V"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission598/Reviewer_7Q1V"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "I can confirm that the authors addressed my concerns. Consequently, I'll keep my initial rating."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission598/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727003833,
                "cdate": 1700727003833,
                "tmdate": 1700727003833,
                "mdate": 1700727003833,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QyDieuFYZr",
            "forum": "pZhdz4oyzo",
            "replyto": "pZhdz4oyzo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission598/Reviewer_sqPk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission598/Reviewer_sqPk"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to demonstrate that the main challenge with single-batch inference in generative Large Language Models (LLMs) is memory bandwidth rather than computing. The authors propose SqueezeLLM, a post-training quantization framework that can compress LLMs to 3-bit without losing performance. It uses the following ideas to optimize each weight tensor's bit precision and memory usage.\n\n- **Sensitivity-based non-uniform quantization**: The optimal bit precision assignment for each weight tensor can be determined by its sensitivity to quantization error, approximated by second-order information.\n- **Dense-and-Sparse decomposition**:  The outliers and sensitive weight values that cannot be quantized effectively can be stored in a sparse format with full precision. The remaining weight values correspond to dense components that could be easily quantized."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written, and the contributions are easy to understand. Figure 1 illustrates the impact of the key approaches of the work. The research community, spanning hardware and ML, has well-acknowledged the memory wall issue for such models. Section 3 provides this line of work justice with supporting illustration in Fig. A.1. \n\nThe paper is well positioned in related works \u2013 focus on weight quantization versus activation, following the OBD vs OBC (GPTQ Frantar et al. (2022), AWQ Lin et al. (2023), and SpQR Dettmers et al. (2023)) framework to preserve final output performance. \n\nFor applications such as LLMs, the authors combine two techniques: K-means clustering for quantization of sensitive weights/outliers guided by second-order derivative and dense and sparse decomposition/quantization. Please refer to the Weakness section for further discussion.\n\nThe experiments and empirical analysis are extensive, supporting the technical contributions."
                },
                "weaknesses": {
                    "value": "Regarding the two approaches, there needs to be more discussion on prior works.\nK-means clustering for Quantization is a popular technique used in signal processing. For neural networks, works such as DeepCompression [Han et al., ICLR 2016], SLQ/MLQ [Xu et al., AAAI 2018], HPTQ [Xu et al., IEEE Signal Processing Letters 2023] employ k-means clustering for quantization. It would be great to acknowledge such and similar works in literature and compare them to justify the novelty claimed in Sec 4.1. Of course, LLMs as a target application is new, but the technique may need to be more novel in its current presentation.\n\nA similar treatment is observed in Dense and Spare Decomposition (Sec 4.2), where similar techniques exist in the literature, not necessarily for LLMs.  The author must acknowledge and discuss the prior works, such as DSD [Han et al., ICLR 2017], Scatterbrain [Chen et al., NeurIPS 2021], Vitality [Dass et al., HPCA 2023], etc to bring out the novelty.\n\nOverall, it seems the LLMs provide opportunities to employ a novel combination of existing techniques. If so, the current presentation of Sec 4 does not paint an accurate picture and needs to be rewritten to acknowledge similar works and present the novelty. In addition, discussing SmoothQuant [Xiao et al., arXiv 2022, ICML 2023] in the context of post-training quantization for LLMs might be worthwhile."
                },
                "questions": {
                    "value": "As discussed in the Weakness section, the authors should clarify the novelty by acknowledging and discussing prior works with the techniques presented. Are the techniques novel, or do LLMs provide opportunities for a novel combination of the existing techniques in literature? Sec 4, in its presentation, implies that techniques are novel, which may not be necessarily true."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission598/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793216929,
            "cdate": 1698793216929,
            "tmdate": 1699635987247,
            "mdate": 1699635987247,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b0Ual2MgJ6",
                "forum": "pZhdz4oyzo",
                "replyto": "QyDieuFYZr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission598/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission598/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer's insightful comments and the references provided. We have carefully reviewed the mentioned works and emphasize our novelty as follows. We will include this comparison in the paper to acknowledge the prior works and clarify the novelty of our approach.\n\n---\n\n## W1. Comparisons with Other Clustering Methods\n\n\n> Regarding the two approaches, there needs to be more discussion on prior works. K-means clustering for Quantization is a popular technique used in signal processing. For neural networks, works such as DeepCompression [Han et al., ICLR 2016], SLQ/MLQ [Xu et al., AAAI 2018], HPTQ [Xu et al., IEEE Signal Processing Letters 2023] employ k-means clustering for quantization. It would be great to acknowledge such and similar works in literature and compare them to justify the novelty claimed in Sec 4.1. Of course, LLMs as a target application is new, but the technique may need to be more novel in its current presentation.\n\nThat is a fair question. About the comparison with prior k-means methods, we kindly emphasize that our approach is different. In particular, SqueezeLLM incorporates a **sensitivity-aware weighted k-means clustering method combined with the dense-and-sparse decomposition, which allows for effective low-bit LLM quantization.**\n\nIn particular, our *sensitivity-aware* weighted clustering is different from the non-weighted (i.e. sensitivity-agnostic) clustering approaches in the prior works and allows minimal performance degradation without retraining. As discussed in Section 4.1, our method ensures minimal loss degradation under widely accepted assumptions, and therefore significantly reduces performance degradation in contrast to the sensitivity-agnostic approach. This is demonstrated in Figure 1, with a considerable improvement from 18.09 (sensitivity-agnostic) to 7.75 (sensitivity-aware). Furthermore, such a precise allocation of LUT entries through the sensitivity-aware approach is especially critical for LLM quantization, where retraining or finetuning can be highly restricted. This is in contrast with methodologies like DeepCompression or SLQ/MLQ, which performs post-clustering retraining to recover performance.\n\n---\n\n## W2. Comparisons with Other Decomposition Methods\n\n\n> A similar treatment is observed in Dense and Sparse Decomposition (Sec 4.2), where similar techniques exist in the literature, not necessarily for LLMs. The author must acknowledge and discuss the prior works, such as DSD [Han et al., ICLR 2017], Scatterbrain [Chen et al., NeurIPS 2021], Vitality [Dass et al., HPCA 2023], etc to bring out the novelty.\n\nDSD is a significant paper that introduces a novel way of training neural networks where the term \"sparse\" represents *post-pruned weights*, and \"dense\" denotes weights that have been recovered post-pruning. However, in our approach, \u201csparse\u201d refers to isolated *outlier weights* to enhance post-training quantization performance. Furthermore, DSD operates as a *training-time technique*, which trains the post-pruned (so-called sparse) weights and the post-recovery (so-called dense) components separately before merging them into a single matrix, whereas our method is tailored for *inference*, with a major focus on enhancing inference-time efficiency and latency. While DSD and ours use the same terms of \u201cdense\u201d and \u201csparse\u201d, their implications are dissimilar.\n\nWith regards to Scatterbrain and Vitality, our method proposes a direct solution to tackle the outlier issue in LLMs, which has been a major obstacle for low-bit LLM quantization. In contrast, both Scatterbrain and Vitality introduce sparsity to represent *attention maps* as a combination of large attention values and the low-rank approximation of the residual matrix. Additionally, our approach further incorporates \u201csensitive\u201d values within the sparse matrix, which significantly mitigates distortion in non-uniform quantization LUT entries, leading to a considerable improvement in post-quantization performance (7.67 vs 7.56 in Figure 1)."
                    },
                    "title": {
                        "value": "Official Comment by Authors [1/2]"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission598/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700165098741,
                "cdate": 1700165098741,
                "tmdate": 1700165199940,
                "mdate": 1700165199940,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wr9LPCxxxW",
                "forum": "pZhdz4oyzo",
                "replyto": "QyDieuFYZr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission598/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission598/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## W3. Novelty and Comparison with SmoothQuant\n\n\n> Overall, it seems the LLMs provide opportunities to employ a novel combination of existing techniques. If so, the current presentation of Sec 4 does not paint an accurate picture and needs to be rewritten to acknowledge similar works and present the novelty. In addition, discussing SmoothQuant [Xiao et al., arXiv 2022, ICML 2023] in the context of post-training quantization for LLMs might be worthwhile.\n\nLLMs indeed present *unique behavior and challenges* compared to previous models, necessitating novel approaches to enable effective low-bit quantization. SmoothQuant, as the reviewer mentioned, exemplifies one such novel approach that offers a quantization method that tackles new challenges in LLMs and facilitates more accurate quantization. We would also like to kindly emphasize that SqueezeLLM has introduced two novel methods, the sensitivity-aware weighted k-means clustering method combined with the dense-and-sparse decomposition, to tackle the *unique* challenges in effective low-bit LLM quantization.\n\nWe acknowledge the reviewer\u2019s suggestion, and we will include SmoothQuant in the paper. At the same time, we kindly emphasize that our approach is fundamentally different from SmoothQuant. While SmoothQuant is a quantization framework for both *weights and activations* with a specific focus on mitigating the *activation outliers* by reweighting the associated weight channels, SqueezeLLM is a *weight-only* quantization method that focuses on *weight outliers*."
                    },
                    "title": {
                        "value": "Official Comment by Authors [2/2]"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission598/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700165147878,
                "cdate": 1700165147878,
                "tmdate": 1700165209107,
                "mdate": 1700165209107,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0rNu561fXE",
            "forum": "pZhdz4oyzo",
            "replyto": "pZhdz4oyzo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission598/Reviewer_Ed31"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission598/Reviewer_Ed31"
            ],
            "content": {
                "summary": {
                    "value": "This study introduces a novel method for compressing memory-limited LLMs by employing two primary techniques: 1) non-uniform quantization, which is based on clustering, and 2) the identification and extraction of outliers within weight parameters. While these strategies have the potential to disrupt acceleration mechanisms in high-performance computing systems, this paper also offers a dedicated kernel tailored for the proposed method. The efficacy of both the method and the kernel is further demonstrated through comprehensive experimental results on contemporary LLMs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper is articulately composed and effectively describes the memory challenges associated with LLMs.\n- The study adeptly builds upon existing compression techniques for LLMs. While many papers focus solely on uniform quantization, this one introduces non-uniform quantization.\n- The experimental results encompass a wide range of models and datasets."
                },
                "weaknesses": {
                    "value": "*Concerns Regarding Citations:*\nThe citation format utilized in the paper is incorrect. I'd recommend adhering to the ICLR latex format for consistency.\nI noticed references to non-uniform quantization, specifically (Chung, 2020) and (Jeon, 2022), in the appendix. These papers employ non-uniform quantization as a structured extension of binary quantization. This differs from the clustering-based vector quantization used in your paper.\n\n*Implementation and Kernel Concerns:*\nI have reservations about the kernel implementation. Introducing such custom kernel, as described in your method, appears to disrupt the established high-performance computing framework. Although an unstructured or non-uniform structure might enhance model performance, it could also negatively impact acceleration performance or complicate it. Hence, a more detailed exposition about the kernel is essential to substantiate the novelty of your method. While your results indicate an improved model performance, the absence of a dedicated kernel might undermine its effectiveness. Additionally, it's worth pondering why there was no effort to implement and optimize the kernel specifically for the A100. Even though the A100/H100 might be a costly choice, they can serve as a foundational hardware for large language models, particularly in tandem with NVIDIA's inference software. It's noteworthy that the memory bandwidth for the RTX series is below 1TB/s.\n\n*Feedback on Model Performance:*\nThe majority of the model performance results focus on PPL outcomes. I believe it would be beneficial to include MMLU or CSR results for larger models within the main content, rather than relegating it to the appendix.\n\nIn conclusion, due to the aforementioned concerns, I am inclined to assign a score of 5 to this paper. While I perceive this paper as integrating various methodologies, I will defer to other reviewers for their viewpoints on this aspect."
                },
                "questions": {
                    "value": "included in weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission598/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823881488,
            "cdate": 1698823881488,
            "tmdate": 1699635987098,
            "mdate": 1699635987098,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o9rHZ1PjFr",
                "forum": "pZhdz4oyzo",
                "replyto": "0rNu561fXE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission598/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission598/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer's insightful comments, and here we address your comments in detail:\n\n## W1. Kernel Concerns and Evaluation on A100\n\n> Implementation and Kernel Concerns: I have reservations about the kernel implementation. Introducing such a custom kernel, as described in your method, appears to disrupt the established high-performance computing framework. Although an unstructured or non-uniform structure might enhance model performance, it could also negatively impact acceleration performance or complicate it. Hence, a more detailed exposition about the kernel is essential to substantiate the novelty of your method. While your results indicate an improved model performance, the absence of a dedicated kernel might undermine its effectiveness. Additionally, it's worth pondering why there was no effort to implement and optimize the kernel specifically for the A100. Even though the A100/H100 might be a costly choice, they can serve as a foundational hardware for large language models, particularly in tandem with NVIDIA's inference software. It's noteworthy that the memory bandwidth for the RTX series is below 1TB/s.\n\nThis is a fair point. We had originally focused on lower-grade GPUs partly due to the need for enabling inference on more affordable GPUs, and partially due to our limited access to A100 systems for our experiments. However, in light of the reviewer\u2019s insightful comments, we have conducted additional experiments using our kernels on an A100 system to run the linear layers of LLaMA model for generating a sequence length of 128. As demonstrated in the table below, our kernel implementation still attains **1.5-2.5x performance speedups** relative to the fp16 matrix-vector multiply kernel across different model sizes, similar to what we have observed with our previous experiments. This is despite any additional optimizations or tuning.\n\nTherefore, our evaluation results indicate that SqueezeLLM **does not introduce any negative impact on acceleration performance** due to our unstructured or non-uniform structure. Furthermore, we would like to highlight that the implementation of SqueezeLLM does not introduce complexity. The core of SqueezeLLM is non-uniform quantization which only involves look-up-table operations, which can be implemented as *simply and efficiently* as those in uniform quantization.\n\\\n\\\n**Matrix-Vector Multiply Kernel Runtime for Generating 128 Tokens**\n\n| LLaMA Model | Baseline (FP16) | SqueezeLLM (4-bit) | SqueezeLLM (4-bit, 0.45% outliers) | SqueezeLLM (3-bit) | SqueezeLLM (3-bit, 0.45% outliers) |\n| ----------- | --------------- | ------------------ | ---------------------------------- | ------------------ | ---------------------------------- |\n| 7B          | 1.21            | 0.83               | 1.09                               | 0.56               | 0.83                               |\n| 13B         | 2.32            | 1.52               | 1.87                               | 0.97               | 1.32                               |\n| 30B         | 5.56            | 3.66               | 4.25                               | 2.26               | 2.86                               |\n\n---\n\n## W2. Feedback on Model Performance\n\n\n> Feedback on Model Performance: The majority of the model performance results focus on PPL outcomes. I believe it would be beneficial to include MMLU or CSR results for larger models within the main content, rather than relegating it to the appendix.\n\nThank you for your constructive feedback. We acknowledge your point regarding the inclusion of MMLU and CSR results for larger models in the main body of the content. As can be seen in Table A.4 and A.7, our method *consistently outperforms* other methods both in terms of perplexity as well as MMLU accuracy on *larger models*, showing the strong transferability of the results in the smaller models to the larger models. The decision to place these results in the appendix was due to the page constraints, and we will include it in the main content in the final version. Reviewer 1 also asked us to perform 5-shot MMLU results which are included in response to their question (please see W3 of Reviewer U5Sb)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission598/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700165434768,
                "cdate": 1700165434768,
                "tmdate": 1700165434768,
                "mdate": 1700165434768,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ot2AfztQjD",
                "forum": "pZhdz4oyzo",
                "replyto": "bj0bM4XFeY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission598/Reviewer_Ed31"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission598/Reviewer_Ed31"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response. However, as I haven't reviewed the Revision paper, the points you've just raised are insufficient to elevate this paper to an acceptable level. Frankly, the issues I've identified are not easily resolved in a short period, and I acknowledge that substantial portions of the paper might need alteration. Including proper experimental and analytical results for the A100 Kernel instead of A6000, and shifting to more advanced evaluation methods like MMLU / CSR, will certainly demand considerable time."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission598/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722624342,
                "cdate": 1700722624342,
                "tmdate": 1700722624342,
                "mdate": 1700722624342,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "S8sMKYJ8H4",
            "forum": "pZhdz4oyzo",
            "replyto": "pZhdz4oyzo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission598/Reviewer_U5Sb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission598/Reviewer_U5Sb"
            ],
            "content": {
                "summary": {
                    "value": "To address memory bandwidth constraints of large language models (LLMs), introduced SqueezeLLM, a post-training quantization framework, compresses LLMs to low precisions (to 4-bit or 3-bit range) without compromising performance. Two strategies underpin SqueezeLLM. One is sensitivity-based non-uniform quantization, which optimizes bit precision based on the weight distributions in LLMs. Second is dense-and-sparse decomposition, which stores outliers and simplifies the quantization process for the remaining weights. Extensive evaluations show that SqueezeLLM consistently surpasses existing quantization techniques across various bit precisions and tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper well explains the \"memory wall\" problem of LLMs and justifies the need for weight-only non-uniform quantization.\n* The proposed method, SqueezeLLM, considers both the sensitivity and outlier of the weights while compressing using LLM's weight-only non-uniform quantization format. \n* The proposed method demonstrates competitive performance across different sizes of models and tasks.\n* Provide a latency report for the proposed kernel and compare it with others.\n* The paper is well-structured and the proposed method is clearly elucidated."
                },
                "weaknesses": {
                    "value": "* SqueezeLLM uses a small sample of the training dataset to execute end-to-end forward and backward passes for gradient computations. This process appears to be more resource-intensive than other methods like RTN, GPTQ, or AWQ. It would be beneficial to understand the time and resources required for SqueezeLLM's dense-and-sparse quantization.\n* Relatedly, in the discussion about the need to minimize overall perturbations for the final loss term in section 4.1's \"Sensitivity-Based K-means Clustering,\" the paper should also compare its performance with methods like AdaRound [1] or FlexRound [2]. These methods utilize a small calibration set and employ layer-wise or block-wise post-training quantization techniques. Notably, since FlexRound reports results on LLaMA using uniform weight-only quantization, it would strengthen the paper's claim about optimization with the final loss.\n* It would be beneficial to display five-shot performance results on the MMLU benchmark using LLaMA, as this would offer a more comprehensive comparison with other methodologies.\n\n[1]Nagel, Markus, et al. \"Up or down? adaptive rounding for post-training quantization.\"\u00a0International Conference on Machine Learning. PMLR, 2020.  \n[2]Lee, Jung Hyun, et al. \"FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization.\"\u00a0International Conference on Machine Learning. PMLR, 2023."
                },
                "questions": {
                    "value": "* Is there a relationship between outliers and sensitive weights? Specifically, are most of the sensitive weights outliers?\n* Have you experimented with combining a uniform quantization scheme with the sparse decomposition concept?\n* In Table 1, there's a comparison with AWQ's latency performance. However, such a comparison is missing in Table 3 (section 5.4). Since the AWQ kernel is well showcased, it would be more convincing to show a comparison of the proposed kernel with AWQ."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission598/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission598/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission598/Reviewer_U5Sb"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission598/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830892453,
            "cdate": 1698830892453,
            "tmdate": 1699635987029,
            "mdate": 1699635987029,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G8po6HGIIE",
                "forum": "pZhdz4oyzo",
                "replyto": "S8sMKYJ8H4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission598/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission598/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer's constructive feedback, and here we address your comments in detail:\n\n## W1. Resource Requirements\n\n> SqueezeLLM uses a small sample of the training dataset to execute end-to-end forward and backward passes for gradient computations. This process appears to be more resource-intensive than other methods like RTN, GPTQ, or AWQ. It would be beneficial to understand the time and resources required for SqueezeLLM's dense-and-sparse quantization.\n\nThat is correct and SqueezeLLM involves an end-to-end forward and backward pass over a small set of sampled data. This method differs from other methods such as RTN, which does not require any forward or backward pass, and GPTQ or AWQ, which requires isolated gradient calculation over individual layers, resulting in lower overhead. However, it is important to note that the overhead of SqueezeLLM is quite manageable and, crucially, our method consistently outperforms RTN, GPTQ, and AWQ as shown in the paper (Table 1).\nTo provide a clearer understanding of SqueezeLLM\u2019s resource requirements, we conducted a comprehensive analysis of the time and memory requirements for (1) Fisher information computation and (2) sensitivity-aware k-means clustering for nonuniform quantization as below.\n\\\n\\\n**1. Fisher Information Computation:**\n\n| LLaMA Model | End-to-End Latency (s) | Peak Memory (GB) |\n| --- | -- | -- |\n| 7B   | 20     | 33   |\n| 13B   | 35     | 61               |\n| 30B         | 80   | 149              |\n| 65B         | 150     | 292              |\n\n\nWe assessed the end-to-end latency and peak memory usage for computing Fisher information on an A100 system across different LLaMA models. The results indicate that the latency for calculating Fisher information (i.e. the sum of squares of gradients) is pretty fast and quantizing the 65B model can be performed on 4 A100 GPUs to address the peak memory usage.\n\\\n\\\n**2. Sensitivity Aware K-means Clustering:**\n\n| LLaMA Model | End-to-End Latency (min) |\n| -- | ----- |\n| 7B  | 18    |\n| 13B   | 33  |\n| 30B  | 71  |\n| 65B  | 120 |\n\n\nAnother key component of SqueezeLLM is the sensitivity-aware K-means clustering applied to each weight matrix. This process can be executed efficiently in parallel across different layers, possibly using multiple cloud CPU instances. Our latency measurements on 8 Intel Xeon Platinum 8380 systems show that the time overhead for this clustering process is quite manageable.\nWe will include these data in the final version of the paper to offer a comprehensive view of SqueezeLLM's time and resource demands.\n\n---\n\n## W2. Comparison with FlexRound\n\n> Relatedly, in the discussion about the need to minimize overall perturbations for the final loss term in section 4.1's \"Sensitivity-Based K-means Clustering,\" the paper should also compare its performance with methods like AdaRound [1] or FlexRound [2]. These methods utilize a small calibration set and employ layer-wise or block-wise post-training quantization techniques. Notably, since FlexRound reports results on LLaMA using uniform weight-only quantization, it would strengthen the paper's claim about optimization with the final loss.\n\n\nThat is a great question. Below in the table, we provide a comparison with AdaRound and FlexRound. We first have to kindly emphasize that it is unfortunately not feasible to do a direct comparison with FlexRound due to their closed-source code and models, as well as a lack of experimental details for their perplexity evaluation on WikiText. However, to address the reviewer\u2019s question to the best of our ability,  we have utilized the results published in the FlexRound paper (for AdaRound and FlexRound) to assess the amount of post-quantization perplexity degradation of each method across different LLaMA models. As one can see, SqueezeLLM consistently exhibits a smaller drop in perplexity compared to AdaRound and FlexRound.\n\nWe should also kindly emphasize that we have performed a comprehensive comparison with the *latest weight-only quantization schemes* designed for LLMs, such as AWQ and SpQR, which share the same goal of minimizing output activation perturbation as FlexRound. In these comparisons, our method consistently demonstrates superior performance, both with 4-bit and 3-bit. Therefore, we maintain that our sensitivity-based k-means clustering that optimizes the final loss provides a more effective post-training quantization solution compared to other existing quantization strategies.\n\\\n\\\n**Perplexity on Wikitext**\n\n| LLaMA Model | AdaRound Perplexity Drop (baseline \u2192  quantized) | FlexRound Perplexity Drop (baseline \u2192  quantized) | SqueezeLLM Perplexity Drop (baseline \u2192  quantized) |\n| ----------- | -- | -- | --- |\n| 7B          | 0.79 (8.90 \u2192 9.69)        | 0.28 (8.90 \u2192 9.18)    | **0.09 (5.68 \u2192 5.77)**     |\n| 13B         | 0.34 (7.73 \u2192 8.07)    | 0.16 (7.73 \u2192 7.90)   | **0.09 (5.09 \u2192 5.18)**     |\n| 30B         | 0.53 (6.35 \u2192 6.88)     | 0.28 (6.35 \u2192 6.63)     | **0.08 (4.10 \u2192 4.18)**   |"
                    },
                    "title": {
                        "value": "Official Comment by Authors [1/2]"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission598/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700165552125,
                "cdate": 1700165552125,
                "tmdate": 1700165681953,
                "mdate": 1700165681953,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eZLXpG0kXz",
                "forum": "pZhdz4oyzo",
                "replyto": "S8sMKYJ8H4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission598/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission598/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors [2/2]"
                    },
                    "comment": {
                        "value": "## W3. 5-shot MMLU Performance\n\n\n> It would be beneficial to display five-shot performance results on the MMLU benchmark using LLaMA, as this would offer a more comprehensive comparison with other methodologies.\n\nThat is an excellent question. We have included five-shot performance results on the MMLU benchmark for Vicuna-v1.1 in the table below and also included a comparison with AWQ with the same model size. Similar to the zero-shot results that we had included in the paper, we can see that SqueezeLLM **consistently achieves higher-quality results**. At the moment, we are also running a comparison with GPTQ but we expect GPTQ to have a similar performance as AWQ. We will include these results in the final version of the paper beside Table 2 where we compared instruction following capabilities of the different models.\n\\\n\\\n**MMLU 5-shot Accuracy**\n\n| Vicuna-v1.1 | Baseline (FP16) | SqueezeLLM (3.24 bit) | AWQ (3.24 bit) |\n| --| -- | -- | --------------------------------------- |\n| 7B          | 45.3%  | **42.2%**  | 41.4%|\n| 13B         | 50.0%  | **48.2%**   | 46.3% |\n\n\n---\n\n\n## Q1. Outliers vs. Sensitive Values\n\n\n> Is there a relationship between outliers and sensitive weights? Specifically, are most of the sensitive weights outliers?\n\nFrom our extensive experiments conducted on Vicuna, LLaMA, and LLaMA2 models, we found no direct correlation between outliers and sensitive weights. In fact, we frequently observed that weights with smaller values can also be sensitive (as in Figure 2, Left). This is expected since the sensitivity measured by the Fisher information is not necessarily correlated with the magnitude of the weight value, but more with the curvature of the loss function with respect to that weight value.\n\n---\n\n## Q2. Uniform Quantization + Dense-and-Sparse Decomposition\n\n\n> Have you experimented with combining a uniform quantization scheme with the sparse decomposition concept?\n\nYes, we have indeed explored integrating uniform quantization with the dense-and-sparse decomposition while developing SqueezeLLM. Specifically, we applied dense-and-sparse decomposition in combination with channel-wise uniform quantization on LLaMA 7B and 13B models. In the table below, we compared the performance of uniform quantization and our sensitivity-aware non-uniform quantization, both paired with the dense-and-sparse decomposition. The results indicate that non-uniform quantization based on our sensitivity-aware K-means method yields significantly better perplexity.\nIt is worth noting that the performance of uniform quantization could potentially be improved through grouped quantization strategies, where a small set of weights (e.g. 128 as in the table below) are grouped together to have their own scaling factor. While grouped quantization, as demonstrated in techniques like GPTQ and AWQ, is a viable approach, the SqueezeLLM approach offers a much simpler implementation by avoiding the complexities of fine-grained grouping and it still outperforms uniform quantization methods with grouping.\n\\\n\\\n**Perplexity on C4**\n\n| LLaMA Model | Uniform | Dense-and-Sparse + Uniform | Dense-and-Sparse + Uniform with grouping = 128 | Dense-and-Sparse + Non-uniform |\n| ----------- | ------- | -------------------------- | ---------------------------------------------- | ------------------------------ |\n| 7B          | 28.26   | 9.25                       | 8.32                                           | **7.56**                       |\n| 13B         | 13.24   | 7.96                       | 7.30                                           | **6.92**                       |\n\n---\n\n## Q3. Comparison with AWQ kernels\n\n\n> In Table 1, there's a comparison with AWQ's latency performance. However, such a comparison is missing in Table 3 (section 5.4). Since the AWQ kernel is well showcased, it would be more convincing to show a comparison of the proposed kernel with AWQ.\n\nWe appreciate the suggestion to include a comparison with AWQ's public kernel. However, as we outlined in Appendix A.3, we chose not to compare our kernels directly with those from AWQ for a couple of key reasons: (1) The 3-bit kernels of AWQ are not publicly available; and (2) AWQ has incorporated optimizations that are unrelated to quantization, such as LayerNorm and positional embedding. These optimizations, while effective, are universally applicable to various quantization methods including SqueezeLLM and GPTQ."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission598/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700165625105,
                "cdate": 1700165625105,
                "tmdate": 1700165625105,
                "mdate": 1700165625105,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JhuSWCq7HI",
                "forum": "pZhdz4oyzo",
                "replyto": "S8sMKYJ8H4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission598/Reviewer_U5Sb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission598/Reviewer_U5Sb"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Author,  \nThank you for responding to my previous concerns.   \nI have an additional question regarding Equation (4) in your paper. It appears that Equation (4) requires a more explicit derivation. The interpretation of Equation (4) as the minimization problem involving the sum of the Fisher Information for the weight difference between the original and quantized weights is somewhat unclear to me. Specifically, I am struggling to understand the connection between this concept and \u201cminimizing the overall perturbation with respect to the final loss term.\u201d which is the term on the paper.\nTo the best of my knowledge, as demonstrated in the BRECQ paper [1], the derivation of Equation (3) should conclude with an expression similar to:  \n  \n$argmin$ $\\sum_{i=1}^{N}$ $\\mathscr{F}$ $(y-\\hat{y})^2$  \n  \nwhere y is an original output of each layer, $\\hat{y}$ is an output of the quantized layer.\nFor additional context, I refer you to Appendix A.1, \u201cProof of Theorem 3.1,\u201d in the BRECQ paper on page 12 [1].  \n\n[1] https://openreview.net/pdf?id=POWv6hDd9XH"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission598/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589999193,
                "cdate": 1700589999193,
                "tmdate": 1700590468340,
                "mdate": 1700590468340,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YxfZRmPazf",
                "forum": "pZhdz4oyzo",
                "replyto": "A0IMomjDhW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission598/Reviewer_U5Sb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission598/Reviewer_U5Sb"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Author,\n\nThank you for your prompt and informative response regarding the derivation process in your paper.\n\nWhile I appreciate the detailed explanation responding to my concern, I find it challenging to clearly discern why differentiating with respect to weights is more advantageous than differentiating with respect to outputs, as more commonly seen in the literature. Additionally, a clear derivation showing how this approach leads to the optimization objective presented in Equation 4 is essential to validate your methodology.\n\nTherefore, I kindly request that you provide a more explicit and detailed derivation of this process in your paper."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission598/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709379292,
                "cdate": 1700709379292,
                "tmdate": 1700709379292,
                "mdate": 1700709379292,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]