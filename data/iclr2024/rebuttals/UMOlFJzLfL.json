[
    {
        "title": "A Precise Characterization of SGD Stability Using Loss Surface Geometry"
    },
    {
        "review": {
            "id": "DyojlzrUsO",
            "forum": "UMOlFJzLfL",
            "replyto": "UMOlFJzLfL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7646/Reviewer_SPXN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7646/Reviewer_SPXN"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript investigates the linear stability of SGD and obtain a sufficient condition for instability (equivalently, a necessary condition for stability).\nThe authors introduce a coherence measure $\\sigma$ to measure the strength of  alignment among Hessian matrices. Using this measure, they derive their main result Theorem 1, which, as they claim, is more general than Theorem 3.3 in [Wu et al, 2022]. The authors also show that Theorem 1 is nearly optimal given that $\\sigma$ and $n/B$ are $O(1)$ quantities. Some experiments are carried out to support the theoretical results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "On a whole I think these results are neat, novel and have their own advantages.  The characterization seems to be rather precise and Lemma 4.1 is of particular interest."
                },
                "weaknesses": {
                    "value": "- The writing of this manuscript needs to be improved. In particular, the many details are present in an unclear way and I find it very hard to follow them smoothly. The citations and references also need to be re-organized. See Question section for more details. \n\n- The definition of ``Coherence measure'' is not intuitive.  Personally, I do not understand why the proposed definition can quantify the coherence among Hessian matrices. In particular, the authors claim that $\\lambda_1(H_i)$ is the i-th diagonal entry of $S$. This is obviously not true unless that $H_i$ is rank-1.\n\n- The authors might make a better interpretation of Theorem 1. What does this result imply about the implicit regularization of SGD (beyond that of GD)? How stability is related to the hyperparameters, e.g. $\\eta$, $B$, and alignment of $H_i$? Some relevant discussion can be found in the experiment part Section 5.2, but I think it would be better to provide some intuition right after Theorem 1.\n\n - In Section 3.2.1 the authors compare Theorem 1 to Theorem 3.3 in [Wu et al, 2022], and stated the advantages of their result. Among these stated advantages,\n   - The first point makes sense to me.\n   - In the second point, why do you say ``This definition is notably weaker than our notion of stability''?\n   - By the third point, you seem to imply that the bound in Theorem 1 is sharper in the low-rank case. But what is the point in considering $\\sigma$ equal to one? To me, the third point is an unclear comparison between two results, which cannot prove the advantage of Theorem 1.\n\n\n- Theorem 2, the optimality of Theorem 1, strongly relies on the condition that $\\sigma, n/B = O(1)$. There are two concerns:\n   - In Theorem 2 the authors assume that $\\sigma \\in [n]$. Is there anything to guarantee $\\sigma \\leq n$? I don't think it is clear from the definition of $\\sigma$. Is $\\sigma$ inherently bounded? Moreover, do you mean $\\sigma\\leq n$? Hence, it is unclear what the assumption $\\sigma=O(1)$ means.\n   - Also, I do not think it is natural to assume $n/B=O(1)$ as usually $B\\ll n$."
                },
                "questions": {
                    "value": "- In paragraph 1, when introducing the concept of \"implicit bias\", instead of citing (Li et al., 2022), I think it is more appropriate to cite the seminar works  (Neyshabur et al., arXiv:1412.6614) and (Zhang et al., ICLR 2017). \n - In paragraph 2, when citing empirical works on relating sharpness to generalization, I think the important comprehensive investigations by (Jiang et al., ICLR2020) is missed.  \n - In paragraph 4, when stating \"GD ... empirically validated to predict the sharpness of overparameterized neural network\", the author cites (Cohen et al., 2021). However, this empirical predictability of linear stability analysis has been observed in (Wu et al., NeurIPS2018).\n - In Section 2\n \t- In paragraph 1, when stating the rationale for assuming over-parameterization, the authors cite the work (Allen-Zhu et al., 2019). This seems quite strange to me. \n \t- In Definition 1,  it is unclear whether the sampling is done with or without replacement.\n- In Section 3\n\t- What is the $\\frac{1}{B}\\sum_{i=1}^n x_i H_i$ in the second paragraph. \n\t- In Theorem 1, what does the subscript in $\\hat{J}_i$ stand for? Complexity measure => coherence measure. \n\t- In Theorem 2, what do you mean $\\sigma\\in [n]$? Is $\\sigma$ a real value?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698746816699,
            "cdate": 1698746816699,
            "tmdate": 1699636929573,
            "mdate": 1699636929573,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XKDTIRPuiX",
                "forum": "UMOlFJzLfL",
                "replyto": "DyojlzrUsO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7646/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments.\n\n> The writing of this manuscript needs to be improved. In particular, the many details are present in an unclear way and I find it very hard to follow them smoothly. The citations and references also need to be re-organized. See Question section for more details.\n\nBelow we have responded to your points in the Question section.\n\n> The definition of ``Coherence measure'' is not intuitive. Personally, I do not understand why the proposed definition can quantify the coherence among Hessian matrices. In particular, the authors claim that $\\lambda_1(H_i)$ is the i-th diagonal entry of $S$. This is obviously not true unless that $H_i$ is rank-1.\n\nThank you for pointing out this omission. We have added that the $\\{H_i\\}$ are rank-1 in this example and additional explanation for why it is reasonable to consider this case.\n\nFor binary classification with $f$ denoting a scalar scoring function of weights and loss being an $\\mathbb{R} \\rightarrow \\mathbb{R}$ function of $f$, the Hessian is the sum of a rank-one term and a full second order term. However, in over-parameterized systems, each $f_i$ attains its least value and hence the full second order term is zero, and so $H_i$ being rank-one is a good assumption. (See eqn. (13) for additional details here: https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/readings/L02_Taylor_approximations.pdf)\n\nWith this in mind, we refer to Section 3.1 of our paper which explains the intuition of the coherence measure using two edge cases where the $H_i$ are rank-1. \n\n> The authors might make a better interpretation of Theorem 1. What does this result imply about the implicit regularization of SGD (beyond that of GD)? How stability is related to the hyperparameters, e.g. $\\eta$, $B$, and alignment of $H_i$? Some relevant discussion can be found in the experiment part Section 5.2, but I think it would be better to provide some intuition right after Theorem 1.\n\nWe agree it is best to immediately explain the key takeaways after the theorem statement.\n\nWe discuss how our analysis implies a \"squared scaling rule\" between $B$ and $\\eta$ after Lemma 4.1, but we now also mention this after Theorem 1. Another point to note regarding how SGD is regularized beyond GD is that GD does not depend on the coherence measure $\\sigma$. We give examples in the paragraph above Theorem 1 demonstrating the importance of the Hessian alignment (captured by $\\sigma$) in characterizing SGD stability. We now emphasize this more explicitly after the theorem statement.\n\n> In Section 3.2.1 the authors compare Theorem 1 to Theorem 3.3 in [Wu et al, 2022], and stated the advantages of their result. Among these stated advantages, 1) The first point makes sense to me. 2) In the second point, why do you say ``This definition is notably weaker than our notion of stability''? 3) By the third point, you seem to imply that the bound in Theorem 1 is sharper in the low-rank case. But what is the point in considering $\\sigma$ equal to one? To me, the third point is an unclear comparison between two results, which cannot prove the advantage of Theorem 1.\n\nRegarding (2), the notion of stability Wu et. al. uses (and restated in Appendix B of our paper) is that $\\mathbb{E}[L(w_t)] \\leq C E[L(w_0)]$ for some constant $C$ and for all $t \\geq 0$. This is weaker than the typical expected mean-squared sense of stability, since one can choose $L(\\cdot)$ such that this definition is satisfied but $\\||w_t - w^*\\||_2 \\rightarrow \\infty$.\n\nWe chose $\\sigma,\\mu = 1$ for simplicity, but the important takeaway from this point is that when these quantities are held constant and stable rank is bounded, then our condition becomes more general than that of [Wu et. al., 2022] as $n$ increases. While you are correct this does not prove a general advantage, it does prove an advantage in an asymptotic sense for these fixed parameter problems, which we would  argue is a theoretically interesting regime."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169275641,
                "cdate": 1700169275641,
                "tmdate": 1700169275641,
                "mdate": 1700169275641,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1cXEy0Y8BJ",
                "forum": "UMOlFJzLfL",
                "replyto": "DyojlzrUsO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7646/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Theorem 2, the optimality of Theorem 1, strongly relies on the condition that $\\sigma, B/n = O(1)$. There are two concerns: 1) In Theorem 2 the authors assume that $\\sigma \\in [n]$. Is there anything to guarantee $\\sigma \\leq n$? I don't think it is clear from the definition of $\\sigma$. Is $\\sigma$ inherently bounded? Moreover, do you mean $\\sigma \\leq n$? Hence, it is unclear what the assumption $\\sigma = O(1)$ means. 2) Also, I do not think it is natural to assume $B/n = O(1)$ as usually $B \\ll n$.\n\nTheorem 2 is provided to give an idea of how sharp our current analysis is and what approaches would be needed to improve it. To clarify, we do not assume that $\\sigma$ is a specific value, rather we state that there exists an input problem with Hessians $\\\\{H\\_i\\\\}\\_{i \\in [n]}$ such that $\\sigma$ equals this value and the linearized dynamics don't diverge. This means any improvement to Theorem 1 cannot imply that these problems diverge.\n\nThe discussion above Theorem 2 where we consider $\\sigma, n/B = O(1)$ is just pointing out that the conditions of Theorem 1 and Theorem 2 are close in this parameter range, and so we cannot expect much improvement in this regime. However, Theorem 2 does not make any assumption regarding this. From a theoretical perspective, we think it is natural to consider this regime to understand its behavior. However, we agree that the case where $B \\ll n$ is important to understand in future work, and so we have added a sentence pointing this out above Theorem 2.\n\n> In paragraph 1, when introducing the concept of \"implicit bias\", instead of citing (Li et al., 2022), I think it is more appropriate to cite the seminar works (Neyshabur et al., arXiv:1412.6614) and (Zhang et al., ICLR 2017).\n\nThank you, we have included these references in the updated manuscript.\n\n> In paragraph 2, when citing empirical works on relating sharpness to generalization, I think the important comprehensive investigations by (Jiang et al., ICLR2020) is missed.\n\nOf course, this is a seminal paper that is worth referring to. Thank you for pointing this out.\n\n> In paragraph 4, when stating \"GD ... empirically validated to predict the sharpness of overparameterized neural network\", the author cites (Cohen et al., 2021). However, this empirical predictability of linear stability analysis has been observed in (Wu et al., NeurIPS2018).\n\nWe apologize for missing this important reference in this place. We have referred to the same paper elsewhere in other contexts.\n\n> In paragraph 1, when stating the rationale for assuming over-parameterization, the authors cite the work (Allen-Zhu et al., 2019). This seems quite strange to me.\n\nWe are happy to add additional references, e.g. Zhang et al 2016.\n\n> In Definition 1, it is unclear whether the sampling is done with or without replacement.\n\nWe are using Bernoulli sampling which is not the same as sampling with or without replacement. The text under eqn. (1) in Definition 1 describes the probabilistic sampling model we work with. To clarify $\\mathcal{S}$ is a set (so there are no repetitions) and $i \\in \\mathcal{S}$ being independent from $j \\in \\mathcal{S}$ implies it cannot be sampling without replacement.\n\n> What is the $\\frac{1}{B}\\sum_{i=1}^n x_i H_i$ in the second paragraph. \n\nThanks for pointing this typo out. We have changed it to $\\frac{1}{B} \\sum_{i \\in \\mathcal{S}} H_i$. We reformulate this later in the proof as $\\frac{1}{B}\\sum_{i=1}^n x_i H_i$ where the $x_i$ are Bernoulli random variables, but you are correct we have not defined it at this point.\n\n> In Theorem 1, what does the subscript in $\\hat{J}_i$ stand for? \n\nWe have changed \"Let $\\hat{J}\\_i$ be independent Jacobians of SGD dynamics\" to \"Let $\\\\{\\hat{J}\\_i\\\\}\\_{i\\in\\mathbb{N}}$ be a sequence of i.i.d. copies of $\\hat{J}$ defined in Definition 1\" to make it clearer we consider an infinite sequence of i.i.d. samples of the stochastic Jacobian.\n\n> In Theorem 2, what do you mean $\\sigma \\in [n]$? Is $\\sigma$ a real value?\n\nFor an arbitrary set of Hessians $\\{H_i\\}_{i\\in [n]}$, the coherence measure $\\sigma$ can be a general real value. However, Theorem 2 is an existential result that says for any chosen $\\sigma \\in \\{1,2,...,n\\}$, there exists a set of Hessians $\\{H_i\\}$ satisfying the properties described in the theorem."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169302303,
                "cdate": 1700169302303,
                "tmdate": 1700175861680,
                "mdate": 1700175861680,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9HBusHX1cW",
            "forum": "UMOlFJzLfL",
            "replyto": "UMOlFJzLfL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7646/Reviewer_ELtf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7646/Reviewer_ELtf"
            ],
            "content": {
                "summary": {
                    "value": "The paper analyzes the linear stability of SGD of any additively decomposable loss function around the minimum $w^*$. The paper then derives a necessary and sufficient condition for the (in)stability, which relies on a novel *coherence measure* $\\sigma$, which is, in turn, intuitively connected to the alignment property of the collection of Hessians (of individual loss function). This is verified experimentally."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. Very well-written. I especially liked how the theoretical discussions are backed with intuitions and specific examples. I very much enjoyed reading this work.\n2. Although the mathematics used here are rather elementary, the theoretical discussions are complete; a necessary-sufficient condition for stability is provided, and the motivation and intuition behind the coherence measure $\\sigma$ are well-explained. (Elementaryness is a plus for me)\n3. Clear contribution and a good advancement in the linear stability analysis of SGD"
                },
                "weaknesses": {
                    "value": "1. The analysis relies on the Bernoulli sampling model, which, although is in expectation the same as with replacement sampling or uniformly sampling from all $B$-sets, still is a bit different as the size of $\\mathcal{S}$ itself now becomes random. Have the authors tried to consider multinomial distribution as the sampling distribution?\n2. Moreover, without replacement sampling where the event $i \\in \\mathcal{S}$ is dependent on $j \\in \\mathcal{S}$ (depending on the order), there should be some theoretical discussions on the effect of using these two (most-widely-used) random sampling schemes. (I appreciate that the paper has experiments on this, but then, theorem-wise, what would possibly change?)"
                },
                "questions": {
                    "value": "1. The analyses presented here are solely on the stability of the iterates, i.e., whether they diverge or not. Is there any chance that this gives some insight into whether they converge? Even further, depending on the alignment of the Hessians, can we say something about the local convergence rate?\n2. The relationship between batch size and learning rate that I'm more familiar with (e.g., starting from Goyal et al. (2017)) is the linear scaling rule, but here it is shown to be squared, which has also been reported in the past (e.g., Krizhevsky (2014)). Can the authors elaborate on why this stability analysis leads to squared? Then, at least locally, is squared scaling law the way to go, given that the Taylor expansion is accurate?\n3. In Figure 1, for small batch sizes, there is a large gap between Theorem 2 and the red boundary for $\\eta = 0.8$. Any particular reason for this?\n4. How would this be extended to momentum gradient descent or any of the adaptive gradient methods (e.g., Adam)? If the time allows, can the authors provide experiments for these as well in the same setting?\n5. In the prior works on linear stability, were there any quantities that resemble $\\sigma$ in their role? If there were, can the authors compare those to the proposed $\\sigma$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7646/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7646/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7646/Reviewer_ELtf"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698778734684,
            "cdate": 1698778734684,
            "tmdate": 1699636929456,
            "mdate": 1699636929456,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o4jYQKQFkl",
                "forum": "UMOlFJzLfL",
                "replyto": "9HBusHX1cW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7646/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments.\n\n> The analysis relies on the Bernoulli sampling model, which, although is in expectation the same as with replacement sampling or uniformly sampling from all $B$-sets, still is a bit different as the size of $\\mathcal{S}$ itself now becomes random. Have the authors tried to consider multinomial distribution as the sampling distribution?\n\n> Moreover, $i \\in \\mathcal{S}$ without replacement sampling where the event is dependent on $j \\in \\mathcal{S}$ (depending on the order), there should be some theoretical discussions on the effect of using these two (most-widely-used) random sampling schemes. (I appreciate that the paper has experiments on this, but then, theorem-wise, what would possibly change?)\n\nWe are optimistic that improved understanding of the Bernoulli sampling model can provide useful insight to other sampling models, as was the case for the previous line of work on matrix completion by Candes, Tao, and others which culminated in Benjamin Recht's seminal paper [Recht, 2011]. We also note that sampling with/without replacement are also just theoretical approximations of the typical true sampling behavior, where the data is shuffled and iterated over in an epoch. In reality, the sampling is often not even independent between iterations.\n\nLemma 4.1 could likely be rewritten using the sampling without replacement formulation for $\\hat{J}$ used in [Ma and Ying, 2021]. However, there doesn't seem to be an obvious way to simplify the expressions, and we would be stuck with a characterization that is no more interpretable than [Ma and Ying, 2021].\n\nAnother possibility would be to rewrite Lemma 4.1 for sampling with replacement. We expect that the coefficients depending on $\\eta$ and $B$ would change, but the proof would generally stay the same. That is, let $x_1,...,x_B$ each be uniformly sampled from $[n]$. Then, $\\mathbb{E}[H_{x_i}H_{x_j}] = \\frac{1}{n}\\sum_k {H_k^2}$ when $i = j$ and $\\mathbb{E}[H_{x_i}H_{x_j}] = \\frac{1}{n^2}H^2$ when $i \\neq j$. Therefore, the proof of Lemma 4.1 would start the same way when $\\hat{H}=\\sum_{i=1}^B H_{x_i}$. Results for the sampling with replacement case would immediately imply asymptotic results for sampling without replacement, since the total variation distance between the two distributions goes to zero as $n \\rightarrow \\infty$ and $B/n$ converges to a constant.\n\n\n> The analyses presented here are solely on the stability of the iterates, i.e., whether they diverge or not. Is there any chance that this gives some insight into whether they converge? Even further, depending on the alignment of the Hessians, can we say something about the local convergence rate?\n\nLemma 4.1 provides a sufficient condition to guarantee the linearized dynamics converge $w^*$. As we simplified part (i) of the Lemma using the Hessian alignment to prove Theorem 1, it is likely that similar manipulations could be used to provide sufficient conditions for convergence of the linear dynamics depending on the Hessian alignment.\n\nWe did not pursue this direction further since there seems to be less interest in this convergence behavior in prior work on linear stability of SGD. However, relating our analysis to local convergence rates of SGD is an interesting idea. For quadratic functions, this should follow directly. Extending this further would be a very interesting future direction, so we have added it to our conclusion as an open direction.\n\n> The relationship between batch size and learning rate that I'm more familiar with (e.g., starting from Goyal et al. (2017)) is the linear scaling rule, but here it is shown to be squared, which has also been reported in the past (e.g., Krizhevsky (2014)). Can the authors elaborate on why this stability analysis leads to squared? Then, at least locally, is squared scaling law the way to go, given that the Taylor expansion is accurate?\n\nThe derivation of $\\mathbb{E}[\\hat{J}^2]$ at the beginning of the proof for Lemma 4.1 may give an idea of how the squared scaling happens, particularly by considering the case where $H_i$ are $1 \\times 1$. This emphasizes a connection to the additivity versus scalar multiplication in the variance of a random variable, i.e., $\\operatorname{Var}(X+Y) = \\operatorname{Var}(X) + \\operatorname{Var}(Y)$ and $\\operatorname{Var}(cX) = c^2\\operatorname{Var}(X)$, where $X,Y$ are independent random variables and $c \\in \\mathbb{R}$ is a constant. We can interpret $\\eta$ as a scalar multiplier in a sum of random variables and $B$ as the number of random variables being summed. This seems to imply that when the Taylor series truncation and Bernoulli dynamics are good approximations, we expect squared scaling to hold."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699987419995,
                "cdate": 1699987419995,
                "tmdate": 1699988661249,
                "mdate": 1699988661249,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1uI3KqfoX5",
                "forum": "UMOlFJzLfL",
                "replyto": "9HBusHX1cW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7646/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> In Figure 1, for small batch sizes, there is a large gap between Theorem 2 and the red boundary for $\\eta = 0.8$. Any particular reason for this?\n\nWe conjecture that this may be due to the discrepancy between Bernoulli sampling used in the theorem and sampling without replacement used in the experiments. Specifically when $B$ is small, differences between the two sampling methods may have more of an effect due to the higher relative variance in the effective batch size. In the case where $\\eta=0.8$ is near the limit of GD instability, the stability condition seems more sensitive to this difference in effective batch size. \n\n> How would this be extended to momentum gradient descent or any of the adaptive gradient methods (e.g., Adam)? If the time allows, can the authors provide experiments for these as well in the same setting?\n\nUnfortunately, our experiments are coded in a way to run SGD on the described quadratic functions using minimal space/time, and as such, we cannot easily change the optimization method during the discussion period. However, we have added this idea as an open direction in our conclusion, as we agree it would be interesting to understanding how momentum affects the stability of SGD.\n\n> In the prior works on linear stability, were there any quantities that resemble \n$\\sigma$ in their role? If there were, can the authors compare those to the proposed $\\sigma$?\n\nThe alignment factor $\\mu$ of [Wu et. al., 2022] is a measure that is similar in spirit to our $\\sigma$ measure. We have described this measure in Appendix B of our paper (see eqn. (4)). \n\nWhile this measure is also used to quantify the stability of SGD, its mathematical formulation is significantly different from ours. It is specific to MSE loss, and it measures the alignment of the gradients for the regression output and the empirical covariance of the MSE loss gradients. Meanwhile, our measure is defined through the Hessians of an additively decomposable loss function, and it applies to this general class of functions. It is difficult to push the comparison much beyond this."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699987434020,
                "cdate": 1699987434020,
                "tmdate": 1699987434020,
                "mdate": 1699987434020,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "llatm3LyiX",
                "forum": "UMOlFJzLfL",
                "replyto": "1uI3KqfoX5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7646/Reviewer_ELtf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7646/Reviewer_ELtf"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for answering my concerns. I'm satisfied with the answers and inclined to keep my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727490490,
                "cdate": 1700727490490,
                "tmdate": 1700727490490,
                "mdate": 1700727490490,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RoL4dJv2IV",
            "forum": "UMOlFJzLfL",
            "replyto": "UMOlFJzLfL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7646/Reviewer_mpeb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7646/Reviewer_mpeb"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors considered quadratic approximations of the loss function around the zero-loss manifold that allows the definition of the 'linearized SGD dynamics' (given in Equation 1). This quadratic approximation allows the definition of the coherence measure (Definition 1), which is used in the statement of Theorem 1: SGD dynamics diverge when the coherence measure lower bounds the first eigenvalue of the Hessian of the loss function. In Theorem 2, they provide a partial converse to their proven divergence results in Theorem 1. The paper concludes with some experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- an important problem on the stability of SGD\n- having experiments"
                },
                "weaknesses": {
                    "value": "The introduction section (i.e., the first two pages) is poorly written. For example, the definition of 'linearized dynamics of SGD' is missing and is just referred to in Section 2. But this is probably one of the most necessary things one needs to know to follow the paper. Moreover, the section 'contributions' is also vague. Instead of long sentences, it's better to use a concise way to deliver the message. It's fairly impossible to identify the contributions of the paper based on that section (before reading the whole paper). \n\n\nSection 2: the approximation is called 'linearized dynamics,' but I think this is not the right word since you are essentially approximating the loss function with a quadratic function. Moreover, this dynamics only happens if you project to the zero-loss manifold at each step; otherwise, there is a term relating to the gradient. As a result, the setting is quite limited to only quadratic loss functions. \n\n\nThe word 'stability of dynamics' is used frequently in the paper while not being explained in the early pages."
                },
                "questions": {
                    "value": "- the font used in the paper looks weird; I think the authors have changed something in the latex code\n\n- In Theorem 1, $\\hat{J}_i$ is used, while it is never defined (the reference to Definition 1 only defines $\\hat{J}$).\n\n- After Theorem 1, why the expectation and the arg max are equal? Also, how does divergence allow you to conclude that for almost all initial parameters, SGD convergences? These claims are not clear and are vague.\n\n\n\n\n\n------------------------------------------------------------------------------------------------------------\n\n\nAfter the rebuttal: I appreciate the authors for their response. They partially answered some of my concerns but this paper is still not well-written, in my opinion. The authors only referred me to another reviewer for this part of my comments which I think is not an approrpiate response."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7646/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7646/Reviewer_mpeb",
                        "ICLR.cc/2024/Conference/Submission7646/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807547734,
            "cdate": 1698807547734,
            "tmdate": 1700790827368,
            "mdate": 1700790827368,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Iw5WyIrXmi",
                "forum": "UMOlFJzLfL",
                "replyto": "RoL4dJv2IV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7646/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments.\n\n> The introduction section (i.e., the first two pages) is poorly written. For example, the definition of 'linearized dynamics of SGD' is missing and is just referred to in Section 2. But this is probably one of the most necessary things one needs to know to follow the paper. \n\nWe believe it is more appropriate to reference Section 2 for the formal definition of the linearized dynamics, as we have done, rather than interrupt the introduction. Our paper advances a long line of work on characterizing the linear stability of SGD. As such, we believe readers who would be most interested in our paper likely have some familiarity with linear stability analysis. We believe our choice in this tradeoff is worthwhile, and Reviewer ELtf finds our paper to be \"very well-written\".\n\n> Moreover, the section 'contributions' is also vague. Instead of long sentences, it's better to use a concise way to deliver the message. It's fairly impossible to identify the contributions of the paper based on that section (before reading the whole paper).\n\nOur contributions give a high-level description of our contributions that separates our results from prior work in the area. We have made an effort to describe our contributions as concisely as possible while including all important information by leaving the technical details to later sections. We would be happy to make changes if you have any specific recommendations for how the contributions could be rephrased.\n\n> Section 2: the approximation is called `linearized dynamics,' but I think this is not the right word since you are essentially approximating the loss function with a quadratic function. \n\nThe term \"linearized dynamics\" is a standard term in the study of dynamical systems; we refer to the book ``Discrete Dynamical Systems'' by Oded Galor as an introductory text to this area. It is called this because the dynamics, i.e., $w_{t+1} = \\hat{J}w_t$, is a linear function from $\\mathbb{R}^n$ to $\\mathbb{R}^n$. The linearized dynamics comes by direct first order Taylor series of the dynamics around a stationary point; we mentioned the quadratic loss approximation route only because it could help understanding through the loss. This term has also been used by many previous works analyzing SGD including [Wu, 2022], [Jastrzebski, 2019], [Ma and Ying, 2021], etc.\n\n\n> Moreover, this dynamics only happens if you project to the zero-loss manifold at each step; otherwise, there is a term relating to the gradient. As a result, the setting is quite limited to only quadratic loss functions.\n\nThis is not correct. The gradient is represented by the linear approximation $g\\approx Hw$. This is a fundamental technique used in the analysis of dynamical systems. We have also cited numerous other papers in the intro that uses this approximation specifically in the context of analyzing general SGD dynamics, e.g., [Wu, 2022], [Jastrzebski, 2019], [Ma and Ying, 2021], etc.\n\n> The word 'stability of dynamics' is used frequently in the paper while not being explained in the early pages.\n\nIn the paragraph before our contributions section, we have added the sentence: \"We consider mean-squared stability, that is, $w^*$ is considered unstable if iterates of SGD diverge from $w^*$ under the $\\ell_2$-norm in expectation\".\n\n> the font used in the paper looks weird; I think the authors have changed something in the latex code\n\nWe have double-checked our paper to ensure it follows all ICLR formatting guidelines and made any necessary changes.\n\n> In Theorem 1, $\\hat{J}_i$ is used, while it is never defined (the reference to Definition 1 only defines $\\hat{J}$).\n\nWe have changed \"Let $\\hat{J}_i$ be independent Jacobians of SGD dynamics\" to \"Let $\\\\{\\hat{J}\\_i\\\\}\\_{i\\in\\mathbb{N}} $ be a sequence of i.i.d. copies of $\\hat{J}$ defined in Definition 1\" to make it clearer we consider an infinite sequence of i.i.d. samples of the stochastic Jacobian.\n\n> After Theorem 1, why the expectation and the arg max are equal?\n\nWe have changed the \"argmax\" to \"max\". For any matrix $M \\in \\mathbb{R}^{n \\times n}$, $\\||M\\||_2 = \\max_w \\||Mw\\||_2$ such that $\\||w\\||_2 = 1$ by definition of the $\\ell_2$-induced vector norm. Hence, our statement immediately follows from our definition of the linearized dynamics, i.e., $w_t = \\hat{J}_t....\\hat{J}_1w_0$. \n\n> Also, how does divergence allow you to conclude that for almost all initial parameters, SGD convergences? These claims are not clear and are vague.\n\nIf you are referring to our statement that $\\||w_t\\||_2 \\rightarrow \\infty$ for almost all starting $w_0$ points under the conditions of Theorem 1, this follows due to the fact that the set of vectors orthogonal to the maximum singular vector of matrix has measure zero."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699986625325,
                "cdate": 1699986625325,
                "tmdate": 1699989835657,
                "mdate": 1699989835657,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Cmetfbx872",
            "forum": "UMOlFJzLfL",
            "replyto": "UMOlFJzLfL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7646/Reviewer_asFh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7646/Reviewer_asFh"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes the linear stability of SGD through the lens of the loss surface geometry. Assuming a scenario where the model perfectly fits the data, the necessary and sufficient conditions for linear stability is characterized by learning rates, batch sizes, sharpness (the maximum eigenvalue of the Hessian), and a coherence measure of the individual loss Hessians, which is newly proposed in this work. The theoretical findings are validated through experiments on a synthetic optimization problem."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper relaxes certain assumptions in characterizing the stability of SGD, e.g., the restriction to MSE loss, from existing works.\n- The mathematical derivations appear to be sound and accurate."
                },
                "weaknesses": {
                    "value": "- The paper lacks intuitive and qualitative explanations of the characterized linear stability, which could enhance its accessibility.\n- The experiments are limited to engineered quadratic losses without considering actual neural networks or real-world data.\n- The paper should address scenarios where the condition $\\nabla_w l_i (w) = 0$ is violated. There may be various cases that $\\| \\nabla_w l_i \\|$ is small but non-zero, e.g., cross-entropy loss without label smoothing, early stopping, and so on. How can the proposed analysis accommodate these cases? \n- There are existing works to characterize the stability of SGD considering the noise covariance matrix $\\Sigma = \\frac{1}{n}\\sum_i \\nabla l_i(w) \\nabla l_i(w)^T - \\nabla L(w) \\nabla L(w)^T$, without assuming $\\nabla_w l_i (w) = 0$. The paper should clarify how its results relate to these existing works."
                },
                "questions": {
                    "value": "- What is the exact notion of stability considered in the paper? It is not clearly explained in the manuscript.\n- Is the coherence measure easily computable for typical neural networks? How complex would it be to compute in practice?\n- On page 4, the term $x_i$ is used but not defined. \n- It seems that the second last paragraph on page 4 assumes that $H_i$ is a matrix of rank one. Is this the case?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7646/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7646/Reviewer_asFh",
                        "ICLR.cc/2024/Conference/Submission7646/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699244817632,
            "cdate": 1699244817632,
            "tmdate": 1700723957184,
            "mdate": 1700723957184,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5V7yox6pty",
                "forum": "UMOlFJzLfL",
                "replyto": "Cmetfbx872",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7646/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments.\n\n> The paper lacks intuitive and qualitative explanations of the characterized linear stability, which could enhance its accessibility.\n\nWe have made an effort to explain our intuitive understanding of our results within the page limits, and we note that Reviewer ELtf states: \"Very well-written. I especially liked how the theoretical discussions are backed with intuitions and specific examples.\". However, we understand that what is intuitive to us may not be to everyone. Hence, we could appreciate any more details on what specifically could be improved to make it accessible to a wider audience.\n\n> The experiments are limited to engineered quadratic losses without considering actual neural networks or real-world data.\n\n> Is the coherence measure easily computable for typical neural networks? How complex would it be to compute in practice?\n\nWe note that our main contributions are theoretical and consider the general problem of minimizing additively decomposable loss functions. By restricting ourselves to these quadratic losses, we are able to avoid introducing unknown structure into the problem. This allows us to validate our general theory and get an idea of its tightness in the worst case.\n\nComputing the coherence measure naively would take about $O(n^2d^3)$ time, which is prohibitive for realistic neural networks. However, it likely can be approximated more efficiently using iterative methods, for example, the main bottleneck is computing $\\||H_i^{1/2}H_j^{1/2}\\||_F$ for all $i,j \\in [n]$. However, we can estimate the Frobenius norm of a matrix product through matrix-vector products much more efficiently than it can be computed exactly.\n\nWe do agree that extending this understanding to neural networks and real-world data is a critical next step, as we noted in our conclusion. However, we believe that handling these numerical challenges is best left as future work since our current paper has a different focus.\n\n> The paper should address scenarios where the condition $\\nabla_w l_i (w) = 0$ is violated. There may be various cases that $| \\nabla_w l_i |$ is small but non-zero, e.g., cross-entropy loss without label smoothing, early stopping, and so on. How can the proposed analysis accommodate these cases?\n\nWe agree that the understanding the behavior of the case where the sample-wise gradients do not completely vanish is an important problem. At the same time, we would like to note that our assumption that $\\nabla \\ell_i(w^*) = 0$ for all $i \\in [n]$ is a generally accepted assumption in this line of work (see e.g., [Ziyin at. al. 2023] and references within); and this is an important and practical regime to study.\n\n> There are existing works to characterize the stability of SGD considering the noise covariance matrix $\\Sigma = \\frac{1}{n}\\sum_i \\nabla l_i(w) \\nabla l_i(w)^T - \\nabla L(w) \\nabla L(w)^T$, without assuming $\\nabla_w l_i (w) = 0$. The paper should clarify how its results relate to these existing works.\n\nThe work most relevant to ours that takes the approach of looking at the noise covariance matrix is [Wu et. al., 2022]. We have restated their main results in Appendix B and compare our results to theirs extensively in the related work section and in Section 3.2.1.\n\nSome important observations we repeat here are that [Wu et. al., 2022] is restricted to MSE loss, and for the conditions of their main theorem to hold, the optimal loss must be non-zero. This makes further comparison somewhat difficult, since their result does not apply to perfectly fit data as we consider and as is more common to consider in overparameterized networks.\n\n> What is the exact notion of stability considered in the paper? It is not clearly explained in the manuscript.\n\nIn the paragraph before our contributions section, we have added the sentence: ``We consider mean-squared stability, that is, $w^*$ is considered unstable if iterates of SGD diverge from $w^*$ under the $\\ell_2$-norm in expectation\".\n\n> On page 4, the term $x_i$ is used but not defined.\n\nThanks for pointing this out. We have fixed the expression.\n\n> It seems that the second last paragraph on page 4 assumes that is a matrix of rank one. Is this the case?\n\nThank you for pointing out this omission. We have added this assumption in the example along with the following explanation for why it is reasonable.\n\nFor binary classification with $f$ denoting a scalar scoring function of weights and loss being an $\\mathbb{R} \\rightarrow \\mathbb{R}$ function of $f$, the Hessian is the sum of a rank-one term and a full second order term. However, in over-parameterized systems, each $f_i$ attains its least value and hence the full second order term is zero, and so $H_i$ being rank-one is a good assumption. (See eqn. (13) for additional details here: https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/readings/L02_Taylor_approximations.pdf)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700168239159,
                "cdate": 1700168239159,
                "tmdate": 1700175915778,
                "mdate": 1700175915778,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZEO0gxnjOm",
                "forum": "UMOlFJzLfL",
                "replyto": "5V7yox6pty",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7646/Reviewer_asFh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7646/Reviewer_asFh"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed answers to the questions. I appreciate the theoretical contributions of the paper.\n\nAfter reading the authors' responses, I have additional comments:\n- Even though the theorems concisely summarize the paper's main results, it would be better if some explanations existed of the intuitive mechanism of how incoherent Hessians (or small $\\sigma$) can lead to diverging behavior. For example, can the overshoot of the second setting explained in Section 3.1 somehow lead to the diverging behavior?\n- In [Wu et al., 2022]'s setting, even if $\\nabla L(w)=0$ and $\\nabla l_i(w)=0$, in their Lemma 2.2, the term $\\frac{Tr(\\Sigma(w)G(w))}{2L(w)\\|G(w)\\|_{F}^2}$ is lower-bounded by a term which can be positive. Hence, their analysis may also be valid when $\\nabla l_i(w)=0$ case. \n- In the additional sentence of the second last paragraph of Section 3.1 in the revised paper, 'binary cross-entropy' may be more appropriate than 'cross-entropy.'"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664990802,
                "cdate": 1700664990802,
                "tmdate": 1700664990802,
                "mdate": 1700664990802,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e3waqRUXg7",
                "forum": "UMOlFJzLfL",
                "replyto": "A6mMsvhLHZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7646/Reviewer_asFh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7646/Reviewer_asFh"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the additional clarifications.\n\nI still struggle to intuitively understand how the SGD becomes unstable in the incoherent Hessian setting.\nCan you elaborate on why the effective increase in stepsize along the sampled directions can lead to instability?\nIn the second setting of Section 3.1, over multiple updates, the effective stepsize of the SGD along each sample direction will be the same as that of GD on average. Is the consecutive overshooting along different sample directions occurring in each update the problem?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696786350,
                "cdate": 1700696786350,
                "tmdate": 1700696786350,
                "mdate": 1700696786350,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1RYImiBUEo",
                "forum": "UMOlFJzLfL",
                "replyto": "wCWcdg54h8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7646/Reviewer_asFh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7646/Reviewer_asFh"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification. I could understand the idea further. I will increase my rating."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723935548,
                "cdate": 1700723935548,
                "tmdate": 1700723935548,
                "mdate": 1700723935548,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]