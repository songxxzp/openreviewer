[
    {
        "title": "FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained Diffusion Models and Monocular Depth Estimators"
    },
    {
        "review": {
            "id": "9CB4GdTEoP",
            "forum": "BPb5AhT2Vf",
            "replyto": "BPb5AhT2Vf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2224/Reviewer_fhKS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2224/Reviewer_fhKS"
            ],
            "content": {
                "summary": {
                    "value": "Overall, the core idea of the paper is interesting, which considers leveraging diffusion networks to achieve feature enhancing in the task of image-point-cloud registration."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, the core idea of the paper is interesting, which considers leveraging diffusion networks to achieve feature enhancing in the task of image-point-cloud registration."
                },
                "weaknesses": {
                    "value": "1. The idea of using diffusion networks is interesting, but the way to use diffusion networks is to some extent trivial. It is not well-motivated  why you choose to use diffusion networks instead of any other pretrained feature extractor, such as those vision foundation backbones (DINO, SAM, CLIP, ...)?\n\n2. In Related Work (Image-to-point cloud registration), \"In contrast, FreeReg does not require task-specific\ntraining and finetuning and exhibits strong generalization ability to both indoor and outdoor scenes\". It seems this description is not solid, as you feature extractors (diffusion networks and depth estimation networks) are already trained on some datasets.\n\n3. The baseline comparison is a little bit weak. More recent and related works should be considered. And in Table S1, the proposed approach seems not solid outperform the counterpart 2D3D-Matr.\n[1] Corri2p: Deep image-to-point cloud registration via dense correspondence. TCSVT 2022.\n[2] EP2P-Loc: End-to-End 3D Point to 2D Pixel Localization for Large-Scale Visual Localization. ICCV 2023.\n[3] CFI2P: Coarse-to-Fine Cross-Modal Correspondence Learning for Image-to-Point Cloud Registration. arXiv 2023.\n[4] CoFiI2P: Image-to-Point Cloud Registration withCoarse-to-Fine Correspondences for Intelligent Driving. arXiv 2023.\n[5] End-to-end 2D-3D Registration between Image and LiDAR Point Cloud for Vehicle Localization. arXiv 2023.\n\n\n4. In Table 4, what is the performance under w=0 and w=1.0 ?\n\n5. The run time speed/memory comparison with other models is missing."
                },
                "questions": {
                    "value": "Please refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2224/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2224/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2224/Reviewer_fhKS"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2224/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698645189198,
            "cdate": 1698645189198,
            "tmdate": 1700379061178,
            "mdate": 1700379061178,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZiKyNLLlhq",
                "forum": "BPb5AhT2Vf",
                "replyto": "9CB4GdTEoP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2224/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2224/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer fhKS (part 1/2)"
                    },
                    "comment": {
                        "value": "**We appreciate the reviewer's insightful comments and address all concerns below.**\n\n**Q1:** The motivation to use diffusion networks rather than other vision foundation backbones (VFB).\n\n**A1:** Existing VFBs are designed to process 2D RGB images and are not directly applicable to 3D point cloud data. Treating depth maps as RGB images and feeding them into these VFBs [1,2,3] for feature extraction and feature matching leads to poor performances as shown in Table 1 below. We have added this experiment in Sec.A.4.1 of the revised supplementary material.\n\n**Table 1: Results on 3DMatch utilizing features from other VFBs**\n| Method           | FMR (%) | IR (%) | IN (#) | RR (%) |\n|------------------|:-------:|:------:|:------:|:------:|\n| DINO-v2[1]       |  64.3   |  13.0  |  8.8   |  9.8   |\n| CLIP[2]          |  21.9   |   3.3  |   2.4  |    /   |\n| DINO-SLayer[3]   |  35.5   |  8.9   |  12.6  |  14.8  |\n| ATTF[3]          |  41.7   |  10.9  |  15.5  |  17.4  |\n| FreeReg          |  94.6   |  47.0  |  82.8  |  63.8  |\n\n[1] Oquab M, Darcet T, Moutakanni T, et al. Dinov2: Learning robust visual features without supervision[J]. arXiv preprint arXiv:2304.07193, 2023.\n\n[2] Radford A, Kim J W, Hallacy C, et al. Learning transferable visual models from natural language supervision[C]//International conference on machine learning. PMLR, 2021: 8748-8763.\n\n[3] Zhang J, Herrmann C, Hur J, et al. A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence[J]. arXiv preprint arXiv:2305.15347, 2023.\n\n\n**Q2:** ''FreeReg does not require task-specific training and finetuning and exhibits strong generalization ability to both indoor and outdoor scenes'' is not solid, as the feature extractors are already trained on some datasets.\n\n**A2:** We use \"task-specific\" to denote the training on the image-to-point cloud registration task. We agree that our pre-trained models are trained on other tasks. We make this clearer in the revised version by using \"FreeReg does not need any training or fine-tuning on I2P registration task.\"\n\n**Q3.1:** FreeReg seems not solid outperform the counterpart 2D3D-Matr.\n\n**A3.1:** FreeReg already achieves 0.9% higher RR than 2D3D-Matr using only pre-trained models. \nNote 2D3D-Matr is fully supervised on the image-to-point cloud registration task and is a concurrent work to our submission.\nIn Sec.A.5.1 of the revised supplementary material, we show that we fine-tune ControlNet on the image generation task (rather than the I2P registration task) for 1 hour and the resulted model outperforms 2D3D-Matr more significantly as shown in Table 2 below.\n \n**Table 2: Comparison of 2D3D-Matr and FreeReg(-Ft).**\n| Method     | FMR (%) | IR (%) | RR (%) |\n|------------|---------|--------|--------|\n| 2D3D-Matr  |   90.8  |  32.4  |  56.4  |\n| FreeReg    |   82.0  |  30.9  |  57.3  |\n| FreeReg-Ft |   86.9  |  38.3  |  68.6  |\n\n\n**Q3.2:** Comparision with additional baselines including CorrI2P, EP2P-Loc, C2FI2P,  E2E-I2P, and CoFiI2P.\n\n**A3.2:** Many thanks for your recommendation. EP2P-Loc (ICCV 2023), C2FI2P (arXiv), E2E-I2P (arXiv), and CoFiI2P (arXiv) are concurrent works to FreeReg and have not been open-sourced. We have added discussions of these concurrent works in Sec.2 of the revised main paper.\n\nCorrI2P [4] is the only open-sourced work and we tried to compare FreeReg with it. The official code of CorrI2P is released but no pretrained model is available. We thus train it on the provided training set. The trained model achieved an average rotation error of 7 degrees on their provided test dataset. However, the model almost fails on our KITTI-DC test data because CorrI2P relies on the intensity information of the point cloud as input, which is not available in our test data. We add such a discussion in Sec.A.2 of the revised supplementary material. \n\n[4] Ren S, Zeng Y, Hou J, et al. Corri2p: Deep image-to-point cloud registration via dense correspondence[J]. IEEE Transactions on Circuits and Systems for Video Technology, 2022, 33(3): 1198-1208.\n\n\n**Q4:** FreeReg performances under $w$=0 and $w$=1 in Table 4.\n\n**A4:** In our submission, FreeReg-D or FreeReg-G means $w=1$ or $w=0$ for feature matching respectively, for which we have reported their performances using PnP or Kabsch in Table 1.\nThe performances with $w=1$ and $w=0$ using Kabsch are reported in Table 3 below and have been added to Table 4 of the revised main paper. \n\n**Table 3: FreeReg performances with w = 0 or 1.**\n\n|        $w$           |       FMR (%)       | IR (%) | IN (#) | RR (%) |\n|:-----------------:|:-------------------:|:------:|:------:|:------:|\n| 1 |        91.9         |  39.6  |  60.8  |  52.6  |\n| 0 |        90.7         |  31.4  |  49.4  |  50.4  |\n| 0.5 (Ours) |        94.6         |  47.0  |  82.8  |  63.8  |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362574088,
                "cdate": 1700362574088,
                "tmdate": 1700362574088,
                "mdate": 1700362574088,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cnRbWpqRwB",
                "forum": "BPb5AhT2Vf",
                "replyto": "ZiKyNLLlhq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2224/Reviewer_fhKS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2224/Reviewer_fhKS"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Many thanks for the authors' responses ($\\bf{A2}$ - $\\bf{A5}$), which have answered my questions.\n\nFor $\\bf{A1}$, I am still conerned about the motivation. A1 is just an illustration of the results, instead of a motivation. What we readers really want is the deeper root reason why the diffusion net is working better than others. For example, is it because different VFBs use different pre-training proxy task and a suitable proxy task is more benificial to the down-stream registration task?\n\nNote that this kind of motivation is exactly the core of this whole paper, which determines whether this paper is a $\\bf{great}$ \"motivating and insightful\" paper or just a $\\bf{normal}$ \"trick and try\" paper. I hope authors could bring more thinkings.\n\nAlso, for \"Existing VFBs are designed to process 2D RGB images and are not directly applicable to 3D point cloud data\", this is not a solid answer, as I can \\\n(1) use powerful 3D backbones, or \\\n(2) project 3D to 2D view to use 2D backbones (the same as this work)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700371803178,
                "cdate": 1700371803178,
                "tmdate": 1700371803178,
                "mdate": 1700371803178,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QlVMMaPuty",
                "forum": "BPb5AhT2Vf",
                "replyto": "sDpNnqn0uV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2224/Reviewer_fhKS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2224/Reviewer_fhKS"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thanks for the timely replies by the authors.\n\nEven though the answer is still a shallow reason, this work overall is still an interesting try and has its value in cross-modal related-tasks. \n\nTherefore, I would like to raise my Rating."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379036638,
                "cdate": 1700379036638,
                "tmdate": 1700379036638,
                "mdate": 1700379036638,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7Xqkgh6HIC",
            "forum": "BPb5AhT2Vf",
            "replyto": "BPb5AhT2Vf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2224/Reviewer_zNJ2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2224/Reviewer_zNJ2"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a image-to-point cloud registration framework. The key idea is to generate RGB image from point cloud and reconstruct depth image from RGB so that correspondences can be established between images of the same modality. Though the image generation of both directions are well studied, a naive implementation does not work well. For this reason, the authors first generate depth image from point cloud and then use intermediate feature maps in the depth-to-image ControlNet to establish semantic correspondence with the original image. At the same time, a depth map is generated from the original image and local geometric features extracted from the depth map are combined with the semantic features for better correspondences. Experiments are conducted on three datasets, including both indoor and outdoor scenes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.The idea of first generating images and point clouds from the other modality and then find correspondence in the same modality is interesting and the authors find practical ways to implement this idea.\n\n2.The performance is promising even without training on the target task with ground-truth correspondence.\n\n3. The paper is well written and the adequate ablation studies are conducted."
                },
                "weaknesses": {
                    "value": "1. As mentioned by the author, inference speed is a limit of the proposed method and 11s per image is quite slow.  I hope that the author can provide their thoughts for further improvement of speed.\n\n2. Another limitation is that the performance is only comparable with the concurrent work 2D3D-MATR on the dataset RGBD-Scene-v2, while  I think that this is not a big problem and the proposed method has its own value. \n\n3. The residual rotation error seems quite large. I'd like to know what's the initial rotation error before registration?"
                },
                "questions": {
                    "value": "1. Is there any way that can are readily to be tried for speed improvement?\n\n2. The residual rotation error is quite high. From a practical point of view, is the rotation error of 10 or 20 degrees a good threshold for recall?  What's the requirements of rotation accuracy in different application areas?\n\n3. The authors \"analyze\" the limitation of straightforward/direct implementations in the 4th and 5th paragraph of Introduction, but did not provide any experimental results to support supporting the conclusion.  For the task of this paper and the fusion of two kinds of features, these straightforward may also work well."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2224/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719750587,
            "cdate": 1698719750587,
            "tmdate": 1699636155663,
            "mdate": 1699636155663,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "43w4c21ULL",
                "forum": "BPb5AhT2Vf",
                "replyto": "7Xqkgh6HIC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2224/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2224/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer zNJ2"
                    },
                    "comment": {
                        "value": "**We appreciate the reviewer's insightful comments and our responses are listed below.**\n\n**Q1:** Only comparable performances with the concurrent work 2D3D-Matr.\n\n**A1:** In Sec. A.5.1 of the revised supplementary material, we fine-tune ControlNet on the image generation task (rather than the difficult I2P registration task) for only 1 hour and the resulted model outperforms 2D3D-Matr more significantly as shown in Table 1 below.\n\n**Table 1: Comparison of 2D3D-Matr and FreeReg(-Ft).**\n| Method     | FMR (%) | IR (%) | RR (%) |\n|------------|---------|--------|--------|\n| 2D3D-Matr  |   90.8  |  32.4  |  56.4  |\n| FreeReg    |   82.0  |  30.9  |  57.3  |\n| FreeReg-Ft |   86.9  |  38.3  |  68.6  |\n\n\n**Q2:** How to accelerate FreeReg?\n\n**A2:** Many thanks for your inspiring comments. There are two possible ways to accelerate FreeReg.\n\n1. **DDIM sampling speed-up.** The most time-consuming part of FreeReg registration is the forward sampling process of ControlNet and we can enlarge the sampling step interval for fewer sampling iterations for acceleration. As shown in Table 2 below, with only 5 sampling iterations, FreeReg achieves a registration time reduction of $\\sim 50\\%$, with only a $1.4\\%$ decrease in RR. This might be further optimized with the emergence of diffusion sampling speed-up strategies[2].\n\n    **Table 2: FreeReg acceleration with fewer DDIM [1] sampling iterations.**\n    | DDIM-Iters (#) | FMR (%) | IR (%) | IN (#) | RR (%) | Time (s) |\n    |----------------|:-------:|:------:|:------:|:------:|:--------:|\n    | 5              |  93.6   |  47.8  |  72.1  |  62.4  |   4.7    |\n    | 10             |  95.0   |  46.6  |  83.0  |  63.5  |   6.4    |\n    | 15             |  94.5   |  47.1  |  82.4  |  63.0  |   8.1    |\n    | 20 (FreeReg)   |  94.6   |  47.0  |  82.8  |  63.8  |   9.3    |\n\n2. **Knowledge distillation.** We can use FreeReg as a teacher network to guide the feature estimation of a lightweight student image/point cloud feature extractor. An analysis has been added in Sec.A.5.2 of the revised supplementary material.\n\n[1] Song J, Meng C, Ermon S. Denoising diffusion implicit models[J]. arXiv preprint arXiv:2010.02502, 2020.\n\n[2] Luo S, Tan Y, Huang L, et al. Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference[J]. arXiv preprint arXiv:2310.04378, 2023.\n\n**Q3.1:** What's the initial rotation error before registration?\n\n**A3.1:** The average initial rotation error between unposed images and posed point clouds is 115.4 degrees and 68.3 degrees on 3Dmatch and KITTI-DC, respectively. \n\n**Q3.2:** Is the rotation error of 10 or 20 degrees a good threshold for recall?\n\n**A3.2:** We agree that 10/20 degrees is a large rotation threshold and we select it based on the following considerations:\n1. **Why 10/20 degrees:** The rotation threshold is typically set to 15/5 degree for the indoor/outdoor point cloud registration task[3]. As a cross-modality task,  Image-to-point cloud registration is more difficult. Thus, we use a looser threshold at 20 degrees for indoor I2P registration and a 10-degree threshold for outdoor registration, which is the same as DeepI2P.\n2. **Performance under different thresholds:** We also report the performance of FreeReg under different thresholds in Fig. 1 and Table 5 in the revised supplementary material. On more than 50% data, our method achieves a rotation error less than 10/5 degrees for indoor/outdoor data.\n\n[3] Choy C, Park J, Koltun V. Fully convolutional geometric features[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2019: 8958-8966.\n\n**Q3.3:** Is a rotation error of 10 or 20 degrees useful in practice?\n\n**A3.3:** Yes. 10/20 degree rotation accuracy can support applications like positioning and navigation in a mall, which do not require very high accuracy. We believe improving FreeReg's accuracy will enlarge the application scope of I2P registration.\n\n**Q4:** Experimental results of straightforward/direct implementations in the 4th and 5th paragraph of Introduction.\n\n**A4:** In the fourth paragraph, we analyzed that naively using ControlNet to directly transform input point clouds into the image modality failed to reliably match with input images. This corresponds to ControlNet+SuperGlue(CN+SG) in Sec.4. In the fifth paragraph, we discussed using Zoe-Depth to transform input images into the point cloud and matching between the point clouds. This corresponds to Zoe-Depth+FCGF, i.e., FreeReg-G in Sec.4. The experimental results indicate that both baselines perform worse than FreeReg. We have improved the introduction about these two baselines and recall the above discussions in Section 4.1 to make it clearer."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363025256,
                "cdate": 1700363025256,
                "tmdate": 1700363025256,
                "mdate": 1700363025256,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Yhcm6gx2Ww",
                "forum": "BPb5AhT2Vf",
                "replyto": "43w4c21ULL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2224/Reviewer_zNJ2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2224/Reviewer_zNJ2"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's response and new results. My questions have been addressed and I keep my rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491257791,
                "cdate": 1700491257791,
                "tmdate": 1700491257791,
                "mdate": 1700491257791,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YF0uAyRIcr",
            "forum": "BPb5AhT2Vf",
            "replyto": "BPb5AhT2Vf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2224/Reviewer_BaLi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2224/Reviewer_BaLi"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the image-to-point-cloud registration problem. The idea is to utilize a diffusion model and ControlNet to generate diffusion features from input point cloud. For images and point clouds, the final features used for matching are composed of both a diffusion part and a geometric part in a weighted average fashion. The latter is extracted from FCGF to serve as geometric features. The pixel-to-point correspondences are then obtained by a NN matching with mutual check. Experiments show the empirical improvements in I2P matching in several benchmark datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- High-quality writing: The content is well-written, with a focus on clarity, coherence, and precision.\n\n- Improved results over baselines: The results outperform standard models, demonstrating significant enhancements in performance and accuracy.\n\n- Efficient feature distillation and cross-modality matching: Large model features are distilled effectively, facilitating feature matching across different modes for improved system performance."
                },
                "weaknesses": {
                    "value": "- The paper primarily focuses on the design of an improved feature that serves as a unifying element for both the image and depth map domains. While this feature has shown remarkable efficacy within this specific context, it may not be directly transferable to other cross-modality problems. Adapting it to different cross-modality tasks would necessitate careful and tailored design to ensure its successful application.\n\n- Efficiency is indeed a concern as pointed out in the limitation section, as feature extraction via stable diffusion and ControlNet is a costly computation."
                },
                "questions": {
                    "value": "1. Can the method work with only diffusion features from RGB and point clouds? e.g. Can the weighting between F_d and F_g be either 1 or 0?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2224/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827737483,
            "cdate": 1698827737483,
            "tmdate": 1699636155576,
            "mdate": 1699636155576,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wT6a6DyCJF",
                "forum": "BPb5AhT2Vf",
                "replyto": "YF0uAyRIcr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2224/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2224/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer BaLi"
                    },
                    "comment": {
                        "value": "**We appreciate the reviewer's encouraging comments and make responses below.** \n\n**Q1:** Use FreeReg features in other cross-modality problems.\n\n**A1:** Many thanks for your insightful comments. In Sec.A.5.3 of the revised supplementary material, we show that the FreeReg features have strong semantic consistency, which suggests the potential ability of these features for other semantic tasks.\n\n**Q2:** How to accelerate FreeReg? \n\n**A2:** There are two possible ways to accelerate FreeReg.\n\n1. **DDIM sampling speed-up.** The most time-consuming part of FreeReg registration is the forward sampling process of ControlNet and we can enlarge the sampling step interval for fewer sampling iterations for acceleration. As shown in Table 1 below, with only 5 sampling iterations, FreeReg achieves a registration time reduction of $\\sim 50\\%$, with only a $1.4\\%$ decrease in RR. This might be further optimized with the emergence of diffusion sampling speed-up strategies[2].\n\n    **Table 1: FreeReg acceleration with fewer DDIM [1] sampling iterations.**\n    | DDIM-Iters (#) | FMR (%) | IR (%) | IN (#) | RR (%) | Time (s) |\n    |----------------|:-------:|:------:|:------:|:------:|:--------:|\n    | 5              |  93.6   |  47.8  |  72.1  |  62.4  |   4.7    |\n    | 10             |  95.0   |  46.6  |  83.0  |  63.5  |   6.4    |\n    | 15             |  94.5   |  47.1  |  82.4  |  63.0  |   8.1    |\n    | 20 (FreeReg)   |  94.6   |  47.0  |  82.8  |  63.8  |   9.3    |\n\n2. **Knowledge distillation.** We can use FreeReg as a teacher network to guide the feature estimation of a lightweight student image/point cloud feature extractor. An analysis have been added in Sec.A.5.2 of the revised supplementary material.\n\n[1] Song J, Meng C, Ermon S. Denoising diffusion implicit models[J]. arXiv preprint arXiv:2010.02502, 2020.\n\n[2] Luo S, Tan Y, Huang L, et al. Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference[J]. arXiv preprint arXiv:2310.04378, 2023.\n\n**Q3:** Can FreeReg work with only diffusion or geometric features by setting $w$ to 1 or 0.\n\n**A3:** \nIn our submission, FreeReg-D or FreeReg-G means $w=1$ or $w=0$ for feature matching respectively, for which we have reported their performances using PnP or Kabsch in Table 1.\nThe performances with $w=1$ and $w=0$ using Kabsch are reported in Table 2 below and have been added to Table 4 of the revised main paper. \n\n**Table 2: FreeReg performances with w = 0 or 1.**\n\n|        $w$           |       FMR (%)       | IR (%) | IN (#) | RR (%) |\n|:-----------------:|:-------------------:|:------:|:------:|:------:|\n| 1 |        91.9         |  39.6  |  60.8  |  52.6  |\n| 0 |        90.7         |  31.4  |  49.4  |  50.4  |\n| 0.5 (Ours) |        94.6         |  47.0  |  82.8  |  63.8  |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363176917,
                "cdate": 1700363176917,
                "tmdate": 1700363176917,
                "mdate": 1700363176917,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]