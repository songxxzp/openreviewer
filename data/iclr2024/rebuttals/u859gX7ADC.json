[
    {
        "title": "Augmenting transformers with recursively composed multi-grained representations"
    },
    {
        "review": {
            "id": "HWeb8Hmxn6",
            "forum": "u859gX7ADC",
            "replyto": "u859gX7ADC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission692/Reviewer_m86F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission692/Reviewer_m86F"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Recursive Composition Augmented Transformer (ReCAT), a model that aims to explicitly incorporate syntactic and structured trees into Transformers. The main change of ReCAT is the proposed contextual inside-outside (CIO) layers, which are inserted into a standard Transformer encoder right after the embedding layer. The CIO layer consists of the bottom-up and top-down modules. By stacking CIO layers, the iterative bottom-up and top-down layer-by-layer passes can unsupervisedly construct syntactic structures for the input sequence and produce contextualized representations for tree nodes.\n\nBy pre-trained with masked language modeling (MLM) and then fine-tuned, ReCAT was compared with Transformer-only methods and Fast-R2D2 and showed superior performance on multiple sentence-level and span-level tasks. Moreover, the syntactic trees induced by ReCAT exhibit strong consistency with human-annotated trees, proving that CIO layers can accurately learn syntactic structures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- It is interesting and useful to augment transformers with multi-grained representations in an unsupervised manner.\n- The proposed method is practical and effective based on the downstream task evaluation."
                },
                "weaknesses": {
                    "value": "- The paper is poorly written, with many notations without explanation. Figure 2 is a copy from Figures in Fast-R2D2, with an unclear caption.\n- Lack of important details, e.g. the computational cost of the proposed layers, how to choose the number of layers, and the hidden dimension of the stacked CIO layers.\n- It would be helpful to explain the main differences and advantages of the proposed method compared with related works, e.g. Fast-R2D2."
                },
                "questions": {
                    "value": "- What are the advantages of the proposed method compared with Fast-R2D2?\n- What is the computational cost and complexity of the proposed layers?\n- How to choose the hyperparameters and the model configurations of the ReCAT models?\n- Can the proposed method be incorporated and augmented to pre-trained models like BERT?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission692/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission692/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission692/Reviewer_m86F"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission692/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698642052277,
            "cdate": 1698642052277,
            "tmdate": 1699635996425,
            "mdate": 1699635996425,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0pS48ZBWEg",
                "forum": "u859gX7ADC",
                "replyto": "HWeb8Hmxn6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission692/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission692/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to reviewer m86F"
                    },
                    "comment": {
                        "value": "Thank you for the time you spent reviewing our paper and for your valuable feedback.\n\nW.1: The paper is poorly written, with many notations without explanation. Figure 2 is a copy from Figures in Fast-R2D2, with an unclear caption.\n\nThank you for reminding us of the clarity issues. We have gone through the paper again and explained some symbols that were not clarified before. To provide a better explanation of the entire pre-training process, we provided a pseudo-code in Algorithm 1.  We will keep improving the paper in terms of clarity.\n\nW.2: Lack of important details, e.g. the computational cost of the proposed layers, how to choose the number of layers, and the hidden dimension of the stacked CIO layers.\n\nThanks for pointing out. \nThe overall time complexity of CIO is $\\mathcal{O}(m^2n)$, where n refers to sequence length and m refers to the pruning threshold(set to 2 as default).\nWe also give a detailed analysis of computational cost in Appendix A.6.  \nMoreover, we reduced the number of steps required to go through the entire CKY chart-table from $2n$ to $2\\textrm{log}(n)$ and thus further reduced the training time on GPUs. Under the fast encoding mode, the overall computational cost is $2\\sim3$ times vanilla Transformers.\nFor a fair comparison, we choose $3$ as the number of the CIO layers and $768$ as the hidden dimension to make the size of the stacked CIO structure comparable to that of Fast-R2D2.\n\nW.3: It would be helpful to explain the main differences and advantages of the proposed method compared with related works, e.g. Fast-R2D2.\n\nThanks for the suggestion. We have elaborated on the differences and the advantages in the general response to all reviewers. We also briefly explain the main differences and advantages of the proposed method in related works in the updated draft.\n\nQ.1. What are the advantages of the proposed method compared with Fast-R2D2?\n\nPlease refer to the general response to all reviewers for details. In short, the major difference lies in the contextual outside pass which is not possessed by Fast-R2D2. Such a mechanism determines that the span representations in ReCAT are contextualized, i.e., with relative positional information and contextual information, and thus can be combined with Transformers. On the other hand, the span representations in Fast-R2D2 are span-local, lacking relative positional information and contextual information.\nMoreover, we also reduce the number of steps required to go through the entire CKY chart-table from $2n$ to $2\\textrm{log}(n)$ which is detailed in Appendix A.5. \n\nQ.2. What is the computational cost and complexity of the proposed layers?\nQ.3 How to choose the hyperparameters and the model configurations of the ReCAT models?\n\nPlease refer to our response regarding to W.2 above. \n\nQ.4. Can the proposed method be incorporated and augmented to pre-trained models like BERT?\n\nYes, it can. A simple approach is to load the BERT parameters into the Transformer layers, and then jointly post-train the whole model. In practice, one may reduce the dimension of the CIO layers to reduce the computational cost but keep the feature of providing disentangled span"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700213466565,
                "cdate": 1700213466565,
                "tmdate": 1700552048541,
                "mdate": 1700552048541,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JV5gThDNtj",
            "forum": "u859gX7ADC",
            "replyto": "u859gX7ADC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission692/Reviewer_fRFA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission692/Reviewer_fRFA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes contextual inside-outside (CIO) layers to explicitly model the syntactic structures of raw text in the encoding process of Transformers. The CIO layers are driven by a variant of the inside-outside algorithm proposed by DIORA (Drozdov et al. (2019) ). The authors leverage the linear neural inside algorithm proposed by R2D2 to prune cells that are unnecessary to encode during the bottom-up inside pass and propose to mix information from both inside and outside a span.\n\nThe benefits brought by CIO include (1) explicitly modeling syntactic structures without requiring parse tree annotations; (2) unlike previous work which only allows information flow within a span, CIO enables cross-span communications in a scalable way; (3) reducing the complexity of the inside-outside algorithm from cubic in DIORA to linear.\n\nThe major contributions are: (1) effectively combining ideas from two methods DIORA and R2D2 with innovative modifications. (2) introducing cross-span communication that breaks the information access constraint. (3) the experimental results on span-level tasks, natural language inference, and grammar induction demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Proposed an effective method to explicitly model the syntactic structure in the encoding process of transformers.\n\n- It\u2019s interesting to see that the Vanilla MLM can be used to replace the training objective of DIORA and the model can effectively learn the syntactic structure with the simple MLM objective.\n\n- Results show that CIO can improve the transformer\u2019s performance on span-based tasks and even help it to learn syntactic trees in an unsupervised manner."
                },
                "weaknesses": {
                    "value": "- Contribution is not significant: the proposed method is an effective way of combining ideas of the inside-outside algorithm from DIORA and node-pruning algorithm from R2D2 with modifications to fit them into the transformer framework and address some weaknesses in DIORA and R2D2 such as the lack of communication between spans.\n\n- Efficiency is low: (1) as mentioned by the author, the computational load is many times higher than that of the vanilla Transformer model and the complexity of a single CIO layer is O(m2n). (2) If using fast-r2d2, the CIO layer needs a pre-trained top-down parser to predict a parse tree for a given sentence.\n\n- The major performance improvement is on Span-level tasks, and on sentence-level tasks ReCAT achieves worse performance compared with a vanilla transformer with similar size. On the structure-prediction task, ReCAT achieves similar performance compared with the previous method TN-PCFG."
                },
                "questions": {
                    "value": "- How do you deal with sub-word tokens: if a word is broken into multiple tokens, how do you handle them in the CIO layer?\n\n- In 3.3, you mentioned \u201cSpecifically, we directly use the pretrained top-down parser to predict a parse tree for a given sentence and apply the trained Compose functions\u201d, does this mean that your method requires a pretrained top-down parser?\n\n- Is it necessary to train the Transformer from scratch in the experiments? One way of using CIO layers is to use them as adaptors which can be plugged into multiple transformer layers. Training a model from scratch can impose significant computational cost and it could be more efficient to harness the pretraining of existing Transformers instead of training from scratch."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission692/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771117871,
            "cdate": 1698771117871,
            "tmdate": 1699635996350,
            "mdate": 1699635996350,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KWhcLo1B66",
                "forum": "u859gX7ADC",
                "replyto": "JV5gThDNtj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission692/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission692/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to reviewer fRFA"
                    },
                    "comment": {
                        "value": "Thank you for the time you spent reviewing our paper and for your valuable feedback.\n\nW.1: Contribution is not significant: the proposed method is an effective way of combining ideas of ..\n\nYes, your understanding of our work is right. But beyond combining them, we further enable it to be jointly pre-trained with Transformers.\nThe key weaknesses along the line of composition-based models are:\n\n(1) bad scalability\n\n(2) difficulty in combining with Transformers.\n\nWe largely improve the scalability and demonstrate the key to combining with Transformers is joint pre-training. Meanwhile, joint pre-training is non-trivial for previous work like Fast-R2D2/DIORA. We explain the details in the general response. \n\nCombining composition-based models with Transformers is a meaningful direction as it allows for the integration of constituent-level symbolic interfaces. Currently, Language Models (LLMs) can engage in natural language conversations with humans. However, researchers face a limitation wherein they can only interact with Transformer models using tokens without a rich symbolic interface.\nRecent studies have demonstrated that utilizing spans as symbolic interfaces can yield fascinating outcomes. For instance, it enables alignment with visual information[1] and facilitates grounded lexicon learning[2]. Offering constituent-level interfaces for Transformers holds great potential. These interfaces can be combined with retrieval, graphs, and more, leading to the development of intriguing features for Transformers. The span-level task is also a good example. In ReCAT, each span has an explicit symbol interface to obtain its representation, and the results significantly outperform Transformers.\nHowever, it's still underexplored on how to integrate constituent-level symbolic interfaces with Transformers in an end-to-end approach, which is the key topic we discuss in this work.\n\n[1] Bo Wan, Wenjuan Han, Zilong Zheng, Tinne Tuytelaars. Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling. ICLR 2022.\n\n[2] Jiayuan Mao, Haoyue Shi, Jiajun Wu, Roger P. Levy and Joshua B. Tenenbaum. Grammar-Based Grounded Lexicon Learning. NeurIPS 2021\n\nW.2: Efficiency is low.\n\n(1) ReCAT is indeed slower than vanilla Transformer,  but the CIO layer has been $5$ times faster than previous works like Fast-R2D2. We update the efficiency analysis in Appendix A.6. Moreover, we can control the cost via the fast encoding technique. \n\n(2) there might be misunderstanding regarding to ``if we need a pre-trained top-down parser''. In fact, the pre-trained top-down parser in fast encoding refers to the parser used in the CIO layers, which is pre-trained along with the CIO layers. Therefore, no extra pre-trained top-down parser is required. We improve the clarity by the pseudo-code (i.e., Algorithm 1) in the update of the draft. \n\nW.3:  About performance on sentence-level tasks compared with a vanilla transformer with similar size. On the structure-prediction task, ReCAT achieves similar performance compared with the previous method TN-PCFG.\n\nYes, but we argue when the total layer numbers are the same, our results are still competitive on GLUE. We additionally add $ReCAT_{noshare}$[3,1,6] whose total layer number is 9. Its performance is competitive with Transformer 9.\nRegarding grammar induction, TN-PCFG is close to ReCAT in terms of performance, but considering its $O(n^3)$ time complexity while ReCAT is just $O(n)$, we think the advantage of ReCAT over TN-PCFG is huge in practice. Meanwhile, we can further improve via pre-training on the larger corpus.\n\nQ.1. How do you deal with sub-word tokens: if a word is broken into multiple tokens, how do you handle them in the CIO layer?\n\nHandling the sub-word tokens does not need any special operations. We just take them as ordinary input tokens. The CIO layers can learn how to compose them properly during pre-training.\n\nQ.2.  In 3.3, you mentioned \u201cSpecifically, we directly use the pre-trained top-down parser to predict a parse tree for a given sentence and apply the trained Compose functions\u201d, does this mean that your method requires a pre-trained top-down parser?\n\nNo and we have clarified this point in response to W.2.\n\nQ.3. Is it necessary to train the Transformer from scratch?\n\nWe suppose it's necessary for a research paper.\nAs there is no previous work about whether span representations are helpful to augment Transformers, we need to compare ReCAT and vanilla Transformers as fairly as possible.\nMeanwhile, since the pre-trained Transformer has never seen span representations, we still need to post-train ReCAT to adopt those new span representations from CIO layers. It costs more computational resources to post-train on a corpus like OpenWebText than from scratch on wiki-103. Moreover, it would be hard to tell if the gain is from extra training steps or CIO layers."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700213172631,
                "cdate": 1700213172631,
                "tmdate": 1700555781311,
                "mdate": 1700555781311,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NsHMgEE4X0",
                "forum": "u859gX7ADC",
                "replyto": "JV5gThDNtj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission692/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission692/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further clarification"
                    },
                    "comment": {
                        "value": "There might be a misunderstanding regarding \"It\u2019s interesting to see that the Vanilla MLM can be used to replace the training objective of DIORA and the model can effectively learn the syntactic structure with the simple MLM objective.\"\n\nSimply replacing the training objective of DIORA with the MLM objective $\\textbf{doesn't work well}$.  In Table 3 and Table 4. $ReCAT_{noshare}[1,3,3]$  can be viewed as an extended version of DIORA (run inside-outside algorithm only once, with 3-layered Transformers as encoder, but replacing the outside pass with our contextual outside pass, pretrained via MLM), but it does not perform well on grammar induction and glue tasks. The original version is supposed to perform worse if pretrained via MLM.\n\nFast-R2D2+DIORA+MLM doens't work well. Fast-R2D2+DIORA+ iterative up-and-down mechanism + contextual outside + MLM makes it work.  The iterative up-and-down mechanism(stackable CIO layers) and contextual outside is our key innovation,  enabling the CIO layer to be pre-trained through Masked Language Modeling, thereby making it possible to achieve joint pre-training with Transformer through MLM. \n\nPlease note that the original inside-outside algorithm is not stackable, as the outside algorithm was originally designed to construct language modeling objectives, thus there is no interaction between information inside and outside of a span. However, our contextual outside pass is designed to contextualize span representations,  allowing for stacking and adapting to Masked Language Modeling."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700318819778,
                "cdate": 1700318819778,
                "tmdate": 1700473365940,
                "mdate": 1700473365940,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qdjcm8r2Ac",
            "forum": "u859gX7ADC",
            "replyto": "u859gX7ADC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission692/Reviewer_46mR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission692/Reviewer_46mR"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a syntax-augmented transformer based on producing span representations following the inside and outside traversals of a parse chart. Unlike existing methods following this idea (Fast-R2D2, DIORA), they incorporate cross-span contextualization. The resulting architecture is competitive with vanilla transformers on span-level and GLUE tasks while achieving strong grammar induction results."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The method performs well compared to both transformers and Fast-R2D2 on span-level and sentence-level tasks, while also achieving strong grammar induction results.\n\n(2) The method is evaluated on a wide range of tasks and compared to a wide range of baselines."
                },
                "weaknesses": {
                    "value": "(1) As the authors mention, one of the main motivations for augmenting transformers with syntax is to improve compositional generalization (as well as potentially interpretability and controllability). Therefore, while the method does outperform transformers for span-level tasks, I think the results would stronger if there were experiments addressing these original motivations.\n\n(2) While I understand that the method is inherently complex, I still think the methods section could be made a little clearer. For example, maybe a self-contained step-by-step summary of the algorithm at the end of the section would be helpful.\n\n(3) While their method does outperform Fast-R2D2, the ideas feel very similar, limiting the scientific contribution of this work. As it stands, it feels a bit like a complex combination of ideas from Fast-R2D2 and DIORA, where the takeaway is a bit unclear. Is the key difference scalability, the use of transformers, pruning, cross-span communication, or all of the above? If the key difference is cross-span communication, why is that important and what are the specific changes in the algorithm that enable this difference? And are there ablation experiments that support the claim that cross-span communication is important?\n\n(4) Related to (3), the paper would benefit from ablations to support the claims of why certain design decisions were effective."
                },
                "questions": {
                    "value": "(1) I find it a bit surprising that Fast-R2D2+Transformer is worse than Fast-R2D2 on GLUE (Table 3). Is there some intuition for why this is the case?\n\n(2) Is there an ablation for ReCat without extra transformer layers?\n\n(3) What are the model sizes for ours_{share} and ours_{noshare} in Table 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission692/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission692/Reviewer_46mR",
                        "ICLR.cc/2024/Conference/Submission692/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission692/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810300663,
            "cdate": 1698810300663,
            "tmdate": 1700714053584,
            "mdate": 1700714053584,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MsJjHQ9JLa",
                "forum": "u859gX7ADC",
                "replyto": "qdjcm8r2Ac",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission692/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission692/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to reviewer 46mR"
                    },
                    "comment": {
                        "value": "Thank you for the time you spent reviewing our paper and for your valuable feedback.\n\nW.1: about compositional generalization.\n\nin addition to the span-level tasks, we also observed significant gains on the grammar induction task (i.e., Table 4 in the paper). This could be another piece of evidence that indicates the compositional generalization of the model. \nAs compositional generalization refers to the ability to recognize or generate novel combinations of observed elementary concepts, our model has never seen sentences in the test set in the grammar induction task but induces their structures well.\nBesides, we also show a few parsing results from ReCAT in Appendix A.7, through which we aim to show what the syntax augmented Transformer really learned. Indeed, the showcases exhibit high consistency with human-annotated structures (i.e., the gold trees), meaning that the superior performance of ReCAT on the span-level tasks may stem from the recovery of the underlying syntactic structures of texts.\n\nW.2: ..., maybe a self-contained step-by-step summary of the algorithm at the end of the section would be helpful. \n\nWe appreciate your suggestion and add pseudo code (i.e., Algorithm 1) in the updated draft to show how the model is pre-trained step-by-step. \n\nW.3: ...,the takeaway is a bit unclear\n\nWe have discussed in detail about the advantages of ReCAT and the innovation we made in the work in the general response. The takeaway is \n\n(1) the key advantages of ReCAT over DIORA and Fast-R2D2 are the capability of joint pre-training with Transformer and scalability; \n\n(2) to achieve the advantages, we develop the iterative up-and-down mechanism (via stacking multiple CIO layers) and the contextual outside pass, which represent the key difference we made in the work comparing to DIORA and Fast-R2D2. \n\nPlease note that the original inside-outside algorithm is not stackable, as the outside algorithm was originally designed to construct language modeling objectives while ours is designed to contextualize span representations, allowing for stacking and adapting to Masked Language Modeling.\n\nW.4: Related to (3), the paper would benefit from ablations to support the claims of why certain design decisions were effective.\n\nThanks for the suggestion. Since the iterative up-and-down mechanism and the contextual outside pass are key technical contributions, it is better for us to ablate the model so as to inspect the effect of each design. In fact, we have done the ablation in the experiments:\n\n (1) ablation of iterative up-and-down. This is done by the comparison between $ReCAT_{share}$[1,3,3] and $ReCAT_{share}[3,1,3]$ in Table 3. The two models have similar parameter sizes, but $ReCAT_{share}[3,1,3]$ contains 3 layers of CIO while $ReCAT_{share}[1,3,3]$ only has one, i.e, no iterative up-and-down.\nThe results in Table 3 indicate that when the total numbers of parameters are close, stacking CIO layers can bring consistent improvement. Additionally, we update Table 4  by including $ReCAT_{share}[1,3,3]$. Again, the big gap between the two models indicates the value of the proposed mechanism.\nWe additional added an explanation for them in the updated draft.\n\n(2) ablation of joint pre-training. This can be checked by comparing ReCAT$_{share}[3,1,3]$ with Fast-R2D2+Transformer and DIORA+Transformer in Table 3.  Transformers used in these baselines are randomly initialized.\nFrom the results, we can conclude that the joint pre-training is important.  \n\nResponse to Q1:\n\nWe were surprised too when we first observed the results. Appending Transformers is helpful for QQP but leads to no gain for MNLI.\nAnd to find out ``why'' motivates the work. As we analyze in Section 4.2, composition-based models are good at tasks that only need literal\ntext information while Transformers are good at tasks requiring information beyond literal texts, i.e., knowledge.\nWe realized the importance of joint pretraining. Knowledge entailed in corpora is compressed into Transformers during the pretraining. \nJust appending Transformers without joint pre-training, the two are incompatible.\nHowever, as we have elaborated in the general response, one cannot perform joint pre-training by simply appending Transformers after Fast-R2D2 or DIORA. Thus we propose CIO layers to address a series of issues even including scalability.\n\nResponse to Q2:\n\nWe update Table 3 in the draft by adding a baseline ReCAT$_{share}[3,1,0]$ (i.e., no transformer layers). As expected, it is worse than Fast-R2D2 and DIORA. The comparison indicates that MLM itself is not so efficient as the original objective used by R2D2 and DIORA (15\\% vs 100\\%). The advantage of MLM lies in its ability to make pre-training with Transformer feasible. \n\nResponse to Q3:\n\nSorry for the abuse of notations. They are $ReCAT_{share}[3,1,3]$ and $ReCAT_{noshare}[3,1,3]$ respectively whose parameter sizes are shown in Table 1. We have updated the draft to improve clarity."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700212874799,
                "cdate": 1700212874799,
                "tmdate": 1700552689197,
                "mdate": 1700552689197,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c0EnlJtX5B",
                "forum": "u859gX7ADC",
                "replyto": "MsJjHQ9JLa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission692/Reviewer_46mR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission692/Reviewer_46mR"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply and revisions. I still think the paper could be made clearer, but I have raised my score to an 8."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714117931,
                "cdate": 1700714117931,
                "tmdate": 1700714117931,
                "mdate": 1700714117931,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v1Xqxi8jag",
            "forum": "u859gX7ADC",
            "replyto": "u859gX7ADC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission692/Reviewer_FCy4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission692/Reviewer_FCy4"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed to augment the Transformer architecture with a contextual inside-outside layer (CIO). The CIO layer model explicitly the recursive syntactic compositions. The CIO layer goes in between the embedding layer and the self-attention layer. The author proposed a new variant of the inside-outside algorithm with contextualization ability. The CIO layer allows modeling of the inter-span and intra-span interactions. Experiments show that the proposed method outperforms the plain Transformer models on span-level tasks. It also outperforms recursive models on natural language inference tasks. The CIO also has good interpretability since it has strong consistency with human-annotated syntactic trees."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The author proposed a novel method that include the recursive syntactic composition information into the Transformer architecture. The method is verified to be effective, especially for span-based tasks, by experiments with"
                },
                "weaknesses": {
                    "value": "As authors pointed out, the CIO layer can be expensive. I'm also wondering how is the effect"
                },
                "questions": {
                    "value": "1. It would be interested to see if the gains still holds when we scale to larger models. (It is recognized it's not always possible/easy to experiment with a larger model.)\n2. I'm wondering if the CIO layer can be trained at the finetuning stage so that we can still utilize a powerful pre-trained model and augment it with the CIO layer only at the finetuning stage.\n3. This might be diverging or outside the scope of the paper, but I'm curious how tokenization would be affecting the effectiveness of the proposed method. For example, if it's byte or character-level vocabulary, do we observe a similar gain."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission692/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission692/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission692/Reviewer_FCy4"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission692/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699601274224,
            "cdate": 1699601274224,
            "tmdate": 1699644610815,
            "mdate": 1699644610815,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hRlN81hzfN",
                "forum": "u859gX7ADC",
                "replyto": "v1Xqxi8jag",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission692/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission692/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Dear Reviewer FCy4, may I have your complete comments?"
                    },
                    "comment": {
                        "value": "Thanks for your insightful comments. But part of them seem incomplete, especially strengths and weaknesses. So could you kindly paste full of them once you have time? Many Thanks.\n\nWe are still preparing the rest part of the authors' response. But the question about character-level parsing we can respond first.\nWe did try applying the inside-outside algorithm on character-level vocabulary to see if it could induce underlying structures of characters. Unfortunately, it doesn't work well in languages based on spelling like English. A key reason we guess is the information entropy is too low as there are only 26 characters in English. Thus feedback is so weak for the inside-outside algorithm. However, it could work well in languages with large basic character-level vocabulary like Chinese."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699935887160,
                "cdate": 1699935887160,
                "tmdate": 1699935887160,
                "mdate": 1699935887160,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GCxul9sA0K",
                "forum": "u859gX7ADC",
                "replyto": "v1Xqxi8jag",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission692/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission692/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to reviewer FCy4"
                    },
                    "comment": {
                        "value": "Thank you for the time you spent reviewing our paper and for your valuable feedback.\n\nW.1: As the authors pointed out, the CIO layer can be expensive. I'm also wondering how is the effect.\n\nAs we have explained in the general response, with all the effort (contextual inside-outside with $\\mathcal{O}(n)$ time complexity and $2\\textrm{log}(n)$ steps), the CIO layer has been $5$ times faster than using the original pruning algorithm proposed in Fast-R2D2. We update the efficiency analysis in Appendix A.6 in the draft. On the other hand, training a ReCAT model is indeed more expensive than training a vanilla Transformer with a similar parameter size ($88M$) due to the dynamic programming in stacked CIO layers. We can control the running time of ReCAT to be about 2 to 3 times that of the vanilla Transformer during inference and fine-tuning using the fast encoding technique described in Appendix A.3. Considering the outstanding performance of ReCAT on span-level tasks and the grammar induction task, we think the cost is worthwhile. \n\nQ.1. It would be interested to see if the gains still holds when we scale to larger models.\n\nWe train a larger ReCAT with 3 CIO layers and 6 Transformer layers (namely ReCAT$_{noshare}[3,1,6]$), and update the results in Table 3 in the draft. We can see that the performance of ReCAT continuously gets improved with the increase in model size ($[1,1,3] \\rightarrow [3,1,3] \\rightarrow [3,1,6]$). \n\nQ.2. I'm wondering if the CIO layer can be trained at the finetuning stage so that we can still utilize a powerful pre-trained model and augment it with the CIO layer only at the finetuning stage.\n\nThough we are not able to finish the extra experiments in such a short rebuttal period, we believe that joint post-training is still necessary to adapt to span-level representations, as the pre-trained model has never seen those span-level representations during its pre-training period. \n\nQ.3. this might be diverging or outside the scope of the paper, but I'm curious how tokenization would be affecting the effectiveness of the proposed method. For example, if it's byte or character-level vocabulary, do we observe a similar gain.\n\nPlease refer to the previous reply."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission692/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700210893212,
                "cdate": 1700210893212,
                "tmdate": 1700222686530,
                "mdate": 1700222686530,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]