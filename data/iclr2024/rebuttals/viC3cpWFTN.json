[
    {
        "title": "Clip21: Error Feedback for Gradient Clipping"
    },
    {
        "review": {
            "id": "0amZdqyF0d",
            "forum": "viC3cpWFTN",
            "replyto": "viC3cpWFTN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6746/Reviewer_Sgc9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6746/Reviewer_Sgc9"
            ],
            "content": {
                "summary": {
                    "value": "Gradient clipping is a commonly-used technique in training deep neural networks to resolve the exploding gradient issue. This paper shows that in a distributed setting, naively clipping the client gradient would result in large estimation error. To overcome this challenge, this paper proposes the Clip21 mechanism inspired by the error-feedback framework. The authors establish theoretical guarantee for the convergence rate of Clip21 to stationary points, and the rate is faster than previous works. Finally, experiments are conducted to demonstrate the efficiency of the proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The Clip21 method proposed in this paper seems to be novel. Although there are previous works on the error-feedback framework, none considers the setting of clipped gradients.\n\n2) The authors establish rigorous theoretical analysis for Clip21 under standard assumptions. Notably, heterogeneous client objective functions are allowed.\n\n3) Overall speaking, the paper is well written and all definitions and theorem are stated clearly."
                },
                "weaknesses": {
                    "value": "1) While gradient clipping is originally known to solve the exploding gradient problem, it seems not reflected in theoretical analysis of this paper. This paper still considers the standard $L$-smooth setting rather than $(L_0,L_1)$-smoothness, so the benefit of using clipped gradients is unclear from the theory.\n\n2) This paper only considers the case for deterministic gradients. Although it is expected that similar results hold for stochastic gradients, in the gradient clipping literature more restrictive assumptions are made on noise (bounded noise tather than the more standard bounded variance, see e.g. [1]). It is unclear whether gradient noise is an issue in the distributed setting.\n\n[1] Zhang, J., He, T., Sra, S., & Jadbabaie, A. (2019). Why gradient clipping accelerates training: A theoretical justification for adaptivity. arXiv preprint arXiv:1905.11881."
                },
                "questions": {
                    "value": "1) In your Example 1.1, what if one chooses different clipping thresholds for different clients, or use a 'soft clipping' $\\nabla f(x)/(\\|\\nabla f(x)\\|+a)$? In these cases the clipped gradient does not seem to cancel out.\n\n2) (related to weaknesses 1) What is the motivation of considering gradient clipping at the client level? Because in a distributed setting, one only wants to optimize the averaged objective function, can we just clipp the averaged gradient at the server to accelerate training? If you clip the server's gradient instead of the clients, then the problem demonstrated by Example 1.1 does not exist, and there may be a more straightforward approach.\n\nI am willing to increase my rating if my concerns are properly addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6746/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698449710451,
            "cdate": 1698449710451,
            "tmdate": 1699636776719,
            "mdate": 1699636776719,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v1zvu6cq5e",
                "forum": "viC3cpWFTN",
                "replyto": "0amZdqyF0d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6746/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6746/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Sgc9 [Part 1/2]"
                    },
                    "comment": {
                        "value": "> **While gradient clipping is originally known to solve the exploding gradient problem, it seems not reflected in theoretical analysis of this paper. This paper still considers the standard $L$-smooth setting rather than $(L_0,L_1)$-smoothness, so the benefit of using clipped gradients is unclear from the theory.**\n\nWe agree that Clip-GD enjoys stronger convergence guarantees than GD for problems under the $(L_0, L_1)$ smoothness assumption, which implies the exploding gradient condition in deep neural network training [1]. However, existing results of gradient clipping algorithms are limited for the centralized case (one master and one client). \n\nEven under the $L$-smoothness assumption, we show that distributed clipping algorithms do not even converge towards the optimal solution (see Example 1). This motivates our investigation to modify standard clipping methods to attain strong convergence for distributed settings. We also prove theoretically and empirically that Clip21-GD outperforms these standard gradient clipping algorithms. \n\nFurthermore, we believe that our analysis is sufficient to prove the convergence under the $(L_0, L_1)$ smoothness. We prove this by finding the upper-bound for $\\Vert \\nabla^2 f(x_k) \\Vert$ and our results apply as follows: \n\n1. By the $(L_0, L_1)$ smoothness and Assumption 3 of [A1],  \n\n$$\n\\Vert \\nabla^2 f(x_k) \\Vert \\leq L_0 + L_1 \\Vert \\nabla f(x_k) \\Vert \\leq L_0 + L_1 \\sqrt{2L[f(x_k)-f_{\\inf}]}.\n$$\n\n2. We then have $\\Vert \\nabla^2 f(x_k) \\Vert \\leq L_0 + L_1 \\sqrt{2L\\phi_0}$ for all $k \\geq 0$ by proving that $f(x_k) \\leq f_{\\inf}+\\phi_0$ by the induction and by setting the step-size $\\gamma$ sufficiently small. \n\nWe also refer the reviewer to our general response, where we explain why the current analysis from our paper holds under $(L_0, L_1)$-smoothness.\n\n[A1]: Khaled, Ahmed, and Peter Richt\u00e1rik. \"Better theory for SGD in the nonconvex world.\" arXiv preprint arXiv:2002.03329 (2020).   \n\n\n> **This paper only considers the case for deterministic gradients. Although it is expected that similar results hold for stochastic gradients, in the gradient clipping literature more restrictive assumptions are made on noise (bounded noise tather than the more standard bounded variance, see e.g. [1]). It is unclear whether gradient noise is an issue in the distributed setting.** \n\nStochastic optimization algorithms suffer from instability for neural network training, which is in the presence of heavy-tailed stochastic gradient noises. This has been observed empirically for training neural network models by centralized stochastic gradient descent in Zhang et al. (2020b), and distributed methods in [M1]. Furthermore, deriving the result for Clip21-GD with stochastic gradients is non-trivial, because of (1) the dependency of the clipping threshold $\\tau$ on the noise variance parameters, and (2) the required condition on the bounded norm of the input vector before clipping. This may be proved by the strong bounded noise condition, e.g. $\\Vert g - \\nabla f(x) \\Vert^\\alpha \\leq \\sigma^\\alpha$ with probability 1. \n\n[M1] Yang, Haibo, Peiwen Qiu, and Jia Liu. \"Taming Fat-Tailed (\u201cHeavier-Tailed\u201d with Potentially Infinite Variance) Noise in Federated Learning.\" Advances in Neural Information Processing Systems 35 (2022): 17017-17029.\n\n\n> **In your Example 1.1, what if one chooses different clipping thresholds for different clients, or use a 'soft clipping' $\\nabla f(x)/(\\vert \\nabla f(x) \\vert + a)$? In these cases the clipped gradient does not seem to cancel out.**\n\nBy using different clipping thresholds for different clients or soft clipping, the clipped gradient may not cancel out in Example 1.1. However, the solution using the clipped gradient from both cases will be further away from the initial solution for simple toy examples. \nFirst, we prove the case for different clipping thresholds by letting in Example 1.1 $\\beta = 10$, $\\alpha=-1$, $x_0=1$, and the clipping thresholds at $0.1$ for $f_1(x)$ and $1$ for $f_2(x)$. Then, \n\n$$\nx_1 =  x_0 - \\gamma ( clip_{0.1}(\\nabla f_1(x)) + clip_{0.1}(\\nabla f_2(x))) = 1 + 0.9 \\gamma. \n$$\n\n$x_1$ is thus further away from the optimum $x^\\star=0$, as the step-size $\\gamma>0$. \nSecond, we prove the case for soft clipping by considering the problem of minimizing $f(x)=f_1(x)+f_2(x)+f_3(x)$ for $x\\in\\mathbb{R}$ and $f_i(x)=(\\alpha_i/2) x^2$ for $\\alpha_i>0$ and $i=1,2,3$. If we choose $\\alpha_1 = 10$, $\\alpha_2=-4$, $\\alpha_3=-4$, $x_0=1$ and $a=1$, then $f$ has Lipschitz continuous gradient with $L=2$, and also \n\n$$\nx_1 =  x_0 - \\gamma ( softclip_a(\\nabla f_1(x)) + softclip_a(\\nabla f_2(x)) + softclip_a(\\nabla f_3(x))  ) = 1 + \\gamma \\frac{38}{55}. \n$$\n\n$x_1$ is therefore more distant from the optimum $x^\\star=0$ as $\\gamma>0$."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6746/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594325882,
                "cdate": 1700594325882,
                "tmdate": 1700594325882,
                "mdate": 1700594325882,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AFajlbFjWX",
                "forum": "viC3cpWFTN",
                "replyto": "0amZdqyF0d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6746/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6746/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Sgc9 [Part 2/2]"
                    },
                    "comment": {
                        "value": "> **(Related to weaknesses 1) What is the motivation of considering gradient clipping at the client level? Because in a distributed setting, one only wants to optimize the averaged objective function, can we just clipp the averaged gradient at the server to accelerate training? If you clip the server's gradient instead of the clients, then the problem demonstrated by Example 1.1 does not exist, and there may be a more straightforward approach.**\n\nImplementing the client-side gradient algorithm is necessary for designing federated averaging algorithms that stabilize neural network training while saving communication costs, e.g. CELGC by Liu et al. (2022). Clip21-GD can be modified for these federated learning applications to accelerate the convergence of CELGC by Liu et al. (2022). This is because distributed gradient clipping algorithms can be viewed as a special case of CELGC by Liu et al. (2022) with the number of local steps $I=1$. Also note that the implementation of clipping the average gradient of clients can be viewed as Clip21-GD with the $n=1$ case supported by our theory."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6746/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594363495,
                "cdate": 1700594363495,
                "tmdate": 1700594363495,
                "mdate": 1700594363495,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AsslNhfvGE",
            "forum": "viC3cpWFTN",
            "replyto": "viC3cpWFTN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6746/Reviewer_y5AM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6746/Reviewer_y5AM"
            ],
            "content": {
                "summary": {
                    "value": "This article presents Clip21-GD, a new  client-side gradient clipping algorithm designed for distributed training. The inspiration for Clip21-GD is drawn from the error feedback mechanism EF-21, employed to accelerate the convergence of gradient-compression distributed optimization algorithms. Notably, Clip21-GD achieves a convergence rate of $O(1/K)$, the same as standard distributed Gradient Descent (GD), and provides a more refined theoretical convergence analysis compared to EF-21, highlighting distinctions between clipping and compression operations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-The theoretical convergence analysis for Clip21-GD is a strong point, as it reveals valuable insights. The theoretical $O(1/K) $ convergence rate surpasses that of the CE-FedAvg, signifying faster convergence."
                },
                "weaknesses": {
                    "value": "- The article's motivation may need further clarity. While it claims to address the importance of deep neural network (DNN) training, the use of gradient descent (GD) in Clip-GD may appear unsuitable for DNNs. While Clip-SGD is briefly mentioned in the appendix for VGG 11 training, its theoretical convergence rate is not thoroughly explored in the main text, leaving room for ambiguity.\n\n- The necessity of a client-side gradient clipping algorithm is not sufficiently explained. The implementation of clipping the average gradient of clients on the server side may seem more straightforward and effective in managing explosive gradients to prevent convergence issues."
                },
                "questions": {
                    "value": "The article's disclosure that gradient clipping in Clip21-GD will not work after a specific number of steps ($K=O(1/\\tau)$) raises questions about the algorithm's practicality and long-term effectiveness. The purpose and usability of Clip21-GD beyond this threshold remain unclear."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6746/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805481420,
            "cdate": 1698805481420,
            "tmdate": 1699636776606,
            "mdate": 1699636776606,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kLaH5JjQVy",
                "forum": "viC3cpWFTN",
                "replyto": "AsslNhfvGE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6746/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6746/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer y5AM"
                    },
                    "comment": {
                        "value": "> **The article's motivation may need further clarity. While it claims to address the importance of deep neural network (DNN) training, the use of gradient descent (GD) in Clip-GD may appear unsuitable for DNNs. While Clip-SGD is briefly mentioned in the appendix for VGG 11 training, its theoretical convergence rate is not thoroughly explored in the main text, leaving room for ambiguity.**\n\nWe believe that Clip21-GD can be modified to obtain similar convergence guarantees when it uses stochastic gradients. This is because EF21 that inspires Clip21-GD attains the $\\mathcal{O}(K^{-1})$ convergence rate when it uses stochastic gradients for nonconvex problems (see Theorem 3 in Appendix D.1 of [L1]). From this theorem, the residual error term of running EF21-SGD decreases when we increase the mini-batch size of stochastic gradients.  \n\nHowever, deriving the result for Clip21-GD with stochastic gradients is non-trivial because of the required condition on the bounded norm of the input vector before clipping. This condition can be proved by the strong bounded noise condition, e.g. $\\Vert g - \\nabla f(x) \\Vert\\leq \\sigma$ with probability 1 [L2]. \n\n[L1] Fatkhullin, Ilyas, Igor Sokolov, Eduard Gorbunov, Zhize Li, and Peter Richt\u00e1rik. \"EF21 with bells & whistles: Practical algorithmic extensions of modern error feedback.\" arXiv preprint arXiv:2110.03294 (2021).\n\n[L2] Zhang, Bohang, Jikai Jin, Cong Fang, and Liwei Wang. \"Improved analysis of clipping algorithms for non-convex optimization.\" Advances in Neural Information Processing Systems 33 (2020): 15511-15521.\n\n\n> **The necessity of a client-side gradient clipping algorithm is not sufficiently explained. The implementation of clipping the average gradient of clients on the server side may seem more straightforward and effective in managing explosive gradients to prevent convergence issues.**\n\nWe believe that implementing the client-side gradient algorithm is necessary for designing federated algorithms that stabilize neural network training while saving communication costs, e.g. CELGC by Liu et al. (2022). Clip21-GD can be extended for these federated set-ups to further accelerate the convergence of CELGC by Liu et al. (2022). This is because distributed gradient clipping algorithms can be viewed as a special case of CELGC by Liu et al. (2022) with the number of local steps $I=1$. Also note that the implementation of clipping the average gradient of clients can be viewed as Clip21-GD with the $n=1$ case supported by our theory.   \n\n> **The article's disclosure that gradient clipping in Clip21-GD will not work after a specific number of steps ( $K=\\mathcal{O}(1/\\tau)$) raises questions about the algorithm's practicality and long-term effectiveness. The purpose and usability of Clip21-GD beyond this threshold remain unclear.**\n\nClip21-GD and Clip-GD become classical gradient descent when the clipping threshold $\\tau$ is extremely large. In this situation, we cannot expect both gradient clipping methods to outperform classical gradient descent. Furthermore, from Figure 1 and 2, Clip21-GD empirically outperforms Clip-GD, especially when the clipping threshold $\\tau$ becomes small. This is because Clip21-GD allows for the clipping to be inactive sooner than Clip-GD. This verifies Proposition 5.2 and 5.5, stating that the step before clipping becomes inactive $K=(1/\\tau)(\\| \\nabla f(x_0) \\| - \\tau)$ is large as the clipping threshold $\\tau$ is small and the initial solution $x_0$ satisfies $\\| \\nabla f(x_0)\\| \\gg \\tau$."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6746/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593534955,
                "cdate": 1700593534955,
                "tmdate": 1700593534955,
                "mdate": 1700593534955,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FPyqjCbKTf",
                "forum": "viC3cpWFTN",
                "replyto": "AsslNhfvGE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6746/Reviewer_y5AM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6746/Reviewer_y5AM"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the responses from the author, yet my concerns persist.\n\n1. The extension from Clip21-GD to Clip21-SGD is non-trivial, challenging the claim that Clip21-GD effectively addresses issues in DNN training. This motivation seems overstated.\n\n2. The rationale behind implementing client-side gradient clipping remains unclear. Employing simpler server-side gradient clipping could also potentially achieve training stability, and the benefits of client-side clipping in reducing communication overhead are not clearly demonstrated."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6746/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671290145,
                "cdate": 1700671290145,
                "tmdate": 1700671355335,
                "mdate": 1700671355335,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qdqPqLJCIs",
            "forum": "viC3cpWFTN",
            "replyto": "viC3cpWFTN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6746/Reviewer_HYZq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6746/Reviewer_HYZq"
            ],
            "content": {
                "summary": {
                    "value": "In this paper the authors study the convergence of Gradient Descent (GD) for the optimization of sum of functions $f:=\\frac{1}{n}\\sum_{i=1}^if_i(x_i)$ (which arises naturally in Empirical Risk Minimization) when each of the individual gradient $\\nabla_xf_i(x)$ is clipped. They assume Lipschitz gradient. Their contribution are the following:\n* naive gradient clipping fail convergence in simple cases, even on mean-estimation tasks\n* by fixing the mean estimation task with clipped gradients, it is possible to fix GD+clipping algorithm itself\n* different variants are proposed, with their own rates  \n  \nThe results are illustrated on logistic regression, *with* and *without* convex regularization (i.e. with a convex and a non-convex task). Importantly, the work generalizes the n=1 case, and retrieve the same rates as existing literature for n=1."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "### Clarity\nThe paper is extremely well written. The position of this work to the related work is clear. The problem is clearly stated. There are very convincing examples, progressive explanations of the proof strategies, and the \"right level\" of details.  \n\n### Significance & Quality\nElementwise Gradient clipping is widespread in the Differential Privacy literature; therefore these theoretical results are significant. The hypothesis on $f$ are realistic even in the context of deep learning, making the paper relevant for the community."
                },
                "weaknesses": {
                    "value": "## Practical relevance\n\nWhile the paper is well written and provide interesting insights, I have a few concerns about its usefulness *in practice*.  \n\n### Relevance to Differential Privacy\n\nElement-wise gradient clipping is typically used DP-SGD (Abadi et al, 2016). DP-SGD uses clipping to control the sensitivity of gradient steps. Then DP-SGD adds a Gaussian noise to the clipped gradient to create a Gaussian mechanism. In Alg 1, the clipped vectors are aggregated, but the influence of a noise $\\zeta_i$ on the mean estimate $\\frac{1}{n}\\sum_iv_i$ is not studied, therefore it is not clear if this algorithm would work in the context of differential privacy.  \n\n### Relevance for exploding gradients\n\nAs written in the paper:\n\n> we are not employing clipping as a tool for taming the exploding gradients problem, and our work is fully complementary to this literature. [...] Clipping after averaging does not cause the severe bias and divergence issues we are addressing in our work.\n\n## Comparison against vanilla GD\n\nThe paper positions itself against the Clip-GD algorithm (among others... ), in all experiments. But experiments with comparison against **vanilla GD** are lacking.  \n\n### Conclusion\n\nSee my question below."
                },
                "questions": {
                    "value": "### When do we care about the average ?  \n  \nThis is a high level question.  \n    \nThe idea of Algorithm 1 is to estimate the average $\\frac{1}{n}\\sum_ia_i$ *exactly*. Clipping may help to circumvent instabilities when some individual gradients $a_i$ are too high (whetever the reason, exploding gradient being an example, numerical inaccuracies another). In this case, **biasing the average direction** with clipping *is* the wanted behavior, since we don't want the descent step to be driven by a single example with exceedingly large gradient norm. The average operator $\\bar a:=a\\mapsto \\frac{1}{n}\\sum_ia_i$ is not robust by definition against outliers. \n\nBy building an estimator of this average operator $\\bar a$, aren't we falling back to the issues the average operator $\\bar a$ was suffering in the first place?  \n\n### Comparison against vanilla GD\n  \nCan you clarify what are the use-cases in which Clip21-GD overcomes **vanilla GD** (and not only Clip-GD) ? What makes your algorithm different from vanilla GD in practice? Experiments are lacking to compare against it.    \n\n### Typo\nFor citations, for example a top of page 5, I recommand to use `\\citep{}` instead of `\\cite{}`, for example in `Richt \u00b4arik et al. (2021); Fatkhullin et al. (2021); Richt \u00b4arik et al. (2022),`.  \n\n> it will become \u201cinactive\u201d in at most k\u22c6 steps (i.e., \u2207fi(xk) \u2212 vik\u22121 \u2264 \u03c4  \n\nMissing `)`."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6746/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6746/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6746/Reviewer_HYZq"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6746/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699022849059,
            "cdate": 1699022849059,
            "tmdate": 1699636776485,
            "mdate": 1699636776485,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ybN8Ettzii",
                "forum": "viC3cpWFTN",
                "replyto": "qdqPqLJCIs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6746/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6746/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HYZq"
                    },
                    "comment": {
                        "value": "> **Relevance to Differential Privacy. Element-wise gradient clipping is typically used DP-SGD (Abadi et al, 2016). DP-SGD uses clipping to control the sensitivity of gradient steps. Then DP-SGD adds a Gaussian noise to the clipped gradient to create a Gaussian mechanism. In Alg 1, the clipped vectors are aggregated, but the influence of a noise $\\xi_i$  on the mean estimate $\\frac{1}{n}\\sum_i v_i$ is not studied, therefore it is not clear if this algorithm would work in the context of differential privacy.**\n\nAlthough DP-Clip-SGD for distributed settings (Abadi et al, 2016) has been studied empirically, there is no theoretical convergence for this method. Even without DP noise, distributed Clip-GD does not converge towards the optimum for quadratic problems in Example 1.1. This motivates us to redesign distributed Clip-GD that leverages the error-feedback framework to ensure convergence. Deriving DP guarantees for a version of Clip21-GD is a prominent research direction for future work.\n\n> **Relevance for exploding gradients. As written in the paper: ``we are not employing clipping as a tool for taming the exploding gradients problem, and our work is fully complementary to this literature. [...] Clipping after averaging does not cause the severe bias and divergence issues we are addressing in our work.\u201d**\n\nWe agree that clipping helps stabilize neural network training which has exploding gradient problems, which can be modeled by the $(L_0,L_1)$ smoothness condition. We believe that our analysis is sufficient to prove the convergence under the $(L_0, L_1)$ smoothness. We prove this by finding the upper-bound for $\\Vert \\nabla^2 f(x_k) \\Vert$ and our results apply as follows: \n\n1. By the $(L_0, L_1)$ smoothness and Assumption 3 of [A1],  \n\n$$\n\\Vert \\nabla^2 f(x_k) \\Vert \\leq L_0 + L_1 \\Vert \\nabla f(x_k) \\Vert \\leq L_0 + L_1 \\sqrt{2L[f(x_k)-f_{\\inf}]}.\n$$\n\n2. We then have $\\Vert \\nabla^2 f(x_k) \\Vert \\leq L_0 + L_1 \\sqrt{2L\\phi_0}$ for all $k \\geq 0$ by proving that $f(x_k) \\leq f_{\\inf}+\\phi_0$ by the induction and by setting the step-size $\\gamma$ sufficiently small. \nWe also refer the reviewer to our general response, where we explain why the current analysis from our paper holds under $(L_0, L_1)$-smoothness.\n\n[A1]: Khaled, Ahmed, and Peter Richt\u00e1rik. \"Better theory for SGD in the nonconvex world.\" arXiv preprint arXiv:2002.03329 (2020).   \n\n\n> **The idea of Algorithm 1 is to estimate the average $\\frac{1}{n}\\sum_{i=1}a_i$ \\textit{exactly}. Clipping may help to circumvent instabilities when some individual gradients $a_i$ are too high (whetever the reason, exploding gradient being an example, numerical inaccuracies another). In this case, biasing the average direction with clipping is the wanted behavior, since we don't want the descent step to be driven by a single example with exceedingly large gradient norm. The average operator $\\bar a := a \\mapsto \\frac{1}{n}\\sum_{i=1}a_i$ is not robust by definition against outliers.** \n\n> **By building an estimator of this average operator $\\bar a$, aren't we falling back to the issues the average operator $\\frac{1}{n}\\sum_{i=1}a_i$ was suffering in the first place?** \n\nWe agree that this naive averaging operator does not ensure the convergence of distributed clipped gradient methods as in Example 1.1. To this end, we show that this naive averaging can be circumvented by leveraging the error-feedback framework to recover the average of full gradients from the average of memory vectors. In particular, from Lemma 4.2 and 4.3, the Clip21 algorithm ensures the following: (1) each memory vector $v_k^i$ approaches $a^i$ at node $i$, and hence (2) we can obtain $v_k := \\frac{1}{n}\\sum_{i} v_i^k$ which approaches $a:= \\frac{1}{n}\\sum_{i} a_i$.  \n\n\n> **The paper positions itself against the Clip-GD algorithm (among others... ), in all experiments. But experiments with comparison against vanilla GD are lacking.**\n\n> **Can you clarify what are the use-cases in which Clip21-GD overcomes vanilla GD (and not only Clip-GD) ? What makes your algorithm different from vanilla GD in practice? Experiments are lacking to compare against it.**\n\nOur theory and experiments suggest that Clip21-GD outperforms the Clip-GD for both deterministic and stochastic gradients since it can handle data heterogeneity and ensures strong convergence guarantees. We believe that Clip21-GD would outperform the naive clipping algorithm, which is faster than vanilla SGD for neural network trainin, e.g., in (Zhang et al.,2020a). \n\n\n> **For citations, for example a top of page 5, I recommand to use `\\citep{}` instead of `\\cite{}`, for example in, `Richt \u00b4arik et al. (2021); Fatkhullin et al. (2021); Richt \u00b4arik et al. (2022),`**\n\n> **it will become \u201cinactive\u201d in at most k\u22c6 steps (i.e., \u2207fi(xk) \u2212 vik\u22121 \u2264 \u03c4. Missing $)$.**\n\nWe thank the reviewer for this recommendation and for spotting this typo. We will correct this in our finalized manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6746/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593480502,
                "cdate": 1700593480502,
                "tmdate": 1700593480502,
                "mdate": 1700593480502,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]