[
    {
        "title": "Mind Your Augmentation: The Key to Decoupling Dense Self-Supervised Learning"
    },
    {
        "review": {
            "id": "KvFU9J8AwO",
            "forum": "WQYHbr36Fo",
            "replyto": "WQYHbr36Fo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission945/Reviewer_VesV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission945/Reviewer_VesV"
            ],
            "content": {
                "summary": {
                    "value": "Submission 945 presents a visual self-supervision method suitable for dense (/local) tasks such as detection/segmentation, etc. It motivates itself by presenting demonstrations that show that attention maps (/point affinities) do not localize well to object-parts and this is due to the false positives generated by current self-supervision methods.\n\nIt presents a variant of CutMix for dense self-supervision that mixes tokens from the input image with tokens from an external image. The mixing strategy is developed such that a token from the input image is largely surrounded by tokens from the output image so that the input token is positionally \u201cout of context\u201d. It then presents a regularizer for any visual self-supervision loss such that features extracted from the \u201cin context\u201d and \u201cout of context\u201d tokens (taken from the input image) are similar."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The presented work is reasonably thorough in its experiments and generality. In particular, I like that it presents a rationale for both ViTs and CNNs.\n- While confusingly presented, the high-level idea of asking token-wise features to be invariant to some of their surrounding tokens is a simple idea and appears to lead to better attention maps and downstream performance."
                },
                "weaknesses": {
                    "value": "(in no particular order)\n\n### Poor presentation\nUnfortunately, the unclear writing and figures significantly dampened any enthusiasm for this paper and it took multiple repeated readings to get a high-level sense of what is proposed. As a few examples,\n- Paragraph 3 of the Introduction only makes sense if you already know the entire method.\n- Figures 1(a) and 2(a) are really hard to understand and are used to motivate the work. For example, what do the red dashed lines, the arrows, and the \u201cViTs-based measuring pipeline\u201d indicate in 2(a)?\n- I don\u2019t understand the written presentation of the mask generation (Sec. 4.1.1, par. 1) at all and its associated algorithm in the appendix uses undefined notation that is hard to follow (if it is defined elsewhere, please also add it to the caption). As a result, figure 4 does not immediately follow either.\n\nIMO the paper requires a significant revision for clarity.\n\n### Engineered token mixing has been done previously:\nThe proposed method has two main contributions: a cutmix style augmentation at the token level and a self-supervised loss leveraging that augmentation. However, while presented as new here, mixing tokens from different images in a carefully engineered way has been done before in TokenMix (ECCV\u201922, https://arxiv.org/abs/2207.08409 ) in the context of supervised classification and some other papers that follow up on it. Please discuss these works and clarify any differences, so as to better contextualize the key novel contribution of the self-supervised loss function that takes advantage of token mixing here.\n\n### Unclear motivation and relationship to current work:\nIn my reading, the main motivation of this work is achieving higher quality part-level attention maps by reducing the dependence between a token\u2019s features and its surroundings. It is then unclear to me if this strategy can then capture long-range nonlocal dependence \u2013 could you please comment on this point and clarify if I misunderstood?\n\nMoreover, in recent work, DinoV2 (https://arxiv.org/abs/2304.07193) demonstrated that high-quality part-level representations can be learned by simply scaling up model and dataset sizes without considering the \u201ccoupling\u201d between objects and background. I am not asking for comparisons, but I would like the response to briefly clarify the motivation of the proposed strategy in this context."
                },
                "questions": {
                    "value": "### Suggestions:\n- Please clarify the differences between TokenMix and similar token mixing works and this paper.\n- Please revise and improve the writing and presentation of the first four sections of this paper to make it more immediately accessible.\n- Please briefly discuss the motivation for the decoupling regularizer in the context of existing methods such as DinoV2 achieving high-quality part-level attention maps without considering the \u201ccoupling\u201d phenomenon.\n- Figure 5 on page 7 is what finally made the method click for me \u2013 please move it to earlier in the paper.\n\n### Minor questions:\n- Experimental clarifications would be beneficial: Why is 800 epochs of pretraining on COCO specifically chosen for all methods? How were the hyperparameters tuned for the baselines? Also, most of the experiments do not mention splits.\n- Apart from the proposed masking, there appears to be no other augmentations mentioned and there is no code. Were standard augmentations such as jitters, flips, blur, etc. (https://github.com/bytedance/ibot/blob/main/main_ibot.py#L574) also used? Did the augmentation strategy match the existing methods on which decoupling was applied?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission945/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission945/Reviewer_VesV",
                        "ICLR.cc/2024/Conference/Submission945/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission945/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698706174775,
            "cdate": 1698706174775,
            "tmdate": 1700721545509,
            "mdate": 1700721545509,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P4p7Q4luAO",
                "forum": "WQYHbr36Fo",
                "replyto": "KvFU9J8AwO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission945/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission945/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VesV"
                    },
                    "comment": {
                        "value": "We sincerely thank Reviewer VesV for acknowledging our contributions. Below are detailed responses to the reviewer's concerns.\n\n>  **Q1: Presentation .**\n\nWe appreciate the reviewer's feedback and acknowledge the concerns about clarity in the writing and figures. Thank you for bringing this to our attention. We rewrote some parts of the paper and also modified Figure 1(a) and Figure 2(a), We put Figure 4 right after the method as suggested by the reviewer, and made corresponding changes to make it easier to understand. \n\n>  **Q2: Differences between our work and current token mixing methods**\n\nThank you for the valuable advice. We will add this to the related work. Although both methods utilize cutmix-style augmentation, our method differs significantly from them in both the mixing pipeline and loss design. \n\nMixing Pipeline: Cutmix, TokenMix, and related works primarily apply rectangular or block-wise masks for mixing tokens, with a focus on image-level labels. In contrast, our Region Collaborative Cutout (RCC) generates background masks tailored for objects of varying scales. The ablation experiments in Section 6.1 demonstrate the superiority of RCC in de-coupling dense-level models compared to other mask types. Additionally, as highlighted in Section 6.3, RCC serves as a novel augmentation strategy beneficial for dense-level SSL pre-training.\n\nLoss Design: While cutmix-style methods in supervised learning aim to enhance the network's ability to capture long-range dependencies with the classification loss, our approach in dense-level SSL addresses a different concern. We empirically demonstrate in our experiments that the shared long-range context between tokens extracted by the student and teacher frameworks can potentially act as a shortcut in a self-supervised manner (without labels). To mitigate this, our de-coupling loss encourages the encoder to focus on capturing discriminative local semantics rather than collapsing to a single dominant long-range representation.\n\n>  **Q3.1: How to capture long-range nonlocal dependency ?**\n\nThank you for the opportunity to clarify the main motivation of our work. Our primary goal is to reduce the dependence between a token's features and its surroundings, aiming to enhance the quality of part-level attention maps. The focus on decoupling dense-level representations is intended to encourage the network to prioritize local semantics over relying heavily on shared long-range context. This strategic approach aims to prevent different semantics from collapsing to the same representation, ensuring that each object remains discriminative. As evidence of the effectiveness of this strategy, we observe that methods incorporating the de-coupling loss outperform the original approach in effectively identifying objects across different locations.\n\n\n>  **Q3.2: Discussion about Dino V2 without \"coupling\"**\n\nThanks for raising this question. Our method bears several differences with and advantages over DinoV2:\n\nDataset Processing: Our motivation centers around training networks without extensive dataset curation, enabling self-supervised learning to be adaptable in various scenarios. Unlike DinoV2, which requires preprocessing the dataset and selecting images with single objects, our method aims to operate in a more versatile manner, not relying on specific dataset characteristics.\n\nLoss Function: DinoV2 employs the iBOT loss function, and we have demonstrated the compatibility of our method and its effectiveness in conjunction with the IBOT loss. This suggests that our approach can serve as a valuable tool to enhance the training of DinoV2 by addressing issues related to object coupling and improving overall understanding.\n\nWe also conducted some visualization for better understanding, detailed comparisons are provided on the last page of the appendix. Despite DinoV2's excellent performance in identifying salient objects, we observed limitations in its ability to distinguish objects or scenes located in the background or less salient regions. The red point in the figure often exhibits high similarities with other regions from different semantic contexts. Increasing the capacity of the backbone network (from ViT-S to ViT-B) did not lead to improvements in DinoV2's performance in these scenarios. By contrast, our method with ViT-S demonstrates a better understanding of local semantics validating our initial intuition. Note that, DINOv2 also captures local semantics, and we are not aiming to compete with DINOv2 but rather proposing another potential avenue for advancing SSL.  \n\nWe hope that these clarifications highlight the distinctions and contributions of our method .\n\n>  **Minor questions: Hyper-parameters, datasets, and augmentations.**\nWe will add those specifications to the paper, and of course, we will release our code very soon.\n\n\nWe value the reviewer's input and will continue refining the content. We will update the paper to integrate the feedback."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570230800,
                "cdate": 1700570230800,
                "tmdate": 1700579062927,
                "mdate": 1700579062927,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EWLT0GTvSG",
                "forum": "WQYHbr36Fo",
                "replyto": "P4p7Q4luAO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission945/Reviewer_VesV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission945/Reviewer_VesV"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your revision and the rebuttal. I will be maintaining my overall borderline accept rating, but raising the presentation score. Some minor assorted points:\n\n  - I agree that the text and figures are now clearer in parts. However, I still have some difficulty with key components. For example, the RCC masking procedure is hard to decipher from the description alone. I suggest that the authors eventually add an iterative figure somewhere in the paper that adds components step-by-step as this is the core contribution.\n\n  - I appreciate that there are new point affinity qualitative comparisons with DinoV2 (fig 18) where the proposed method achieves more interpretable maps. However, I do not think that it is fair to claim that the requisite data curation strategies for DinoV2 are hard to implement for new datasets. It only involves automated and scalable duplicate and near-duplicate removal (appendix A in their paper) which is straightforward to implement on new large datasets.\n\n  - (super minor point) On the note of DinoV2, in the final paper, it would be beneficial to briefly discuss [concurrent work](https://openreview.net/forum?id=2dnO3LLiJ1) that demonstrates a completely different approach to achieve interpretable attention maps without considering \u201ccoupling\u201d. I am not at all suggesting a comparison or an in-depth discussion as this is a concurrent submission, but future readers may benefit from contrasting two approaches to the same goal in the final related work section."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721518116,
                "cdate": 1700721518116,
                "tmdate": 1700721518116,
                "mdate": 1700721518116,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "50Dx1WjK2T",
            "forum": "WQYHbr36Fo",
            "replyto": "WQYHbr36Fo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission945/Reviewer_GFda"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission945/Reviewer_GFda"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new method called Region Collaborative Cutout for self-supervised learning to alleviate the object coupling issue. This simple and straightforward method achieves evident gains over previous methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is clearly written, and it is easy to catch the main motivation and solution. And I think the proposed method is well motivated.\n\n2. Multiple previous methods are used as baselines to build the proposed Region Collaborative Cutout upon, and non-trivial improvements are observed. Besides, the method is proved effective for both CNNs and ViTs.\n\n3. The ablation studies are comprehensive and convincing."
                },
                "weaknesses": {
                    "value": "I am not very familiar with the research line of SSL. So I have no further suggestions for this paper. Generally, I like this simple yet effective method. It further highlights that constructing appropriate positive pairs by delicately designed strong augmentations is important.\n\nHowever, one of my slight concerns is about the whole area of SSL since DINOv2 was released. It is pre-trained on extremely large-scale and curated data with several practical SSL optimization targets. It is very strong in many applications, such as retrieval, segmentation, and detection. Even with a frozen DINOv2 backbone, we can achieve state-of-the-art performance in some challenging tasks. Therefore, could the authors discuss the position of this submission by taking the recent SSL trend into consideration?"
                },
                "questions": {
                    "value": "No further questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission945/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698904381578,
            "cdate": 1698904381578,
            "tmdate": 1699636021127,
            "mdate": 1699636021127,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SMKgDMXALv",
                "forum": "WQYHbr36Fo",
                "replyto": "50Dx1WjK2T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission945/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission945/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GFda"
                    },
                    "comment": {
                        "value": ">  **DINOv2 Discussion**\n\nWe appreciate the reviewer's constructive suggestion about DINOv2. Our method bears several differences with and advantages over DINOv2:\n\nDataset Processing: Our motivation centers around training networks without extensive dataset curation, enabling self-supervised learning to be adaptable in various scenarios. Unlike DinoV2, which requires preprocessing the dataset and selecting images with single objects, our method aims to operate in a more versatile manner, not relying on specific dataset characteristics.\n\nLoss Function: DinoV2 employs the iBOT loss function, and we have demonstrated the compatibility of our method and its effectiveness in conjunction with the IBOT loss. This suggests that our approach can serve as a valuable tool to enhance the training of DinoV2 by addressing issues related to object coupling and improving overall understanding.\n\nExperimental Observations: In our experiments, detailed comparisons are provided in the last page of the appendix. Despite DinoV2's excellent performance in identifying salient objects, we observed limitations in its ability to distinguish objects or scenes located in the background or less salient regions. The red point in the figure often exhibits high correlation with other regions from different semantic contexts. Increasing the capacity of the backbone network (from ViT-S to ViT-B) did not lead to improvements in DinoV2's performance in these scenarios. By contrast, our method with ViT-S demonstrates a better understanding of the entire scene, validating our initial intuition."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568911530,
                "cdate": 1700568911530,
                "tmdate": 1700568911530,
                "mdate": 1700568911530,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wy7rsDybzH",
                "forum": "WQYHbr36Fo",
                "replyto": "SMKgDMXALv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission945/Reviewer_GFda"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission945/Reviewer_GFda"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the feedback. Could the author provide more clarification on why DINOv2 is not good at distinguishing less salient regions? DINOv2 also considers the local features."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571691190,
                "cdate": 1700571691190,
                "tmdate": 1700571691190,
                "mdate": 1700571691190,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6HqZRW0RXe",
                "forum": "WQYHbr36Fo",
                "replyto": "jkxDWtHG5I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission945/Reviewer_GFda"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission945/Reviewer_GFda"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the response and clarification. I appreciate the contributions of this paper. Since my original score has already been high enough, I will keep the original score of Accept (8)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624264194,
                "cdate": 1700624264194,
                "tmdate": 1700624264194,
                "mdate": 1700624264194,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2K5IeP2wz1",
            "forum": "WQYHbr36Fo",
            "replyto": "WQYHbr36Fo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission945/Reviewer_wwZs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission945/Reviewer_wwZs"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of information leakage from the neighboring contextual regions in dense self-supervised learning. The contributions of this paper include the identification and confirmation of the coupling phenomenon when pairs have a limited overlap, the design of a decoupling branch, a novel region collaborative cutout augmentation, and the effectiveness of the proposed approach in the dense self-supervised learning frameworks. This approach can be applied to both CNNs and ViTs as the approach is only related to the augmentation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is well written and easy to follow.\n\nThe proposed method is simple and can be directly combined with existing dense self-supervised learning without introducing additional loss types.\n\nThe proposed method, RCC and decoupling branch, are demonstrated to be effective with several existing self-supervised learning methods in the experiments."
                },
                "weaknesses": {
                    "value": "While the augmentation in Fig. 5 is easy to understand, the random masking in Fig. 2(a) is vague in terms of the illustration purpose.\n\nThe main text is expected to be self-contained. However, there are several cases where content is put in the appendix. For example, the figure 7(a) is useful to understand the definition of variables in equation 4, however, is put in the appendix. One possible way to address this problem is to merge Fig. 7(a) with part of Fig. 3. Again, the Alg 2 is another example for the understanding of the proposed RCC (region collaborative cutout).\n\nThe original contribution of the approach may be limited as the proposed RCC could be viewed as the combination of thresholding the cutout ratio within a region and filling the cutout region with background images. This is different from the Cutout, but more like combining existing strategies.\n\nThe order of Tables 4-6 does not match the appearance in the text."
                },
                "questions": {
                    "value": "Context within the region. The shape of each object varies and is irregular. And the region is defined by a bounding box (at least in object detection task), There would be context information in the bounding box. How to measure or address the coupling or leakage for this part of information?\n\nThis method would introduce another hyperparameter, the threshold of cutout ratio. How to set this across different datasets?\n\nAblation study. The numbers about COCO Det. in Table 6 do not match the numbers reported in Table 2 for iBOT. This is because the training epochs are different. If we view the results in Table 2 as the converged one, the comparison made in Table 6 may not lead to a convincing conclusion. While three experiments are presented in this section, a more interesting ablation study would be the effectiveness of the decoupling branch. \n\nWhile the authors claim that the proposed method can be combined with existing SSL methods, there is a concern that the proposed method may not work well for the method with contrastive loss within each batch. The reason for this is that the decoupling branch serves a similar purpose. However, there may be additional contribution as the losses are computed at different levels with different masks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission945/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698964079309,
            "cdate": 1698964079309,
            "tmdate": 1699636021055,
            "mdate": 1699636021055,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BMxEEcoClL",
                "forum": "WQYHbr36Fo",
                "replyto": "2K5IeP2wz1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission945/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission945/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wwZs - 1"
                    },
                    "comment": {
                        "value": "We sincerely thank Reviewer wwZs for acknowledging our contributions. Below are detailed responses to the reviewer's questions:\n\n>  **W1,W2,W4: Presentation.**\n\n1Thanks for pointing out the unclear parts and for the constructive suggestions to improve the readability of our paper. Please check our updated version and let us know if we can still improve it.\na. Figure 2: we have re-designed the figure based on your suggestion and updated it in the revised version. \nb. Figure 7 and Alg. 2. We have added Figure 7 in the context. We cannot find space for Alg. 2, but we have revised Section 4.1.1 to make it clearer.\nc. We have moved the table to their corresponding text. \n\n>  **W3: RCC is developed by combining existing strategies**\n\n\nWe appreciate the reviewer's acknowledgment of the simplicity of our method. However, it is essential to note that our design is not merely a straightforward combination of existing strategies; rather, it involves a thoughtful and nontrivial adaptation to the unique challenges posed by dense SSL.\nThe original Cutout algorithm is designed for CNN-based image-level augmentation methods; our RCC algorithm is specifically crafted to address the challenges of dense SSL and is applicable to both CNNs and Vision Transformers (ViTs). The essence of RCC lies in its grid-based bounding box generation, which introduces a collaborative cutout strategy to enhance feature robustness and mitigate the issues associated with coupling. Besides, Cutout aims to incorporate the background region, whereas RCC is used for storing the foreground.\n\nIn our experiments, we conducted a thorough investigation of the compatibility of the original Cutout technique with our bounding box generation pipeline in the context of dense-level models. The combination did not yield favorable results and, in fact, led to performance degradation. We discussed this finding in Section 6.1 and provided detailed insights in Appendix G.1. Specifically, the experiments involving iBOT+Cutout and DenseCL+Cutout demonstrate degraded performance, with iBOT+Cutout ranking as the second-worst performer and DenseCL+Cutout even underperforming the original DenseCL. These results emphasize that addressing the issue of overcut is a critical factor in ensuring the effective collaboration of Cutout with dense-level SSL.\n\n>  **Q1: How to deal with objects with irregular shapes ?**\n\nThanks for your question. This is one of the nice properties of RCC as it generates region-level bounding boxes at multiple scales and aspect ratios, allowing for a diverse scale of de-coupling. Consequently, the tokens representing objects of different sizes and shapes will be effectively encompassed by background image tokens, enabling de-coupling from irrelevant context at a finer grain. The intra-object coupling ratio in Sec 3.2 can measure the coupling issues in this context as it directly applies object segmentation masks instead of bounding boxes. The results in Fig. 3 and Section 6 show that de-coupling with RCC achieves the lowest CR-intra.\n\n>  **Q2: Cutout ratio in different Datasets.**\n\nThe cutout ratio controls the balance between foreground regions for prediction and background regions for de-coupling. Throughout our extensive experiments, we have observed that this hyperparameter remains stable across different datasets. Its stability indicates that the proposed cutout ratio is robust and effective in achieving a favorable balance for both prediction and de-coupling, making it applicable to various datasets without the need for dataset-specific tuning. To provide further insight into the robustness of our approach, we conducted additional pre-training experiments with iBOT-D using different mask ratios on ImageNet-100[1] and OHMS (a subset of OpenImage) [2] for 200 epochs. The results consistently demonstrate the effectiveness of our de-coupling strategy across diverse datasets, reinforcing the notion that the cutout ratio parameter is a stable and generalizable setting.\n\n##### 1. Pre-trained on ImageNet-100\n| **Model Name**|**Mask Ratio**|**COCO Det. AP**|\n| ----- | :-----: | :-----: |\n|iBOT|-|38.3|\n|iBOT-D|[0.2,0.4]|40.5|\n|iBOT-D |[0.3,0.5]|40.7|\n|iBOT-D |[0.4,0.6]|40.4|\n\n##### 2. Pre-trained on OHMS\n| **Model Name**|**Mask Ratio**|**COCO Det. AP**|\n| ----- | :-----: | :-----: |\n|iBOT|-|41.2|\n|iBOT-D|[0.2,0.4]|43.8|\n|iBOT-D |[0.3,0.5]|44.2|\n|iBOT-D |[0.4,0.6]|44.0|"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568533968,
                "cdate": 1700568533968,
                "tmdate": 1700570615643,
                "mdate": 1700570615643,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cntH6Qpfic",
                "forum": "WQYHbr36Fo",
                "replyto": "2K5IeP2wz1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission945/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission945/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wwZs -2"
                    },
                    "comment": {
                        "value": ">  **Q3-a: Ablation study in Tab. 6 about running epochs .**\n\nWe acknowledge your concern regarding the duration of pre-training, and we appreciate the valuable insight. In our experiments, we observed consistent performance trends between 200-epoch and 800-epoch models when pre-trained with the same backbone and loss. However, due to limitations in computation, we primarily report the results of models pre-trained for 200 epochs in our study.\nTo address the concern raised by the reviewer, we conducted additional experiments, pre-training iBOT-D with Blockwise Mask, the second-best model according to Table 6 (Table 4 in the updated version), for 800 epochs. The results clearly demonstrate that, even with an extended training schedule, our Region Collaborative Cutout (RCC) strategy continues to outperform the other mask strategies, supporting the robustness and effectiveness of our approach over longer pre-training durations. If the reviewer considers it necessary, we will continue running experiments for other ablation studies with 800 epochs.\n\n| **Model Name**|**COCO Det. AP**|\n| ----- | :-----: |\n|iBOT-D (RCC)| 45.1|\n|iBOT-D (Blockwise)| 44.0|\n\n>  **Q4: Compatibility of our method with methods employing contrastive learning.**\n\nDenseCL indeed employs contrastive learning, and our proposed method still integrates effectively within such a framework. In our approach, each decoupled view samples only one image as the background, and the decoupling loss is applied exclusively to the foreground patches, minimizing the impact of intra-batch contrast. To address the reviewer's concerns, we performed additional experiments where we randomly sampled the background image from the entire dataset outside the batch at each iteration. The results from these experiments, with models pre-trained on COCO for 200 epochs, demonstrate that the choice of sampling background images within or outside the batch has a negligible impact on the model's performance.\n\n| **Model Name**|**COCO Det. AP**|\n| ----- | :-----: |\n|iBOT-D (Within-Batch)| 42.0 |\n|iBOT-D (Out-of-Batch)| 42.2 |\n\n\nWe appreciate the insightful questions and feedback. We plan to incorporate the provided suggestions into our paper for improvement. If you have any additional inquiries or require further clarification, please feel free to reach out. Thank you for your valuable input.\n\n\n__References__\n\n[1] Tian, Yonglong, Dilip Krishnan, and Phillip Isola. \"Contrastive multiview coding.\" Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XI 16. Springer International Publishing, 2020.\n\n[2] Mishra, Shlok, et al. \"Object-aware cropping for self-supervised learning.\" arXiv preprint arXiv:2112.00319 (2021)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568645432,
                "cdate": 1700568645432,
                "tmdate": 1700570589760,
                "mdate": 1700570589760,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wdx5yd3cCW",
                "forum": "WQYHbr36Fo",
                "replyto": "BMxEEcoClL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission945/Reviewer_wwZs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission945/Reviewer_wwZs"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the response.\n\nI want to follow up the Q1. Although multiple bounding boxes at multiple scales can be generated, to my understanding, the proposed method will only select only one out of them. That cause to the leakage of context information. Please correct me if I am wrong."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580723658,
                "cdate": 1700580723658,
                "tmdate": 1700580723658,
                "mdate": 1700580723658,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YU42IuSud1",
                "forum": "WQYHbr36Fo",
                "replyto": "2K5IeP2wz1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission945/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission945/Authors"
                ],
                "content": {
                    "title": {
                        "value": "response to Reviewer wwZs"
                    },
                    "comment": {
                        "value": "Thank you for your prompt response. Our RCC generates cutout masks for each bounding box, and each image contains N^2 bounding boxes, i.e. N^2 cutout regions in total. Note that the size and position of bounding boxes in each image differ, thus the masks generated by RCC also vary. Let me clarify the intuition behind our RCC. Existing augmentations, such as cutout, are all image-level. Once the cutout region is large enough to cause incorrect alignment between the teacher and student encoder, the feature of the nearby region will be pushed towards the masked region, causing coupling. Therefore, as one of our main modifications, we iteratively restore the overcut regions in each bounding box and perform the cutout. Our primary goal is to achieve effective augmentation at a dense level without causing inaccurate alignment. We present some examples of the RCC masks in Fig.12, and the pseudo code is available in Alg. 2 in our paper. Please feel free to provide any feedback."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission945/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585925664,
                "cdate": 1700585925664,
                "tmdate": 1700648338223,
                "mdate": 1700648338223,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]