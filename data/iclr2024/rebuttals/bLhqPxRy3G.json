[
    {
        "title": "Linear programming using diagonal linear networks"
    },
    {
        "review": {
            "id": "8tHtYPW4Ez",
            "forum": "bLhqPxRy3G",
            "replyto": "bLhqPxRy3G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8323/Reviewer_9DAk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8323/Reviewer_9DAk"
            ],
            "content": {
                "summary": {
                    "value": "This paper shows that gradient descent on a diagonal linear network (with a quadratic parametrization) under specific initialization leads to an approximate solution to the entropy-regularized solution to a linear programming problem. The convergence results are presented for both gradient flow and gradient descent algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper presents a new idea that solves LPs via training a diagonal linear network, which exploits the implicit bias of diagonal networks.\n2. linear convergence results are shown for training diagonal linear networks with GD, which is different from what has been established, for example, the results in Va\u0161kevi\u010dius et. al. 2019."
                },
                "weaknesses": {
                    "value": "The main weaknesses are the presentation and the significance of the results, mainly for Theorem 3.4 and 3.5.\n1. There needs more discussion on the upper bound $\\bar{\\eta}$ on the step size, and the linear rate $\\rho$ in Theorem 3.4, specifically their dependence on 1) the underlying LP problem $(A,b,c)$, and its scale (# of decision variables, #of constraints, etc.); 2) the initialization $u_0$. For example, if either $A$ is ill-conditioned, or the initialization is close to the origin, then I believe $\\rho$ should be close to one. Merely showing that GD converges linearly does not make a significant contribution if what authors propose is to implement this GD algorithm for solving real LP problems. \n2. Theorem 3.5 only shows that the GD converges to some $x^\\infty$ that is close to the desired solution to the LP, but the result is weak in the sense that it doesn't suggest an upper bound on the # of GD iterations for achieving certain accuracy. Specifically, the convergence result one expects is that given some $\\epsilon>0$, the GD with some step size $\\eta(\\epsilon)$ takes $T(\\epsilon)$ iterations to achieve either 1) $\\|x^T-x^*\\|\\leq \\epsilon$, where $x^*$ is the true optimal solution; 2) or the optimality gap is less than $\\epsilon$. \n3. Another concern I have is that I don't find, from the discussions and experiments in this paper, any evidence that the proposed algorithm has advantages in solving certain LPs, compared to existing methods."
                },
                "questions": {
                    "value": "See \"weaknesses\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8323/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8323/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8323/Reviewer_9DAk"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8323/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698604235752,
            "cdate": 1698604235752,
            "tmdate": 1700591269954,
            "mdate": 1700591269954,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QlIgwxF7x3",
                "forum": "bLhqPxRy3G",
                "replyto": "8tHtYPW4Ez",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8323/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8323/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (Thanks for the review and incisive questions. We truly appreciate your comments.)"
                    },
                    "comment": {
                        "value": "$\\textbf{Response on the comments on stepsize and linear rate:}$ The upper bound on $\\eta$ was explicitly written down in equation (6.1) in the appendix\n\t\t$$\\eta := \\min_{ u\\in  B_R(0) }  \\Big(   \\min \\big(  \\frac{1}{4\\text{Res}}, \\frac{1}{5L|u|_{\\infty}}  \\big)  \\Big) $$ where $\\text{Res}:= |A^\\top( A(u\\circ u)-b )|_2$ and the value of $R$ can be large in the worst case (leading to a conservative constant for the convergence rate), it can be much better in cases where the matrix $A$ satisfies some regularity properties. For example, if all the entries of $A$ are i.i.d. Gaussian, then one can show that with high probability, $R$ has a polynomial dependence on the problem sizes. \n\nThe dependence of $\\rho$ on $A$, $b$ and the initial point can be derived from the proof of Theorem 3.4. This dependence is hard to be written in a clean way, as we have used few constants that can be shown to exist but cannot be written in a closed form (see Appendix H.1). Furthermore, since our problem is non-convex in nature, it makes it lot more harder to delineate exact expression of the linear rate. However, we agree that if $A$ is ill-conditioned or the initial point is close to $0$, the value of $\\rho$ is close to $1$.\n\nFinally, we note that conservative constants (e.g. Hoffman constant) have been commonly used in deriving the linear convergence of first order methods for solving LP problems.  Although such constants may not reflect the actual performance of the algorithm, it provides some hint on the convergence properties of the algorithm. Some of the impactful papers from the past with similar problem includes [1] and [2]. \n\n$\\textbf{Response on the dependence of stepsize and running time on optimality gap:}$ This is an excellent question. We definitely like to include more details in the Appendix. To emphasize how the stepsize depends on the gap, let us define \n  $$  C_1 := \\sup_{ x \\in (x^*) \\cup (x: \\|x\\|_2 \\leq R) } \\Big| \\sum^n x_i \\log(x_i) - x_i \\Big|, \\quad   C_2 := \\frac{1+ c}{2} \\max ( R, \\| x^*\\|_2 )\n  $$\nwhere $x^*$ is the solution of the LP problem and $R$ is the same constant as in the response of the previous comment. Now a simple but tedious computation would show that if the regularization $\\lambda(\\epsilon)$ and stepsize $\\eta(\\epsilon)$ is chosen in way so that $2\\lambda(\\epsilon) C_1 + 2\\eta(\\epsilon) C_2 <\\epsilon$, then the optimality gap after large enough time $T$ will be less than $\\epsilon$.\n\nThe a number of iterations $T(\\epsilon)$ to reach an $\\epsilon$-optimality gap is more subtle. \nSince our analysis here is significantly different than usual first order methods, it is hard to write a closed-form expression of $T(\\epsilon)$. This is partly because our results shows global convergence rate of GD for a non-convex problem. Unlike the analysis of a convex problem, the typical per-iteration analysis is not working here. We have to define some global constants that can be shown to exist but cannot be written in a closed form (see Appendix H.1). The dependence on $\\epsilon$ is therefore encoded in a sophisticated way.\n\n$\\textbf{Response on comparison with other commercial LP solver:}$  We have run a new experiment comparing our method against \n the commercial LP solver Gurobi and another first-order method: an ADMM-based solver SCS. In particular, we simulate the data with the same data-generating process as in Section 4 of the paper, but with $m = 500$ and $n = 50000$. Here is a brief comparison of the methods in this example: Gurobi, SCS and our algorithm achieve respectively 0, -0.971,0.005 primal gaps and 0, 2.21e-10, 4.17e-11 feasibility gaps in 16.77, 40.49 and 9.26 seconds. \n\nHere the primal gap is defined as $(c^\\top \\hat x - c^\\top x^*) / c^\\top x^*$, with $\\hat x$ be the solution by an algorithm and $x^*$ is the optimal solution (by Gurobi). The feasibility gap is defined as $\\| A \\hat x - b \\|_2^2 / \\max\\{1, \\| b\\|_2^2 \\} $. \nFirst, it can be seen that our method is much faster than SCS, \nwhere our method computes a solution with a smaller primal gap in a much shorter time. \nOur method is also faster than Gurobi if a low-accuracy solution with a primal gap $0.005$ is satisfactory for the underlying application. \nMoreover, we note that Gurobi takes $10.11$ seconds for preprocessing, which is already similar to the runtime of our method. \n\nAdmittedly, our method is not able to obtain high-accuracy solutions as fast as Gurobi, but it is useful for quickly obtaining an approximate solution. Moreover, since Algorithm 1 only involves matrix-vector multiplications, it is easily implemented in a GPU-acceleration setting, which can potentially outperform Gurobi for larger problems.\n\n[1] Hong, M., Luo, Z.-Q. On the,  linear convergence of the alternating direction method of multipliers. (2019) Mathematical Programming. \n\n[2] Luo, Z.-Q., Tseng, P. On the linear convergence of descent methods for convex essentially smooth minimization (1992) SIAM Journal on Control and Optimization"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8323/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700332464238,
                "cdate": 1700332464238,
                "tmdate": 1700466826212,
                "mdate": 1700466826212,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iZn9g5oTNZ",
            "forum": "bLhqPxRy3G",
            "replyto": "bLhqPxRy3G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8323/Reviewer_p2u6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8323/Reviewer_p2u6"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript studies the implicit bias of re-parametrized gradient descent in which the macroscopic learning rates are used. By leveraging the characterization of the implicit bias and the convex geometry of a linear program, they prove the linear convergence of GD on the linear regression problem under the quadratic parametrization. Importantly, they make minimal assumptions to prove their results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The extension of the previous results [1,2] on reparametrized gradient descent/flow under minimal assumptions on data and macroscopic choice of learning rate is a nontrivial result. The analysis under this generality introduces additional complexity, but this work successfully establishes (linear) convergence for this setting.\n\n[1] Woodworth, B.E., Gunasekar, S., Lee, J., Moroshko, E., Savarese, P.H., Golan, I., Soudry, D., & Srebro, N. (2019). Kernel and Rich Regimes in Overparametrized Models. ArXiv, abs/2002.09277.\n\n[2] Even, M., Pesme, S., Gunasekar, S., & Flammarion, N. (2023). (S)GD over Diagonal Linear Networks: Implicit Regularisation, Large Stepsizes and Edge of Stability. ArXiv, abs/2302.08982."
                },
                "weaknesses": {
                    "value": "The presentation can be further improved in several ways to enhance clarity and readability:\n*  Firstly, I could not follow how the similarity between Algorithm 1 and the Sinkhorn algorithm is used in the paper. \n* Additionally, to the best of my knowledge, the result in [1] proves a similar result in the manuscript in a fairly general setting too. I think it would be helpful for the readers if the authors could elaborate on these points more in their revised manuscript.\n\nAnother aspect that requires attention is the lack of characterization of the effect of using large step sizes in the paper. In particular, [1] studies the effect of large step size in the same setting on the generalization; however, this study does not provide insight about the consequence of using large step size.\n\n[1] Even, M., Pesme, S., Gunasekar, S., & Flammarion, N. (2023). (S)GD over Diagonal Linear Networks: Implicit Regularisation, Large Stepsizes and Edge of Stability. ArXiv, abs/2302.08982."
                },
                "questions": {
                    "value": "In the experiments, the authors show that for the large step size cases, re-parametrized GD converges faster than the exponentiated gradient algorithm (or mirror descent with the entropy potential). Can they comment on why this is the case and if their results imply anything in that direction?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8323/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8323/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8323/Reviewer_p2u6"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8323/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790601847,
            "cdate": 1698790601847,
            "tmdate": 1699637035392,
            "mdate": 1699637035392,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Nh5WcOpwJB",
                "forum": "bLhqPxRy3G",
                "replyto": "iZn9g5oTNZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8323/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8323/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (Thanks for the review and incisive questions. We truly appreciate your comments.)"
                    },
                    "comment": {
                        "value": "$\\textbf{Response on the comparison with the Sinkhorn algorithm:}$ Certainly, this is an intriguing question. We acknowledge that the elucidation of the comparison between our algorithm and the Sinkhorn algorithm might not be entirely clear. Our intention was to delineate the similarities and distinctions between these two algorithms by scrutinizing the update rules in each iteration. For instance,   we demonstrated in Section 2.3 that our algorithm behaves as row and column rescaling algorithms just like as the Sinkhorn algorithm.\n\nSimilar to the Sinkhorn, our neural reparametrized gradient descent initialize the coupling matrix of OT at  $X^{(0)} := K$ where $K_{ij} := e^{- C_{ij}/\\lambda }$. When the stepsize is small, like as in Sinkhorn, the gradient descent rescales the rows and columns of the coupling matrix. In other words, as the step size gets smaller, gradient descent iterates look like\n  $$X^{(k+1)} \\approx D(1_n - g^{(k)}) X^{(k)} D(1_m - h^{(k)})  $$ where $g^{(k)}$ and $h^{(k)}$ are respectively the vectors of residuals of the row sums and column sums of the coupling matrix $X^{(k)}$.\n\tHence the updates above can also be viewed as row and column rescalings, although with different rescaling rules than the Sinkhorn.  Given the parallels between the Sinkhorn algorithm and the updates in our gradient descent, it's reasonable to assert that our approach introduces a Sinkhorn-like algorithm for solving linear programming problems within the context of diagonal linear networks. As far as our knowledge extends, this novel application has not been documented in prior works.\n\n$\\textbf{Resnpose on the comparison with other works on DLN:}$  Although our work bears certain similarities to the references [1] and [2] cited by the reviewer, the primary distinction lies in the diverse approaches to reparametrization. In [1] and [2], the authors have used the reparametrization $\\beta= u\\odot u - v\\odot v$ in the basis pursuit problem while we use a slightly different reparametrization which leads to different final representations of the limit of the gradient descent in our case. However, in both cases, the limit of the gradient descent solve regularized $L_1$-norm minimization problem of $\\beta$. But the nature of regularization is different in our work compared to [1] and [2]. Moreover, at the time of submitting our work, the rate of convergence result for gradient descent, as presented in [1] or [2], was not available. To the best of our knowledge, this result was initially introduced in our work. In the following, we discuss the effect of stepsize in our gradient descent in the light of [2] and [3]. \n\n$\\textbf{Response on the effect of stepsize:}$  As the reviewer rightly mentioned, the stepsize play a crucial role in shaping the quality of the algorithm. Theoretical thresholds for the step size necessary for global linear convergence are detailed in Theorem 3.5. However, recent investigations, exemplified in the reference [2] pointed out by the reviewer and the recently published paper in arxiv [3], scrutinize the impact of an increasing step size. These studies, especially the latter one, reveals that, as the step size amplifies, gradient descent in quadratic regression traverses distinct phases. Beyond the theoretical bounds, the loss function oscillates around a diminishing trend, signifying the emergence of the 'catapult' phase. Notably, during the catapult phase, the application of specific ergodic averaging methods has been demonstrated to enhance generalization error. Further escalation of the step size leads to the onset of the periodic phase, marked by periodic oscillations in the loss function without a discernible downward trend. Subsequent to the periodic phase, the system transitions into the chaotic phase, characterized by a lack of convergence signals, ultimately culminating in divergence. Given that our gradient descent iterates mimic those of the least square quadratic regression, we anticipate that the insights from [3] will be applicable to our framework. We would be pleased to incorporate a comprehensive discussion of this in the revised version.\n\n[3] Chen, X., Balasubramanian, K., Ghosal, P., Agrawalla, B. (2023). From Stability to Chaos: Analyzing Gradient Descent Dynamics in Quadratic Regression. ArXiV:  2310.01687\n\n$\\textbf{Response on the faster rate compared to mirror gradient descent:}$ That is an excellent question. We posit that our algorithm outpaces mirror descent due to the tendency of mirror descent updates to overshoot frequently, resulting in unnecessary excursions near the limit for extended periods. Nevertheless, it remains plausible that a judicious selection of the step size in mirror descent, akin to our algorithm, could potentially enhance its speed."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8323/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323943674,
                "cdate": 1700323943674,
                "tmdate": 1700466804112,
                "mdate": 1700466804112,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nr5b21W0mV",
            "forum": "bLhqPxRy3G",
            "replyto": "bLhqPxRy3G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8323/Reviewer_xDbj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8323/Reviewer_xDbj"
            ],
            "content": {
                "summary": {
                    "value": "The authors prove linear convergence rates for the discrete and continuous versions of gradient on diagonal linear networks. In addition, they show that the continuous and discrete versions converge toward the solution of an entropically regularized linear problem."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The theoretical results are new and elegant, I am not aware of previous results connecting linear programming and diagonal linear networks."
                },
                "weaknesses": {
                    "value": "- Section 2.2 and 2.3 are a little bit scientifically \"loose\", it draws some connections with other methods, but I am not sure there are proper theoretical results that can be extracted from these parts\n\n- Experiments. I understand this is a theoretical paper, but I think the paper would have more impact with more extensive experiments. The current experimental section illustrates the linear convergence of the algorithm and has one comparison to the mirror descent. Maybe the authors could provide an experiment with optimal transport and compare the proposed method to the standard Sinkhorn algorithm"
                },
                "questions": {
                    "value": "- In the experiment section, the authors mention that the theoretical step size found is too conservative. This conservative step size problem can usually be overcome with coordinate descent-like methods, that use a larger coordinate-specific step size. In addition, the authors mention some previous results on coordinate descent for diagonal linear networks. I was wondering if it is possible to extend the theoretical results of the authors to coordinate descent."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8323/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698935250121,
            "cdate": 1698935250121,
            "tmdate": 1699637035284,
            "mdate": 1699637035284,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lyXZBoGWRp",
                "forum": "bLhqPxRy3G",
                "replyto": "nr5b21W0mV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8323/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8323/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (Thanks for the review and incisive questions. We truly appreciate your comments.)"
                    },
                    "comment": {
                        "value": "$\\textbf{Response on the use of co-ordinate descent:}$ We are very grateful to you evaluation. This is indeed a great question.\n     Certainly, the selection of the step-size is a subject that merits deeper exploration, and we intend to delve into this aspect, drawing insights from recent studies elucidating the impact of step size on gradient descent. Exploring unique step-sizes for coordinate-wise descent presents a fascinating avenue for investigation. We are optimistic about the potential acceleration of our algorithm through coordinate descent; nevertheless, the analysis of its convergence and rate appears to pose distinctive challenges compared to our current approach.\n\n$\\textbf{Response on the experimental comparison with the Sinkhorn algorithm and other LP solver:}$ Thank you for the question! Actually we compared our method with standard Sinkhorn, but unfortunately, we found that Sinkhorn is better for OT -- it more directly uses the structure of the problem. Nonetheless, we found that our algorithm can be useful for general LPs where Sinkhorn method is not applicable. For instance, we have performed a new experiment, comparing our method with Gurobi and another ADMM-based method.  In particular, we simulate the data with the same data-generating process as in Section 4 of the paper, but with with $m = 500$ and $n = 50000$. Here is a brief comparison of the methods in this example: Gurobi, SCS and our algorithm achieve respectively 0, -0.971,0.005 primal gaps and 0, 2.21e-10, 4.17e-11 feasibility gaps in 16.77, 40.49 and 9.26 seconds.\n\nHere the primal gap is defined as $(c^\\top \\hat x - c^\\top x^*) / c^\\top x^*$, with $\\hat x$ be the solution by an algorithm and $x^*$ is the optimal solution (by Gurobi). The feasibility gap is defined as $\\| A \\hat x - b \\|_2^2 / \\max\\{1, \\| b\\|_2^2 \\} $. \nFirst, it can be seen that our method is much faster than SCS, \nwhere our method computes a solution with a smaller primal gap in a much shorter time. \nOur method is also faster than Gurobi if a low-accuracy solution with a primal gap $0.005$ is satisfactory for the underlying application. \nMoreover, we note that Gurobi takes $10.11$ seconds for preprocessing, which is already similar to the runtime of our method. \n\nAdmittedly, our method is not able to obtain high-accuracy solutions as fast as Gurobi, but it is useful for quickly obtaining an approximate solution. Moreover, since Algorithm 1 only involves matrix-vector multiplications, it is easily implemented in a GPU-acceleration setting, which can potentially outperform Gurobi for larger problems."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8323/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292501926,
                "cdate": 1700292501926,
                "tmdate": 1700466782440,
                "mdate": 1700466782440,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yREmahpdMV",
            "forum": "bLhqPxRy3G",
            "replyto": "bLhqPxRy3G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8323/Reviewer_FLD4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8323/Reviewer_FLD4"
            ],
            "content": {
                "summary": {
                    "value": "This work looks into the dynamics of gradient descent (or GD) optimization, specifically focusing on the reparameterized GD for linear programmings. By reparameterizing the problem, the author(s) claim to provide a clearer understanding of the implicit bias of GD, particularly how it induces sparsity in solutions. The paper's theoretical contributions demonstrate that this reparameterized GD converges to zero-loss solutions and biases the flow toward solutions with better sparsity properties than conventional vanilla GD. The author support their claims with mathematical proofs and experiments that compare the performance of reparameterized GD with traditional GD and mirror-descent methods, suggesting advantages in terms of sparsity and convergence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This study's approach, reparameterizing GD for linear programs, offering a novel perspective on the implicit bias of GD. The quality is demonstrated through rigorous mathematical analyses, which include bounding the iterates of the algorithm and characterizing the limit points of the convergence. \n\nClarity is another strength, with the paper presenting its methodology and findings in a structured and understandable manner."
                },
                "weaknesses": {
                    "value": "- a notable weakness is the limited scope of the experimental setup; their simulation relies on isotropic Gaussian features, which may not be representative of real datasets that often contain features with varying scales and correlations. Moreover, the paper does not discuss the impact of non-Gaussian noise or different initialization schemes, which could potentially affect the generalization of the results\n  - can the author provide some results on real-world benchmarks? If conditions permit, I also suggest that the author compare it with sota linear programming algorithms (like some commercial solvers) and plot learning curves of the objective func value decreasing over time t\n\n- the paper's analysis assumes a batch size of $m$, which may not scale well or apply directly to the common practice of using mini-batches. Moreover, the discussion on the impact of step size and batch size on the effective initialization scale is not sufficiently detailed, potentially limiting the applicability of their findings\n\n- while the paper offers a comparison with mirror descent, it may benefit from a broader comparison with other optimization algorithms in the community (empirically, or theoretically) to establish a more comprehensive understanding of its advantages"
                },
                "questions": {
                    "value": "1. In practical applications, mini-batch GD is common; could the authors speculate on how their results might change with the introduction of mini-batches? Could the authors discuss the limitations of their assumptions regarding initialization and step sizes in more technical depth, possibly suggesting how these might be relaxed or generalized?\n\n2. Are there any theoretical insights from the paper that could suggest practical guidelines for tuning hyperparameters (like the step-size) in GD to leverage the sparsity-inducing properties observed in the reparameterized model?\n\n3. The theoretical framework is focused on diagonal linear networks; could the authors discuss the potential challenges and modifications required to extend their framework to deep / non-linear networks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8323/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699256079842,
            "cdate": 1699256079842,
            "tmdate": 1699637035154,
            "mdate": 1699637035154,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8WIhv0AM7m",
                "forum": "bLhqPxRy3G",
                "replyto": "yREmahpdMV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8323/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8323/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (Thanks for the review and incisive questions. We truly appreciate your comments.)"
                    },
                    "comment": {
                        "value": "$\\textbf{Response on the use of minibatch SGD:}$  We appreciate your inquiries. The substitution of minibatch gradient descent for gradient descent is not only feasible but can potentially offer advantages. Upon closer examination of our proofs and drawing on similarities between GD and one pass SGD on DLN, it becomes evident to us that the theoretical findings presented in our paper remain robust for minibatch GD with an appropriate choice of the batch size. \n\n $\\textbf{Response on the effect of initialization:}$ Exploring the impact of step size and initialization is indeed a crucial avenue of investigation. The initialization of our gradient descent proves to be a pivotal factor in guiding the algorithm toward the desired solution. As demonstrated in Theorem ~2.2 and 3.5, we established that initializing the gradient descent with\n      $u^0= \\exp(-c/2\\lambda)$ leads to convergence toward the solution of $$\\min_u\\{\\langle c, u\\odot u\\rangle + \\frac{\\lambda}{1-\\lambda}\\sum_{i} u_i\\odot u_i \\log (u_i\\odot u_i)\\}.$$  Indeed, selecting a small positive value for \n$\\lambda$ results in a very small initialization. This, in turn, enhances the quality of the solution, given that the regularization term diminishes as $\\lambda$ approaches $0$.\n\n$\\textbf{Response on the effects of stepsize:}$  The impact of the step size is pivotal in achieving a superior generalization error. The theoretical bounds on the step size required for global linear convergence have been established in Theorem. However, recent works, such as in [1], delve into the effect of an increasing step size. In [1], it is demonstrated that, as the step size increases, gradient descent in quadratic regression undergoes various phases. Beyond theoretical bounds, the loss function oscillates around a diminishing trend, indicating the onset of the 'catapult' phase. Notably, in the catapult phase, employing certain ergodic averaging methods has been shown to lead to improved generalization error. Further increasing the step size leads to entry into the periodic phase, characterized by periodic oscillations in the loss function with no discernible downward trend. Subsequently, the chaotic phase ensues, devoid of any signs of convergence, ultimately culminating in divergence. Given that our gradient descent iterates mirror those of least square quadratic regression, we anticipate that the observations made in [1] will hold for our framework. We would be delighted to incorporate a detailed discussion of this in the revised version.\n\n[1] Chen, X., Balasubramanian, K., Ghosal, P., Agrawalla, B. (2023). From Stability to Chaos: Analyzing Gradient Descent Dynamics in Quadratic Regression. ArXiV:  2310.01687\n\n$\\textbf{Response on comparison with other commercial LP solver:}$  We have run a new experiment comparing our method against \n the commercial LP solver Gurobi and another first-order method: an ADMM-based solver SCS. In particular, we simulate the data with the same data-generating process as in Section 4 of the paper, but with $m = 500$ and $n = 50000$. Here is a brief comparison of the methods in this example: Gurobi, SCS and our algorithm achieve respectively 0, -0.971,0.005 primal gaps and 0, 2.21e-10, 4.17e-11 feasibility gaps in 16.77, 40.49 and 9.26 seconds. \n\nHere the primal gap is defined as $(c^\\top \\hat x - c^\\top x^*) / c^\\top x^*$, with $\\hat x$ be the solution by an algorithm and $x^*$ is the optimal solution (by Gurobi). The feasibility gap is defined as $\\| A \\hat x - b \\|_2^2 / \\max\\{1, \\| b\\|_2^2 \\} $. \nFirst, it can be seen that our method is much faster than SCS, \nwhere our method computes a solution with a smaller primal gap in a much shorter time. \nOur method is also faster than Gurobi if a low-accuracy solution with a primal gap $0.005$ is satisfactory for the underlying application. \nMoreover, we note that Gurobi takes $10.11$ seconds for preprocessing, which is already similar to the runtime of our method. \n\nAdmittedly, our method is not able to obtain high-accuracy solutions as fast as Gurobi, but it is useful for quickly obtaining an approximate solution. Moreover, since Algorithm 1 only involves matrix-vector multiplications, it is easily implemented in a GPU-acceleration setting, which can potentially outperform Gurobi for larger problems.\n\n$\\textbf{Response on generalization for deep/non-linear network:}$  Instead of a diagonal linear network, one might consider employing an $k$-layer deep diagonal linear network. It amounts to parametrizing the gradient descent as $x= u\\odot \\cdots \\odot u$ over $k$ layers for $k>2$. The gradient descent initialized at $u_0 = \\alpha1_n$ then $u_k\\odot \\cdots \\odot u_k$ converges to the solution of $\\min_x\\{\\langle 1_n, x\\rangle - \\frac{k \\alpha^{k-2}}{2} \\sum_{i=1}^d x_i^{2/k}\\}$.  Extending this result to intricate architectures of deep non-linear networks is open. However, the optimal choice of initialization in such scenarios remains an area of active research."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8323/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289078414,
                "cdate": 1700289078414,
                "tmdate": 1700466761140,
                "mdate": 1700466761140,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]