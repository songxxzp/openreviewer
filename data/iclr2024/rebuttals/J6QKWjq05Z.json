[
    {
        "title": "TreeDQN: Learning to minimize Branch-and-Bound tree"
    },
    {
        "review": {
            "id": "J3hdQWvseg",
            "forum": "J6QKWjq05Z",
            "replyto": "J6QKWjq05Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission186/Reviewer_vJVo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission186/Reviewer_vJVo"
            ],
            "content": {
                "summary": {
                    "value": "This paper extended the on-policy learning to branch method introduced by Scavuzzo et al. in 2022 to an off-policy setting by offering a proof of contraction in mean, a modified mean squared logarithmic error, and an adapted Double Dueling DQN scheme."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The evaluation experiments demonstrate a noteworthy improvement compared to previous work and other state-of-the-art approaches.\n    \n2. The modified mean squared logarithmic error proves to be well-suited for long-tailed distributions of BB tree sizes and exhibits superior performance compared to the mean squared error in the ablation study."
                },
                "weaknesses": {
                    "value": "My main concerns about this paper are generalization ability, scalability, and some basic assumptions. Please find details in the questions."
                },
                "questions": {
                    "value": "1. Regarding the Assumption in Theorem 4.1: The paper assumes that the probability of having left and right children does not depend on the state because the pruning decision depends on the global upper bound instead of the parent node. However, the global upper bound can change dynamically during the search, which might influence the probability. Does this paper use optimal solutions as upper bounds? Could the authors provide further clarification on this assumption?\n    \n2. Exploring Limited Generalization Ability: In comparing the results presented in Table 5 and Table 3, it is observed that TreeDQN appears to exhibit less stability in the context of transfer tasks. Could you please offer insights or explanations regarding this phenomenon?\n    \n3. A Traditional vs. RL-based Variable Selection Perspective: Traditional variable selection methods rely on human-designed criteria, such as pseudocosts. One advantage of these traditional approach is its applicability to various problem types. On the other hand, current RL-based methods require training an optimal policy for each specific problem. Given the noted limitations in generalization ability, RL methods seem to necessitate training on problem instances of a similar size as the target problems. Could you provide any comments or insights on the potential implications of this limitation? (This question is optional, and your input is welcomed purely out of curiosity.)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission186/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission186/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission186/Reviewer_vJVo"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission186/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698125594918,
            "cdate": 1698125594918,
            "tmdate": 1699635944393,
            "mdate": 1699635944393,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o7YvMPOcv0",
                "forum": "J6QKWjq05Z",
                "replyto": "J3hdQWvseg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission186/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission186/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review"
                    },
                    "comment": {
                        "value": "> However, the global upper bound can change dynamically during the search, which might influence the probability. Does this paper use optimal solutions as upper bounds? Could the authors provide further clarification on this assumption?\n\nIn our experiments, we use DFS node selection during training and switch to the default node selection strategy during testing. We also experimented with training using the default node selection strategy. We observed that the agent trained with the default node selection strategy performs slightly less, compared to our final setup, due to higher stochasticity and violation of the Markov property of the training environment.\n\nWe present Theorem 4.1 to support the statement that a reinforcement learning agent should be able to converge to an optimal policy in a fully observed tree MDP when the probabilities  $p^+$ and $p^-$ do not depend on the state. Indeed, the branching process in the actual B&B solver may violate some initial assumptions like the Markov property or the independence of $p^+$, $p^-$ probabilities from the state. However, if the state does not contain the exact value of the global upper bound, we can not accurately predict the number of child nodes. Thus, we can consider probabilities $p^+$ and $p^-$ independent from the state to some extent. So, if the violation of initial assumptions is small enough, the agent should be able to converge to a well-performing policy, as we demonstrate in our experiments.\n\n> Exploring Limited Generalization Ability: In comparing the results presented in Table 5 and Table 3, it is observed that TreeDQN appears to exhibit less stability in the context of transfer tasks. Could you please offer insights or explanations regarding this phenomenon?\n\nThank you for raising an interesting question! We train the TreeDQN agent to optimize the geometric mean of the expected return. So rare, large trees may have a less significant impact on the final policy. It can be seen from the Probability-probability plot for the Maximum Independent Set task (Fig. 4) that the TreeDQN starts falling behind the IL agent when the tasks become harder (in the upper right corner). Thus, in complex transfer tasks, the TreeDQN agent may underperform.\n\n> A Traditional vs. RL-based Variable Selection Perspective\n\nRL methods can find a new branching strategy that could perform better than the traditional methods for a specific distribution of tasks. We believe that RL methods can improve the performance of Branch-and-Bound solvers in areas that require frequent solving of similar MILPs. For example, when a logistics company ships goods to customers or when we need to allocate limited resources like human workers or computing resources of a data center. At the present time, human-designed heuristics are better for tasks when you need to solve problems with significantly varying complexity and type or have only several task instances of the same kind.  However, the development of multitask and meta-learning approaches may extend the applicability of reinforcement learning methods."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission186/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587509276,
                "cdate": 1700587509276,
                "tmdate": 1700634777123,
                "mdate": 1700634777123,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "i5gvaqtNHx",
            "forum": "J6QKWjq05Z",
            "replyto": "J6QKWjq05Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission186/Reviewer_X3jH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission186/Reviewer_X3jH"
            ],
            "content": {
                "summary": {
                    "value": "This work studies the variable selection problem in the branch-and-bound algorithm from the point of view of Tree-MDPs, which, instead of the \u201clinear\u201d time-axis present in ordinary markov decision processes, models the decision history as a binary tree.\nThey show that under mild assumptions tree-MDPs allow for a contractive Bellman operator, justifying a Tree-MDP version of deep q-learning dubbed TreeDQN. Finally, the authors demonstrate their performance against the \u201cstrong branching\u201d baseline and other learnt variable selectors on a large set of synthetic instances."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Inherently, the idea of modelling variable selection as a Tree-MDP is a great idea as it allows the incorporation of the branch-and-bound structure into the decision process. The modification of the loss function to stably regress towards the geometric mean is also clever and might prove useful even outside the learnt variable selection domain. In general, the presentation of the work is clean and easy to read."
                },
                "weaknesses": {
                    "value": "1. Perhaps the biggest limitation is the assumption that the upper bound has to be derivable from the current node or known ahead of time. The authors assert that this (as well as more intricate node selection policies) lead to at most a moderate distribution shift, but never demonstrate this effect.\n2. Another concern is regarding the difficulty distribution of instances. Random instance generation has been known to generate significant amounts of trivial instances compared to real-world equivalents. However, this is a limitation of most prior work on learnt variable selection rules as well.\n3. TreeDQN is also more expensive in terms of wall-clock-time than prior work (especially the IL agent), which can be seen in Figure 4. The paper does not make it clear whether this is due to TreeDQN using a different architecture, or TreeDQN simply creating more expensive nodes during branching.\n4. An important missing baseline in their comparisons is out-of-the-box SCIP, acting as an automatic state-of-the-art hand-crafted tradeoff between SB and cheaper heuristics.\n\n\nThe paper needs an extensive re-write in terms of argumentation and clarity.\n\n\nSome more points:\n- Abstract: BnB solver[s] split a task\u2026\n- Abstract: \u2026the Bellman operator adapted for the tree MDP is contracting in mean\u2026 - initially I did not understand what you mean with that (only at some later point into the paper)\n- Intro: with [the] Branch-and-Bound algorithm (B&B). |[The] B&B algorithm employs\u2026\n- \u201cThe variable selection process is the most computationally expensive and crucial for the performance of the whole algorithm\u201d \u2013 is there a reference to prove this? If not, omit this sentence\n- Intro: \u201cproblematic\u201d \uf0e0 challenging\n- Intro: \u201csingle next state [the] agent\u201d\n- Intro: the contribution list at the end of the section looks like a draft and comes out of nothing\n- Sec. 2: where objective\u2026 sentence broken\n- Sec. 2: B&B [-algorithm-] builds\n- Sec. 2: explain \u201crelaxed\u201d\n- Sec. 2: Fig. 1 does not bring much to the table. I suggest to explain B&B with Fig. 1 right from the beginning (add primal/dual, relaxation, variables). This does not cost more space but helps to understand B&B\n- Sec. 2.: [A] straight forward strategy\n- Sec. 2.: [The] tree MDP was proposed by \u2026 In the tree MDP [the] value\u2026\n- Sec. 2.: The variable selection process \u2026 this paragraph is hard to understand\n- Sec. 3.: \u201cOur work improves\u2026\u201d please add some (technical) argument why this is the case\n- Sec. 4.0: this part takes much space and can be omitted imho. Instead focus on explaining the bullet-point list at the end of 4.0 in more detail. Why must a successful RL method should have off-policy as a property? Policy gradient methods are great, and they are on-policy\u2026 Here are a lot of arguments that need more justification.\n- Sec. 4.1 [E]quation3, [E]quation 4\n- Sec. 4.1 is not satisfying to me. The section and with an inequality and tells me that the proof follows from the fact that the tree is finite. Please work out this prove in more detail.\n- Sec. 4.2 the loss function [from E]quation 5\n- Sec. 4.3 we use loss function equation 5 \u2013 please re-write\n- Fig. 3 put the description into the plots"
                },
                "questions": {
                    "value": "- Is the method run on CPU or GPU?\n- What is the performance of SCIP with default parameters on these instances (I.e. reliability pseudocost branching)?\n- What is the model architecture (or more importantly: is it the same for all methods)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission186/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698560792150,
            "cdate": 1698560792150,
            "tmdate": 1699635944313,
            "mdate": 1699635944313,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3haUGks4BQ",
                "forum": "J6QKWjq05Z",
                "replyto": "i5gvaqtNHx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission186/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission186/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review"
                    },
                    "comment": {
                        "value": "> Perhaps the biggest limitation is the assumption that the upper bound has to be derivable from the current node or known ahead of time. The authors assert that this (as well as more intricate node selection policies) lead to at most a moderate distribution shift, but never demonstrate this effect.\n\nIn our experiments we use DFS during training and switch to the default node selection strategy during evaluation. If we assume that the Strong Branching heuristic is close to optimal in our benchmark tasks (Combinatorial Auction, Set Cover, Maximum Independent Set, Facility Location), we can estimate the maximum value of possible performance degradation as the difference between the performance of TreeDQN and Strong Branching:\n\nCombinatorial Auction - 22%, Set Cover - 30%, Maximum Independent Set - 5%, Facility Location - 10%.\n\n> Another concern is regarding the difficulty distribution of instances. Random instance generation has been known to generate significant amounts of trivial instances compared to real-world equivalents. However, this is a limitation of most prior work on learnt variable selection rules as well.\n\nIn our work, we focus on the same tasks as previous works (Gasse, NeurIPS 2020, Scavuzzo, NeurIPS 2022) to fairly benchmark our method with methods from the literature. In the updated version of our paper, we added a more challenging Balanced Item Placement task (see general response and updated paper, Appendix D).\n\n> TreeDQN is also more expensive in terms of wall-clock-time than prior work (especially the IL agent), which can be seen in Figure 4. The paper does not make it clear whether this is due to TreeDQN using a different architecture, or TreeDQN simply creating more expensive nodes during branching.\n\nThank you for spotting this. We use the same Graph Convolutional Neural Network encoder architecture for all models in our benchmarks (IL, tmdp+DFS, FMCTS, TreeDQN). The TreeDQN is creating more expensive nodes during branching. It is clearly seen from the evaluation results on the Balanced Item Placement task (see Appendix D). In this task, all problem instances were finished by reaching a timeout. The TreeDQN agent solves much fewer LPs but achieves a much higher reward and much lower primal bound than the IL agent.\n\n> An important missing baseline in their comparisons is out-of-the-box SCIP, acting as an automatic state-of-the-art hand-crafted tradeoff between SB and cheaper heuristics.\n\nWe added SCIP to our evaluation results. Please see the general response and updated paper.\n\n> The paper needs an extensive re-write in terms of argumentation and clarity.\n\nThank you for providing detailed feedback! We corrected the issues. Please see the updated version of our paper. \n\n> Is the method run on CPU or GPU?\n\nGPU\n\n> What is the performance of SCIP with default parameters on these instances (I.e. reliability pseudocost branching)?\n\nPlease see updated version of the paper\n\n> What is the model architecture (or more importantly: is it the same for all methods)?\n\nWe use the same Graph Convolutional Neural Network encoder architecture for all models in our benchmarks (IL, tmdp+DFS, FMCTS, TreeDQN)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission186/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581737287,
                "cdate": 1700581737287,
                "tmdate": 1700736036967,
                "mdate": 1700736036967,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Dlg1nJjECb",
            "forum": "J6QKWjq05Z",
            "replyto": "J6QKWjq05Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission186/Reviewer_ZMqh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission186/Reviewer_ZMqh"
            ],
            "content": {
                "summary": {
                    "value": "The authors use the TreeMDP framework introduced by Scavuzzo et al. to study RL methods for improved variable selection/branching in branch-and-bound for integer programming with the ultimate goal being smaller search trees. They propose a more stable and sample efficient RL training procedure by choosing a loss function to minimize the geometric mean of tree size during training, and use a deep Q network for training rather than the REINFORCE method used by Scavuzzo et al."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Branching is a critical aspect of integer programming solvers, and the authors provide an interesting new contribution towards RL based methods for the design of branching rules. The new methods are shown to produce smaller branch-and-bound trees than previous RL based variable selection methods, making this work a promising advance in the \u201clearning to branch\u201d line of work."
                },
                "weaknesses": {
                    "value": "Section 2.2 \u201cTree MDP\u201d needs way more explanation. It more or less assumes familiarity with the Tree MDP work of Scavuzzo et al., and a more self-contained exposition would be very helpful.\n\nThe theoretical contribution is very hazy to me. Contraction in mean is not really well-motivated. Does the cited theorem (Jaakkola \u201893) apply to the setting of tree operators here? That seems like a nontrivial assumption that is missing justification. Rather than just including a theorem about contraction in mean, the authors should have a main theorem that states the actual convergence guarantee that follows.\n\nMy understanding is that this paper is methodologically very similar to Scavuzzo et al., and only differs in the mechanics of how the RL algorithm is trained. This is discussed in Sections 4.2 and 4.3. In Section 4.2, the main difference is that the authors use a loss function that appears to be selectively picked based on the objective of minimizing the geometric mean of the tree sizes during training/testing. This to me feels like a specific and brittle design choice.\n\nThe new method is shown to yield smaller branch-and-bound trees than previous RL based variable selection policies, but no comparison is made to the default settings of any state-of-the-art solver (e.g., Gurobi, CPLEX, SCIP). This is an important comparison that should be included.\n\nOverall the presentation did not convince me that this is a sufficiently novel contribution for ICLR. It seems like the authors just slightly tweaked some aspects of the methodology of Scavuzzo et al. It\u2019s great that these modifications work and yield promising experimental results, but I just did not find the current writeup to be a sufficiently original contribution. The writeup itself also needs quite a bit of work to make it a cohesive, readable, and self-contained (the theory is presented in a very ad-hoc manner without formal definitions) contribution."
                },
                "questions": {
                    "value": "\u201cIn the B&B search trees, the local decisions impact previously opened leaves via fathoming due to global upper-bound pruning. Thus the credit assignment in the B&B is biased upward, which renders the learned policies potentially sub-optimal.\u201d I understand the first sentence, but what does the second sentence mean? What is \u201ccredit assignment\u201d, and why is it/what does it mean for it to be biased upward?\n\nSee also questions in the \u201cweaknesses\u201d section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission186/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission186/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission186/Reviewer_ZMqh"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission186/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801933304,
            "cdate": 1698801933304,
            "tmdate": 1699635944242,
            "mdate": 1699635944242,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zPiVzusI3g",
                "forum": "J6QKWjq05Z",
                "replyto": "Dlg1nJjECb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission186/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission186/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review"
                    },
                    "comment": {
                        "value": "> Section 2.2 \u201cTree MDP\u201d needs way more explanation.\n\nWe updated the background section and extended the description of Tree MDP. Please see the updated version. \n\n>  Rather than just including a theorem about contraction in mean, the authors should have a main theorem that states the actual convergence guarantee that follows.\n\nThe convergence of Q-learning methods can be rigorously proved only for tabular MDPs, which is out of the scope of the present paper. To provide an intuition as to why our method should work, we prove the contraction in mean property of the tree Bellman operator.  \n\n> My understanding is that this paper is methodologically very similar to Scavuzzo et al., and only differs in the mechanics of how the RL algorithm is trained.\n\nOur method has two significant differences from the work of Scavuzzo et.al.:\n1. Use of MSLE loss function. Distribution of tree sizes in the B&B method will always have a long tail (if we can not prune a tree, it will be a complete binary tree with the size growing exponentially with its depth). The MSLE loss function should generally work better than the MSE when predicting the size of the tree because it is more stable to outliers.\n2. Our method is much more sample efficient than the method of Scavuzzo et.al.(tmdp+DFS). To make one gradient update tmdp+DFS needs to solve a batch of MILP tasks until completion. In our approach, we make as many gradient updates as the size of the tree. To further prove the quality of our method, we tested it on a challenging Balanced Item Problem from the ML4CO competition (NeurIPS 2022, see Appendix D). We show that our TreeDQN agent can get a mean reward higher than the Imitation learning baseline. Since solving a single MILP task in this problem takes 15 minutes, training a tmdp+DFS in a reasonable amount of time would be impossible. \n\n> No comparison is made to the default settings of any state-of-the-art solver (e.g., Gurobi, CPLEX, SCIP)\n\nWe added evaluation for the SCIP solver with the default set of parameters. Initially, we did not include it because the internal branching rules can make various modifications to the state of the solver, so it can not be considered a direct competitor to other methods. For more information see:\n1. Discussion by Maxim Gasse (https://github.com/ds4dm/ecole/discussions/286#discussioncomment-2317487)\n2. Gerald Gamrath and Christoph Schubert. Measuring the impact of branching rules for mixed-integer programming. In Operations Research Proceedings 2017, pages 165\u2013170. Springer, 2018.\n\n> Overall the presentation did not convince me that this is a sufficiently novel contribution for ICLR.\n\nWe updated our paper to address the issues you mentioned. We would like to emphasize that our method is significantly more sample efficient than the method of Scavuzzo et.al. When solving a single MILP task, we can perform N gradient updates where N is the size of the resulting B&B tree, while tmdp+DFS needs to solve a batch of MILPs to make a single gradient update. This results in a much higher sample efficiency of our method. To further demonstrate the sample efficiency of our method,  we trained and evaluated TreeDQN on the challenging Balanced Item Placement task from the ML4CO competition. The results in Appendix D demonstrate that our method outperforms Imitation Learning and Strong Branching in terms of total reward by a high margin. This result will not be possible with less sample-efficient methods since solving a single MILP instance of this problem requires 15 minutes.\n\n>  I understand the first sentence, but what does the second sentence mean? What is \u201ccredit assignment\u201d, and why is it/what does it mean for it to be biased upward?\n\nCredit assignment is a problem in reinforcement learning when the immediate actions of an agent lead to rewards in the distant future, and the agent needs to figure out the connection between its actions and the delayed reward signal. We agree that this sentence is not clear and changed it to:\n\nIn the B&B search trees, the local decisions impact previously opened leaves via fathoming due to global upper bound pruning, which violates the Markov property."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission186/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581342397,
                "cdate": 1700581342397,
                "tmdate": 1700591075867,
                "mdate": 1700591075867,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I86nVrGEIZ",
            "forum": "J6QKWjq05Z",
            "replyto": "J6QKWjq05Z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission186/Reviewer_Uoqv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission186/Reviewer_Uoqv"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces TreeDQN, a reinforcement learning algorithm based on DQN for solving Tree MDPs. TreeDQN is trained on the mean squared logarithmic error loss. Specifically, the algorithm is used to learn branching heuristics for branch and bound in the context of mixed integer linear programming problems. \n\nEmpirical results on a set of benchmark problems show some of the advantages of TreeDQN for the purpose of learning a branching heuristic. The results on unseen tasks are somewhat mixed, with some advantage to the branching heuristic learned with TreeDQN."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper presents an algorithm for solving Tree MDPs with the specific application to learning branching heuristics for branch and bound algorithms in the context of solving mixed integer linear programming problems. TreeDQN presents better results on some of the benchmark problems used in the paper."
                },
                "weaknesses": {
                    "value": "The presentation is *possibly* the paper's weakest point. The lack of clarity makes me wonder about the value of the value of the contributions of the paper. The main contribution of the paper, TreeDQN, is explained in a single paragraph in the main text. Since the text only states that the algorithms is an adaptation of Double Dueling DQN, I assume TreeDQN is a straightforward adaption of DQN to Tree MDPs.\n\nThe paper builds on a couple of previous papers, which I had to skim over in order to understand the present paper. I am not entirely familiar with the line of work of using RL to learn how to branch and I can tell that the paper wasn't written for me. These are the two papers that helped me understand this submission:\n\nExact Combinatorial Optimization with Graph Convolutional Neural Networks \nand\nLearning to Branch with Tree MDPs\n\nThe example on Mixed Integer Linear Programming isn't very helpful. The tree shown in Figure 1 is uninformative; it simply shows nodes in a tree where the color scheme differs the root of the inner nodes and from some of the leaf nodes. It would have been more helpful to not show a tree and give the reader a full example on how the branch and bound search works. I asked ChatGPT for an example and it gave me an example (without any drawings, of course) that was more helpful than the tree example shown in the paper. \n\nOverall the background section could be re-written to use less space and pack more information to help the reader understand the work.\n\nI cannot understand the last paragraph of Section 2.2 without reading the paper by Scavuzzo et al. (2022). Here are the question I asked myself while reading that paragraph. \n\n1. Why do we need to use DFS as node selection or set the global upper bound in the root to the optimal solution cost to guarantee the Markov property? \n2. The gap between training and testing is due to assuming that one has the optimal solution in training? Why not use DFS and not assume that you have the optimal solution in training? \n3. How can more efficient heuristics for node selection also induce a gap between training and testing? And why is this important? \n\nSection 4 lists properties of a successful RL method for this problem, which includes off-policy and \"work with tree MDP instead of temporal MDP\". Why is it important to learn off-policy? We know of many successful on-policy algorithms for RL, what am I missing here? Why do they have to work with tree MDPs? \n\nThe empirical setting is described in previous papers and the current paper relies on that. How is the training data generated? Do the problems differ in difficulty? Do we have to optimally solve the problem to attain the Markov property to then train the model? If so, how are the problems solved? Assuming that the training instances are easy (one needs to solve them optimally), how does the learned heuristic scale to larger problems? \n\nThe number of seeds also seems to be small (5), for the kind of learning being done. \n\nOverall, it seems that the paper has some interesting ideas, but I don't fully understand them. The paper was written for people who already knows the details of this line of work, and it isn't friendly to newcomers to the point that the paper isn't self contained."
                },
                "questions": {
                    "value": "I would like to hear clarifications on the empirical setup on how the training of the branching function is done, as I listed in the weaknesses section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission186/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698875373571,
            "cdate": 1698875373571,
            "tmdate": 1699635944181,
            "mdate": 1699635944181,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V37oMrqUFB",
                "forum": "J6QKWjq05Z",
                "replyto": "I86nVrGEIZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission186/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission186/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review"
                    },
                    "comment": {
                        "value": "> The main contribution of the paper, TreeDQN, is explained in a single paragraph in the main text.\n\nOur method is described in 3 pages (pp. 4 - 6). It includes analysis of distributions of tree sizes, proof of convergence of the tree Bellman operator, discussion of our proposed loss function, implementation details (use of Double Dueling DQN adapted for tree MDP process), and training details.\n\n> Overall the background section could be re-written to use less space and pack more information to help the reader understand the work.\n\nWe updated the background section and improved the description of the Branch-and-Bound algorithm. Please see the updated version. \n\n> Why do we need to use DFS as node selection or set the global upper bound in the root to the optimal solution cost to guarantee the Markov property? \n\nWe improved this section. Please see the updated version. To guarantee the Markov property, we need probabilities $p^+$ and $p^\u2212$ depending only on the parent state and action. However, they depend on the global upper bound (GUB), which can vary for different visiting orders. To enforce the Markov property, one can either set the GUB in the root node equal to the optimal solution or choose Depth First Search as a node selection strategy.\n\nIf GUB is set equal to the optimal solution, it will remain the same for every node of the tree since a node can not contain a solution better than the optimal.\n\nWe also can compute GUB for the descendant nodes if we know the parent node and use a deterministic top-down visiting strategy like DFS.\n\n> The gap between training and testing is due to assuming that one has the optimal solution in training? Why not use DFS and not assume that you have the optimal solution in training?\n\nWe do not assume that we have an optimal solution. In our experiments, we use DFS during training and switch to the SCIP default node selection strategy during testing since it is more efficient (but can violate the Markov property). In our experiments, we see that our TreeDQN agent performs close to the Imitation Learning agent and Strong Branching heuristic, so the gap between training and testing environments is sufficiently small.\n\n> How can more efficient heuristics for node selection also induce a gap between training and testing? And why is this important? \n\nMore efficient heuristics may violate the Markov property ($p^+$ and $p^-$ depend only on the current state). If they do, the reinforcement learning method may not learn an optimal policy since the state would not contain all the information to choose an optimal action.\n\nIt is important because we want to test and train our network on the same distribution. If we apply a different node selection strategy, it may lead to a different distribution of the number of descendant nodes, which could decrease the efficiency of our method. In our experiments, we do not see significant performance degradation.\n\n> Section 4 lists properties of a successful RL method for this problem, which includes off-policy and \"work with tree MDP instead of temporal MDP\". Why is it important to learn off-policy? We know of many successful on-policy algorithms for RL, what am I missing here? Why do they have to work with tree MDPs?\n\nWe updated this section. Solving MILP tasks is time-consuming. We need sample efficient methods to learn a variable selection policy. Off-policy methods are generally much more sample-efficient than on-policy since they can reuse arbitrary old experiences during training. In the paper, we updated this requirement from \"Be off-policy\" to \u201cBe sample efficient\u201d.\n\nWhen solving a MILP task, the Branch-and-Bound solver splits the problem into subproblems and produces a binary tree. To train an RL agent, we need to map this tree to an episode. Considering the whole tree as a single episode under the tree MDP paradigm lets an agent directly optimize the size of the tree.\n\n> How is the training data generated? \n\nFor each task we have a distribution of parameters and randomly sample task instances from that distribution. \n\n> Do the problems differ in difficulty?\n\nYes. Depending on sampled parameters, the task could be easy (LP relaxation of the initial problem provides an integer feasible solution, so the size of the resulting B&B tree is 1) or require multiple branching decisions to find an optimal solution.\n\n> Do we have to optimally solve the problem to attain the Markov property to then train the model?\n\nNo. In our work, we use DFS during training and the default node selection strategy during testing."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission186/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580294073,
                "cdate": 1700580294073,
                "tmdate": 1700593450194,
                "mdate": 1700593450194,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "43XksGPtRQ",
                "forum": "J6QKWjq05Z",
                "replyto": "I86nVrGEIZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission186/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission186/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Assuming that the training instances are easy (one needs to solve them optimally), how does the learned heuristic scale to larger problems?\n\nIn general, testing an ML model on out-of-domain tasks is hard. In our work, we demonstrate that our model can generalize to some extent to more complex tasks with a larger number of branching variables. The intuition of why it works is twofold:\n1. The model learns a policy that can generalize to a larger number of branching variables. \n2. During the solution, the domain of some variables is getting tightened to a single integer, and the actual amount of variables that can be branched decreases. \n\n> The number of seeds also seems to be small (5), for the kind of learning being done.\n\nWe follow Gasse, Exact combinatorial optimization with graph convolutional neural networks., NeurIPS 2020 and Scavuzzo, Learning to branch with tree mdps, NeurIPS 2022 and use 5 random seeds during testing. Using a much larger amount of seeds would be difficult since solving MILP tasks is time-consuming."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission186/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580361168,
                "cdate": 1700580361168,
                "tmdate": 1700580399748,
                "mdate": 1700580399748,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]