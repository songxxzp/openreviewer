[
    {
        "title": "Perceptual Metrics for Video Game Playstyle Similarity and Diversity"
    },
    {
        "review": {
            "id": "eQfVChGwaa",
            "forum": "hfAEEsIQ6D",
            "replyto": "hfAEEsIQ6D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission117/Reviewer_MAaz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission117/Reviewer_MAaz"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of defining a metric for comparing the similarity of playtraces from videogames (sequences of video frames and actions). The work improves the prior method called Playstyle Distance in three ways:\n1) use of multiple encodings of states\n2) use of an exponential kernel to scale the distance metric and treat it as a probability\n3) use of the Jaccard index to improve assessment of overlap compared to the prior intersection\n\nThe evaluations assess each of these changes in isolation (to the degree possible), along with an overall assessment. Evaluations primarily used two datasets (one from humans, one from AI agents), with a third dataset used to evaluate the complete set of techniques. Results show the full metric improves over the prior Playstle Distance metric."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "# originality\nThe paper focuses on a problem with limited prior work - developing ways to quantify similarity of playstyles. The originality of the work lies in the three improvements made to the Playstyle Distance metric. These are all reasonable incremental improvements.\n\n# quality\nExperiments vary single elements of the proposed changes to evaluate their efficacy; this provides some rigor in the evaluation of the overall method. The technical improvements are motivated by limitations of the Playstyle Distance and employ theoretically well-motivated adjustments.\n\n# clarity\nThe paper articulates the insight behind each of the three core extensions to the previous algorithm and describes the changes clearly.\n\n# significance\nPlaystyle metrics working from video (and action) data are valuable tools for a variety of applications including assessing \"human-likeness\", characterizing reinforcement learning algorithm behavior diversity, to stylometry work to model how humans play games. The paper targets a problem with wide reuse, while building on core components of a prior model."
                },
                "weaknesses": {
                    "value": "# originality\nThe original contribution is limited to extending a previous model. This is in line with the intended contribution of the paper, and remains in line with the broader objective of behavioral playstyle metrics that require little manual tuning. Thus it is not a major weakness.\n\n# quality\nThe experiments are only weakly supportive of individual components of the changed method. Section 5.3 (the full model) provides the clearest results showing superiority of the new technique over prior efforts. Results in section 5.1 and 5.2 seem to show improvements for only 1 domain (of 2 tested), where the two domains are quite similar in structure (though one uses humans and one AI agents). Below are more detailed questions on the evaluation.\n\n# clarity\nNo substantial issues with clarity. Only minor clarification questions (below).\n\n# significance\nThe primary weakness is the technique is only compared to a single prior effort. Thus there is no sense of how this technique compares to the state of the art in performance. This makes it hard to claim the technique has major significance, beyond the clear feature that it depends on very little manual tuning (of features, heuristics, thresholds, or other parameters)."
                },
                "questions": {
                    "value": "- What other baselines could be used for comparison in these games?\n\t- The empirical evaluations focus on ablations of the existing metric, but do not compare against any other metrics. This makes it difficult to assess how the current approach fares compared to alternative ways of conceptualizing playstyle measurement.\n- Table 1:\n\t- Can multiple seeds or variations of the method be run to quantify uncertainty in the performance?\n\t- The text claims multiscale features (\"mix\") are superior. Looking at RGSK this result is not clear: the $2^20$ result for t=1 is quite close to mix for t=1. For TORCS the mix results are more clearly better (modulo the lack of any estimate of uncertainty).\n\t- Better results on 1 of 2 cases evaluated (TORCS) is not very strong evidence for superiority of the method. Perhaps this would be stronger with evidence from Atari as well?\n- Figure 3:\n\t- As with table 1, is it possible to add uncertainty estimates to the results presented?\n\t- The results on probabilistic similarity again show clear results for TORCS, but more ambiguous outcomes for RGSK. Lacking any confidence intervals, the results in RGSK look very close to one another for all methods.\n\t- Is it possible the dataset is the root cause of these issues? TORCS is from AI agents, which may be easier to classify in style than humans (though it's not obvious a priori why this would be true).\n- Figure 4: What are the shaded areas?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission117/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698023140483,
            "cdate": 1698023140483,
            "tmdate": 1699635936942,
            "mdate": 1699635936942,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x7cSuyW2L8",
                "forum": "hfAEEsIQ6D",
                "replyto": "eQfVChGwaa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission117/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission117/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission117/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Questions"
                    },
                    "comment": {
                        "value": "## Questions\n* > What other baselines could be used for comparison in these games?\n    * Within the datasets provided by Playstyle Distance, it is challenging to identify more comparable baselines due to the lack of playstyle labels in the training datasets. Additionally, the dataset for TORCS and the Atari games contains only one playstyle, which makes contrastive learning impractical.\n    * > The empirical evaluations focus on ablations of the existing metric, but do not compare against any other metrics. This makes it difficult to assess how the current approach fares compared to alternative ways of conceptualizing playstyle measurement.\n        * Regarding the empirical evaluations focusing on ablations of the existing metric and not comparing against other metrics, we note that a possible unsupervised clustering method mentioned in Playstyle Distance shows less than 30% accuracy for a 5-class classification (compared to over 80% accuracy for Playstyle Distance). Playstyle Distance itself is claimed as the first metric of its kind for video game playstyles in its abstract, which is why we focused on improving this baseline.\n    \n\n* > Table 1:\n    * > Can multiple seeds or variations of the method be run to quantify uncertainty in the performance? The text claims multiscale features (\"mix\") are superior. Looking at RGSK this result is not clear: the     result for t=1 is quite close to mix for t=1. For TORCS the mix results are more clearly better (modulo the lack of any estimate of uncertainty).\n        * Our experiments, including those using HSD, were conducted with three different HSD encoders, as provided in the official release of Playstyle Distance on PapersWithCode. Below is a detailed table with results from these three HSD encoders, including the corresponding standard deviations. If more detailed standard deviation data for each accuracy in the 100-round random subsampling is needed, we can conduct further experiments, though this will require additional time.\n\n| - | $1$ | $2^{20}, t=2$ | $2^{20}, t=1$ | $256^{res}, t=2$ | $256^{res}, t=1$ | $\\textbf{mix}, t=2$ | $\\textbf{mix}, t=1$ |\n| -------- | -------- |  -------- |  -------- |  -------- |  -------- |  -------- |  -------- | \n| TORCS | 35.16 | 66.52/69.40/76.12 (std=4.02) | 56.76/63.32/62.40 (std=2.90) | 6.36/3.72/3.72 (std=1.24) | 49.40/62.84/70.04 (std=8.55) | 73.28/71.40/79.64 (std=3.53) | 72.88/76.68/75.60 (std=1.60) |\n| RGSK | 80.14 | 79.92/78.71/76.54 (std=1.40) | 92.67/97.42/90.2 (std=3.00) | 5.58/5.08/6.21 (std=0.46) | 25.50/20.67/33.58 (std=5.33) | 91.75/83.33/90.46 (std=3.70) | 95.54/91.88/95.21 (std=1.65) |\n\n* > Better results on 1 of 2 cases evaluated (TORCS) is not very strong evidence for superiority of the method. Perhaps this would be stronger with evidence from Atari as well?        \n    * The result shows that the baseline is sensitive to hyperparameters, and the hierarchical state version appears more stable in different scenarios.\n* > Figure 3:\n    * > As with table 1, is it possible to add uncertainty estimates to the results presented?\n        * The curves in Figure 3 include very narrow shaded regions representing the minimum and maximum values from the three HSD accuracies. \n    * > The results on probabilistic similarity again show clear results for TORCS, but more ambiguous outcomes for RGSK. Lacking any confidence intervals, the results in RGSK look very close to one another for all methods. Is it possible the dataset is the root cause of these issues? TORCS is from AI agents, which may be easier to classify in style than humans (though it's not obvious a priori why this would be true).\n        * Regarding the results on probabilistic similarity, it's true that the improvement on RGSK is minor. In TORCS, each playstyle is closely controlled, with slight variations in driving target speed and noise, necessitating a more precise playstyle metric.\n\n* > Figure 4: What are the shaded areas?\n    * The shaded areas represent the minimum and maximum values of the evaluation results from the three HSD encoders. These models were trained from different random seeds, as released in the official implementation of the Playstyle Distance paper on PapersWithCode."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699790957813,
                "cdate": 1699790957813,
                "tmdate": 1699791071493,
                "mdate": 1699791071493,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wo8J4PH1KU",
                "forum": "hfAEEsIQ6D",
                "replyto": "sQ7LwgLcwE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission117/Reviewer_MAaz"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission117/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission117/Reviewer_MAaz"
                ],
                "content": {
                    "comment": {
                        "value": "I'm pretty confused by these tables. Could the authors please put this in an appendix with the interpretation and a visualization that can help understand the consistency outcomes?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700012697831,
                "cdate": 1700012697831,
                "tmdate": 1700012697831,
                "mdate": 1700012697831,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1TWS10VLFM",
                "forum": "hfAEEsIQ6D",
                "replyto": "x7cSuyW2L8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission117/Reviewer_MAaz"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission117/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission117/Reviewer_MAaz"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the responses! The clarifications on the shaded areas are helpful and it would help to include those in the text (please let me know if I missed where they were during my read).\n\n> Playstyle Distance itself is claimed as the first metric of its kind for video game playstyles in its abstract, which is why we focused on improving this baseline.\n\nI understand the novelty of the approach compared to prior methods, so it makes sense that no alternatives were compared.\n\n> Within the datasets provided by Playstyle Distance, it is challenging to identify more comparable baselines due to the lack of playstyle labels in the training datasets. Additionally, the dataset for TORCS and the Atari games contains only one playstyle, which makes contrastive learning impractical.\n\nAre there other game datasets that could be evaluated that were not evaluated in the original paper? Providing a clear demonstration of superior results would convince me of the strength of the method.\n\nFrom the results in the new table it looks like the 95% confidence intervals overlap for t=1 and mix. So it does seem like the improvements may be quite limited."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700013208425,
                "cdate": 1700013208425,
                "tmdate": 1700013208425,
                "mdate": 1700013208425,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZTMdCW6dJD",
                "forum": "hfAEEsIQ6D",
                "replyto": "rTWO7hgmec",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission117/Reviewer_MAaz"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission117/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission117/Reviewer_MAaz"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you, I look forward to the new results."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694809350,
                "cdate": 1700694809350,
                "tmdate": 1700694809350,
                "mdate": 1700694809350,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4F6N13lMtc",
            "forum": "hfAEEsIQ6D",
            "replyto": "hfAEEsIQ6D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission117/Reviewer_EUGE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission117/Reviewer_EUGE"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose extensions of a previous approach, \"Playstyle Distance,\" which attempted to quantify different playstyles by DRL agents in applications such as video game play. The authors argue why different modifications to this approach are needed for improvement, such as multi-scale state encoding and incorporating other metrics. They provide experiments showing the improvements in playstyle accuracy classification attributable to these modifications."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors do a good job of explaining previous work.\n- The authors provided code to recreate the experiments.\n- The derivations in the paper seem correct."
                },
                "weaknesses": {
                    "value": "- The theoretical contribution is marginal, including the connection to video game play. The concept of \"Playstyle Distance\" itself does not seem to be specific to game play---it is a straightforward distance between policies using encoded state/action pairs [while \"straightforward\" is a feature (and not a bug) here, it raises the question as to why this paper needs to be framed in this language at all].\n- As \"Playstyle Distance\" and related concepts are previous work, the additional theoretical contributions are minimal. Incorporating the Bhattacharyya distance is fine, but the connections made to human psychology are a bit flimsy. Additionally, the \"Playstyle Similarity\" metric is ad-hoc (which is isn't inherently bad); it's also not immediately clear to me (or at least it wasn't argued) if it's a metric, as the product of two metrics isn't a metric in general.\n- Given the above points, the experimental section is not substantive enough. While the authors compare Playstyle Distance against their various improvements, they use classification accuracy to compare. However, taking a step back---this is now just a supervised learning problem. Why are any of these needed at all? If the _actual_ task is playstyle classification, then algorithms need to be compared against supervised learning approaches. \n- While I appreciate the fact that code was provided, there weren't good instructions in the supplementary material zip file. Clicking the PapersWithCode link in the paper then brought me to a page that clearly has the author names and affiliations listed."
                },
                "questions": {
                    "value": "- Is Playstyle Similarty a metric?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission117/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission117/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission117/Reviewer_EUGE"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission117/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698433357098,
            "cdate": 1698433357098,
            "tmdate": 1699635936879,
            "mdate": 1699635936879,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v7AZkwiHWu",
                "forum": "hfAEEsIQ6D",
                "replyto": "4F6N13lMtc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission117/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission117/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission117/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Concerns"
                    },
                    "comment": {
                        "value": "## Concerns\n* > The theoretical contribution is marginal, including the connection to video game play. The concept of \"Playstyle Distance\" itself does not seem to be specific to game play---it is a straightforward distance between policies using encoded state/action pairs (while \"straightforward\" is a feature (and not a bug) here, it raises the question as to why this paper needs to be framed in this language at all).\n    * Our distance metric still relies on the Wasserstein distance for measuring the effort required to change playstyles. The Bhattacharyya distance, as a variant, aligns with our proposed probability mapping function derived from the Weber-Fechner law (as detailed in Section A.1).\n* > As \"Playstyle Distance\" and related concepts are previous work, the additional theoretical contributions are minimal. Incorporating the Bhattacharyya distance is fine, but the connections made to human psychology are a bit flimsy. Additionally, the \"Playstyle Similarity\" metric is ad-hoc (which is isn't inherently bad); it's also not immediately clear to me (or at least it wasn't argued) if it's a metric, as the product of two metrics isn't a metric in general.\n    * Playstyle Similarity is indeed a metric, incorporating a conditional consideration of intersection over union. In Section 5.3, we explain that without the Jaccard index, the metric focuses only on intersection states, ignoring samples outside this intersection set. With only the Jaccard index, action distribution is not measured. Thus, it's not merely a product of two metrics, but a combination that leverages all observed state-action pairs, including:\n        * Playstyle Intersection Similarity: A metric of action distribution similarity in the intersection set.\n        * Playstyle Jaccard Index: A metric of all observed states.\n        * Playstyle Similarity: A comprehensive metric of all observed state-action pairs.\n* > Given the above points, the experimental section is not substantive enough. While the authors compare Playstyle Distance against their various improvements, they use classification accuracy to compare. However, taking a step back---this is now just a supervised learning problem. Why are any of these needed at all? If the actual task is playstyle classification, then algorithms need to be compared against supervised learning approaches.\n    * We use a classification task with a similarity metric to validate these metrics. In these cases, it can be assumed that all supervised learning approaches would exhibit random-level accuracy since there are no playstyle labels in the training dataset. Additionally, contrastive learning is impractical for the TORCS and Atari games datasets as they contain only one playstyle, making it unsuitable for such learning.\n* > While I appreciate the fact that code was provided, there weren't good instructions in the supplementary material zip file. Clicking the PapersWithCode link in the paper then brought me to a page that clearly has the author names and affiliations listed.\n    * If you encounter any issues with running our code in the Supplementary Material, we are available to assist. The PapersWithCode link in Section 4 directs to the official dataset and codes of \"Playstyle Distance,\" which serves as our baseline. Our implementation is a clone from their Git repository, and we have ensured not to reveal our names and affiliations."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699790867998,
                "cdate": 1699790867998,
                "tmdate": 1699790867998,
                "mdate": 1699790867998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oq6UJSI5oa",
                "forum": "hfAEEsIQ6D",
                "replyto": "4F6N13lMtc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission117/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission117/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission117/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Questions"
                    },
                    "comment": {
                        "value": "## Questions\n* Is Playstyle Similarty a metric?\n    * Yes, Playstyle Similarity is a metric comprising several aspects:\n        * Playstyle Intersection Similarity: Measures action distribution similarity in the intersection set.\n        * Playstyle Jaccard Index: Measures similarity across all observed states.\n        * Playstyle Similarity: Encompasses all observed state-action pairs for a comprehensive metric."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699790897849,
                "cdate": 1699790897849,
                "tmdate": 1699790912315,
                "mdate": 1699790912315,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fkqGVR3Dr3",
                "forum": "hfAEEsIQ6D",
                "replyto": "v7AZkwiHWu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission117/Reviewer_EUGE"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission117/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission117/Reviewer_EUGE"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you, I appreciate your responses to my concerns."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700083284460,
                "cdate": 1700083284460,
                "tmdate": 1700083284460,
                "mdate": 1700083284460,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Am5Rf0I7v3",
                "forum": "hfAEEsIQ6D",
                "replyto": "oq6UJSI5oa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission117/Reviewer_EUGE"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission117/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission117/Reviewer_EUGE"
                ],
                "content": {
                    "title": {
                        "value": "Response to Questions"
                    },
                    "comment": {
                        "value": "Sorry, I meant the mathematical definition of metric (e.g. satisfying triangle inequality). Is that true?"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700083361582,
                "cdate": 1700083361582,
                "tmdate": 1700083361582,
                "mdate": 1700083361582,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OyxIcOw9ML",
            "forum": "hfAEEsIQ6D",
            "replyto": "hfAEEsIQ6D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission117/Reviewer_crkG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission117/Reviewer_crkG"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a novel playstyle similarity metric, based on several extensions to the work of Lin et al. (2021). These modifications entail a multiscale metric, an exponential scaling, justified by psychophysics research and a probabilistic interpretation of the Jaccard index. The resulting variants are evaluated on two racing games and 7 Atari games, in a playstyle classification scenario."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well structured, explaining each modification in turn. Each modification is reasoned for and realized using proven concepts. The range of evaluation domains is diverse and all modifications are evaluated independently, showing that each is contributing to the whole."
                },
                "weaknesses": {
                    "value": "Several statements are not clear to the reviewer or hard to parse:\n- It is worth noting that Lin et al. (2021) were able to distinguish intersection states even with unprocessed screen pixels in Atari games. - Seems to state \"In two different datasets, Lin et al. (2021) did find pixel-identical screens\"?\n-  In a different scenario, when treating each state as equivalent, we can invariably pinpoint an intersection state. - Relates to identical, not just equivalent?\n- Unclear what exactly is new in Sec. 3.1 and what was introduced as part of HDS (Lin et al. (2021)).\n- While distance is a common metric for determining similarity, a larger distance value conveys primarily that two entities are different, without giving much insight into the degree of their similarity - Why doesn't a larger distance relates to a smaller degree of similarity?\n- Drawing from the concept of similarity, we can infer that a smaller distance provides more definitive information about the similarity - Needs to be explained.\n- General wording: \"intersecting states/samples\" suggest a partial equivalence of a single state/sample. E.g. \"intersecting set of states/samples\" could be more appropriate. This is especially relevant, because the HSD model can be used to defined a intersection over (single) states (based on the hierarchy), and therefore the concepts are not clearly distinguishable.\n- Sec 4.2.: \"Space size\" seems not to be introduced - probably number of discrete states?\n- To evaluate the efficacy of the proposed multiscale state space and to compare it fairly with Playstyle Distance, we primarily focus on the TORCS and RGSK platforms - Needs to be explained.\n- In this section, we perform a comprehensive evaluation of various metrics, including leveraging full data with union operations. - Seems to state, that state samples from all games are used as a single set?\n\nBesides these clarity issues, the experimental Section could use some improvements:\n- This is based on the assumption that variations in game content can be interpreted as different states - This statement was not empirically evaluated?\n- The \"game-merging\" study in Sec 5.3. is an interesting piece of additional information, but the per-game results are potentially more relevant. The results should be added to the main paper and in case of space constraints, one may think about rank metrics or a table with just the most interesting dataset sizes.\n- Why was Playstyle Similarity not evaluated in Sec. 5.2? \n- Most importantly, additional comparison baselines from related work should be added. E.g. clustering or supervised methods should be applicable.\n- The evaluation is only performed via classification, but a similarity metric should preserve a distance relation beyond \"Top-1\". Therefore, a ranking/continuous study should be performed. E.g. by creating or ordering existing playstyles along a continuous spectrum (like passive/aggressive driving) and evaluating the correlation.\n- Sec 5.2 a, b has nearly indistinguishable intervals. A tabular presentation or non-linear graph scaling should be used to improve clarity.\n\nOverall, the evaluation is still strong, but additional baselines and per-game results would greatly contribute to its value. The contribution was only deemed fair, mostly because the work does only modify an existing idea and the topic is quite niche."
                },
                "questions": {
                    "value": "See confirmative questions above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission117/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698655566700,
            "cdate": 1698655566700,
            "tmdate": 1699635936799,
            "mdate": 1699635936799,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bTIAriw56t",
                "forum": "hfAEEsIQ6D",
                "replyto": "OyxIcOw9ML",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission117/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission117/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission117/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Concerns Part1"
                    },
                    "comment": {
                        "value": "## Concerns\n* > It is worth noting that Lin et al. (2021) were able to distinguish intersection states even with unprocessed screen pixels in Atari games. - Seems to state \"In two different datasets, Lin et al. (2021) did find pixel-identical screens\"?\n    * In Atari games, pixel-identical consecutive screens can exist in two game trajectories due to the simplicity of the game environment and low randomness. In common DRL applications, we often find almost no repeated visited states due to high-dimensional observations.\n* > In a different scenario, when treating each state as equivalent, we can invariably pinpoint an intersection state. - Relates to identical, not just equivalent?\n    * When discretized states are considered, they can be treated as identical. However, when a state has a corresponding action, we do not eliminate the same state-action pair for constructing the sampling action distribution.\n* > Unclear what exactly is new in Sec. 3.1 and what was introduced as part of HDS (Lin et al. (2021)).\n    * Lin et al. (2021) used a single discrete state mapping and a sample count threshold to discard states with unstable action distributions. They did not use the hierarchical design in HSD for Playstyle Distance as it prefers stable action distributions and not very large discrete state spaces. Our new method in Section 3.1 enables the simultaneous use of different discrete state mappings.\n* > While distance is a common metric for determining similarity, a larger distance value conveys primarily that two entities are different, without giving much insight into the degree of their similarity - Why doesn't a larger distance relates to a smaller degree of similarity? Drawing from the concept of similarity, we can infer that a smaller distance provides more definitive information about the similarity - Needs to be explained.\n    * As the distance increases, the number of potential candidates with the same distance also increases significantly. In a 2D space, these candidates form a circle around a reference point; as the distance increases, the perimeter of the circle increases. Therefore, we focus more on the similar parts for a more definitive prediction.\n\n* > General wording: \"intersecting states/samples\" suggest a partial equivalence of a single state/sample. E.g. \"intersecting set of states/samples\" could be more appropriate. This is especially relevant, because the HSD model can be used to defined a intersection over (single) states (based on the hierarchy), and therefore the concepts are not clearly distinguishable.\n    * We can edit this part for a more precise description as long as the page limit is not exceeded in the camera reday version.\n* > Sec 4.2.: \"Space size\" seems not to be introduced - probably number of discrete states?\n    * The term \"Space size\" refers to the maximum count of different states in the discrete representation. We will use the term \"state space\" for more precise descriptions.\n* > To evaluate the efficacy of the proposed multiscale state space and to compare it fairly with Playstyle Distance, we primarily focus on the TORCS and RGSK platforms - Needs to be explained.\n    * We validate Playstyle Distance with solely multiscale state space on TORCS and RGSK to make sure multiscale state space is effective. For Atari games, single state space version already has a high accuracy with 512 sample size (over 90% accuracy). With the page limit, we move this part with curves to appendix (Figure 11 and 12).\n* > In this section, we perform a comprehensive evaluation of various metrics, including leveraging full data with union operations. - Seems to state, that state samples from all games are used as a single set?\n    * In Sections 5.1 and 5.2, our focus was on the observed state in the intersecting set of two datasets. In Section 5.3, we expanded this to include all observed states. The introduction of the Jaccard index allows us to define action distribution similarity even outside the intersecting set of samples."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699790764020,
                "cdate": 1699790764020,
                "tmdate": 1699790764020,
                "mdate": 1699790764020,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nIQecf7uSJ",
                "forum": "hfAEEsIQ6D",
                "replyto": "OyxIcOw9ML",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission117/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission117/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission117/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Concerns Part2"
                    },
                    "comment": {
                        "value": "* > This is based on the assumption that variations in game content can be interpreted as different states - This statement was not empirically evaluated?\n    * This assumption was validated using the Atari Console, employing a discrete encoder from other games.\n* > The \"game-merging\" study in Sec 5.3. is an interesting piece of additional information, but the per-game results are potentially more relevant. The results should be added to the main paper and in case of space constraints, one may think about rank metrics or a table with just the most interesting dataset sizes.\n    * The per-game results of Atari games in Section 5.3 have been moved to Section A.3. We can add a table in the appendix to report accuracy values. If necessary, we can consider removing or moving parts of the main paper to accommodate this table.\n* > Why was Playstyle Similarity not evaluated in Sec. 5.2?\n    * In Section 5.2, our focus remained on states within the intersection. The concept of Playstyle Similarity, which includes the Jaccard index, is discussed in Section 5.3, as it encompasses all observed samples.\n* > Most importantly, additional comparison baselines from related work should be added. E.g. clustering or supervised methods should be applicable.\n    * Common methods like the Fr\u00e9chet Inception Distance (FID) use latent feature distribution distances, which have been deemed impractical for classification in TORCS (as shown in Playstyle Distance). Supervised or contrastive learning baselines are challenging due to the absence of playstyle labels in training datasets or the presence of only one playstyle in the dataset.\n* > The evaluation is only performed via classification, but a similarity metric should preserve a distance relation beyond \"Top-1\". Therefore, a ranking/continuous study should be performed. E.g. by creating or ordering existing playstyles along a continuous spectrum (like passive/aggressive driving) and evaluating the correlation.\n    * We plan to use the TORCS dataset for an experiment evaluating continuous spectrum playstyles (5 target speeds from 60 to 80, and 5 levels of noise). This experiment will be conducted during the rebuttal period.\n* > Sec 5.2 a, b has nearly indistinguishable intervals. A tabular presentation or non-linear graph scaling should be used to improve clarity.\n    * We will include tables in the appendix to provide a clearer presentation of the results from Figure 3(a)(b)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699790821745,
                "cdate": 1699790821745,
                "tmdate": 1699790821745,
                "mdate": 1699790821745,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pGVlRr703L",
                "forum": "hfAEEsIQ6D",
                "replyto": "nIQecf7uSJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission117/Reviewer_crkG"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission117/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission117/Reviewer_crkG"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed answers. They clarify several of my questions and i also acknowledge the additional results. They are also not directly clear to me, but this issue was already raise by another reviewer and does not need any further discussion from my side. Assuming the clarifications will be introduced into a potential CR version, i will not argue against an accept (while already being slightly in favor anyhow)."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700123424543,
                "cdate": 1700123424543,
                "tmdate": 1700123424543,
                "mdate": 1700123424543,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NRcTTRKhXY",
            "forum": "hfAEEsIQ6D",
            "replyto": "hfAEEsIQ6D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission117/Reviewer_6U4t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission117/Reviewer_6U4t"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of discerning similarities among datasets containing state/action pairs derived from video games. This aids in pinpointing distinct playstyles and understanding the diversity of human behaviors within these games. The work enhances existing methods, particularly the Playstyle Distance technique, by modifying this approach. This traditional method involves an initial discretization of the state space, followed by a comparison of action distributions based on these discrete states using a wasserstein distance. The authors propose three advancements. Firstly, they introduce multiscale states, a refined discretization technique that uses a combination of mappings rather than just one. Secondly, they use the Bhattacharyya distance as an alternative to the Wasserstein distance, arguing that the former aligns more with human perception. Lastly, the authors use the Jaccard index to weight the distance when comparing conditional action distributions across intersecting states in both reference and analyzed datasets, ensuring a more accurate assessment of the intersection's significance. The researchers conducted experiments on three distinct games: two racing games and a collection of Atari games. When compared with various versions of the Playstyle Distance, their proposed methodology appears more proficient. It more effectively captures similarities, leading to a more accurate identification of players."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The proposed variations over existing approaches sound interesting and grounded on human cognition. By building upon the foundations laid out by prior research, this paper takes strides in refining and enhancing what has been previously suggested in the domain. It is a simple approach that performs well on the described datasets and that extends the possible playstyle identification set of methods."
                },
                "weaknesses": {
                    "value": "I find it challenging to grasp the exact task the authors aim to address, even though I recognize it's based on prior work. The paper's objective seems to be playstyle identification based on a few gameplay samples by comparing them with reference datasets,, but the experiments are more about player identification. While understanding the need for reference datasets for different playstyles, the creation and existence of these references remain ambiguous. A discussion on this would be beneficial.\nMoreover, the assumption is that the more the distributions of actions are different, the further the playstyles are. But is it really waht defines playstyle? What about identifying playstyles that are only different in very few states (for instance, two chess players that are using two different openings). Since the definition of palystyle provided here is not grounded on any concrete application, it is difficult to understand the relevance of the work.\n\nSecond, the approach hinges on discretizing the state space. While states are assumed to be continuous and actions discrete, how would this apply when actions are also continuous, as seen in many games? The method's efficacy seems tied to the state discretization's capability to reflect genuine state distances. This might work for pixel-based games, but what about more structured observations, like chess? The multiscale approach's specifics, including the number of mapping functions and their selection, are unclear, yet these details likely influence the outcome significantly.\n\nLast, the approach is an unsupervised method and is evaluated on very few use-cases. It makes it difficult to understand if the proposed approach is good 'in general' or if the choices made by the authors have been 'over-fitted' on the three single use cases they propose. The paper lacks a real dataset captured from real video games, with complex states, actions, and many more players than what is in the article. \n\nIf the contribution sounds right and improves over existing publications, the validation does not allow us to conclude if the approach is really good for identifying playstyles or not. The article lacks a clear definition of what a playstyle is, and why identifying playstyles is interesting. While the subject may appeal to a niche audience, particularly those in the video game research community  (e.g Cog conference), it might not meet the broader criteria for acceptance at ICLR"
                },
                "questions": {
                    "value": "* What are the real use-cases that the identification of playstyle is targeting? Why is it an unsupervised problem and not a supervised one ? How the playstyle references dataset are built and is it realistic?\n* What is the effect of the state discretization technique that is used ? How do you tune the multiscale approach ? Since you are only using few datasets, the way you tune it may be overfitting the datasets, isn't it ?\n* How do you deal with continuous actions since your work focuses on discrete action spaces?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concerns"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission117/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770219131,
            "cdate": 1698770219131,
            "tmdate": 1699635936705,
            "mdate": 1699635936705,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ak9TjRJtTP",
                "forum": "hfAEEsIQ6D",
                "replyto": "NRcTTRKhXY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission117/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission117/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission117/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Concerns"
                    },
                    "comment": {
                        "value": "## Concerns\n### In summary\n* Our distance is still based on Wasserstein distance for measuring the effort between changing playstyle. Bhattacharyya distance is a variant that meets our proposed probability mapping function derived form Weber-Fechner law (Section A.1).\n### In weakness\n* The objective of this paper is to define a similarity metric. The experiments for validating this metric involve identification/classification tasks and the application of diversity measurement in DRL agents. The reference datasets used in the experiments serve as examples of targeted playstyles. For instance, to measure an unknown playstyle, we can compare it against known playstyles and identify the most similar one. This scenario is akin to a zero-shot classification task, where no classification task occurs during the training of discrete representations.\n* For new games without existing reference sets, it's possible to first collect some trajectories and label them manually (giving meaning to the playstyle) or use a similarity threshold to create reference datasets.\n* Regarding concerns about very few intersection states (as in chess), this highlights why discrete state representation is important for determining comparable cases. If the playstyle (decision-making style) is indeed different, it should show action distribution differences in those intersection states (like opening or initial board). If the playstyle is very similar yet leads to different states, the result is either influenced by environmental randomness or decisions made by other players, which are not under the control of the player we are measuring. Otherwise, the same action in the same state should lead to the same next state, making it unlikely to have very few intersection states.\n* Regarding the concern about continuous action spaces, TORCS, as described in Section 4.1, is a game with a continuous action space.\n* The multiscale approach is rooted in human cognition and involves several levels of granularity. In Table 1 and Section A.2, we demonstrate that each state mapping function contributes some information to playstyle measurement (accuracy > $\\frac{1}{\\text{number of playstyles}}$). The combined version further improves accuracy (exceeding the use of just one state mapping). As this research is not focused on training discrete states, we directly utilize the hierarchical states from $\\textit{Playstyle Distance}$ and also test with downsampling discretization. Our experiments show that more state mappings lead to more accurate predictions. These mappings can be obtained by training more discrete encoders or leveraging HSD (simultaneous training of several discrete encoders with customized granularity).\n* Regarding the concern about generality, our metric is built on general decision-making and tested on rule-based agents, human players, and DRL agents. Our experiments are performed on video games, and we claim applicability to video games due to the straightforward scenario of demonstrating multiple playstyles. Should open benchmarks for decision-making styles become available, we believe our approach would be suitable for claiming broader generality. These decision-making problems could include robot control, natural language processing, and others, as described in the introduction, where DRL is a solution for general decision-making."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699790622220,
                "cdate": 1699790622220,
                "tmdate": 1699790622220,
                "mdate": 1699790622220,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hxCoWhnm6l",
                "forum": "hfAEEsIQ6D",
                "replyto": "NRcTTRKhXY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission117/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission117/Reviewers"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission117/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Questions"
                    },
                    "comment": {
                        "value": "## Questions\n* > What are the real use-cases that the identification of playstyle is targeting? \n    * Our metrics are designed to concretely define the similarity and diversity of decision-making styles (playstyles). They are useful in validating the diversity of AI policies, assessing whether imitation learning fits a given playing dataset, and for human behavior modeling. For instance, they can detect the similarity of a Go player's playstyle to that of the superhuman AlphaZero AI to prevent cheating. Playstyle metrics are also useful in recommendation systems for identifying potential customers with similar or new playstyles.\n* > Why is it an unsupervised problem and not a supervised one ? \n    * The metric and discrete representation are unsupervised, meaning no playstyle labels are used in training. This approach is akin to zero-shot classification, with no learning how to classify during the discrete representation learning phase. States are used for identifying comparable states for action distribution comparison, which implies playstyle information. \n    * According to Appendix D of the HSD paper (Playstyle Distance), it is impractical to perform supervised learning on training the discrete representation or playstyle classification, as either the datasets are sampled from different sources (TORCS and RGSK) or there is only one playstyle (Atari games). \n    * Besides, as described in Section 2.1, playstyle classification can be trained by supervised learning, but it relies on predefined labels and cannot handle playstyles not in the training data (the datasets we used cannot be solved by supervised learning).\n* > How the playstyle references dataset are built and is it realistic?\n    * The datasets used in playstyle analysis consist of gameplay collected by rule-based agents, human players, and DRL agents. For performing few-sample analysis, we use random subsampling without replacement, as shown in Figure 2(d). \n    * As for the realism of the games used in our experiments, TORCS and Atari games are common in DRL research. RGSK is a racing game available on the Unity Asset Store (https://assetstore.unity.com/packages/templates/systems/racing-game-starter-kit-22615), designed for human players.\n* > What is the effect of the state discretization technique that is used?\n    * The state discretization technique identifies comparable states for comparing action distributions. Without state discretization, it is challenging to measure the action distribution distance directly from playing datasets (observation-action pairs). \n    * In Section A.2, our experiments show that the state discretization technique we used (HSD) performs better than downsampling. The learned discrete representation focuses on features more related to gameplay rather than arbitrary features.\n* > How do you tune the multiscale approach ? Since you are only using few datasets, the way you tune it may be overfitting the datasets, isn't it?\n    * We do not tune the multiscale approach, as it does not require any hyperparameters for our proposed metric. The elements that can be tuned are the discrete state mappings. However, the scope of this paper is not about training new discrete representations; therefore, we directly use the trained models released by the baseline method (Playstyle Distance).\n* > How do you deal with continuous actions since your work focuses on discrete action spaces?\n    * Our metrics can handle both discrete and continuous action spaces, with the difference being the adoption of different types of probability distribution distance metrics. Specifically, TORCS has a continuous action space, while RGSK and Atari games have discrete action spaces, as described in Section 4.1."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission117/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699790668863,
                "cdate": 1699790668863,
                "tmdate": 1699790668863,
                "mdate": 1699790668863,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]