[
    {
        "title": "In-Context Unlearning: Language Models as Few Shot Unlearners"
    },
    {
        "review": {
            "id": "ZSDglKzHH2",
            "forum": "5LhYYajlqV",
            "replyto": "5LhYYajlqV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7822/Reviewer_fKNk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7822/Reviewer_fKNk"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce an innovative method termed 'Incontext machine unlearning.' A standout feature of this method is its ability to operate without needing access to the model's parameters, instead relying on in-context querying during inference. The crux of their work is a unique approach to unlearn specific training data points. Instead of resorting to computationally expensive retraining or finetuning, they suggest querying the language model with a carefully curated prompt during inference. This prompt is crafted to give the impression to the language model that it hasn't encountered a specific training instance. To implement this, upon receiving a deletion request, they recommend flipping the label of the targeted instance and appending it accordingly. Subsequent steps involve incorporating a set of randomly selected labeled example pairs, followed by the query input. The intention behind this structured query prompt is to effectively unlearn a designated training instance. While I find the core idea compelling, I feel the experiments section could benefit from enhanced clarity and depth. The presented results, especially the ROC curves, demonstrate the method's superiority over baseline techniques. The concept genuinely intrigues me, but I believe its presentation in the paper could be further refined for clarity and impact. I also think comparisons with prompt based adversarial attack methods would be necessary ([1] or related works). I have explained in detail about this below.\n\n[1] Raman, Mrigank, et al. \"Model-tuning Via Prompts Makes NLP Models Adversarially Robust.\" arXiv preprint arXiv:2303.07320(2023)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The idea of unlearning a subset of the training dataset through Incontext querying is particularly intriguing. This approach could significantly lessen the computational burden compared to previous machine unlearning techniques, like gradient ascent.\n\n2) The presented ROC curves for the ICUL method are close to the diagonal, sufficiently backing the hypothesis that the proposed method is indeed successful in reducing the likelihood of the forget set belonging to the training set. (However, there are a few points that I would like to make regarding the presentation. Please see below.)"
                },
                "weaknesses": {
                    "value": "Firstly, thank you for an insightful work. I hope my comments will further strengthen your work.\n\n1) Though, the presented idea is labeled as machine unlearning, I would like to point out that the presented approach queries the language model using a specific prompt designed to reduce the confidence of the model on a few training instances from the forget set. When queried using alternative prompts, there is no guarantee that the language model would perform with similar characteristics. Empirical evidence is the \u201cDependence on forget point\u201d ablation study where using a random instance instead of the forget instance (i.e., a changed prompt) lead to inferior results. However, methods such as gradient ascent, though computationally more expensive than the proposed method, would lead to a reduction in likelihood of the forget set belonging to the training set irrespective of the query. In Fact they alter the weights such that the traces of forget set on the model weights would be minimized. Hence I believe this setting would ideally be more suitable for machine unlearning context. Speaking about the computational complexity of the gradient ascent method, I believe since the forget set cardinality is much less than the training set, the computational cost would not be very demanding and also it will be similar to finetuning language model on extremely low-resource scenarios.\n\n2) I think it is necessary to compare with a few prompt based adversarial attack methods such as [1] or related methods. As stated above, the presented approach queries the language model using a specific prompt designed to reduce the confidence of the model on a few training instances from the forget set. So an alternative way to view the proposed method (deviating from machine unlearning) is to query a language model with a prompt such that the likelihood of the forget set belonging to the training set is reduced. Thus, it is like learning the perturbations to the input prompts that change the model predictions. Thus learn for such perturbations to the prompts only for the forget set. I believe that it serves a crucial baseline.\n\n[1] Raman, Mrigank, et al. \"Model-tuning Via Prompts Makes NLP Models Adversarially Robust.\" arXiv preprint arXiv:2303.07320(2023). \n\n3) The presentation of the experiments section needs to be improved. For instance, i) Section 4.2 is redundant and is better explained in the ablations section. ii) In table 1, ICUL is mentioned without the query length (s) however it is mentioned with the query length in table 2. It is important to maintain consistency. Also it makes it more clear for the reader to mention what it means by ICUL (s) in the caption of the tables.\n\n4) Baselines are not clearly mentioned in the experiments section. For instance, GA is not clearly mentioned. Also when mentioning the \u2018Baseline\u2019 in section 5.2, \u201cconsists of the decision not to unlearn the point from the model\u201d. It is not clear what it means. I am assuming that the authors are referring to \"Compare train vs. held out samples on the initial model f\u03b8(S)\" in section 5.1. Maybe mentioning it directly where they first introduce will make it less confusing. \n\n5) The experiments section requires further elaboration. While the authors assert that their method outperforms the baselines, they should delve into why this is the case. For example, the claim that their method consistently surpasses the GA baseline is supported by empirical evidence however remains unexplained. Intuitively, the GA method, which uses gradient ascent to intentionally forget the 'forget set', should be on par with or even superior to the proposed method. Clarifying this would provide valuable insight."
                },
                "questions": {
                    "value": "1) The format for the incontext unlearning is as follows: \u201c[Forget Input] [Flipped Label] \\n [Input 1]1 [Label 1]1 \\n \u00b7 \u00b7 \u00b7 [Input s]s [Label s]s [Query Input]s+1 \u201d. My question is if I want to query on [Forget Input] again the format is as follows, \u201c[Forget Input] [Flipped Label] \\n [Input 1]1 [Label 1]1 \\n \u00b7 \u00b7 \u00b7 [Input s]s [Label s]s [Forget Input] \u201d. Is my understanding correct? If so, it would be interesting to know how many times the output of the language model is [Flipped Label]? We can understand the broader impact of such a querying by further knowing these dynamics.\n2) Also from the results, it is evident that the method performs at par with the GA method however it is not really clear why the GA method performs worse than the Baseline (especially the sudden spike towards 10^(-1) FPR for Amazon and Yelp). Could you explain why that is the case? We are optimizing by performing gradient ascent so it has to be below baseline at least?\n3) In section 4, it is mentioned that \u201cFor finetuning, we are using the standard causal language loss which encourages the model to predict the next token correctly\u201d. So when the models are fine-tuned on the downstream dataset, why is the result on the train set low, especially for ICUL(6) on SST-2 dataset (Table-2).\n4) Please correct the typos and ensure consistency in notation. For example, 'membership inference' is abbreviated in some instances, while in others, it's written out in full.\n5) The authors claim \"This finding challenges earlier research that argued label flipping of context examples had an insignificant impact on smaller LLMs \" in the conclusions. It would be more insightful if the authors can point out why is the case? Or if there is any assumption difference between these works and the proposed work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7822/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7822/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7822/Reviewer_fKNk"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7822/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698655274090,
            "cdate": 1698655274090,
            "tmdate": 1700594595477,
            "mdate": 1700594595477,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rbClP0EMDO",
                "forum": "5LhYYajlqV",
                "replyto": "ZSDglKzHH2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7822/Reviewer_fKNk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7822/Reviewer_fKNk"
                ],
                "content": {
                    "comment": {
                        "value": "Firstly, the method is heavily inspired from the paper \"Larger language models do in-context learning differently\". The authors propose that it works even for smaller models which challenges the observations of the earlier paper. However, it is not clear why it is the case. Were there any assumptions that were made in this paper that differed from the settings of the former paper?\n\nThough this paper is proposed as machine unlearning, I am not really convinced if it falls into the realms of machine unlearning since the model weights still preserve the information of the user. So information is never really lost. As I mentioned above, by changing the prompt you can always retrieve the information. \n\nSince my questions are not adequately answered, I am reducing the score from 6->5."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7822/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594583899,
                "cdate": 1700594583899,
                "tmdate": 1700594583899,
                "mdate": 1700594583899,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BZOaAIDgT8",
                "forum": "5LhYYajlqV",
                "replyto": "ZSDglKzHH2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7822/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7822/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to comment by Reviewer fKNk"
                    },
                    "comment": {
                        "value": "We are working on additional experiments and responses to address reviewer questions. We will be posting them very soon. Thanks for your patience."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7822/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637658987,
                "cdate": 1700637658987,
                "tmdate": 1700637676244,
                "mdate": 1700637676244,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "27aM7nLcwu",
                "forum": "5LhYYajlqV",
                "replyto": "ZSDglKzHH2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7822/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7822/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fKNk (Part 1)"
                    },
                    "comment": {
                        "value": "We are very grateful for the reviewer's positive and thoughful comments and believe that they have contributed to improving our work. Below we address individual points raised by the reviewer.\n\n**Querying with alternative prompts**\n\n> When queried using alternative prompts, there is no guarantee that the language model would perform with similar characteristics. \n\nThis is correct. However, recall that our methodology is tailored to the real-world constraints of 'ML as a Service' platforms. There, the model owner always prepends the required unlearning contexts in front of the queries since it is the premise that the LLM operates as black-box (see Figure 1). In such scenarios, where model parameters are inaccessible, and standard unlearning techniques relying on gradient ascent are infeasible, our approach of pre-appending required unlearning contexts becomes crucial. For example, a bank finetunes a GPT3.5 model through OpenAI\u2019s API for the task of deciding credit worthiness of customers. The bank would need to finetune the entire model from scratch when deletion requests come in since standard techniques like gradient ascent do require parameter access which we do not have when using the API. Hence, our technique is particularly useful when standard unlearning methods that operate via gradient descent on the model\u2019s parameters cannot be implemented as the lack of parameter access impedes traditional unlearning methods.\n\n\n**Computational aspects of unlearning**\n> Speaking about the computational complexity of the gradient ascent method, I believe since the forget set cardinality is much less than the training set, the computational cost would not be very demanding and also it will be similar to finetuning language model on extremely low-resource scenarios.\n\nContrary to the hypothesis regarding the computational aspects of unlearning, our experiments demonstrate the distinct memory requirements for gradient ascent (GA) and our proposed In-Context Unlearning (ICUL). In response to the reviewers hypothesis, we consider unlearning of sets of size 1 on the Llama2 7b model fine-tuned on the SST2 dataset (see Appendix C.1). Here, ICUL proves to be resource-efficient. Running ICUL on a single Tesla V100 GPU with 32GB of RAM contrasts sharply with the demands of gradient ascent (GA), which necessitates an A100 GPU with 80GB of RAM. \n\n**Prompt based adversarial attacks**\n\n> I think it is necessary to compare with a few prompt based adversarial attack methods such as [1] or related methods.\n\nWe appreciate the reviewer's insightful suggestion regarding alternative ICUL prompts. Indeed, our approach can be seen as a perturbation to the query point that reduces the likelihood of the query point belonging to the training set. We acknowledge that there may be other effective unlearning contexts. Unfortunately, due to resource constraints, we were unable to run the proposed method from [1] in addition to our new experiments (see Appendix C) as an alternative ICUL prompt. However, we thank the reviewer for this valuable suggestion and are committed to exploring adversarially perturbed prompts, as proposed in [1], in our future work. This aligns with our acknowledgment that there could be multiple routes to achieve unlearning, and we look forward to further investigations in this direction.\n\n**Presentation**\n> The presentation of the experiments section needs to be improved.\n\nThank you for this suggestion. We have updated the manuscript by moving section 4.2 to Section 5.4 where we conduct the sensitivity analysis. We also added three corresponding values for s that were used for ICUL in Table 1, one value of s for each dataset. \n\n\n**Correcting typos**\n> Please correct the typos and ensure consistency in notation. For example, 'membership inference' is abbreviated in some instances, while in others, it's written out in full.\n\nWe made sure that MI is used consistently throughout. Please let us know if the reviewer has found additional typos. \n\n**Mentioning baselines**\n> Baselines are not clearly mentioned in the experiments section. For instance, GA is not clearly mentioned [...]\n\nWe have made this more clear now. We have highlighted the abbreviation gradient ascent (GA) in the preamble of  section 5 again. We also introduce the term Baseline in Section 5.1 more clearly. Please refer to the text highlighted in green for the concrete section we made to Section 5."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7822/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679198745,
                "cdate": 1700679198745,
                "tmdate": 1700679198745,
                "mdate": 1700679198745,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9qnprTFl7G",
                "forum": "5LhYYajlqV",
                "replyto": "ZSDglKzHH2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7822/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7822/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fKNk (Part 2)"
                    },
                    "comment": {
                        "value": "**Further elaborations**\n> The experiments section requires further elaboration. While the authors assert that their method outperforms the baselines, they should delve into why this is the case. \n> Also from the results, it is evident that the method performs at par with the GA method however it is not really clear why the GA method performs worse than the Baseline (especially the sudden spike towards 10^(-1) FPR for Amazon and Yelp).\n\nWhen using GA, we have been strictly following the methodology proposed in [1]. We performed gradient ascent for one epoch using adamw on the forget set $S_f$ tuning learning rates in the range of [5e-05, 3e-05, 1e-05] as a hyperparameter, consistent with the approach in [1]. It is conceivable that using different optimizers would avoid the bump and could lead to improved unlearning results for GA.\n\n**Questions**\n\n> My question is if I want to query on [Forget Input] again the format is as follows, \u201c[Forget Input] [Flipped Label] \\n [Input 1]1 [Label 1]1 \\n \u00b7 \u00b7 \u00b7 [Input s]s [Label s]s [Forget Input] \u201d. Is my understanding correct? If so, it would be interesting to know how many times the output of the language model is [Flipped Label]? We can understand the broader impact of such a querying by further knowing these dynamics.\n\nYes, your understanding is correct! Also, the output flips in 1 to 5 percent of all cases. While the output is usually not flipped, it is the confidence in the true class that decreases. \n\n> In section 4, it is mentioned that \u201cFor finetuning, we are using the standard causal language loss which encourages the model to predict the next token correctly\u201d. So when the models are fine-tuned on the downstream dataset, why is the result on the train set low, especially for ICUL(6) on SST-2 dataset (Table-2).\n\nThis phenomenon is most pronounced on the smaller Bloom 560M model (3rd column in Table 2). The larger Bloom 1.1B model manages to use the correctly labeled examples from its context more effectively than the smaller 560M model (6th column in Table 2).\n\n> The authors claim \"This finding challenges earlier research that argued label flipping of context examples had an insignificant impact on smaller LLMs \" in the conclusions. It would be more insightful if the authors can point out why is the case? Or if there is any assumption difference between these works and the proposed work.\n\nIn contrast to the findings in [2], our research suggests that the performance of large language models (LLMs) may not be solely determined by their size. While [2] highlighted the tendency of smaller LLMs to neglect prompt context, emphasizing the superior context utilization of larger models, our work introduces a nuanced perspective. We argue that the capacity to selectively forget specific instances from the training set is not exclusive to larger models, thus challenging the notion that only increased model size allows for effective knowledge overwriting.\n\nFurther, these two works fundamentally differ in the questions they aim to answer. While both our work and the the work from [2] utilize label flipping, it's crucial to emphasize the divergence in our research objectives and evaluations. The commonality lies in the application of label flipping, but the motivations behind its use and the questions addressed are markedly different. In [2], the authors employ *random label flipping* primarily as an evaluation technique to study the impact of label flipping itself on model predictions. In contrast, we propose *label flipping of specific points targeted for removal* as an unlearning procedure and subsequently employ a combination of sample splitting and Membership Inference Attacks (MI Attacks) for our evaluation. This nuanced approach allows us to make specific claims about the unlearning capabilities of ICUL. In summary, their focus on the impact of label flipping do not facilitate claims about unlearning success. Our methodology, centered around unlearning through label flipping and the subsequent evaluations, provides a distinct angle that contributes to the broader understanding of how unlearning can be achieved in-context.\n\n----\n\n**References**\n\n[1] Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. Knowledge unlearning for mitigating privacy risks in language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), 2023\n\n[2] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv:2303.03846, 2023"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7822/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679730654,
                "cdate": 1700679730654,
                "tmdate": 1700727392098,
                "mdate": 1700727392098,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZlWvA7Vrk6",
            "forum": "5LhYYajlqV",
            "replyto": "5LhYYajlqV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7822/Reviewer_Awru"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7822/Reviewer_Awru"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an innovative method called \"In-Context Unlearning\" (ICUL) designed for large language models (LLMs). ICUL facilitates the unlearning of specific training instances by providing contextual inputs at the inference stage, thereby eliminating the need for modifying model parameters. Experiments confirm that ICUL can effectively remove targeted training information while maintaining performance levels."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper brings a novel approach to the domain of machine unlearning, effectively leveraging the In-Context Learning (ICL) paradigm to achieve unlearning in text classification tasks without any modification to model parameters.\n* The design of ICUL requires minimal computational resources, offering a cost-effective alternative to traditional unlearning methods that involve model retraining.\n* The paper includes a thorough ablation study, which affirms the effectiveness of the key components in ICUL, specifically label flipping and the addition of unforgotten samples."
                },
                "weaknesses": {
                    "value": "* The experiments are conducted only on Bloom's 560M and 1.1B models, lacking comparison with newer, larger models like LLaMA, Falcon, or ChatGPT. Moreover, ICUL is compared to only a single benchmark method (GA), which in some settings even outperforms ICUL, making it difficult to assert ICUL's superiority.\n* The method is tailored for binary classification tasks and does not easily extend to multi-class or more complex NLP tasks. The paper also falls short in clarifying the real-world scenarios and problems it aims to address, leaving its practical utility ambiguous.\n* The method's design, focusing on unlearning single samples in classification tasks, makes it less suitable for handling large volumes of unlearning requests in realistic scenarios.\n* The paper doesn't delve into the theoretical underpinnings that explain why ICUL is effective at unlearning, leaving a gap in our understanding of the method's robustness.\nThe paper fails to compare or reference highly related work in the area of concept erasure in NLP, e.g., 1,2,3.\n\n1. Shauli Ravfogel, Francisco Vargas, Yoav Goldberg, Ryan Cotterell. Adversarial Concept Erasure in Kernel Space. EMNLP 2022\n2. Shauli Ravfogel, Michael Twiton, Yoav Goldberg, Ryan Cotterell. Linear Adversarial Concept Erasure. ICML 2022\n3. Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, Stella Biderman. LEACE: Perfect linear concept erasure in closed form."
                },
                "questions": {
                    "value": "How is ICUL designed to scale for real-world applications that demand continuous unlearning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7822/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7822/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7822/Reviewer_Awru"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7822/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767098068,
            "cdate": 1698767098068,
            "tmdate": 1699636957683,
            "mdate": 1699636957683,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xAclP61zjg",
                "forum": "5LhYYajlqV",
                "replyto": "ZlWvA7Vrk6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7822/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7822/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Awru (Part 1)"
                    },
                    "comment": {
                        "value": "We are grateful for the reviewer's critical and thoughful comments and believe that they have substantially contributed to improving our work. Below we address individual points raised by the reviewer.\n\n**Single benchmark**\n\n> [...] ICUL is compared to only a single benchmark method (GA) [...]\n\n1) *Current SOTA unlearning methods rely on the Hessian and do not apply to LLMs*: Unlearning training points in Language Models (LLMs) is an emerging area, and the limited availability of viable alternatives, exemplified by [2], underscores the current landscape. The primary reason for this scarcity is rooted in the design constraints of existing methods. Many prevalent approaches, both exact and approximate, necessitate the computation and inversion of the Hessian matrix ([6,7]). However, for LLMs, the computation of the Hessian and its inverse become computationally infeasible. Notably, the alternative used by [2], as suggested in [8], leverages gradient ascent on the points targeted for deletion, avoiding the need for Hessian matrix computations.\n\n2) *ICUL is close to the optimal performance*: In terms of unlearning efficacy, it's crucial to highlight that our method achieves performance comparable or extremely close to the optimal scenario of randomly guessing whether the forgotten point is still part of the training set (refer to Figure 3).Furthermore, concerning overall model performance, both Gradient Ascent (GA) and ICUL exhibit similar performance for the larger Bloom 1.1B LLM. Both methods showcase classification accuracy that is remarkably close to the Baseline, where no unlearning has occurred. This parity underscores the effectiveness of ICUL in preserving model performance despite unlearning.\n\n3) *No points of comparison*: Furthermore, our suggested approach is fundamentally different from any other unlearning mechanism in literature. As opposed to [2] and the additional works that the reviewer suggested for comparison [3-5], our work does not require updating of model parameters. Further, and more fundamentally, [3-5] focus on unlearning (human defined) concepts, while our work focuses on unlearning individual points, and hence [3-5] are not applicable in our work.\n\n\n**Comparison with newer and larger language models**\n\n> The experiments are conducted only on Bloom's 560M and 1.1B models, lacking comparison with newer, larger models like LLaMA, Falcon, or ChatGPT [...]\n\nAddressing the suggestion from the reviewer and to underscore the general applicability of our method across various large language models, we conducted additional experiments to evaluate the performance of our unlearning algorithm in forgetting points from the SST2 dataset additionally using Pythia 1B and LLAMA2 7B. For these experiments, we employed the same methodology for forgetting points as previously described in Section 4 of our paper. These additional results are depicted in Appendix C1 and show that ICUL successfully forgets points across different language model architectures. The results for GA are still running and will be included in the plots as soon as they are complete. \n\n**Forgetting multiple points** \n\n>The method's design, focusing on unlearning single samples in classification tasks, makes it less suitable for handling large volumes of unlearning requests in realistic scenarios.\n\nOur method is suitable for larger volumes of unlearning requests. Addressing the suggestion from the reviewer and to underscore the general applicability of our method across multiple deletion requests, we conducted additional experiments to evaluate the performance of our unlearning algorithm in forgetting  2, 4  and 10 points from the SST2 dataset using the 1.1B Bloom model. We employed the same methodology for forgetting multiple points as previously described in Section 4 of our paper. Specifically, we construct prompts by first flipping the labels on the designated points for forgetting (i.e., 2, 4 or 10), followed by the inclusion of additional correctly labeled examples. The results presented in Appendix C.2 of the updated manuscript confirm that ICUL maintains forgetting efficacy in this extended scenario. We are committed to add experimental results on all remaining models (Bloom 560M, Bloom 1.1B, Pythia 1B and Llama2 7B) and datasets (Amazon and Yelp) once they will finish running."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7822/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683362125,
                "cdate": 1700683362125,
                "tmdate": 1700683362125,
                "mdate": 1700683362125,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vY6QRkxgPF",
                "forum": "5LhYYajlqV",
                "replyto": "ZlWvA7Vrk6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7822/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7822/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Awru (Part 3)"
                    },
                    "comment": {
                        "value": "**Practicality**\n> The paper also falls short in clarifying the real-world scenarios and problems it aims to address, leaving its practical utility ambiguous.\n\nOur methodology is tailored to the real-world constraints of 'ML as a Service' platforms. There, the model owner always prepends the required unlearning contexts in front of the queries since it is the premise that the LLM operates as black-box (see Figure 1). In such scenarios, where model parameters are inaccessible, and standard unlearning techniques relying on gradient ascent are infeasible, our approach of pre-appending required unlearning contexts becomes crucial. For example, a bank finetunes a GPT3.5 model through OpenAI\u2019s API for the task of deciding credit worthiness of customers. The bank would need to finetune the entire model from scratch when deletion requests come in since standard techniques like gradient ascent do require parameter access which we do not have when using the API. Hence, our technique is particularly useful when standard unlearning methods that operate via gradient descent on the model\u2019s parameters cannot be implemented as the lack of parameter access impedes traditional unlearning methods.\n\n> The method is tailored for binary classification tasks and does not easily extend to multi-class or more complex NLP tasks. \n\nThanks for this suggestion. In this work, we have outlines the fundamental mechanism to unlearn in-context using binary classification problem. We are commited to extending our method beyond the binary case in follow-up work.\n\n\nWe thank the reviewer again for their thoughtful comments and feedback. We hope we addressed all your questions/concerns/comments adequately. In light of our clarifications, please consider increasing your score.\n\n---\n**References**\n\n[1] Biderman et al (2023, \u201cPythia: A Suite for Analyzing Large Language Models Across Training and Scaling\u201d, Proceedings of the 40 th International Conference on Machine Learning (ICML), 2023 \n\n[2] Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. Knowledge unlearning for mitigating privacy risks in language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), 2023\n\n[3] Shauli Ravfogel, Francisco Vargas, Yoav Goldberg, Ryan Cotterell. Adversarial Concept Erasure in Kernel Space. EMNLP 2022\n\n[4] Shauli Ravfogel, Michael Twiton, Yoav Goldberg, Ryan Cotterell. Linear Adversarial Concept Erasure. ICML 2022\n\n[5] Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, Stella Biderman. LEACE: Perfect linear concept erasure in closed form.\n\n[6] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Forgetting outside the box: Scrubbing deep networks of information accessible from input-output observations. arXiv:2003.02960, 2020b.\n\n[7] Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember what you want to forget: Algorithms for machine unlearning. In Advances in Neural Information Processing Systems, 2021\n\n[8] Seth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi. Descent-to-delete: Gradient-based methods for machine unlearning. In Proceedings of the 32nd International Conference on Algorithmic Learning Theory (ALT), 2021."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7822/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685004389,
                "cdate": 1700685004389,
                "tmdate": 1700733972846,
                "mdate": 1700733972846,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CoD2piuekC",
            "forum": "5LhYYajlqV",
            "replyto": "5LhYYajlqV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7822/Reviewer_poKX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7822/Reviewer_poKX"
            ],
            "content": {
                "summary": {
                    "value": "This paper looks at the problem of machine unlearning in the context of LLM. In particular, the authors look at in-context unlearning, where the forget example is fed into the model context window with a flipped sign, and no gradient update is necessary. The authors show that the proposed method is competitive regarding unlearning effectiveness and accuracy on the retain set."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper poses a very interesting problem and it can motivate future works. The paper is written nicely and it is easy to follow."
                },
                "weaknesses": {
                    "value": "I found the particular setting in this paper to be a little restrictive (but it's not crucial since this paper is almost initiating a new setting) that only one example is forgetting."
                },
                "questions": {
                    "value": "1. In eq(2), what is $\\hat \\theta$, does this mean in different scenarios, you test with different estimators, e.g. the ERM $\\theta(S)$ and the unlearned model $\\bar f$?\n2. When you actually perform the LRT in eq(2), do you take each (x,y) as one test samples?\n3. When you fine-tune with GA, what are the stopping criteria? Do you have a suggestion on some rule-of-thumb?\n4. One related prior work should be included and discussed, see [1].\n\n[1] KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7822/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805208072,
            "cdate": 1698805208072,
            "tmdate": 1699636957548,
            "mdate": 1699636957548,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6P4IrY6H4T",
                "forum": "5LhYYajlqV",
                "replyto": "CoD2piuekC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7822/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7822/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer poKX"
                    },
                    "comment": {
                        "value": "We are very grateful for the positive comments from the reviewer, which highlight the novelty of our suggested unlearning method. Below we address individual points raised by the reviewer.\n\n**Forgetting multiple points**\n\n>I found the particular setting in this paper to be a little restrictive (but it's not crucial since this paper is almost initiating a new setting) that only one example is forgetting.\n\nAddressing the suggestion from the reviewer and to underscore the general applicability of our method, we conducted additional experiments to evaluate the performance of our unlearning algorithm in forgetting 2, 4 and 10 points from the SST2 dataset using the 1.1B Bloom LLM. We employed the same methodology for forgetting multiple points as previously described in Section 4 of our paper. Specifically, we construct prompts by first flipping the labels on the designated points for forgetting (i.e., 2, 4 or 10), followed by the inclusion of additional correctly labeled examples. The results presented in Appendix C.2 of the updated manuscript confirm that ICUL maintains its strong forgetting efficacy in this extended scenario. We are committed to add experimental results on all remaining models (Bloom 560M, Bloom 1.1B, Pythia  1B and Llama2 7B) and datasets (Amazon and Yelp) once all these experimental runs are complete. \n\n\n**Approximating the likelihood ratio in equation  (2)**\n\n>In eq(2), what is $\\hat{\\theta}$, does this mean in different scenarios, you test with different estimators, e.g. the ERM and the unlearned model ?\n\n$\\hat{\\theta}$: This was a typo and should have been replaced by $\\bar{\\theta}$, the unlearned model as the nominrator represents the distribution of model losses after unlearning. To be more specifc, to approximate the univariate distributions in the numerator and denominator of equation (2), we employ a sample-splitting approach. Specifically, we fine-tune models on sub-sampled datasets that either include or exclude $S_f$. For the numerator, we apply $\\mathcal{U}$ to unlearn $S_f$ on datasets containing it, followed by computing the updated model\u2019s loss on $S_f$. For the denominator, we compute the losses on $S_f$ for models not trained on $S_f$. We have made this more clear by slightly rewriting Section 3.2.\n\n\n>When you actually perform the LRT in eq(2), do you take each (x,y) as one test samples?\n\nGiven our use of shadow models as outlined above, to approximate the likelihood ratio, each (x,y) is anticipated to serve as a test sample in half of the shadow models and as a training sample in the remaining half.\n\n\n**Gradient Ascent on the Forget set**\n\n> When you fine-tune with GA, what are the stopping criteria? Do you have a suggestion on some rule-of-thumb?\n\n Using the methodology proposed in [2], we performed gradient ascent for one epoch on the forget set $S_f$ tuning learning rates as a hyperparameter, consistent with the approach in [2]. Hence, GA terminates after 1 epoch. The comprehensive results of this hyperparameter search are available in the Appendix, with a focus on presenting the strongest results in the main paper.\n\n**Updated related work**\n\n> One related prior work should be included and discussed, see [1].\n\nThanks for suggesting [3] as related work. The manuscript has been revised accordingly, with [3] now appropriately cited within the related work section.\n\n\nWe thank the reviewer for their thoughtful comments and feedback. We hope we addressed all your questions/concerns/comments adequately. In light of our clarifications, please consider increasing your score to accept.\n\n---\n\n**References**\n\n[1] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership inference attacks from first principles. In 2022 IEEE Symposium on Security and Privacy (SP), pages 1897\u20131914. IEEE, 2022\n\n[2] Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. Knowledge unlearning for mitigating privacy risks in language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), 2023.\n\n[3] KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment.  In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL), 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7822/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677045669,
                "cdate": 1700677045669,
                "tmdate": 1700734030168,
                "mdate": 1700734030168,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]