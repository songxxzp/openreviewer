[
    {
        "title": "Multi-Vision Multi-Prompt for Few-Shot Learning in Vision-Language Model"
    },
    {
        "review": {
            "id": "HDyUdEzb2U",
            "forum": "j1FLTvgyAh",
            "replyto": "j1FLTvgyAh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1541/Reviewer_RSHG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1541/Reviewer_RSHG"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the Multi-Vision Multi-Prompt (MVMP) method for few-shot learning in vision-language models such as CLIP. MVMP employs prompts of varying detail levels throughout training, utilizes a mixed self-augmentation framework to boost image diversity, and applies text distillation using multiple text features to enhance image-text alignment. Experimental results indicate that MVMP offers notable improvements compared to existing methods in few-shot environments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The method consistently delivers great improvements across different tests, supported by comprehensive experimentation that showcases its effectiveness.\n\nThe enhancements achieved by the approach are multi-dimensional. By adjusting text prompts at distinct training stages, it is designed to gradually learn and adapt to a myriad of features. This systematic progression ensures that the model captures both broad and nuanced details effectively.\n\nBeyond text prompt adjustments, the method places a significant emphasis on diversifying the dataset. This is achieved by strategies aimed at enriching both image and text data, ensuring a balanced and diverse training environment that contributes to the model's performance."
                },
                "weaknesses": {
                    "value": "There's ambiguity regarding the benchmarking of the proposed methods. It's essential to ensure that each method has been equitably compared to provide a clear understanding of its efficacy.\n\nFigure 6 indicates that a score of 73.8 is achieved by integrating multi-prompt, multi-vision, and text diversity. However, Table 2 seems to compare this with CLIP combined with other mix-up methods along, which would be more appropriate to compare it with just CLIP + multi-vision. According to Figure 6, multi-vision alone offers a mere 0.5% improvement. Given the renowned effectiveness of mix-up methods as data augmentation, it's not evident if the inclusion of multi-vision is truly indispensable. Moreover, the approach bears notable resemblance to mix-up, with distinctions primarily in implementation specifics.\n\nRegarding multi-prompt, it might be more insightful if it was contrasted with a baseline where diverse prompts are introduced simultaneously across training stages. Introducing different prompts at separate stages complicates the algorithm, necessitating elements like a prompt bank. At present, it's challenging to discern the imperative of such staged or \"curriculum\" learning.\n\nThe paper's primary arguments seem to combine various methods from distinct directions. However, there's a lack of explicit justification underscoring the necessity and efficiency of each technique, particularly when benchmarked against its direct counterparts. Comparing with prior state-of-the-art methods doesn't necessarily bolster the paper's credibility, especially given the significant differences in implementation and methodologies."
                },
                "questions": {
                    "value": "For Table 6, can we show CLIP +Multi-Vision; CLIP + Text Diversity additionally? \nAnd then, for CLIP + Multi prompt; CLIP +Multi-Vision; CLIP + Text Diversity, they should be individually benchmarked with their own baselines."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1541/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1541/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1541/Reviewer_RSHG"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1541/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698608695585,
            "cdate": 1698608695585,
            "tmdate": 1699636082616,
            "mdate": 1699636082616,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "HwORItAXmr",
            "forum": "j1FLTvgyAh",
            "replyto": "j1FLTvgyAh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1541/Reviewer_bsQr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1541/Reviewer_bsQr"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into the challenges of few-shot learning in vision-language models, particularly the Contrastive Language-Image Pre-Training model (CLIP). Recognizing that previous prompt-learning techniques typically depend on a singular prompt, which may not adequately differentiate between categories with diverse features and contexts, the authors introduce a novel method named Multi-Vision Multi-Prompt (MVMP). Designed specifically for CLIP in few-shot scenarios, MVMP avoids increasing model parameters by employing multiple prompts at various training phases and averages the outcomes. The study also introduces a combined self-augmentation framework and text distillation to boost the model's efficacy. Through rigorous experimentation, it's demonstrated that the MVMP method notably surpasses existing standards in few-shot learning classification tasks, enhancing accuracy rates by 4.6% and 2%."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. An intuitive perspective to improve the inference capabilities of CLIP-like vision-language models.\n2. Good empirical results. The new method demonstrates superior performance than its baselines in this paper."
                },
                "weaknesses": {
                    "value": "Overall, the contribution of this paper is quite limited. The two basic components, i.e., multi-view augmentation and multiple prompts, of the newly introduced method have actually been explicitly explored in the community, yet this paper does not yield significant improvements with its simple and straightforward design. The weaknesses are detailed below:\n\n1. The design of feeding multiple prompts into text encoder has long been a conventionally accepted method for zero-shot vision-language inference. For example, the original CLIP paper leverages 80 different text prompts for ImageNet inference. This technique is generally termed as prompt engineering but this paper lacks related discussions about that.\n2. The mechanism of introducing text prompts for multiple transformer layers is also confusing. I understand multi-layer prompting might facilitate extracting multi-scale visual features for the image encoder, but why also applying it to text encoders? The paper does not give an explanation or discussion of this design, and it seems that the performance gain is merely because of introducing additional parameters.\n3. Some important details are also missing. How are the prompt tokens processed in the transformers? Are prompts introduced in the i-th layer simply pruned after this layer, or are they retained until the last layer? These two protocols do make difference.\n4. The Mixed Self-Augmentation looks more like an additional trick to enhance performance. This augmentation does not exhibit too much relation to your prompting framework, i.e., even without this augmentation, your method can achieve comparable performance, and when equipped with it, your baseline models also seem to be able to obtain the same level of improvements.\n5. Some baseline performances might be underestimated. For example, why CLIP's test accuracy on ImageNet is only 32.1%? In CLIP's original paper it obtains 63.2% and 68.6% zero-shot accuracy with ViT-Base/32 and ViT-Base/16, respectively.\n6. Comparisons are unfair. You method has both data augmentation and prompt learning but every baseline only has one of them. You should at least give ablation results of your method without augmentation/prompting or baselines with both sophisticated augmentation and prompting.\n7. Minor comments: the paper is not well-written and hard to understand; many grammatical errors and imprecise expressions in the paper; In figure 6, the table should not be screenshotted to an image, but there are many ways to place images and tables side by side in Latex."
                },
                "questions": {
                    "value": "See \"Weaknesses\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1541/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1541/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1541/Reviewer_bsQr"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1541/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698617894750,
            "cdate": 1698617894750,
            "tmdate": 1699636082524,
            "mdate": 1699636082524,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "KXrpg4ZjZ2",
            "forum": "j1FLTvgyAh",
            "replyto": "j1FLTvgyAh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1541/Reviewer_gxiR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1541/Reviewer_gxiR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes MVMP, a multi-prompt method for CLIP in few-shot learning. The authors propose an image augmentation technique to diversify the visual samples, a text distillation technique to enhance textual features, and lastly, a Gaussian weight averaging for aggregating multilayer prompts learned from different network levels. Experiments show MVMP performs well on the 1-shot adaptation setting."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- This paper presents an easy-to-understand method that is especially effective on fewer-shot CLIP-based adaptation tasks.\n- The experimental performance on the 1-shot setting is good and consistent."
                },
                "weaknesses": {
                    "value": "- The paper writing and structuring can be substantially improved. \n- All argument referencing Figure 2 is weak as they are not a direct reflection of the proposed method in this paper. \n- Figure 4 is confusing, readers are unable to tell frozen parameters versus learnable parameters. \n- The proposed image augmentation, i.e. mixup regions of image cross samples, is not well motivated and does not make much sense. The ablation result in Figure 6 also does not justify the necessity. \n- Gaussian weighting for high-layer prompts seems like a fixed heuristic with no technical novelty.\n- The results for cross-dataset benchmark evaluation are underwhelming, with the average accuracy higher than MaPLe by merely 0.2%."
                },
                "questions": {
                    "value": "Please refer to the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1541/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1541/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1541/Reviewer_gxiR"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1541/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699100720297,
            "cdate": 1699100720297,
            "tmdate": 1699636082431,
            "mdate": 1699636082431,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "1IAUhjl3Bt",
            "forum": "j1FLTvgyAh",
            "replyto": "j1FLTvgyAh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1541/Reviewer_ihSe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1541/Reviewer_ihSe"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method called Multi-Vision Multi-Prompt (MVMP) for few-shot learning in vision-language models. The authors claim that MVMP is the first approach to employ multiple prompts and a mixed self-augmentation framework to enhance the performance of CLIP in few-shot learning. The experimental validation shows that MVMP significantly outperforms the state-of-the-art methods in few-shot learning classification tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper addresses an important problem in few-shot learning, which is the limited availability of labeled data. The proposed method, MVMP, is a novel approach that employs multiple prompts and a mixed self-augmentation framework to enhance the performance of CLIP in few-shot learning."
                },
                "weaknesses": {
                    "value": "- There remain many unclear points regarding the proposed method.\n  - What do $P_{fixed}$ and $P_t$ refer to? In other words, where do you obtain $P_{fixed}$ and $P_t$? Are $P_t$ and $P_{w}^{(e)}$ the same in the context? These notations are very confusing without clear explanations.\n  - What does evaluation parameter $\\epsilon_e$ mean? What is the formula to calculate it?\n  - What does $H(x)$ mean in equation (8)? I assume you are referring to image features; however, it is somewhat challenging to interpret this notation if it's being introduced for the first time. \n  - Is there any explanation why enforcing the consistency loss between original images and mixed images benefits few-shot learning?\n  - Similarly for the text distillation loss? Please do not repeat the sentences mentioned in your paper in your rebuttal, since they do not provide any insights towards understanding your method.\n\n- The conclusion in Figure 5 is that leveraging 5 prompts works the best. Here comes a question: how do you select the 5 prompts given so many combinations among hand-designed prompts?\n\n- The paper claims that their approach doesn't introduce additional parameters or escalate computational costs. The question that arises is whether employing more prompts results in an increase in computational expenses. Nevertheless, the authors do not provide any comparison of computational costs between their method and other approaches.\n\n- There are some other state-of-the-art few-shot approaches [1] missing for comparison, where the results are much better.\n\n- Poor writing. \n  - While the performance of few-shot learning can improve through meta-learning -> While the performance of few-shot learning can be improved through meta-learning\n  - While the performance of few-shot learning can improve through meta-learning or image augmentation strategies, these approaches may increase computational cost and affect accuracy. This sentence is self-contradictory. You mentioned that the few-shot performance could be improved through those strategies, however, you later state that they affect accuracy. \n  - improving accuracy by 4.6% and 2%. What do you mean?\n  - our method improves overall accuracy by 2% to 4.6% -> our approach improves the overall accuracy by a range of 2% to 4.6%.\n\n\n[1] Exploiting Category Names for Few-Shot Classification with Vision-Language Models"
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1541/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699118236734,
            "cdate": 1699118236734,
            "tmdate": 1699636082343,
            "mdate": 1699636082343,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]