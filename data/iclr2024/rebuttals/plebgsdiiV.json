[
    {
        "title": "Kernel Metric Learning for In-Sample Off-Policy Evaluation of Deterministic RL Policies"
    },
    {
        "review": {
            "id": "L0h1TXsIRy",
            "forum": "plebgsdiiV",
            "replyto": "plebgsdiiV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4400/Reviewer_pubB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4400/Reviewer_pubB"
            ],
            "content": {
                "summary": {
                    "value": "The paper extends in-sample OPE methods to deterministic target policies by using the kernel approximation. The paper calculates the bias and variance of the estimation error resulting from this relaxation and present analytical solutions for the ideal kernel metric. Through empirical study, it demonstrate superior performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is easy to follow, solid, and studies an important problem"
                },
                "weaknesses": {
                    "value": "The novelty is marginal, as the key components (such as kernel relaxation, in-sample learning, and metric learning) are standard. The main contribution is on the theoretical derivation."
                },
                "questions": {
                    "value": "It would help to comment on how to extend the method to policy learning and discuss recent words e.g. \"Singularity-aware Reinforcement Learning\" and \"Policy learning \"without'' overlap: Pessimism and generalized empirical Bernstein's inequality\". \n\nBesides, as your key contribution, it might help to highlight how you do metric learning (currently it takes efforts to find)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4400/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4400/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4400/Reviewer_pubB"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4400/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698603884069,
            "cdate": 1698603884069,
            "tmdate": 1700593467785,
            "mdate": 1700593467785,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FkaNqgl9Rt",
                "forum": "plebgsdiiV",
                "replyto": "L0h1TXsIRy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4400/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4400/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer $\\text{\\textcolor{red}{pubB}}$"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to provide thoughtful feedback on our work. Please let us know if any of our responses below need further clarification. \n\n$ $\n\n**W1. Novelty of our work**\n\nPerforming in-sample off-policy evaluation (OPE) of a deterministic policy is challenging because in-sample OPE necessitates importance sampling (IS), while the IS ratios are zeros for a deterministic target policy. To address this issue, we utilized kernel relaxation and metric learning, which have not been used for OPE  in MDP settings. Our novelty lies in the integration of kernel relaxation, metric learning, and in-sample OPE to address the issue. To integrate the components, we derived the MSE of the in-sample estimated Q update vector regarding kernel relaxation and metric learning in MDP settings. To the best of our knowledge, our method is the first in-sample OPE algorithm for evaluating deterministic target policies. We believe our work\u2019s novelty is also in creating the theoretical guarantee of the proposed method.\n\n\n$ $\n\n**Q1. How to extend our method to policy learning**\n\nExtending our method to policy improvement (PI) is our future work. Naive application of our work on PI will result in an actor-critic algorithm for offline RL where we evaluate the target policy with the critic learned by our method and improve policy by some PI methods such as the one used by TD3+BC [Fujimoto et al., 2021].\n\nThe reviewer referred to the papers that have not been published yet. However, in considering how to extend our work to PI regarding those works, work by Jin et al. [Jin et al., 2023] is an offline contextual bandit policy learning algorithm that works on discrete action spaces and tries to learn the policy that has the highest Q-estimation while avoiding learning policy that has high uncertainty (or variance) in the Q-estimation. The variance comes from a large IS ratio due to small behavior policy density values. Because our method also uses IS, we may adopt their method when we expand our work to offline RL. The work by Chen et al. [Chen et al., 2023] is an offline policy iteration algorithm that works on infinite horizon MDPs with continuous states and actions. Upon analysis of an OPE error, they decomposed the error into two parts using Lebesgue\u2019s decomposition theorem: 1) absolutely continuous part w.r.t. data distribution where importance sampling can be applied, 2) singular part (e.g., Dirac measure, deterministic target policy). For the singular part, they used maximum mean discrepancy to make the upper bound of the OPE error. Since our method is developed for OPE of deterministic policies, our method may be integrated into the singular part and be used for PI.\n\n$ $\n\n[Fujimoto et al., 2021] Fujimoto et al. \u201cA minimalist approach to offline reinforcement learning\u201d NeurIPS 2021.\n\n[Jin et al., 2023] Jin et al. \u201cPolicy learning \u201cwithout\u201d overlap: Pessimism and generalized empirical Bernstein\u2019s inequality\u201d Arxiv 2023.\n\n[Chen et al., 2023] Chen et al. \u201cSTEEL: Singularity-aware reinforcement learning\u201d. ArXiv 2023.\n\n\n\n$ $\n\n**Q2. The detailed procedure on metric learning**\n\nTo clear up how metric learning is done in our method, we explicitly state in the last paragraph of section 4.2 that the pseudo-code of our algorithm in Algorithm 1 in Appendix B, and modified Algorithm 1 to contain more details on metric learning."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4400/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700087068995,
                "cdate": 1700087068995,
                "tmdate": 1700296826282,
                "mdate": 1700296826282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NVnCLY4uwq",
                "forum": "plebgsdiiV",
                "replyto": "FkaNqgl9Rt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4400/Reviewer_pubB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4400/Reviewer_pubB"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I find it helpful. I raised my score, conditioned on the discussions on novelty and policy optimization being incorporated to the manuscript."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4400/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593536520,
                "cdate": 1700593536520,
                "tmdate": 1700593536520,
                "mdate": 1700593536520,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0Hhm5gNANo",
            "forum": "plebgsdiiV",
            "replyto": "plebgsdiiV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4400/Reviewer_bgFP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4400/Reviewer_bgFP"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Kernel Metric learning for In-sample Fitted Q Evaluation (KMIFQE), for off-policy evaluation of deterministic target policies in in-sample learning in continuous control tasks. The issue with the evaluation of a deterministic policy is that the importance sampling ratio, a component used in the evaluation, is almost zero. To fix this, KMIFQE learns a Gaussian kernel for the target policy and applies the kernel relaxation to the deterministic target policy, to avoid the zero in the importance sampling ratio."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper clearly defines the research question, which is solving issues in the off-policy evaluation for a deterministic policy when applying in-sample learning. \n\n- The theoretical part is sound. The paper provides the error bound of the value estimation and mathematically analyzes the bias and variance of the kernel relaxation. The optimal bandwidth of the kernel is also derived, for balancing the bias and variance. The theoretical results about the bias, variance, and optimal bandwidth are empirically checked as well.\n\n- The experiment result section includes a visualization of the estimated Q value in a low-dimensional toy environment. This helps the reader to understand how the new method works, and I appreciate that."
                },
                "weaknesses": {
                    "value": "I have questions about the significance of the paper and the experiment setting. Please see the questions below. I would be happy to change my score after our discussion."
                },
                "questions": {
                    "value": "- I would appreciate it if the authors could explain more on the significance of the research question. The paper indicates the method is designed for continuous control tasks. In continuous control tasks, it is common to see an agent learn a stochastic policy instead of a deterministic policy, as many of the reinforcement learning algorithms suitable for continuous control tasks can work with a stochastic policy (BCQ, IQL, InAC, IAC, etc.), and it is also a simple and straightforward solution to avoid the zero importance sampling ratio. I think the importance of solving the issue in deterministic policy evaluation can be clearer if the paper can include an explanation of when people suffer from this issue in practice.\n\n- Another question is about the experiment setting. When proposing the new method, the paper indicates that the method is for in-sample learning. However, in the experiment section, the target policy is trained by TD3. The behavior policy is different from the target policy and does not guarantee to be in-sample (the target policy is a stochastic policy generated from another TD3 learned policy, whose performance is 70%~80% of the target policy, according to the paper). It is not clear to me how the in-sample condition is ensured in the experiment."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4400/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4400/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4400/Reviewer_bgFP"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4400/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779361766,
            "cdate": 1698779361766,
            "tmdate": 1700095248494,
            "mdate": 1700095248494,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D2OE9iV7Ae",
                "forum": "plebgsdiiV",
                "replyto": "0Hhm5gNANo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4400/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4400/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer $\\text{\\textcolor{blue}{bgFP}}$"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to provide valuable feedback on our work. Please let us know if any of our responses below need further clarification. \n\n$ $\n\n**Q1. The practicality of evaluating a deterministic policy and its example**\n\nPlease note that our algorithm is not an offline RL algorithm that can choose the form of the target policy (stochastic/deterministic) but an algorithm for evaluating a given deterministic policy.\n\nWe focused on evaluating deterministic target policies since there are many cases where the evaluation of deterministic policies is needed, e.g., safety-critical systems such as industrial robot control and drug prescription, where the sequences of actions need to be precisely and consistently controlled without introducing variability [Silver et al., 2014][Kallus and Zhou, 2018]. In these situations, one may want to evaluate the performance of the deterministic policies based on offline data when the robot control or drug prescription policy is updated before deployment.\n\nWe included an explanation of the practicality of our problem setting and an example in the second paragraph of the introduction section.\n\n\n$ $\n\n[Silver et al., 2014] Silver et al. \u201cDeterministic Policy Gradient Algorithms\u201d ICML 2014.\n\n[Kallus and Zhou, 2018] Kallus and Zhou. \"Policy Evaluation and Optimization with Continuous Treatments..\" AISTATS 2018.\n\n$ $\n\n**Q2. How in-sample learning is conducted in the experiments?**\n\nPlease note that in-sample learning does not mean on-policy learning. In-sample learning denotes the method that learns the value function by only using actions in the (off-policy) dataset [Xu et al., 2023][Kostrikov et al., 2022]. Even though the stochastic behavior policy and the deterministic target policy differ, it is in-sample learning if the actions used in the Q updates are in the dataset. Our method updates the Q-function with the update vector estimated only using the samples in the data following Eq.(6). Detailed procedure of our algorithm used in the experiments is presented in Algorithm 1 in Appendix B. Our algorithm works when the support of a deterministic target policy relaxed by a kernel is in the support of a stochastic behavior policy, and the experiment setting is made to satisfy the condition.\n\nTo clear up how our algorithm does in-sample learning, we explicitly state in the main text that the pseudo-code of our algorithm is in Algorithm 1 in Appendix B. Furthermore, we modified Algorithm 1 to explain how the samples in the data are used to learn Q-function.\n\n$ $\n\n[Xu et al., 2023] Xu et al., \"Offline RL with no OOD actions: In-sample learning via implicit value regularization.\" ICLR 2023.\n\n[Kostrikov et al., 2022] Kostrikov et al., \"Offline reinforcement learning with implicit q-learning.\" ICLR 2022."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4400/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700085074736,
                "cdate": 1700085074736,
                "tmdate": 1700085288473,
                "mdate": 1700085288473,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CeX9I7iU83",
                "forum": "plebgsdiiV",
                "replyto": "D2OE9iV7Ae",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4400/Reviewer_bgFP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4400/Reviewer_bgFP"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "I would like to thank the author for a detailed explanation in their reply and the revised submission. After reading the response and other reviews, both the method and the contribution are more clear to me. I have increased my score."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4400/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700095560466,
                "cdate": 1700095560466,
                "tmdate": 1700095560466,
                "mdate": 1700095560466,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bzSDHBQaax",
            "forum": "plebgsdiiV",
            "replyto": "plebgsdiiV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4400/Reviewer_PFzG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4400/Reviewer_PFzG"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an extension of the in-sample TD learning algorithms of Schlegel et al and Zhang et al to the case where there is a deterministic target policy for continuous actions. Their extension modifies the deterministic policy to have support of a gaussian kernel centered on the action taken by the target policy (as opposed to a direct-delta function). They then derive the bias and variance of the new estimators, the mean squared error of the estimator, and other ancillary artifacts of the estimator."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, I believe the paper is well positioned it the literature and the motivation behind the algorithm is stated clearly. While densely written, I believe the assumptions and proofs are reasonable, but I did not check each proof in-depth."
                },
                "weaknesses": {
                    "value": "1. The paper is begging for an example of when you might want to evaluate a deterministic policy using data generated by a stochastic policy. From my experience, in reinforcement learning we often have to inverse problem. For instance, if using expert data generated through a pid controller or other classic control algorithm we will encounter deterministic behavior policies. Other than as baselines to compare RL algorithms, I\u2019m struggling to understand when we might encounter the need to evaluate a deterministic policy. If this is the only example, that isn\u2019t a deal breaker but an example would make the paper much better.\n2. It is unclear what parts of the algorithm are learned and what is not learned/given. I believe A(s) is learned (in the full version of KMIFQE) and the value function it self is learned. Are any other parts learned? Maybe a section clearly stating the algorithm succinctly before or after proving the various artifacts would be beneficial to clarify exactly the moving parts.\n3. There should be a statement on how the hyperparameters are chosen for all the methods. While this is in the appendix, there should be space made for this in the main paper. One notable missing part of this was a discussion on how to set the bandwidth. Is this given or determined by the data (through equation 10)? If it is given, are there any rules of thumb that would help practitioners? This is discusse briefly how this determines the bias-variance trade-off, but I believe this is lacking further exploration for practical use-cases.\n\n**Suggestions:**\n\n- Section 4/4.1 is written in an extremely dense manner. This is partially necessary due to the complexity of the math behind the kernel relaxation for the target policy, but I believe the authors could have done more to explain the theorems and propositions in more intuitive forms. I also believe separating section 4.2 into two sections could be beneficial (Optimal bandith/metric in one and bounding the contraction of the bellmen operators)."
                },
                "questions": {
                    "value": "See Above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4400/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789956091,
            "cdate": 1698789956091,
            "tmdate": 1699636413440,
            "mdate": 1699636413440,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EiObxZ7DmG",
                "forum": "plebgsdiiV",
                "replyto": "bzSDHBQaax",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4400/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4400/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer $\\text{\\textcolor{green}{PFzG}}$"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to provide detailed feedback on our work. Please let us know if any of our responses below need further clarification. \n\n$ $\n\n**W1. Practicality of evaluating a deterministic policy using data generated by a stochastic policy and its example**\n\nThere are many cases where the evaluation of deterministic policies is needed, e.g., safety-critical systems such as industrial robot control and drug prescription, where the sequences of actions need to be precisely and consistently controlled without introducing variability [Silver et al., 2014][Kallus and Zhou, 2018]. In these situations, one may want to evaluate the performance of the deterministic policies based on offline data when the robot control or drug prescription policy is updated before deployment. For the offline data, if the data is collected from the policies used in the past, the data can be viewed as sampled from a single stochastic behavior policy.\n\nWe included an explanation of the practicality of our problem setting and an example in the second paragraph of the introduction section.\n\n$ $\n\n[Silver et al., 2014] Silver et al. \u201cDeterministic Policy Gradient Algorithms\u201d ICML 2014.\n\n[Kallus and Zhou, 2018] Kallus and Zhou. \"Policy Evaluation and Optimization with Continuous Treatments..\" AISTATS 2018.\n\n$ $\n\n**W2. Parts of the algorithm that are learned and not learned**\n\nWe learn the kernel metric composed of $h$ (scale of the metric, referred to as bandwidth) and $A$ (shape of the metric, referred to as metric) as well as $Q$ function, using Eq. (10), Eq. (13), and Eq. (6) respectively. The pseudo-code of our algorithm is provided in Algorithm 1 in Appendix B, where we use the learned bandwidth $h$. \n\nWe modified the paper's main text to guide readers to our pseudocode in Algorithm 1 in Appendix B and clear up what is learned and given in our algorithm. Furthermore, we modified Algorithm 1 to contain more details. We also modified the caption of Table 1 to explicitly mention what is learned in our algorithm in the experiment.\n\n$ $\n\n**W3. Hyperparameter selection**\n\nFor the hyperparameters in our method, bandwidth $h$ in our method is learned by Eq. (10).  As for the IS ratio clipping range, we selected the range by grid search on Hopper-v2 domain from the cartesian product of minimum IS ratio clip values  {1e-5, 1e-3, 1e-1} and maximum IS ratio clip values  {1, 2, 10} and applied same hyperparameters on the other environments. For the hyperparameters in the baselines, we mostly followed the settings in SR-DICE as they experimented on the same or similar environments.\n\nWe added a brief explanation of the hyperparameter settings in the experiment section and details in Appendix C.1, \u201cHyperparameters\u201d section.\n\n$ $\n\n**S1. Intuitive explanation of the theorems and propositions**\n\nThank you for your suggestion. We added intuitive explanations of the theorems and propositions in the main text. For Theorem1, we included an intuitive analysis of the leading-order bias in the paragraph after Theorem 1. For Proposition 3, we included an intuitive explanation of how the metric measures similarity between actions and how it is reflected in the importance resampling in the second paragraph after Proposition 3.\n\n$ $\n\n**S2. Separation of section 4.2 into Optimal bandith/metric section and error bound analysis section**\n\nThank you for your suggestion. The contents related to the error bound are put in a separate subsection \u201c4.3 Error Bound Analysis\u201d"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4400/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700084110244,
                "cdate": 1700084110244,
                "tmdate": 1700084110244,
                "mdate": 1700084110244,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]