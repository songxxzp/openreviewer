[
    {
        "title": "Recursive Generalization Transformer for Image Super-Resolution"
    },
    {
        "review": {
            "id": "2wb1nTmnFz",
            "forum": "owziuM1nsR",
            "replyto": "owziuM1nsR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission31/Reviewer_bq6U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission31/Reviewer_bq6U"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a recursive generalization transformer for image SR. In which, the recursive-generalization self-attention (RG-SA) recursively aggregates input features into representative feature maps, and then extract global information via the cross-attention. Moreover, the authors combine the RG-SA with local self-attention to enhance the exploitation of the global context, and further propose the hybrid adaptive integration (HAI) for module integration. Experiments demonstrate that the proposed RGT achieves good performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.The paper is well writting and easy to following. \n2.The author proves the effectiveness of the proposed components by sufficient experiments."
                },
                "weaknesses": {
                    "value": "1. The comparisons with recent ViT-based methods are insufficient.\n2. The author should also make comparisons on inference speed and GPU usage to evaluate the model computational efficiency.\n3. What is the performance of a larger window SA with HAI?\n4. In the ablation of RG-SA, the authors state that \"a smaller cr reduces redundancy between channels\". Why does fewer channels reduce feature redundancy? \n5. What are the implementation details of dynamically choosing the recursion time T in RGM\uff1f"
                },
                "questions": {
                    "value": "See the weaknesses part"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission31/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698416982947,
            "cdate": 1698416982947,
            "tmdate": 1699635926726,
            "mdate": 1699635926726,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ymNFAYL2FM",
                "forum": "owziuM1nsR",
                "replyto": "2wb1nTmnFz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission31/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission31/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bq6U (denoted as R4) part 1"
                    },
                    "comment": {
                        "value": "`Q4-1:` The comparisons with recent ViT-based methods are insufficient.\n\n`A4-1:` \n\nThanks for pointing it out. We compare our method with more ViT-based methods: EDT [ref1], ART [ref2], and DAT [ref3].\n\nThe models are tested on Urban100 (\u00d72). The input size for calculating FLOPs is 3\u00d7256\u00d7256. The results are as follows:\n\n| Method           | Params |   FLOPs   | PSNR(dB) |  SSIM  |\n| ---------------- | :----: | :-------: | :------: | :----: |\n| EDT-B (arXiv'22) | 11.48M | 1,105.16G |  33.81   | 0.9427 |\n| ART-S (ICLR'23)  | 11.72M | 1,228.98G |  34.02   | 0.9437 |\n| DAT-S (ICCV'23)  | 11.06M |  773.35G  |  34.12   | 0.9444 |\n| RGT-S (ours)     | 10.05M |  793.45G  |  34.32   | 0.9457 |\n\n1. Compared to EDT-B and ART-S, our method has fewer FLOPs and Params while obtaining better performance.\n2. Compared to DAT-S, our RGT-S demonstrates superior performance (improving by 0.2dB of PSNR) with similar FLOPs and Params.\n3. These results further demonstrate the effectiveness of the linear global attention proposed in our method for the SR task.\n\n\n\n[ref1] Wenbo Li, Xin Lu, Jiangbo Lu, Xiangyu Zhang, and Jiaya Jia. On efficient transformer and image pre-training for low-level vision. arXiv preprint arXiv:2112.10175, 2021.\n\n[ref2] Jiale Zhang, Yulun Zhang, Jinjin Gu, Yongbing Zhang, Linghe Kong, XinYuan. Accurate Image Restoration with Attention Retractable Transformer, In ICLR, 2023.\n\n[ref3] Zheng Chen, Yulun Zhang, Jinjin Gu, et al. Dual Aggregation Transformer for Image Super-Resolution, In ICCV, 2023.\n\n\n\n`Q4-2:` The author should also make comparisons on inference speed and GPU usage to evaluate the model computational efficiency.\n\n`A4-2:` \n\nThanks for your valuable suggestions. We compare SwinIR, CAT-A, RGT-S, and RGT. The tests are conducted on one A100 GPU at scale \u00d74. We evaluate both inference speed (latency) and GPU usage. The input size is set to 3\u00d7128\u00d7128. For accuracy, we calculated the average time with 100 inputs.\n\n| Method                | SwinIR | CAT-A  | RGT-S  |  RGT   |\n| --------------------- | :----: | :----: | :----: | :----: |\n| Inference Speed (/ms) | 121.67 | 240.10 | 116.19 | 157.77 |\n| GPU usgae (GiB)       | 18.78  | 19.50  | 19.54  | 22.97  |\n\n1. For **inference speed**, our RGT-S is faster than SwinIR and CAT-A. Meanwhile, RGT's speed is superior to CAT-A. This demonstrates the effectiveness of our approach.\n2. For **GPU usage**, our method consumes less memory than CAT-A. Compared to SwinIR, the memory usage is slightly higher, which may be due to implementation aspects of the code. For example, using `contiguous()` operations in PyTorch can consume more memory to speed up computation. We plan to optimize the code implementation to reduce GPU usage."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission31/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217346232,
                "cdate": 1700217346232,
                "tmdate": 1700217346232,
                "mdate": 1700217346232,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "02tbpw5zMr",
                "forum": "owziuM1nsR",
                "replyto": "2wb1nTmnFz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission31/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission31/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bq6U (denoted as R4) part 2"
                    },
                    "comment": {
                        "value": "`Q4-3:` What is the performance of a larger window SA with HAI?\n\n`A4-3:` \n\nThanks for your question. We conduct experiments to compare different window sizes in window SA.\n\nWe compared the 8\u00d732 window (used in our RGT-S) with the 32\u00d732 window. Other settings of the two models are consistent with RGT-S. Both models are trained under the same settings with iterations=50K. We test models on Urban100 (x2). The input size for calculating FLOPs is 3\u00d7256\u00d7256.\n\n| Method | Params(M) | FLOPs(G) | PSNR(dB) |  SSIM  |\n| ------ | :-------: | :------: | :------: | :----: |\n| 8\u00d732   |   10.05   |  793.45  |  32.23   | 0.9301 |\n| 32\u00d732  |   10.05   | 1,119.61 |  32.32   | 0.9306 |\n\n1. Increasing the window size can improve the model's performance (increase PSNR by 0.09dB). \n2. However, this also increases the computational complexity (increase FLOPs by 326.16 G). \n3. Considering both performance and computational cost, we select the current 8\u00d732 window setting.\n\n\n\n`Q4-4:` In the ablation of RG-SA, the authors state that \"a smaller cr reduces redundancy between channels\". Why does fewer channels reduce feature redundancy?\n\n`A4-4:` \n\nThank you for your questions. We explain below:\n\n1. **For redundancy:** Since we aggregate feature maps through the recursive generalization module, maintaining the original number of channels could lead to a certain level of redundancy.\n\n2. **For reducing redundancy:** By scaling down the channels, we reduce the number of feature maps. This helps to enhance information aggregation and increase sensitivity (discrimination) between different feature maps. Consequently, when computing cross-attention, this allows for better focus on essential features and improves information transfer.\n\n3. **Experiments:** We compared models with different numbers of channels in our ablation study, with results as follows. The input size for calculating FLOPs was 3\u00d7128\u00d7128.\n\n   | Method    | Params(M) | FLOPs(G) | PSNR(dB) |  SSIM  |\n   | --------- | :-------: | :------: | :------: | :----: |\n   | $c_r$=1   |   11.37   |  189.62  |  33.54   | 0.9404 |\n   | $c_r$=0.5 |   10.05   |  183.08  |  33.68   | 0.9414 |\n\n   Reducing the number of channels effectively improves model performance, which further supports our analyses.\n\n\n\n`Q4-5:` What are the implementation details of dynamically choosing the recursion time T in RGM\uff1f\n\n`A4-5:` \n\nThank you for your questions.\n\nThe recursion time $T$ is determined by the formula $T=\\lfloor\\log_{s_r}{\\frac{H}{h}}\\rfloor$. The stride size $s_r$ is set to 4, and the representative map size $h$ is 4 for training and 16 for testing.\n\n**For $s_r=4$**, we choose this setting following the default selection in previous works [ref4].\n\n**For $h=4/16$**, we choose this to balance performance and computational complexity.\n\n1. For training, setting $h=4$ can speed up the process (compared to h=16). Meanwhile, given the input patch size of 64\u00d764, $h=4$ cannot lead to significant information loss and performance drop.\n\n2. For testing, $h=4$ is too small compared to the full image resolution and would result in substantial information loss. For instance, img001.png in Urban100 (\u00d72) is 512\u00d7322 ($\\gg$4\u00d74). Conversely, h=16 effectively improves performance without significantly increasing computational complexity. \n\n3. We compare h=4/16/64 in RGT-S on Urban100 (\u00d72). The input size for calculating FLOPs is 3\u00d7256\u00d7256.\n\n   | Method | Params(M) | FLOPs(G)  | PSNR(dB) |  SSIM  |\n   | ------ | :-------: | :-------: | :------: | :----: |\n   | h=4    |   10.05   |  716.82G  |  32.78   | 0.9336 |\n   | h=16   |   10.05   |  793.45G  |  34.32   | 0.9457 |\n   | h=64   |   10.05   | 2,019.55G |   N/A    |  N/A   |\n\n   **Note:** When h=64, the complexity is too high that it cannot be tested on the full image.\n\n   It is observed that compared to h=4 or 64, h=16 achieves a better balance between performance and complexity.\n\n4. Therefore, we use h=4 for training and h=16 for testing.\n\n\n\n[ref4] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In ICCV, 2021."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission31/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217357248,
                "cdate": 1700217357248,
                "tmdate": 1700217357248,
                "mdate": 1700217357248,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wvtA4cWNLy",
            "forum": "owziuM1nsR",
            "replyto": "owziuM1nsR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission31/Reviewer_Pj4p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission31/Reviewer_Pj4p"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a novel image SR method, Recursive Generalization Transformer (RGT). The RGT uses recursive-generalization self-attention (RG-SA) and hybrid adaptive integration (HAI) to model global information. Extensive experiments demonstrate that the proposed RGT achieves state-of-the-art performance on image SR."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The paper's writing and organization are good. All illustrations, tables, and visual results are intuitive and clear.\n- The motivation for the proposed method is reasonable. The global information in SR is important while reducing the complexity of global attention is crucial for its application in SR tasks. \n- The proposed components RG-SA and HAI are novel and valuable.\n- The ablation study is extensive. The effectiveness of each part in RGT is demonstrated.\n- The authors provide multiple model versions and compare them with state-of-the-art methods. All results prove that the RGT proposed in this paper is a promising SR method."
                },
                "weaknesses": {
                    "value": "- When compared with CAT-A, the improvements of RGT on some datasets (Set5, Set14) are not very obvious (< 0.1 dB).\n- Although FLOPs are provided in Sec. 4.4, the running time of the model on real devices should also be provided.\n- The primary evaluation metrics used in the paper are PSNR and SSIM. However, these metrics may not reflect actual SR performance. Some perceptual metrics, such as LPIPS, should be evaluated."
                },
                "questions": {
                    "value": "- The author mentioned in the paper that the proposed RG-SA realizes linear complexity, which is better than other global methods. So, has the author tried other global attention, such as the attention proposed in PVT or XCiT? How do the computational complexity and performance of RG-SA compare with these methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission31/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698427192147,
            "cdate": 1698427192147,
            "tmdate": 1699635926601,
            "mdate": 1699635926601,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YJF1wDLogz",
                "forum": "owziuM1nsR",
                "replyto": "wvtA4cWNLy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission31/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission31/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Pj4p (denoted as R3)"
                    },
                    "comment": {
                        "value": "`Q3-1:` When compared with CAT-A, the improvements of RGT on some datasets (Set5, Set14) are not very obvious (< 0.1 dB).\n\n`A3-1:` \n\nThank you for pointing this out. We provide more analysis on this results.\n\n1. **Fewer Params and FLOPs.** Compared to CAT-A, our RGT-S has a lower parameter count and computational complexity. Therefore, any performance improvement itself demonstrates the effectiveness of our proposed method.\n\n2. **Global Attention.** The core of our proposed RGT is RG-SA, which effectively models global information. This is more advantageous for processing **high-resolution** images. The images in Set5 and Set14 are relatively small (e.g., bird is 144\u00d7144 in Set5-\u00d72), hence the marginal differences. However, the improvements are more significant on high-resolution datasets (e.g., img001 is 512\u00d7322 in Urban100-\u00d72) like Urban100 and Manga109.\n\n3. Our comparisons at scales \u00d72, \u00d73, and \u00d74 between RGT and CAT-A show increasing performance improvements as we move from scale 4 to 2 (with gradually increasing input image resolution). This further validates our analysis.\n\n   | Method |  \u00d72   |  \u00d73   |  \u00d74   |\n   | ------ | :---: | :---: | :---: |\n   | CAT-A  | 34.26 | 30.12 | 27.89 |\n   | RGT    | 34.47 | 30.28 | 27.98 |\n   | \u25b3      | +0.21 | +0.16 | +0.09 |\n\n4. **Visual Comparison.** Our method shows more noticeable improvements in visual comparisons. We have provided corresponding results in **Sec. 7 of the supplementary material**.\n\n\n\n`Q3-2:` Although FLOPs are provided in Sec. 4.4, the running time of the model on real devices should also be provided.\n\n`A3-2:` \n\nThank you for your suggestions. We assess the running times at \u00d74 scale on one A100 GPU. The input size is 3\u00d7128\u00d7128. Our comparison includes SwinIR, CAT-A, RGT-S, and RGT. To ensure accuracy, the average time is calculated over 100 input images.\n\n| Method            | SwinIR | CAT-A  | RGT-S  |  RGT   |\n| ----------------- | :----: | :----: | :----: | :----: |\n| Running Time (ms) | 121.67 | 240.10 | 116.19 | 157.77 |\n\n1. Our RGT-S and RGT models demonstrate faster running times compared to CAT-A.\n2. Additionally, the running time of our RGT-S is also lower than that of SwinIR.\n\n\n\n`Q3-3:` The primary evaluation metrics used in the paper are PSNR and SSIM. However, these metrics may not reflect actual SR performance. Some perceptual metrics, such as LPIPS, should be evaluated.\n\n`A3-3:` \n\nThank you for your suggestions. We compare our RGT with SwinIR and CAT-A using perceptual metrics LPIPS and DISTS. The test is conducted on the Manga109 and Urban100 datasets at \u00d74 scale.\n\n| Method | Urban100 |        | Manga109 |        |\n| ------ | :------: | :----: | :------: | :----: |\n|        |  LPIPS   | DISTS  |  LPIPS   | DISTS  |\n| SwinIR |  0.1840  | 0.1533 |  0.0926  | 0.0766 |\n| CAT-A  |  0.1801  | 0.1502 |  0.0906  | 0.0753 |\n| RGT    |  0.1759  | 0.1469 |  0.0893  | 0.0719 |\n\nOur method achieves better perceptual scores than other methods, consistent with the visual comparisons. This further demonstrates the effectiveness of our proposed method.\n\n\n\n`Q3-4:` The author mentioned in the paper that the proposed RG-SA realizes linear complexity, which is better than other global methods. So, has the author tried other global attention, such as the attention proposed in PVT or XCiT? How do the computational complexity and performance of RG-SA compare with these methods?\n\n`A3-4:` \n\nThank you for your question. We compare our RG-SA with XCA, as proposed in XCiT, and the sparse attention proposed in ART.\n\n**XCA**: A global attention mechanism with linear complexity using a \u201ctransposed\u201d operation, but it cannot explicitly model the spatial relationship.\n\n1. We compare RG-SA with XCA. For a fair comparison, we directly replace RG-SA in RGT-S with XCA and get the model RGT-XCA. We train both RGT-S and RGT-XCA under the same settings (iterations: 50K). Models are tested on Manga109, with an input size of 3\u00d7256\u00d7256 for calculating FLOPs.\n\n   | Method  | Params(M) | FLOPs(G) | PSNR(dB) |  SSIM  |\n   | ------- | :-------: | :------: | :------: | :----: |\n   | RGT-XCA |   10.88   |  824.70  | 38.0601  | 0.9770 |\n   | RGT-S   |   10.05   |  793.45  | 38.4279  | 0.9771 |\n\n2. Our RGT-S outperforms RGT-XCA while having a similar computational complexity and parameters. This demonstrates the effectiveness of RG-SA in modeling global spatial information.\n\n\n\n**Sparse attention**: A spatial global attention with quadratic complexity.\n\n1. Due to computational resource limitations, we directly compare RGT-S with ART-S without retraining.\n\n   | Method | Params(M) | FLOPs(G) | PSNR(dB) |  SSIM  |\n   | ------ | :-------: | :------: | :------: | :----: |\n   | ART-S  |   11.72   | 1,228.98 |  40.11   | 0.9804 |\n   | RGT-S  |   10.05   |  793.45  |  40.18   | 0.9805 |\n\n2. Our RGT-S achieves better performance with lower computational complexity. This indicates that RG-SA can effectively capture global attention while maintaining a lower (linear) computational complexity."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission31/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217275783,
                "cdate": 1700217275783,
                "tmdate": 1700217275783,
                "mdate": 1700217275783,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pfn1gfMMkR",
                "forum": "owziuM1nsR",
                "replyto": "YJF1wDLogz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission31/Reviewer_Pj4p"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission31/Reviewer_Pj4p"
                ],
                "content": {
                    "comment": {
                        "value": "All my concerns have been well addressed, and I have no further comments."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission31/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669240100,
                "cdate": 1700669240100,
                "tmdate": 1700669240100,
                "mdate": 1700669240100,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5J0Qct1VXG",
            "forum": "owziuM1nsR",
            "replyto": "owziuM1nsR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission31/Reviewer_fY4p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission31/Reviewer_fY4p"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents the Recursive Generalization Transformer (RGT) for image SR. Specifically, the recursive-generalization self-attention (RG-SA) is proposed to capture global spatial information. And the hybrid adaptive integration (HAI) is designed for module integration. Experiments show that the proposed RGT achieves improvements over recent methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors propose the recursive-generalization self-attention (RG-SA), which controls computational complexity while achieving global modeling.\n2. They also design the hybrid adaptive integration (HAI). It is a simple yet effective.\n3. The paper's experiments are comprehensive. The ablation study demonstrates the effects of each component.\n4. Quantitative and qualitative results indicate that the proposed method outperforms SwinIR and CAT-A.\n5. The authors provide various visual results: feature maps and CKA heatmap, which greatly support the author's claims and enrich the overall quality of the paper.\n6. The paper is well-organized, with clear and readable content."
                },
                "weaknesses": {
                    "value": "1. Some details in the paper are not clear. For example, the representative map size \"h\" is set as 4 for training but 16 for testing. Why not use the same settings?\n2. The RGT-S and RGT all adopt a larger window size than SwinIR. To establish a fairer comparison, it is recommended to use the same window size.\n3. It would be beneficial to include comparisons with more recent methods, such as RGT, to evaluate the effectiveness of the proposed method.\n4. A comparison of running times should be given, which is important for practical applications of the model."
                },
                "questions": {
                    "value": "1. Some details in the paper are not clear. For example, the representative map size \"h\" is set as 4 for training but 16 for testing. Why not use the same settings?\n2. The RGT-S and RGT all adopt a larger window size than SwinIR. To establish a fairer comparison, it is recommended to use the same window size.\n3. It would be beneficial to include comparisons with more recent methods, such as RGT, to evaluate the effectiveness of the proposed method.\n4. A comparison of running times should be given, which is important for practical applications of the model."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "M/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission31/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission31/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission31/Reviewer_fY4p"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission31/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698624920415,
            "cdate": 1698624920415,
            "tmdate": 1699635926343,
            "mdate": 1699635926343,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D7cX6wL5aA",
                "forum": "owziuM1nsR",
                "replyto": "5J0Qct1VXG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission31/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission31/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fY4p (denoted as R2)"
                    },
                    "comment": {
                        "value": "`Q2-1:` Some details in the paper are not clear. For example, the representative map size \"h\" is set as 4 for training but 16 for testing. Why not use the same settings?\n\n`A2-1:` \n\nThank you for your question. We use different settings as a trade-off between complexity and performance.\n\n\n\n1. For **training**, we set h=4 to speed up the training process (compared to h=16). Since the training input patch size is 64\u00d764, this setting does not result in significant information loss.\n\n2. For **testing**, we need to process the whole image, where the input image resolution may be large. For instance, the img001 in Urban100 (\u00d72) is 512\u00d7322, much larger than 4\u00d74. The h=4 may lose too much information, leading to a substantial decrease in performance. Therefore, we apply h=16.\n\n3. We conduct tests with h=4, 16, and 64 in RGT-S on Urban100 (\u00d72). The input size is 3\u00d7256\u00d7256 for calculating FLOPs.\n\n   | Method | Params(M) | FLOPs(G)  | PSNR(dB) |  SSIM  |\n   | ------ | :-------: | :-------: | :------: | :----: |\n   | h=4    |   10.05   |  716.82G  |  32.78   | 0.9336 |\n   | h=16   |   10.05   |  793.45G  |  34.32   | 0.9457 |\n   | h=64   |   10.05   | 2,019.55G |   N/A    |  N/A   |\n\n   **Note:** When h=64, the complexity is too high that it cannot be tested on the full image.\n\n4. Compared to h=4, h=16 achieves a slight increase in complexity but a significant performance improvement. However, further increasing h would result in excessively high complexity, rendering the model inoperable.\n\n5. In summary, we set h=4 during training and h=16 for testing to balance performance and complexity.\n\n\n\n`Q2-2:` The RGT-S and RGT all adopt a larger window size than SwinIR. To establish a fairer comparison, it is recommended to use the same window size.\n\n`A2-2:` \n\nThank you for your valuable suggestions. We have provided a model with the same window size as SwinIR, named RGT-sw, in the supplementary material. Details in Sec. 1.1. Here, we present the corresponding table (only PSNR).\n\n| Method | Params |  FLOPs  | Set5  | Set14 | B100  | Urban100 | Manga109 |\n| ------ | :----: | :-----: | :---: | :---: | :---: | :------: | :------: |\n| SwinIR | 11.75M | 205.31G | 38.42 | 34.46 | 32.53 |  33.81   |  39.92   |\n| RGT-sw | 11.60M | 187.73G | 38.46 | 34.54 | 32.54 |  33.90   |  40.12   |\n\nOur RGT-sw, with similar computational complexity and parameters, outperforms SwinIR in performance. In particular, our RGT-sw achieves an improvement of 0.2dB on Manga109. This demonstrates the effectiveness of our proposed method.\n\n\n\n`Q2-3:` It would be beneficial to include comparisons with more recent methods, to evaluate the effectiveness of the proposed method.\n\n`A2-3:` \n\nThank you for your suggestions. We have conducted comparisons with more recent SR methods, including EDT [ref1], ART [ref2], and DAT [ref3]. Models are tested on Urban100 (\u00d72). The input size for calculating FLOPs is 3\u00d7256\u00d7256.\n\n| Method           | Params |   FLOPs   | PSNR(dB) |  SSIM  |\n| ---------------- | :----: | :-------: | :------: | :----: |\n| EDT-B (arXiv'22) | 11.48M | 1,105.16G |  33.81   | 0.9427 |\n| ART-S (ICLR'23)  | 11.72M | 1,228.98G |  34.02   | 0.9437 |\n| DAT-S (ICCV'23)  | 11.06M |  773.35G  |  34.12   | 0.9444 |\n| RGT-S (ours)     | 10.05M |  793.45G  |  34.32   | 0.9457 |\n\n1. Compared with **EDR-B** and **ART-S**, our RGT-S performs better with lower computational complexity and fewer parameters.\n2. Compared with **DAT-S**, our method shows an improvement of 0.2dB on PSNR with comparable complexity and parameters.\n3. These comparisons underscore the effectiveness of our proposed method, which utilizes a global attention mechanism with linear complexity.\n\n\n\n[ref1] Wenbo Li, Xin Lu, Jiangbo Lu, Xiangyu Zhang, and Jiaya Jia. On efficient transformer and image pre-training for low-level vision. arXiv preprint arXiv:2112.10175, 2021.\n\n[ref2] Jiale Zhang, Yulun Zhang, Jinjin Gu, Yongbing Zhang, Linghe Kong, XinYuan. Accurate Image Restoration with Attention Retractable Transformer, In ICLR, 2023.\n\n[ref3] Zheng Chen, Yulun Zhang, Jinjin Gu, et al. Dual Aggregation Transformer for Image Super-Resolution, In ICCV, 2023.\n\n\n\n`Q2-4:` A comparison of running times should be given, which is important for practical applications of the model.\n\n`A2-4:` \n\nThank you for your suggestions. We calculate the running times on one A100 GPU at \u00d74 scale. The input size is set to 3\u00d7128\u00d7128. We compare SwinIR, CAT-A, RGT-S, and RGT. For accuracy, we calculate the average time with 100 inputs.\n\n| Method            | SwinIR | CAT-A  | RGT-S  |  RGT   |\n| ----------------- | :----: | :----: | :----: | :----: |\n| Running Time (ms) | 121.67 | 240.10 | 116.19 | 157.77 |\n\n1. Our RGT-S and RGT achieve lower running times than CAT-A.\n2. Our RGT-S also exhibited a lower running time compared to SwinIR."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission31/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217114081,
                "cdate": 1700217114081,
                "tmdate": 1700217114081,
                "mdate": 1700217114081,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eecFebdpB6",
            "forum": "owziuM1nsR",
            "replyto": "owziuM1nsR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission31/Reviewer_B1Rr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission31/Reviewer_B1Rr"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose the Recursive Generalization Transformer (RGT) for image super-resolution. The network can capture global spatial information and is more suitable for high-resolution images. It introduces Recursive-generalization self-attention (RG-SA) to recursively aggregate input features and Hybrid Adaptive Integration (HAI) to integrate global and local modules. Experiments show that the proposed method outperforms recent methods quantitatively and qualitatively."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of using global attention in the Transformer is widespread. But, the authors effectively maintain low computational complexity, which is meaningful in image SR.\n2. Additionally, the proposed HAI is simple yet effective. Both the ablation experiments (Table 1 (c), (d)) and the visual results (Figs. 3, 4, 5) strongly support the authors' claim: integrate global and local modules.\n3. The main comparisons with recent methods demonstrate the superiority of this method. I also notice that the authors provide model size comparisons and a more fair comparison with SwinIR in the supplementary materials.\n4. The paper is carefully organized. The paper is well-written and easy to read. The proposed methods are easy to follow."
                },
                "weaknesses": {
                    "value": "1. The experiments on RG-SA are not enough. The authors claim the superiority of RG-SA, but it is not compared with other global attention mechanisms.\n2. The authors only provide visual comparisons on Urban100 and Manga109 datasets. Comparisons on other datasets are lacking."
                },
                "questions": {
                    "value": "1. The authors should compare RG-SA with other global attention, such as sparse attention proposed in ART (Accurate Image Restoration with Attention Retractable Transformer).\n2. It is suggested to add more comparisons with more SR methods like ART.\n3. Providing visual comparison results on more datasets to strengthen the effectiveness of the model.\n4. Compared with other Transformer methods like SwinIR, are their training datasets the same? Are there any differences in the training strategy?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission31/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698636034494,
            "cdate": 1698636034494,
            "tmdate": 1699635926189,
            "mdate": 1699635926189,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l5UssiaZPq",
                "forum": "owziuM1nsR",
                "replyto": "eecFebdpB6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission31/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission31/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer B1Rr (denoted as R1) part 1"
                    },
                    "comment": {
                        "value": "`Q1-1:` The experiments on RG-SA are not enough. The authors claim the superiority of RG-SA, but it is not compared with other global attention mechanisms.\n\n`A1-1:` \n\nThank you for your valuable feedback. We conduct additional experiments to compare RG-SA with other global attention mechanisms.\n\n\n\n1. We compare RG-SA with the **cross-covariance attention** (**XCA**) proposed in XCiT [ref1]. XCA is a global attention mechanism with linear complexity, implemented using a \"transposed\" approach.\n\n   For a fair comparison, we replace RG-SA in RGT-S with XCA to build the model RGT-XCA. Both RGT-S and RGT-XCA are trained under the same settings (iterations: 50K). Models are tested on Manga109 (\u00d72), with an input size of 3\u00d7256\u00d7256 for calculating FLOPs.\n\n   | Method  | Params(M) | FLOPs(G) | PSNR(dB) |  SSIM  |\n   | ------- | :-------: | :------: | :------: | :----: |\n   | RGT-XCA |   10.88   |  824.70  | 38.0601  | 0.9770 |\n   | RGT-S   |   10.05   |  793.45  | 38.4279  | 0.9771 |\n\n   Our RGT-S outperforms RGT-XCA, while having a similar computational complexity and parameters. This demonstrates the effectiveness of RG-SA in modeling global **spatial** information.\n\n\n\n2. We compared RG-SA with the **sparse attention** proposed in ART [ref2], which is a global spatial attention with quadratic complexity.\n\n   Considering that ART also uses a structure combining global and local window attention, similar to our RGT, we directly compare RGT-S and ART-S without retraining, due to limited computational resources. \n\n   Note that the training settings of ART and our RGT are the same. Therefore, the comparison is fair. Models are tested on Manga109 (\u00d72), with an input size of 3\u00d7256\u00d7256 for calculating FLOPs.\n\n   | Method | Params(M) | FLOPs(G) | PSNR(dB) |  SSIM  |\n   | ------ | :-------: | :------: | :------: | :----: |\n   | ART-S  |   11.72   | 1,228.98 |  40.11   | 0.9804 |\n   | RGT-S  |   10.05   |  793.45  |  40.18   | 0.9805 |\n\n   Our RGT-S achieves superior performance with much lower computational complexity. This highlights RG-SA's ability to effectively capture global attention while maintaining a lower (linear) computational complexity.\n\n\n\n**In conclusion**, comparisons with other global attention mechanisms demonstrate the superiority of our proposed RG-SA.\n\n\n\n[ref1] Alaaeldin Ali, Hugo Touvron, Mathilde Caron, Piotr Bo- janowski, Matthijs Douze, et al. Xcit: Cross-covariance image transformers, In NeurIPS, 2021.\n\n[ref2] Jiale Zhang, Yulun Zhang, Jinjin Gu, Yongbing Zhang, Linghe Kong, XinYuan. Accurate Image Restoration with Attention Retractable Transformer, In ICLR, 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission31/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217034123,
                "cdate": 1700217034123,
                "tmdate": 1700217034123,
                "mdate": 1700217034123,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7SQHED8yM5",
                "forum": "owziuM1nsR",
                "replyto": "eecFebdpB6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission31/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission31/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer B1Rr (denoted as R1) part 2"
                    },
                    "comment": {
                        "value": "`A1-2:` The authors only provide visual comparisons on Urban100 and Manga109 datasets. Comparisons on other datasets are lacking.\n\n`Q1-2:` \n\nThanks for pointing it out. We have added additional visual comparisons on the Set14 and B100 datasets in **Sec.7 of the supplementary material**.\n\n\n\n`A1-3:` The authors should compare RG-SA with other global attention, such as sparse attention proposed in ART (Accurate Image Restoration with Attention Retractable Transformer).\n\n`Q1-3:` \n\nThanks for your valuable suggestions. We compare RG-SA and other global attention mechanisms, such as XCA from XCiT and sparse attention from ART. Our experiments demonstrate the effectiveness of our proposed RG-SA.\n\n\n\n**We have responded to another similar question, `Q1-1`. Please refer to `A1-1` for more details.**\n\n\n\n`Q1-4:` It is suggested to add more comparisons with more SR methods like ART.\n\n`A1-4:` \n\nThank you for your valuable suggestions. We conduct comparisons with more SR methods, including EDT [ref3], ART [ref2], and DAT [ref4]. Models are tested on Urban100 at scale \u00d72. The input size used for calculating FLOPs is 3\u00d7256\u00d7256.\n\n| Method           | Params |   FLOPs   | PSNR(dB) |  SSIM  |\n| ---------------- | :----: | :-------: | :------: | :----: |\n| EDT-B (arXiv'22) | 11.48M | 1,105.16G |  33.81   | 0.9427 |\n| ART-S (ICLR'23)  | 11.72M | 1,228.98G |  34.02   | 0.9437 |\n| DAT-S (ICCV'23)  | 11.06M |  773.35G  |  34.12   | 0.9444 |\n| RGT-S (ours)     | 10.05M |  793.45G  |  34.32   | 0.9457 |\n\n1. Compared with **EDT-S** and **ART-S**, our method performs better with lower computational complexity and fewer parameters.\n2. Compared with **DAT-S**, our RGT-S has similar computational complexity and parameters. However, our RGT-S exhibits an improvement of 0.2dB on PSNR.\n3. These comparisons collectively demonstrate the effectiveness of our proposed method.\n\n\n\n[ref3] Wenbo Li, Xin Lu, Jiangbo Lu, Xiangyu Zhang, and Jiaya Jia. On efficient transformer and image pre-training for low-level vision. arXiv preprint arXiv:2112.10175, 2021.\n\n[ref4] Zheng Chen, Yulun Zhang, Jinjin Gu, et al. Dual Aggregation Transformer for Image Super-Resolution, In ICCV, 2023.\n\n\n\n`Q1-5:` Providing visual comparison results on more datasets to strengthen the effectiveness of the model.\n\n`A1-5:` \n\nThank you for your suggestions. We have provided visual comparisons on more datasets  in **Sec.7 of the supplementary material**. We will further provide more visual comparisons and ensure timely updates.\n\n\n\n`Q1-6:` Compared with other Transformer methods like SwinIR, are their training datasets the same? Are there any differences in the training strategy?\n\n`A1-6:` \n\nThank you for your question. The training of RGT is entirely consistent with that of SwinIR, including the use of the same training datasets (DIV2K and Flickr2K) and the same training settings."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission31/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217060761,
                "cdate": 1700217060761,
                "tmdate": 1700217060761,
                "mdate": 1700217060761,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]