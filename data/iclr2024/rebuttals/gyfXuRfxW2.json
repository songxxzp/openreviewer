[
    {
        "title": "Learning Polynomial Problems with $SL(2, \\mathbb{R})$-Equivariance"
    },
    {
        "review": {
            "id": "82AyLxJYX5",
            "forum": "gyfXuRfxW2",
            "replyto": "gyfXuRfxW2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6163/Reviewer_J1SQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6163/Reviewer_J1SQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of learning how to provide SDP positivity certificates for polynomials. This problem can be solved using convex solvers but this is typically rather time consuming. \n\nThe paper observes that the mapping from positive polynomials to their `maximal entropy' SDP solution is SL(d) equivariant. Focusing on the d=2 case,the paper suggests an SL(2) equivariant architectures based on the Clebsch-Gordan methodology often used for SO(3) and other groups. In practice, this architecture does not perform as well as augmentation based on SO(2) equivariant baselines. The paper suggests an interesting theoretical find to (possibly) explain this: While the Clebch-Gordan architecture can construct all equivariant polynomials, the equivariant function considered in the paper cannot be approximated by equivariant polynomials."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. I am not aware of previous work considering the problem the paper considered: learning SDP positivity certificates. Given the high time complexity of these solvers, their centrality in convex programming, and the fact that certificates are verifiable as explained in the paper, I believe this is a very interesting problem to consider and should be considered further. The paper does a good job, in my opinion, of setting up a first empirical and theoretical baseline to consider this problem. \n\n2. Writing is good, it is an interesting story to read.\n\n3. Theorem 1 regarding non-universality seems an interesting result (despite possible error, and needing some tuning down or context as I discuss in the questions part)"
                },
                "weaknesses": {
                    "value": "1. I have some issues re the technical details of the main theorem and the premises of the method, see below. If these issues prove to be non-issues I will raise the score\n2. The architecture that actually works is rather basic: MLPs with augmentations. On the other hand one could credit the paper in finding the equivariant structure and hence what the relevant augmentations are.\n3. The argument that the SL_2(R) equivariant architecture doesn't work because of lack of universality is difficult to actually substantiate. There are many reasons why an architectures may not work well. Maybe a different SL_2 equivariant architecture will work better?"
                },
                "questions": {
                    "value": "The formulation of finding the positive-definite witness with maximal determinant assumes that there are many such witnesses. Are there many witnesses? e.g. when we discuss polynomials of degree 2 and the monomail vector is (x,y) I think that a symmetric matrix uniquely Q uniquely defines a quadratic polynomial (x,y)Q(x,y).\nWhen we discuss polynomials of higher degree there are ambiguities that come from the fact that, say, (x^2)(y^2)=(xy)(xy). But this can be dealt with directly by adding more symmetry constraints into the matrix. In other words, the matrix should be a moment matrix as defined in [Lasserre 2001]. Once these constraints are added I believe that there will be no more ambiguities. Do you agree? If so wouldn't it make sense to incorporate the symmetries and forget about optimizing over logdet?\n\nI have two issues with the non-universality proof. The first issue has to do with the correctness of the proof. In the proof of theorem 1 you display the matrix f(x^8+y^8) (let's call it M) which was computed numerically using Mosek. Is this matrix really a factorization of x^8+y^8? \nIf I understood everything correctly, denoting v=[x^4, x^3y,...,y^4]^T we should have that for all x,y\nx^8+y^8=v^TMv\nis this correct? Trying this on numpy with the M you specified and x=1, y=1 I get \nv^TMv=1.76\nwhile for x=1 y=1\nx^8+y^8=2\nNote also that the trivial factorization of x^8+y^8 would be M0=diag(1,0,0,0,1). which is not in the domain since det(M0)=0. Thus I would suspect that this polynomial is not in the domain of f. Is that true? Or is it possible for a polynomial to have different factorizations of different ranks? Authors please let me know if there is something I misunderstood of if there is some error. Due to this possible error I'm currently setting the rating at 5 and soundness at 2. I will be happy to raise the rating if there is in fact no error. \n\n\nA second issue is with the result concerning the non-universality of the SL(2) network is not correctness but just about the exposition. It is neat that you prove that the function f you're actually  interested in cannot be approximated by SL(2) equivariant polynomials. But I do think you should note that your function f is not defined on all of the vector space: namely f(p) is only defined if p is indeed positive, and moreover there exists a *strictly* positive definite matrix verifying this. So f is defined on some subset of your vector space. The universality results in [Bogatskiy] pertain to the complex SL_2, but also to functions continuous on the whole domain, and this may end up being the more substantial difference. Another example: in  [Villar et al.] all continuous functions invariant with respect to the non-compact Lorenz group action are shown to be approximated by polynomials. Here again the continuous functions are defined on all of the domain.\n\nAnother angle to think of these issues is: For non-compact groups often distinct orbits cannot be separated by continuous functions. For example: consider the action of SL_d on d by d matrices by multiplication from the right: you can see that a d by d matrix which does not have full rank, say A=diag(0,1,1,...,1), is not in the same orbit as the zero matrix, but its orbit contains all matrices of the form diag(0,epsilon,..,epsilon) and thus any SL_d *invariant* function F continuous on all of the domain will satisfy F(A)=F(0). For more on this see [Dym and Gortler] Section 2.5 and Section 1.4, especially the paragraph titled `algebraic separation vs. orbit separation'.  \n\nSo to be concrete about this: I think you should mention in the paper that the function f is not defined everywhere, and would suggest to change the paragraph `why is SL(2,R) different' and other places where this issue is discussed, to note that this also might be a reason for the difference between universality results elsewhere and your non-universality result here.\n\nOther remarks, questions, suggestions, according to order in the paper and not importance:\nSomewhere in the paper- explain why you decided to restrict yourselves to polynomials of two variables.\n\nIn your discussion of Schur's Lemma in page 6: the lemma applies to complex representations and not real. Do you address this (if not, maybe just add a disclaimer)?\n\nPage 4: when you introduce the function f discuss its domain. Mention that in its domain the function is well defined since the opimization problem has a unique maximizer. \n\nPage 6: I didn't understand your explanation of the last layer.\n\nPage 8 timing: The accuracy you achieve is not bad, but probably can be achieved by first order methods which can be much fast than Mosek. You should at least mention this, even if you do not compare against such a method in practice.\n\nPage 9: you reference the wrong paper by Puny. You meant [Puny 2021] not [Puny 2023]\n\n\n \n\n\nReferences mentioned above:\n[Villar et al.]  Scalars are universal: Equivariant machine learning,\nstructured like classical physics\n[Dym and Gortler] Low Dimensional Invariant Embeddings for Universal Geometric\nLearning\n[Puny 21]  Frame averaging for invariant and equivariant network design\n[Lasserre 2001] Global Optimization with polynomials and the problem of moments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6163/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6163/Reviewer_J1SQ",
                        "ICLR.cc/2024/Conference/Submission6163/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697535831232,
            "cdate": 1697535831232,
            "tmdate": 1700399358445,
            "mdate": 1700399358445,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6cqn0FEjI2",
                "forum": "gyfXuRfxW2",
                "replyto": "82AyLxJYX5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer's comments/questions (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for their thorough review of our paper, and for acknowledging the significance of the proposed problem and our contribution to it. We now respond to individual points below.\n\n### Weakness 1: technical detail clarification\n\nWe clarify the reviewer\u2019s points in the sections below. Indeed, we believe these issues are non-issues, but the reviewer should please feel free to follow up if anything remains unclear (and we very much thank the reviewer for their attention to detail).\n\n### Weakness 2: MLP with augmentations performs best\n\nIndeed, the message of our empirical findings is that a simple method (data augmentation) works best, although as noted in Section 5, some care must be taken to augment with reasonably well-conditioned elements of $SL(2,\\mathbb{R})$. This could save practitioners time, in not designing a complex polynomial-based equivariant architecture; it could also prompt the development of different equivariant paradigms. \n\n### Weakness 3: Why our $SL(2,\\mathbb{R})$-equivariant architecture failed, and whether another architecture will work better\n\nThe reviewer is correct that multiple factors can contribute to the failure of an architecture to train. However, we demonstrate not just that the architecture isn\u2019t universal, but that it can\u2019t represent the very function we would like to train it to represent for positivity verification. Therefore, it is at the very least one problem in our experiments with this architecture. Moreover, the conclusion of Section 4.3 is not just that this particular architecture will not work -- it\u2019s that the function we are trying to learn cannot be approximated by any equivariant polynomial. Therefore, any architecture based on approximation via polynomials will fail for this application. However, we agree that designing an architecture that goes beyond approximating equivariant polynomials is a natural future direction. Should such an architecture continue to underperform relative to an augmented MLP, it would likely then be due to another reason.\n\n### Question: Certificate degrees of freedom\n\nAs you pointed out, for the degree 2 case there is a unique feasible point. However, as soon as the polynomial is degree 4, there is a degree of freedom. For example, consider $p = x^4  - x^2y^2 + y^4$. For any $z$, a matrix of the form $$Q =\n\\begin{pmatrix} 1 & 0 &      z\\\\\\  0 &-1-2z& 0\\\\\\ z &0  &     1\n\\end{pmatrix}$$\nis a witness for the positivity of $p$. Restricting that the main antidiagonal has equal entries (like in a moment matrix) would give the matrix\n$\\begin{pmatrix}1&   0&    -\\frac13\\\\\\ 0 &   -\\frac13&  0\\\\\\ -\\frac13 & 0  &  1\\end{pmatrix}$,\nwhich does not have a positive determinant. On the other hand, \n$\\begin{pmatrix}1  &    0  & -.75 \\\\\\ 0  & .5 & 0\\\\\\ -.75 &  0 & 1\\end{pmatrix}$\nis also a witness (with $z = -.75$), and the eigenvalues are positive. This is just one example of why additional restrictions will sometimes prohibit you from finding a PSD certificate. (See Example 4.1 of [Parrilo] for another example of how playing with the free parameter may uncover a PSD certificate). We selected the certificate of maximal determinant because it is the analytic center of all such possible witnesses.\n\nTo be clear, we do require $(xy)(xy) = x^2  y^2$. For example, consider the following bilinear form:\n$$ \\begin{pmatrix} x_1^2 \\\\\\ x_1y_1 \\\\\\ y_1^2 \\end{pmatrix}^T \\begin{pmatrix}1 & 2& 3\\\\\\ 2 & 4& 5\\\\\\ 3& 5& 6\\end{pmatrix}  \\begin{pmatrix} x_2^2 \\\\\\ x_2y_2 \\\\\\ y_2^2 \\end{pmatrix} =\\cdots + 3y_1^2x_2^2 + 4x_1y_1x_2y_2+3x_1^2y_2^2 + \\cdots$$\nBecause we require $x_1 = x_2$ and $y_1 = y_2$, these 3 terms collapse to $10 x^2y^2$. However, there is no reason to require the coefficient 3 to equal the coefficient 4. \n\n### Question: Rounding in non-universality proof\nYou are correct that the matrix M does not precisely satisfy the linear equations. However, this is due to rounding (which we tried to imply by using \u201c$\\approx$\u201d). As you pointed out, this is confusing, so we can include more digits\n$$M = \n\\begin{pmatrix}\n1&  0& -1.56344& 0& 1/3 \\\\\\ 0 &3.12688& 0& -8/3& 0 \\\\\\ -1.56344 &0 &14/3 &0& -1.56344 \\\\\\ 0 &-8/3 &0 &3.12688& 0 \\\\\\ 1/3 &0& -1.56344& 0& 1 \n\\end{pmatrix} $$\nThis exactly represents $x^8 + y^8$ and the determinant is $2.3703703172386135$, which is to several significant figures the determinant of M found from Mosek.\n\nIt is possible that different matrices representing a polynomial have different ranks \u2013 one example is exactly $x^8 + y^8$, like you point out. Any binary form positive on $\\mathbb{R^2}\\setminus (0,0)$ is in the domain of $f$ because there exists a feasible full rank matrix. In particular, $x^8 + y^8$ is in the domain of $f$."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238601072,
                "cdate": 1700238601072,
                "tmdate": 1700238601072,
                "mdate": 1700238601072,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lM1aB6X2Ww",
                "forum": "gyfXuRfxW2",
                "replyto": "6cy2CKXjEF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6163/Reviewer_J1SQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6163/Reviewer_J1SQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the insightful comment. I am now pretty convinced in the technical soundness of the paper (though still relying on Mosek for a proof is somewhat problematic). I think it is a very interesting paper overall. I raised my score to 8. Good job!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700399519275,
                "cdate": 1700399519275,
                "tmdate": 1700399519275,
                "mdate": 1700399519275,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OuiqNwHVjI",
            "forum": "gyfXuRfxW2",
            "replyto": "gyfXuRfxW2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6163/Reviewer_b6w7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6163/Reviewer_b6w7"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel approach to learning polynomial problems with -equivariance. The authors demonstrate the effectiveness of neural networks in solving polynomial problems in a data-driven fashion, achieving tenfold speedups while retaining high accuracy. They also adapt their learning pipelines to accommodate the structure of the non-compact group , including data augmentation and new -equivariant architectures. The paper presents a thorough analysis of the proposed approach, including theoretical proofs and experimental results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+The paper presents a novel approach to solve polynomial problems with -equivariance, which is a significant contribution to the field.\n+ The authors provide a detailed analysis of the mathematical properties of the proposed approach, including its equivariance and homogeneity properties. This analysis is essential for understanding the theoretical foundations of the approach.\n+The authors provide a detailed comparison with existing methods, highlighting the advantages of their approach."
                },
                "weaknesses": {
                    "value": "- The paper could benefit from more detailed explanations of some of the technical concepts and methods used, particularly for readers who are not familiar with the field. For example, the paper could provide more details on the mathematical background of  and its relevance to the problem at hand.\n\n- The paper could provide more details on the implementation of the proposed approach, including the datasets used in the experiments, the choice of neural network architecture and optimization algorithm. \n\n- The paper could benefit from a more detailed discussion of the limitations and potential future directions of the proposed approach.\n\n- While the proposed architecture is effective for learning equivariant polynomials, the LACK OF UNIVERSALITY mentioned could limit its applicability to more complex or diverse datasets. This could be a potential drawback when applying the proposed approach to real-world problems.\n\n- While the experimental results are promising, the authors could provide more detailed analysis and discussion of the results to further support their claims. For example, the paper could provide more details on the sensitivity of the proposed approach to hyperparameters and the robustness of the approach to noisy data."
                },
                "questions": {
                    "value": "Please check the Weaknesses listed above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825952339,
            "cdate": 1698825952339,
            "tmdate": 1699636669289,
            "mdate": 1699636669289,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "p3d2rfnHpa",
                "forum": "gyfXuRfxW2",
                "replyto": "OuiqNwHVjI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive review of our paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236836591,
                "cdate": 1700236836591,
                "tmdate": 1700236836591,
                "mdate": 1700236836591,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0wUkuuqQhN",
            "forum": "gyfXuRfxW2",
            "replyto": "gyfXuRfxW2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6163/Reviewer_AJ4x"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6163/Reviewer_AJ4x"
            ],
            "content": {
                "summary": {
                    "value": "This paper poses to solve certain polynomial optimization problems using architectures which respect the SL(2,R) symmetry. \n\nBut most of the critical details are looking very opaque."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper has definitely identified a very novel use case for neural nets \u2013 like positivity certification for polynomials. \n\nThe experimental data also seems reasonable."
                },
                "weaknesses": {
                    "value": "What is this $\\psi_n$ function in equation 2? This does not look like a Clebsch-Gordon coefficient.  \n\nSection 4.2 is extremely vague. The pseudocode is almost unreadable because it is calling functions (in lines 8 and 10) which has never been defined. Also, the entire motivation of this Section seems unclear to me, even if I assume the correctness of Lemma 1. How is this related to the training problem that eventually seems to be the target?  \n\nThe issues delineated in Section 4.3 do not seem relevant to the immediate question at hand which are all about certain polynomial optimizations. Or am I missing something? It would have been much better to use the space to explain what the experimental setup. Like it seems pretty critical to understand what is the author\u2019s idea of a \u201cnatural\u201d polynomial and these details are missing from the main paper! The loss functions used in this experiment also seem to be not clearly specified and that makes it further challenging to understand what is happening."
                },
                "questions": {
                    "value": "Q1. \n\nWhy is SL(2,R) equivariance crucial to the usecases identified here? \n\nIts not possible to make the connection between this group and the problem as stated in equation 1.  \n\nQ2. \n\nWhat is the training time for the nets involved in Table 2? I guess what is reported as \u201cMLP times\u201d are just the inference times, right? \n\nBut the timings specified for the other methods are probably the \u201ctotal\u201d time they take to run and there are no other time costs there.  \n\nQ3. \n\nWhat is the full and explicit specification of the loss function that is being optimized in the experiment in Section 5? \n\nAnd how does this respect SL(2,R)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698971476772,
            "cdate": 1698971476772,
            "tmdate": 1699636669183,
            "mdate": 1699636669183,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4YObUkFqRm",
                "forum": "gyfXuRfxW2",
                "replyto": "0wUkuuqQhN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer's comments/questions (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for providing feedback on our paper, and for highlighting the novelty of the positivity certification application. We hope to clarify the reviewer\u2019s questions below.\n\n### Transvectant/Clebsch-Gordan Clarification\n\nAlthough the form may look unfamiliar, the transvectant, which we denote by $\\psi$, indeed describes precisely the map from two irreps to their tensor product\u2019s decomposition back into irreps. It may be helpful to recall that the finite dimensional irreducible vector spaces of $SL(2,\\mathbb{R})$ can be identified with the homogeneous polynomials of a given degree, so the Clebsch-Gordan coefficients for $SL(2,\\mathbb{R})$ should map two input polynomials to an output polynomial. ($\\psi_n$ indeed maps two input polynomials to an output polynomial.) Please also see the first paragraph of the introduction to B\u00f6hning [1] for formal clarification that the transvectant operation, which we denote by $\\psi$, is exactly the Clebsch-Gordan map for this group. \n\n### Section 4.2 and pseudocode\n\nThank you for pointing out the ambiguity in the functions in the pseudocode in Section 4.2. We have corrected this in the updated draft.\n\nWe have also rewritten the description of the last layer in Section 4.2, deferring the exact details of the mathematical derivation (which may have been confusing) to the appendix. \n\n### Relevance of Section 4.3 to polynomial optimizations, and experimental details\n\nPositivity verification, as described in Section 3, is an equivariant learning task. Therefore, one might naturally design an equivariant architecture for this problem, as we do in Sections 4.1-4.2. The surprising, and central, theoretical contribution of this paper is that an equivariant network ostensibly designed for the very purpose of positivity verification, is in fact unable to approximate the function of interest for positivity verification. Therefore, any network designed to solve this problem must be able to output more than just equivariant polynomials, which is a significant restriction on the architectural design space relative to prior work! To the best of our knowledge, such a finding is also without precedent in the equivariance literature. It also provides some motivation for using data augmentation, rather than an equivariant architecture. This is the content of Section 4.3. Corollary 1 implies that it doesn\u2019t matter what loss function we use -- no equivariant polynomial can approximate the function of interest under any standard loss function. In the paper, we used the normalized mean squared error loss (with a small additive stabilization term in the denominator), as noted in the caption of Figure 2. \n\nBy \u201cnatural\u201d polynomials, we just meant that they come from a naturally occurring, application-specific distribution, such that it is empirically possible to well-approximate an NP-hard optimization like polynomial positivity verification over this distribution. (Intuitively, we would expect such a distribution to have low entropy -- it should differ significantly from a uniform distribution over polynomials in a norm-bounded ball, e.g.) For example, one of our experiments is on the polynomials that arise from spherical code bounds; this provides one example of a \u201cnatural\u201d set of polynomials, but there is no single definition of such a distribution. This point should not be critical to our main arguments.\n\n### Q1: Utility of $SL(2,\\mathbb{R})$ equivariance \n\nWhile the symmetry group is not crucial for studying the problem, equivariant learning has been helpful from a sample complexity perspective in many other contexts. Since our positivity verification problem is equivariant with respect to the large group  $SL(2,\\mathbb{R})$, it is natural to try exploiting this structure in the learning pipeline. Please see the paragraph after (1) for an explanation of the precise equivariance property, and please feel free to follow up with further questions if this point remains unclear.\n\n### Q2: Clarification on timings\n\nIndeed, the MLP times are for inference, while the Mosek times are full solves. The motivation of this work is to speed up the *online* determination of nonnegativity, which is possible with just MLP inference. Amortizing the runtime of a traditional solver over many instances is not possible, hence why we compare to the full solve time. For reference, the training time for the MLP was on the order of 90 minutes, which even included many different loss evaluations (e.g. evaluating the loss under random $SL(2,\\mathbb{R})$-augmentations)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236786254,
                "cdate": 1700236786254,
                "tmdate": 1700236786254,
                "mdate": 1700236786254,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]