[
    {
        "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens"
    },
    {
        "review": {
            "id": "2kSxDzbTv6",
            "forum": "HKJfSd5hcb",
            "replyto": "HKJfSd5hcb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2099/Reviewer_sgCo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2099/Reviewer_sgCo"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed to leverage LLMs to process and generate multimodal contents. Specifically, the paper leverages the pretrained MiniGPT-4, and extend it to generate interleaved image/text contents, by finetuning its output to contain both text tokens and visual tokens, where the visual tokens are fed to Stable Diffusion models in place of the original text-condition features to generate the images. The paper finetunes MiniGPT-4 on VIST and MMDialog dataset and demonstrates better results than the baselines."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Enhancing multimodal models to generate multimodal content is an important enhancement in capabilities with many potential applications.\n- The proposed model achieves a better performance than the baselines on two evaluated datasets."
                },
                "weaknesses": {
                    "value": "- It is unclear why GILL cannot be compared on VIST and MMDialog.\n  * GILL trains on CC3M image-text pairs dataset, and the images and captions are also used in this paper.\n  * (please correct me if I am wrong) GILL does not finetune on VIST / MMDialog while this paper does.\n\n- Lack of the synthetic caption baseline.\n  * Despite the paper argues the scarcity of the descriptive captions in the existing datasets and it is hard to automatically generate high-quality captions, there is no quantitative numbers justifying that the synthetic caption has poor quality.\n\n- The baseline experiments are not properly explained, and there can be as simple but stronger baselines for comparison\n  * Comparison with MiniGPT-4: the authors do not clearly explain the inputs and outputs used to finetune MiniGPT-4. A proper baseline would be to use MiniGPT-4 to caption the training images and train it to output both a text (story) and a description (prompt for SD).\n\u00a0\n- The design choices are not well-ablated/well-justified.\n\n  * The proposed voken dropping for CFG is not ablated. The simplest alternative is to just use the default empty feature of the SD in inference. An alternative is to learn the empty features, without messing the weights in the Feature Mapper.\n\n  * Why the number of vokens $n=8$? In Supp. Fig. 6, $n=16$ is consistently better than $n=8$ (significantly better in $IS$). Also, given the trend (especially given the large leap for IS from 8$\\rightarrow$16), why not ablating $n>16$? Furthermore, there are 77 tokens for the text encoder in Stable Diffusion -- in Table 6, MiniGPT-5 performs worse than Stable Diffusion, is this because $n$ is too small?\n\n- The name of the proposed approach is confusing and needs justification.\nThe proposed method is named MiniGPT-5. However, the naming is confusing and needs better justification -- there is no GPT-5 from OpenAI available yet. The naming itself can make people confused on whether OpenAI has released GPT-5 or not. Also, it is unclear why the authors name the approach this way."
                },
                "questions": {
                    "value": "### Questions\n\n> Two-stage Training Strategy: After the unimodal alignment stage, the model is capable of generating images for single text descriptions but struggles with interleaved vision-and-language generation, which includes multiple text-image pairs and requires complicated reasoning for both text and image generation.\n\nIs this because the base MiniGPT-4 is not trained to understand the interleaved image-text conversations? Is the second-stage training still needed if the base VLM is capable of understanding / processing interleaved image/text pairs?\n\n> The inclusion of classifier-free guidance during the training phase further re- fines generation quality.\n\nCan we really say that we include CFG during the training? Token dropping is not the same as CFG.\n\n> We also established unprecedented benchmarks on prominent datasets, including VIST and MMDialog.\n\nIt would be better justified on *unprecedented* benchmarks, if the authors can provide more substantitated explanations / justifications.\n\n### Minor Questions\n\n> We employ the loss of the latent diffusion model (LDM) for guidance.\n\nThe term \"guidance\" in the sentence creates ambiguity, especially when juxtaposed with terms like \"classifier guidance\" and \"classifier-free guidance.\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2099/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819637700,
            "cdate": 1698819637700,
            "tmdate": 1699636142402,
            "mdate": 1699636142402,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "ypWc0ytjrV",
            "forum": "HKJfSd5hcb",
            "replyto": "HKJfSd5hcb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2099/Reviewer_vkr4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2099/Reviewer_vkr4"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an interleaved vision-and-language generation model named MiniGPT-5. Architecturally, the model employs PEFT on MiniGPT-4 to generate text and vokens, and use a feature mapper consisting of an encoder-decoder to map vokens to the conditions of SD, which then generates the final image. For training, a Two-stage Training Strategy is used, starting with the Unimodal Alignment Stage (UAS) followed by the Multimodal Learning Stage (MLS). The model was evaluated on the MMDialog and VIST datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Generating text and images under Language and Image Context is an intriguing task with potential real-world applications.\n2. The proposed method leverages pre-trained text and image generation models, requiring only paramter-efficient fine-tuning the language model and the training of the Feature Mapper, resulting in reduced training overhead."
                },
                "weaknesses": {
                    "value": "1. Based on the experimental results, the proposed approach did not exhibit significant improvements on the VIST dataset compared to standalone SD. On the MMDialog dataset, its Inception Score (IS) was lower than that of Divter. The quality of showcased generated image (such as human) is also not satisfying\n2. Several existing methods can achieve interleaved vision-and-language generation, such as Visual ChatGPT. A straightforward method might involve a language model determining if image generation is needed, then producing a descriptive segment about the image to input into Stable Diffusion. The paper does not compare against such methods nor articulates advantages over them."
                },
                "questions": {
                    "value": "1. A potential advantage of this method may lie in its better understanding of image context, linking prior images to generate new ones. An example is editing images based on prior images and language instructions. Can the authors provide examples in the realm of image editing?\n2. In the results for the VIST dataset (Table1), (1) With Text Context, MiniGPT-5 underperforms compared to Finetuned SD; (2) MiniGPT-5's FID in *Image-Text Context* is even worse than in *Image-Context*. Why?\n3. Can the authors provide additional examples, particularly multi-turn image-text dialogues?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "1. The experimental results are not compelling.\n2. The work lacks comparison with sufficient kinds of interleaved vision-and-language generation methods. It does not display evident advantages over methods like Visual ChatGPT."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2099/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2099/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2099/Reviewer_vkr4"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2099/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839441887,
            "cdate": 1698839441887,
            "tmdate": 1699636142337,
            "mdate": 1699636142337,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "P6ogfteEW2",
            "forum": "HKJfSd5hcb",
            "replyto": "HKJfSd5hcb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2099/Reviewer_zEXf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2099/Reviewer_zEXf"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces miniGPT-5, a model capable of generating multimodal outputs, including both text and images. The authors integrate a vision encoder with Large Language Models (LLMs) to produce vokens, facilitating multimodal generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The concept of the paper is clear and straightforward, presenting a meaningful advancement in the integration of multimodal inputs and outputs within LLMs.\n\nThe authors have conducted comprehensive experiments across various cases, such as text-to-image, (text, image) to image, and (text, image) to (text, image), demonstrating the versatility of miniGPT-5."
                },
                "weaknesses": {
                    "value": "The paper lacks clarity on how the rationality of human evaluation in Table 4 is assessed, raising questions about the validity and reliability of these subjective measures.\n\nIn terms of image generation performance, miniGPT-5 falls short of surpassing the results achieved by the SD2 model on the CC3M dataset.\n\nThe use of the title \"miniGPT-5\" could potentially lead to confusion within the community, as the structure and capabilities of GPT-5 remain undefined at this time."
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2099/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2099/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2099/Reviewer_zEXf"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2099/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698844037386,
            "cdate": 1698844037386,
            "tmdate": 1699636142249,
            "mdate": 1699636142249,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "hEEtOWKXxl",
            "forum": "HKJfSd5hcb",
            "replyto": "HKJfSd5hcb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2099/Reviewer_yx5e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2099/Reviewer_yx5e"
            ],
            "content": {
                "summary": {
                    "value": "This paper connects the pretrained MinIGPT-4 and text-to-image Stable Diffusion model via a feature mapper module for multimodal generation. The feature mapper module includes a two-layer MLP and an encoder-decoder transformer to transform the voken features as a conditional signal for the following SD-based image generation. \n\nThe authors also propose a two-stage training method, Unimodal Alignment Stage (UAS) and Multimodal Learning Stage (MLS). The former one utilizes image caption dataset to enhance the single text-based image generation capability. The latter one aims to improve the interleaved visual-textual generation. \n\nThe experiments on three datasets (VIST, MMDialog and CC3M) verify the proposed MiniGPT-5."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) Multimodal generation is now a popular direction. This paper improves MiniGPT-4 so that the model can output text and images. Previous MiniGPT-4 can not generate visual content.\n\n2) The proposed solution is simple and easy to understand. They combine MiniGPT-4 and SD model for generating text and image respectively. In order to improve the quality of multimodal generation, the authors propose an effective two-stage training method and some auxiliary losses.\n\n3) The authors provide extensive analysis for their experimental results."
                },
                "weaknesses": {
                    "value": "1) Unfair comparison. \n\na) [VIST Human Evaluation] The previous MiniGPT-4 itself does not have multimodal generation capability, and it is unacceptable to directly use its output as the input of SD model. Thus, the results on Table 4 cannot illustrate the effectiveness of the proposed method.\n\nb) It is necessary to list and compare model parameters and training data with some baselines, such as SD 2, MiniGPT-4, Divter, GILL.\n\n\n2) The experimental results are not satisfactory.\n\nThe proposed method performs worse than fine-tuned MiniGPT-4 on narration generation, as shown in Table 3.\n\n3) The presentation is general. Fig 2 can be improved. \n\nOther comments:\n1) Page 2: Stable DIffusion -> Stable Diffusion\n\n2) Caption of Fig 2: Stable Duffision -> Stable Diffusion\n\n3) Fig 2: Learable -> Learnable"
                },
                "questions": {
                    "value": "1) How to understand the learnable queries in Figure 2? Why is it input directly into transformer's decoder.\n\n2) Why is the proposed method a description-free learning process?\n\n3) There are many multimodal generation methods. Why did you not conduct a performance comparison, but focused on SD 2 and MiniGPT-4. Moreover, when comparing with GILL, the original experimental settings were not followed.\n\n[1] Grounding Language Models to Images for Multimodal Inputs and Outputs\n\n[2] NExT-GPT: Any-to-Any Multimodal LLM"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2099/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699016287190,
            "cdate": 1699016287190,
            "tmdate": 1699636142184,
            "mdate": 1699636142184,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]