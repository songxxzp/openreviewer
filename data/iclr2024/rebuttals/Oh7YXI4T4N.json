[
    {
        "title": "Learning Diverse Quadruped Locomotion Gaits via Reward Machines"
    },
    {
        "review": {
            "id": "hGEFnjmLu5",
            "forum": "Oh7YXI4T4N",
            "replyto": "Oh7YXI4T4N",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8468/Reviewer_i6Hh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8468/Reviewer_i6Hh"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a reward machine to allow learning different quadrupedal gaits for quadrupedal robots. Multiple gaits are learned for a real quadrupedal robot, including novel gaits such as Three-One."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. A novel way to encourage policies to produce different gaits for quadrupedal robots. Including the learning of novel gaits.\n\n2. Good ablation studies to evaluate the importance of different components."
                },
                "weaknesses": {
                    "value": "It will be nice to also learn transitions between gaits."
                },
                "questions": {
                    "value": "It will be interesting to evaluate energy consumption for different gaits at different speeds, e.g., one will expect certain gaits to be more energy efficient at high speed while less at low speed. It will also be fun to try out gaits that are typical at high speed in nature, like galloping, even if only demonstrated in simulation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8468/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698296726456,
            "cdate": 1698296726456,
            "tmdate": 1699637057140,
            "mdate": 1699637057140,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oeveTwXA09",
                "forum": "Oh7YXI4T4N",
                "replyto": "hGEFnjmLu5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8468/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8468/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We respond to each comment below.\n\n> 1. A novel way to encourage policies to produce different gaits for quadrupedal robots. Including the learning of novel gaits.\n> 2. Good ablation studies to evaluate the importance of different components.\n\nWe thank the reviewer for the positive comments.\n\n> It will be nice to also learn transitions between gaits.\n\nTransitioning between locomotion gaits is an interesting open problem in legged locomotion. We believe that transitioning between different reward machines can be a useful contribution to this problem. We mention this as a future direction in section 5: \u201c...nor have we studied how to smoothly transition between gaits\u201d. We feel this is better suited for future work due to the challenging nature of this problem - it requires smoothly transitioning between policies, or training a unified policy for multiple gaits.\n\n> It will be interesting to evaluate energy consumption for different gaits at different speeds, e.g., one will expect certain gaits to be more energy efficient at high speed while less at low speed. It will also be fun to try out gaits that are typical at high speed in nature, like galloping, even if only demonstrated in simulation.\n\nWe agree with both of these comments. There are many dimensions to evaluate different gaits on, including speed (linear and angular), gait frequency, and terrain. Across each dimension, we can measure different variables such as energy consumption, stability, and velocity tracking accuracy. This evaluation can occur in simulation and on hardware. We can also further expand the diversity of learned gaits to include galloping, canter, and skipping to name a few."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8468/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700080845883,
                "cdate": 1700080845883,
                "tmdate": 1700080845883,
                "mdate": 1700080845883,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uIB2qFEQx1",
            "forum": "Oh7YXI4T4N",
            "replyto": "Oh7YXI4T4N",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8468/Reviewer_6Qms"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8468/Reviewer_6Qms"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces reward machines for learning different quadrupedal gaits called Reward Machine-based Locomotion Learning (RMLL). The key to the proposed approach is introducing high-level gait specifications as automaton states and a counter to control gait frequency. The authors construct a automaton via LTL formulas representing foot-contacts. PPO is used to learn a policy which takes the state as a combination of automaton state, frequency counter, proprioception and commands to output target joint angles. More rewards are given for transitioning to subsequent automaton states. All the learned gaits are demonstrated in the real-world."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The presented approach shows a straightforward way of learning different gaits via adding automaton structure to reward function. High-level state conditions and transitions are characterized by foot contacts and used to motivate the desired automaton state transitions. The added benefit of controlling the gait frequency at execution adds to the contributions of RMLL."
                },
                "weaknesses": {
                    "value": "The proposed method\u2019s novelty is highly limited and is more a robotics application of the base method proposed for using reward machines with RL (Icarte et al. 2018, 2019, 2022). Using automaton states with the state of the MDP has been introduced in Icarte et al. 2018. The authors introduce the timestep counter for controlling automaton state transitions.\n\nThe authors have not mentioned the exact representation of the automaton state $u$ in the input to the policy. Is it a vector of boolean? How is it different when the RM state is replaced with that of foot contacts? In that ablation, do you still keep $\\phi$ for the No-RM-Foot-Contacts case?\n\nCan you please clarify?: With foot-contacts, two consecutive states can have two different foot-contacts with random policy. However, with reward machines the RM state is constant until a transition happens? Also, in all the ablation of state space, the reward structure is based on the RM right?\n\nLearning policy for individual gaits is limited contribution in itself. How is the energy consumption study relevant to show the efficacy of the proposed approach? If you already know which gait consumes least energy while maintaining stability as a function of the terrain, why cannot a terrain based reward machine states be formulated?\n\n\n[1] Rodrigo Toro Icarte, Toryn Klassen, Richard Valenzano, and Sheila McIlraith. Using reward ma- chines for high-level task specification and decomposition in reinforcement learning. In Interna- tional Conference on Machine Learning, pp. 2107\u20132116. PMLR, 2018.\n\n[2] Rodrigo Toro Icarte, Ethan Waldie, Toryn Klassen, Rick Valenzano, Margarita Castro, and Sheila McIlraith. Learning reward machines for partially observable reinforcement learning. Advances in Neural Information Processing Systems, 32:15523\u201315534, 2019.\n\n[3] Rodrigo Toro Icarte, Toryn Q Klassen, Richard Valenzano, and Sheila A McIlraith. Reward ma- chines: Exploiting reward function structure in reinforcement learning. Journal of Artificial In- telligence Research, 73:173\u2013208, 2022."
                },
                "questions": {
                    "value": "See weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8468/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795726434,
            "cdate": 1698795726434,
            "tmdate": 1699637057024,
            "mdate": 1699637057024,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BgY16b60n7",
                "forum": "Oh7YXI4T4N",
                "replyto": "uIB2qFEQx1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8468/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8468/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We respond to each comment below.\n\n> The presented approach shows a straightforward way of learning different gaits via adding automaton structure to reward function. High-level state conditions and transitions are characterized by foot contacts and used to motivate the desired automaton state transitions. The added benefit of controlling the gait frequency at execution adds to the contributions of RMLL.\n\nWe thank the reviewer for the positive comments.\n\n> The proposed method\u2019s novelty is highly limited and is more a robotics application of the base method proposed for using reward machines with RL (Icarte et al. 2018, 2019, 2022). Using automaton states with the state of the MDP has been introduced in Icarte et al. 2018. The authors introduce the timestep counter for controlling automaton state transitions.\n\nWhile true that our base method relies on the existing reward machine framework, we are the first to leverage this framework for the important application of quadruped locomotion, which improves over existing quadruped locomotion approaches by specifying and learning **arbitrary** gaits through foot contact sequences **with minimal manual effort** (i.e. no motion priors or dynamics models needed). See Section 2.3 for discussion on our novelty compared with other locomotion approaches which learn diverse gaits. We leverage ideas from the machine learning community (specifically, the RM framework) to enable both easy specification and efficient learning of new gaits, which is the main contribution of this work.\n\n> The authors have not mentioned the exact representation of the automaton state $u$ in the input to the policy. Is it a vector of boolean? How is it different when the RM state is replaced with that of foot contacts? In that ablation, do you still keep $\\phi$ for the No-RM-Foot-Contacts case?\n\nIn Section 3.2 we mention: \u201cThe RM state is encoded as a one-hot vector, \u2026\u201d. When the RM state is replaced with foot contacts for \u201cNo-RM-Foot-Contacts\u201d and \u201cNo-RM-History\u201d ablations, we remove the RM state from the state space, and replace it with a boolean vector of foot contacts (four dimensions, one boolean value per foot). We have updated the paper (section 4.2 in blue) to make it more clear. We keep $\\phi$ in the state space for all ablations, including No-RM-Foot-Contacts.\n\n> Can you please clarify?: With foot-contacts, two consecutive states can have two different foot-contacts with random policy. However, with reward machines the RM state is constant until a transition happens? \n\nWe are not positive we understand what the reviewer is referring to with respect to \u201ctwo consecutive states\u201d (automata states?) and \u201crandom policy\u201d? It is the case that the RM state is constant until a desired **gait transition** occurs. Note that the RM takes a transition at every timestep (following the RM framework in Icarte et al., 2018), however in our work we define the transition function to take a self-loop unless a desired gait transition occurs (see Figure 3, and Appendix A). In other words, we take an RM transition at every timestep, however the RM does not transition to a **new** automaton state unless the next desired gait pose is reached.\n\n> Also, in all the ablation of state space, the reward structure is based on the RM right?\n\nEach of our ablations are trained on and evaluated over the same reward function, which is based on the RM.\n\n> Learning policy for individual gaits is limited contribution in itself. \n\nWe respectfully disagree. Compared to existing works, we can specify arbitrary gaits given a desired foot contact sequence (beyond two-beat gaits) with minimal human efforts (no motion priors or dynamics models). To the best of our knowledge, no existing methods support this capability.\n\n> How is the energy consumption study relevant to show the efficacy of the proposed approach? If you already know which gait consumes least energy while maintaining stability as a function of the terrain, why cannot a terrain based reward machine states be formulated?\n\nThe purpose of the energy consumption study is to motivate the reason for learning diverse locomotion gaits at all. According to the results of the study, different gaits are beneficial in terms of stability and energy consumption based on terrain type. Thus, learning diverse quadruped locomotion gaits is an important problem. We do not already know \u201cwhich gait consumes least energy while maintaining stability as a function of the terrain\u201d before the study is conducted. While possible to create terrain-based reward machines, it is unclear to us what their objective is. Also, we would need to somehow estimate the terrain conditions on the real robot. It is an interesting direction, but out of the scope of our current work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8468/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700080626796,
                "cdate": 1700080626796,
                "tmdate": 1700080626796,
                "mdate": 1700080626796,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "z1Hi6KhEN0",
            "forum": "Oh7YXI4T4N",
            "replyto": "Oh7YXI4T4N",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8468/Reviewer_UMdH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8468/Reviewer_UMdH"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce reward machine, a state machine based mechanism to shape complex, state/time dependent rewards for dynamic locomotion control problems. For each desired quadruped gait, an automata is constructed to modulate foot contact transitions and timings. Then, the automata state, proprioceptive state (including estimations) from the robot, as well as gait parameters are used as the state vector for reinforcement learning training. The authors train a few different gaits in simulation and transfer the policies to the real hardware."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The strengths of the paper include:\n\n1) Introduction of the reward machine for locomotion control and gait specification.\n2) Sim2real transfer of learned policies to the real robot"
                },
                "weaknesses": {
                    "value": "The weaknesses of the paper are:\n\n1) While the concept of the reward machine is new especially in the locomotion learning community, in reality it is merely a fancy way of constructing a state machine which controls the gait transition. \n2) The tasks in this paper are not novel. I see only flat terrain locomotion with a few gaits, and it is hard to justify why a complex state machine is needed, given there are works that can also achieve diverse gaits with time based rewards: \"Walk These Ways: Tuning Robot Control for Generalization with Multiplicity of Behavior\".\n3) Other than that, the learning is conducted in Isaac Gym with PPO and there is limited novelty."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8468/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699216190550,
            "cdate": 1699216190550,
            "tmdate": 1699637056910,
            "mdate": 1699637056910,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C7EVhZqCGa",
                "forum": "Oh7YXI4T4N",
                "replyto": "z1Hi6KhEN0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8468/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8468/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We respond to each comment below.\n\n> In this paper, the authors introduce reward machine, a state machine based mechanism to shape complex, state/time dependent rewards for dynamic locomotion control problems. \n\nThis is a major misunderstanding. This paper does not introduce reward machines. Reward machines were introduced by Icarte et al., 2018, which was cited in Section 2.1. In this paper, we introduce reward machines into the literature for easily specifying and learning diverse locomotion gaits.\n\n>The strengths of the paper include:\n\n> 1. Introduction of the reward machine for locomotion control and gait specification.\n> 2. Sim2real transfer of learned policies to the real robot\n\nWe thank the reviewer for the positive comments.\n\n> The weaknesses of the paper are:\n\n  > 1.  While the concept of the reward machine is new especially in the locomotion learning community, in reality it is merely a fancy way of constructing a state machine which controls the gait transition. \n\nWe are not sure what weakness the reviewer is trying to point out here. Indeed we are \u201cconstructing a state machine which controls the gait transition\u201d. This state machine specifies arbitrary locomotion gaits over foot contact sequences, along with gait frequencies. This enables diverse gait specification and learning with minimal manual efforts compared to related works.\n\n> 2. The tasks in this paper are not novel.\n\nWe specify and learn **arbitrary** gaits through foot contact sequences **with minimal manual effort** (i.e. no motion priors or dynamics models needed). While the task of learning diverse quadruped locomotion gaits certainly is not novel (there are many papers on this task throughout the years, as discussed and cited in Section 2.3), we approach the problem from a unique angle.\n\n> I see only flat terrain locomotion with a few gaits, and it is hard to justify why a complex state machine is needed, given there are works that can also achieve diverse gaits with time based rewards: \"Walk These Ways: Tuning Robot Control for Generalization with Multiplicity of Behavior\".\n\nWe already mention how our work differs from \u201cWalk These Ways: Tuning Robot Control for Generalization with Multiplicity of Behavior\" in Section 2.3: \n\n\u201cOther similar works involve specifying and learning diverse locomotion gaits through explicitly defining swing and stance phases per leg (Siekmann et al., 2020; Margolis & Agrawal, 2023). The former approach is designed for a bipedal robot, while the later only supports two-beat quadruped gaits. In contrast, RMLL only needs foot contact sequences (instead of leg-specific timings), and can specify and learn arbitrary quadrupedal gaits well beyond the set of two-beat gaits.\u201d\n\nNote that in Section 2.3, we discuss how our approach differs from various other related works which also achieve diverse gaits. \n\n> 3. Other than that, the learning is conducted in Isaac Gym with PPO and there is limited novelty.\n\nWe respectfully disagree that using Isaac Gym negatively affects the novelty of our approach (or other approaches in the literature). It is very common to train quadruped locomotion policies in Isaac Gym with PPO (See Agarwal et al.,2023, Zhuang et al., 2023, or \u201cDeep whole-body control: learning a unified policy for manipulation and locomotion\u201d for example). We do not claim to improve the Isaac Gym physics simulation environment, nor do we claim to improve PPO. We are happy to leverage an existing physics simulation environment and RL algorithm for our work, which contributes to quadruped locomotion in a different dimension."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8468/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700079842695,
                "cdate": 1700079842695,
                "tmdate": 1700079842695,
                "mdate": 1700079842695,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EYXQwZd5m2",
            "forum": "Oh7YXI4T4N",
            "replyto": "Oh7YXI4T4N",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8468/Reviewer_MmwQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8468/Reviewer_MmwQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a reward design system via a set of state machines that consists of individual conditions on each of the foot contacts. This allows the user to specify different types of gaits easily. Diverse gait policies are trained via sim2real (in isaac gym) and deployed on hardware. The videos show gaits like jumping, trotting, running etc."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The problem of automatic reward design is important for training more general policies\n- The robot videos look good and the gaits are very diverse\n- The motivation and approach are well-presented\n- There is a lot of good analysis on the experiments and ablations, especially about the differences in the gaits"
                },
                "weaknesses": {
                    "value": "- I am unclear about the exact novelty of this approach - as it is already common in RL-based locomotion setups to use foot poses to generate different gaits. \n\n- It would be more interesting to see how these reward machines can be used to do more complex, long-horizon tasks, like walking with multiple gaits or imitating a long reference trajectory. Specifically, how can one transition between different machines?"
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8468/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8468/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8468/Reviewer_MmwQ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8468/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699723910721,
            "cdate": 1699723910721,
            "tmdate": 1699723910721,
            "mdate": 1699723910721,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z2kIqN9ThE",
                "forum": "Oh7YXI4T4N",
                "replyto": "EYXQwZd5m2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8468/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8468/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We respond to each comment below.\n\n>Strengths:\n\n> - The problem of automatic reward design is important for training more general policies\n> - The robot videos look good and the gaits are very diverse\n> - The motivation and approach are well-presented\n> - There is a lot of good analysis on the experiments and ablations, especially about the differences in the gaits\n\nWe thank the reviewer for the positive comments.\n\n> Weaknesses:\n\n> - I am unclear about the exact novelty of this approach - as it is already common in RL-based locomotion setups to use foot poses to generate different gaits.\n\nThere are indeed existing RL-based locomotion setups which use foot poses to generate different gaits (we cite some of these approaches in Section 2.3). Our approach (RMLL) is a novel paradigm for gait specification, which requires less manual effort (only an automaton over the desired foot contact sequence), and can specify arbitrary gaits beyond simple two-beat gaits when compared to existing works such as Margolis & Agrawal, 2023; Tang et al., 2023. We believe our approach is novel w.r.t. the literature, and hope the reviewer appreciates our contribution.\n\n> - It would be more interesting to see how these reward machines can be used to do more complex, long-horizon tasks, like walking with multiple gaits or imitating a long reference trajectory. \n\nAlthough RMs are commonly used for high-level, long-horizon tasks (i.e. Toro Icarte et al., 2019), the focus of our work is on low-level locomotion. While it would be interesting to leverage RMs for long-horizon quadruped tasks, this is somewhat orthogonal to locomotion policy learning. An interesting follow-up work can involve optimally leveraging these different gaits to efficiently traverse various terrains. This follow-up approach can leverage reward machines for long-horizon tasks.\n\n> Specifically, how can one transition between different machines?\n\nTransitioning between locomotion gaits is an interesting open problem in legged locomotion. We believe that transitioning between different reward machines can be a useful contribution to this problem. We mention this as a future direction in section 5: \u201c...nor have we studied how to smoothly transition between gaits\u201d. We feel this is better suited for future work due to the challenging nature of this problem - it requires smoothly transitioning between policies, or training a unified policy for multiple gaits."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8468/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700079083756,
                "cdate": 1700079083756,
                "tmdate": 1700079083756,
                "mdate": 1700079083756,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]