[
    {
        "title": "Weakly Supervised Graph Contrastive Learning"
    },
    {
        "review": {
            "id": "coigI72Gfw",
            "forum": "4UP387Adir",
            "replyto": "4UP387Adir",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6809/Reviewer_YxoE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6809/Reviewer_YxoE"
            ],
            "content": {
                "summary": {
                    "value": "This paper targets the weak/noisy label contrastive learning task. The contributions lie in two perspectives: Under the context of weak label  graph representation learning 1) the authors demonstrate that prior graph contrastive learning works do not show obvious robustness across different levels of noise; 2) the authors propose WSNet, which shows relatively superior robustness over weak/noisy labels. The authors also conduct ablation experiments to prove the necessity of the combination of the two defined losses."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper is well-written. It is very easy to follow, and the authors provide thorough details about the problem setting, loss definition, as well as experimental settings.\n2.\tThe experiments are extensive under the weak/noisy label setting.\n3. The authors provide codes for reproduction."
                },
                "weaknesses": {
                    "value": "1. The weak/noisy label setting appears to be confined to a limited context, especially on graphs. While I acknowledge the contribution of this paper in this specific area, its applicable generality to real-world graph datasets is questionable.\n2. The analysis to the baseline GCL methods performance under noisy settings are shallow to some extent. The authors may consider some further analysis. For example, how well would each baseline perform under different types of classifier? What are the samples in common that are \"robust\" to such weak/noisy labels for each baseline?\n3. An MLP is widely-used classifier as well. The authors may consider adding it to the experiments."
                },
                "questions": {
                    "value": "Refer to weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6809/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6809/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6809/Reviewer_YxoE"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6809/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698458532388,
            "cdate": 1698458532388,
            "tmdate": 1699636787189,
            "mdate": 1699636787189,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Utfu4ShYTQ",
                "forum": "4UP387Adir",
                "replyto": "coigI72Gfw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6809/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6809/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer YxoE"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback and suggestions.\n\n1. *The weak/noisy label setting appears to be confined to a limited context, especially on graphs. While I acknowledge the contribution of this paper in this specific area, its applicable generality to real-world graph datasets is questionable.*\n\nResponse: We believe that such a weak supervision approach will benefit graph learning especially in text-attributed graphs where it is easier to obtain weak labels based on keyword matching, etc. Even with the recently growing popularity of text-attributed graphs such as in recommender systems[2] and fake news detection[1], where obtaining ground truth labels is challenging, the proposed weak supervision approach is useful. \n\n2. *The analysis to the baseline GCL methods performance under noisy settings are shallow to some extent. The authors may consider some further analysis. For example, how well would each baseline perform under different types of classifier? What are the samples in common that are \"robust\" to such weak/noisy labels for each baseline?*\n\nResponse: We agree that experimenting with different types of classifiers is interesting and we will consider it for our future work. In the current paper, since we are interested in studying the robustness of the learned GCL embeddings to label noise in downstream classification, we think that the choice of the classifier would have relatively less effect on the final conclusion as the same classifier is used for comparing all baseline methods. It might be challenging to identify the common samples that are robust to weak labels for each baseline. Do you mean to consider robustness to be based on low noise or based on correct prediction? Please clarify the question. \n\n3. *An MLP is widely-used classifier as well. The authors may consider adding it to the experiments.*\n\nResponse: Typically in contrastive learning, the learned embeddings are evaluated for linear separability using a simple linear classifier such as logistic regression. Although MLPs are generally better than LR, they further transform the learned node representations to another embedding space (if multiple layers). Having said that, it is an experiment that can be added but we believe tells us little more about the effectiveness of our method.\n\n[1] Adrien Benamira, Benjamin Devillers, Etienne Lesot, Ayush K Ray, Manal Saadi, and Fragkiskos D Malliaros. 2019. Semi-supervised learning and graph neural networks for fake news detection. In Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining. 568\u2013569.\n\n[2] Jason Zhu, Yanling Cui,Yuming Liu, Hao Sun, Xue Li, Markus Pelger, Tianqi Yang, Liangjie Zhang, Ruofei Zhang, and Huasha Zhao. 2021. Textgnn:Improving text encoder via graph neural network in sponsored search. In Proceedings of the Web Conference 2021.2848\u20132857."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6809/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241204745,
                "cdate": 1700241204745,
                "tmdate": 1700241204745,
                "mdate": 1700241204745,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dX5hyMOeyR",
                "forum": "4UP387Adir",
                "replyto": "Utfu4ShYTQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6809/Reviewer_YxoE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6809/Reviewer_YxoE"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Authors"
                    },
                    "comment": {
                        "value": "Thank you for the response. I agree that the type of the classifier may not effect much on the conclusion. However, there are still some other ways to validate the qualities of the embeddings (under different levels of noise). For example, show TSNE under different levels and compare it with other baselines (this is a shallow sample. the authors may consider others). I apologize if I am not clear enough. By \"the samples in common that are 'robust' to such weak/noisy labels for each baseline\", I mean identifying the sample group that is robust to the noise. For example, nodes with high degrees. I would like to see deeper analysis to show why/how WSNet works, rather than simply showing that it works. I hope it clarifies your questions."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6809/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674757961,
                "cdate": 1700674757961,
                "tmdate": 1700674757961,
                "mdate": 1700674757961,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WzvVKpDq2y",
            "forum": "4UP387Adir",
            "replyto": "4UP387Adir",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6809/Reviewer_waBU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6809/Reviewer_waBU"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a noisy-label learning method for graph contrastive learning which incorporates signals from weak labels. The  authors aim to explore the robustness of GCL methods to label noise and combine weak labels with graph communities to obtain better node representations. Extensive experiments illustrate the robustness of the node representations learned using GCL to weakly supervised classification and the effectveness of using weak labels to learn more robust embeddings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.  The description and formulation of the proposed algorithm are clear. The idea of combining the weak labels with graph communities to learn node representations  is novel. \n\n2. The experimental analysis is extensive and the article provides a comprehensive evaluation of the proposed algorithm on multiple benchmark datasets, demonstrating its effectiveness in various noise settings. \n\n3. The authors have provided sufficient details about the datasets and the experimental setup, which is commendable."
                },
                "weaknesses": {
                    "value": "1. How does the weak label be generated? The author only state that the weak label is generated by the labeling function but without no more elaboration. If the label is not given, then how the weak label can be generated with a certain accuracy? A comparison on different labeling function could also be helpful. \n\n2. The definition of robustness of GCL to label noise is very confusing. 1) Is label noise meaning the inaccuracy of true label or generated weak label? 2) It follows a logical intution that when there is a high level of lable noise in a dataset, the accuracy of a model trained on this dataset is likely to be low. If the label noise in a dataset as high as 53%, ahieveing high accuracy could be somewhat meaningless. \n\n3. The idea of this paper is very similar to cluster-based graph contrastive learning such as [1] which utilize the cluster or community information as auxiliary information for learning objective.  Thus, the author should concentrate on comparining with these baselines. \n\n4. The methods should be evaluated on more large-scale datasets such as ogbn datasets or Aminer-CS datasets. The datasets containing only hundreds or thousands of nodes are less convincing. \n\n5. The presentation of the paper should be improved. For example, the caption of table should apperaove above the table; It is better not to place any context between two tables. \n[1] CLEAR: Cluster-Enhanced Contrast for Self-Supervised Graph Representation Learning"
                },
                "questions": {
                    "value": "See weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6809/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698569353448,
            "cdate": 1698569353448,
            "tmdate": 1699636787068,
            "mdate": 1699636787068,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gm2JuukhLz",
                "forum": "4UP387Adir",
                "replyto": "WzvVKpDq2y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6809/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6809/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer waBU"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback and suggestions.\n\n1. *How does the weak label be generated? The author only state that the weak label is generated by the labeling function but without no more elaboration. If the label is not given, then how the weak label can be generated with a certain accuracy? A comparison on different labeling function could also be helpful.*\n\nResponse: In our paper, the weak labels are generated using Algorithm 3 which requires the ground truth label as it is a synthetic experiment setup. In real-world problems, when the ground truth is not available, LFs look for clues within the data. For example, in citation networks like Cora, the abstract of the papers (nodes) contains certain keywords related to a class label. So if these keywords are present in a paper abstract, we can assign the corresponding class as its weak label.\n\n2. *The definition of robustness of GCL to label noise is very confusing. 1) Is label noise meaning the inaccuracy of true label or generated weak label? 2) It follows a logical intution that when there is a high level of lable noise in a dataset, the accuracy of a model trained on this dataset is likely to be low. If the label noise in a dataset as high as 53%, ahieveing high accuracy could be somewhat meaningless.*\n\nResponse: To clarify, the weak label or label noise is only on the generated weak label and not the ground truth label. The proposed WSNet model is trained using the noisy labels but evaluated with the ground truth. Despite training the model with 50% noisy labels, it achieves a 68% F1 score (on Cora) when the predicted labels are compared to the ground truth labels (which have no noise).  In other words, the train labels are always noisy and the test labels used for evaluating the predictions are clean. \n\n3. *The idea of this paper is very similar to cluster-based graph contrastive learning such as [1] which utilize the cluster or community information as auxiliary information for learning objective. Thus, the author should concentrate on comparining with these baselines.*\n\nResponse: Thank you for sharing this reference. This work is focused on graph prediction whereas we are focused on node-level tasks. However, we will try our best to compare WSNet with this method for the future versions of our paper due to the time constraint and inaccessibility of their code. \n\n5. *The presentation of the paper should be improved. For example, the caption of table should apperaove above the table; It is better not to place any context between two tables.*\n\nResponse: Thank you for the suggestion. We will update this in the revised version."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6809/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240985157,
                "cdate": 1700240985157,
                "tmdate": 1700240985157,
                "mdate": 1700240985157,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xrn3wcESgu",
                "forum": "4UP387Adir",
                "replyto": "gm2JuukhLz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6809/Reviewer_waBU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6809/Reviewer_waBU"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for their response to my initial review. Despite the clarifications provided, my primary concern regarding the novelty of this work, especially in relation to existing literature, remains unaddressed:\n\n1. Comparison with Cluster or Community-Based Methods: The concept presented in the paper closely aligns with cluster or community-based graph contrastive learning approaches. These methods categorize nodes into different clusters/communities and utilize these auxiliary information in contrastive learning, often performing \"contrastive learning\" within the same community/cluster. A search  keywords including \u201ccluster-based\u201d and \u201cgraph contrastive learning\u201d yields [1]. The authors need to conduct a more comprehensive survey of these methods and draw clear distinctions, highlighting the unique advantages of their approach. Even if some of these methods are not open-sourced, it\u2019s essential to articulate what sets this work apart from such cluster/community-based strategies.\n\n2. Application of CV Domain Methods to Graphs: The paper seems to transpose contrastive learning techniques from the CV domain, as seen in [2], to graph data. However, [2] is only briefly mentioned in the related work section, without a detailed comparison. The authors should thoroughly compare their approach with [2], clarifying which aspects are specifically designed for graphs. If the methodology primarily involves substituting CV encoders with GNNs and replacing image clustering with node clustering methods, it might limites the novelty of the paper.\n\nIn conclusion, a more extensive literature survey is necessary, comparing the proposed method with similar cluster-based graph contrastive learning approaches and the foundational weakly supervised contrastive learning method in [2]. The authors should intuitively explain their method\u2019s advantages at the very least on a methodological level. Therefore, I maintain my original score for this paper.\n\n[2] Weakly supervised contrastive learning. ICCV 2021."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6809/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469628159,
                "cdate": 1700469628159,
                "tmdate": 1700469628159,
                "mdate": 1700469628159,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VKkddQR3Iv",
            "forum": "4UP387Adir",
            "replyto": "4UP387Adir",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6809/Reviewer_oC5W"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6809/Reviewer_oC5W"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates node representation learning in Graph Contrastive Learning (GCL) under weak supervision. Firstly, the paper analyzes the robustness of node representations learned by existing methods under weak supervision and concludes that they are all affected by label noise. To mitigate this issue, this work leverages graph structures to identify more relevant positive sample pairs. Specifically, it identifies nodes belonging to the same community from the entire graph as positive sample pairs. \n\nOverall, I think the paper is good but not quite up to ICLR standards. Mainly, it lacks some necessary experiments, such as efficiency analysis, feasibility on large-scale graphs, and so on. Further analysis of the possible impacts of the work would be helpful."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The experimental performance is good.\n\n2. The work conducts experiments on different levels of homogenous graph datasets, constructing positive sample pairs by selecting nodes belonging to the same community. I believe this is meaningful for graph contrastive learning."
                },
                "weaknesses": {
                    "value": "1. Identifying nodes belonging to the same community from the entire graph is a computationally intensive operation. Including an analysis of efficiency would enhance the completeness of this work.\n\n2. The paper lacks diagrams/figures, training details, or efficiency analysis.\n\n3. Theoretical analysis in this paper is insufficient.\n\n4. The dataset used in this work is relatively small. The authors mention in the paper that this work can be extended to larger datasets. Conducting experiments on larger datasets,  such as the OGB dataset, would further demonstrate the effectiveness of the work. Additionally, analyzing the feasibility of identifying positive sample pairs on large graphs with acceptable efficiency is worth considering.\n\n5. The paper proposes a way to improve the learning of node embeddings learned with graph contrastive under weak supervision. The paper also provides some experiments that show that their model can achieve good performance. However, the experiments are not sufficient, and the innovativeness is not up to ICLR standards."
                },
                "questions": {
                    "value": "1. Which labeling functions (LFs) were used to derive the weak label matrix, Lambda? As far as I remember, the famous citation network triplet (Cora, Citeseer, and Pubmed) does not come with such weak labels. Furthermore, in the context of Majority Voting, how is a tie among LFs resolved?\n2. The Louvain algorithm [1] identifies exclusive communities, implying each node associates with only one community. This exclusivity is often incongruent with real-world scenarios. For instance, individuals in social networks typically affiliate with multiple groups, such as family, friends, and colleagues. Similarly, in biological contexts, genes or proteins often participate in multiple pathways. Furthermore, the Louvain method can yield poorly connected communities [2]. Could you elucidate further on the algorithm's application, such as the number of communities detected versus class count and how effective it helps with \u201cfinding nodes with a similar graph structure\u201d? Why not opt for other superior methods, such as the Leiden algorithm? \n3. Hard samples, particularly hard negatives, are pivotal for representation learning under the Contrastive framework [3]. The Supervised Contrastive Loss study [4] further emphasizes the significance of hard samples over easy ones. Could you provide further insight into L-supcon, given its centrality in your method?\n4. The proposed loss combines Self-Supervised Contrastive Loss L-s and Supervised Contrastive Loss L-supcon. This combination suggests equal influence from both losses, yet intuitively, L-supcon seems more potent than L-s. Do you think it is necessary to account for their respective contributions to representation learning, perhaps by introducing and searching for a hyperparameter?\n5. In PI-GNN [5], the authors employ noise ratios of 0.0, 0.2, 0.4, 0.6, and 0.8, which appear more intuitive. Despite PI-GNN focusing on image datasets and WSNET on graph datasets, The chosen noise ratios in this study (High 53%, Medium 32%, Low 10%) are notably specific. Could you elucidate the rationale behind these values and explain your approach to introducing noise to the original labels?\n6. How do you account for the enhanced performance on non-homophilous graphs? Might this improvement be ascribed to the community detection algorithm?\n7. In the paper, it is mentioned that weak labels have accuracies set at 47\\%, 68\\%, and 90\\%. I am curious about how these weak labels are generated and how their accuracies are controlled.\n\n**Typos, Formatting Issues, and Grammatical Errors:**\n1. In abstract sentence 3, add a comma after \u201cinstead\u201d, before \u201cparticularly\u201d in sentence 10, and before \u201cand\u201d in sentence 11.\n2. In Section 1, paragraph 3, sentence 7, add a comma before \u201cor\u201d, and \u201ccitations\u201d should be in its singular form to align with \u201cnetworks\u201d.\n3. In section 1, paragraph 5, sentence 5, \u201care\u201d should be \u201cis\u201d to agree with \u201canswering these questions\u201d.\n4. In section 2, paragraph PWS, sentence 3, add a comma before \u201cand\u201d.\n5. In section 2, paragraph PWS, sentence 6, \u201cstraight-forward\u201d should be \u201cstraightforward\u201d.\n6. In section 2, paragraph PWS, sentence 8, \u201cto study\u201d should be \u201con studying\u201d to align with \u201con weak label aggregation\u201d.\n7. In section 2, paragraph NLL, sentence 1, \u201cstraight-forward\u201d should be \u201cstraightforward\u201d.\n8. In section 2, paragraph NLL, sentence 3, \u201cMost\u201d should be \u201cThe most\u201d.\n9. In Section 4, Paragraph 1, in sentence \u201cHere, we sample a node\u2019s positive from the set of nodes that has it\u2019s same aggregated label and negatives from the remaining nodes\u201d, \u201cit\u2019s\u201d seems redundant and erroneous.\n\n**References**\\\n[1] Blondel, Vincent D; Guillaume, Jean-Loup; Lambiotte, Renaud; Lefebvre, Etienne (9 October 2008). Fast unfolding of communities in large networks. Journal of Statistical Mechanics: Theory and Experiment. 2008 (10): P10008.\\\n[2] Traag, V.A., Waltman, L. & van Eck, N.J. From Louvain to Leiden: guaranteeing well-connected communities. Sci Rep 9, 5233 (2019).\\\n[3] Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C., & Krishnan, D. (2020). Supervised Contrastive Learning.\\\n[4] Kalantidis, Y., Sariyildiz, M.B., Pion, N., Weinzaepfel, P., & Larlus, D. (2020). Hard Negative Mixing for Contrastive Learning. \\\n[5] Du, X., Bian, T., Rong, Y., Han, B., Liu, T., Xu, T., Huang, W., Li, Y., & Huang, J. (2021). Noise-robust Graph Learning by Estimating and Leveraging Pairwise Interactions.\\\n[6] \"Grammarly.\" Wikipedia, Wikimedia Foundation, 27 September 2023, en.wikipedia.org/wiki/Grammarly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6809/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6809/Reviewer_oC5W",
                        "ICLR.cc/2024/Conference/Submission6809/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6809/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761410522,
            "cdate": 1698761410522,
            "tmdate": 1700662460497,
            "mdate": 1700662460497,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EVpBmO0GqS",
                "forum": "4UP387Adir",
                "replyto": "VKkddQR3Iv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6809/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6809/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer oC5W"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback and suggestions.\n\n1. *Identifying nodes belonging to the same community from the entire graph is a computationally intensive operation. Including an analysis of efficiency would enhance the completeness of this work.*\n\nResponse: This is true that there is a computational overhead, however, this can be implemented such that it only incurs a one-time computation before the model training. The model training itself is pretty fast. Specifically, we need to first find the communities. Community detection is a one-time computation that happens before the model training. Since we use the Louvain algorithm for detecting communities, there is an overhead of O($nlogn$) for graphs with $n$ nodes. Next, we identify and sample all the positives and negatives we need during the training based on the communities, and the similarities of their weak label distributions. This has a runtime of  $O(np^2)$, where $p$ is the size of the largest community in the graph and $p << n$ in most real world graphs. We also report the runtime used for these pre-training computations as well as the actual training on Cora and Pubmed in Tables 1 and 2 using Louvain in the common response. \n\n2. *The paper lacks diagrams/figures, training details, or efficiency analysis.*\n\nResponse: We will add the training details and run-time analysis to the revised paper. All the experiments were run locally on an 8-cpu core Macbook M2 computer. WSNet was trained for 50 epochs and the results averaged over 5 runs is reported in the paper. For all the other GCL methods, we used the official code released by the authors of the respective papers and tried our best to use the recommended hyperparameters wherever relevant. \n\n5. *The paper proposes a way to improve the learning of node embeddings learned with graph contrastive under weak supervision. The paper also provides some experiments that show that their model can achieve good performance. However, the experiments are not sufficient, and the innovativeness is not up to ICLR standards.*\n\nResponse: We would like to highlight that the main novelty of our work is to introduce weakly supervised graph contrastive learning. There are no other works, to the best of our knowledge, that do GCL with weak labels. We also believe that weakly supervised graph learning is an important field of study for solving graph-based real world problems such as in recommender systems or misinformation detection where only weak labels may be available. Secondly, we systematically study existing GCL methods in the framework of weak supervision which is novel and our results show that there is scope for further research in this direction. \n\nQ1. *Which labeling functions (LFs) were used to derive the weak label matrix, Lambda? As far as I remember, the famous citation network triplet (Cora, Citeseer, and Pubmed) does not come with such weak labels. Furthermore, in the context of Majority Voting, how is a tie among LFs resolved?*\n\nResponse: The LFs were synthetically generated as per Algorithm 3. For a given accuracy/coverage, we flip the labels (given the ground truth) to get weak labels. We repeat this process $m$ separate times to obtain $m$ weak labels for each node. In case of majority voting tie, one of the tied labels is selected uniformly at random."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6809/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238360533,
                "cdate": 1700238360533,
                "tmdate": 1700238360533,
                "mdate": 1700238360533,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VclPNt37bv",
                "forum": "4UP387Adir",
                "replyto": "VKkddQR3Iv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6809/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6809/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer oC5W (contd.)"
                    },
                    "comment": {
                        "value": "Q2. *The Louvain algorithm [1] identifies exclusive communities, implying each node associates with only one community. This exclusivity is often incongruent with real-world scenarios. For instance, individuals in social networks typically affiliate with multiple groups, such as family, friends, and colleagues. Similarly, in biological contexts, genes or proteins often participate in multiple pathways. Furthermore, the Louvain method can yield poorly connected communities [2]. Could you elucidate further on the algorithm's application, such as the number of communities detected versus class count and how effective it helps with \u201cfinding nodes with a similar graph structure\u201d? Why not opt for other superior methods, such as the Leiden algorithm?*\n\nResponse: Thank you for your suggestion. We acknowledge that Leiden is a good alternative to Louvain especially for scalability and we will add this discussion to the revised version of the paper. However, we would also like to highlight that our proposed method is not dependent on the community detection algorithm. Our contribution is more on the idea of sampling positives from detected communities for robustness to weak labels. We also experimented with using Leiden instead of Louvain and reported the results in Tables 1 (Cora) and 2 (Pubmed) in the common response. We observe that there is not much difference in the number of detected communities and mean entropy of class labels within communities for both the methods. We do note that Leiden has a slightly higher NMI and ARI score with the ground-truth labels compared to Louvain which could be the reason why WSNet performs slightly better using Leiden on Cora. The NMI and ARI between the detected community labels of the two methods is also high for Cora but quite low for Pubmed. \n\u201cCould you elucidate further on the algorithm's application, such as the number of communities detected versus class count and how effective it helps with \u201cfinding nodes with a similar graph structure\u201d?\u201d Do you mean to ask if matching the number of communities and number of classes would result in better performance? Clarification on this question will be helpful. \n\nQ3. *Hard samples, particularly hard negatives, are pivotal for representation learning under the Contrastive framework [3]. The Supervised Contrastive Loss study [4] further emphasizes the significance of hard samples over easy ones. Could you provide further insight into L-supcon, given its centrality in your method?*\n\nResponse: Thanks for bringing up this point. We agree that hard negatives are important for contrastive learning and this is an interesting future work for consideration. As our work is a preliminary study of weakly supervised GCL, we simply sample positives and negatives based on the  *aggregated weak labels*. Nodes from different classes are considered negatives and nodes from the same class are positives, similar to the original L_{SupCon} loss.  \n\nQ4. *The proposed loss combines Self-Supervised Contrastive Loss L-s and Supervised Contrastive Loss L-supcon. This combination suggests equal influence from both losses, yet intuitively, L-supcon seems more potent than L-s. Do you think it is necessary to account for their respective contributions to representation learning, perhaps by introducing and searching for a hyperparameter?*\n\nResponse: We actually had experimented with this idea and did not see improvement, in particular in the high noise setting. When the quality of labels is low, weighting $L_{SupCon}$ higher, negatively impacts the overall performance. This is expected as $L_{SupCon} relies entirely on the weak labels for sampling positives and negatives. Here we report the performance of WSNet with (40, 60), (30, 70) and (20, 80).\n| Cora                   | 53%              | 32%              | 10%              |\n| ---------------------- | ---------------- | ---------------- | ---------------- |\n| L_S + L_{SupCon}       | 0.68 $\\\\pm$ 0.02 | 0.80 $\\\\pm$ 0.04 | 0.92 $\\\\pm$ 0.02 |\n| 0.4L_S + 0.6L_{SupCon} | 0.41 $\\\\pm$ 0.04 | 0.79 $\\\\pm$ 0.07 | 0.93 $\\\\pm$ 0.04 |\n| 0.3L_S + 0.7L_{SupCon} | 0.43 $\\\\pm$ 0.03 | 0.73 $\\\\pm$ 0.05 | 0.92 $\\\\pm$ 0.04 |\n| 0.2L_S + 0.8L_{SupCon} | 0.52 $\\\\pm$ 0.03 | 0.76 $\\\\pm$ 0.06 | 0.92 $\\\\pm$ 0.04 |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6809/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238620231,
                "cdate": 1700238620231,
                "tmdate": 1700238620231,
                "mdate": 1700238620231,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2264wAx6jk",
                "forum": "4UP387Adir",
                "replyto": "VKkddQR3Iv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6809/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6809/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer oC5W (contd.)"
                    },
                    "comment": {
                        "value": "Q5. *In PI-GNN [5], the authors employ noise ratios of 0.0, 0.2, 0.4, 0.6, and 0.8, which appear more intuitive. Despite PI-GNN focusing on image datasets and WSNET on graph datasets, The chosen noise ratios in this study (High 53%, Medium 32%, Low 10%) are notably specific. Could you elucidate the rationale behind these values and explain your approach to introducing noise to the original labels?*\n\nResponse: Since we are interested in the setting where each node is associated with multiple weak labels we varied both the number of LFs ($m$) and individual LF accuracies ($p_a$) pairs as ($m$, $p_a$) =  [(5, 0.45), (10, 0.65), (50, 0.55)]. These weak labels are aggregated using majority vote to obtain, 53%, 32% and 10% accuracies. In other words, we only control the $m$ and $p_a$ parameters and not the final accuracy/noise of the aggregated labels. We will highlight this better in the revised version of the paper. \n\nQ6. *How do you account for the enhanced performance on non-homophilous graphs? Might this improvement be ascribed to the community detection algorithm?*\n\nResponse: Yes, we believe the improved performance on non-homophilous graphs may be attributed to positive sampling from communities. We will highlight this in the revised version. \n\nQ7: *In the paper, it is mentioned that weak labels have accuracies set at 47%, 68%, and 90%. I am curious about how these weak labels are generated and how their accuracies are controlled.*\n\nResponse: Individual $m$ weak labels are generated using Algorithm 3 and are then aggregated using majority vote. We have clarified this in the revised version. \n\nThe grammatical errors and typos have been noted with thanks and corrected in the revised version."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6809/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238716028,
                "cdate": 1700238716028,
                "tmdate": 1700238716028,
                "mdate": 1700238716028,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1yg2qKtQuQ",
                "forum": "4UP387Adir",
                "replyto": "2264wAx6jk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6809/Reviewer_oC5W"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6809/Reviewer_oC5W"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed reply. The authors have resolved most of the concerns and I will increase the score to 5."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6809/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662711448,
                "cdate": 1700662711448,
                "tmdate": 1700662711448,
                "mdate": 1700662711448,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ducmahaiRA",
            "forum": "4UP387Adir",
            "replyto": "4UP387Adir",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6809/Reviewer_G4h6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6809/Reviewer_G4h6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel graph contrastive learning method, named WSNET, to learn node representations when weak or noisy labels are present. The authors conducted experiments to compare the robustness of current GCL node representation methods under weak supervision and found that incorporating weak label information into contrastive learning using WSNET can improve node representation learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors propose a novel approach to improve the node representation learning ability of GCL when there is noise in the node labels."
                },
                "weaknesses": {
                    "value": "1. The authors spend too much space on Research Question 1, which evaluates the robustness of existing GCL methods. This is only a process of running baselines and cannot be the main innovation and contribution of the paper. The authors should focus more on the proposed method.\n2. Even with the amount of space devoted to RQ1, I don't think the authors' analysis is deep enough. For example, the authors conclude from the experimental results that baselines that use neighborhood-based sampling of positive pairs perform better than others in the high noise setting. What is the reason for this, and the authors should provide some analysis.\n3. The notations are inconsistent, which makes it difficult for readers to understand the algorithm. For example, in the first line of Algorithm 1, $\\lambda_i$ looks like a scalar, but Section 3 says that $\\lambda$ is a label function. Additionally, are $\\Lambda_i$ and $\\Lambda[V_i]$ representing the same vector?\n4. Will the algorithm's results differ significantly depending on the community search algorithm used and the label aggregation method used? The authors should provide more experimental results to compare the possibilities.\n5. Will the size of the negative samples $r$ have a significant impact on the results? The authors should provide a sensitivity analysis of the parameters.\n6. I believe that the authors should introduce larger-scale datasets, such as OGB data, as the current experimental datasets cannot verify the effectiveness of the proposed method in real-world situations."
                },
                "questions": {
                    "value": "1. It is unclear why only the node with the highest similarity is sampled as a positive sample for the $L_S$ loss. It would be interesting to investigate whether using a top-k sampling strategy would be feasible and potentially improve the results.\n2. It is unclear why WSNET performs better in the High Noise setting than in the Medium Noise setting on the Texas dataset."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6809/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6809/Reviewer_G4h6",
                        "ICLR.cc/2024/Conference/Submission6809/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6809/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698928057043,
            "cdate": 1698928057043,
            "tmdate": 1700673555090,
            "mdate": 1700673555090,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lq5sdFZNNU",
                "forum": "4UP387Adir",
                "replyto": "ducmahaiRA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6809/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6809/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer G4h6"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments. \n\n1.  *The authors spend too much space on Research Question 1, which evaluates the robustness of existing GCL methods. This is only a process of running baselines and cannot be the main innovation and contribution of the paper. The authors should focus more on the proposed method.*\n\nResponse: We would like to clarify that although RQ1 is not the main innovation point in the paper, there is still value in studying how different GCL methods perform in the presence of label noise as this comparative study has not been done before. We clarify that these experiments are not only baselines but also constitute the first study on the robustness of GCL methods to weak labels. Moreover, we observe that no single method consistently outperforms others showing that there is scope for further research in this direction. We will discuss this better in the revised version of the paper. \n\n2. *Even with the amount of space devoted to RQ1, I don't think the authors' analysis is deep enough. For example, the authors conclude from the experimental results that baselines that use neighborhood-based sampling of positive pairs perform better than others in the high noise setting. What is the reason for this, and the authors should provide some analysis.*\n\nResponse: GNNs typically implement neighborhood aggregation for learning efficient and robust node representations and this supports our observations that neighborhood-based sampling makes the embeddings more robust to weak labels compared to node augmentations. On contrasting views of the same nodes versus neighboring nodes, the learned embeddings contain more information about the neighbors in the latter than the former. We will expand on this discussion in the revised version.\n\n3. *The notations are inconsistent, which makes it difficult for readers to understand the algorithm. For example, in the first line of Algorithm 1, $\\lambda_i$ looks like a scalar, but Section 3 says that \\lambda is a label function. Additionally, are $\\Lambda_i$ and $\\Lambda[V_i]$ representing the same vector?*\n\nResponse: Thank you for pointing these out. We will clarify these notations in the revised version of the paper. Mainly that $\\lambda_i$ is not a scalar but the output vector obtained by applying labeling functions $\\lambda$ on $i$th node and $\\Lambda_i$ and $\\Lambda[V_i]$ do represent the same vector.\n\n4. *Will the algorithm's results differ significantly depending on the community search algorithm used and the label aggregation method used? The authors should provide more experimental results to compare the possibilities.*\n\nResponse: Both the community search algorithm and the label aggregation methods are modules that can be replaced in our proposed WSNet framework. Our method is not tied to these modules. For our experiments, we chose the simplest and most popular method in both cases. \n\n  a) More specifically, we used the simple majority vote label aggregator to demonstrate the effectiveness of our proposed WSNet. On using better label aggregation methods the accuracy of the aggregated labels would increase resulting in the improvement of the overall performance of WSNet. In our work we show that even using a simple majority vote label aggregation, WSNet outperforms other baselines. We repeated our experiment using the Snorkel[1] label aggregator and reported the results in Table below. We observe that the Snorkel accuracy of the aggregated labels is lower than MV for all noise levels and as a result WSNet trained using the corresponding labels has a lower performance than MV. \n| Cora                     | 53%              | 32%              | 10%              |\n| ------------------------ | ---------------- | ---------------- | ---------------- |\n| MV accuracy              | 0.47             | 0.68             | 0.90             |\n| WSNET with Majority Vote | 0.68 $\\\\pm$ 0.02 | 0.80 $\\\\pm$ 0.04 | 0.92 $\\\\pm$ 0.02 |\n| Snorkel accuracy         | 0.43             | 0.69             | 0.84             |\n| WSNET with Snorkel       | 0.39 $\\\\pm$ 0.04 | 0.73 $\\\\pm$ 0.07 | 0.86 $\\\\pm$ 0.09 |\n\n b) Similarly, for community detection, we address the comments in the common response."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6809/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237454121,
                "cdate": 1700237454121,
                "tmdate": 1700237747352,
                "mdate": 1700237747352,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eH55mbi38L",
                "forum": "4UP387Adir",
                "replyto": "ducmahaiRA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6809/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6809/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer G4h6 (contd.)"
                    },
                    "comment": {
                        "value": "5. *Will the size of the negative samples r have a significant impact on the results? The authors should provide a sensitivity analysis of the parameters.*\n\nResponse: Generally, the higher the value of $r$, the better the performance. However, with higher values of $r$, the computational complexity also increases as the denominator term in the $L_S$ (Eq 3 in the paper). We ran experiments on the Cora dataset for varying values of $r$ and reported the results below. We will add these results and discussion to the appendix of the revised paper due to space constraints. We observe a similar performance for $r=10$ and $r=20$ with slight increase in runtime. Given that the performance is similar, we used $r=10$ as in our experiments in the paper. \n| Cora     | 53%              | 32%              | 10%              | Runtime |\n| -------- | ---------------- | ---------------- | ---------------- | ------- |\n| $r = 5$  | 0.40 $\\\\pm$ 0.04 | 0.74 $\\\\pm$ 0.07 | 0.93 $\\\\pm$ 0.05 | 25.4    |\n| $r = 10$ | 0.68 $\\\\pm$ 0.02 | 0.80 $\\\\pm$ 0.04 | 0.92 $\\\\pm$ 0.02 | 28.3    |\n| $r = 20$ | 0.68 $\\\\pm$ 0.04 | 0.80 $\\\\pm$ 0.06 | 0.93 $\\\\pm$ 0.07 | 28.6    |\n\nQ1. *It is unclear why only the node with the highest similarity is sampled as a positive sample for the L_S loss. It would be interesting to investigate whether using a top-k sampling strategy would be feasible and potentially improve the results.*\n\nResponse: We used only one positive sample keeping with the norm in contrastive learning. That is the standard for InfoNCE loss because it is considered to be similar to cross-entropy loss for classifying the correct positive sample from the remaining negative ones. Additionally, since the labels are noisy, by considering the top-k samples with similar weak label distribution, there is a higher chance of the noisy labels affecting the embeddings. Moreover, this can also become an issue for communities with only 1 node. However, we think this is an interesting setup to investigate. We ran an experiment to see if top-5 improves the results. As expected, for the higher noise settings, $k=1$ does better.\n| Cora  | 53%              | 32%              | 10%              | Runtime |\n| ----- | ---------------- | ---------------- | ---------------- | ------- |\n| $k=1$ | 0.68 $\\\\pm$ 0.02 | 0.80 $\\\\pm$ 0.04 | 0.92 $\\\\pm$ 0.02 | 28.3    |\n| $k=5$ | 0.53 $\\\\pm$ 0.05 | 0.75 $\\\\pm$ 0.07 | 0.94 $\\\\pm$ 0.03 | 29.5    |\n\nQ2. *It is unclear why WSNET performs better in the High Noise setting than in the Medium Noise setting on the Texas dataset.*\n\nResponse: Considering that Texas is the smallest dataset we have with only 183 nodes, we observe a high variance in the results and although on average it seems the performance on high noise setting is better compared to medium noise setting, considering the variance, there is overlap in the ranges. This is also only observed with linear regression and not the random forest classifier, which might highlight this is partly due to simplicity of the downstream logistic regression classifier, more than the quality of the embeddings from WSNet. WSNet + RF shows the expected increase in performance with decrease in noise. We will expand the discussion in the paper on this.   \n\n[1] Ratner A, Bach SH, Ehrenberg H, Fries J, Wu S, R\u00e9 C. Snorkel: Rapid Training Data Creation with Weak Supervision. Proceedings VLDB Endowment. 2017 Nov;11(3):269-282. doi: 10.14778/3157794.3157797. PMID: 29770249; PMCID: PMC5951191."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6809/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237601083,
                "cdate": 1700237601083,
                "tmdate": 1700237601083,
                "mdate": 1700237601083,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mwP65Ea4za",
                "forum": "4UP387Adir",
                "replyto": "eH55mbi38L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6809/Reviewer_G4h6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6809/Reviewer_G4h6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I have no more questions, and I will raise my score to 6."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6809/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673531373,
                "cdate": 1700673531373,
                "tmdate": 1700673531373,
                "mdate": 1700673531373,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]