[
    {
        "title": "SPADE: Sparsity-Guided Debugging for Deep Neural Networks"
    },
    {
        "review": {
            "id": "JqvfUiWuIn",
            "forum": "ffcHGwb4KF",
            "replyto": "ffcHGwb4KF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8073/Reviewer_fCAG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8073/Reviewer_fCAG"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces SPADE, a per-sample sparsification method towards feature disentanglement. To this end, given a selected input sample, SPADE performs augmentation to create a \"batch of samples\"; then, for each layer of a considered network, a custom sparsity solver aiming to find a sparse set of weights to best approximate the output is considered. This results in a sparse model, specialized for the considered sample, upon which saliency maps and other neuron visualization techniques can be applied. Experimental evaluations on a variety of models (ResNet, MobileNet, ConvNext) and datasets (ImageNet-1k, CelebA, Food-101) are shown to yield accuracy improvements in various Saliency Maps methods, while human studies further support the quantitative findings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors propose a method for disentangling the features in each layer of a pretrained network on a per sample basis. This is a very intuitive approach, bypassing the typically considered limiting assumption of a per-class sparsity. The paper is overall well written and easy to follow. \n\nThe experimental evaluation considers a variety of datasets and architectures (albeit only convolutional ones) and the model exhibits improvements compared to a baseline and the alternative method of Wong et al. 2021."
                },
                "weaknesses": {
                    "value": "The approach falls under the umbrella of sparsity-aware methods towards interpretability. To this end, the authors consider a post-hoc per-example sparsity scheme to disentangle representations in the context of multifaceted neurons. To do so, they consider a custom sparsity solver, namely OBC to solve the constrained optimization problem. \n\nThroughout the paper, the authors highlight the efficiency of the proposed mechanism compared to alternative methods (specifically [1]). At one point, the authors note:\n\n\"using our approach, it takes 41 minutes to preprocess the ResNet50 network for a single example, on a single RTX 2080 GPU (Table F.15). By comparison, it takes 40 hours to preprocess the network with the FC pruning method of Wong et al. (2021).(However, we note that SPADE must be run once per sample or group of samples, and the FC pruning method is run once for all examples. Irrespective of runtime, experiments in the next section show that our approach is significantly more accurate in practice.)\". \n\nIn this context, and considering for example the ImageNet validation set, the proposed SPADE algorithm would require a **Massive** number of weeks to train for all the examples. At the same time, it would require the user to re-run the inference algorithm for each example in the set or save a different set of weights for all the different examples. This is evidently impossible and greatly limits the applicability of the approach for any real-world setting. \n\nMoreover, the usage of post-hoc custom sparsity solvers (and as the authors note) require ad-hoc thresholds. The authors try to mitigate this issue by using \"100 calibration samples\" to maximize the average input pixel AUC score for the saliency method of interest in cases where the ground truth is known\". There are three major issues with this approach: (i) the authors consider additional information of how to tune the sparsity ratio, rendering the comparison with [1] unfair, since the aim of the latter is a balance between sparsity and accuracy and not optimization in the context of the considered saliency method (ii) to compute these sparsity ratios, they use all the 100 examples, thus utilizing augmented \"global\" information, somewhat undermining the per-example sparsity argument, and (iii) this adds further complexity to the already computationally intensive formulation of the method. \n\nIt is important to note that, in my understanding, the work of [1] aims to create **highly** sparse networks, while retaining the accuracy of the model, allowing for inspecting individual neurons. At the same time it does this only for the last linear layer. Thus, comparing SPADE to [1] is not exactly appropriate. \n\nFinally, the per-example sparsity rationale is not novel in the community. Indeed, the work of [2] recently proposed a data-driven mechanism for per-example sparsity based on a Bernoulli formulation. This was applied in the context of Concept Bottleneck Models (CBMs) allowing the model to select which \"features\" are considered in the last linear layer, also bypassing the limitation of [1], while being highly efficient and generalizable. This requires training a single linear layer to do the selection and can possibly mitigate many of the issues of the considered approach. \n\n\n[1]  Wong et al., Leveraging sparse linear layers for debuggable deep networks, In Proc. ICML 2021\n[2] Panousis et al., Sparse Linear Concept Discovery Models, In Proc. ICCVW 2023"
                },
                "questions": {
                    "value": "Apart from the concerns raised in the previous section, some specific questions are:\n1) What are the sparsity ratios for each different method and layer arising through the introduced calibration method?\n2) How did the authors decide the samples in the held-out calibration samples?\n3) Is there a way to efficiently store the sparse structure for each example?\n4) What are the differences in the sparse structure between semantically similar images?\n5) What are the limitations for the considered operations? The method seems to not work for depthwise convolutions, as the authors exclude them from the considered architectures (Appendix B). Is application of the SPADE rationale possible in the ViT setting? If so, only in the MLP?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8073/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8073/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8073/Reviewer_fCAG"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8073/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698004241752,
            "cdate": 1698004241752,
            "tmdate": 1699636999348,
            "mdate": 1699636999348,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oaYZiCfxYk",
                "forum": "ffcHGwb4KF",
                "replyto": "JqvfUiWuIn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8073/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8073/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to your review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for noting the experimental strengths of our method, as well as the intuitive design and presentation.\nWe address the weaknesses raised by the reviewer below.\n\n* **Slow computation time/massive resource requirements to apply at scale.** We thank the reviewer for raising this concern, which was also raised by others. To address this, we note that the bottleneck is the sparsification cost. Therefore, we ran additional experiments with a more efficient solver, to provide speed-accuracy tradeoffs. The results are described in the general response above, but in a nutshell, *using the fastest pruning strategy drops the per-example requirements to 70 seconds on ResNet 50, at a small fidelity drop that still improves substantially over the baselines.*\n\n* **Feasibility and fairness of calibration.**  We agree that the calibration can be infeasible. In that case, we propose a fixed sparsity schedule independent of interpretability method, that increases linearly with layer depth, and demonstrate that such a schedule is also quite effective (Appendix D/Table D.14). We also believe that our calibration does not unfairly leak information from the test data, as we use different data samples (also drawn from the ImageNet validation set) and different Trojan patches (emoji) for calibration.\n\n* **Comparison with Wong et al.** We agree that our approach differs from Wong et al, in that we use per-example sparsity, whereas they train a single sparse FC layer for use in all examples, and for all interpretation methods. However, our approach is quite original (something that we see as a strength), and so Wong et al. was the closest approach available in the literature, thus, the best competitive baseline. We updated Section 4.1 to highlight the issues that you have raised.\n\n* **Novelty.** We feel that our method is novel in the sense that it uses *sample-specific, dynamic* sparsity to improve model interpretations -  so much so that we had trouble finding baselines to compare against, except for Wong et al, which as you note is an imperfect comparison. Regarding Panousis et al., we first note that ICCV was in October of this year, while the deadline for ICLR was in September; thus this is a concurrent work. Nevertheless, note that the method described in this paper creates sparse interpretable networks with a specific architecture (concept bottleneck), which is substantially different from what our method does - namely, provide a way to create better interpretations for *dense* models with a standard architecture.\n\n### Questions:\n\n1. We show the sparsity ratios in the left panel of Figure 4 of the paper; generally, later layers have very high sparsity, while earlier layers have less sparsity.\n\n1. The samples in the calibration sets were randomly sampled from the ImageNet validation set; we ensured that there was no intersection between these samples and the samples that we used for the evaluation.\n\n1. SPADE traces could perhaps be stored as sparse matrices. But we do not foresee a use case where large numbers of SPADE traces would need to be stored, as these are just intermediate computations used to compute an interpretation. They could also be deterministically recreated by randomly generating and then storing the exact image transforms used to prune the network (most efficiently by fixing and storing the random seed).\n\n1. Semantically similar images have a higher overlap in their sparse traces than do unrelated images.  We added a figure to demonstrate this in Appendix J.\n\n1. Our method can be applied to virtually any architecture, as e.g. convolutional layers can be unfolded to linear operators, on which SPADE is applied. We have illustrated this by showing results on ConvNEXT models, which are a hybrid between CNNs and Transformer-based architectures. We will try to add further results on this point in the next revision."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241146389,
                "cdate": 1700241146389,
                "tmdate": 1700241146389,
                "mdate": 1700241146389,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "b6k49ZhN4k",
            "forum": "ffcHGwb4KF",
            "replyto": "ffcHGwb4KF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8073/Reviewer_4QUk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8073/Reviewer_4QUk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes SPADE for preprocessing a given DNN model with respect to the prediction of a given input image. SPADE attempts to prune the weights in the model under the constraint that the prediction as well as the intermediate activation patterns do not change as much as possible for the given input. Experimental results suggest that applying existing XAI methods, including input saliency mapping or neuron visualization, to the model preprocessed by SPADE leads to a better understanding of the prediction."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea of pruning the model for a specific input for better explanation is clear.\n- Extensive experimental results demonstrate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "- The computational effort required for preprocessing by SPADE is relatively high, making it difficult to use in practical situations.\n- I could not fully understand the details of the human study."
                },
                "questions": {
                    "value": "- What is the definition of $W_\\text{sparse}$ in Eq.(1)?\n\n- Is the re-calibration of batchnorm mentioned in page 4 also applied to the model after SPADE preprocessing?\n\n- Figure 2 (left) implies that image rotation is used as data augmentation, which is not the case according to Section 3.2.\n\n- In page 9, I could not understand the meaning of \"the image patches were always generated from the dense model\". What are \"image patches\" in this context?\n\n- In page 9: \"there were were\" -> \"there were\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8073/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698312502990,
            "cdate": 1698312502990,
            "tmdate": 1699636999233,
            "mdate": 1699636999233,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IuvXgFnKAf",
                "forum": "ffcHGwb4KF",
                "replyto": "b6k49ZhN4k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8073/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8073/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to your review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive review, and for noting the clarity of our idea and the demonstrated effectiveness of our method.\n\nAs all reviewers noted that the computational requirements of SPADE can limit its practical use, we amended the paper to propose other variants of SPADE that provide different speed/accuracy tradeoffs. In particular, for a small accuracy drop, SPADE preprocessing can be done in 70 seconds per example.\n\nWe respond to the reviewer\u2019s questions below.\n\n1. W_sparse is meant to indicate that the argmin is taken over all weight matrices W that are sparse according to a preset sparsity level. We agree that this was not well explained and have adjusted the formula.\n\n1. The re-calibration of the batchnorm was not applied to the model after SPADE preprocessing, as the model trace produced by SPADE is not useful for inference.  The similarity result for which the batchnorm is applied was only presented as a sanity check that the top-1 prediction of the trace is still close to the network prediction.\n\n1. Thank you for  observing that we do not use rotations; we corrected this in the figure.\n\n1. The image patches are contiguous regions of the image that, according to a low-resolution saliency map method (applied without SPADE) are the most important evidence for a specific possible predicted class. In our human evaluation, we used misclassified examples and produced image patches for the correct and predicted class, filtering only for those input images for which these do not intersect. The idea is that if the class visualization is human interpretable, the evaluator would be able to use the visual similarities between the class neuron visualization and the image patch to find the part of the image that resembles what the neuron is \u2018looking for\u2019.\n\n1. Thank you for the correction, we will fix the typo."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240657908,
                "cdate": 1700240657908,
                "tmdate": 1700240657908,
                "mdate": 1700240657908,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kioftyeepE",
                "forum": "ffcHGwb4KF",
                "replyto": "IuvXgFnKAf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8073/Reviewer_4QUk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8073/Reviewer_4QUk"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your responses"
                    },
                    "comment": {
                        "value": "Thank you for your responses to my questions, which addressed all my concerns.\nI would like to keep my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700474615803,
                "cdate": 1700474615803,
                "tmdate": 1700474615803,
                "mdate": 1700474615803,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ClxLWhUqoi",
            "forum": "ffcHGwb4KF",
            "replyto": "ffcHGwb4KF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8073/Reviewer_5ZSW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8073/Reviewer_5ZSW"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces SPADE, which recommends conducting sample-wise targeted pruning to obtain a sparse version of the original network, before interpreting the network's predictions (w.r.t. the specific sample) using any interpretation method. Given an image to interpret, SPADE first applies various augmentation techniques to generate a batch of different views of the given image, then uses the OBC sparsity solver to find a sparse set of weights which matches the layer-wise activations of the original model. With this sparsified model, different path-based and perturbation based interpretation techniques are applied to generate input saliency maps and neuron activation maps. The authors perform experiments on 3 convolutional architectures, 3 vision datasets to demonstrate improved saliency map accuracies (as measured by AUC and Pointing Game scores) and enhanced usefulness of neuron activations (as measured by human task success rate)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. **Reproducibility** \u2014 The authors describe experimentation settings (including human experiments, datasets, metrics, sparsity ratios, etc) in detail, open-source their code and provide model weights (with Trojan backdoors) for reproducibility.   \n2. **Organisation and writing** \u2014 This paper is very well-written and meticulously organised. The Appendix includes a table of contents and is highly readable.     \n3. **Evaluation** \u2014 SPADE evaluates on a variety of path-based and perturbation-based saliency attribution methods; on 3 convolutional architectures; on AUC and Pointing Game metrics; on human task performance. Evaluation is relatively thorough, though I have critical concerns about how evaluations are conducted on a very small fraction of the validation set of ImageNet-1K, CelebA and Food-101 (see W1)."
                },
                "weaknesses": {
                    "value": "1. **Small-scale ImageNet experiments?** \u2014 Please clarify if I misunderstood but it appears that the main result (i.e., saliency map \"accuracy\" of SPADE vs. Dense vs. Sparse FC on ResNet50/ImageNet) is only calculated for **140 test samples out of the available 50,000** in the ImageNet-1K validation set. It seems bold to claim AUC improvements and Pointing Game score gains when evaluation is done on 0.0028 of the actual validation set, especially when it is unclear how/why these 140 chosen samples are to be representative of the wider dataset.  \n   \n2. **Non-negligible cost for constrained optimisation of sparse network** \u2014 As described in Section 3, SPADE relies on activation matching for every layer and takes 41 minutes for single example preprocessing. This method becomes prohibitively costly with increasing cost proportional to dataset size and the number of network layers, rendering it impractical for large-scale deep neural network debugging.   \n  \n3. **Limited novelty** \u2014 Using sparsity to factorise / disentangle concepts is a known direction in literature. SPADE adopts this perspective, then leverages an existing OBC sparsity solver to obtain sparse subnetworks w.r.t. every example (and its augmentations), then applies off-the-shelf interpretability techniques to generate various interpretation maps. SPADE does not seem to present novel insights, methods or findings.  \n  \n4. **Fairness and fidelity of input saliency maps (not neuron visualisations)** \u2014 SPADE requires sample-wise sparsification before interpreting each different image sample, meaning that the saliency map is generated with respect to a different subnetwork for each image. Sparse networks A and B could exhibit drastically different performance (accuracy and interpretability) on examples A and B, it therefore seems unfair to interpret different examples using different subnetworks. SPADE saliency map visualisations can only be matched / replicated by using the exact same sparse subnetworks for every example and is hence costly to reproduce. It is furthermore unclear to me why SPADE saliency maps would be representative of those of the original dense model."
                },
                "questions": {
                    "value": "1. The motivation of SPADE is to reduce the multifacetedness of neurons through pruning but it is unclear to me how SPADE is able to accomplish this. Could the authors elaborate on the intuition of why/how does the constrained optimisation in Equation 1 disentangle multifaceted neurons? Multifaceted neurons encode richer and more concepts; these concepts typically generalise not only across samples but also across augmentations, whereas concept-specific neurons might activate only for 1 or few specific augmented view(s). For maximal sparsification while matching layer-wise activations, wouldn't the optimiser retain multifaceted neurons and discard highly specific neurons?  \n  \n2. Do interpretation maps differ significantly for dense and sparse networks? Are saliency maps obtained from different sparse subnetworks representative of the interpretability of the original dense network?   \n  \n3. Does the sparsity solver find highly similar or dissimilar sparse subnetworks for different image examples? In other words, are the same set of neurons consistently retained for different images?    \n  \n4. Are there any specific patterns or properties of retained neurons in the sparse subnetworks? Do they have larger activation magnitudes, or perhaps do they belong to earlier / later or wider / narrower layers?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8073/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698495996192,
            "cdate": 1698495996192,
            "tmdate": 1699636999061,
            "mdate": 1699636999061,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "00RkmynUeJ",
                "forum": "ffcHGwb4KF",
                "replyto": "ClxLWhUqoi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8073/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8073/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to your review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful review, and for noting the quality of our presentation and evaluation. With regard to the weaknesses described by the reviewer, we address them below. \n\n* **Small-scale ImageNet experiments.** The 140 test samples were chosen at random from the ImageNet validation dataset, subject to the following conditions: that the number of samples for each Trojan patch (emoji) was the same, and that only samples where the predicted label was changed by the Trojan were selected. The small number is due to the fact that each sample required custom pruning with SPADE, which takes ~40 minutes for each saliency method. We do believe that while the relatively small sample size makes the evaluation more noisy, it does present a useful comparison of the methods, especially as the results are quite consistent across datasets and saliency methods.\n\n   However, due to your and others\u2019 feedback, we will add to our paper experiments on a modified version of SPADE, that uses a more efficient pruner to create the saliency maps. Using this version of the method, **we were able to run SPADE + LRP on 21121 samples from the ImageNet validation set (all that met our other criteria) in 120 GPU- hours on GeForce RTX 3090 GPUs.**\n\n* **Non-negligible cost for constrained optimisation of sparse network.** In the original work, we conceived of SPADE as a method for human debugging, for which perhaps a few dozen examples would be examined, and so we believed the per-example cost to be acceptable. However, as described in the previous answer, SPADE can also be used with more efficient sparsity solvers at a small accuracy cost (but still an improvement over the baselines), **reducing the cost to 70 seconds per example for a ResNet50**. We provide these tradeoffs below, and also in Appendix I of the revision.\n\n* **Limited novelty.** We respectfully disagree with this assertion. Other works (with the exception of Wong et al (2021), which we address separately), construct sparse networks and demonstrate that they are somewhat more interpretable than dense networks, at a cost of some degradation in the performance of the network on the desired task, e.g. image classification. In particular, these networks are intended for inference, and replace the dense network entirely (in cases where the sparse network is obtained from a dense one, rather than trained from scratch). Conversely, the sparsification in SPADE is used as an additional step during the computation of an example interpretation, and does not produce a replacement network for inference or any other purpose. The sparsification in SPADE removes network connections that are least relevant to the particular example being studied. This preprocessing reduces noise in interpretability methods and increases their accuracy and usefulness on that example, but is not intended to produce networks useful for classification; nor do we propose using SPADE during regular inference. Note that this means that there is no inference performance tradeoff (in time or quality)  to using SPADE, as SPADE is used solely for creating interpretations, and regular inference is not affected.\n\n   The only other work to our knowledge that uses sparsity in this way -  solely for the creation of better interpretations - is the work of  Wong et al, which replaces the final layer with a sparse one. However, unlike that work, our method introduces the creation of example-specific traces through the network. This, to our knowledge, is completely novel, and we believe to be the core driver of the improved saliency map accuracy results.\n\n\n* **Fairness and fidelity of input saliency maps.** We respectfully disagree with large parts of this point as well. We consider the preprocessing with SPADE to be a denoising step that is part of the saliency map creation, and so would argue that the saliency maps are still created with respect to the original, dense network. As network fidelity is always a concern with saliency maps, we validate this in the human study by using SPADE in the process of creating neuron visualizations and verifying that these are still relevant to the original network. We also performed an additional experiment (please see general comment above), which demonstrates that preprocessing with SPADE allows saliency methods to identify pixels that have a greater effect on the confidence of the dense network, providing additional evidence of our method\u2019s applicability to the original, dense network.\n\n  We do, however, note the reviewer\u2019s excellent observation that combining interpretability methods with SPADE can lead to some randomness in the resulting saliency map, due to a small amount of randomness in the pruning algorithm. This can be overcome if necessary by randomly generating and then fixing the transforms used to create sample batches used for pruning."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240346281,
                "cdate": 1700240346281,
                "tmdate": 1700240346281,
                "mdate": 1700240346281,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sjEnsJrSRP",
            "forum": "ffcHGwb4KF",
            "replyto": "ffcHGwb4KF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8073/Reviewer_mZPn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8073/Reviewer_mZPn"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose SPADE: a new post-hoc local explanation methodology for deep neural networks.  Specifically, the authors propose training a new sparse DNN to approximate the predictions of the original DNN under examination.  Importantly, they argue in favor of training a new sparse DNN for each individual example, and later show the value of this approach for detecting Trojans.  The authors propose that one can then run typical post-hoc explanation methods (like saliency maps or neuron visualizations) on the sparse approximation network.\n\nThe authors present several empirical comparisons that use proxy measures and human subject evaluations of their method to common approaches (such as explaining the original dense network, or other post-hoc explanations based on learning a sparse explanation model)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors' proposed method, SPADE, is straightforward to understand and makes use of recent innovations in related bodies of work (e.g., sparsity solvers) in a clever way.  The authors introduce SPADE as a general and customizable approach, and clearly outline different design decisions that one could make to change SPADE in practice (like use of different solvers, objectives, or explainers once the sparse network has been learned).  They also thoroughly ablate their approach which I appreciated.\n2. The authors conduct a thoughtful evaluation of their proposed method by using Trojans to design a scenario where they have access to ground-truth information about the model's behavior.  By doing so, they avoid common pitfalls in related work that evaluates a new proposed explanation method.\n3. I appreciate the authors' commitment to reproducibility.  All methods and experiments are clearly described, and substantial additional information about each experiment is provided in the Appendices."
                },
                "weaknesses": {
                    "value": "I am happy to consider adjusting my score if my concerns are addressed.\n\n* **Weakness #1: Transparency about limitations of the proposed approach**.  I believe that the present draft would be made much stronger if it dedicated more time to thoughtfully discussing limitations and implications of the author's proposed approach.  I list what I believe are significant limitations below.  I do not think that these limitations weaken the proposed method (all explanation approaches have their own limitations!), but being clear about them will help readers and potential users of this work.\n  * _Selecting an appropriate sparsity ratio for each layer_.  (Section 3.2) I am hesitant of how you chose to use cross-validation given ground-truth information to select the optimal \"sparsity ratio\". I think this may lead to an overly optimistic representation of SPADE's performance because in practice, a user of SPADE would not have such ground truth information available. Can you acknowledge this as a limitation and also examine SPADE's sensitivity to different sparsity ratio values (e.g., if I use the wrong ratio, is it now useless) in the draft?\n  * _Computational cost_.  Given that SPADE is so computationally expensive, how would you recommend that users choose individual samples to explain? \u2013 is there a more principled strategy than explaining all of the misclassified or \"surprising\" examples?\n  * _Improvement over baselines is not that strong_. I am surprised that the baseline saliency and visualization methods actually establish a pretty strong baseline (i.e., the difference between the SPADE vs. baseline settings is actually not that large in Table 1 and Figure 4).  Can you further elaborate on this in your paper text \u2013 is it because existing explanations are actually quite well-suited for the Trojan task, but maybe less appropriate in settings where the \"bug\" is something more subtle (like presence of a spurious object)?\n* **Weakness #2: Clarity about experimental design**.  Overall, I found the paper to be well-written.  However, I believe there are some experimental design details that should be surfaced more clearly in the main text.\n  * _Clarify motivation for the Trojan experiment set-up_.  My understanding from reading [1] is that in real-life scenarios where we may wish to find unknown \"backdoors\" or Trojans in a model, we don't have access to individual examples where the Trojan is present \u2013 i.e., if we had a datapoint with the Trojan in it, then we would know that something is up.  (Casper et al. says, \"Finding trojans using interpretability tools mirrors the practical challenge of finding flaws that evade detection with a test set because Trojans cannot be discovered with a dataset-based method unless the dataset already contains the trigger features. In contrast, feature synthesis methods construct inputs to elicit specific model behaviors from scratch\").  This entire problem set-up is seemingly incompatible with the data-dependent workflow you're proposing, where I would need to \"explain\" the network on an individual data-point that has the Trojan, in order to see an explanation that reveals that the model is relying on it. Can you please clarify why you chose to use this Trojan set-up to motivate and evaluate your data-dependent explanation method?\n  * _Questions about saliency map experiments (Section 4.1)_. In this set-up, do we only calculate the \"accuracy\" of saliency maps for the images with the Trojans in them, as these are the only images where we have ground-truth information (i.e., are the \"140 examples\" in Table 1 all images with Trojans)? How exactly do you calculate each saliency map? \u2013 are you taking the gradient of the neuron that is the predicted class, or true (Trojan) class of the image?\n  * _Questions about the user study (Section 4.2.2)_. What is an \"image patch\"? How is it computed? How is it an accurate representation of the \"ground truth\" reasoning of the dense model? Can you provide more detail inline about what the \"correct answer\" is for the question we asked users (\"which of the two regions activates the neuron\")? Here are the \"regions\" the image patches for each class, and the ground truth is which class the neuron responds to?\n\n[1] https://arxiv.org/pdf/2302.10894.pdf"
                },
                "questions": {
                    "value": "See the above \"Weaknesses\" section for my high priority questions and concerns.\n\nI also had a few lower priority suggestions that did not affect my score:\n* In general, when you say that your method is more \"accurate\" than others in this work (e.g. the statement \"experiments in the next section show that our approach is significantly more 'accurate' in practice\"), can you be more specific about what exactly you mean by \"accurate\"? Do you simply mean that the sparse models learned by SPADE are a more faithful approximation of the true prediction model; or that the explanations produced by SPADE allow users to complete some task more accurately?\n* A nit about language:\n  * \"debugging\": \"interpreting a model's predictions on specific examples\" is actually more commonly known in this community as a \"local explanation\" (see Section 4.2 of [1]).  I believe the term \"debugging\" implies that the end user is a developer, who is hoping to understand what action should be taken to improve the model (e.g., as used in [2]).   I think it is fine to call your method \"sparsity-guided debugging\" as the interpretations provided could be used downstream to fix the model.  Maybe you could consider \"sparsity-guided interpretability\" or \"sparsity-guided explanations\" instead? :-) \n  * \"preprocessing\": I was confused by how the authors used the term \"preprocessing\" to describe the model pruning step of SPADE, given that I've typically only ever seen this term used to discuss data preprocessing that occurs before model training (vs. here you are referring to a post-hoc interpretation method that learns a sparse model).  Maybe you can use the term \"pruning\" instead?\n* Section 3.1: Can you provide a more formal definition of what you mean by \"sparse\" in this section? I am assuming you mean \"sparse\" in an L0 norm sense = \"the majority of weights are 0\", rather than an L2 sense = \"the weights have very small values\"? (I see later in Section 3.2 that you use an L2 sparsity constraint.  Can you provide intuition as to why you used L2 (rather than a different norm)?)\n* Section 3.1: Can you provide further intuition or evidence for the \"thinning\" hypothesis? I don't understand why sparsity discourages 'multifacetism'.  Intuitively, if the neurons at every layer at 'multifaceted', then even a sparse combination of multifaceted neurons will still be multifaceted\u2026\n* Section 3.2: Can you clarify in your draft what the 96.5% \"agreement percentage\" measures? Is it the agreement of the single final class prediction?\n\n[1] https://arxiv.org/abs/1702.08608\n[2] https://scholar.google.com/citations?view_op=view_citation&hl=en&user=y1bnRg4AAAAJ&sortby=pubdate&citation_for_view=y1bnRg4AAAAJ:aqlVkmm33-oC"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8073/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8073/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8073/Reviewer_mZPn"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8073/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698986321672,
            "cdate": 1698986321672,
            "tmdate": 1699636998800,
            "mdate": 1699636998800,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BLJl5txUxY",
                "forum": "ffcHGwb4KF",
                "replyto": "sjEnsJrSRP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8073/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8073/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to your review"
                    },
                    "comment": {
                        "value": "Thank you for your clear explanation of the strengths and weaknesses of SPADE. We would like to address the weaknesses below.\n\n### Weakness #1: Transparency about the limitations of the proposed approach.\n\n Thank you for your suggestion that we better address the limitations of our approach. We have updated the submission to more directly clarify these points, in particular addressing the case where ground truth is not available for any data, and so tuning is not possible, and provide results for a much faster implementation of SPADE, using a faster sparsity solver, reducing per-example computation time to 70 seconds.\n \nTo go through your points individually:\n\n* **Sparsity ratio tuning.**  We cross-validate by using a different set of emoji (and images) for tuning than we use for the main experiment. This is, of course, an imperfect proxy for the case where we are not looking for emoji trojans at all. Therefore, we also ran experiments where we used fixed sparsity ratios that are linearly interpolated by layer depth, from 0 in the initial layer to 99% in the final layer. We show in Table D13 that our method is also effective using these ratios.\n\n* **Computational cost.** We acknowledge that the computational cost of using SPADE can be high when using the method  on many examples, although we believe it is reasonable in cases where examples are created for human review, and so perhaps a few dozen are needed. Based on your and other reviewers\u2019 feedback, *we added a faster version of SPADE that requires 70 seconds per sample.* Please see the general response to the reviewers above, and appendix I of the revision for details.\n\n* **Strength of results.** We would push back on the assertion that the results are not that strong. Preprocessing with SPADE improves the attribution map AUC for all methods tried. The gains are certainly larger where the accuracy gap is higher, and smaller where the accuracy is already very high, and so there is not much room for improvement. Concretely, the prior work of Wong et al., ICML 2021 improved attribution AUC on average by 0.5%, whereas SPADE improves results by 4.2-5.5% on average (depending on the variant used). \n\n   We are not aware of any research that shows that the Trojan emoji evaluation favors any specific methods, and so have no reason to believe that gains from SPADE would be greater in \u2018real-world\u2019 scenarios. Specifically, we emphasize that SPADE *largely closes the gap* between different saliency attribution methods, allowing the field practitioner to choose one that is best suited for their task on other criteria.\n\n### Weakness #2: Clarity about experimental design\n\nThank you for your very concrete and helpful suggestions on improving the text. We will update the draft to better clarify these points.\n\nTo summarize our answers here:\n\n* **Clarify motivation for the Trojan experiment setup.** Our motivation for using them in this paper is that, to our knowledge, Trojan patches are the best available way to evaluate saliency map accuracy, which was our motivation for using them. We agree that our method cannot be used to find backdoors that are not present in the test set; we will add this limitation to the paper.\n\n* **Saliency map experiments.** We calculate accuracy only for test samples with Trojan examples, as those are the ones for which there is a ground truth to compare to. As the Trojan patches are not 100% effective for changing the label, we only use examples in which the predicted label (of the original network) is changed due to the presence of the Trojan. We always compute gradients (or any other saliency metric) with respect to the predicted (Trojan) class for the experiment, as that is the one for which the ground truth is available. However, SPADE can be used to compute saliency for any possible class as well.\n\n* **User study.**  An \u201cimage region\u201d is a section of an image that ScoreCAM, a broad-resolution saliency method (without SPADE preprocessing), selects as the most highly relevant for a predicted class label. While it is true that this method for creating regions is imperfect and possibly noisy, due to the very concerns regarding saliency attribution methods that our paper is addressing, we believe that this is the best proxy for the true model behavior that was possible to use.  We compute these regions for two classes for each model (so one region is the \u2018best\u2019 evidence for Class A and the other region is the \u2018best\u2019 evidence for Class B) and require that the two not intersect. The  neuron visualization (but not the name or any other description) of one of the two classes is then shown to the human evaluator, who is then asked to choose the correct patch for that class. Screenshots of this flow are provided in Appendix H."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700239866644,
                "cdate": 1700239866644,
                "tmdate": 1700239866644,
                "mdate": 1700239866644,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YtFZe7Oh6d",
                "forum": "ffcHGwb4KF",
                "replyto": "BLJl5txUxY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8073/Reviewer_mZPn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8073/Reviewer_mZPn"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your prompt and detailed response to my comments!  I include further clarifying questions and comments below.\n\n* **Terminology**: Thanks for your explanation!  I am OK with the authors' use of the terms \"debugging\" and \"pre-processing\".\n* **User study**: Thanks for your clarifying comment.  I am still a bit hesitant to call the region that is selected by ScoreCAM the \"ground truth\" important region, e.g. I hesitate to call this region the \"correct\" interpretation of the behavior of the dense model.  However, despite this fact, I do think it is an interesting result that the users are more likely to select an attribution that matches ScoreCAM when you show them SPADE explanations.  Concretely, can you stop referring to the ScoreCAM explanations as \"ground truth\" and instead just report \"alignment with ScoreCAM\"? \n\nFurther, can you clarify what you mean by this sentence:\n\"_When the network was preprocessed via SPADE, the users were over 10% more likely to **choose to make a decision** on which of the patches were responsible for the class prediction (87.4% when SPADE was used, versus 77.1% when it was not)._\"\nWhen you say \"choose to make a decision\", do you mean that the user did *not* select a \"Can't decide\" option (from Figure H.9)? \n\nFurther, in your study, did you show multiple users the same images/activations?  If so I am curious to see if the inter-annotator agreement is higher or lower for the SPADE condition."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700366519707,
                "cdate": 1700366519707,
                "tmdate": 1700366519707,
                "mdate": 1700366519707,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "in0ycrUKaO",
                "forum": "ffcHGwb4KF",
                "replyto": "sjEnsJrSRP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8073/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8073/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to your reply"
                    },
                    "comment": {
                        "value": "Thanks for your response and suggestions.\n\n**User Study**: We agree with the recommendation to avoid using the term \"ground truth\". Although ScoreCAM has been shown to be accurate, it is not perfect. Therefore, we have updated our manuscript, and replaced \"ground truth\" with your suggested term \"alignment with ScoreCAM\".\n\n**Question**: Yes, \"choosing to make a decision\" refers to instances where the user did not select the \"Can't decide\" option.\n\n**Agreement**: Yes, there are cases where users see the same image. We calculated the number of response pairs where users viewed the same image and did not select the \"Can't decide\" option. We then determined how many of these pairs resulted in the same decision. The results indicate that using SPADE leads to higher agreement among users.\n\n| Method   | Agree | Disagree | Agree/Disagree Ratio|\n|-------------|-------------------------|-----------------------------|----------------------------|\n| Dense     | 247                      | 89                              | 2.77                         |\n| SPADE   | 275                       | 83                             | 3.31                          |"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409236753,
                "cdate": 1700409236753,
                "tmdate": 1700409714523,
                "mdate": 1700409714523,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aIjfLaOhEj",
                "forum": "ffcHGwb4KF",
                "replyto": "in0ycrUKaO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8073/Reviewer_mZPn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8073/Reviewer_mZPn"
                ],
                "content": {
                    "title": {
                        "value": "Response acknowledged"
                    },
                    "comment": {
                        "value": "Thank you for responding to my clarifying questions, and engaging thoughtfully throughout the rebuttal!  I find it interesting and evidence that SPADE explanations are \"less confusing\" to users that they are more likely to agree with each other."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497491576,
                "cdate": 1700497491576,
                "tmdate": 1700497491576,
                "mdate": 1700497491576,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]