[
    {
        "title": "Sp-R-IP: A Decision-Focused Learning Strategy for Linear Programs that Avoids Overfitting"
    },
    {
        "review": {
            "id": "97pNqbcRj3",
            "forum": "o0oroLuPLZ",
            "replyto": "o0oroLuPLZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4549/Reviewer_AP5r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4549/Reviewer_AP5r"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an extension to the SPO+ reformulation of the decision-focused learning problem where the downstream decision problem can be framed with a linear objective. The SPO+ reformulation turns the decision-focused learning problem which is a bi-level optimization problem into solving a linear programming problem. The paper makes extensions on top of the SPO+ reformulation by introducing a neural network to forecast training and using the interior point method to solve the extended problem. To demonstrate the usefulness of the proposed method, the paper carried out experiments on a day-ahead energy scheduling dataset, comparing the proposed method with a wide variety of alternative methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* originality: the paper extends the SPO+ reformation of the decision-focused learning problem. I think the extension is not trivial but still somewhat incremental.\n* quality: I think the paper properly identifies the limitation of the SPO+ reformation in using a linear forecaster. The paper also provides a solid solution to mitigate this limitation. The proposed method is compared to a variety of alternatives in the experiments.\n* clarity: The presentation is clear. I can follow the paper.\n* significance: It may help improve the effectiveness of solving the decision-focused learning problem via SPO+ reformulation, although I find it to be an improvement in a niche area."
                },
                "weaknesses": {
                    "value": "* I find the paper's contribution to be somewhat incremental. Although I think the technical treatment described in the paper is non-trivial, the idea of replacing a linear forecaster with a non-linear one and the deployment of the interior point method appears to be straightforward observations.\n* While the paper compared the proposed method with other competing methods, experiments are conducted on one dataset. This may suggest that the proposed method solves a problem in a niche area.\n* The method appears to not be fairly efficient and scalable, as evidenced by the need for forecasting and warm starts."
                },
                "questions": {
                    "value": "typo: \" a second, reforumation, approach\"\nhow is x^* compted?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4549/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698701358447,
            "cdate": 1698701358447,
            "tmdate": 1699636432451,
            "mdate": 1699636432451,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F0NLTqPjC3",
                "forum": "o0oroLuPLZ",
                "replyto": "97pNqbcRj3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4549/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4549/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to weaknesses and questions"
                    },
                    "comment": {
                        "value": "Thank you for the time spent on this review and the feedback. Below you can find our response to the listed weaknesses and question, which we hope can convince you of the added value of the paper:\n\nW1: Although the proposed method builds upon the SPO+ reformulation framework as proposed by Elmachtoub & Grigas (2022), we believe that the use of a tailored interior point method to derive intermediate results that may generalize better on unseen data is a non-trivial step. Indeed, this contrasts with the prevailing emphasis in the optimization community on optimizing for speed in finding an optimum solution. Additionally, the extension to non-linear forecasters and the mini-batch approach that was added in the revised version of the manuscript substantially enhance the versatility of this reformulation approach.\n\nW2: See remark on \u2018Amount of experiments\u2019 in our comment to all reviewers.\n\nW3: See remark on \u2018Scalability\u2019 in our comment to all reviewers.\n\nQ1: Do you mean in equation (9) how you should compute $x^*(c_i)$? Since $c_i$ is the ground truth, this is pre-computed before optimization: $x^*(c_i)$ constitutes the optimal decision in the case of perfect foresight."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4549/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699379988,
                "cdate": 1700699379988,
                "tmdate": 1700699379988,
                "mdate": 1700699379988,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "p31flEt0bG",
            "forum": "o0oroLuPLZ",
            "replyto": "o0oroLuPLZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4549/Reviewer_G7zN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4549/Reviewer_G7zN"
            ],
            "content": {
                "summary": {
                    "value": "This paper builds upon the dual formulation of the Linear Program (LP) Decision Focused Learning (DFL) framework, as initially introduced in Elmachtoub & Grigas (2022), in two notable ways:\n\n1. The introduction of an interior point method to tackle the constraint optimization problem, enhancing the computational efficiency and accuracy of the framework.\n\n2. Expansion of the framework to accommodate non-linear and non-convex mappings, such as neural networks, as opposed to the original work, which exclusively considered linear mappings.\n\nIn an empirical assessment conducted on a single dataset, the paper substantiates the effectiveness of the proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper presents a commendable mathematical formulation in Section 4, characterized by its clarity and logical coherence. This robust formulation lays a solid foundation for the subsequent analyses and conclusions.\n\n- The experimental comparison conducted in this study is particularly commendable for its reliance on real-world practical data. This empirical approach not only enhances the relevance and applicability of the findings but also underscores the potential real-world impact of the proposed methodology."
                },
                "weaknesses": {
                    "value": "- **Novelty and contribution**: The method outlined in this paper is a direct extension of the dual linear programming (LP) approach put forth in Elmachtoub & Grigas (2022). The first contribution, introducing the application of an interior point method (IP) for constrained optimization, represents a straightforward but meaningful addition from an optimization standpoint. Similarly, the second contribution involving the incorporation of neural network (NN) layers into constraints is conceptually clear-cut. \n\n- **Scalability**: The optimization problem defined in equation 15 encompasses all NN layers and parameters, potentially leading to scalability challenges. The authors acknowledge this concern in Section 4.3 and propose heuristics to partially address it. Nonetheless, it is plausible that this issue may persist despite these mitigating measures. The experimental results also reflect this, as the use of a neural network with just one hidden layer took a considerable amount of time to train, surpassing 14,000 seconds. This highlights a critical scalability concern that may limit the practical applicability of the proposed method.\n\n- **Limited experiments**: The validation of the proposed method exclusively on a single dataset may be insufficient to establish its robustness and generalizability. It is advisable to broaden the experimental scope by evaluating the approach across multiple datasets. This would provide a more comprehensive understanding of its performance under diverse conditions and enhance the overall confidence in the proposed methodology. Expanding the experimental validation to encompass a wider range of scenarios would strengthen the empirical foundation of the study."
                },
                "questions": {
                    "value": "The paper refers to Appendix A to detailed explanation. However, Appendix A lacks such a description. I understand that the derivation follows from Elmachtoub & Grigas (2022) but still, it would be nice to incorporate the detailed derivation in the paper. This is also true for eq. 15."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4549/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698713985303,
            "cdate": 1698713985303,
            "tmdate": 1699636432278,
            "mdate": 1699636432278,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gWQVFn9Wq9",
                "forum": "o0oroLuPLZ",
                "replyto": "p31flEt0bG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4549/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4549/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to weaknesses and question"
                    },
                    "comment": {
                        "value": "Thank you for the diligent and fair review. We hope that our replies to the listed weaknesses and question below can convince you of the added value of our paper.\n\nW1: Although the proposed method builds upon the SPO+ reformulation framework as proposed by Elmachtoub & Grigas (2022), we believe that the use of a tailored interior point method to derive intermediate results that may generalize better on unseen data is a non-trivial step. Indeed, this contrasts with the prevailing emphasis in the optimization community on optimizing for speed in finding an optimum solution. Additionally, the extension to non-linear forecasters and the mini-batch approach that was added in the revised version of the manuscript substantially enhance the versatility of this reformulation approach.\n\nW2: See remark on \u2018Scalability\u2019 in our comment to all reviewers.\n\nW3: See remark on \u2018Amount of experiments\u2019 in our comment to all reviewers.\n\nQ1: In the revised manuscript we have extended appendix A to include more details on the reformulation of both the linear and non-linear ERM."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4549/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699244907,
                "cdate": 1700699244907,
                "tmdate": 1700699244907,
                "mdate": 1700699244907,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GnskP0JmJ1",
            "forum": "o0oroLuPLZ",
            "replyto": "o0oroLuPLZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4549/Reviewer_XyMG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4549/Reviewer_XyMG"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an approach for decision focused learning in a setting where a neural network is trained to predict latent objective coefficients for a linear program. The proposed approach builds on previous work that trains a linear model by solving an optimization problem that identifies the best linear model leading to high-quality downstream decisions as evaluated by the true objective coefficients. The authors extend this work by proposing a method for finding the best neural network parameters that lead to high quality downstream decisions. The proposed approach formulates this end-to-end pipeline as an optimization problem which is solved using an interior point method which maintains a current solution, comprised of neural network weights, and iteratively updates the solution to trade off avoiding constraint violation versus rewarding higher-quality solutions. The authors compare three versions of their approach for training both neural networks as well as linear models against three previous approaches: an implicit differentiation approach with quadratic smoothing, a subgradient approach, and a subgradient approach with reformulation. They evaluate these methods on one real world dataset for optimal scheduling of energy storage. The results demonstrate that their approach improves over the baselines in performance while it does take extensive time to train. \n\nWith the main strength being the novelty of the approach, there are several limitations in the method and empirical evaluation. If these are addressed, I am happy to increase my score."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The main strength of this approach is that their formulation and solving approach are novel, and they demonstrate improved performance on a real world setting over reasonable baselines. \n\nThe solving approach of interior point method is promising in that it can potentially combine the gradient-based methods that can be used to solved linear programs with the gradient-based methods for training neural networks. Proper combination of these two has the potential to tightly integrate the learning and optimization components for improved performance as suggested in this work.\n\nThe method additionally does seem to give improved performance over the investigated baselines in a realistic setting. Additionally, the contribution of this new setting to the space of decision-focused learning will greatly improve the space by introducing another method for evaluation that has real world impact."
                },
                "weaknesses": {
                    "value": "The main weaknesses of the proposed approach are the running time and the empirical evaluation.\n\nThe approach overall seems to take longer due to requiring the solving of a large optimization model with the interior point method, an approach that is known to not scale well. The authors hint that this might be alleviated by using minibatches which seems reasonable in that they could simply iterate by optimizing over a minibatch of problem instances from one iteration to the next. It would be great to understand whether using minibatches improves or harms the training performance as it may make the training process more unstable. \n\nIn the evaluation, there are a number of aspects that would improve the paper.\n\nIt would be helpful to evaluate the proposed approach against the relevant baselines. For instance, it would be helpful to compare against the other interior point method for decision-focused learning, in the cited Mandi & Guns 2020 paper. Additionally, consider evaluating against the implicit MLE paper [1], differentiable perturbed optimizers [2], dfl without optimization [3], and using CvxpyLayers [4]. \n\nFurthermore, it would be helpful to evaluate some of the LP-based settings used in previous work. For instance, the bipartite matching setting from the cited Wilder et al. 2018, the warcraft path planning setting from [5], or the shortest path setting from the cited Mandi and Guns 2020 paper. Since the evaluation is solely empirical, it would help to further improve the empirical evaluation by demonstrating that the method works in more settings.\n\n\n\n[1] Niepert, Mathias, Pasquale Minervini, and Luca Franceschi. \"Implicit MLE: backpropagating through discrete exponential family distributions.\" NeurIPS (2021) \n\n[2] Berthet, Q., Blondel, M., Teboul, O., Cuturi, M., Vert, J. P., & Bach, F. (2020). Learning with differentiable perturbed optimizers. NeurIPS (2020).\n\n[3] Shah, Sanket, et al. \"Decision-focused learning without decision-making: Learning locally optimized decision losses.\" Advances in Neural Information Processing Systems 35 (2022): 1320-1332.\n\n[4] Agrawal, Akshay, et al. \"Differentiable convex optimization layers.\" NeurIPS (2019).\n\n[5] Pogan\u010di\u0107, Marin Vlastelica, et al. \"Differentiation of blackbox combinatorial solvers.\" ICLR. 2019."
                },
                "questions": {
                    "value": "Is this method potentially applicable to other optimization frameworks which use interior methods for solving? It seems that it requires taking the dual of the downstream optimization problem. Would it be readily applicable for prediction plus optimization for quadratic programs? Is it possible to extend this framework to differentiation of nonlinear optimization problems?\n\nWhy is time bolded for the proposed method when it seems to consistently have high running times especially when compared to the implicit differentiation method?\n\n\nWhat is the impact of penalizing the difference between the initial predictions in the formulation? It seems that this is present only for the proposed method but not for the baselines whereas the penalty term could be easily added to the implicit differentiation method by adding a penalty term. It might help to preform an ablation study to understand the impact of training using IP versus adding a penalty for deviating from the initial prediction. For instance, this deviation could also be used for the ID models by adding a loss that penalizes deviation from the initial cost prediction. \n\nAlong the lines of using a pretrained model, does the cold start method have access to the predictions of the pretrained model as it is solving 15? Do the other methods have access to the pretrained model as well in the cold start?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4549/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775501598,
            "cdate": 1698775501598,
            "tmdate": 1699636432187,
            "mdate": 1699636432187,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bWxCkk7iP3",
                "forum": "o0oroLuPLZ",
                "replyto": "GnskP0JmJ1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4549/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4549/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to weaknesses and questions"
                    },
                    "comment": {
                        "value": "Thank you for the diligent and fair review, and the useful suggestions. We hope that our replies to the listed weaknesses and questions below can convince you of the added value of our paper.\n\nW1 (scalabiliyt): See remark on \u2018Scalability\u2019 in our comment to all reviewers.\n\nW2 (benchmark methods): We agree with the reviewer that a thorough benchmarking exercise is needed to fully validate the proposed method. In the context of this paper being a proof of concept of a largely unexplored method, we do believe that the current set of methods gives a good first view of the relative performance. The main objective is to show that the method outperforms the available SPO+-based methods (being the subgradient method, Sp-SG, and the current state-of-the-art reformulation without validation performance tracking, Sp-R). Many other decision-focused methods have been proposed and providing a fair comparison requires a diligent effort in ensuring the method is implemented and tuned correctly, which we believe is not feasible from a practical point of view for all the methods proposed. For that reason, we have focused on the method of implicit differentiation, which seems to be the main benchmark from the literature. We did, however, extend our analysis in this revised version to also include the interior point method proposed in (Mandi & Guns, 2020) by considering the log-barrier smoothing function. The result of this method is added under the name ID-LB to Table 1 and Table 3 of the revised manuscript. The results show a slight improvement compared to the ID-Q method, but still a significantly worse performance than what we propose. Please note that the implicit differentiation method with quadratic smoothing, ID-Q, that was also included in the first version is also implemented with Cvxpylayers.  We detail this in Appendix D1 of the revised manuscript.\n\nW3 (limited experiments): Thank you for the useful suggestions of possible other experiments to test the methodology on. See the remark on \u2018Amount of experiments\u2019 for our general comment on this.\n\nQ1: The proposed methodology is tailored for linear optimization problems that need to be informed by unknown cost/price information, and is not immediately appliable to quadratic or general non-linear optimization problems. The reason is the requirement of the surrogate SPO+ loss function: to be able to perform the reformulation procedure outlined in Appendix A (which is extended in the revised manuscript), you need a surrogate loss function that can be integrated in the outer optimization problem of training the forecaster. One quality of the SPO+ loss function that is crucial for this is that it involves the objective value of a linear optimization program (as opposed to arguments leading to the optimal value in the case of the regret loss function). Whereas (Elmachtoub & Grigas, 2022) show that the SPO+ loss function is Fisher consistent with the regret loss function in problem setting (1) of our manuscript, it is unclear if a similar surrogate exists for a quadratic (or any general non-linear) optimization problem, and what it would look like. However, similar to the knapsack experiment performed in (Mandi & Guns, 2020), this approach could be implemented for mixed-integer linear problems by considering its continuous relaxation.\n\nQ2: Thank you for pointing this out. We have adjusted the bolding in Tables 1 and 3 of the revised manuscript\n\nQ3: Thank you for this useful suggestion. Due to the limited time for this revision, we were unable to implement this at this point.\n\nQ4: For all models, the following holds: \n\n\u2022\tCold start: the initial price forecast of the MSE-trained model is included as a feature\n\n\u2022\tWarm start: has the same features as the cold start, but the starting point of the training constitutes a forecaster that either produces exactly the initial forecast (for the linear re-forecast) or produces prices very close to the initial forecast (for the softplus re-forecaster). The latter is accomplished by pre-training the re-forecaster to minimize the MSE between its output and the initial forecast"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4549/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698992614,
                "cdate": 1700698992614,
                "tmdate": 1700698992614,
                "mdate": 1700698992614,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xjae0RAuDk",
            "forum": "o0oroLuPLZ",
            "replyto": "o0oroLuPLZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4549/Reviewer_pKJU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4549/Reviewer_pKJU"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the problem of learning cost functions of linear programs. The decision focused learning aspect incorporates the downstream decisions obtained on solving the estimated linear program. This is usually accomplished by adding a regret loss term in the training procedure. The paper builds upon the SPO+ framework which constructs a convex surrogate for the generally non-convex decision focused loss term. The key idea is to use an interior point method for solving the same surrogate while also incorporating early stopping. The proposed approach extends to both linear and non-linear model classes. Experiments are performed on day-ahead scheduling problem for energy storage."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper does a really good job of introducing the literature to a reader not well-versed with this literature. I think section 3 is a really good setup for understanding both the problem and solution space.\n\n- Although the idea mostly builds upon SPO+ framework, I think it is still valuable as it allows non-linear model classes not possible with the earlier approach.\n\n- The proposed approach does well on an important real-world application related to electricity market."
                },
                "weaknesses": {
                    "value": "- I think the comment about treating intermediate points on the path of a interior point solver as intermediate solutions require more justification. I am referring to \"However, we argue that when the optimization program is an ERM for training a forecaster, the points on the central path should be regarded as actual intermediate solutions to be tested on the validation set.\" Please provide some principled justification as this is critical to the early stopping procedure. \n\n- Does the method's extension to neural networks in Section 4.2 work for any general activation function or is it restricted to just the ReLU function? How does the choice of activation function (i.e. 3rd constraint in (15)) affect the optimization problem? \n\n- If possible, can you please add error bars to the result in Table 1."
                },
                "questions": {
                    "value": "Please see weaknesses section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4549/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698872770206,
            "cdate": 1698872770206,
            "tmdate": 1699636432119,
            "mdate": 1699636432119,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rz1AfH9Jop",
                "forum": "o0oroLuPLZ",
                "replyto": "xjae0RAuDk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4549/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4549/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to weaknesses"
                    },
                    "comment": {
                        "value": "Thank you for the the time and effort spent on this review. Please find below our response to the listed weaknesses.\n\nW1: See remark on \u2018Interpretation of interior points\u2019 in our comment to all reviewers.\n\nW2: In our implementation, we used the softplus activation function, which is a smooth variant of the ReLU. This approach would work for any activation function. However, we observe that some activation functions are more suitable for the interior point method in terms of required train time. For example, replacing the softplus activation with ReLU leads to an average increase in train time of around 30% in our experiment. We agree that further investigations are worthwhile to fully understand the interplay between the IP solver and the architecture of the NN, but in the context of this paper, what matters is the generic nature of the proposed framework, paving the way for future improvements. \n\nW3: We agree with the relevance of such sensitivity analysis. However, given the time constraints for this revision, we were unable at this point to add such error bars."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4549/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698716624,
                "cdate": 1700698716624,
                "tmdate": 1700698716624,
                "mdate": 1700698716624,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ibOnQS52x9",
            "forum": "o0oroLuPLZ",
            "replyto": "o0oroLuPLZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4549/Reviewer_ysWa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4549/Reviewer_ysWa"
            ],
            "content": {
                "summary": {
                    "value": "Machine learning forecasts are often used as part of downstream decision making tasks, motivating the area of decision-focused learning. In decision-focused learning, the downstream optimization problem is included in the forecasting pipeline by using a task-aware loss function, such as the regret loss. However, these loss functions often have ill-defined gradients, making it difficult to apply gradient descent to minimize the risk. As one way of approaching this problem, a previous work has used a subgradient methods to minimize a convex surrogate of the regret loss. This work proposes an interior point method to solve the forecaster training problem that avoids overfitting by tracking the validation regret obtains by different iterates. An advantage of this approach is that it can accomodate training of neural network forecasters."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The problem that the authors study--broadly, how to solve optimization problems by machine learning forecasters, is an important and broadly applicable problem. Practitioners from economics and operations research often interested in problems of this flavor, and it's great that this work aims to develop algorithms that can incorporate flexible forecasters to solve optimization problems.\n\n2. The authors do a good job of summarizing and contextualizing previous work, and they carefully distill the shortcomings of previous approaches.\n\n3. The idea of using a \"validation performance tracking procedure,\" inspired by early stopping, is a simple data-driven approach for preventing overfitting. A common concern in solving data-driven constrained optimization problems is that the solution to the problem may not generalize well out-of-sample. I think this procedure offers a simple, transparent, and promising approach of preventing over-fitting. It's an idea that has come to mind before but I actually haven't seen it formally written up in any works so far. Although the idea is simple, I think this idea could be very useful strategy for handling overfitting in constrained optimization problems, and I would like to see this idea developed with more careful theory in future works.\n\nOverall, I recommend to accept this paper and am willing to raise my score if the authors adequately address the questions listed below. This paper tackles an important and challenging problem, and the ideas that they propose (1) a validation performance tracking strategy to evaluate iterates of an interior point method (2) using interior-point methods to facilitate using neural network forecasters as part of optimization problems--are promising and have the potential to be useful in practice."
                },
                "weaknesses": {
                    "value": "1. One weakness of this paper is that the authors do not provide very much theoretical justification for why their validation performance tracking procedure may yield improved out-of-sample performance. As a result, the strategy that they propose is largely a heuristic. That being said, I think this heuristic is quite promising and I would hope that future works can develop a rigorous theory to justify this approach.\n\n2. There are some clarity concerns that I had (see questions below), but I believe that these can be addressed with proper writing and motivation.\n\n3. The authors demonstrate a nice proof-of-concept, but I would be interested in seeing a more thorough empirical evaluation, even on synthetic optimization problems."
                },
                "questions": {
                    "value": "1. It is somewhat unclear what \"the ERM\" is in the following sentence in the introduction ``Secondly, the ERM can be re-written to a single-level\noptimization program by applying duality theory`` -- what ERM problem are the authors referring to? The problem of training the forecaster? The problem of minimizing the regret loss? While this is clarified later in the paper, it would be helpful to make this clear in the introduction as well.\n\n2. In equation 4, the set $S$ is not yet defined? I assume that $S$ is the feasible set for $x$. From the example in Equation 1, I presume that $S = \\{ x \\mid Ax \\geq b}.$ I think it would be helpful if the authors could reiterate the downstream optimization task again in Section 3 for clarity.\n\n3. Do the authors have any intuition on why training a neural network to minimize the SPO+ loss function $l^{SPO+}$ performs poorly?\n\n4. How does the validation performance tracking procedure that the authors propose compare to just solving the original SPO+ problem (Equation 9) with regularization?\n\n5. Could the authors elaborate more on the following sentence: ``We argue\nthat when the optimization program is an ERM for training a forecaster, the points on the central\npath should be regarded as actual intermediate solutions to be tested on the validation set\"?\n\n6. The authors make the following comment: ``Since the proposed methodology currently does not involve mini-batches, the IP solution method processes all the train data in a single run.``\nDo the authors think it would be possible to implement a ``stochastic`` version of the IP method where a new mini-batch of data could be used to solve for each the optimal parameters in each iteration of the interior-point method? Could this also prevent overfitting? Or do the authors expect this approach to be unstable?\n\n7. The idea of warm-starting the forecaster by first fitting the forecaster with the MSE loss is another useful heuristic--this one is probably used in various prior works. Also, it would great if the authors could comment on how much benefit is derived from each stage of training (how far does fitting with the MSE loss get you? how much additional benefit does the IP method provide?). Is the forecaster that is fit with the MSE loss the ``Initial FC`` in Table 1? In addition, the authors comment that a ``refined`` set of features are used to train the second-stage forecaster, how is this refined set of features selected? Also, could the authors add citations to other works that use the warm-start strategy? \n\n8. I am struggling to interpret Figure 2. Could the authors explain the difference betwen the SP-R-IP methods they evaluate in the experiments? What is the expected behavior under each of these 3 different algorithms? Are they all variants on the authors' proposed approach? What is the reason that the validation regret of SP-R-IPs and SP-R-IPd oscillates (instead of decreasing monotonically)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4549/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698886946248,
            "cdate": 1698886946248,
            "tmdate": 1699636432054,
            "mdate": 1699636432054,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5tcoe3hqFc",
                "forum": "o0oroLuPLZ",
                "replyto": "ibOnQS52x9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4549/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4549/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to questions 1-7"
                    },
                    "comment": {
                        "value": "Thank you for the diligent review. Please find below our answers to the specific questions.\n\nQ1: It is indeed the problem of training the forecaster, in this case with the regret loss function. We have re-formulated this to \u201cSecondly, the training problem can be \u2026\u201d in the revised manuscript.\n\nQ2: We agree that it improves clarity to reiterate the specifics of the set S, which indeed corresponds to the set {x|Ax>=b}. We have included this in the revised version of the manuscript.\n\nQ3: As a clarification, we would like to remind the reviewer that our approach (any form of Sp-R-IP) involves using the SPO+ loss function. The good performance of our model therefore contributes to the validation of the SPO+ loss function, which was already theoretically shown to be consistent with the regret in (Elmachtoub and Grigas, 2022). What we see is that training a model with their originally-proposed subgradient descent method may be less performant than our proposed interior point method. The intuition behind this is two-fold: the first reason is that training a forecaster to minimize such a decision-focused loss function is a highly non-convex problem. Whereas our method smoothens the problem by introducing the log-barrier term, (sub)gradient methods are very prone to getting stuck in local optima. This intuition is underscored by the results in Table 3, where we show that the (sub)gradient methods ID-Q and Sp-SG perform very poorly when cold started, whereas the proposed Sp-R-IP method is able to attain similar performance when cold started compared to the warm start. The second reason is that the expression of the subgradient will result in model updates even when the current forecast leads to the perfect decision, which we showcase in Appendix D.2.\n\nQ4: We observe that when a regularizer is used, the difference between the best performing model along the points accessed on the central path and the final resulting model generally reduces, but there is still a noticeable difference. This can be concluded from Tables 1 and 2: Table 2 shows that for the Sp-R methods, we explore multiple values of the regularization, whereas this clearly still results in improved performance of the Sp-R-IP methods as compared to the Sp-R method in Table 1.\n\nQ5: See remark on \u2018Interpretation of interior points\u2019 in our comment to all reviewers.\n\nQ6: See remark on \u2018Scalability\u2019 in our comment to all reviewers.\n\nQ7: The row named \u201cInitial FC\u201d indeed represents the performance of the forecaster that was trained to minimize the MSE. The relative performance (second column in Table 1) of the other methods constitutes the difference compared to this baseline. The regret is improved by 19% for the best model, Sp-R-IPs-softplus. The features were selected based on our experience in energy systems. Here, we leverage our understanding of the market clearing mechanism in Europe that considers supply and demand in European countries jointly. The features used in re-forecasting should in this sense capture the most important information.\nAs detailed in our general comment, this warm start naturally emerges from the observation that decision-focused learning methods seem to perform poorly when not warm started, even when the original price forecast is included in the feature set. Other examples of such a two-stage methodology are extreme learning machines and large language models adapters, see Section 4.3."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4549/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698443016,
                "cdate": 1700698443016,
                "tmdate": 1700698443016,
                "mdate": 1700698443016,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jHP4Fidqqj",
                "forum": "o0oroLuPLZ",
                "replyto": "ibOnQS52x9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4549/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4549/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to question 8"
                    },
                    "comment": {
                        "value": "The models compared in these figures are indeed variants of our proposed approach of training the forecaster by solving the ERM that follows from using the SPO+ loss function and reformulating it to a single-level optimization program, solving it with an interior point method. The way they obtain points along the central path differs:\n\n\u2022\tSp-R-IP directly uses the default IPOPT algorithm for getting the values of \\mu (the barrier term). This algorithm prioritizes speed of finding a solution over granularity and amount of points obtained along the central path. This makes sense in the general setting where people try to find an optimal solution as fast as possible. However, this results in fewer evaluations of the validation performance as is shown in Figure 4: only 4 values of \\mu are considered. Even though this is not necessarily a problem for the performance on the train set, where you expect to find the best result for the smallest value of \\mu, this may lead you to oversee manifestations of the forecaster on the central path that generalize better on the validation set, as can be seen in Figure 2b.\n\n\u2022\tTo overcome that issue, we propose to manually set the value of \\mu for which IPOPT should optimize iteratively:\n\no\tSp-R-IPs (static) decreases $\\mu$ at a steady pace, in the case of Figure 2 by a factor 1.5, i.e. $\\mu_{n+1} = \\mu_{n}/1.5$.\no\tSp-R-IPd (dynamic) decreases $\\mu$ dynamically based on the validation performance, i.e. slowing down the rate of decreasing the value of $\\mu$ based on equations (12)-(14).\n\nThere is no immediate explanation as to why the validation performance oscillates, other than that there is no perfect match between the train and validation set. This is a behaviour that can be expected in gradient descent approaches too."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4549/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698563099,
                "cdate": 1700698563099,
                "tmdate": 1700698563099,
                "mdate": 1700698563099,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]