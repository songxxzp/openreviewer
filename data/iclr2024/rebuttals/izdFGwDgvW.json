[
    {
        "title": "ICE: Image-Caption Encoding for Improved Out-Of-Distribution Generalization In Vision-Language Models"
    },
    {
        "review": {
            "id": "i7xo20uiUX",
            "forum": "izdFGwDgvW",
            "replyto": "izdFGwDgvW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3752/Reviewer_jLvo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3752/Reviewer_jLvo"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method called Image-Caption Encoding (ICE) to improve the out-of-distribution generalization performance of vision-language models on image classification tasks. \nICE is a training-free method that combines information from both image embeddings and text caption embeddings generated by the model to make a more informed prediction at test time. The top-k predictions are weighted from the caption predictions and image predictions for improved classification and correcting any mistakes from the image classifier. \nExtensive experiments show ICE provides consistent improvements of around 0.5% on average and up to 3% on challenging datasets when added to existing state-of-the-art baseline methods for both zero-shot classification and few-shot fine-tuning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Originality**\n\nThe idea of using image captions at evaluation time to improve OOD image classification is novel. The approach of combining image and caption probabilities is also creative, building on ideas from ensembling while utilizing unique properties of captions.\n\n**Quality** \n\nThe paper presents thorough experiments across 15 datasets with multiple SOTA baselines. Ablation studies analyze the impact of key parameters. The paper also provides examples and analysis to develop an intuition for why and how ICE works, what are it's limitations and when it fails.\n\n**Clarity**\n\nThe paper is clearly written and easy to follow. The method is intuitively explained with figures. Experiments and results are well-organized.\n\n**Significance** \n\nImproving out-of-distribution generalization is an important problem. The consistent gains from this simple approach could make the method widely applicable by utilizing informative captions in unique ways to improve OOD classification."
                },
                "weaknesses": {
                    "value": "- As discussed in the limitations, the method relies on captions providing useful supplementary information. Generating captions from a stronger vision-language model (such as BLIP) and then combining those captions with image predictions could help to make caption selection more robust.\n\n- Determining the optimal weight between image and caption probabilities seems challenging. Would a learning-based approach that adaptively learns weights for each branch work better?  Exploring other ways to set this weight adaptively could strengthen the approach and give more insights into how failure can be handled.\n\n- There is no comparison to other ensembling techniques that could provide diversity. While the motivation behind using captions to improve the OOD is interesting, the improvements from the model are small which raises two questions - a) are the captions generated the main problem, or b) are the way they are used to correct the prediction? A more descriptive SOTA captioning model would help in answering the first question and hence lead the way to design better ensembling techniques."
                },
                "questions": {
                    "value": "No specific questions, please look at weaknesses for certain clarifications."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3752/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698239796442,
            "cdate": 1698239796442,
            "tmdate": 1699636331546,
            "mdate": 1699636331546,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i5RXvuJEak",
                "forum": "izdFGwDgvW",
                "replyto": "i7xo20uiUX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3752/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3752/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jLvo"
                    },
                    "comment": {
                        "value": "**Comment:** As discussed in the limitations, the method relies on captions providing useful supplementary information. Generating captions from a stronger vision-language model (such as BLIP) and then combining those captions with image predictions could help to make caption selection more robust.\n\n**Response:** Thank you for your suggestion, this is an interesting topic of investigation. Here are some supplementary results on ImageNet and 10 other datasets using BLIP-2 captions instead of CoCa captions:\n\n|  |  INet | Caltech | Pets | Cars | Flowers | Food | Aircraft | SUN | DTD | EuroSAT | UCF | Mean | \n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Zero-shot (image) | 75.1 | 97.6 | 93.8 | 92.7 | 77.3 | 87.5 | 36.6 | 73.6 | 57.2 | 58.5 | 73.4 | 74.8 | \n|     + CoCa ICE | 75.6 | 97.0 | 93.5 | 93.0 | 77.6 | 87.6 | 40.0 | 73.9 | 59.8 | 61.1 | 74.3 | 75.8| \n|     + BLIP2 ICE | 75.3 | 97.2 | 93.5 | 92.7 | 77.8 | 87.8 | 36.6 | 74.1 | 59.8 | 60.9 | 75.4 | 75.6 | \n\nWe use the largest available BLIP-2 model, which contains a ViT-g image encoder with a FlanT5-XXL LLM. We observe a similar accuracy gain when applying ICE with these BLIP-2 captions. BLIP-2 captions achieve slightly lower numerical results than CoCa captions on average, despite using a much larger model. We think this is because BLIP-2 sacrifices fine-grained information in favor of a more descriptive caption. For example, for one image in the Pets dataset, the CoCa caption is: \"a photo of a abyssinian cat'', while the BLIP-2 caption reads: \"a photo of a cat standing on a table''. The CoCa caption gives us the correct classification for the pet in the image, while the BLIP-2 caption describes the scene in general terms without providing the information necessary for pets classification. These results illustrate that simply using a better captioner does not necessarily lead to better ZS accuracy under our ICE framework. However, we believe that using LLM prompting techniques to elicit more fine-grained information from large captioning models and incorporating that information into ICE is an interesting direction for future work.\nWe will include our analysis with BLIP in our revision as part of the supplementary material.\n\n[blip2] Li, Junnan, et al. \"Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\" arXiv preprint arXiv:2301.12597 (2023).\n\n**Comment:** Determining the optimal weight between image and caption probabilities seems challenging. Would a learning-based approach that adaptively learns weights for each branch work better? Exploring other ways to set this weight adaptively could strengthen the approach and give more insights into how failure can be handled.\n\n**Response:** Thank you for your suggestion, this is an excellent point. \nWe emphasize that ICE is a zero-shot method, i.e. we assume no additional training or access to test-time information. \nHowever, if we relax the constraints to allow for some learning, there can definitely be room for improvement.\n\nOne method we attempted was adaptively learning the value of $\\xi$ as a function of image and caption embeddings. $\\xi$ determines the weight between the image and caption predictions according to Equation (3). We use a simple Multi-Layer Perceptron with ReLU activations and sigmoid output to predict $\\xi$ and train the network on few-shot ImageNet data. We use the same $\\xi$ prediction network for all datasets at test time. The results are provided in the tables below:\n\n|  | INet-V2 | Sketch | INet-A | INet-R | DG Mean | \n| --- | --- | --- | --- | --- | --- | \n|  ICE accuracy (fixed $\\xi$) | 67.7 | 63.8 | 54.4 | 87.4 | 68.3 |\n| ICE with adaptive $\\xi$      | 67.9 | 63.9 | 54.8 | 87.5 | 68.5 | \n\n|  |  INet | Caltech | Pets | Cars | Flowers | Food | Aircraft | SUN | DTD | EuroSAT | UCF | Mean | \n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  ICE accuracy (fixed $\\xi$) | 75.6 | 97.0 | 93.5 | 93.0 | 77.6 | 87.6 | 40.0 | 73.9 | 59.8 | 61.1 | 74.3 | 75.8 |\n| ICE with adaptive $\\xi$       | 75.7 | 97.4 | 93.7 | 92.9 | 77.7 | 87.2 | 40.2 | 73.7 | 58.4 | 59.3 | 74.6 | 75.5 |\n\nWe observe that the adaptive $\\xi$ improves accuracy on all the ImageNet datasets, including both the ImageNet validation set and the domain-shifted versions. This improvement is relatively small due to the limited capacity of the $\\xi$ network, but it is consistent. This is a promising result that demonstrates that the $\\xi$ can be learned. The adaptive $\\xi$ degrades the accuracy on some datasets due to the large label-space shift relative to ImageNet classes (which the $\\xi$ network was trained on). However, the promising results on the domain generalization targets could warrant further work. We welcome further discussion on alternative methods to find better weights between the image and caption embeddings."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700273705011,
                "cdate": 1700273705011,
                "tmdate": 1700297949185,
                "mdate": 1700297949185,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dFSyH3Wx2h",
                "forum": "izdFGwDgvW",
                "replyto": "i7xo20uiUX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3752/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3752/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jLvo continued"
                    },
                    "comment": {
                        "value": "**Comment:** There is no comparison to other ensembling techniques that could provide diversity. While the motivation behind using captions to improve the OOD is interesting, the improvements from the model are small which raises two questions - a) are the captions generated the main problem, or b) are the way they are used to correct the prediction? A more descriptive SOTA captioning model would help in answering the first question and hence lead the way to design better ensembling techniques.\n\n**Response:** Our method is complementary to other ensembling methods, such as ensembling image augmentations or ensembling multiple pretrained models, because we leverage a different source of diversity. \nFrom our experiments, we believe that we have performed a comprehensive analysis of our method stacked on top of prior SoTA few-shot OOD methods. \nAs an aside, we claim ICE to be different from other ensembling techniques because we do not use other independently trained models as part of our predictions; \nwe specifically take advantage of captioning properties inherent in the CoCa architecture to better guide downstream classification.\n\n\nTo address whether the captions being generated are the main problem or the way they are being used: the BLIP experiment above confirms that merely more \"descriptive'' captions do not increase the zero-shot accuracy. \nHowever, this was likely because BLIP-2 captions do not contain more information relevant to the classification problem. \nWe believe that captions which contain more specific information about objects in the image could increase the performance of zero-shot ICE. \nHence, automated prompting of large VL models like BLIP-2 to elicit more specific information would be an interesting direction for future work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700273873444,
                "cdate": 1700273873444,
                "tmdate": 1700273873444,
                "mdate": 1700273873444,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CXKbvVTw1c",
                "forum": "izdFGwDgvW",
                "replyto": "i7xo20uiUX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3752/Reviewer_jLvo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3752/Reviewer_jLvo"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewers"
                    },
                    "comment": {
                        "value": "Thanks to the authors for the detailed response and new experimental results. \nMy main concern is regarding the new BLIP-2 experiments as it does not lead to improvements in performance. As also pointed out by reviewer oC1d, this could be due to the caption itself or LLM prompting. As for fine-grained classes it is hard to get captions that are useful for improving classification accuracy, this method will fall short. Maybe combining this with other sources of knowledge and such would lead to better gains\n\nGiven the response from other reviewers and the limitations of the method, I would like to keep the borderline score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738722073,
                "cdate": 1700738722073,
                "tmdate": 1700738722073,
                "mdate": 1700738722073,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lXtcoiKjd8",
            "forum": "izdFGwDgvW",
            "replyto": "izdFGwDgvW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3752/Reviewer_DXxh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3752/Reviewer_DXxh"
            ],
            "content": {
                "summary": {
                    "value": "The address the problem of out-of-distribution (OOD) generalization of image classification models. The proposed method: ICE, is build on the premise that when a OOD datapoint is misclassified, the correct class can sometimes be found in the Top-K predicted classes. To take advantage of this property, ICE enforces consistency between the image-conditioned and caption-conditioned predictions at evaluation time. Evaluation shows that Top-1 OOD accuracies improve by 0.5% on average when the proposed ICE framework is used."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The proposed approach is well-motivated. Section 4.3 provides details of why the proposed ICE method can improve over baseline zero-shot/few-shot classification approaches in certain cases.\n\n+ The paper is well written, Figure 4 provides a good overview of the proposed approach.\n\n+ The paper reports hyper-parameters and training details, which improves reproducibility.\n\n+ The paper includes adequate ablations in Figure 6, discussion the effect of the weight parameters \\lambda and \\eta.\n\n+ The paper discussion its limitations in detail."
                },
                "weaknesses": {
                    "value": "- Inference time: Compared to prior work such as  CoCa (Yu et al., 2022), the proposed ICE framework needs to encode/decode multiple captions. This would likely significantly increase inference time compared to prior work. A thorough analysis of inference time with respect to prior work is necessary.\n\n-  Performance improvement over baselines is limited. While the average performance improvement is 0.5%, in many datasets the performance improvement over the baseline is less than 0.1%, e.g., Caltech, Food, SUN, UCF in case of zero-shot cross-dataset generalization. Furthermore, in the case of few-shot domain generalization in Table 2 the best performance is obtained by the baseline CLIPood method.\n\n- In-domain performance: the proposed method is evaluated primarily on cross-dataset and domain generalization settings. However, the in-domain performance, e.g., on ImageNet, is not evaluated (\\cf Figure 4 in CoCa).\n\n-  The proposed method seems to be applicable only to image datasets. However, prior work such as CoCa is applicable even to video datasets. A discussion on the applicability of the proposed approach to video datasets would be highly appreciated.\n\n- As the approach looks only at the Top-K classes, its performance is inherently limited. Is the proposed approach helpful in case the correct class is not within the initial Top-K predictions?"
                },
                "questions": {
                    "value": "1. A detailed analysis of inference speeds with and without the use of the proposed ICE method would be helpful.\n2. The paper should discuss in more detail in which scenarios ICE provides a performance boost, as in the case of many datasets, e.g., Caltech, Food, SUN, and UCF, the performance improvement is less than 0.1%.\n3.  The paper should also discuss in more detail the applicability of the proposed approach to video data."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3752/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698729195858,
            "cdate": 1698729195858,
            "tmdate": 1699636331474,
            "mdate": 1699636331474,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1RQLuSH7XP",
                "forum": "izdFGwDgvW",
                "replyto": "lXtcoiKjd8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3752/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3752/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DXxh"
                    },
                    "comment": {
                        "value": "**Comment:** A detailed analysis of inference speeds with and without the use of the proposed ICE method would be helpful.\n\n**Response:** Thank you, we provide a supplemental analysis on inference speed in this response. This information is provided in the following table. We use a V100 GPU with batch size 32, no gradient tracking, image size 224x224, and the CoCa ViT L/14 model.\n\n| Method | Evaluation throughput (images/second) |\n| --------- | ---------|\n| Zero-shot (image embeddings only) | 65.9 |\n| ICE zero-shot | 16.8 |\n\nICE evaluation is indeed slower than zero-shot inference with only image embeddings, due to the additional time for generating captions. However, we emphasize that our method does not focus on inference speed, but rather demonstrating a simple method to improve zero-shot accuracy by incorporating captions into the prediction. It is important to weigh the gains in zero-shot accuracy against the additional inference time. Furthermore, there is extensive work in the area of fast LLM inference we can leverage to speed up caption evaluation (such as quantization methods like [smoothquant]). We can also use simple tricks such as decreasing the number of tokens in the generated caption. This is outside the scope of the current work.\n\n[smoothquant] Xiao, Guangxuan, et al. \"Smoothquant: Accurate and efficient post-training quantization for large language models.\" International Conference on Machine Learning. PMLR, 2023.\n\n**Comment:** The paper should discuss in more detail in which scenarios ICE provides a performance boost, as in the case of many datasets, e.g., Caltech, Food, SUN, and UCF, the performance improvement is less than 0.1%.\n\n**Response:** We acknowledge that ICE provides smaller gains on several datasets for few-shot classification in Table 2, especially for domain generalization. \nHowever, we first note that ICE is a zero-shot method, and thus our gains of 0.5% on average in Table 1 better reflect the capability of our approach. \nWe include supplementary few-shot training results in Table 2 to show that even when deploying a zero-shot method to a few-shot context, we are able to enjoy gains of 0.5% on average on cross-dataset generalization. \nFurthermore, on several of the datasets where we fail to see gains, we note that there was little room for improvement to begin with. \nThis can be observed in the table we display here. \nOne hypothesis that can be made is if the classification accuracy is already very high, ICE may underperform because it is more likely to incorrectly reclassify a previously correct sample than vice versa.\n\n|  |  INet | Caltech | Pets | Cars | Flowers | Food | Aircraft | SUN | DTD | EuroSAT | UCF | Mean | \n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Base accuracy (image zero-shot) | 75.1 | 97.6 | 93.8 | 92.7 | 77.3 | 87.5 | 36.6 | 73.6 | 57.2 | 58.5 | 73.4 | 74.8 | \n| ICE accuracy | 75.6 | 97.0 | 93.5 | 93.0 | 77.6 | 87.6 | 40.0 | 73.9 | 59.8 | 61.1 | 74.3 | 75.8 | \n| Both correct (\\%) | 73.4 | 96.8 | 92.5 | 92.1 | 75.4 | 86.3 | 34.1 | 72.1 | 56.6 | 57.7 | 72.0 | 73.6 | \n|  Base incorrect, ICE correct (\\%) | 2.2 | 0.2 | 1.0 | 0.9 | 2.2 | 1.3 | 5.9 | 1.8 | 3.1 | 3.4 | 2.3 | 2.2 | \n|  Image correct, ICE incorrect (\\%) | 1.7 | 0.8 | 1.2 | 0.6 | 1.9 | 1.2 | 2.5 | 1.5 | 0.5 | 0.8 | 1.4 | 1.2 | \n| Both incorrect (\\%) | 22.7 | 2.2 | 5.2 | 6.4 | 20.5 | 11.2 | 57.5 | 24.6 | 39.7 | 38.1 | 24.2 | 23.0 | \n\nWe would like to highlight that ICE achieves a 2\\% gain over few-shot baselines, and a 3\\% gain over zero-shot baselines on the aircraft dataset. This is likely because aircraft classification requires attending to details in the image (such as wingtip styles, numbers on the fuselage etc.). Traditional zero-shot CLIP uses the global image feature for classification, which may ignore these details. On the other hand, the CoCa decoder considers all spatial image tokens. Thus the caption is better at capturing local details. We can clearly see this when asking CoCa to describe an image with writing in it, since the resulting caption usually faithfully transcribes the writing. Therefore, it is intuitive that using ICE to take into account caption information would be beneficial to classification problems where discriminative visual attributes are small. We will include this additional analysis in our revision as part of the supplementary material.\n\n**Comment:** In-domain performance: the proposed method is evaluated primarily on cross-dataset and domain generalization settings. However, the in-domain performance, e.g., on ImageNet, is not evaluated (cf Figure 4 in CoCa).\n\n**Response:** Thank you for pointing this out.\nWe have conducted an in-domain evaluation of ImageNet (labeled as *INet*) in Table 1 and Table 2. \nWe will reorganize our results before publishing to clarify any confusion."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700274442407,
                "cdate": 1700274442407,
                "tmdate": 1700297884475,
                "mdate": 1700297884475,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vd0f2MlOaQ",
                "forum": "izdFGwDgvW",
                "replyto": "lXtcoiKjd8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3752/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3752/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DXxh continued"
                    },
                    "comment": {
                        "value": "**Comment:** The paper should also discuss in more detail the applicability of the proposed approach to video data.\n\n**Response:** Thank you for your suggestion, video data is indeed an important component of the original CoCa paper (Yu et. al., 2022).\nOne of the limitations of our method is that it requires additional forward passes on the base model to generate captions, creating a time bottleneck.\nThus, it would be difficult to scale ICE to video data where we must perform online predictions on a stream of data.\nHowever, we note that our current focus is to provide gains for zero-shot image classification, and we believe that our method provides a novel perspective on how to correctly pivot toward the correct class.\nFuture work can focus on reducing the computational load of our method. \nFor example, one potential direction is tuning the captioner to give more descriptive captions (e.g. color of the image, textures, etc.) rather than directly state the objects in the scene. \nIf we can generate an informative caption in the first pass, we can reduce the number of captions needed for ICE. \nAnother potential direction is detecting change in video data, and only regenerating captions when necessary.\n\nWe were not able to perform video classification experiments in the short span of the rebuttal period, but we propose the following more scalable way of extending ICE to video recognition. Captioning each frame of a video is impractical, since tokens must be generated one after another. However, we can consider captioning multiple frames at a time. The CoCa decoder takes as input a spatial token map from the image encoder. We could pool the spatial tokens from multiple video frames before sending them to the text decoder. The authors of CoCa already proposed an attention pooling mechanism to ensemble the spatial tokens from different frames; we could leverage this to generate a video caption.  We can then use ICE to aggregate the predictions based on the pooled video image embedding and the generated caption.\nWe welcome additional discussion for how our approach can be extended toward video data.\n\n**Comment:** As the approach looks only at the Top-K classes, its performance is inherently limited. Is the proposed approach helpful in case the correct class is not within the initial Top-K predictions?\n\n**Response:** Correct, the proposed method only allows for pivoting within the Top-$K$ predictions, which means that we are unable to correctly reclassify to far-away classes. \nHowever, we believe that this is a reasonable assumption to make, since we are motivated by the observation in Figure 2a that in most cases, the correct class can already be found in the Top-$K$ predictions.\nThus, in those cases, we wish to provide additional information to the model to pivot toward the correct class."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700274704862,
                "cdate": 1700274704862,
                "tmdate": 1700274704862,
                "mdate": 1700274704862,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h2HfCCPbuh",
                "forum": "izdFGwDgvW",
                "replyto": "Vd0f2MlOaQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3752/Reviewer_DXxh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3752/Reviewer_DXxh"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for their efforts.\n \nThe limited applicability of the approach is due to inference time enhancements with captions and the limited improvements in terms of percentage accuracy.\n\nI will therefore keep my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677280289,
                "cdate": 1700677280289,
                "tmdate": 1700677280289,
                "mdate": 1700677280289,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "B332kauulN",
            "forum": "izdFGwDgvW",
            "replyto": "izdFGwDgvW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3752/Reviewer_oC1d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3752/Reviewer_oC1d"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to improve the zero-shot and few-shot image classification performance for contrastive vision-language models by utilizing the caption embeddings, given an additional caption model. The proposed method is straightforward and comprehensive evaluations on 11 downstream datasets demonstrates the effectiveness of the method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well written and the organization of the paper is clear.\n- Experiments are comprehensive with multiple downstream datasets under the zero-shot and few-shot setting."
                },
                "weaknesses": {
                    "value": "- It remains unclear why the text decoder from CoCa is used. It seems that the proposed method only requires a textual description of the input image (any off-the-self high-quality caption models may work). It would be interesting to compare with the performance when using other caption models.\n\n- Compared to CLIP, the approach requires an additional caption model (as the image decoder) and is dependent on the quality of the caption, which can be hard to measure. \n\n- Empirical improvement in the few-shot setting seems marginal compared to the baselines. It may be arguable whether the additional computational cost (of forwarding passing the image to the text decoder) is desirable."
                },
                "questions": {
                    "value": "- Can authors explain why CoCa is used, instead of an arbitrary caption model? If we replace CoCa with SoTA caption model, will the performance be significantly improved? \n\n- The performance gain in the few-shot setting seems marginal (Table 2). Does this mean that in the few-shot adaptation setting, we may just need a few images, instead of using an additional decoder?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3752/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826665248,
            "cdate": 1698826665248,
            "tmdate": 1699636331391,
            "mdate": 1699636331391,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ikEILE0e0w",
                "forum": "izdFGwDgvW",
                "replyto": "B332kauulN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3752/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3752/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oC1d"
                    },
                    "comment": {
                        "value": "**Comment:** Can authors explain why CoCa is used, instead of an arbitrary caption model? If we replace CoCa with SoTA caption model, will the performance be significantly improved?\n\n**Response:** Good question. We would first like to reiterate our rationale for choosing CoCa as our base model; then we will present supplementary results with the SoTA BLIP-2 Vision-language (VL) model. \n\nIn the context of ICE, CoCa has the following nice properties as compared to other VL models:\n\n * CoCa is optimized to perform zero-shot classification (same as image-to-text retrieval), while many SoTA VL models such as BLIP-2 and Flamingo place more emphasis on generating human-like responses to queries. Consequently, SoTA VL models do not output a caption that is fine-grained enough for certain classification tasks, at least not without additional prompting. \n\n * CoCa contains a text encoder, not just a text decoder. This allows us to efficiently perform zero-shot classification with just one model.  \n\nWe present results on ImageNet and 10 other datasets using BLIP-2 captions instead of CoCa captions in the following table. \n\n|  |  INet | Caltech | Pets | Cars | Flowers | Food | Aircraft | SUN | DTD | EuroSAT | UCF | Mean | \n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Zero-shot (image) | 75.1 | 97.6 | 93.8 | 92.7 | 77.3 | 87.5 | 36.6 | 73.6 | 57.2 | 58.5 | 73.4 | 74.8 | \n|     + CoCa ICE | 75.6 | 97.0 | 93.5 | 93.0 | 77.6 | 87.6 | 40.0 | 73.9 | 59.8 | 61.1 | 74.3 | 75.8| \n|     + BLIP2 ICE | 75.3 | 97.2 | 93.5 | 92.7 | 77.8 | 87.8 | 36.6 | 74.1 | 59.8 | 60.9 | 75.4 | 75.6 | \n\nWe use the largest available BLIP-2 model, which contains a ViT-g image encoder with a FlanT5-XXL LLM. We observe a similar accuracy gain when applying ICE with these BLIP-2 captions. BLIP-2 captions achieve slightly lower numerical results than CoCa captions on average, despite using a much larger model. We think this is because BLIP-2 sacrifices fine-grained information in favor of a more descriptive caption. For example, for one image in the Pets dataset, the CoCa caption is: \"a photo of a abyssinian cat'', while the BLIP-2 caption reads: \"a photo of a cat standing on a table''. The CoCa caption provides the correct classification for the pet in the image, while the BLIP-2 caption describes the scene in general terms without providing the information necessary for pets classification. These results illustrate that simply using a better captioner does not necessarily lead to better ZS accuracy under our ICE framework. However, we believe that using LLM prompting techniques to elicit more fine-grained information from large captioning models and incorporating that information into ICE is an interesting direction for future work. It would be very helpful for us to hear your thoughts on our analysis here.\n\n[blip2] Li, Junnan, et al. \"Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\" arXiv preprint arXiv:2301.12597 (2023).\n\n\n\n**Comment:** The performance gain in the few-shot setting seems marginal (Table 2). Does this mean that in the few-shot adaptation setting, we may just need a few images, instead of using an additional decoder?\n\n**Response:** Thank you for pointing this out. From Table 2, we observe that the Domain Generalization (DG) targets have the lowest average gain; the Cross-Dataset (CD) Evaluation targets still enjoy 0.5% performance boosts on average. \nHowever, we still see consistent improvements on hard DG datasets such as ImageNet Adversarial (INet-A) and ImageNet Rendition (INet-R).\nWe note that similar trends can be found in the zero-shot data in Table 1, and thus believe that it is more important to evaluate the importance of using an additional decoder based on the properties of the target dataset (as elaborated in Section 3.4 Caption Properties).\nIn addition, we note that ICE is designed to be a zero-shot method, making Table 1 results more relevant to our target domain, but we think it is promising that ICE can provide benefits even in a few-shot context."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700272603780,
                "cdate": 1700272603780,
                "tmdate": 1700272640010,
                "mdate": 1700272640010,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "smgUbfTXsH",
                "forum": "izdFGwDgvW",
                "replyto": "B332kauulN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3752/Reviewer_oC1d"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3752/Reviewer_oC1d"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the detailed response"
                    },
                    "comment": {
                        "value": "Thank you for providing detailed responses and the additional experiments with BLIP-2.\n\nIt is interesting to see that a better captioning model such as BLIP-2 does not necessarily lead to superior zero-shot classification performance. It remains a bit unclear to me if is it because the fine-grained information from large captioning models is not accurate or the language encoder cannot properly handle such information. This brings some further questions such as what properties are desirable from image captions for zero-shot visual classification and how to better exploit a general caption model beyond utilizing a specific pre-trained model such as CoCa? For example, does BLIP-2 really do not understand what an abyssinian cat is? I appreciate the authors' efforts in showing this addition results and it is a good starting point for answering one of the core scientific questions of this work. \n\n\nI understand that the motivation for using CoCa is that it allows authors to \"efficiently perform zero-shot classification with just one model\". However, this framework can be interpreted as both an advantage (simplicity) and disadvantage (e.g., post-hoc adjustment, strong reliance on CoCA and therefore would be difficult to generalize to different domains or general visual reasoning tasks). \n\nAs for efficiency, the proposed method uses three components: image encoder -> text decoder -> text encoder (as shown in Figure 3). Alternatively, if we use simple vision-language models such as LLaVA [1], we still have the same number of components: image encoder (in LLAVA) -> text decoder (in LLAVA) -> text encoder (in CLIP). Therefore, it might be hard to argue which one is more efficient. \n\nGiven the above considerations, other reviewers' comments, and the empirical performance, my judgement for the work remains borderline (either marginally below or above the acceptance threshold). \n\n[1] Liu et al., Visual Instruction Tuning, NeurIPS 2023"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684008179,
                "cdate": 1700684008179,
                "tmdate": 1700684158847,
                "mdate": 1700684158847,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QpmPrlWQFh",
            "forum": "izdFGwDgvW",
            "replyto": "izdFGwDgvW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3752/Reviewer_F7uJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3752/Reviewer_F7uJ"
            ],
            "content": {
                "summary": {
                    "value": "The authors observed that even when image classification models make a mistake, the correct answer frequently appears within the top five guesses. Inspired to nudge the model towards the right choice among these top contenders, they introduced a novel approach for zero-shot image classification by combining image and text caption embeddings. This technique enhances the performance of the SOTA methods by an average of 0.5%, and by as much as 3%. They have tested their method on cross-dataset generalization datasets and domain generalization datasets. They also performed simple qualitative error analysis."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Tested their method using a wide range of image recognition datasets (11 cross-dataset generalization datasets and 4 domain generalization datasets)\n- Ablation studies on the parameters of ICE is performed."
                },
                "weaknesses": {
                    "value": "- The performance gains are slight; examining Table 1 and Table 2 shows that ICE only slightly increases performance over the baseline by 0.1 to 0.4 on variations of ImageNet. Moreover, for certain datasets like INet-Sketch, the baseline actually performs better.\n- Image captioning poses a greater challenge compared to image classification because, particularly for complex images (including some found in ImageNet) a single image may not correspond to just one correct label. Consider, for instance, the top-right picture in Figure 5 labeled as \"strawberry.\" Labeling this image solely as a strawberry seems inaccurate. More broadly, it's impractical to use images featuring multiple objects for a single-label classification. Image captioning is the suitable approach for these types of images. Pushing for precise classification on such images may simply be tailoring models to fit benchmark datasets without truly enhancing their comprehension of the images. Therefore, I don't see how their approach can be applied for complex images. I agree that this approach works for simple images, but I'd assume the existing image models already work well for such cases, as evidenced by Table 1 and 2.\n- In Section 4.3, \"Understanding Why ICE Provides Improvements,\" the explanation is merely qualitative, based on just four examples. It's difficult to gauge the representativeness of these cases. A deeper quantitative analysis\u2014for example, determining how common these cases are in standard image datasets\u2014would be necessary to fully understand why and how ICE is effective."
                },
                "questions": {
                    "value": "Related to the last point in the Weaknesses section, how representative are four scenarios discussed in Section 4.3 within standard image datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3752/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699237591328,
            "cdate": 1699237591328,
            "tmdate": 1699636331329,
            "mdate": 1699636331329,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GCyHm8Leme",
                "forum": "izdFGwDgvW",
                "replyto": "QpmPrlWQFh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3752/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3752/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer F7uJ"
                    },
                    "comment": {
                        "value": "**Comment:** Image captioning poses a greater challenge ...\n\n**Response:** Thank you for providing valuable insight into this problem. \nWe agree that image captioning poses a greater challenge than image classification, and captions can describe several objects in the scene that may conflict with the single-label classification objective. \nWe first note that if multiple objects exist in the image which correspond to different labels in the dataset, then it is likely a poor problem formulation for single-label image classification to begin with. \nThus, dealing with these conflicting scenarios would be out of the scope of our method. \nSecond, our method works only to pivot toward the correct class in the top-$K$ predicted classes. \nThis means that even if the caption provides descriptions about unrelated objects, we can only select a class that was already predicted with high probability by the original image encoder. \nThe combination of our image and caption embeddings ensures that we will not stray too far from the initial prediction for image classification which provides a useful constraint.\nFinally, Tables 1 and 2 show that our method can give good zero-shot improvements to existing state-of-the-art models.\n\n**Comment:** In Section 4.3, \"Understanding Why ICE Provides Improvements,\" the explanation is merely qualitative, based on just four examples. It's difficult to gauge the representativeness of these cases. A deeper quantitative analysis\u2014for example, determining how common these cases are in standard image datasets\u2014would be necessary to fully understand why and how ICE is effective.\n\n**Response:** Thank you for suggesting a quantitative analysis of the four situations presented in Section 4.3 and Fig. 5; we agree that this enhances our discussion. Accordingly, we provide the quantity of images in each category as a percentage of total test images in the tables below, for each dataset. In all the datasets, most images are either correctly predicted by both methods or incorrectly predicted by both methods. This is expected, since every dataset contains a large amount of easy images and a large amount of impossible images given a constant model capacity. However, we do observe that in all datasets except Caltech and Pets, the percentage of images where ICE successfully reclassifies an initially incorrect prediction exceeds the percentage of images where ICE incorrectly reclassifies an initially correct prediction. Furthermore, the percentage of failed re-classifications is small when compared to the percentage of successful re-classifications on datasets such as DTD (0.5\\% compared to 3.1\\%) and EuroSAT (0.8\\% compared to 3.4\\%). Finally, we note that images incorrectly classified by ICE may be images that are hard to classify without additional context or ambiguous images.\nFuture work can help identify which samples to apply ICE to decrease the number of correct-to-incorrect ICE re-classifications, and we welcome further discussion about alternative directions.\nWe will include this additional analysis in our revision as part of the supplementary material.\n\n|  |  INet | Caltech | Pets | Cars | Flowers | Food | Aircraft | SUN | DTD | EuroSAT | UCF | Mean | \n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Base accuracy (image zero-shot) | 75.1 | 97.6 | 93.8 | 92.7 | 77.3 | 87.5 | 36.6 | 73.6 | 57.2 | 58.5 | 73.4 | 74.8 | \n| ICE accuracy | 75.6 | 97.0 | 93.5 | 93.0 | 77.6 | 87.6 | 40.0 | 73.9 | 59.8 | 61.1 | 74.3 | 75.8 | \n| Both correct (\\%) | 73.4 | 96.8 | 92.5 | 92.1 | 75.4 | 86.3 | 34.1 | 72.1 | 56.6 | 57.7 | 72.0 | 73.6 | \n|  Base incorrect, ICE correct (\\%) | 2.2 | 0.2 | 1.0 | 0.9 | 2.2 | 1.3 | 5.9 | 1.8 | 3.1 | 3.4 | 2.3 | 2.2 | \n|  Image correct, ICE incorrect (\\%) | 1.7 | 0.8 | 1.2 | 0.6 | 1.9 | 1.2 | 2.5 | 1.5 | 0.5 | 0.8 | 1.4 | 1.2 | \n| Both incorrect (\\%) | 22.7 | 2.2 | 5.2 | 6.4 | 20.5 | 11.2 | 57.5 | 24.6 | 39.7 | 38.1 | 24.2 | 23.0 | \n\n\n|  | INet-V2 | Sketch | INet-A | INet-R | DG Mean | \n| --- | --- | --- | --- | --- | --- | \n| Base accuracy (image zero-shot) | 67.5 | 63.5 | 53.8 | 87.0 | 67.9 |\n| ICE accuracy | 67.7 | 63.8 | 54.4 | 87.4 | 68.3 | \n| Both correct (\\%) | 65.5 | 61.6 | 51.8 | 86.3 | 66.3 | \n|  Base incorrect, ICE correct (\\%) | 2.2 | 2.2 | 2.6 | 1.1 | 2.0 | \n|  Image correct, ICE incorrect (\\%) | 1.9 | 1.9 | 2.0 | 0.7 | 1.7 | \n| Both incorrect (\\%) | 30.4 | 34.3 | 43.6 | 11.9 | 30.0 |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700272074366,
                "cdate": 1700272074366,
                "tmdate": 1700297807165,
                "mdate": 1700297807165,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "I8EWx9XheG",
                "forum": "izdFGwDgvW",
                "replyto": "GCyHm8Leme",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3752/Reviewer_F7uJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3752/Reviewer_F7uJ"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for providing the quantitative analysis for the third point. And I appreciate your explanation regarding the second point. While I understand that this method deals with single-object image settings, since the benefits of using this method in practice seems limited (as I clarified in my first point in the weaknesses section), I maintain my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3752/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727845987,
                "cdate": 1700727845987,
                "tmdate": 1700727845987,
                "mdate": 1700727845987,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]