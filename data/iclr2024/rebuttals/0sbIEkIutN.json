[
    {
        "title": "From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers"
    },
    {
        "review": {
            "id": "74zOzM7DCq",
            "forum": "0sbIEkIutN",
            "replyto": "0sbIEkIutN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5813/Reviewer_hH7Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5813/Reviewer_hH7Y"
            ],
            "content": {
                "summary": {
                    "value": "The authors achieve perfect length generalization on algorithmic tasks (x+1, x+y, and N x 1). The models are trained with as little as a sequence length of 6, and they generalize to sequences with a length of 5. They achieve this by learning an attention pattern on short sequences which interpolates and extrapolating the attention pattern to long sequences, in the form of an additive bias. This bias is somewhat learned, however it has a very strong structural prior in the form of lines in the attention matrix."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Perfect length generalization in addition\n- Clear presentation\n- The method is based on the analysis of the attention patterns\n- The paper has a clear scope\n- The method clearly shows that the attention pattern is the bottleneck in extrapolating to longer sequences"
                },
                "weaknesses": {
                    "value": "- The inductive bias is very strong: it biases the attention matrix to be diagonal lines. The method might not work in more general cases in 2 different ways: it might not help with generalization, but it might also hinder learning (I strongly suspect that this would be the case with language-related tasks). Thus, the limitations of the method are not clear. I think this is the biggest weakness of the paper.\n- The authors already compare to lot of baselines. However, these relative positional encodings are known to have extrapolation issues. The authors also cite references for this. However, the authors do not cite some relevant work that has been shown to help with extrapolating to longer sequences [1, 2], for example by Transformer-XL style attention [3]. Other work already showed extrapolation on some algorithmic tasks, for example, ListOps and modular arithmetic [4]. Please cite it.\n\n1. Csordas et al, 2022: The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers\n2. Ruoss et al, 2023: Randomized Positional Encodings Boost Length Generalization of Transformers\n3. Dai et al, 2019: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\n4. Csordas et al, 2021: The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization"
                },
                "questions": {
                    "value": "- Adding a task where the attention pattern should not be a straight line would be interesting, to see the limitations of the method. One interesting task could be ListOps (for an analysis of attention patterns, see [4] from the \"Weaknesses\" chapter)\n- On page 6, when explaining how the attention matrices are extracted, it is written \"are parameter matrices in the last decoder layer\". Does this mean that only the last decoder layer has the attention bias? Or do all of them have it, but set to the same as the last layer?\n- The absolute PEs are usually not injected in the attention head directly, but to the residual stream immediately after the token embeddings. Given this, in the first eq. on P6, what are the additive p_i and p_j components? Do you do a custom method, where you inject the PE directly into the attention at each layer?\n- How do you handle the cross-attention? Does that have this bias as well?\n- Have you tried a less aggressive bias, where you don't set the elements to -inf, but to some finite negative number, such that the network has the chance to overcome the bias if it is good for the task? Do you think it would work in this case?\n\nI'm willing to raise my score if my questions about the positioning of the biases are cleared, and if the authors could run their method on any task that does not require strictly monotonic attention (e.g ListOps), regardless of the outcome of the experiment. Just to make it clear what are the limitations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698425212102,
            "cdate": 1698425212102,
            "tmdate": 1699636613282,
            "mdate": 1699636613282,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IpPBQzoXaC",
                "forum": "0sbIEkIutN",
                "replyto": "74zOzM7DCq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5813/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful review and valuable feedback. We have revised the paper based on your suggestions. Before we address your specific questions, here is a summary of the improvements we made to the paper:\n\n1. We simplified the extensive notations used in section 5 and added more visuals to explain the concept more clearly.\n2. In section 8, we added a discussion on the scope and limitations of our work to clarify its applicability.\n3. We conducted additional experiments with ListOps and discussed the results in section 8 and appendix C.\n4. We added citations to the works in Systematic Generalization. Thanks for pointing them out!\n5. We improved section 4 by showing the attention patterns in the main text.\n\n**Adding a task where the attention pattern should not be a straight line would be interesting**\n\nThank you for such a great suggestion. ListOps is an excellent test case for learning more complex structures. We conducted additional experiments with ListOps and discussed the results in section 8 and appendix C. Unfortunately, with the model scales we experimented with and the amount of resources we had, we could only solve a simplified single-operator, single-depth ListOps task via ABS. Learning multi-depth ListOps may require scaling up the model dramatically, as the heuristic in the NDR paper [2] indicates. Our goal is certainly to study the learning of more complex tasks and we plan them as future research.                                    \n\n**Does this mean that only the last decoder layer has the attention bias? Or do all of them have it, but set to the same as the last layer?**\n\nWe apologize for the confusion. For the tasks presented in this work, the bias matrices are obtained from the last decoder layer and applied to all layers during re-train, on a per-head basis. More flexible configurations such as per-layer bias could work better for more complex tasks, which is what we currently are exploring. We added clarification on this issue at the end of section 5.\n\n**Given this, in the first eq. on P6, what are the additive p_i and p_j components? Do you do a custom method, where you inject the PE directly into the attention at each layer?**\n\nAgain we apologize for the confusion.  We apply PE at the start of the process as the original Transformer paper does [1], rather than injecting it at each layer. The notation we used is misleading and we have revised it in section 5. Thanks for pointing it out.\n\n**How do you handle the cross-attention? Does that have this bias as well?**\n\nWe used ABC on cross-attention as well. It works the same way as self-attention, and Figures 4 and 5 are both cross-attention patterns. \n\n**Have you tried a less aggressive bias, where you don't set the elements to -inf, but to some finite negative number, such that the network has the chance to overcome the bias if it is good for the task? Do you think it would work in this case?**\n\nThis is an excellent idea and we are in the process of trying it out on NLP tasks which we think is a more suitable situation. We believe that in NLP, ABC could be possibly used to capture some global or background dependency patterns. However, the patterns are likely to be less rigid and clear than those in arithmetic tasks. A \"soft\" ABC, i.e., something like\n$$attn = (1-\\lambda) \\cdot attn_0+\\lambda \\cdot bias$$\nwhere bias acts as a global context while $attn_0$ captures the local dependency, might work better. This is our ongoing work.  Thanks for your suggestion!\n\n**If the authors could run their method on any task that does not require strictly monotonic attention**\n\nWe have run experiments on ListOps which we discussed earlier.\n\nThanks again for the constructive feedback that helped make this paper better! We hope our responses addressed your concerns.\n\n**References:**\n\n[1] Attention is All You Need\n\n[2] The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629958879,
                "cdate": 1700629958879,
                "tmdate": 1700631455263,
                "mdate": 1700631455263,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n831uuTyss",
                "forum": "0sbIEkIutN",
                "replyto": "74zOzM7DCq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5813/Reviewer_hH7Y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5813/Reviewer_hH7Y"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their updates and clarifications. Most of my questions have been answered. \n\nI find the outcome of the ListOps experiments very surprising. The authors use a simplified version of ListOps, where there is only a single operation applied to a list of numbers. My original intent was the opposite: to run it on recursively embedded operations, to see if the model can allow non-monotonic attention patterns. The surprising part of the current results is that for this simplified version of ListOps, monotonic memory access patterns should be enough, so I think the method should work (and my prediction was that it would fail on the original, recursive version). Given these negative outcomes, I think the authors should emphasize the limitations of the paper more so that it can be clear to the reader that this is not a ready-to-go solution for length generalization.\n\nThe other remaining weakness is not compared with relative positional encodings.\n\nOverall, I think the paper contributes an interesting new idea. I consider it worth publishing even if it is not clear yet if it is widely applicable to other tasks. It could serve as a base for interesting future research."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658707093,
                "cdate": 1700658707093,
                "tmdate": 1700664924184,
                "mdate": 1700664924184,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BbilducxOL",
            "forum": "0sbIEkIutN",
            "replyto": "0sbIEkIutN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5813/Reviewer_25Z2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5813/Reviewer_25Z2"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an analysis of length generalization in sequence to sequence transformers for four arithmetic tasks: successor (adding 1), parity integration, addition, and multiplication by one digit operands.\n\nIn a first set of experiments, they observe that on these four operations, length generalization can be achieved if the attention window for the model is constrained to focus on a small number of tokens, either directly, by forcing the window to be of width one, or indirectly, by using small modular integers for positional encoding. On these tasks, such constrained attention models can effectively replace positional encodings. \n\nFinally, the authors propose a technique for creating such attention constraints for a specific task. To this effect, they average the biases in attention weights when a model, trained to generalize to sequence of the same length, performs the task. Then, they retrain on the specific task, using these biases to constrain attention. Experiment indicate that this allows the model to generalize to longer sequences for three of the four tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper focuses on an important limitation of transformers: length generalization. The analysis of the role of the attention window is compelling, and the experimental results from appendix A.5 are quite convincing. \n\nThe calibration technique proposed in section 5 is interesting, and experiments demonstrate its worth on the three tasks considered. \n\nOverall, the paper provides is a solid analysis of length generalization, and interesting ideas on how to solve it."
                },
                "weaknesses": {
                    "value": "* The important results from section 4 are hard to understand if one only reads the main paper. In particular, the claim that constrained attention solves parity is only supported in the appendix. Also the logic behind section 5 is hard to comprehend without reading parts of section A.4 (the precise description of windowed attention, and the cyclic encoding). The paper would be made much stronger by moving a significant part of appendices A.4 and A.5 into section 4. This can probably be done at the expense of figure 1 (not very informative), and parts of section 5. \n\n* The claim that constrained attention, and ABC, offers a **complete** solution to length generalization on problems of arithmetic is exaggerated. All the tasks considered in the paper share a common trait: their solution can be computed by looking at just a few consecutive digits of the problem and the currently computed solution. This would break for such a basic arithmetic task as summing $n$ k-digit integers, and generalizing on larger $n$. As for multiplication, $N\\times 1$ multiplication is closer to $10$ unary operation than a binary operation. $N\\times N$ multiplication would have the same non local behavior.\n\n* Given the comments in section 7, about the similarity between ABC and RPE, it is regrettable that the experiments in section 4 do not feature RPE (except addition, which corresponds to a different architecture). It would be interesting, in particular, to see how RPE perform on the successor and parity task."
                },
                "questions": {
                    "value": "* You use 1/6 layer transformers, what is the benefit of a shallow encoder, compared to a (smaller) 6/1 encoder, or 4/4 model?\n* All your vanilla models use the cosine positional embeddings. You observe that it sometimes interferes with the sliding window attention, could you try learnable PE? \n* Could you try RPE on the successor and parity experiments? Your conclusions suggest that it would help generalize.\n* In the experiments from appendix A.5 (table 3), you try cyclic PE on top of a w=1 windowed attention, what happens if you use it without windowed attention? (it is another form of RPE)\n* Have you tried $N\\times N$ multiplication (for smaller values of $N$)? of $N\\times k$ for $k$ larger than 3 (so that the model cannot memorize all cases as so many unary operations)?\n* The Jelassi results in table 3 and figure 1 correspond to a different architecture (encoder-only, with shared layers), it would be better to rerun them, or at least to clarify this."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698687782962,
            "cdate": 1698687782962,
            "tmdate": 1699636613172,
            "mdate": 1699636613172,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gG65VDO5lE",
                "forum": "0sbIEkIutN",
                "replyto": "BbilducxOL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5813/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful review and valuable feedback. We have revised the paper based on your suggestions. Before we address your specific questions, the following is a summary of the improvements we made to the paper:\n\n1. We simplified the extensive notations used in section 5 and added more visuals to explain the concept more clearly.\n\n2. In section 8, we added a discussion on the scope and limitations of our work to clarify its applicability.\n\n3. We conducted additional experiments with ListOps and discussed the results in Section 8 and Appendix C.\n\n4. We added citations to the works in Systematic Generalization.\n\n5. We improved section 4 by showing the attention patterns in the main text.\n\n**The important results from section 4 are hard to understand if one only reads the main paper.**\n\nWe agree that section 4 was a bit crammed and some material was left in Appendix A. We have reformatted sections 4 and 5 to be more clear and easy to understand. \n\n**The claim that constrained attention, and ABC, offers a complete ...**\n\nWe apologize for the confusion. By \"complete length generalization\" we didn't mean that we solved all length generalization problems. Instead, we meant that our solution solves some specific arithmetic algorithm learning tasks, allowing the model to generalize (completely) to long lengths on those tasks. Ours was the first solution to achieve such results. We clarified the notion in the paper and added a discussion on the scope and limitations of our work in section 8.\n\n**You use 1/6 layer transformers, what is the benefit of a shallow encoder, compared to a (smaller) 6/1 encoder, or 4/4 model?**\n\nWe have conducted tests with 6/6, 3/6, 3/3, and 1/3 transformers. We see little to no difference regarding both interpolation and extrapolation performances. We suspect that this is due to the simplicity of our tasks.\n\n**You observe that it sometimes interferes with the sliding window attention, could you try learnable PE?**\n\nYour suggestion to examine learnable positional encodings (PE) is well-taken, but we did not try learnable PE because studies have shown that learnable PE does not achieve good results when doing length extrapolation. More specifically, as [1]  mentioned, \"We also experimented with using learned positional embeddings instead, and found that the two versions produced nearly identical results\". The learnable PE referenced here cannot enable extrapolation because all the encodings are learned on the in-training length. Our decision not to include learnable PEs in our experiments was also influenced by hardware limitations.\n\n**Could you try RPE on the successor and parity experiments? Your conclusions suggest that it would help generalize.**\n\nThe potential of relative positional encodings (RPE) to improve length generalization is a promising direction for future research. While we have not included RPE in our current experiments, we have contrasted ABS with the top-performing variant of RPE as per [2] in the appendix, showing the superiority of ABS in Parity.\n\n**In the experiments from Appendix A.5 (table 3), you try cyclic PE on top of a w=1 windowed attention, what happens if you use it without windowed attention?**\n\nWe also think that cyclic PE has some potential to act as a sort of RPE. However, when we tested cyclic position indexing (CPI) without attention windows, it did not yield satisfactory results and the model did not even interpolate. CPI appears effective only on very short windows.\n\n**Have you tried $n\u00d7n$ multiplication (for smaller values of $n$)? of $n\u00d7k$ for $k$ larger than 3 (so that the model cannot memorize all cases as so many unary operations)?**\n \nWe have attempted $n\u00d7n$ multiplication and $n\u00d7k$ multiplication with $k$ larger than 3. Unfortunately, these experiments did not yield successful interpolation, rendering ABC unusable in this context. We have addressed this limitation in the revised paper in section 8.\n\n**The Jelassi results in Table 3 and Figure 1 correspond to a different architecture (encoder-only, with shared layers), it would be better to rerun them, or at least to clarify this.**\n\nWe apologize for the misleading caption, this has been revised.\n\nThank you for your insightful response.\n\n**References:**\n\n[1] Attention is All You Need\n\n[2] Neural Networks and the Chomsky Hierarchy"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630414970,
                "cdate": 1700630414970,
                "tmdate": 1700631187810,
                "mdate": 1700631187810,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F1E2HfU9pJ",
                "forum": "0sbIEkIutN",
                "replyto": "gG65VDO5lE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5813/Reviewer_25Z2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5813/Reviewer_25Z2"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their thoughtful responses. I will take them into account in the upcoming discussion with other reviewers and area chairs."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639895074,
                "cdate": 1700639895074,
                "tmdate": 1700639895074,
                "mdate": 1700639895074,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "D0RQbDA5m0",
            "forum": "0sbIEkIutN",
            "replyto": "0sbIEkIutN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5813/Reviewer_usKR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5813/Reviewer_usKR"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies length generalization of Transformer models on algorithmic tasks. It shows that the vanilla Transformer correctly learns the attention patterns up to the training length and fails beyond that. To address this problem, the authors propose Attention Bias Calibration (ABC), which introduce an additional stage to obtain proper attention biases from training data. Experiments show the effectiveness of the proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The experiment results are good on the tasks considered in this paper.\n- The authors provide some interesting empirical study and visualization of the attention."
                },
                "weaknesses": {
                    "value": "- The presentation of this paper looks good in the first two sections, but the presentation quality drops in the subsequent parts. In particular, section 5 is very hard to read due to the intensive definition and sometimes abuse of notations albeit the idea is very simple. The definitions of some notations are far from where it is used. For example, the authors should have mentioned how $threshold$ is calculated when it appears for the first time \u2013 In the paper, it is not defined until section 6, which can confuse the readers.\n- The method implicitly encodes sparse attention patterns, since the attention bias $\\tilde A_{i,j}$ can be set to $-\\infty$ for a large number of tokens. While it can be helpful for simple tasks, I\u2019m not convinced of its effectiveness in more general settings (e.g., for natural language). I'm worried ABC can only be effective for a small number of tasks. The tasks in this paper also seemed to be oversimplified. I'm wondering if ABC can still work well for multi-digit multiplication rather than the simple $N\\times 1$.\n- ABC is only compared against weak baselines in the experiments. As discussed in the paper, existing papers have shown Alibi and RoPE are suboptimal on algorithmic tasks. The authors should compare ABC against stronger baselines, e.g., Randomized Positional Encodings, RoPE with Position Interpolation, etc. \n- Typos.\n  - Footnote 4, \u201c16 etc\u201d $\\to$ \u201c16, etc\u201d.\n  - Footnote 6, \u201ctasks\u201d $\\to$ \u201ctasks.\u201d."
                },
                "questions": {
                    "value": "- Can ABC still work well for more complex task, e.g., multi-digit multiplication, or even natural language modeling?\n\n- Can you compare ABC with more advanced baselines?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5813/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5813/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5813/Reviewer_usKR"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775143044,
            "cdate": 1698775143044,
            "tmdate": 1699636613058,
            "mdate": 1699636613058,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DFRvwJirym",
                "forum": "0sbIEkIutN",
                "replyto": "D0RQbDA5m0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5813/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful review and valuable feedback. We have revised the paper and the following is a summary of the main changes:\n\n1. We simplified the extensive notations used in section 5 and added more visuals to explain the concept more clearly.\n2. In section 8, we added a discussion on the scope and limitations of our work to clarify its applicability.\n3. We conducted additional experiments with ListOps and discussed the results in Section 8 and Appendix C.\n4. We added citations to the works in Systematic Generalization. \n5. We improved section 4 by showing the attention patterns in the main text.\n\nNow addressing your questions:\n\n**The presentation of this paper looks good in the first two sections, but the presentation quality drops in the subsequent parts.**\n\nWe have revised Section 5, reducing the complexity of notations and augmenting the textual explanations with additional visuals to facilitate a clearer understanding of the concepts presented.\n\n**While it can be helpful for simple tasks, I\u2019m not convinced of its effectiveness in more general settings (e.g., for natural language)**\n\nWe added a discussion on the scope and limitations of our work in section 8. The current sparse attention bias works for the arithmetic tasks we study. For more complex tasks, it is likely to require a scaling up of the model, as the heuristic in the NDR paper [3] indicates, and much more expressible ways to implement the bias. Although exploring ABC in more generalized settings is beyond the scope of this current paper, we intend to address this in our subsequent research. Sparse global attention has been shown to have a great effect on the transformer model's performance. More specifically, in [1], they demonstrated that the transformer model can still hold their abilities even when the entire attention mechanism is replaced with a static attention matrix generated from averages of the attention matrix across a corpus of data, only suffering from an 8% decrease in accuracy in some cases. These findings, alongside ABC\u2019s conceptual similarity with Relative Positional Encodings (RPE), provide a promising foundation for future investigations into ABC's utility in complex NLP tasks. We are currently in the process of exploring more tasks in NLP.\n\n**Can you compare ABC with more advanced baselines?**\n\nThe suggestion to benchmark ABC against more sophisticated baselines is well-taken. While ABC has shown remarkable performance in the experiments conducted, outperforming all relevant baselines in [2] and [4] and achieving exceptional accuracies of 99+ percent over extended sequence lengths (over 8 times the training length), we acknowledge the value of broader comparisons. The benchmarks that we compare against include results from [2] and [4], which are very recent works and include extensive experiments. For example, [4] considered five major positional encodings: none, classical sin/cos, RoPE, ALiBi, and the relative positional encodings of Transformer-XL, and report the best-performing configurations.  We believe they are strong SOTA results. Yet the transformer did poorly on those tasks. This indicates that the current state of transformers on arithmetic learning is rather weak. And ABC enables the model to generalize on certain tasks which we believe is a big step forward.  \n\n**References:**\n\n[1] How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers\n\n[2] Randomized Positional Encodings Boost Length Generalization of Transformers\n\n[3] The Neural Data Router: Adaptive ControlFlow in Transformers Improves Systematic Generalization\n\n[4] Neural Networks and the Chomsky Hierarchy"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630602876,
                "cdate": 1700630602876,
                "tmdate": 1700630602876,
                "mdate": 1700630602876,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sgB1mAcQ5a",
                "forum": "0sbIEkIutN",
                "replyto": "DFRvwJirym",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5813/Reviewer_usKR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5813/Reviewer_usKR"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' responses and clarifications. However, several concerns remain:\n\n- The major concern is that ABC only solves very simple tasks that are carefully chosen. It even fail on moderately complex tasks like ListOps. Without experiments on larger models, it remain doubtful to me whether the failure is because of model size or fundamental limitations of ABC. Given current scope of the experiments, I don't think ABC is a big step forward in arithmetic Transformers.\n\n- On the baseline choices, I understand that the authors use the positional encoding methods in [1] as baselines. However, all these methods appeared on or before 2021. I reiterate that the authors should also compare ABC with more recent methods like Randomized Positional Encodings [1], RoPE with Position Interpolation [2], etc.\n\n[1] Randomized Positional Encodings Boost Length Generalization of Transformers\n\n[2] Extending Context Window of Large Language Models via Positional Interpolation\n\nGiven these reasons, I maintain my original rating."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709817153,
                "cdate": 1700709817153,
                "tmdate": 1700709817153,
                "mdate": 1700709817153,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6cRq0TKapF",
            "forum": "0sbIEkIutN",
            "replyto": "0sbIEkIutN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5813/Reviewer_69hA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5813/Reviewer_69hA"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores how transformer models can learn arithmetic algorithms and achieve optimal length generalization. It introduces attention bias calibration (ABC) to guide the model to focus on the right tokens. Using ABC, the Transformer model can perfectly generalize on certain arithmetic tasks. Finally, the paper also makes a connection between ABC and relative position encoding (RPE)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The raised research problem is well-motivated and interesting. The authors show that additional training time is negligible in the paper."
                },
                "weaknesses": {
                    "value": "1. The writing should be improved in terms of clarity. Several terms and concepts are introduced without adequate definitions or elaboration. Specifically, the terms \"complete length generalization\" and \"the organic Transformer\" seem ambiguous.\n\n2. ABC constructs attention biases based on task-specific data. It's not clear if it can be applied in multi-task learning settings or even serve as a building block for general-purpose language models.\n\n3. More experiments should be conducted. While the results are promising on the four tasks detailed in Sec. 3.1, more extensive experiments are anticipated. It might be worthwhile for the authors to explore tasks in [1,2].  \n\n4. Factual errors in the discussion on related works. Sec. 2 mentions that [3] is a follow-up of Alibi. This can\u2019t be true because [3] appears much earlier than Alibi. Sec. 7 mentions that \u201cInterestingly, such clipping is also used in RoPE\u201d. I believe this claim is incorrect and can be misleading. \n\n[1] Neural networks and the chomsky hierarchy.\n\n[2] Randomized Positional Encodings Boost Length Generalization of Transformers.\n\n[3] Longformer: The long-document transformer."
                },
                "questions": {
                    "value": "Please see weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788273591,
            "cdate": 1698788273591,
            "tmdate": 1699636612953,
            "mdate": 1699636612953,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SGR23kUVMm",
                "forum": "0sbIEkIutN",
                "replyto": "6cRq0TKapF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5813/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback! We have uploaded a revised version of our paper and will address your questions below:\n\n**Writing Clarity:**\nWe have made changes to improve readability and make the line of reasoning more clear. In particular, we simplified the extensive notations used in section 5 and added more visuals to explain the concept more clearly.\n\n**ABC's Generality and Use Cases:**\nIn section 8, we added discussion on the scope and limitations of our work to clarify its applicability. Ours is an initial attempt to study the roles of attention in transformer\u2019s learning. To this end, we need to find tasks on which transformers can generalize. Surprisingly, even for tasks as simple as addition, no previous transformer model ever succeeded. We take this as an indication that our understanding of the model\u2019s learning mechanism is inadequate. Therefore, the successful cases obtained in this paper, either via ABS or ABC, even though only on simple recurrent patterns and task-specific models, solve a few long-standing difficult or even \u201cimpossible\u201d tasks (e.g., Parity) and represent a significant step forward.\n\n**Additional Tasks:**\nAdditional and more complex tasks appear to require composition of simple patterns. Learning such compositions may require scaling up the model dramatically, as the heuristic in the NDR paper (Csord\u00e1s et al., 2021b) indicates. We are exploring them in our current research.\n\n**Factual Errors:**\nYou are right on both points, and we have made appropriate modifications in the revision. Thank you very much for pointing them out!  Those are very keen observations!\n\nAgain, we thank you for your valuable feedback and we hope that our answers and revision addressed your questions."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579713993,
                "cdate": 1700579713993,
                "tmdate": 1700579713993,
                "mdate": 1700579713993,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]