[
    {
        "title": "Adversarial Data Robustness via Implicit Neural Representation"
    },
    {
        "review": {
            "id": "jpYgpYZipU",
            "forum": "ByAhXwV4bH",
            "replyto": "ByAhXwV4bH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2306/Reviewer_GzWz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2306/Reviewer_GzWz"
            ],
            "content": {
                "summary": {
                    "value": "Considering that adversarial training for models are sometimes difficult for common users, the paper proposes a data-level robustness method called Implicit Neural Representation (INR) for data-level adversarial training, which adversarially trains INR to get robust data features without losing their contents."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The adversarial data robustness proposed in this paper is original, since most of the previous works focused on models' robustness, while the abstacles in their applications were hardly considered.\n\n2. The idea of double projection innovatively explored the functions of projection in gradient-based attacks."
                },
                "weaknesses": {
                    "value": "1. Since data-level robustness is a new area, the meaning about robustness designed with spatial coordinates is not clear.\n\n2. The explanation about why the robust data can only be created via adversarial training in Section 3.1 is not convincing enough."
                },
                "questions": {
                    "value": "1. Since data-level robustness is a new area, the meaning about robustness designed with spatial coordinates is not clear.\n\n2. The explanation about why the robust data can only be created via adversarial training in Section 3.1 is not convincing enough."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2306/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698540192333,
            "cdate": 1698540192333,
            "tmdate": 1699636163325,
            "mdate": 1699636163325,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WkcmvduFk3",
                "forum": "ByAhXwV4bH",
                "replyto": "jpYgpYZipU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2306/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2306/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer GzWz"
                    },
                    "comment": {
                        "value": "> Since data-level robustness is a new area, the meaning about robustness designed with spatial coordinates is not clear.\n\n**Response:** Robustness is not directly related to spatial coordinates. The spatial coordinates are used as the input of the INR to output the corresponding RGB values. Our method allows for the incorporation of the classifier's relevant information into the parameters of the INR to force the INR to reach a robust state.\n\n> The explanation about why the robust data can only be created via adversarial training in Section 3.1 is not convincing enough.\n\n**Response:** Thank you for your suggestion. We think adversarial training could be an effective method for robust data generation. So, we change this sentence into:\n\n\"Considering the effectiveness of adversarial training in countering adversarial attacks, it could also be a valuable approach for generating robust data.\""
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2306/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557305060,
                "cdate": 1700557305060,
                "tmdate": 1700557305060,
                "mdate": 1700557305060,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U2h0mPlw3U",
            "forum": "ByAhXwV4bH",
            "replyto": "ByAhXwV4bH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2306/Reviewer_7WdV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2306/Reviewer_7WdV"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates adversarial data robustness by applying Implicit Neural Representation (INR), storing the image as a network-based representation, and then reconstructing the representation with robustness enhancement before calling task models. With adversarial data robustness, the users do not need to worry about the model-level robustness with adversarial training in practice. The paper looked into two different attack stages and proposed adaptive attack DPGD. Last, the paper proposed a new defense-in-creation strategy for defense and extensively evaluate the performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper proposes a novel direction for adversarial robustness on data preparation stages. It prevents user enhancing model robustness with adversarial training while still achieving good robustness behavior.\n\nAccording to the empirical experiments, the strategies achieve promising results compared to traditional model-level robust training.\n\nThe paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "The authors motivate the adversarial data robustness by assuming many users do not have knowledge for adversarial training on their models. However, the proposed strategy still needs robust training for INR model. This is one caveat for the motivation.\n\nIt would be helpful to evaluate stronger attacks like Auto Attack besides CW and PGD."
                },
                "questions": {
                    "value": "Is this approach compatible to model-level adversarial training? I am curious if applying both data-level and model-level approaches would lead to stronger robustness over SOTA?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2306/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698602742506,
            "cdate": 1698602742506,
            "tmdate": 1699636163239,
            "mdate": 1699636163239,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e62NePLtT4",
                "forum": "ByAhXwV4bH",
                "replyto": "U2h0mPlw3U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2306/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2306/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 7WdV"
                    },
                    "comment": {
                        "value": "> The authors motivate the adversarial data robustness by assuming many users do not have knowledge for adversarial training on their models. However, the proposed strategy still needs robust training for INR model. This is one caveat for the motivation.\n\nOur approach is particularly suitable for situations where adversarial training at the model level is challenging. For instance, adversarially training a large-scale foundational model can be complex. In response, model developers can define a secure data format within their frameworks to enhance safety. Additionally, our concept of 'defense-in-creation' could serve as a safety module integrated with the foundational model. This setup would require that any images inputted into such a large-scale foundational model first pass through this safety module, which would remove adversarial perturbations.\n\n> It would be helpful to evaluate stronger attacks like Auto Attack besides CW and PGD.\n\nThank you for your suggestion. We conducted experiments by adapting AutoAttack for images during INR creation, and the results show that our defense-in-creation can reach high accuracy. We will add the results to the revised paper. For attack during transmission, due to the data being represented as INR, it is not feasible to directly apply AutoAttack to the INR parameters. Thus, the current AutoAttack cannot directly attack INR parameters.\n\n> Is this approach compatible with model-level adversarial training? I am curious if applying both data-level and model-level approaches would lead to stronger robustness over SOTA?\n\nYes. It is compatible with model-level adversarial training. As we do not have specific requirements for downstream models, they can also be an adversarial-robust network model. We will further study the cooperation with model-level approaches in our future work."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2306/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557204811,
                "cdate": 1700557204811,
                "tmdate": 1700557204811,
                "mdate": 1700557204811,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mT8Gb0c5LW",
            "forum": "ByAhXwV4bH",
            "replyto": "ByAhXwV4bH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2306/Reviewer_bc25"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2306/Reviewer_bc25"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method that makes data resistant to adversarial perturbations, ensuring that downstream models remain robust even when subjected to attacks. The author achieves this by representing data as Implicit Neural Representations (INRs), without the need to modify the deep learning models\u2019 weights."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The proposed concept of *adversarial data robustness* is interesting and leveraging the INR framework to achieve data robustness is novel."
                },
                "weaknesses": {
                    "value": "This paper is subject to several weaknesses that need to be addressed.\n\n1. **(Motivation)** The paper's motivation for storing data as a learnable representation via INR in the context of adversarial robustness is not well-established. Additionally, the proposed attack scenario, involving direct manipulation of INR parameters during data transmission, raises concerns about its practicality and real-world relevance. It would be beneficial for the authors to provide a more robust justification for these choices and consider the feasibility of such attacks in practical applications to ensure a more realistic context for the proposed defense strategy.\n\n2. **(Method)** The paper introduces the concept of storing data as INRs for adversarial robustness, but it lacks clarity on how these representations can be derived for testing data. Given that testing data is typically only accessible during the testing phase, the paper should address the challenge of generating specific INRs for each testing image. For now, I would treat INR as a mere image generator. \n\n3. **(Experiment)** The experimental design in the paper appears to lack fairness, which fails to prove the efficacy of the proposed method. Also, it is unclear if the proposed framework could be used to defend against unseen attacks (e.g., [1]).\n\n\n[1] Perceptual adversarial robustness: Defense against unseen threat models. (ICLR 2021)"
                },
                "questions": {
                    "value": "1. Please provide a more in-depth justification for the choice of INR as a data representation method for adversarial robustness.\n\n2. How do the authors envision real-world applications where adversarial data robustness using INR would be particularly beneficial?\n\n3. In the proposed setup, how can users derive INR representations for testing data, considering that in adversarial training, testing data is typically not accessible during training?\n\n4. Are there any potential biases or confounding factors in the experimental setup that need to be addressed?\n\n5. Can the proposed framework used to defend unseen attacks?\n\n6. Please provide the computational cost of the proposed method and compare it with other baselines."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2306/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698651661253,
            "cdate": 1698651661253,
            "tmdate": 1699636163150,
            "mdate": 1699636163150,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eDyPT3CUOP",
                "forum": "ByAhXwV4bH",
                "replyto": "mT8Gb0c5LW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2306/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2306/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer bc25"
                    },
                    "comment": {
                        "value": "> The paper introduces the concept of storing data as INRs for adversarial robustness, but it lacks clarity on how these representations can be derived for testing data. Given that testing data is typically only accessible during the testing phase, the paper should address the challenge of generating specific INRs for each testing image. For now, I would treat INR as a mere image generator.\n\n**Response:** As we said in our submission, even for a new image, we only need to represent it to its corresponding INR format by incorporating the classifier property. Thus, in our experiments, for each classifier, we directly transfer their corresponding testing data into the INR format by incorporating the classifier property. Then, we can achieve the envisioned robustness.\n\n> The experimental design in the paper appears to lack fairness, which fails to prove the efficacy of the proposed method. \n\n**Response:** We propose to incorporate the classifier's related information into the data format. Therefore, we should train INR for each image individually. We spend time on INR creation, but not on training models adversarially. Therefore, it is hard to compare the efficacy with other methods simply.\n\n> Please provide a more in-depth justification for the choice of INR as a data representation method for adversarial robustness.\n\n**Response:**  We just utilize the distinguished overfitting property of INR [1] to incorporate the classifier property into the data format. Then, we can achieve higher robustness.  We can also consider image pixels as learnable parameters and optimize these learnable parameters adversarially. However, such a straightforward solution significantly undermines image quality since the pixel values are directly manipulated. \n\n[1] Str\u00fcmpler, Yannick, et al. \"Implicit neural representations for image compression.\" ECCV. 2022.\n\n> How do the authors envision real-world applications where adversarial data robustness using INR would be particularly beneficial?\n\n**Response:**  This is needed when the model developers have a higher demand for robustness especially for some established specific models. In this situation, if they have high demands for the safety of their models and feel reluctant to do any alterations to their models, our setup would be useful for them.  They only need to create robust data without any alternations to their models.\n\n> In the proposed setup, how can users derive INR representations for testing data, considering that in adversarial training, testing data is typically not accessible during training?\n\n**Response**: Even for a totally new image, we only need to overfit it to its corresponding INR format by considering the classifier property. Thus, in our experiments, for each classifier, we directly transfer their corresponding testing data into the INR format by incorporating the classifier property. Then, we can achieve the envisioned robustness.\n\n> Are there any potential biases or confounding factors in the experimental setup that need to be addressed?\n\n**Response**: We have emphasized in the article that due to the distinct nature of the envisioned scenario from traditional adversarial defense, our experimental setup differs accordingly. We obtain robust data representations for each image through defense-in-creation, which can withstand attacks during both creation and transmission. Similar to other INR methods, we do not specifically require a training dataset, which sets it apart from the traditional scenario.\n\n> Can the proposed framework used to defend unseen attacks?\n\nYes. In our experiment setting, we use 10-step PGD for INR training, while we test FGSM, 100-step PGD, and CW attacks for evaluation. We present the results of the experiments in Table 2. \n\n\n> Please provide the computational cost of the proposed method and compare it with other baselines.\n\nIn our experiments, we did not focus on time efficiency or computational costs, as most of the tasks could be completed swiftly using a single GPU. Typically, standard INR encoding is accomplished in about 7.12 seconds. When incorporating data robustness into the process, the entire training session can be concluded in under 175.50 seconds. Since conventional adversarial training methods like PGD-AT take 3-30 times longer than standard training [1], the time consumption of our method is also acceptable. It is also worth noting from prior research that 7-PGD adversarial training for CIFAR-10 takes more than 5000 minutes [2]. Besides, adversarial training for a diffusion model may require \"millions of generated data even on small datasets such as CIFAR-10 on 8 $\\times$ A100 GPUs, which is inefficient in the training phase\"[3].\n\n\n[1] Bai, Tao, et al. \"Recent Advances in Adversarial Training for Adversarial Robustness.\" IJCAI 2021\n\n[2] Shafahi, Ali, et al. \"Adversarial training for free!.\" NeurIPS 2019.\n\n[3] Wang, Zekai, et al. \"Better diffusion models further improve adversarial training.\" ICML 2023."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2306/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557099424,
                "cdate": 1700557099424,
                "tmdate": 1700557099424,
                "mdate": 1700557099424,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BvDuYbAAuX",
                "forum": "ByAhXwV4bH",
                "replyto": "eDyPT3CUOP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2306/Reviewer_bc25"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2306/Reviewer_bc25"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response from the authors. However, my primary concern remains unaddressed.\n\nLabeling during the testing phase is *de facto* the problem. As I stated in my review: \"*Given that testing data is typically only accessible during the testing phase, the paper should address the challenge of generating specific INRs for each testing image.*\" It is not fair *to use the labels from the test set to generate robust data and study its robustness*. \n\nIt is also weird why the INRs of the testing data should be released by the model developer, the reviewer is still struggling to come up with a scenario where the proposed work could be applied."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2306/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729233616,
                "cdate": 1700729233616,
                "tmdate": 1700729233616,
                "mdate": 1700729233616,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pAEDCFXS9p",
            "forum": "ByAhXwV4bH",
            "replyto": "ByAhXwV4bH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2306/Reviewer_Gy4W"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2306/Reviewer_Gy4W"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes adversarial data robustness, aiming to allow the data to resist adversarial perturbations. The proposed method stores the data as a learnable representation via Implicit Neural Representation (INR), and trains such a representation adversarially to achieve data robustness. Empirical evaluations are done on CIFAR-10/100 and SVHN, against PGD/DPGD, FGSM, and CW attacks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strengths of this paper include:\n- The writing is clear with intuitive explanations such as Figures 1 and 3. The formulas in Section 3 is straightforward and easy to follow.\n- I like the high-level idea of data robustness, which is supposed to be (conceptually) more efficient than model robustness."
                },
                "weaknesses": {
                    "value": "The weaknesses of this paper include:\n- The attack during creation formulated in Eq (2) is not an *adaptive attack* [1,2]. Specifically, an adaptive attacking objective should be like\n$$\\\\max\\_{\\\\Delta I\\_{i}}\\\\mathcal{L}\\_{CE}(g\\_{\\\\phi}(\\\\Psi\\\\circ f\\_{\\theta}(I\\_{i}+\\\\Delta I\\_{i})),y\\_{i})\\\\textrm{,}$$\nwhere the INR decoding and reconstruction process implemented by $\\Psi\\circ f\\_{\\theta}$ should be involved.\n\n- There are two extra computations introduced by the proposed method: first is the optimization process of $f\\_{\\theta}$, which is required to be optimized for each input image (i.e., cannot be amortized); second is the defense-in-creation process of Eq (4), which requires adversarial training by perturbing model parameters $\\\\theta$. The authors should report the empirical cost (e.g., computational time) for these two operations.\n\n- The considered attacking methods such as PGD/DPGD, CW and FGSM are not strong. The authors should evaluate their methods under strong attacks like AutoAttack[3] and compare with the state-of-the-art models listed on RobustBench[4].\n\n\nReferences: \\\n[1] Athalye et al. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. ICML 2018 \\\n[2] Carlini et al. On evaluating adversarial robustness. arXiv 2019 \\\n[3] Croce and Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. ICML 2020 \\\n[4] https://robustbench.github.io/"
                },
                "questions": {
                    "value": "From my experience, the proposed method (with 92.51% accuracy against PGD) probably has gradient obfuscation [1,2]. The authors should evaluate their method under adaptive attacks that involving the defense mechanism (i.e., INR mechanism), as well as strong off-the-shelf attacks such as AutoAttack.\n\n\nReferences: \\\n[1] Athalye et al. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. ICML 2018 \\\n[2] Carlini et al. On evaluating adversarial robustness. arXiv 2019"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2306/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757652086,
            "cdate": 1698757652086,
            "tmdate": 1699636163079,
            "mdate": 1699636163079,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gmCgxU6Be8",
                "forum": "ByAhXwV4bH",
                "replyto": "pAEDCFXS9p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2306/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2306/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Gy4W - Part 1"
                    },
                    "comment": {
                        "value": "> The attack during creation formulated in Eq (2) is not an *adaptive attack*. Specifically, an adaptive attacking objective should be like\n$$\n\\max\\_{\\Delta I\\_{i}}\\mathcal{L}\\_{CE}(g\\_{\\phi}(\\Psi\\circ f\\_{\\theta}(I\\_{i}+\\Delta I\\_{i})),y\\_{i})\\textrm{,}\n$$\nwhere the INR decoding and reconstruction process implemented by $\\Psi\\circ f\\_{\\theta}$ should be involved.\n\n**Response:** We elucidate the connection between Eq. (2) and the adaptive attack. We define $\\Phi: I \\mapsto f\\_\\theta$ as the mapping from the image $I$ to the intermediate representation (INR) $f\\_\\theta$, and $\\Psi: f\\_\\theta \\mapsto \\hat{I}$ as the mapping from the INR to the reconstructed image $\\hat{I}$. Considering the adaptive attack, Eq. (2) should be written as\n\n$$\n\\max\\_{\\Delta I\\_{i}}\\mathcal{L}\\_{CE}(g\\_{\\phi}(\\Psi\\circ \\Phi(I\\_{i}+\\Delta I\\_{i})),y\\_{i})\\textrm{.}\n$$\nHowever, the mapping from the image $I$ to the intermediate representation (INR) $f\\_\\theta$ is obtained through optimizing Eq. (1). Therefore, the process $\\Psi\\circ \\Phi$ is not amenable to gradient propagation. However, we can observe a property that $\\Psi\\circ \\Phi(I)\\approx I$. Based on the Backward Pass Differentiable Approximation (BPDA) method described in reference [1], we can estimate the gradient as $\\nabla\\_{I}\\Psi\\circ \\Phi(I) \\approx \\nabla\\_{I} I = 1$. Consequently, the above equation can be written as\n\n$$\n\\max\\_{\\Delta I\\_{i}}\\mathcal{L}\\_{CE}(g\\_{\\phi}(I\\_{i}+\\Delta I\\_{i}),y\\_{i})\\textrm{.}\n$$\nThis aligns with our formulation of Attack during creation in our paper. In fact, based on experimental results, this attack method is capable of successfully attacking methods utilizing conventional INR encoding. However, it can only be applied to the images undergoing encoding. Empirical evidence demonstrates that by considering additional adversarial loss during INR creation, we can effectively eliminate the perturbations added to the image.\n\n[1] Athalye et al. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. ICML 2018\n\n> There are two extra computations introduced by the proposed method: first is the optimization process of $f\\_{\\theta}$, which is required to be optimized for each input image (i.e., cannot be amortized); second is the defense-in-creation process of Eq (4), which requires adversarial training by perturbing model parameters. The authors should report the empirical cost (e.g., computational time) for these two operations.\n\nThe time for the first process is 7.12 seconds. The time for the second process is 175.50 seconds when no optimization for computational costs is considered. Besides, our setup does not need any time for training an adversarially robust downstream network model like previous methods. Since conventional adversarial\ntraining methods like PGD-AT take 3-30 times longer than standard training [1], the time consumption of our method is also acceptable.\n\n[1] Bai, Tao, et al. \"Recent Advances in Adversarial Training for Adversarial Robustness.\" IJCAI 2021\n\n> The considered attacking methods such as PGD/DPGD, CW and FGSM are not strong. The authors should evaluate their methods under strong attacks like AutoAttack[3] and compare with the state-of-the-art models listed on RobustBench[4].\n\nReferences:\n\n[1] Athalye et al. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. ICML 2018\n\n[2] Carlini et al. On evaluating adversarial robustness. arXiv 2019\n\n[3] Croce and Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. ICML 2020\n\n[4] https://robustbench.github.io/\n\n**Response:** The AutoAttack has not been specifically studied for INR. We conducted experiments by adapting AutoAttack for images during INR creation, and the results show that our defense-in-creation can reach high accuracy. For attack during transmission, due to the data being represented as INR, it is not feasible to directly apply AutoAttack to the INR parameters."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2306/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556885983,
                "cdate": 1700556885983,
                "tmdate": 1700581800981,
                "mdate": 1700581800981,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YAAl4hWUdc",
            "forum": "ByAhXwV4bH",
            "replyto": "ByAhXwV4bH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2306/Reviewer_DRHK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2306/Reviewer_DRHK"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the problem of adversarial attacks in a new threat model designed around Implicit Neural Representations (INRs). INRs are a new family of data representations where each data point is represented via neural networks. To this end, often an MLP is used to overfit a data point. Then, the MLP weights can be used instead of each data point. Examples of INRs used in real applications include 2D images and 5D radiance fields (NeRFs).\n\nIn this context, this paper argues that INRs can be used to transmit data points between users, and as such, are prone to adversarial manipulation. In particular, an adversary might add a perturbation to the image data _before_ being encoded as INR, or they may opt to manipulate the INR weights _during transmission_. The former is called \"attack during creation\" while the latter is named \"attack during transmission.\"\n\nThe paper presents a formulation for generating each of these attacks. Specifically, a projected gradient descent (PGD) attack is used to generate adversarial perturbations for attack during creation. Then, this adversarial example is encoded via an INR. Also, for attack during transmission a double projection gradient descent (DPGD) is used to ensure that while INR manipulation fools the downstream classifier, it generates attacks that are imperceptible in the image domain. Empirical evaluations indicate that both of these attacks are effective against CIFAR-10, CIFAR-100, and SVHN datasets.\n\nFinally, as a potential defense against these attacks, the paper proposes to make the INR representation of the data robust to manipulation, resulting in an algorithm dubbed \"defense in creation.\" To this end, the paper proposes a new objective function for creating the INR representation of the data by adding a regularization term. This term aims to add perturbations to the INR during creation while ensuring that these changes have minimal effect on the downstream classification task. Empirical results indicate that this strategy is helpful in making INRs robust to adversarial attacks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper investigates a new problem in adversarial machine learning. As far as this reviewer knows, there are no prior works that investigates robustness of INRs. Thus, the setting seems interesting, although its practicality needs to be discussed (see the Weaknesses).\n\n- Setting the problem's definition aside, the paper explores the problem from different perspectives. In particular, exploring both the possible attacks and proposing a new defense strategy against them is rather appealing.\n\n- Finally, the paper is very well-written and provides enough detail (such as figures and pseudo-codes) to understand each aspect of it."
                },
                "weaknesses": {
                    "value": "- The major issue of this paper in this reviewer's view is its motivation and threat model. Specifically:\n\n1. The paper starts its discussion by arguing that adversarial training is hard to deploy. The paper reads: \"_Despite its effectiveness, adversarial training requires that users possess a detailed understanding of training settings._\" In my view, this argument can even be used for using vanilla neural networks. One can make the same argument that even for using/training regular neural networks users need a detailed understanding of model architecture, optimization process, etc. There is no major differences that separates adversarial training from vanilla training, and I feel that motivating the core problem around such arguments is weak.\n\n2. More importantly, the threat model is not intuitive. In particular, the paper assumes that one uses the INRs to encode the data, then send them to a model, which again decodes the INR to query a classifier. Why this process is efficient? Why the user doesn't send their data directly? What is special about this threat model? I believe that the proposes threat model is not making intuitive sense, and it requires a better design. For instance, using NeRF to encode a scene and then transmitting it would have made a much better threat model as NeRFs are encoding multiple scenes in one representation. However, using the current threat model for 2D images seems less intuitive and might not make any practical sense.\n\n- The empirical evaluations are also limited. The paper only uses small scale datasets such as CIFAR and SVHN. It would be crucial to see how the proposed attacks and defense work in more large scale datasets such as ImageNet. Given that there are many pre-trained models available for the ImageNet dataset online, I believe that such evaluations would be feasible."
                },
                "questions": {
                    "value": "- Please clarify what do you mean by arguments like \"_our setup is rooted in the reality that many model users often fail to understand the settings associated with adversarial training._\"\n\n- Given that $\\ell_p$ norms are used to enforce visual image similarity in the image domain, its use for other domains such as INR weights seems less intuitive. What are other alternatives for the similarity in the weight space that could have been used? In other words, are there any other alternatives to the $\\ell_p$ norm used in Eq. (3)?\n\n- What do you mean by $||\\hat{I}\\_{t-1}+\\nabla-\\hat{I}\\_{\\mathrm{org}}||_p \\leq \\zeta$? Is this a typo?\n\n- It would be nice to add a few more sentences to the last paragraph of Section 3.2 explaining why projecting the gradient of the image space would help with having a better image quality. In other words, what motivated this step?\n\n- What is the training time difference between finding an INR using Eq. (1) versus the defense in creation in Eq. (4)?\n\n- Did the paper also test the transferability of the defense in creation? In other words, what happens if we find the robust INRs for a classifier $g\\_{\\boldsymbol{\\phi}}^{(1)}$ while trying to defend another model $g\\_{\\boldsymbol{\\phi}}^{(2)}$ during inference?\n\n- Run experiments on large scale datasets such as ImageNet-1k or ImageNet-100.\n\n- How do you specify the upper-bound for adversarial attack against INR weights? In other words, what makes a good $\\delta$ for Eq. (3)? Because using $\\ell_p$ norm in the INR weight space is not intuitive.\n\n- In my view, using attack success rate (ASR) would be a better measure when trying to evaluate attacks. Using accuracy as the current version makes it difficult to interpret the results.\n\n- How is it possible that the natural accuracies in Table 2 are 100%?\n\n- Use a larger font size for the tables."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2306/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811834035,
            "cdate": 1698811834035,
            "tmdate": 1699636163003,
            "mdate": 1699636163003,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9Sw42xBK0J",
                "forum": "ByAhXwV4bH",
                "replyto": "YAAl4hWUdc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2306/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2306/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer DRHK - Part 1"
                    },
                    "comment": {
                        "value": "> The paper starts its discussion by arguing that adversarial training is hard to deploy. The paper reads: \"*Despite its effectiveness, adversarial training requires that users possess a detailed understanding of training settings. *\" In my view, this argument can even be used for using vanilla neural networks. One can make the same argument that even for using/training regular neural networks users need a detailed understanding of model architecture, optimization process, etc. There are no major differences that separates adversarial training from vanilla training, and I feel that motivating the core problem around such arguments is weak.\n\n**Response:** We want to emphasize that adversarial training must consider potential attacks during model training or finetune the specific model to achieve adversarial robustness. However, when large-scale foundational model has become more popular today. Retraining specific models may become more difficult or even impossible [1]. In this situation, those model creators may define a specific data format suitable for their large-scale foundational models. Then, rather than directly retraining the model for adversarial robustness, our data robustness provides a more versatile solution. Besides, our experiments demonstrates that this unique robust data is able to achieve high robustness for specific models.\n\n[1] Hoffmann, Jordan, et al. \"An empirical analysis of compute-optimal large language model training.\" Advances in Neural Information Processing Systems 35 (2022): 30016-30030.\n\n> Why this process is efficient? Why the user doesn't send their data directly?\n\n**Response:** This process may undermine the efficiency at the data side. However, it eliminates the need for time-consuming retraining or fine-tuning of the specific model. By incorporating the classifiers during the creation of INR, our method has achieved higher robustness for specific models in the current setting. \n\nIf we directly send images, such incorporation can significantly undermine the image quality as shown in the \"Pixel manipulation\" experiment. \n\n> What is special about this threat model?\n\n**Response:** INR can encode the data into network parameters. By representing the data into INR, we are able to easily incorporate information of specific classifier into INR. Then, such unique data representation is able to achieve high robustness for specific models. \n\nSince we utilize INR for data representation, we need to consider perturbations to the INR parameters in the threat model.\n\n> Using NeRF to encode a scene and then transmitting it would have made a much better threat model as NeRFs are encoding multiple scenes in one representation. However, using the current threat model for 2D images seems less intuitive and might not make any practical sense.\n\n**Response:** NeRF is designed to encode a single, specific scene, utilizing multiple images from that scene for training. It is not capable of encoding multiple scenes within a single model. NeRF is also inapproproate for the tasks in our scenario requiring the representation of individual images. Besides, the use of INR for 2D image storage and transmission has been extensively studied. For example, [1] proposed that INR can achieve super-resolution image storage. [2] [3] considered using INR for image compression and transmission. [4] considered representing images as INR for neural network training. [5] implemented INR representation on the ImageNet dataset. Therefore, adopting INR as a cornerstone is practical and feasible.\n\n[1] Nguyen, Quan H., and William J. Beksi. \"Single image super-resolution via a dual interactive implicit neural network.\" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023.\n\n[2] Str\u00fcmpler, Yannick, et al. \"Implicit neural representations for image compression.\" European Conference on Computer Vision. 2022.\n\n[3] Dupont, Emilien, et al. \"COIN: COmpression with Implicit Neural representations.\" Neural Compression: From Information Theory to Applications--Workshop@ ICLR 2021. 2021.\n\n[4] Rusu, Andrei A., et al. \"Hindering Adversarial Attacks with Implicit Neural Representations.\" International Conference on Machine Learning. PMLR, 2022.\n\n[5] Singh, Rajhans, Ankita Shukla, and Pavan Turaga. \"Polynomial Implicit Neural Representations For Large Diverse Datasets.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2306/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700556024517,
                "cdate": 1700556024517,
                "tmdate": 1700556024517,
                "mdate": 1700556024517,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]