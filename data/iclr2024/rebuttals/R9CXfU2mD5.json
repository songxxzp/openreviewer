[
    {
        "title": "Score Propagation as a Catalyst for Graph Out-of-distribution Detection: A Theoretical and Empirical Study"
    },
    {
        "review": {
            "id": "2leOQImaBw",
            "forum": "R9CXfU2mD5",
            "replyto": "R9CXfU2mD5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission504/Reviewer_13zH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission504/Reviewer_13zH"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method called GRaph-Augmented Score Propagation (GRASP) for enhancing out-of-distribution (OOD) detection in graphs. It propagates OOD scores among neighboring nodes to leverage graph structure. The paper investigates whether score propagation will always help graph OOD detection. The authors find the ratio of intra-edges (ID-ID and OOD-OOD) to inter-edges (ID-OOD) must be high for propagation to be beneficial. To improve this ratio, GRASP strategically adds edges to a subset G of training nodes that are assured to be in-distribution. This enhances the intra-edge ratio and thus OOD detection performance after propagation. Theoretically, the paper shows that if G connects predominantly to ID data over OOD data, GRASP can provably improve post-propagation OOD detection outcomes. The paper evaluates GRASP on benchmark graph datasets and pre-trained GNNs. It demonstrates GRASP outperforms baselines\nOverall, the paper is well-written and provides good insight. However, the experiment was all conducted on small-scale of data, which  limits the ability to fully validate the proposed approach and conclusions. More experiments on large-scale real world data would strength the claims."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper provides one of the first theoretical analyses of score propagation for graph OOD detection. The theoretical analysis is derived rigorously and proves helpful conditions for when propagation enhances OOD detection. The formulations and proofs are clear.\n2. The paper is well-written, the motivation, problem definition, methodology and conclusions are explained clearly throughout the paper. \n3. The proposed GRASP method is original in its strategic augmentation of edges to a subset of training nodes to boost intra-edge ratios."
                },
                "weaknesses": {
                    "value": "1. However, the experiment was all conducted on small-scale of data, which  limits the ability to fully validate the proposed approach and conclusions. More experiments on large-scale real world data would strength the claims.\n2. The time complexity of GRASP compared to baselines is not mentioned in the paper. A thorough accounting of computation/memory demands compared to baselines is important as it relates to practical deployment."
                },
                "questions": {
                    "value": "1. The theoretical analysis assumes edges follow a Bernoulli distribution. How sensitive are the results to this assumption?\n2. How do different propagation mechanisms, like higher-order diffusion, impact the findings? \n3. What is the time complexity of GRASP compared to baselines?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission504/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698365280204,
            "cdate": 1698365280204,
            "tmdate": 1699635977013,
            "mdate": 1699635977013,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IYb4mKXXwE",
                "forum": "R9CXfU2mD5",
                "replyto": "2leOQImaBw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission504/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission504/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response - Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful questions! Below we address each of your comments in detail.\n\n> **Q1. Experiments on large-scale real world data.**\n\nWe are glad to share with you that we have conducted experiments on large datasets in paper, i.e. arxiv-year, snap-patents, and wiki, which are three large-scale real world datasets reflecting real-world scenarios. Specifically, Arxiv-year has node and edge counts in the order of hundreds of thousands and millions, respectively. Snap-patents features millions of nodes and tens of millions of edges, while wiki, being a large dense graph, boasts millions of nodes and billions of edges. The detailed statistics of these datasets [1] are provided in the following table. We believe these datasets sufficiently meet the requirements of real-world applications. If the reviewer has an interest in other large datasets, feel free to share with us and we would be more than willing to conduct additional experiments on these datasets to further check the effectiveness of our proposed method.\n\n|Dataset | #Nodes | #Edges |\n|:-------- |--------:|--------:|\n| arXiv-year | 169,343 | 1,166,243 |\n|snap-patents |**2,923,922** |13,975,788 |\n|wiki|**1,925,342**|**303,434,860**|\n\n[1] Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Bhalerao and Ser-Nam Lim. Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods. NeurIPS 2021.\n\n\n\n> **Q2. The time complexity of GRASP and a thorough accounting of computation/memory demands compared to baselines to account for practical deployment.**\n\nGreat question! We first present the conclusion and then provide the support from both mathmatical and empirical points. \n\n**Takeway:** Our method's computational footprint in terms of a single propagation\u2019s runtime and memory usage is:\n- Time Complexity: $O(N+|\\mathcal{E}|+n)$,\n- Memory Complexity: $O(N+|\\mathcal{E}|+n)$,\n\nwhere $N$ is node count of the entire graph $\\mathcal{G}$ with $|\\mathcal{E}|$ edges and subset $G$ has $n$ nodes. Hence, our algorithm has a linear complexity and this makes our method applicable to large-scale networks.\n\n**Justification:** While our algorithm introduces the fully connected matrix $E$, we adeptly transform the propagation formula to eliminate direct matrix computation between $E$ and the OOD score vector $\\mathbf{g}$, resulting in an efficient linear complexity. This can be  confirmed by the computational complexity  analysis presented below. \n- **Computational complexity analysis.** As described by Equation (5) in our paper, given a raw OOD scoring vector $\\hat{\\mathbf{g}} \\in \\mathbb{R}^{N}$, the propagated scoring vector using augmentated adjacency matrix after propagation for $k$ times is given by: \n \\begin{align*}\n     \\mathbf{g} \\_{GRASP} &= (\\bar{A} \\_+)^k \\hat{\\mathbf{g} } \n     = ({D}^{-1} \\_+A \\_+)^k \\hat{\\mathbf{g}} = ({D}^{-1} \\_+(A+E))^k \\hat{\\mathbf{g}} \\\\\n     &= ({D}^{-1} \\_+A \\_+)^{k-1} \\cdot (D \\_+^{-1}\\boxed{A\\hat{\\mathbf{g}}}+D \\_+^{-1}\\boxed{E\\hat{\\mathbf{g}}})   \\\\\n     &= (\\text{run $k-1$ times ...}) \\\\\n\\end{align*}\nIn the above equation, ($a$) $E\\hat{\\mathbf{g}}$ can be computed with time/space complexity $O(n)$ by a simple summation operation ($\\mathbf{g} \\_i$ in $G$ will be replaced by $\\sum \\_{i\\in G} \\mathbf{g} \\_i$ ) to get rid of the matrix multiplication; ($b$) $A\\hat{\\mathbf{g}}$ can be computed in $O(|\\mathcal{E}|)$ with sparse matrix multiplication; ($c$) $D \\_+^{-1}$ is an $O(N)$ operation since it is scaling over all elements in the vector. In all, the time/memory complexity of GRASP is $O(N+|\\mathcal{E}|+n)$."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission504/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700315667145,
                "cdate": 1700315667145,
                "tmdate": 1700422052307,
                "mdate": 1700422052307,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZxFvYyPVtj",
                "forum": "R9CXfU2mD5",
                "replyto": "2leOQImaBw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission504/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission504/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response - part 2"
                    },
                    "comment": {
                        "value": "(Connecting to the Q2 above ^ )\n- **Empirical results.** \nThen we conduct comprehensive experiments comparing the time and space costs of our algorithm with various baselines (both post-hoc and training-based approaches). From the experimental results, it is evident that our algorithm is highly efficient across all datasets. \n    - We first present the time(s)/memory(M) of all post-hoc baseline methods consumed over all datasets considered. \n    \n        |            | cora       |           | amazon    |           | coauthor-cs |         | chameleon |         |\n        |:-----------|:-----------|:----------|:----------|:----------|:------------|:--------|:----------|:--------|\n        |  | time       | memory    | time      | memory    | time        | memory  | time      | memory  |\n        | MSP        | 0.03       | 622.79    | 0.04      | 635.41    | 0.13        | 1092.05 | 0.05      | 634.05  |\n        | Energy     | 0.05       | 624.10    | 0.10      | 636.52    | 0.11        | 1094.72 | 0.11      | 634.75  |\n        | KNN        | 0.24       | 630.92    | 0.13      | 658.21    | 0.30        | 1101.30 | 0.17      | 636.44  |\n        | ODIN       | 0.02       | 621.82    | 0.05      | 636.40    | 0.07        | 1093.64 | 0.16      | 635.41  |\n        | Mahalanobis| 0.15       | 627.39    | 0.22      | 641.21    | 0.29        | 1099.38 | 0.21      | 637.18  |\n        | GNNSafe    | 0.06       | 626.23    | 0.15      | 652.80    | 0.35        | 1106.19 | 0.12      | 637.95  |\n        | **GRASP**  | **0.05**       | **628.79**    | **0.15**      | **677.26**    | **0.25**        | **1118.02** | **0.15**      | **644.09**  |\n\n\n        |            | squirrel   |           | arxiv-year |           | snap-patents |         | wiki     |         |\n        |:-----------|:-----------|:----------|:-----------|:----------|:-------------|:--------|:---------|:--------|\n        |   | time       | memory    | time       | memory    | time         | memory  | time     | memory  |\n        | MSP        | 0.05       | 658.00    | 0.36       | 726.59    | 3.41         | 4014.22 | 3.01     | 9765.80 |\n        | Energy     | 0.12       | 660.50    | 0.52       | 726.17    | 3.24         | 4039.91 | 3.13     | 9788.45 |\n        | KNN        | 0.14       | 660.18    | 3.77       | 732.90    | 6241.01      | 3933.32 | 273.96   | 9812.89 |\n        | ODIN       | 0.06       | 659.58    | 0.61       | 727.68    | 3.61         | 4005.23 | 3.00     | 9760.24 |\n        | Mahalanobis| 0.19       | 661.44    | 1.28       | 730.41    | 5.93         | 4056.08 | 6.17     | 9829.16 |\n        | GNNSafe    | 0.10       | 662.48    | 0.97       | 790.20    | 13.48        | 4187.67 | 62.09    | 9655.11 |\n        | **GRASP**  | **0.13**       | **668.07**    | **0.68**       | **768.21**    | **13.63**        | **4279.92** | **218.67**   | **9824.79** |\n\n    - Additionally, we provide comparison with training-based methods in the literature. It is worth noting that the training-based methods incur substantial time and space costs due to training from scratch,  exhibit a significant performance gap compared to our algorithm and therefore are practically infeasible for practical deployment.\n\n        |\t\t\t|cora\t\t|\t\t\t|amazon\t\t|\t\t\t|coauthor-cs|\t      |chameleon|         |\t\n        |:--------  |:--------|:--------|:--------|:--------|:--------|:--------|:--------|:--------|\n        ||time\t\t|memory\t\t|\ttime\t|memory\t\t|time\t\t|memory   |time\t\t|memory   |\n        | GKDE [1]  | 26.72  | 3366.01   | 160.00   | 3388.59   | 1004.73   | 4199.84  | 37.24   | 3310.92  |\n        | GPN [2]  | 28.64  | 3702.57   | 58.72    | 3719.10   | 77.55     | 3634.66  | 50.65   | 3620.97  |\n        | OODGAT [3] | 77.99  | 3369.00   | 395.88   | 3390.88   | 411.44    | 3851.37  | 182.11  | 3291.63  |\n        | **GRASP**   | **0.05**   | **628.79**    | **0.15**     | **677.26**    | **0.25**      | **1118.02**  | **0.15**    | **644.09**  |\n\n\n        |            | squirrel   |           | arxiv-year |           | snap-patents |         | wiki     |         |\n        |:-----------|:-----------|:----------|:-----------|:----------|:-------------|:--------|:---------|:--------|\n        ||time\t\t|memory\t\t|\ttime\t|memory\t\t|time\t\t|memory   |time\t\t|memory   |\n        | GKDE  [1]    | 170.26     | 3234.38   | OOT        | -         | -            | OOM     | -        | OOM     |\n        | GPN [2]   | 50.70      | 3650.58   | 444.84     | 3808.72   | -            | OOM     | -        | OOM     |\n        | OODGAT  [3]  | 454.62     | 3319.99   | 1214.98    | 3491.11   | -            | OOM     | -        | OOM     |\n        | **GRASP**  | **0.13**       | **668.07**    | **0.68**       | **768.21**    | **13.63**        | **4279.92** | **218.67**  | **9824.79** |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission504/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700315769685,
                "cdate": 1700315769685,
                "tmdate": 1700554004761,
                "mdate": 1700554004761,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A5JRnPqs0e",
                "forum": "R9CXfU2mD5",
                "replyto": "2leOQImaBw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission504/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission504/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response - part 3"
                    },
                    "comment": {
                        "value": "(Connecting to the Q2 above ^ )\n\nBased on the  above experimental results, we have:\n\n- In comparison to training-based methods, post-hoc methods exhibit significantly lower runtime and memory consumption.\n- Considering that the minimum time and space complexity required to run a graph algorithm is $O(N+|\\mathcal{E}|)$, as outlined in our algorithm complexity analysis, our algorithm incurs limited additional time and space costs, specifically $O(n)$. This can be validated by the small extra overhead incurred by our algorithm compared to MSP or Energy (8 times n) from the table.\n- Compared with training-based methods, our algorithm demonstrates substantially lower time and space demands on three large-scale datasets. And when compared to post-hoc baselines on these datasets, the overhead of our method is also reasonable. The performance of our method on these three large-scale datasets underscores the strong practicality of our approach.\n\n\n[1] Xujiang Zhao, Feng Chen, Shu Hu and Jin-Hee Cho. Uncertainty Aware Semi-Supervised Learning on Graph Data. NeurIPS 2020.\n\n[2] Maximilian Stadler, Bertrand Charpentier, Simon Geisler, Daniel Z\u00fcgner and Stephan G\u00fcnnemann. Graph Posterior Network: Bayesian Predictive Uncertainty for Node Classification. NeurIPS 2021.\n\n[3] Yu Song and Donglin Wang. Learning on Graphs with Out-of-Distribution Nodes. KDD 2022.\n\n\n> **Q3. \"Why does the theoretical analysis assume that the edges follow a Bernoulli distribution?\"**\n\nAs we are dealing with discrete graphs, where edges either exist or do not, the adjacency matrix values are binary, taking on either 0 or 1. This naturally aligns with the Bernoulli distribution, which is frequently employed in graph structure learning, as exemplified in several pertinent references in [1,2,3].\n\n[1] Luca Franceschi, Mathias Niepert, Massimiliano Pontil and Xiao He. Learning Discrete Structures for Graph Neural Networks. ICML 2019.\n\n[2] Pantelis Elinas, Edwin V. Bonilla and Louis C. Tiao. Variational Inference for Graph Convolutional Networks in the Absence of Graph Data and Adversarial Settings. NeurIPS 2020.\n\n[3] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang and Neil Shah. Data Augmentation for Graph Neural Networks. AAAI 2021.\n\n\n> **Q4. \"The impact of other propagation mechanisms, like higher-order diffusion.\"**\n\nExcellent suggestions! We have conducted an analysis of the hyper-parameter $k$ (order of propagation) and its impact on the AUROC, averaged across all datasets using a GCN backbone. Our findings indicate that OOD detection performance benefits from propagation orders within a reasonable range. However, excessive propagation (greater than 10) may be detrimental.\n\n\n| |  |  |  |  | |\n|:-------- |:--------:|:--------:|:--------:|:--------:| :--------:|\n|$k$|2|4|8|10|16|\n|AUROC|79.38 |80.87 |82.45|80.53 |71.19 |\n\n\n\nIn addition, we have investigated various classical propagation mechanisms, including Personalized PageRank (PPR) [1], Heat Kernel Diffusion (GraphHeat) [2], Graph Diffusion Convolution (GDC) [3], Mixing Higher-Order Propagation (MixHop) [4], and Generalized PageRank (GPR) [5]. The results (**AUROC** in percentages) demonstrate that GRASP emerges as the most effective propagation strategy in our study.\n\n| |cora| amazon| coauthor| chameleon| \n|:-------- |:--------:|:--------:|:--------:|:--------:|        \n|PPR|82.68 |83.34 |85.18 |52.79 |\n|GraphHeat\t|92.76 \t|76.99 |\t70.53 |\t58.48 |\t\n|GDC|89.48 |92.33 |96.52 |54.47 |\n|MixHop|79.80 |82.68 |86.04 |55.03 |\n|GPR|78.70 |81.57 |81.84 |52.86 |\n|**GRASP**|**94.65** |**96.76** |**97.94** |**67.97** |\n\n\n| |squirrel| arxiv-year| snap-patents| wiki |\n|:-------- |:--------:|:--------:|:--------:|:--------:|    \n|PPR|48.17 |    56.87 |46.70 |34.83 |\n|GraphHeat\t|45.63 |38.40 |\t41.00 |OOM |\n|GDC|48.82  | OOM|OOM|OOM  |     \n|MixHop|48.66 | 34.41 |28.93 |39.22 |\n|GPR|49.64 | 34.30 |29.66 |36.98 |\n|**GRASP**|**54.93** | **74.66** |**67.36** |**65.56** |\n\n\n\n[1] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Predict then Propagate: Graph Neural Networks Meet Personalized PageRank. ICLR 2019.\n\n[2] Bingbing Xu , Huawei Shen , Qi Cao , Keting Cen and Xueqi Cheng. Graph Convolutional Networks using Heat Kernel for Semi-supervised Learning. IJCAI 2019.\n\n[3] Johannes Gasteiger, Stefan Wei\u00dfenberger and Stephan G\u00fcnnemann. Diffusion Improves Graph Learning. NeurIPS 2019.\n\n[4] Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr Harutyunyan, Greg Ver Steeg, and Aram Galstyan. MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing. ICML 2019.\n\n[5] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank graph neural network. ICLR 2021.\n\n\nFinally, we would like to express our gratitude for the insightful questions raised by the reviewer. We would be more than grateful if the reviewer would like to champion our paper in the discussion phase. Thanks!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission504/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700316327873,
                "cdate": 1700316327873,
                "tmdate": 1700596131599,
                "mdate": 1700596131599,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "P45FW7ZLum",
            "forum": "R9CXfU2mD5",
            "replyto": "R9CXfU2mD5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission504/Reviewer_EGVF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission504/Reviewer_EGVF"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies graph OOD detection through OOD score propagation. The paper theoretically proves that propagation can enhance OOD detection when there are more intra-edges within ID/OOD nodes than inter-edges between ID and OOD nodes. The paper further proves that the efficacy of propagation can be improved by adding edges to nodes that have more connections to ID nodes than to OOD nodes. Then, the paper designs a simple augmentation strategy to boost the performance of propagation.  Experimental results demonstrate the effectiveness of their proposed strategy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper theoretically analyzed when OOD score propagation will work and how to boost its performance.\n- The proposed method is simple and experimental results validate its effectiveness.\n- This paper is well-written and easy to understand."
                },
                "weaknesses": {
                    "value": "- The proposed method is developed based the assumption that intra-edges dominate the graph. However, the challenge in OOD detection arises from the unknown pattern of OOD nodes. This strong assumption makes the analysis less insightful and constrains the practicality of the proposed method.\n- The experiments are conducted exclusively with one type of OOD nodes (Label Leave-Out), which limits the comprehensive evaluation of the proposed method under different distribution shifts.\n- The selection of Sid/Sood is based on the pre-computed OOD scores (using MSP), rendering the proposed method ineffective when MSP fails, as observed in dataset Squirrel in Table 2."
                },
                "questions": {
                    "value": "- According to Theorem 3.2, the propagation is deemed effective only when intra-edges dominate. Why does it seem to work well in heterophily dataset Chameleon?\n- Why does the improvement of the proposed method over GNNSafe appear to be marginal in homophily datasets like Amazon and Coauthor, while the improvement is more significant in the two heterophily datasets in Table 2?\n- How does the proposed method perform when faced with different types of OOD, such as structural manipulation and feature interpolation as addressed in GNNSafe?\n- Is the proposed method sensitive to hyperparameter k, $\\alpha$, and $\\beta$? How to choose the $\\alpha$ and $\\beta$ when MSP\u2019s performance varies in different datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission504/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698716648758,
            "cdate": 1698716648758,
            "tmdate": 1699635976929,
            "mdate": 1699635976929,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dVO7sdjzZ6",
                "forum": "R9CXfU2mD5",
                "replyto": "P45FW7ZLum",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission504/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission504/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response - Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful questions! Below we address each of your comments in detail.\n\n> **1. \"Assumption of the intra-edge domination.\"**\n\nWe respectfully point out that the reviewer's interpretation of \"*The proposed method is developed based on the assumption that intra-edges dominate the graph*\" is incorrect. The intra-edge-domination assumption is only required for the *naive propagation method* as proved in Theorem 3.2. One of the primary objectives of our algorithm is to address the hard problem of graph node ood detection in heterophilic scenarios, **where intra-edges do not dominate the graph.**\n\nFor instance, on three particularly challenging heterophily datasets\u2014Chameleon, arXiv-year, and snap-patents, the application of naive propagation method leads to a significant performance decline. In contrast, our approach not only reversed this trend but also demonstrates substantial improvement compared to pre-propagation performance. Refer to the table below for detailed results, where the performance values  are evaluated on backbone GCN. From the table we can see that our method yields a maxmium performance increase of up to **32.5** percentage points on these challenging heterophilic datasets (AUROC on arXiv-year for MSP). This robustly underscores the effectiveness of our proposed approach.\n\n| | Chameleon| |arXiv-year| |snap-patents ||\n|:-------- |:--------| :--------|:--------|:--------|:--------|:--------|\n||AUROC\t|FPR95\t|AUROC\t\t|FPR95\t\t|AUROC\t\t|FPR95\t|\n|MSP|59.91|90.87| 43.35|95.60 | 51.42|93.10| \n|MSP+prop|42.82|98.17 |  35.30|100.0  | 27.25|100.0  | \n|MSP+**GRASP**|**69.57**|**72.77** | **75.87**|**85.78** | **67.80**|**74.91** | \n|Energy\t|59.68|94.98|47.58|95.00 |46.93|93.85 |\n|Energy+prop|49.65|97.74 | 35.40|100.0   |  27.26|100.0 | \n|Energy+**GRASP**|**67.97**|**74.54**| **74.66**|**86.70** | **67.36**|**74.60** |\n\n\n> **2. \"How does the proposed method perform when faced with structural manipulation and feature interpolation as addressed in GNNSafe?\"**\n\nThanks for raising this point! We adopt the \"label leave-out\" setting since it aligns closely with practical scenarios where OOD data comes from a different class. The other two settings (structural manipulation and feature interpolation) construct OOD data through manual perturbation, which significantly differs from real-world OOD datas. Even if a method performs well on artificially synthesized OOD data, it does not necessarily reflect its real performance in real-world scenarios. Therefore, we exclusively adopt the \"label leave-out\" setting.\n\n\nAs the reviewer suggest, we also conduct experiments on the two different types of OOD (structural manipulation and feature interpolation) proposed by GNNSafe, and the results (AUROC) indicate that our method still performs well in these different constructed OOD scenarios.\n\n|\t\t\t|cora\t|\t\t|Amazon\t|\t\t|coauthor|\t\t|Average|\t  |\n|:-------- |:--------| :--------|:--------|:--------|:--------|:--------|:--------|:--------|\n||S\t\t|F \t\t|S\t\t|F \t\t|S\t\t|F \t\t|S\t\t|F    |\n|MSP\t\t\t|70.90 \t|85.39 \t|98.27 \t|97.31 \t|95.30 \t|97.05 \t|88.16 \t|93.25|\n|ODIN\t\t|49.92 \t|49.88 \t|93.24 \t|81.15 \t|52.14 \t|51.54 \t|65.10 \t|60.86|\n|Mahalanobis\t|46.68 \t|49.93 \t|71.69 \t|76.50 \t|80.46 \t|93.23 \t|66.28 \t|73.22|\n|Energy\t\t|71.73 \t|86.15 \t|98.51 \t|97.87 \t|96.18 \t|97.88 \t|88.81 \t|93.97|\n|GKDE\t\t|68.61 \t|82.79 \t|76.39 \t|58.96 \t|65.87 \t|80.69 \t|70.29 \t|74.15|\n|GPN\t\t\t|77.47 \t|85.88 \t|97.17 \t|87.91 \t|34.67 \t|72.56 \t|69.77 \t|82.12|\n|GNNSafe\t\t|87.52 \t|93.44 \t|99.58 \t|98.55 \t|99.60 \t|99.64 \t|95.57 \t|97.21|\n|**GRASP**\t\t|**95.29** \t|**98.93** \t|**98.50** \t|**98.45** \t|**99.99** \t|**99.80** \t|**97.93** \t|**99.06**|"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission504/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700315420264,
                "cdate": 1700315420264,
                "tmdate": 1700467812887,
                "mdate": 1700467812887,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ejP6gAH37o",
                "forum": "R9CXfU2mD5",
                "replyto": "P45FW7ZLum",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission504/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission504/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response - Part 2"
                    },
                    "comment": {
                        "value": "> **3. \"The proposed method may be ineffective when MSP fails, as observed in dataset Squirrel in Table 2.\"**\n\nWe would like to share a differing viewpoint from the reviewer.  The effectiveness of our method, GRASP, is evidenced through two key observations:\n\n1. As shown in Table 2, GRASP significantly surpasses MSP by a margin of **6.76** in AUROC on the Squirrel dataset using the GCN model. This improvement underscores the benefits of score propagation augmented by our method. **Notably, this enhancement is also observed in scenarios where MSP alone is ineffective**.\n\n2. On the other hand, the efficacy of our strategy is primarily measured by the extent to which it enhances Out-of-Distribution (OOD) detection performance beyond the initial OOD scores prior to propagation. Among traditional OOD detection methods, the KNN scores achieved the highest AUROC on the Squirrel dataset. However, when our propagation technique is applied to KNN, there is a notable improvement in its pre-propagation performance, as demonstrated in the table below. Furthermore, across all datasets, applying our method to KNN consistently improves performance in terms of both AUROC and FPR95. This consistently better performance across various metrics highlights the robustness of our approach. It is important to note that this improvement is observed even in cases where MSP underperforms.\n\n    ||cora\t|\t\t|amazon\t\t|\t\t\t|coauthor\t|\t\t|chameleon\t|\t\t|squirrel\t|      |\n    |:--------|:--------|:--------|:--------|:--------|:--------|:--------|:--------|:--------|:--------|:--------|\n    ||AUROC\t|FPR95\t|AUROC\t\t|FPR95\t\t|AUROC\t\t|FPR95\t|AUROC\t\t|FPR95\t|AUROC |FPR95 |\n    |KNN\t\t|81.24\t|72.29\t|86.01\t\t|60.61\t\t|91.34\t\t|47.99\t|62.09\t\t|93.43\t|56.74\t|94.42 |\n    |KNN+**GRASP**|**88.23**|**33.16**|**95.30**|**21.43**|**97.57**|**8.86** |**74.44**\t\t|**62.29**\t|**58.57**\t\t|**87.96** |\n\n\nWe are also more than happy to provide more discussion to address the concerns regarding the \"MSP reliance\" as follows:\n1.  As proved in Theorem 4.2, the conditions required for our method to work well are easy to satisfy. The chosen node set G only needs to satisfy the condition $|\\mathcal{E} \\_{G\\leftrightarrow S \\_{ID}}|>|\\mathcal{E} \\_{G \\leftrightarrow S \\_{OOD}}|$. \n2.  Our method selectively considers data points at the two ends of the score distribution. This selection minimizes the error in selecting $S \\_{ID}$ and $S \\_{OOD}$.\n\n\n> **4. \"Why naive propagration seems to work well in heterophily dataset Chameleon?\"**\n\nWe are happy to provide more clarification here.  As illustrated in Table 3, on the Chameleon dataset, both MSP and Energy exhibit **degraded** performance after naive propagation (evaluated using FPR95, where lower values indicate better results). Notably, our proposed method successfully reverses this trend, significantly improving performance. Table 3 provides a clear demonstration of this enhancement.\n\n\n\n> **5. \"Why does the improvement of the proposed method over GNNSafe appear to be marginal in homophily datasets, while the improvement is more significant in the heterophily datasets?\"**\n\nWe respectfully disagree with the reviewer's observation. Firstly, our method's improvement over GNNSafe in homophily datasets is not marginal, which can be verified by the results in the following table (Averaged on the three homophily datasets Cora, Amazon and Coauthor). From the results we can see that our method outperforms GNNSafe by **7.10** and **1.62** on FPR and AUROC respectively. Secondly, the more pronounced improvement on heterophily datasets aligns with our theoretical derivation, since the augmentation strategy aims to improve the ratio of \"intra-edges\" after propagation. Notably, if inter-edges dominate, traditional propagation methods would lead to performance degradation, while our approach successfully reverses this trend, as evidenced by certain results in Table 3.\n|\t\t|FPR95|AUROC|\n|:--------|:--------|:--------|\n|GNNSafe|22.25|94.83\t|\n|**GRASP**|**15.15**|**96.45**\t|\n\n\n> **6. \"Hyperparameter sensitive analysis and how to choose $\\alpha$ and $\\beta$ when MSP\u2019s performance varies in different datasets\"**\n\nGreat suggestions! We conduct a sensitivity analysis of all the hyper-parameters on all the datasets and the averaged results (**AUROC** in percentages) on model GCN with Energy are displayed in the following table. The performance comparison in the table for each hyper-parameter is reported by fixing other hyper-parameters. From the results we can see that within the range of chosen hyperparameter values, our proposed method's performance does not vary significantly, which constantly outperforms baselines by a large margin.\n\n\n| |  |  |  |  | |\n|:-------- |:--------:|:--------:|:--------:|:--------:| :--------:|\n| $\\alpha$ | 1 | 2.5 | 5 | 10 |15 |\n|AUROC |81.23 |81.24 |82.45|80.83 |80.35 |\n|$\\beta$|10|40|50|60|90|\n|AUROC|77.78 |81.22 |82.45|81.79  |80.12 |\n|$k$|2|4|8|10|16|\n|AUROC|79.38 |80.87 |82.45|80.53 |71.19 |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission504/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700315455214,
                "cdate": 1700315455214,
                "tmdate": 1700468636458,
                "mdate": 1700468636458,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VQBdkCCYhC",
                "forum": "R9CXfU2mD5",
                "replyto": "P45FW7ZLum",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission504/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission504/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up on the response"
                    },
                    "comment": {
                        "value": "Dear Reviewer, \n\nWe are eager to receive your feedback and any suggestions you might have. The insights from you would be invaluable in enhancing the quality of this work. If there is any additional information or clarification needed from our end, please feel free to let us know.\n\nThank you once again for your time and consideration. We look forward to hearing from you soon.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission504/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591817539,
                "cdate": 1700591817539,
                "tmdate": 1700591845907,
                "mdate": 1700591845907,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2hXIPTTzh5",
                "forum": "R9CXfU2mD5",
                "replyto": "P45FW7ZLum",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission504/Reviewer_EGVF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission504/Reviewer_EGVF"
                ],
                "content": {
                    "title": {
                        "value": "Concerns"
                    },
                    "comment": {
                        "value": "Thanks for the response that addresses part of my concerns. In your paper, Theorem 3.2 is as important as a theorem presented before technical details, and it is important to motivate and justify your method. However, after I raised my concern, in your response, you are downplaying the importance of it. This confuses me a lot. Also, on squirrel data, the question of why your method is not good is not well-justified for the heavy dependency on MSP to precompute OOD scores. Although you provided results on different types of OODs, and the results show that your method is better, it remains unclear how this experiment is done, and it is unclear if the results are reproducible, since no code is released to reviewers."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission504/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714842115,
                "cdate": 1700714842115,
                "tmdate": 1700714879143,
                "mdate": 1700714879143,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oQYywpKGze",
                "forum": "R9CXfU2mD5",
                "replyto": "P45FW7ZLum",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission504/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission504/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to R2's concerns"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback! We are pleased that our response has addressed some of your concerns, and we are more than willing to address the additional issues you have.\n\n> In your paper, Theorem 3.2 is as important as a theorem presented before technical details, and it is important to motivate and justify your method. However, after I raised my concern, in your response, you are downplaying the importance of it\n\nWe respectfully disagree with the reviewer's perspective. Theorem 3.2 indicates that only when intra-edges dominate, direct propagation on $A$ (naive propagation) can enhance OOD detection performance. However, if intra-edges do not dominate, naive propagation does not guarantee performance improvement. The significance of Theorem 3.2 lies in highlighting the limitation of naive propagation, and our method is designed to address this limitation. The experimental results provided in our previous responses serve as empirical evidence supporting these points. Therefore, we do not agree with the reviewer's statement regarding \"downplaying the importance of Theorem 3.2.\" We hope this clarification addresses the reviewer's concerns.\n\n> Also, on squirrel data, the question of why your method is not good is not well-justified for the heavy dependency on MSP to precompute OOD scores. \n\nRegarding this point, we would like to provide further clarifications:\nThe results of our proposed method presented in Table 2  (denoted as **GRASP (ours)** in the \"OOD Detection Method\" column) are the performance after propagating on the augmented $A$, using the pre-propagated Energy score. Below, we present the performance of MSP, Energy, and KNN before and after using our method respectively. As shown in the table, there is a significant improvement in performance for all datasets when various scores are propagated using our method. Notably, on the Squirrel dataset, the performance of MSP, after using our method, surpasses that of KNN, even when MSP itself is ineffective before propagation (MSP's AUROC is only 48.17 on Squirrel before propagation). This demonstrates the superiority of our method over all baselines.\n\n||cora\t|\t\t|amazon\t\t|\t\t\t|coauthor\t|\t\t|chameleon\t|\t\t|squirrel\t|      |\n|:--------|:--------|:--------|:--------|:--------|:--------|:--------|:--------|:--------|:--------|:--------|\n|method\t\t|AUROC\t|FPR95\t|AUROC\t|FPR95\t|AUROC\t\t|FPR95\t|AUROC\t\t|FPR95\t|AUROC\t\t|FPR95|\n|MSP\t\t\t|89.33 \t|52.23 \t|90.47 \t|49.52 \t|95.29 \t\t|23.87 \t|59.91 \t\t|90.87 \t|48.17 \t\t|91.99| \n|MSP+prop\t|91.78 \t|31.24 \t|95.37 \t|26.87 \t|97.70 \t\t|8.74 \t|42.82 \t\t|98.17 \t|53.26 \t\t|91.77| \n|MSP+**GRASP**\t|**95.13** \t|**19.31** \t|**96.86** \t|**13.58** \t|**97.92** \t\t|**7.80**\t|**69.57** \t\t|**72.77** \t|**57.99** \t\t|**88.10**| \n|Energy\t\t|89.48 \t|52.05 \t|92.33 \t|39.49 \t|96.52 \t\t|14.98 \t|59.68 \t\t|94.98 \t|45.06 \t\t|94.29| \n|Energy+prop\t|90.34 \t|38.03 \t|96.36 \t|23.42 \t|96.89 \t\t|11.86 \t|49.65 \t\t|97.77 \t|53.95 \t\t|91.00| \n|Energy+**GRASP**|**94.65** \t|**21.92** \t|**96.76** \t|**15.64** \t|**97.94** \t\t|**7.88** \t|**67.97** \t\t|**74.54** \t|**54.93** \t\t|**90.21**| \n|KNN\t\t\t|81.24 \t|72.29 \t|86.01 \t|60.61 \t|91.34 \t\t|47.99 \t|62.09 \t\t|93.43 \t|56.74 \t\t|94.42| \n|KNN+prop\t|84.10 \t|59.43 \t|93.01 \t|38.95 \t|96.19 \t\t|15.05 \t|61.63 \t\t|92.71 \t|58.43 \t\t|92.48| \n|KNN+**GRASP**\t|**88.23** \t|**38.49** \t|**95.30** \t|**22.59** \t|**97.57** \t\t|**8.72** \t|**74.44** \t\t|**64.61** \t|**58.57** \t\t|**88.59**|\n\nWe hope our explanations  can alleviate the reviewer's concerns.\n\nThank you for your attention!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission504/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740391612,
                "cdate": 1700740391612,
                "tmdate": 1700742203068,
                "mdate": 1700742203068,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YTXfqcZUfs",
            "forum": "R9CXfU2mD5",
            "replyto": "R9CXfU2mD5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission504/Reviewer_hJQH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission504/Reviewer_hJQH"
            ],
            "content": {
                "summary": {
                    "value": "This paper study detecting Out-of-Distribution (OOD) nodes on graphs. The authors first demonstrate through empirical and theoretical evidence that previous OOD detection methods relying on information propagation are only applicable to scenarios where the number of intra-edges is greater than inter-edges. Following this, based on their analytical conclusions, the authors propose an edge augmentation strategy called GRASP to enhance the effectiveness of these methods. Experimental results on several datasets demonstrate the effectiveness of their approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "A. This paper is well organized and written, with a clear definition of the research problem and detailed introductions of the motivation and methodology. Key conclusions are clearly marked in the paper.\n \nB. The proposed method has good generalizability. The post-processing strategy does not require retraining the model, allowing for flexible application to various existing methods and improving their effectiveness on OOD node detection task.\n \nC. The experimental results are impressive. The authors have compared their method with baselines on many datasets and conducted a detailed analysis of the experimental results, showing that the proposed method can significantly enhance OOD node detection."
                },
                "weaknesses": {
                    "value": "A. The data augmentation relies on a robust base model. During the graph augmentation, ID nodes and OOD nodes are sampled based on the base model's OOD prediction scores, which may lead to potential error propagation.\n \nB. Analysis of some key hyper-parameters is missing. It appears that the two hyper-parameters \\alpha and \\beta in the proposed method significantly affect the sampling results, and I would like to know the impact of different \\alpha and \\beta values on the results."
                },
                "questions": {
                    "value": "A. According to the authors, the number of intra-edges and inter-edges has a significant impact on OOD detection. What are the respective proportions of these two types of edges in different datasets? And what are their proportions after graph augmentation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission504/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839275227,
            "cdate": 1698839275227,
            "tmdate": 1699635976861,
            "mdate": 1699635976861,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Tp97VPUXX7",
                "forum": "R9CXfU2mD5",
                "replyto": "YTXfqcZUfs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission504/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission504/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful questions! Below we address each of your comments in detail.\n\n> **Q1. \"The base model's OOD prediction scores may lead to potential error propagation\"**\n\nWe are more than happy to provide more discussion to address the concerns as follows:\n\n1. We would like to highlight that our augmentation approach will not incorporate error-edges connecting ID and OOD, since we add edges in the training dataset and the augmentation is performed **exclusively** on in-distribution (ID) data. \n2. As proved in Theorem 4.2, the conditions required for our method to work well are easy to satisfy. The chosen node set G only needs to satisfy the condition $|\\mathcal{E} \\_{G\\leftrightarrow S \\_{ID}}|>|\\mathcal{E} \\_{G \\leftrightarrow S \\_{OOD}}|$. This condition, compared to directly augmenting edges on the test set, allows for a stronger degree of tolerance to errors.\n3. Our method selectively considers data points at the two ends of the Maximum Softmax Probability (MSP) distribution. This selection minimizes the error in selecting $S \\_{ID}$ and $S \\_{OOD}$. \n\nFor reviewer's interest, we are happy to provide comparison with an alternative approach -- augmenting edges directly on the test data. Such strategy would lead to severe error propagation, which is known as \"confirmation bias\". To validate this point, we conduct experiments comparing the effectiveness of \"directly augmenting edges on test data\" versus \"our proposed method of augmenting edges in the training dataset.\" From the results, we can see that our method has achieved an improvement of nearly **10** percentage points in AUROC compared to the approach of directly adding edges on test data, which supports our assertion.\n\n(Cells' values represent AUROC in percentages on backbone GCN with Energy Score.)\n\n| Method| Cora | Amazon | Coauthor | Chameleon | Squirrel | Average |\n|:-------- |:--------:|:--------:|:--------:|:--------:| :--------:| :--------:|\n|Before Augmetation (Energy Score)|89.48|92.33|96.52|59.68|45.06|76.61|\n|Augmentation on Test (with confirmation bias)|85.90|87.99|94.72|48.32|46.96|72.78|\n|Augmentation on Train (**Ours**)|**94.65**|**96.76**|**97.94**|**67.97**|**54.93**|**82.45**|\n\n> **Q2. \"Analysis of two hyper-parameters $\\alpha$ and $\\beta$ is missing\"**\n\nGreat suggestions! We conduct sensitivity analysis of hyper-parameters $\\alpha$ and $\\beta$ on all the datasets and the averaged results (**AUROC** in percentages) on model GCN with Energy are displayed in the following table. The performance comparison in the table for each hyper-parameter is reported by fixing other hyper-parameters. From the results we can see that within the range of chosen hyperparameter values, our proposed method's performance does not vary significantly, which constantly outperforms baselines by a large margin.\n\n\n\n|          |       |       |       |       |       |\n|:-------- |:-----:|:-----:|:-----:|:-----:| -----:|\n| $\\alpha$ |   1   |  2.5  |   5   |  10   |    15 |\n| AUROC    | 81.23 | 81.24 | 82.45 | 80.83 | 80.35 |\n| $\\beta$  |  10   |  40   |  50   |  60   |    90 |\n| AUROC    | 77.78 | 81.22 | 82.45 | 81.79 | 80.12 |\n\n> **Q3. \"The respective proportions of intra-edges and inter-edges before and after graph augmentation\"**\n\nGreat suggestions! We provide proportions of intra-edges and inter-edges after two rounds of propagations in each dataset  before and after graph augmentation respectively in the following table. From the results we can see that the proportions of intra-edges increase after graph augmentation, which lead to improved ood performance, validating the correctness of our theory. \n\n|Dataset| intra-edges ratio(Before Aug) | inter-edges ratio(Before Aug) | intra-edges ratio(After Aug) | inter-edges ratio(After Aug) | \n|:--------|:--------:|:--------:|:--------:|:--------:|\n|cora\t\t|0.8155 \t|0.1845 \t|**0.9188** \t|0.0812| \n|amazon-photo|0.6405 \t|0.3595 \t|**0.7264** \t|0.2736| \n|coauthor-cs\t|0.8105 \t|0.1895 \t|**0.8690** \t|0.1310| \n|chameleon\t|0.5082 \t|0.4918 \t|**0.5274** \t|0.4726| \n|squirrel\t|0.4972 \t|0.5028 \t|**0.5167** \t|0.4833|\n\nFinally, we would like to express our gratitude for the insightful questions raised by the reviewer. We would be more than grateful if the reviewer would like to champion our paper in the discussion phase. Thanks!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission504/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700315193473,
                "cdate": 1700315193473,
                "tmdate": 1700467297602,
                "mdate": 1700467297602,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]