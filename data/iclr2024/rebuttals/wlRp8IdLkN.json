[
    {
        "title": "Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning"
    },
    {
        "review": {
            "id": "IcQu5OFAfl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9302/Reviewer_tJXC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9302/Reviewer_tJXC"
            ],
            "forum": "wlRp8IdLkN",
            "replyto": "wlRp8IdLkN",
            "content": {
                "summary": {
                    "value": "The authors propose to use RL to learn a policy for sampling diverse instructions to generate a dataset for instruction tuning for downstream LLM alignment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "--there is an interesting idea at the core of this paper: rather than just prompting to generate new diverse instructions for your initial instruction tuning prompt set, you can actually finetune the model for generating that dataset in the first place. this kind of suggests a \"hierarchy\" of sorts in the data generation, where you generate your data on multiple levels, starting from just your action set in 3.1.1."
                },
                "weaknesses": {
                    "value": "--performance still seems a bit mixed compared to LLAMA, despite you leveraging ChatGPT/GPT4 and also WizardLM13b. Also, is the comparison to WizardLM7b in Fig 6 fair, given that you used WizardLM13b extensively in your pipeline?\n\n--similarly, it seems like you rely on having a strong instruction-tuned model already (WizardLM13b) as the \"Advanced LLM\" in Fig1 to be able to train your initial policy for sampling diverse instructions, which seems like maybe a bit of a chicken-and-egg problem. i think it might be more convincing if you were able to use a weaker model to do the initial judgments (e.g., why not use LLaMA7b, or WizardLM7b? are those not good enough for your purposes?), or show that you can later outperform whichever model you use for the initial Advanced LLM.\n\n--unless i missed it, there's no comparison on final downstream performance to any baseline that generates its instruction set just by prompting an LM rather than finetuning, which seems like it'd be the most direct baseline\n\n--there are typos/grammar errors even in the model prompts- these are arguably \u201cbugs\u201d"
                },
                "questions": {
                    "value": "--i don't understand step 4 in algorithm 1 - how is chatgpt/gpt4 also being used to help you generate the complex instructions? are you just prompting it for more instructions to add to your instruction set, in addition to what you generated previously using your smaller RL-trained model?\n\n--nit: some typos and tense changes here and there, might want to proofread"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9302/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9302/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9302/Reviewer_tJXC"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9302/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697401439167,
            "cdate": 1697401439167,
            "tmdate": 1699637171399,
            "mdate": 1699637171399,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B9SLaiYEao",
                "forum": "wlRp8IdLkN",
                "replyto": "IcQu5OFAfl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9302/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9302/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer tJXC,\n\n\nThank you for your constructive and valuable reviews, which have significantly enhanced the quality of our paper.\n\n- First, we have outlined our key contributions, which include low costs, the use of high-quality data, enhanced model privacy performance, and potential generalizability to other areas. We have also elucidated the performance improvements as illustrated in Figures 4 and 5. Furthermore, our training process requires only a limited number of samples from WizardLM-13B (371 samples for the revised version, and 896 samples for the initial version). This demonstrates that our method can significantly reduce the query count of WizardLM-13B by over 94% while achieving comparable results, indicating a more cost-effective and sustainable approach to LLM training and data quality enhancement.\n\n- Second, we have clarified performance evaluation on a well-established benchmark that can provide a more robust and objective assessment.\n\n- Third, we have described the use of ChatGPT (GPT-3.5) and GPT-4 in our work, where we developed a method for training a policy that teaches ChatGPT or GPT-4 to generate high-quality data, specifically focusing on steps 3 and 4 in the latest version of our manuscript.\n\nAs the deadline for discussion is approaching, we highly value your reviews and eagerly await your feedback. We kindly ask you to review our responses once more and let us know if they have fully or partially addressed your concerns. We are immensely grateful for your time and effort in reviewing our paper. \n\nWith gratitude,\n\nAuthors of Paper 9302"
                    },
                    "title": {
                        "value": "Looking forward to your feedback"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9302/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700351918298,
                "cdate": 1700351918298,
                "tmdate": 1700351968313,
                "mdate": 1700351968313,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GwnzbyTHuJ",
            "forum": "wlRp8IdLkN",
            "replyto": "wlRp8IdLkN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9302/Reviewer_6muu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9302/Reviewer_6muu"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel approach to generate complex instruction-tuning data through reinforcement learning. It negates the need for subsequent RLHF stages. Their method can diminish the dependence on human instructors and moderates the need for constant queries to external models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposes a novel way to evolve instructions through reinforcement learning. The experiment results on LM-Eval benchmark demonstrate the effectiveness of their method."
                },
                "weaknesses": {
                    "value": "1. The paper focuses on enhancing the instructions by iteratively optimizing the policy through RL. However, directly evolving instructions through WizardLM or Tree-Instruct prompts also avoids the need for training a large language model. The benefit brought by their method is constrained.\n2. Whether RLHF will further facilitate human alignment is not verified in this paper. Involving RL in the stage of SFT is computationally expensive."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9302/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9302/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9302/Reviewer_6muu"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9302/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830910859,
            "cdate": 1698830910859,
            "tmdate": 1699637171270,
            "mdate": 1699637171270,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AOkIsZRsyG",
                "forum": "wlRp8IdLkN",
                "replyto": "GwnzbyTHuJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9302/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9302/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer 6muu,\n\nThank you for your constructive and valuable reviews, which have significantly enhanced the quality of our paper.\n\n- First, we have outlined our key contributions (low costs, high-quality data, model privacy performance, and general for other areas potentially) and clarified the disadvantages of directly evolving instructions through WizardLM or Tree-Instruct prompts. \n- Second, our method differs from RLHF, and the training data is derived from teaching ChatGPT (GPT-3.5) and GPT-4. We believe the data from these advanced models aligns well with human values [OpenAI-learning-from-human-preferences, 2017; Christiano, P. F., et al., 2017; Ouyang, L., et al., 2022] . \n- Third, In the STF stage, our method does not require RL training, and our approach is designed to train models more efficiently using small, high-quality datasets rather than relying on large volumes of data. We kindly request that you review our responses once again and inform us whether they have fully or partially addressed your concerns.\n\nAs the deadline for discussion is approaching, we highly value your reviews and eagerly await your feedback. We are immensely grateful for your time and effort in reviewing our paper. \n\n- [OpenAI-learning-from-human-preferences, 2017] https://openai.com/research/learning-from-human-preferences, June 13, 2017.\n- [Christiano, P. F., et al., 2017] Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., & Amodei, D. (2017). Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30.\n- [Ouyang, L., et al., 2022] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... & Lowe, R. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35, 27730-27744.\n\n\nWith gratitude,\n\nAuthors of Paper 9302"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9302/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700348973650,
                "cdate": 1700348973650,
                "tmdate": 1700349311718,
                "mdate": 1700349311718,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1rOW916DJU",
            "forum": "wlRp8IdLkN",
            "replyto": "wlRp8IdLkN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9302/Reviewer_1H8F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9302/Reviewer_1H8F"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to first train a language model to generate instructions diverse from the seed. This is done using RL where the reward comes from another LM on whether or not the output instruction is of good quality. Then this model is used to generate diverse instructions, responses to which are generated by gpt-3.5 and other LMs, to create a dataset for instruction fine-tuning. When fine-tuned using this data, models like Llama2-chat-7b and WizardLM-7b are shown to improve on some benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The direction of lowering the cost of collecting instruction fine-tuning data and eliminating need for human feedback is important in making conversational LLMs more accessible."
                },
                "weaknesses": {
                    "value": "This work seems to be leveraging additional instruction fine-tuning data (see Step 4 & 5) derived from ChatGPT and GPT-4 without clearly describing how it does so in the corresponding Sections. The contribution of this work seems weak if the responses are generated primarily using external models.\n\nMixed results with marginal gains compared to the checkpoints they start with, in some cases a drop (Fig 4 & 5). TruthfulQA performance of llama-2-chat-7b is under-reported as 38.98, Table 14 from the Llama2 paper reports the performance of the 7B chat model to be at 57.04.\n\nI find it very hard to comprehend the problem being solved and the approach being proposed in this submission. At least some parts of the paper seem to be LLM-generated."
                },
                "questions": {
                    "value": "Questions on steps of the Algorithm proposed\n\nStep 1: Desigin of actions - do you simply use the same actions as proposed by WizardLM?\n\nStep 2: What does discrete value-based action space S mean?\n\nStep 3: How is TRPO used here? The binary feedback ('reward') that you get on diversity of the generated instructions is used to train the base LM, this appears to be the rejection sampling approach.\n\nStep 4 & 5: How is ChatGPT or GPT-4 used here? \n\nWhy are LLMs called Advanced LLM in Fig 1 & 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9302/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9302/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9302/Reviewer_1H8F"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9302/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699152339193,
            "cdate": 1699152339193,
            "tmdate": 1699637171161,
            "mdate": 1699637171161,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cmyZgbJ54C",
                "forum": "wlRp8IdLkN",
                "replyto": "1rOW916DJU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9302/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9302/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer  1H8F,\n\nThank you for your constructive and valuable reviews, which have significantly enhanced the quality of our paper.\n\nWe have outlined our key contributions (low costs, high-quality data, model privacy performance, and general for other areas potentially), explained models' performance on a well-known benchmark, [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), and elucidated how our method works, focusing on the main steps outlined in steps 3 and 4 of the latest version of our manuscript. \n\nAs the deadline for discussion is approaching, we highly value your reviews and eagerly await your feedback. We are immensely grateful for your time and effort in reviewing our paper. We kindly ask you to review our responses once more and let us know if they have fully or partially addressed your concerns.\n\nWith gratitude,\n\nAuthors of Paper 9302"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9302/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700346574333,
                "cdate": 1700346574333,
                "tmdate": 1700346574333,
                "mdate": 1700346574333,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]