[
    {
        "title": "Generative Modeling with Phase Stochastic Bridge"
    },
    {
        "review": {
            "id": "SSFef9xelE",
            "forum": "tUtGjQEDd4",
            "replyto": "tUtGjQEDd4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6045/Reviewer_5NhC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6045/Reviewer_5NhC"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel generative modeling framework called Acceleration Generative Modeling (AGM), which is grounded in phase space dynamics. The authors leverage insights from Stochastic Optimal Control to construct a path measure in the phase space that enables efficient sampling. The framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation, which sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. The model yields favorable performance over baselines in the regime of small Number of Function Evaluations (NFEs) and rivals the performance of diffusion models equipped with efficient sampling techniques."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The proposed AGM framework offers a new perspective on accelerating sampling in generative modeling by leveraging additional velocity information.\n\n2. The model demonstrates competitive results compared to diffusion models equipped with efficient sampling techniques, particularly in small NFE settings.\n\n3. The paper provides a clear and detailed explanation of the AGM framework, its training, and sampling procedures."
                },
                "weaknesses": {
                    "value": "1. The paper could provide more insights into the potential applications of the AGM framework beyond image generation.\n\n2. The paper could discuss potential improvements to the AGM framework, such as enhancing the training quality through data augmentation, fine-tuned noise scheduling, and network preconditioning."
                },
                "questions": {
                    "value": "Can the AGM framework be applied to other domains beyond image generation, such as natural language processing or time series data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6045/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698247538995,
            "cdate": 1698247538995,
            "tmdate": 1699636650176,
            "mdate": 1699636650176,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qsvp2NWTeS",
                "forum": "tUtGjQEDd4",
                "replyto": "SSFef9xelE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6045/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6045/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer 5NhC"
                    },
                    "comment": {
                        "value": "We deeply thank the reviewer for all the comments. The summary is accurate and the questions are interesting and helpful.\n\nPlease kindly see our itemized replies below to address the reviewer\u2019s concerns.\n\n#### **1. potential applications of the AGM framework beyond image generation.**\nThis is a great suggestion and we do believe that this framework might be particularly useful for trajectory inference tasks in which the Newtonian dynamics (phase dynamics) constraint is imposed, for example, single cell trajectory inference [2] or molecular dynamics  [3] simulation. Combining our framework with the approach described in reference [1] would not only be interesting but could also provide a scientifically robust foundation for addressing these trajectory inference challenges. \n\nRegrettably, owing to the authors' limited expertise in these particular domains, they cannot make further concrete assertions regarding the appropriateness of AGM within these domains.\n\n[1] Liu, Guan-Horng, et al. \"Generalized Schr\\\" odinger Bridge Matching.\" arXiv preprint arXiv:2310.02233 (2023).\n\n[2] Tianrong Chan et al . 'Deep Multi Marginal Momentum Schrodinger Bridge'\n\n[3] Holdijk, Lars, et al. \"Stochastic Optimal Control for Collective Variable Free Sampling of Molecular Transition Paths.\" Thirty-seventh Conference on Neural Information Processing Systems. 2023.\n\n#### **1. Can the AGM framework be applied to other domains beyond image generation, such as natural language processing or time series data?**\nSimilar to the diffusion model [1,2] for discretized variables in language generation tasks, it is possible to utilize AGM for language generation. In this case, the transition matrix $\\Phi(\\cdot,\\cdot)$ will be matrix-lized. We leave further explorations in sequence modeling as future work which we believe is an exciting direction.\n\n[1]Zhang, Yizhe, et al. \"PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model.\" arXiv preprint arXiv:2306.02531 (2023).\n\n[2] Austin, Jacob, et al. \"Structured denoising diffusion models in discrete state-spaces.\" Ad"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6045/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700108020285,
                "cdate": 1700108020285,
                "tmdate": 1700108020285,
                "mdate": 1700108020285,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "saz6dq8ncL",
            "forum": "tUtGjQEDd4",
            "replyto": "tUtGjQEDd4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6045/Reviewer_76FN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6045/Reviewer_76FN"
            ],
            "content": {
                "summary": {
                    "value": "The papers uses the tools of stochastic optimal control theory to define the forward pass for a kind of generative diffusion model strictly related to Diffusion Schr\u00f6dinger Bridge Matching. The approach combines the velocity augmentation used in Critical-damped Langevin Dynamics with the bridge approach by solving a linear Gaussian control problem in closed-form. This solution leads to relatively straight paths that are suitable for fast-sampler acceleration both in the stochastic and in the deterministic case. The method has competitive performance for small numbers of functional evaluations, but it lags behind other methods when more evaluations are used (>100)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper uses stochastic control theory effectively in order to construct a forward process with the desired properties. I believe that this is highly promising as optimal control and diffusion modeling are deeply related and many advanced control techniques can be imported in the diffusion literature using a similar approach. \n\n- The paper provides a rigorous description of the algorithm and the math behind it without requiring an excessive level of mathematical sophistication in the reader. \n\n- The experiments are rigorous and comprehensive and properly show the performance profile of the method and most relevant baselines under different conditions."
                },
                "weaknesses": {
                    "value": "- The exposition is rather dense and, as a consequence, the paper is somewhat difficult to read. This is a pity since the underlying concept are rather intuitive and can be understood by a wide audience. \n\n- As also stated by the authors, the performance of the method is inferior to several baselines for a large number of functional evaluations. However, I do not think that this is a major issue since this class of models are generally designed to work well in the low NFE range, and the results are good in this relevant range. It is quite intuitive to me that there should be a trade off between straight paths and high NFE performance, since the smoothness constraints can limit the expressivity and probabilistic coverage of the method."
                },
                "questions": {
                    "value": "I find the pseudocode in Algorithm 1 and 2 to be rather uninformative. A good pseudo-code should allow the reader to implement the algorithm almost without referring to the rest of the paper. In this case, the most important parts of the code (e.g. the form of the loss) are omitted. Could you update it to make it more self-contained? \n\n- The idea of the initial velocity conditioning  is interesting, but it is difficult to evaluate its potential without quantitative results and comparisons. Intuitively, it seems to me that it will likely lead to a substantial drop in diversity. Can you report the FID for the conditional sampler?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6045/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698406750033,
            "cdate": 1698406750033,
            "tmdate": 1699636650063,
            "mdate": 1699636650063,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZkCOolMHHC",
                "forum": "tUtGjQEDd4",
                "replyto": "saz6dq8ncL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6045/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6045/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer 76FN"
                    },
                    "comment": {
                        "value": "We extend our sincere gratitude to the reviewer for the valuable comments. Please kindly find below our itemized responses, presented in an effort to address each of the reviewer\u2019s concerns.\n\n#### **1. The exposition is rather dense and, as a consequence, the paper is somewhat difficult to read.**\nWe apologize for the difficulties caused by the dense exposition. We add one more gentle introduction of SOC in the appendix which hopefully can increase the readability.\n\n#### **2. pseudocode in Algorithm 1 and 2 to be rather uninformative.**\nThanks for this valuable suggestion. In the revised version, we have included an enhanced pseudocode that provides more comprehensive information.\n\n#### **3. For conditional generation, it seems to me that it will likely lead to a substantial drop in diversity.**\nThe velocity conditioning experiment was designed to qualitatively showcase the properties of the velocity space, but the reviewer touched upon a subtle but very interesting complication (thank you!). It is indeed leading to a substantial drop in diversity. We found that it corresponds to the value of the hyperparameter $\\xi$. We provide an ablation study in the Appendix F. When $\\xi=0$, the trajectory degenerates to the unconditional case which leads to the highest diversity with the lowest faithfulness of reference data point. When $\\xi$ is large, the diversity drops dramatically but the faithfulness increases. To achieve a balance between faithfulness and diversity, one needs to tune the hyperparameter $\\xi$."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6045/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107456804,
                "cdate": 1700107456804,
                "tmdate": 1700107456804,
                "mdate": 1700107456804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6mE5dhdyVM",
                "forum": "tUtGjQEDd4",
                "replyto": "ZkCOolMHHC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6045/Reviewer_76FN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6045/Reviewer_76FN"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nI am satisfied by the revision and I am happy that my observations were useful. As reflected in my original score, I am strongly in favor of accepting this work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6045/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575211201,
                "cdate": 1700575211201,
                "tmdate": 1700575211201,
                "mdate": 1700575211201,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9XwMoPKMEu",
            "forum": "tUtGjQEDd4",
            "replyto": "tUtGjQEDd4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6045/Reviewer_rgEV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6045/Reviewer_rgEV"
            ],
            "content": {
                "summary": {
                    "value": "The work proposes Acceleration Generation Modeling (AGM) as an extension to Critical-damped Langevin Dynamics (CLD) based on the theoretical results of stochastic optimal control. The proposed acceleration term has the effect of straightening the sample trajectories in the sampling process and reducing sampling complexity. The linearity of sampling trajectory enables the AGM generation process to take less number of evaluations and make sampling hops. AGM is compatible with both deterministic (ODE) and stochastic (SDE) samplers. Experiment results on CIFAR-10, AFHQ, and ImageNet show that AGM demonstrate competitive results with less number of evaluations with smaller number of evaluations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. Overall speaking, the proposed idea is simple yet effective and the motivation is backed by solid theoretical results in the domain of stochastic optimal control. \n2. Both quantitative and qualitative results support the motivation of AGM. Quantitative results under different settings show AGM is able to achieve competitive or better results with similar to less number of evaluations. Qualitative results also show the better ability of AGM to make sampling hops and recover the denoised images at an early stage compared to CLD. \n3. The presentation of the work is also of high quality. The introduction of the theoretical results is concise but also critical to motivate the proposal of AGM. The rest of Section 3 presenting AGM in technical details is also well-structured and easy to follow."
                },
                "weaknesses": {
                    "value": "The work does not have significant weakness. Minor weakness points include\n1. The work only shows experiment results on CIFAR-10, ImageNet 64, and AFHQv2 without scaling to higher resolution images.\n2. As the author points out in limitations, AGM is not performing as good as some existing methods especially when the number of evaluations is large. I don't think this is a major weakness as the major benefit of AGM and straight sampling trajectories is the reduced number of evaluations during sampling."
                },
                "questions": {
                    "value": "In Table 4 which shows experiment results on ImageNet 64, DDPM uses a stochastic sample while the other approaches including FM-OT, MFM, and AGM-ODE all use deterministic samplers. This may not be a fair comparison because even for the same type of diffusion model, the sample quality and sampling efficiency could be very different with different types of sampler and deterministic samplers based on ODE numerical methods generally take less number of steps than stochastic sampler. I would suggest the author include both AGM-SDE and AGM-ODE results under different number of evaluations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6045/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812997345,
            "cdate": 1698812997345,
            "tmdate": 1699636649961,
            "mdate": 1699636649961,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PxTeJnU2Us",
                "forum": "tUtGjQEDd4",
                "replyto": "9XwMoPKMEu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6045/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6045/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To reviewer rgEV"
                    },
                    "comment": {
                        "value": "We deeply thank the reviewer for all the comments. The summary is accurate and the questions are interesting and helpful.\n\nPlease kindly see our itemized replies below to address the reviewer\u2019s concerns.\n#### **1. Larger scale dataset**\nWe understand the reviewer's concern and we agree that AGM may face unexpected difficulties when scaled to larger resolutions which is also the known issue [1,2] posed for general Dynamical Generative Modeling including Diffusion Model and Flow Matching.  This issue partially (informally) explains the success of latent-space-based Diffusion Models [3,4] in high-resolution scenarios. \n\nDue to the computation resource and time limitation, we were unable to conduct experiments at larger scale about which we apologize. \n\nThis is something of high priority in our todo list for further work.\n\n[1] Jiatao Gu et al. 'Matryoshka Diffusion Models'\n\n[2] Zahra Kadkhodaie et al. 'Learning multi-scale local conditional probability models of images.'\n\n[3] Rombach, Robin, et al. \"High-resolution image synthesis with latent diffusion models. 2022 IEEE.\" CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2021.\n\n[4] Vahdat, Arash, Karsten Kreis, and Jan Kautz. \"Score-based generative modeling in latent space.\" Advances in Neural Information Processing Systems 34 (2021): 11287-11302.\n\n#### **1. Unfair comparison with DDPM and FID evaluation for different NFE**\nWe totally agree with the reviewer's suggestion and we have removed DDPM from the table. \n\nFor AGM, We reported the performance of our model at 0.8M training iteration which turns out the model is under-trained. After the submission, we kept training the model for another 0.8 iterations and now the performance has increased dramatically. We achieved FID 10.97 at 40 NFE compared with SoTA model [1] (to the best of our knowledge) which achieved 11.82 at 132 NFE. We updated the table with varying NFE as reviewer suggested. \n\nRegrettably, in our model, one can only opt to train either AGM-SDE or AGM-ODE, and it is not possible to switch between the two interchangeably like a diffusion model. We acknowledge that this limitation is one of the drawbacks of our approach. Moreover, we deeply apologize for not being able to train an AGM-SDE from scratch during the rebuttal phase, primarily due to the significant training complexity associated with ImageNet. Given our current computational resources (8 x Nvidia A100), training an AGM-SDE from scratch would require approximately 8 weeks [1], for which we sincerely apologize.\n\n[1] Karras et al. \"EDM\""
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6045/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107294859,
                "cdate": 1700107294859,
                "tmdate": 1700108049516,
                "mdate": 1700108049516,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pFGsU7EJ91",
            "forum": "tUtGjQEDd4",
            "replyto": "tUtGjQEDd4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6045/Reviewer_gcJ1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6045/Reviewer_gcJ1"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed the acceleration generative model (AGM), which is Bridge Matching method with a dynamics-based diffusion model with stochastic optimal control (SOC) theory that rectifies the trajectory of the second-order momentum dynamics which is first introduced in CLD. First, the optimal acceleration function is derived to the solution of the stochastic bridge problem, which is given by minimizing the SOC objective function. Different from CLD whose velocity field is defined by the score function of the pre-defined critically-damped Langevin diffusion process, the velocity field of AGM is learned to rectify the particle trajectory. This SOC problem is designed to minimize the distance between the ground-truth (GT) destination and the trajectory destination. Then like in CLD, this paper took advantage of the momentum-based approaches and proposed that the sampling-hop, the estimated data point $x_1$ given the early sampling stage outputs, is predicted more accurately compared to existing methods. In the empirical experiments, the sampling quality is improved especially in the low-NFE regime."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The idea of rectifying the particle trajectory with the velocity field is an intuitive approach, which is widely used in the literature. Existing works used handcrafted way to design the velocity field, but this paper aimed to both optimize this part of the stochastic process by using the SOC theory.\n* The objective is well-defined: when the control regularization term approaches to zero, then the objective directly turns into the square mean\n* Even though the velocity should be trained, the whole training process is simulation-free: we do not need any further simulation process like in current SoTA models that require further self-distillation for high-quality image generation in low-NFE regime. Furthermore, this method can be pipelined with the distillation techniques like other diffusion model methods."
                },
                "weaknesses": {
                    "value": "* The clarity of the paper will be better if the conditions of the Lemmas and Propositions written in this paper is stated more concretely and with full notations, especially in the appendix.\n  - In the sampling-hop part, the writing does not fully cover how the sampling-hop is more accurately evaluated compared to the CLD case. Both this method and CLD make predictions of the data from both the current state and the velocity, while the compared EDM (Figure 2) does it from state alone.\n  - In the Probabilistic ODE part of (7), an additional notation rather than $g(t)$ is recommended to be used, like $g_t \\to h_t$ in the matrix notation and $g(t)=h_t$ for BM-SDE part. Because the notation $g(t)$ or $g_t$ is abused, it can be misleading that the score term of the probabilistic ODE is neglected.\n* The SOC theorem is only used limitedly; the regularization in terms of $\\int ||a_t||^2$ is ignored and this can threaten the stability of the acceleration space, even though this is not directly revealed in the paper.\n* Whereas the theoretical background is sound and the improved performance is guaranteed, the hyperparameters such as the diffusion coefficients and the SDEs are not optimized, which causes its lacking performance compared to EDM (look at Figure 5). However, this is expected to be enlightened with further works."
                },
                "questions": {
                    "value": "* It will be helpful if the acceleration coefficient $a_t$ for image datasets is depicted, as the momentum of how the image data is being generated in Figure 1 or Figure 2. It is expected that the acceleration coefficients show similar semantic features like $x_t$ and $v_t$, but have varying scales.\n* Can you provide elementary introduction of the stochastic optimal control? While this paper works only the simple case of the SOC (no regularization case), introducing some details, or at least some introductory materials will help the readers to follow up the backgrounds.\n* I guess that the ImageNet64 performance is not yet optimized: the generative performance of the SoTA models are expected to be much better than the paper have proposed. I think at least the performance should be compared with CLD-SGM from the same architecture.\n\n================================\n\n* It will be helpful for the readers' understanding if you use the colored hyperlinks by reference (\\ref) or citation (\\cite, \\citep, \\citet) commands."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6045/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6045/Reviewer_gcJ1",
                        "ICLR.cc/2024/Conference/Submission6045/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6045/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699378542971,
            "cdate": 1699378542971,
            "tmdate": 1700686692584,
            "mdate": 1700686692584,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZBiYJWyHsb",
                "forum": "tUtGjQEDd4",
                "replyto": "pFGsU7EJ91",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6045/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6045/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer gcJ1"
                    },
                    "comment": {
                        "value": "We would like to express our sincere gratitude for your valuable feedback and comments. We truly appreciate the time and effort you invested in assessing our submission.\n\nPlease kindly see our itemized replies below in order to address the reviewer\u2019s concerns.\n\n#### **1. The clarity of the paper will be better if the conditions of the Lemmas and Propositions written in this paper is stated more concretely**\nThank you for raising this issue. we have made revisions to the notations and refined the statements of the lemmas and propositions (Lemma 7 and Section D.10 in appendix, Proposition 5 in main paper).\n#### **2. Both this method and CLD make predictions of the data from both the current state and the velocity, providing the data prediction result from CLD.**\nThank you for mentioning this aspect which is indeed valuable. \n\nIt has come to our attention that the training objective of CLD solely focuses on the score function with respect to velocity (scaled $\\epsilon_1$). Consequently, there is no apparent method for CLD to effectively reconstruct the data point using the intermediate state and velocity. Specifically, in order to reconstruct $x_1$,the information of stochasticity in the position channel, denoted as $\\epsilon_{0}$, is required. However, it is important to note that this information is not available after the training phase of CLD. Therefore, the reconstruction of $x_1$ is not feasible.\n\nIn our case, the special structure of acceleration, which includes the stochasticity information of state and velocity channel (eq.9 in revision), allows us to reconstruct the $x_1$ (Proposition.5).\n\nTo address any confusion resulting from our previous insufficient explanation, we have included additional clarification in Section 3.2 in revision.\n\n#### **3. Notation abuse in eq.7.**\nThank you for careful reading! We have fixed this issue in the revision!\n#### **4. provide an elementary introduction of the stochastic optimal control?.**\nThanks for the valuable suggestion. We have provided a gentle introduction of stochastic optimal control in the appendix C.\n#### **5. the ImageNet64 performance is not yet optimized,  the generative performance of the SoTA models are expected to be much better than the paper has proposed.**\nThe experiment we evaluate on is the **unconditional** imagenet-64 and we do not have computation resources to conduct the training on this dataset for all baselines. As a result, We report the performance of baseline models from a recent paper (see Table.8 from [1]).\n\nFor AGM, We reported the performance of our model at 0.8M training iteration which turns out that the model was under-trained. After the submission, we kept training the model for another 0.8M iterations and the performance increased dramatically. We achieved FID 10.97 at 40 NFE compared with SoTA model [1] (to the best of our knowledge) which achieves 11.82 at 132 NFE. \n\nWe kindly invite the reviewer to suggest any other baselines we are missing, and we are more than happy to incorporate them into our paper.\n\n[1] Pooladian, Aram-Alexandre, et al. \"Multisample flow matching: Straightening flows with minibatch couplings.\" arXiv preprint arXiv:2304.14772 (2023).\n#### **6. HyperLink color**\nWe have added the hyperlink color back in the revision for better visualization as the reviewer suggested!\n\n#### **7. Trajectory of acceleration**\nWe have added the plot of position, and velocity together with acceleration in the appendix G in the revision. Upon reviewing Appendix G, it becomes evident that the trajectories of the variables remain consistent over varying NFEs, despite the trajectories of any variable not being perfectly linear in the infinite timestep limit.\n#### **8. The problem formulation ignores the regularization of acceleration**\nWe would like to respectfully stress that the regularization of the acceleration $||a_t||_2^2$ is not ignored in our framework (see, eg, Equation 5).\n\nTo be more precise, as $r$ approaches positive infinity, there still exist multiple possible stochastic processes which start from $x_0$ at $t=0$ and converge to $x_1$ at $t=1$ and all of them are minimizers of the objective function **without** regularization. Among these processes, our preferred stochastic process should also minimize the control effort $\\int_{0}^{1}||a_t||_2^2 dt$ (regularization). This solution, which aligns with the unique solution presented in Definition 2 (In general, the solution of SOC is not unique. But in our simple Linear Quadratic case, the solution is unique, see Theorem 6.1 in [this textbook](https://www.control.utoronto.ca/people/profs/kwong/ece410/2008/notes/chap6-08.pdf)), is what we have employed to construct our methodology.\n\nTo address any confusion that may have arisen, we have provided additional clarification in Appendix C.1 in the revision. This supplementary section aims to resolve any ambiguities and enhance the understanding of the subject matter."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6045/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107129968,
                "cdate": 1700107129968,
                "tmdate": 1700107129968,
                "mdate": 1700107129968,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mgD1dLIRd8",
                "forum": "tUtGjQEDd4",
                "replyto": "ZBiYJWyHsb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6045/Reviewer_gcJ1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6045/Reviewer_gcJ1"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "1. Thank you for the detailed response, especially for the regularization of acceleration part in the problem formulation. According to my understanding, the acceleration term $\\|\\|a_t\\|\\|_2^2$ was ignored since $R$ goes to infinity. If there exists multiple processes that satisfies the condition without regularization, then your explanation makes sense. Specifically, I am greatly impressed on the complement of abundant materials in the \n\n2. We also appreciate you to precisely mention the difference between CLD and AGM, and I think that this part is one of the key aspect that AGM has improved the CLD-SGM.\n\nSince all the important issues are resolved and since I considered that the concept of SOC should be an important blueprint for continuous-time generative model, I raise my review score and suggest this paper as the core contribution of the conference.\n\n* * *\n\n> Minor clarifications\n\n* The state and velocity term in Proposition 5 looks to be mis-compiled. It should be corrected in the camera-ready version."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6045/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686675925,
                "cdate": 1700686675925,
                "tmdate": 1700686675925,
                "mdate": 1700686675925,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]