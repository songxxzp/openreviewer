[
    {
        "title": "Unpaired Panoramic Image-to-Image Translation Leveraging Pinhole Images"
    },
    {
        "review": {
            "id": "SdJR33l6aa",
            "forum": "bRm0rul3SZ",
            "replyto": "bRm0rul3SZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission581/Reviewer_7NBx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission581/Reviewer_7NBx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new approach for unpaired panoramic image-to-image translation, which addresses the challenges of modifying naive 360-degree panoramic images using readily obtainable pinhole images as the target domain for training. The authors introduce a new model that leverages spherical position embedding and sphere-based rotation augmentation to address discontinuities at the panorama edges. The proposed method is evaluated on several datasets and compared with state-of-the-art methods, demonstrating superior performance in terms of both quantitative metrics and visual quality."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method is well-motivated and addresses the problem in the field of panoramic image processing. The authors provide a clear and detailed description of the model architecture and training procedure, which makes it easy to reproduce the results. The experimental evaluation is thorough and includes both quantitative and qualitative analysis, which demonstrates the effectiveness of the proposed method. The use of spherical position embedding and sphere-based rotation augmentation is a novel contribution that significantly improves the performance of the model."
                },
                "weaknesses": {
                    "value": "1: Enhancing the motivation behind the task is crucial; specifically, showing practical applications of Panoramic Image-to-Image Translation would be great.\n\n2: How can the proposed method  help downstream applications such as AR/VR and driving? It would be great to see more results and discussions on this part.\n\n3: Delving into the computational aspects, including time, memory, and overall efficiency, and comparing these metrics with other techniques, would add substantial value to the discussion.\n\n4: It appears that the Ensemble technique and SPE deform conv have a relatively minor impact on the overall pipeline. Given this, I recommend conducting an ablation study, progressing step by step, as demonstrated in Table 1 of the StyleGAN paper (\"A Style-Based Generator Architecture for Generative Adversarial Networks\") to better presents the contribution of each module."
                },
                "questions": {
                    "value": "Please see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission581/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697905632571,
            "cdate": 1697905632571,
            "tmdate": 1699635985559,
            "mdate": 1699635985559,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1qtBbZd1gV",
                "forum": "bRm0rul3SZ",
                "replyto": "SdJR33l6aa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission581/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission581/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment to Reviewer 7NBx"
                    },
                    "comment": {
                        "value": "We highly appreciate your constructive comments. Our response is shown below. If our responses do not adequately address your concerns, please let us know and we will get back to you as soon as possible.\n\n### Practical applications\n\nSince existing I2I methods on outdoor scenes are often adopted for data augmentation for autonomous driving, our task can be directly applied for data augmentation for adverse conditions in AR/VR, autonomous driving, and street view services.\nTo demonstrate this, we have visualized the results of object detection in Appendix G on translated panoramas using YOLOv5, showing our method excels in preserving objects.\nAlso, some of the motivations from our work can be also adopted in another tasks, *e.g.,* domain adaptation for semantic segmentation.\n\n### More analysis on ablations\n\nWe apologize that we could not conduct additional ablation studies for our SPE and ensemble technique at this time, due to the lack of training resources and time. We will add this in the final version.\n\nWe believe that SPE helps preserve structural continuity, since SPE is applied at feature-level. The ensemble technique is conducted at image-level, so it works for style continuity at the left-right boundaries of an image.\n\n### Comparisons on model complexity\n\nWe compare an inference time and the number of the parameters below. It shows a comparison on inference time per 100 images and the number of parameters. While ours has more parameters since our method is based on ViT, it achieves competitive time consumption.\n\n|  | CUT | FSeSim | MGUIT | InstaFormer | Ours |\n| --- | --- | --- | --- | --- | --- |\n| Time per 100 images (sec.) | 40.36 | 40.19 | 39.20 | 43.02 | 39.58 |\n| # of param (M.) | 14.7 | 14.3 | 49.3 | 75.5 | 74.3 |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission581/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700445922962,
                "cdate": 1700445922962,
                "tmdate": 1700452421613,
                "mdate": 1700452421613,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kSqLtPoYm9",
                "forum": "bRm0rul3SZ",
                "replyto": "1qtBbZd1gV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission581/Reviewer_7NBx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission581/Reviewer_7NBx"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Thank you for providing insights into the practical applications of visual recognition tasks and presenting results.Also its good to see the inclusion of comparisons regarding complexity. \n\nThe response has mostly addressed my concerns, and I am inclined to maintain my rating as 5."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission581/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578417833,
                "cdate": 1700578417833,
                "tmdate": 1700578417833,
                "mdate": 1700578417833,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3jnwuxNIY4",
            "forum": "bRm0rul3SZ",
            "replyto": "bRm0rul3SZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission581/Reviewer_4pKv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission581/Reviewer_4pKv"
            ],
            "content": {
                "summary": {
                    "value": "This work designed a new method for panorama style transfer. Unlike natural images, panorama has several differences, such as distortion. To bridge the gap between image-to-image translation approaches and panoramic images, this submission proposed distortion-aware panoramic modeling techniques and distortion-free discriminators for this problem. Experimental results on some benchmarks show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Though image-to-image translation (style) is a typical problem, this paper focuses on a new setting, style transfer for panorama. I believe there would be a lot of applications of panoramic image style transfer in XR.\n- Given the differences between panorama and natural photos, this paper designed some modules for panorama, and experiments show improvement over previous methods."
                },
                "weaknesses": {
                    "value": "- The writing can be further improved and polished to make the submission stronger. Some equations such as EQ(8)(9) confuse me, as there are so many symbols and notations. It would be better to include more insights and explanations in the context rather than listing equations. Also, Fig. 3 is hard to follow and I would suggest the authors re-organizing it. Which parts are Stage I? Which parts are Stage II? What do notaitons mean in Fig.3? It would be better to make the figure self-contained. \n- Baselines. The tasks of Day-to-Night and Day-to-Rain are very similar to unpaired image-to-image translation tasks in CycleGAN and many follow-up works. This task is also related to image style transfer, as there is a content panorama and a style pinhole image. Could you compare your method with some style transfer works? Also, the baseline results in Fig.4 are too bad, and the shapes are even not preserved. I would suggest double-checking the baseline results and comparing to some up-to-date image-to-image translation works with diffusion models."
                },
                "questions": {
                    "value": "- Could you show some failure cases of the proposed method?\n- The submission only shows results on two types of translation, i.e., Day-to-Night, Day-to-Rain. This makes it difficult to determine whether the proposed method is general. Could you show more results on more styles?\n- Compared to pretrained methods. One motivation of this paper is it is difficult to collect paired data for training. Could you discuss the pros and cons compared with methods with pretraining [1]?\n[1] Pretraining is All You Need for Image-to-Image Translation. 2022"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission581/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission581/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission581/Reviewer_4pKv"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission581/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698038362960,
            "cdate": 1698038362960,
            "tmdate": 1699635985477,
            "mdate": 1699635985477,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RIRDQOG0r1",
                "forum": "bRm0rul3SZ",
                "replyto": "3jnwuxNIY4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission581/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission581/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment to Reviewer 4pKv"
                    },
                    "comment": {
                        "value": "We highly appreciate your constructive comments. Our response is shown below. If our responses do not adequately address your concerns, please let us know and we will get back to you as soon as possible.\n\n### Writing, equations, notations\n\nWe agree that equations and notations are complicated. We will further elaborate our writing.\nWe kindly refer the reviewer to CUT[ECCV'20], FSeSim[CVPR'21] for Eq(8) and Eq(9), as we borrowed the equations from those literature. Also, we kindly remind that we have provided the meaning of notations in Appendix A.\nAlso, please refer to the pseudo-code illustrating the training algorithm of each stage in Appendix D.1 and D.2 for more details.\n\n### Fig.3 is complex\n\nWe noticed that the architecture (Fig. 3) that illustrates Stages 1 and 2 could lead to a misunderstanding. To further reflect your comment, we added simplified diagrams for each stage in Appendix D.3.\n\n### More comparisons with other methods (style transfer, Diffusion models)\n\nThank you for your constructive comment. Following this, we conducted experiments and added comparisons with SoTA style transfer methods (EFDM, DTP) in Appendix E.3.\nOur method clearly outperforms other methods in terms of style translation regarding the context and content preservation, since they fail to capture semantic relation between source and target due to their large domain gap.\n\nAlso, we kindly remind you to Appendix E.3 which includes qualitative and quantitative comparisons with more recent I2I methods based on GANs (GP-UNIT, Decent) and recent image editing methods based on Diffusion models (IP-Adapter, T2IAdapter).\n\n### Suggestion for double-checking the baseline results\n\nThanks for the suggestion. We conduct experiments using default hyper-parameter settings for other methods. Also, as our architecture design is based on one of the comparison methods (InstaFormer), we are convinced that the results of comparison methods are not wrong.\n\n### Failure cases\n\nPlease refer to Appendix G to find the limitations and failure cases.\n\n### More translation results except for day-night and day-rainy\n\nWe kindly remind you that our results on the Dark Zurich dataset (night, twilight) are in the main paper and Appendix.\n\n### Comparison with pretrained methods\n\nWe respectfully remind you to see Appendix E.3; we have also provided comparisons with recent diffusion-based image editing methods that are based on pretrained networks.\nFollowing your comment, we also added qualitative comparisons with the suggested work (PITI) in Appendix E.3. For generating semantic masks, we adopt Segment Anything.\n\nAs shown in Fig.15 in Appendix, although PITI has strength in maintaining the panoramic structure compared to existing I2I methods, it cannot capture any semantic information from the source, and outputs unrelated content. Also, it generates unseen objects and unrelated content in the background, as seen in both results.\nIn contrast, our results successfully capture the structure and semantic information from the source, while translating into desired conditions (day\u2192rainy and day\u2192night, respectively).\nNote that PITI only takes a source as the input, and it is unable to control the style."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission581/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700445877820,
                "cdate": 1700445877820,
                "tmdate": 1700452411817,
                "mdate": 1700452411817,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ghtB0itXBH",
                "forum": "bRm0rul3SZ",
                "replyto": "3jnwuxNIY4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission581/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission581/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Hope to get the feedback."
                    },
                    "comment": {
                        "value": "Dear reviewer 4pKv,\n\nSorry for bothering you. We genuinely appreciate the time and effort you've invested in our paper. As our rebuttal and the revision of our paper have been submitted for a while, we do want to know whether our explanations have settled the concerns. We are eager to have further discussions, so please let us know if you have additional feedback.\n\nBest,\nAuthors of submission 581."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission581/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641272767,
                "cdate": 1700641272767,
                "tmdate": 1700641272767,
                "mdate": 1700641272767,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5KWRFBtLRe",
            "forum": "bRm0rul3SZ",
            "replyto": "bRm0rul3SZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission581/Reviewer_HC91"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission581/Reviewer_HC91"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors propose a new task, to learn the mapping from unpaired panoramic source to non-panoramic target. They propose a new I2I model for it. The experimental results show the effectiveness of their model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is well-written and easy to follow. The new task is interesting. They have done comparison and ablation studies to validate their proposed components."
                },
                "weaknesses": {
                    "value": "However, there the following concerns to prevent me vote for acceptance. \n\nThe first one is the unfair comparison, and some examples might not be sufficient supports. In Fig. 2, the FSeSim is designed for planar images, not panoramic images. If directly using FSeSim in panoramic images, it\u2019s unfair comparison. The authors should compare the style transfer ability by using cube map project, which transfers the sphere data into six faces. And FSeSim is used on each face image and finally combined. In this way, the comparison is fair. Similar in Fig. 4, compared SOTA are designed for planar images, and it\u2019s unfair in this directly inference. \n\nThe second one is the used projection, where the better one is cube map, not ERP. Using cube map projection, the main things is to make consistency between six face, even for the existing method."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission581/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698506127203,
            "cdate": 1698506127203,
            "tmdate": 1699635985407,
            "mdate": 1699635985407,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "azF8auC6D1",
                "forum": "bRm0rul3SZ",
                "replyto": "5KWRFBtLRe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission581/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission581/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment to Reviewer HC91"
                    },
                    "comment": {
                        "value": "We highly appreciate your constructive comments. Our response is shown below. If our responses do not adequately address your concerns, please let us know and we will get back to you as soon as possible.\n\n### Unfair comparisons\n\nRegarding that, this is the first work on both 1) image translation for panoramas, and 2) image translation for panoramas leveraging pinhole images as target, we believe that our comparison pipeline (training existing SoTA I2I methods with panoramic source and pinhole target) was the best choice.\nAlso, there is no prior work on image translation using cubemap projection.\n\n### Cubemap projection\n\nThank you for your suggestion. Following it, we conducted an experiment using cubemap projection for FSeSim, shown in Appendix E.3.\nWhile it shows better results than original FSeSim, unfortunately, we observed that this setting did not converge as we desired. Since each cubemap contains a different content distribution and it also shows a large discrepancy compared to the pinhole image source, some cubemaps naively become to follow the target distribution without considering the context of the source. In addition, it contains artifacts and undesired patterns in the output image, and shows structural discontinuity and style inconsistency at the boundaries.\n\nAdditionally, after projecting the generated cubemaps into an equirectangular, the structural discontinuity and style difference at the boundaries seem worse than the original FSeSim.\n\nIn addition to the observation from the experimental results, we respectfully disagree that cubemap projection is optimal for image translation for two reasons.\nCubemap projection could be an optimal design choice if both source and target are panoramic images since they have the same content distribution. However, in our task, *i.e.,* *generating panoramic outputs with panoramic source and pinhole target images*, the source and target have critically different content distribution.\nAfter cubemap projection for a full-FoV panorama image, some of the projected cubemaps exhibit a content distribution that differs from that of the target pinhole image (i.e., containing only sky or road), thus forcing output cubmaps to follow the distribution from pinhole target images (we visualize this case in Appendix). If we train existing I2I methods with those cubemaps as source and pinhole images as a target, where the source and target often contain unrelated content distributions, it would fail to align with the desired objectives and tend to cause to mode collapse.\n\nAlso, it brings out another challenge of maintaining both structural continuity and global style consistency with the desired style, which has not been explored yet."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission581/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700445812544,
                "cdate": 1700445812544,
                "tmdate": 1700466680890,
                "mdate": 1700466680890,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7vaQ3LBpP1",
                "forum": "bRm0rul3SZ",
                "replyto": "5KWRFBtLRe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission581/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission581/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Hope to get the feedback"
                    },
                    "comment": {
                        "value": "Dear reviewer HC91,\n\nSorry for bothering you. We genuinely appreciate the time and effort you've invested in our paper. As our rebuttal and the revision of our paper have been submitted for a while, we do want to know whether our explanations have settled the concerns. We are eager to have further discussions, so please let us know if you have additional feedback.\n\nBest,\nAuthors of submission 581."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission581/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641243257,
                "cdate": 1700641243257,
                "tmdate": 1700641243257,
                "mdate": 1700641243257,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TOXyFFJTMR",
            "forum": "bRm0rul3SZ",
            "replyto": "bRm0rul3SZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission581/Reviewer_jBQQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission581/Reviewer_jBQQ"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a panoramic I2I task by translating panoramas with pinhole images as a target domain. In particular, a versatile encoder and distortion-free discrimination are designed to efficiently bridge the large domain gap between panoramic and pinhole images. To address the discontinuities at the panorama edges, the strategies of spherical position embedding and sphere-based rotation augmentation are proposed. Experimental results demonstrated that the proposed work significantly outperforms the previous methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ To my knowledge, this is the first work that leverages pinhole images to guide the panoramic I2I task. It well addresses two challenges: directly applying conventional I2I methods cannot perceive the specific geometric distortion in panoramic images; panoramic image translation is the absence of sufficient panorama datasets covering diverse conditions.\n+ The supplementary materials allow quite convincing supports for this work. More implementation details, network architectures, comparison evaluations, and deep analyses of experiments are offered.\n+ The experiments are comprehensive, and some comparison samples look promising."
                },
                "weaknesses": {
                    "value": "- While this work provides the first step for the panoramic I2I task by translating panoramas with pinhole images as a target domain, the novelty seems kind of limited from my perspective. For example, the adaptation from source (pinhole) dataset with annotated label to target (panoramic) dataset (Mutual Prototypical Adaptation) has already explored in \"Bending Reality: Distortion-aware Transformers for Adapting to Panoramic Semantic Segmentation\"; the similar panoramic modeling strategy in encoders has been exploited in \"Disentangling Orthogonal Planes for Indoor Panoramic Room Layout Estimation with Cross-Scale Distortion Awareness\" and \"Spherical Convolution\"; the discontinuity elimination strategy is also studied in \"Cylin-Painting: Seamless 360\u00b0 Panoramic Image Outpainting and Beyond with Cylinder-Style Convolutions\", \"Diverse plausible 360-degree image outpainting for efficient 3dcg background creation\", \"Spherical Image Generation From a Few Normal-Field-of-View Images by Considering Scene Symmetry\", etc. Please briefly review the above literature and discuss your customized contribution beyond these baselines.\n- In Figure 3, the authors claimed that Stage I only learns to reconstruct the panorama source, and Stage II learns panorama translation. However, it is hard to discriminate the difference between stage I and stage II in the figure.\n- For the architecture, the motivation of using a shared content encoder and a shared style encoder to learn both panoramic images and pinhole images is ambiguous and unclear, given the context in which their domains significantly differ.\n- Did the comparison method retrain on the panoramic images? More implementation details are expected to be provided."
                },
                "questions": {
                    "value": "Could the pinhole-image-guided panoramic method be extended to other tasks, such as panoramic image inpainting and outpainting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission581/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739195477,
            "cdate": 1698739195477,
            "tmdate": 1699635985338,
            "mdate": 1699635985338,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sKp4OscIwh",
                "forum": "bRm0rul3SZ",
                "replyto": "TOXyFFJTMR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission581/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission581/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment to Reviewer jBQQ (1/2)"
                    },
                    "comment": {
                        "value": "We highly appreciate your constructive comments. Our response is shown below. If our responses do not adequately address your concerns, please let us know and we will get back to you.\n\n### The novelty seems kind of limited from my perspective.\n\nWe would like to clarify that our method differs from the mentioned works and additional technical contributions.\n\n> Domain adaptation for segmentation (\"Bending Reality\")\n\nSuch works on domain adaptation for segmentation require class-wise labels since the loss functions and training strategies are mostly based on class-wise information, and they focus on aligning the distributions between source and target class-wise.\nWhereas our task does not exploit any class label or bounding boxes within an image, we only use the information about per-sample conditional annotations (e.g., daytime, night, rainy).\n\nAnother difference is that one main point for the mentioned task is the **style similarity** for the segmentation map for the source and target, as seen in ``Learning to Adapt Structured Output Space for Semantic Segmentation[CVPR'22]'''s mention that \"While images may be very different in appearance, their outputs are structured and share many similarities.\" \nIn contrast, our task poses challenges as the source and the target have different styles and different distortions. Therefore, one of the main points of our work is to disentangle content and style for source and target and generate a translated image considering left-right structural- and style-continuity. To do so, we have proposed distortion-free discrimination and adopted several techniques (spherical PE, rotational augmentation, and fusion).\nFor those reasons, the approaches from such works (prototype, clustering, adversarial learning) cannot be directly adopted into image translation tasks.\n\n> Similar panoramic modeling strategy in encoders (\"Disentangling Orthogonal Planes for Indoor Panoramic Room Layout Estimation\", \"Spherical Convolution\")\n\nAs mentioned in our paper, we indeed adopted the deformable convolution motivated by existing works (PanoFormer, PAVER).\nThough our deformable conv is inspired by PAVER, the motivation is significantly different. 1) We handle panorama source and pinhole target inputs using heterogeneous encoders but with different offsets for deformable convolutions and different PE techniques to reduce the geometrical gap. Still, we process them via a shared network to align them in a shared representation space, which helps to disentangle content and style better.\nIn addition, this approach is efficient as the same encoders learn the compact representation that explains both types of images.\nWe would like to emphasize that there has been no prior research utilizing a shared network while adjusting offsets based on whether it is the source or target, considering distortion in both panorama and pinhole within the content encoder and discriminator.\n\n> Discontinuity elimination strategy (\u201dCylin-Painting\u201d, Diverse plausible 360-degree image outpainting for efficient 3dcg background creation,'' ``Spherical Image Generation From a Few Normal-Field-of-View Images by Considering Scene Symmetry'')\n\nThe paper ``Diverse plausible 360-degree image outpainting for efficient 3dcg background creation'' employs a Transformer-based approach to address left-right discontinuity through circular padding and circular inference, which is a commendable method. However, in our work, we introduce the absolute Spherical Patch Embedding (SPE) as a straightforward solution. Additionally, we extend our focus beyond panorama images by handling pinhole images with a shared network. To achieve this, we divide the Patch Embedding (PE) into Spherical PE and fixed PE, applying them separately to the source and target. Notably, we share the parameters of the Transformer between the two types of inputs. In contrast, the method proposed in this paper is applicable **only to panorama images** due to variations in patch numbers for a panorama and a pinhole image and even involves more parameters for the Transformer than our SPE.\n\nIn the case of \"Cylin-Painting,\" the paper provides an analysis solely on Sinusoidal PE and Padding PE, but not Spherical PE.\n\nFinally, the last paper also employs circular padding.\n\nIn addition, we address other technical novelty as below:\n\n> Additional technical novelty\n\nAlso, we emphasize our differentiable rectilinear projection is prominent to deal with the distortion and distribution differences between source/target, aligning them for the discriminator and adversarial losses.\n\nIn addition, our motivation for the ensemble is to specifically resolve style discrepancy at boundaries in panoramic I2I, considering rotational equivariance in panoramas(Eq.5), which has not been explored yet, compared with existing works that ensemble different types of projections (UniFuse, BiFuse, OmniFusion) or tangent images into a panorama (360MonoDepth), often requiring additional blending techniques."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission581/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700445574043,
                "cdate": 1700445574043,
                "tmdate": 1700452369757,
                "mdate": 1700452369757,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3Uphf7LDRJ",
                "forum": "bRm0rul3SZ",
                "replyto": "TOXyFFJTMR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission581/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission581/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment to Reviewer jBQQ (2/2)"
                    },
                    "comment": {
                        "value": "### Fig.3 is complex\n\nWe noticed that the architecture (Fig. 3) that illustrates Stages 1 and 2 could lead to a misunderstanding. To further reflect your comment, we added simplified diagrams for each stage in Appendix D.3.\nAlso, please refer to the pseudo-code illustrating the training algorithm of each stage in Appendix D.1 and D.2 for more details.\n\n### Details on shared encoders\n\nWe noticed that Fig.3 appears to be complicated and may have led to your misunderstandings.\n\nWe handle panorama source and pinhole target inputs using heterogeneous encoders but with different offsets for deformable convolutions and different PE techniques to reduce the geometrical gap. For pinhole inputs, we set zero for the offset values (\u0398\u2205), so it works as vanilla convolution when patchfying the inputs. In contrast, for panorama inputs, we set \u0398ERP for the offset.\nAlso, it should be noted that we have a style encoder only for the target style in Stage 2 with \u0398\u2205, while Stage 1 only contains the style encoder with \u0398ERP for source style (daytime).\nMore specifically, we can divide Stage 2 into two processes: (1) image translation with a pinhole target and panoramic source, (2) image reconstruction with the pinhole image as source and target.\nIn (1), we use randomly generated style code from Gaussian distribution and process it into AdaIN, so in this process, a style encoder is not used.\nIn (2), we aim to reconstruct pinhole image input. Thus, we extract the style from the pinhole input using a style encoder, and process it into AdaIN.\n\nThus, we re-initialize our style encoder at the beginning of Stage 2; we do not share the style encoder across different styles. We clarified this in Appendix D., and we added simplified diagrams describing each training stage in Appendix D.3, so please refer to Figure 9 in Appendix D.3.\n\n### Details on training comparison methods\n\nAs you mentioned, we retrain the comparison methods using panoramas as the source and pinhole images as a target. We conduct experiments using default hyper-parameter settings for other methods. We clarified this in Experiments section.\n\n### Future work\n\nWe think our network cannot be straightforwardly adopted to inpainting or outpainting, but we believe our network can be adopted as a baseline for such image editing tasks. Also, our framework can be directly applied to some downstream applications, including objection detection and segmentation. For example, we have visualized the results of object detection in Appendix E.4 on translated panoramas using YOLOv5, showing our method excels in preserving objects."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission581/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700445668483,
                "cdate": 1700445668483,
                "tmdate": 1700445759320,
                "mdate": 1700445759320,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0ToK1FawfM",
                "forum": "bRm0rul3SZ",
                "replyto": "TOXyFFJTMR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission581/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission581/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Hope to get the feedback"
                    },
                    "comment": {
                        "value": "Dear reviewer jbQQ,\n\nSorry for bothering you. We genuinely appreciate the time and effort you've invested in our paper. As our rebuttal and the revision of our paper have been submitted for a while, we do want to know whether our explanations have settled the concerns. We are eager to have further discussions, so please let us know if you have additional feedback.\n\nBest,\nAuthors of submission 581."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission581/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641204528,
                "cdate": 1700641204528,
                "tmdate": 1700641204528,
                "mdate": 1700641204528,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MtJgdsOKsz",
                "forum": "bRm0rul3SZ",
                "replyto": "3Uphf7LDRJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission581/Reviewer_jBQQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission581/Reviewer_jBQQ"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the Response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response, which addressed some of my concerns. Please further incorporate the above reply into the manuscript. However, the contributions of the panoramic model strategy and the discontinuity elimination strategy still seem weak to me. Therefore, I would like to keep the original rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission581/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660709535,
                "cdate": 1700660709535,
                "tmdate": 1700660709535,
                "mdate": 1700660709535,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]