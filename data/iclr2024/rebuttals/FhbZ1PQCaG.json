[
    {
        "title": "Think Before You Act: Decision Transformers with Internal Memory"
    },
    {
        "review": {
            "id": "7fB5DMCcN2",
            "forum": "FhbZ1PQCaG",
            "replyto": "FhbZ1PQCaG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission277/Reviewer_NgFj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission277/Reviewer_NgFj"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a memory module for Transformer-based architecture that can store and retrieve information for multiple Reinforcement Learning (RL) tasks. Memory update modifies existing information in the memory matrix based on the input sequence and the attention mechanism. Memory retrieval accesses the memory matrix based on content-based addressing. This memory module is integrated with a pre-trained Decision Transformer ((GPT2 architecture) for multi-task RL settings, coupled with a low-rank adaptation fine-tuning method (LoRA). The paper examines the proposed method on multi-game Atari and meta-world object manipulation benchmarks, showing consistent improvements in terms of generalization, adaptation and scaling."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The general motivation is good, a memory module will enhance memorization and can be potentially beneficial for the multi-task RL setting\n- The experiments show good results with clear improvement gains"
                },
                "weaknesses": {
                    "value": "- The novelty is limited. The main idea is to integrate an external memory for Transformers to improve memorization and reasoning. There have been many works along this line of research.  The proposed memory read and write mechanism is also straightforward and heavily based on the mechanism (content-based attention, add, erase, ...) of NTM and DNC. \n- It is unclear why the proposed memory has advantages over other Transformers with internal/external memory or even just long-range attention [1,2,3,4,5]. More explanations are required in the method and more baselines need to be included in the experiments. In particular, the current baselines have only RMDT as a memory-based Transformer, which is not enough, especially when other memory-based Transformers can be adapted easily to offline RL in the same way as the Decision Transformer. Also, retrieval-based RL [6] can be a good baseline as well to highlight the benefit of internal memory. \n- The writing lacks sufficient background content. The paper should provide more details on Decision Transformer and offline RL setting.\n- Although the main message is about memory, there is no experimental analysis of how the memory module helps improve performance. Please consider ablation studies and visualization to prove that memory is the real contribution (not representation and LoRA tricks). There are some results in Appendix, but they are not helpful (see Questions for more discussion)\n- The related work section should include more memory-based Transformer papers \n\n[1] Rae, Jack W., Anna Potapenko, Siddhant M. Jayakumar, and Timothy P. Lillicrap. \"Compressive transformers for long-range sequence modelling.\" arXiv preprint arXiv:1911.05507 (2019).  \n[2] Martins, Pedro Henrique, Zita Marinho, and Andr\u00e9 FT Martins. \"$\\infty $-former: Infinite Memory Transformer.\" arXiv preprint arXiv:2109.00301 (2021).  \n[3] Wang, Weizhi, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. \"Augmenting Language Models with Long-Term Memory.\" arXiv preprint arXiv:2306.07174 (2023).\n[4] Wu, Yuhuai, Markus N. Rabe, DeLesley Hutchins, and Christian Szegedy. \"Memorizing transformers.\" arXiv preprint arXiv:2203.08913 (2022).  \n[5] Wang, Sinong, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. \"Linformer: Self-attention with linear complexity.\" arXiv preprint arXiv:2006.04768 (2020).  \n[6] Goyal, Anirudh, Abram Friesen, Andrea Banino, Theophane Weber, Nan Rosemary Ke, Adria Puigdomenech Badia, Arthur Guez et al. \"Retrieval-augmented reinforcement learning.\" In International Conference on Machine Learning, pp. 7740-7765. PMLR, 2022."
                },
                "questions": {
                    "value": "- Are $w$ in Line 202, 215, and 232 the same?\n- What is the motivation to compute the strength $\\beta$ using attention?  Why do we need to use $\\beta$ in both erasing and adding vectors?\n- Is $t$ in Line 222 the step in the trajectory? Can you provide an algorithm to explain clearly how memory read and write are executed within a trajectory?\n- Is Step 1 Line 187 important? Do you have an ablation study on Step 1? \n- Based on Table 5, it seems that LoRA is the main contributor to your method. Can you have the ablation on LoRA using Atari games? Also ablation study on memory adding and erasing would be helpful. \n- Can you have visualization to show that the memory stores important data and your model actually reads meaningful memory data from the memory? E.g., when taking action, the model refers to a meaningful timestep in the past to support your idea \"think before you act\"\n- Fig. 3 does your method perform well at 10M parameters?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission277/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698110083623,
            "cdate": 1698110083623,
            "tmdate": 1699635953385,
            "mdate": 1699635953385,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sQCYFuoMCn",
                "forum": "FhbZ1PQCaG",
                "replyto": "7fB5DMCcN2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission277/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission277/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful feedback! \n\n## Q1\n\n>The novelty is limited. The main idea is to integrate an external memory for Transformers to improve memorization and reasoning. There have been many works along this line of research. The proposed memory read and write mechanism is also straightforward and heavily based on the mechanism (content-based attention, add, erase, ...) of NTM and DNC.\n    \nA: **Internal Memory vs. NTM**: While both the internal memory module and NTM aim to introduce memory mechanisms to neural architectures, our internal memory is inspired by human cognitive processes and is specifically tailored to decision-making agents. The design and integration of this module into DT, and its subsequent application, is a **novel contribution** in the context of our work. Besides, there are other papers inspired by NTM: \\[1\\] Memory Augmented Neural Networks with Wormhole Connections. ArXiv:1701.08718 \\[2\\] Hybrid computing using a neural network with dynamic external memory. Nature 2016. Same as the above mentioned papers, despite that our work inspired by NTM, we design a new memory reading and writing techniques for DT.\n\n**Use of LoRA**: While the concept of LoRA might not be new, its application in conjunction with our internal memory module and within the realm of LLM-based decision-making agents brings a novel perspective. The combination of these components, and the synergies they produce, offers a unique approach to addressing the generalization and adaptation in MDT.\n\nThe novelty of our work isn't merely in the individual components but in the holistic approach and the results achieved. By blending inspiration from human cognition with state-of-the-art machine learning techniques, our paper is the **first** paper that introduces internal memory in DT improved training efficiency and generalization in a variety of tasks, as evidenced in our experiments.\n\n**Contextual Novelty**: It's crucial to recognize that novelty doesn't always stem from introducing entirely new concepts. Often, the innovative combination, adaptation, and application of existing ideas to new contexts or challenges can bring significant advancements to a field."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699934157320,
                "cdate": 1699934157320,
                "tmdate": 1699971935432,
                "mdate": 1699971935432,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f2hxalkM5m",
                "forum": "FhbZ1PQCaG",
                "replyto": "7fB5DMCcN2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission277/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission277/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Q5\n\n>Are $\\omega$\u00a0\u00a0in Line 202, 215, and 232 the same?\n    \nA: Yes, the address $\\omega$ remains the same in these lines. Line 202 defines how we compute the address. Line 215 defines how we write information to the memory, and line 232 defines how we read from the memory.\n    \nThe reasons are as follows:\n\n1. **Content-based Addressing**: DT-Mem uses a content-based addressing mechanism, where the content to be read or written influences the memory address. Since we are calculated the address using current input embeddings $E$, this mechanism aligns with using the same address for both operations, as the content being processed is directly tied to where it is stored or retrieved from.\n2. **Memory Consistency and Coherence**: This approach allows the model to immediately read back what it has just written, ensuring that the memory reflects the latest changes made by the network.\n3. **Simplified Memory Management**: Using the same address for reading and writing simplifies the memory management process. It reduces the complexity of the controller's operations, as it doesn't need to maintain separate mechanisms or algorithms for addressing reads and writes.\n\n## Q6\n\n>What is the motivation to compute the strength\u00a0$\\beta$\u00a0using attention? Why do we need to use\u00a0$\\beta$\u00a0in both erasing and adding vectors?\n    \nA: The primary motivation behind employing attention mechanisms is to achieve **Context-Awareness**. By using cross-attention between the input embeddings, $E$, and the memory, $M$, the model can selectively concentrate on pertinent aspects of the input. This attention-based computation ensures that memory updates are context-dependent, enhancing both the dynamism and efficiency of the process.\n\nThe computation of $\\beta$ plays a crucial role in regulating the flow of information to the memory. As $\\beta$ represents a balance between the information in the addition vector $ \\epsilon^a$ and the erasure vector $ \\epsilon^e$, it effectively governs the extent of new information added to, and old information removed from, the current memory. This parameter allows for dynamic control over memory updates. For instance, when the input information is highly relevant to specific memory segments of interest (as determined by the content-address $\\omega $), $\\beta$ assumes a larger value. Consequently, only a small portion of memory is erased (calculated as 1-$\\beta$, as outlined in line 216) and predominantly mixed with the addition vector (as described in line 222). Conversely, if the input information significantly differs, the model erases more from the current memory, adding only a minimal amount of new information.\n    \n\n## Q7\n\n>Is $t$\u00a0\u00a0in Line 222 the step in the trajectory? Can you provide an algorithm to explain clearly how memory read and write are executed within a trajectory?\n    \nA: Thank you for highlighting this point of confusion. In this context, the symbol $t$ represents the $N$th timestep of memory operations that occur within the interactions between the transformer module and memory. It specifically refers to the internal loop of memory operations and is not associated with the trajectory. For a detailed explanation, we have included the relevant algorithm in Appendix Algorithm 3.\n    \n\n## Q8\n\n>Is Step 1 Line 187 important? Do you have an ablation study on Step 1?\n    \nA: Step 1 serves two main purposes:\n\n1. To map various input dimensions into a consistent dimension for storage. Given that our memory is a matrix with fixed dimensions (i.e., number_of_slots * dimensions), it's imperative to harmonize input dimensions for storage.\n2. As highlighted in our paper, the input sequence comprises multiple steps of a tuple $< \\hat{r}_t, s_t, a_t >$. We use the timestep $t$ to arrange these inputs, thereby preserving the temporal relationships among them.\n\nWe've conducted results without including input sequence organization:\n\n|Game|DT-Mem (No-step 1)|DT-Mem|\n|---|---|---|\n|Alien|211.9|239.6|\n|MsPacman|637.1|713.4|\n|Pong|19.0|19.1|\n|SpaceInvaders|165.7|171.2|\n|StarGunner|620.7|709.3|\n\nThough minor differences exist between the two sets of inputs, the variance in outcomes between the two methodologies is not pronounced."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699934213502,
                "cdate": 1699934213502,
                "tmdate": 1699971432579,
                "mdate": 1699971432579,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kgqnGeWb8s",
            "forum": "FhbZ1PQCaG",
            "replyto": "FhbZ1PQCaG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission277/Reviewer_qzjQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission277/Reviewer_qzjQ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce a novel agent algorithm called Decision Transformer with Memory (DT-Mem), which incorporates a memory layer between the attention and MLP layers in the Transformer architecture. This addition allows the agent to memorize knowledge in memory, rather than relying solely on learnable parameters. Empirical evidence demonstrates that DT-Mem outperforms existing methods in terms of generalization and can quickly adapt to new tasks through fine-tuning, such as LoRA."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper presents the Decision Transformer with Memory (DT-Mem) model, which incorporates a learnable memory module and demonstrates superior performance in pre-training, generalization, and fine-tuning. It distinguishes itself from related work, such as RMDT, by its ability to learn sequences in parallel training and use an advanced learnable memory architecture based on the NTM model.\n- In section 4, the paper's clarity for the proposed model is commendable, with a well-structured architecture diagram and detailed explanations for each step of inference and training.\n- The evaluation methodology includes several well-designed questions that effectively assess the hypotheses presented in the paper.\n- Comparative evaluation against diverse baselines, including another memory-equipped Decision Transformer model (RMDT), strengthens the paper's contributions.\n- In Figures 3 and 4, comparison with diverse size of MDT is interesting, through this, we can clearly see that the memorization through the neural network parameters is inefficient compared to the implicit learnable memory.\n- The additional experimental results in the appendix are helpful to understand deeply (especially Figure 10 is good)."
                },
                "weaknesses": {
                    "value": "- Some explanations in the paper are unclear. For instance, the reference to \"Large Language Model based decision making agents\" needs clarification. It's unclear if this refers to Transformer-based agents or models within the Decision Transformer family.\n- The motivation for this work is not entirely clear. While the paper argues that memorization through neural network parameters can lead to weaker performance in forgetting during training, it remains unclear how this relates to generalization performance for unseen tasks.\n- The paper contains some comments that are difficult to understand, such as the mention of NFT and FT in Figure 5, where no results are presented for these abbreviations.\n- There's a minor typographical error in line 297, where \"we generat\" should be corrected to \"we generate.\""
                },
                "questions": {
                    "value": "- Is the memory not initialized throughout the entire training process? Clarifying this point could help readers better understand the novelty of this approach, as memory initialization is typically performed per episode (e.g., NTM).\n- Could you investigate whether MDT can be fine-tuned using the LoRA technique? As LoRA is applicable to the general Transformer architecture, it would be insightful to assess the potential for fine-tuning MDT using this approach.\n- Have you considered testing an external memory-equipped DT? Many recent attempts to incorporate memory utilize a naive appending-style external memory, which lacks the sophistication of models like NTM. Comparing your memory architecture to this version could help highlight its strengths.\n- Expanding the tasks to include those with long-term dependencies, such as MemoryMaze, could be valuable. Given that DT-Mem has a memory module capable of retaining distant knowledge, it may excel in tasks where other DTs, except RMDT, struggle.\n- In line 243, the phrase \"the Transformer module to generate actions that can reduce this value as close to zero as possible\" may benefit from clarification. The objective appears to be maximization rather than reduction, as it involves the sum of rewards.\n- Figure 5 raises questions about the absence of NFT results. Are all results in the plot derived from fine-tuned models?\n- In Figure 6, where you mention the top 3 rollout, could you provide more context or clarification about what this entails?\n\n### Additional Comments\nDT-Mem demonstrates superior performance in pre-training, generalization, and fine-tuning, particularly evident in Figure 10. However, there seems to be a disconnect between the paper's motivation and the observed results. Clarifying the link between implicit memory's ability to mitigate the forgetting phenomenon and the improved generalization and fine-tuning performance would strengthen the paper's alignment and overall impact.\n\n### After reading the author's rebuttal\nWe thank the authors for their efforts to clarify their arguments. I agree with their rebuttal, in particular, the part related to the connection between their motivation and their methodologies. I hope they will try to test the appending-style external memory also, but I am satisfied to their rebuttal, so I increase my score to lean to the acceptance.\n\nTo authors, as I know, there is no prior work for the appending-style external memory + DT, but the external memory equipped agents have been studied actively. I leave some references.\n\nLampinen, Andrew, et al. \"Towards mental time travel: a hierarchical memory for reinforcement learning agents.\" Advances in Neural Information Processing Systems 34 (2021): 28182-28195.\n\nParisotto, Emilio, et al. \"Stabilizing transformers for reinforcement learning.\" International conference on machine learning. PMLR, 2020."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission277/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission277/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission277/Reviewer_qzjQ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission277/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698735775645,
            "cdate": 1698735775645,
            "tmdate": 1700536624988,
            "mdate": 1700536624988,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uln8bzrl0r",
                "forum": "FhbZ1PQCaG",
                "replyto": "kgqnGeWb8s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission277/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission277/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful feedback! \n\n## Q1\n\n> Some explanations in the paper are unclear. For instance, the reference to \"Large Language Model based decision making agents\" needs clarification. It's unclear if this refers to Transformer-based agents or models within the Decision Transformer family.\n    \nA: This phrase generally refers to all the large language model-based agents, which includes both decision transformer, trajectory transformer [1] and the following works. In this paper, we mainly focus on decision transformer family. We\u2019ve revised this sentence in the paper revision line 1.\n\n[1] Offline Reinforcement Learning as One Big Sequence Modeling Problem. [https://arxiv.org/abs/2106.02039v4](https://arxiv.org/abs/2106.02039v4)\n    \n\n## Q2\n\n>The motivation for this work is not entirely clear. While the paper argues that memorization through neural network parameters can lead to weaker performance in forgetting during training, it remains unclear how this relates to generalization performance for unseen tasks.\n    \nA: **Memorization and Forgetting in Neural Networks**: When a neural network is trained on a series of tasks, it tends to \"memorize\" the specifics of those tasks through its parameters. This process can lead to a phenomenon known as \"catastrophic forgetting,\" where learning new tasks impairs the network's performance on previously learned tasks [1]. This is a significant issue in the context of decision-making models, where adaptability and the ability to handle a variety of scenarios are crucial.\n\n**Generalization to Unseen Tasks**: Generalization refers to a model's ability to perform well on new, unseen tasks, not just the ones it was trained on. The problem with the traditional approach of memorizing through parameters is that it often leads to overfitting on the training data, which can diminish the model's ability to generalize. When a model is overfitted, it becomes too tailored to the specifics of its training data and loses the flexibility needed to adapt to new situations or tasks [2].\n\n**Relation Between Forgetting and Generalization**: The link between the forgetting phenomenon and generalization is that both are impacted by how a model learns and stores information. A model that is prone to forgetting may struggle with generalization because it cannot effectively retain and utilize the broad range of knowledge required to handle new tasks [3]. Conversely, a model designed to minimize forgetting (such as through an internal memory mechanism like DT-Mem) can potentially maintain a more diverse and generalizable knowledge base.\n\n**Motivation of DT-Mem**: The motivation for introducing DT-Mem, as such, is to address these interrelated issues. By incorporating an internal memory module, DT-Mem aims to reduce the reliance on parameter-based memorization, thereby mitigating the effects of catastrophic forgetting. This is hypothesized to improve the model's ability to generalize to new tasks, as it can more effectively retain and apply knowledge from a broader range of experiences. Section 5.3 Table 1 the results of model generalization support this findings.\n\n[1] Overcoming catastrophic forgetting in neural networks. National academy of sciences 2017.\n\n[2] Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530.\n\n[3] Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13), 3521-3526. 2017\n    \n\n## Q3\n\n> Is the memory not initialized throughout the entire training process? Clarifying this point could help readers better understand the novelty of this approach, as memory initialization is typically performed per episode (e.g., NTM).\n    \nA: The memory is initialized only once during the entire training process. This approach ensures that the knowledge acquired from one task can be leveraged in subsequent tasks, leading to improved pre-training performance, as evidenced in Fig. 6.\n    \n\n## Q4\n\n> Could you investigate whether MDT can be fine-tuned using the LoRA technique? As LoRA is applicable to the general Transformer architecture, it would be insightful to assess the potential for fine-tuning MDT using this approach.\n    \nA: The results shown in Section 5.5 Figure 5 are all fine-tuned using the LoRA technique. As we concluded in lines 342-351. The results show that \u201cthe consistent superior performance of DT-Mem across most games suggests that this method might have a more adaptable approach.\u201d"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699933807334,
                "cdate": 1699933807334,
                "tmdate": 1699971833760,
                "mdate": 1699971833760,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dPMFjTcXeQ",
                "forum": "FhbZ1PQCaG",
                "replyto": "kgqnGeWb8s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission277/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission277/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Results about MemoryMaze Environment"
                    },
                    "comment": {
                        "value": "We evaluated both DT-Mem and MDT using the Memory Maze offline probing benchmark to determine if DT-Mem enhances performance in tasks requiring long-term dependency. Performance was measured using wall prediction accuracy (%) for the Walls benchmarks (higher the better) and mean-squared error for the Objects benchmarks (lower the better). We report the average score across three training runs. The results are as follows:\n\n|  | Memory 9x9 Walls | Memory 15x15 Walls | Memory 9x9 Objects | Memory 15x15 Objects |\n| --- | --- | --- | --- | --- |\n| Constant Baseline | 80.8% | 78.3% | 23.9 | 64.8 |\n| MDT | 87.2%$\\pm 0.4$ | 79.3%$\\pm 0.7$ | 8.6$\\pm 0.3$ | 32.2$\\pm 0.2$ |\n| DT-Mem | 96.9%$\\pm 0.3$ | 80.6%$\\pm 0.5$ | 3.0$\\pm 0.3$ | 27.8$\\pm 0.3$ |\n\nThese results indicate that DT-Mem achieves superior performance in both environments across all four metrics. This suggests that the incorporation of a memory module in DT-Mem effectively stores past information for future decision-making. It highlights the method's efficacy and its advantages in handling long-term dependency tasks."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479828029,
                "cdate": 1700479828029,
                "tmdate": 1700480613630,
                "mdate": 1700480613630,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PpMvYikml4",
                "forum": "FhbZ1PQCaG",
                "replyto": "dPMFjTcXeQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission277/Reviewer_qzjQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission277/Reviewer_qzjQ"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "We thank the authors for their efforts to clarify our concerns. They addressed our concerns properly, and we agree with their explanation about the connection between their motivation and methodology. Thus, we increased our score to lean to the acceptance. We leave the additional comments in our review, please check that."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536580843,
                "cdate": 1700536580843,
                "tmdate": 1700536580843,
                "mdate": 1700536580843,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0hsU2pQtUT",
            "forum": "FhbZ1PQCaG",
            "replyto": "FhbZ1PQCaG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission277/Reviewer_hmxu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission277/Reviewer_hmxu"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the memory into the decision transformer. Basically, the memory is a matrix which store the embedding of the transition tuple, and when decision making using decision transformer, the retrieved information from the memory is used to generate the action. Experiments on Atari and Meta-world demonstrate the effectiveness of the proposed methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The motivation of this paper is clear, i.e., inspiring by the human decision making process.\nThe writing of the paper is good, i.e., easy to follow."
                },
                "weaknesses": {
                    "value": "The main contribution of the paper is the memory. however, more in-depth investigation of the memory can be conducted. There are some issues about the clarity."
                },
                "questions": {
                    "value": "There are several questions I want the author to address during the rebuttal:\n1. Some issues about the clarity:    \na. For Section 3.1. It seems that you focus on offline RL, rather than RL. The two fields have differences. You may need a brief introduction of offline RL, as well as existing methods as baselines, such as DT, MGDT, RMDT, HDT.    \nb. Figure 2 is somehow misleading. Figure 2a, does the memory module is a layer of the transformer? I think they are separated, however, the plot seems that the memory is stacked with the transformer. Figure 2b, the retrieved memory is another memory? I think should be the retrieved experiences, or other terms. Please make the terms unique and clear.\n\n2. Some issues about the technical contributions.  \na. The introduction uses large language model as the motivation, however, even using GPT-2 architecture, the embedding and the tokens I believe is not about words, it should be game-specific tokens. so please using transformer or decoder-only transformer to avoid any confusion.    \nb. It seems that the largest model used in the paper is 50M. Compared with LLM, it still very small. Does LoRA is necessary? LoRA can be used to any models, which is not a technical contribution of this paper. However, that may harm the performance of DT-Mem. So I would suggest just not using LoRA and fine-tuning all the model to focus on the memory part. This can help the reviewer to fully evaluate the importance of the memory.   \nc. About the memory. There are some related methods, neural episodic control (https://arxiv.org/abs/1703.01988) for the writing and lookup. The two methods share many similarities, so maybe add a detailed comparison of the proposed methods and all related methods, so we can fully understand the contributions.    \nd. Still about the memory.  What is exactly the difference between the external memory and the internal memory? Could you provide an example about the two kinds of memories, as well as the advantages of the proposed memory. \n\n3. Some issues about the experiments. I generally think the experiments are sufficient, but with some suggestions: i) can we use the prompting for the DT-Mem? as we know prompting is much easier than fine-tuning. And ii) what is the limit of the internal memory, given the fixed size, i.e., parameters, of the memory?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission277/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806924660,
            "cdate": 1698806924660,
            "tmdate": 1699635953156,
            "mdate": 1699635953156,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v4YFrcGOAh",
                "forum": "FhbZ1PQCaG",
                "replyto": "0hsU2pQtUT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission277/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission277/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful feedback! \n\n## Q1.a\n\n>For Section 3.1. It seems that you focus on offline RL, rather than RL. The two fields have differences. You may need a brief introduction of offline RL, as well as existing methods as baselines, such as DT, MGDT, RMDT, HDT.\n    \nA: We appreciate the insightful suggestion. Accordingly, a concise overview of offline RL has been incorporated into Section 3.1 (lines 141-146). Additionally, due to page constraints, we have briefly introduced the baselines in Section 5.2.\n    \n\n## Q1.b\n\n> Figure 2 is somehow misleading. Figure 2a, does the memory module is a layer of the transformer? I think they are separated, however, the plot seems that the memory is stacked with the transformer. Figure 2b, the retrieved memory is another memory? I think should be the retrieved experiences, or other terms. Please make the terms unique and clear.\n    \nA: We appreciate the suggestion. To clarify, in Figure 2a, the memory module is depicted as an independent module, distinct from the transformer module. Conversely, in Figure 2b, the memory that is retrieved serves as an intermediate input to the transformer module, integrating with it more directly. We have updated Figure 2 in the revised version of the paper to better illustrate these configurations and their respective operational dynamics.\n    \n\n## Q2.a\n\n> The introduction uses large language model as the motivation, however, even using GPT-2 architecture, the embedding and the tokens I believe is not about words, it should be game-specific tokens. so please using transformer or decoder-only transformer to avoid any confusion.\n    \nA: Thank you for the comments. We have revised this term in line 16 and line 20.\n    \n\n## Q2.b\n\n>It seems that the largest model used in the paper is 50M. Compared with LLM, it still very small. Does LoRA is necessary? LoRA can be used to any models, which is not a technical contribution of this paper...\n    \nA: **Model Size:** The choice of a 50M model size was dictated by the computational resources available to us. Nonetheless, in Figure 3, we explore the scaling law of DT-Mem, which reveals an interesting trend: as the number of parameters increases, so does performance. Importantly, when compared to the 200M MDT model, our DT-Mem, with only a quarter of the parameter size, demonstrates superior performance. This not only underscores the efficiency of DT-Mem in generalizing across games but also empirically supports its effectiveness even with fewer parameters.\n    \n**Motivation of LoRA**: The motivations of using LoRA to fine-tune the model can be concluded in following two reasons:\n    \n1. According to a paper \\[1\\], compared to other adapter methods, LoRA method not only don't introduce inferences latency nor reduce input sequence length while retaining high model quality. It allows for quick task-switching when deployed as a service by sharing the vast majority of the model parameters\n2. Parameter-efficient fine-tuning (PEFT) approaches (such as LoRA) only fine-tune a small number of (extra) model parameters while freezing most parameters of the pretrained LLMs, thereby greatly decreasing the computational and storage costs \\[1\\]. This also overcomes the issues of\u00a0catastrophic forgetting \\[2\\]. PEFT approaches have also shown to be better than fine-tuning in the low-data regimes and generalize better to out-of-domain scenarios \\[3\\].\n    \n**Full Fine-tuning (FFT) vs. LoRA**: To assess whether the use of LoRA adversely affects performance, we conducted experiments contrasting Full Fine-Tuning (FFT) of memory parameters with LoRA. In this context, FFT-single refers to fine-tuning all parameters exclusively on a single game, whereas FFT-All represents fine-tuning on the entire set of games simultaneously. Results are DQN-normalized score.\n    \n|Game|LoRA|FFT-Single|FFT-All|\n|---|---|---|---|\n|Alien|127.4%|116.8%|113.9%|\n|MsPacman|130.8%|122.8|77.1%|\n|SpaceInvaders|100.8%|86.8%|73.4%|\n|StarGunner|158.3%|55.7%|40.6%|\n\nBased on above results, we conclude the following observations:\n\n- LoRA appears to be the most consistently effective strategy across the games provided.\n- While **FFT-Single** occasionally outperforms PEFT (like in Alien), **FFT-All** consistently trails behind the other two.\n\nThe reason full fine-tuning is not comparable to PEFT comes from the following parts: 1. Fine-tuning dataset size. Note that we only use 50k data in LoRA and full fine-tuning compares on 500k used in MDT paper 2. The benefits of LoRA listed above: \"This approach also addresses catastrophic forgetting [2] and has outperformed standard fine-tuning in low-data and out-of-domain situations [3]\u201d\n\n[1] LoRA: Low-Rank Adaptation of Large Language Models, ICLR 2022\n[2] Adapter-Fusion: Non-destructive task composition for transfer learning.EACL, 2021.\n[3] Prefix-tuning: Optimizing continuous prompts for generation. ACL, 2021."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699933616387,
                "cdate": 1699933616387,
                "tmdate": 1699971816632,
                "mdate": 1699971816632,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ybQir5iWM0",
                "forum": "FhbZ1PQCaG",
                "replyto": "0hsU2pQtUT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission277/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission277/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Q2.c\n\n>About the memory. There are some related methods, neural episodic control ([https://arxiv.org/abs/1703.01988](https://arxiv.org/abs/1703.01988)) for the writing and lookup. The two methods share many similarities, so maybe add a detailed comparison of the proposed methods and all related methods, so we can fully understand the contributions.\n    \nA: Thank you for the comments. Due to the page limit, we have added these comparisons in the paper revision Appendix Section E\n    \n\n## Q2.d\n\n>Still about the memory. What is exactly the difference between the external memory and the internal memory? Could you provide an example about the two kinds of memories, as well as the advantages of the proposed memory.\n    \n### External Memory vs. Internal Memory\n\n1. **External Memory**:\n\t- **Definition**: External memory in AI and machine learning models refers to a separate storage unit that is not part of the core neural network architecture. It's like an additional database or repository that the model can access.\n\t- **Example**: A neural Turing machine (NTM) or Differentiable Neural Computer (DNC), which uses an external memory matrix that it can read from and write to, separate from its neural network parameters.\n\t- **Advantages**:\n\t\t- **Scalability**: Can store large amounts of data beyond the capacity of the model's parameters.\n\t\t- **Flexibility**: Allows the model to store and retrieve information in a more structured way, similar to how a computer uses RAM and disk storage.\n2. **Internal Memory**:\n\t- **Definition**: Internal memory refers to the capacity of a neural network to store information within its own architecture, such as in its weights and activations. It does not use a separate storage unit but relies on the network's inherent structure.\n\t- **Example**: LSTM (Long Short-Term Memory) networks that use their recurrent connections to store information over time within the network.\n\t- **Advantages**:\n\t\t- **Efficiency**: More efficient in terms of retrieval speed, as the information is stored within the network.\n\t\t- **Integration**: Better integrated with the model's learning process, enabling more seamless information flow and processing.\n\n### Advantages of the Proposed Memory\n\n1. **Contextual Retrieval**: Utilizing mechanisms like attention to retrieve and store information more contextually and relevantly.\n2. **Enhanced Generalization**: By having a more flexible memory system, the model generalize better to new, unseen tasks.\n\n## Q3.i\n\n> Some issues about the experiments. I generally think the experiments are sufficient, but with some suggestions: i) can we use the prompting for the DT-Mem? as we know prompting is much easier than fine-tuning.\n    \nA: Thank you for the suggestion. The primary objective of the fine-tuning in Section 5.5 is to investigate whether modifications to the memory module can boost the performance of DT-Mem without altering other transformer parameters. This is intended to assess the role of the memory module's stored information in aiding decision-making for new tasks. Since the memory module engages in read and write operations through cross-attention, the use of prompt tuning, which does not alter DT-Mem's stored information, is not aligned with our approach.\n\nNevertheless, we have conducted a comparative analysis of prompt-tuning DT (PDT) and DT-Mem in Table 2. The results demonstrate the effectiveness of our proposed method and highlight the advantages of fine-tuning the memory module.\n    \n\n## Q3.ii\n\n> And ii) what is the limit of the internal memory, given the fixed size, i.e., parameters, of the memory?\n    \nA: The major limitation of internal memory is **Capacity Limitation**: The capacity of internal memory is inherently limited by the size of the neural network, i.e., the number of its parameters and the architecture's complexity. Once these parameters are set, the capacity to store and recall information is fixed. The ability to store information depends on how densely information can be encoded within the network's parameters, which has practical limits."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699933685390,
                "cdate": 1699933685390,
                "tmdate": 1699971565178,
                "mdate": 1699971565178,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gH9hBYpDW8",
                "forum": "FhbZ1PQCaG",
                "replyto": "ybQir5iWM0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission277/Reviewer_hmxu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission277/Reviewer_hmxu"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the response. I think that generally address my concerns and I would keep my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700537324366,
                "cdate": 1700537324366,
                "tmdate": 1700537324366,
                "mdate": 1700537324366,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CJzL2HQ0pm",
            "forum": "FhbZ1PQCaG",
            "replyto": "FhbZ1PQCaG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission277/Reviewer_gRQQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission277/Reviewer_gRQQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Decision Transformers with Memory, which introduces internal memory mechanism into RL field and improves training efficiency and generalization in both Atari games and meta-world object manipulation tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The experiments are sufficient and the results prove the superiority of this method.\n- The paper is well written."
                },
                "weaknesses": {
                    "value": "- The internal memory formulation and some specific details, such as content-based addressing, seems a bit incremental from previous work, the authors should explain the difference more clearly in the method section."
                },
                "questions": {
                    "value": "1. Fig. 2(b) is too simple, making it difficult to correspond one-to-one with the steps in the method part. The authors should make the figure more comprehensive and understandable.\n2. There are many papers demonstrating the ideas about internal memory, and the authors should explain the differences with similar methods in more detail in the method section.\n3. Add more analysis about different situations, such as the input misleading by the content stored in the memory (i.e., noise or dissimilar pattern), how does the method eliminates this type of impact."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission277/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission277/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission277/Reviewer_gRQQ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission277/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699458719287,
            "cdate": 1699458719287,
            "tmdate": 1699635953092,
            "mdate": 1699635953092,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gYDLITtHTi",
                "forum": "FhbZ1PQCaG",
                "replyto": "CJzL2HQ0pm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission277/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission277/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful feedback! \n## Q1\n\n> Fig. 2(b) is too simple, making it difficult to correspond one-to-one with the steps in the method part. The authors should make the figure more comprehensive and understandable.\n    \nA: Thank you for the comments. We have revised this figure in the paper revision Fig. 2\n    \n\n## Q2\n\n> There are many papers demonstrating the ideas about internal memory, and the authors should explain the differences with similar methods in more detail in the method section.\n    \n    \nA: Thank you for pointing this out. To the best of my knowledge, the work most closely related to ours (DT with internal memory) is RMDT, which we have discussed extensively in the paper. I would greatly appreciate it if the reviewer could share specific papers they believe should be compared to our work. We are more than willing to discuss these differences in the revised version of the paper.\n    \n\n## Q3\n\n> Add more analysis about different situations, such as the input misleading by the content stored in the memory (i.e., noise or dissimilar pattern), how does the method eliminates this type of impact.\n    \n    \nA: We thank the reviewer for their valuable suggestion. Following this, we conducted an experiment to assess the robustness of the proposed method against input distortion. This involved adding Gaussian noise to the input frames of Atari games. Specifically, we set the mean to 0 and experimented with various standard deviation values. The results are detailed in the table below:\n    \n|  | Alien | MsPacman | SpaceInvaders | StarGunner |\n| --- | --- | --- | --- | --- |\n| MDT | 3.8% | 13.2% | 8.6% | 2.3% |\n| DT-Mem | 51.0% | 69.3% | 53.6% | 62.2% |\n| DT-Mem (std=0.5) | 55.3% | 67.6% | 53.0% | 57.8% |\n| DT-Mem (std=1) | 35.6% | 56.1% | 40.0% | 34.6% |\n| DT-Mem (std=2) | 25.9% | 35.6% | 30.5% | 21.1% |\n    \nFrom the results above, we conclude that the proposed DT-Mem demonstrates greater robustness to noisy inputs compared to the MDT method. This is evident as the DT-Mem consistently outperforms MDT under various levels of Gaussian noise. Notably, the performance with a standard deviation of 0.5 shows minimal difference compared to the no-noise scenario, illustrating DT-Mem's effectiveness in mitigating the impact of varying input distortions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission277/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699933362097,
                "cdate": 1699933362097,
                "tmdate": 1699971800731,
                "mdate": 1699971800731,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]