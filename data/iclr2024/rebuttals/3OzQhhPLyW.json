[
    {
        "title": "Meta-Value Learning: a General Framework for Learning with Learning Awareness"
    },
    {
        "review": {
            "id": "WZqxYDzWYB",
            "forum": "3OzQhhPLyW",
            "replyto": "3OzQhhPLyW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6122/Reviewer_1uYc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6122/Reviewer_1uYc"
            ],
            "content": {
                "summary": {
                    "value": "Recent works in multi-agent learning focus on reasoning about inter-agent interactions. One such algorithm is Learning with Opponent Learning Awareness (LOLA), which looks ahead utilizing gradient descent. The work builds on the LOLA framework by casting the problem as a meta-game of optimization. Meta-Value Learning (MeVa) learns a meta-value function (expected discounted return over future iterates) and applies a form of Q-learning to evade representing the continuous action space. The inner loop corresponds to collecting a policy optimization trajectory using gradient descent on iterates of the meta-value. The outer loop corresponds to learning the meta-value function by minimizing the TD error. MeVa incorporates design considerations such as empirical corrections in Bellman updates, variable discounting and gaussian noise as exploration. Results in a logistic game and repeated matrix games demonstrate improvements over LOLA agents."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper is well-written and positioned within the learning with learning awareness literature.\n* Authors have highlighted relevant design considerations and their motivation."
                },
                "weaknesses": {
                    "value": "* **Variance in Iterates:** My main concern is the variance in iterates of the optimization process. Using a correction-based formulation leads MeVa to estimate $\\bar{\\nabla} f(x)$ using REINFORCE which is known to have high variance in the policy gradient. Specifically, it is essential that the gradient of the iterates is consistent. It would be helpful if the authors can highlight any practical considerations to mitigate high variance in the gradients. Authors could also provide standard deviations of different trials for Tables 2 and 3 in order to evaluate the effect of noisy gradients (if any).\n* **Ablations:** Appendix F provides a range of ablations for design considerations utilized in MeVa. However, an important comparison would be to assess the importance of the meta-value function formulation itself. Authors should compare the surrogate formulation of Eq. 6 with the naive infinitely discounted sum of Eq. 5. Similarly, the choice of a sophisticated exploration strategy, which is Gaussian noise, could be evaluated by comparing it with standard exploration schedules used for TD learning such as $\\epsilon$-greedy exploration or action noise. Current ablations only enable/disable exploration which do not reason about the choice of schemes and their importance.\n* **Related Work:** The paper cites relevant works from multi-agent learning and opponent awareness literature. However, their organization within the paper could be improved. Authors could organize recent literature in a dedicated related works section or discuss the improvements of MeVa over prior methods (as done for LOLA in Introduction).\n\n[1]. Foerster et al., \"Learning with Opponent-Learning Awareness\", AAMAS 2018."
                },
                "questions": {
                    "value": "* How can the high variance of policy gradients in REINFORCE be tackled? Can you please discuss some practical considerations or the impact of noise on meta-value iterates? Can you please provide standard deviations for head-to-head comparisons in Tables 2 and 3?\n* Can you please compare between meta-value function formulations of Eq. 5 and Eq. 6? What is the need for a sophisticated exploration strategy such as Gaussian noise when compared to standard exploration schedules such as $\\epsilon$-greedy or action noise?\n* Can you please organize the discussion on relevant works in a related works section? Alternatively, can the discussion be moved to a common section discussing the improvements/differences between MeVa and LOLA, HOLA, COLA and other opponent aware algorithms?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6122/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6122/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6122/Reviewer_1uYc"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6122/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698677261611,
            "cdate": 1698677261611,
            "tmdate": 1699636662980,
            "mdate": 1699636662980,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aaJuDtADrJ",
                "forum": "3OzQhhPLyW",
                "replyto": "WZqxYDzWYB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6122/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to review our paper. Please see our responses below:\n- Variance in iterates: as also noted in our response to reviewer c4Mz, MeVa does not rely on REINFORCE in principle. We did not use any REINFORCE gradients for the experiments because these games can be differentiated exactly. Thus the results in Tables 2 and 3 are not subject to REINFORCE variance.\n  However, we did note that our $U$ formulation requires a way of estimating $\\nabla f$, and REINFORCE is the obvious choice. REINFORCE is in no way required though, as one could also use a model $\\hat{f}$ to estimate $f$ and approximate $\\nabla f$ by $\\nabla \\hat{f}$ (this is what the $V$ formulation with $\\gamma=0$ would provide).\n- Ablations: unfortunately Eq 5 can not be used directly, as the rollouts $x_{>t}$ need to come from the meta-policy itself. Eq 5 (and 6 for that matter) are implicit, and can only be made explicit by fitting a model. We will however include a comparison (on the matrix games) to MetaMAPG, which can be seen as using Eq 5 but evaluating the expectation using naive learning as the meta-policy for the rollouts. For a fair comparison we will modify MetaMAPG to use exact gradients rather than policy gradients, which is the M-MAML variant discussed in the M-FOS paper.\n- Exploration: our exploration is done on the meta level, where actions are continuous, and it is not obvious what form the \"default\" exploration should take. The typical $\\epsilon$-greedy approach is undefined, as there is no uniform distribution on $\\mathbb{R}^n$. We could use the uniform distribution on the sphere to get uniform random directions, but then it is unclear how to scale these. We could add $\\mathcal{N}(0,\\epsilon^2)$ noise to $\\bar{\\nabla}\\hat{V}$, however this scheme seems problematic in general as different parameters can have wildly different sensitivities (think neural networks), in which case we do not expect perturbing them with the same variance will lead to meaningful diversity. Nevertheless, we will rerun the ablation with this additive gaussian noise as the control.\n- We can reorganize related work, but please hear us out on the current structure:\n  - We discuss LOLA front and center because it is the seminal work in this area.\n  - In Section 1 we mention MetaMAPG and M-FOS because they are meta-level approaches like MeVa.\n  - In Section 2.2 we go over the idea of looking ahead, the *inconsistency* in LOLA and how it may be resolved. Here HOLA and COLA are briefly relevant.\n  - In Section 2.3 we discuss *looking further ahead*, which brings in the meta-game. Here Meta-(MA)PG and M-FOS become relevant.\n  - In Section 3.1 we introduce MeVa, and relate it to MetaMAPG and M-FOS.\n  \n  We propose to make two improvements:\n  - We will add discussion of LOLA and COLA in Section 3.1.\n  - We will discuss COLA at the first mention of consistency (\"It is self-consistent: ...\"), which gives us an opportunity to clarify what is meant by (in)consistency.\n\nThank you."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699975455850,
                "cdate": 1699975455850,
                "tmdate": 1699975455850,
                "mdate": 1699975455850,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zimJhwQs3w",
                "forum": "3OzQhhPLyW",
                "replyto": "aaJuDtADrJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6122/Reviewer_1uYc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6122/Reviewer_1uYc"
                ],
                "content": {
                    "title": {
                        "value": "Response To Authors"
                    },
                    "comment": {
                        "value": "I thank the authors for responding to my comments and refining the paper with connections to Q learning and M-MAML comparisons. After going through other reviews and response from authors, my following concern remains unadressed-\n* **Ablations:** Authors provided ablations for exploration utilizing noisy networks. However, my concern was mostly centered towards the effect of the exploration strategy. Meta-learned agents are sensitive towards hyperparameters and design choices. Considering games where exact solutions can be learned, minimal exploration should be sufficient for players to reason about opponent actions. It would be helpful if authors could compare between random sign flips/noisy networks against minimal exploration schedules for future work. This would answer the question _To what degree does MeVa require exploration for finding exact solutions?_ Note that this does not undermine the efficacy of noisy networks as the paper identifies them as a useful strategy for meta games.\n\nOn a general note, the overall quality of the paper is improved and the work presents useful insights. In future, authors could highlight the portions they have modified (with a different color) in the paper for brevity. Future work could consider additional matrix games (rock-paper-scissor and battle of the sexes) which have mixed strategy Nash Equilibria or games where $\\epsilon$-Nash deviates from the true Nash Equilibrium ([[(100, 100), (0, 0)], [(1, 2), (1, 1)]]). Currently, only Matching Pennies presents a mixed strategy equilibrium. In my opinion, these experiments would help us understand the convergence and generality of MeVa. Considering symmetric games does enable transfer between agents which players in antisymmetric games may not enjoy. This does not hamper the utility of meta-learned agents. On the other hand, this is an important finding which could be further studied from an algorthmic perspective."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616044300,
                "cdate": 1700616044300,
                "tmdate": 1700616044300,
                "mdate": 1700616044300,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IRT6k5JKor",
                "forum": "3OzQhhPLyW",
                "replyto": "vAUxp04JGi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6122/Reviewer_1uYc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6122/Reviewer_1uYc"
                ],
                "content": {
                    "title": {
                        "value": "Response to Follow Up"
                    },
                    "comment": {
                        "value": "I thank the authors for the follow up response. Regarding exploration, the ablation of an absent exploration scheme does answer my question. However, it raises the question of why add exploration at all if MeVa can perform well in its absence. Considering only matrix games and simple policies excluding neural networks, a sophisticated design strategy for exploration is not required. If it helps, authors should consider removing exploration in order to simplify the MeVa algorithm. As for neural policies, these would require an in-depth evaluation of design choices which can be reserved for future works.\n\nRegarding the provided game, the payoff matrix has a simple structure but it presents vastly different approximate and exact equilibria. The game has an exact Nash equilibrium at (100, 100) but also an $\\epsilon$-Nash equilibrium at (1, 1) for $\\epsilon = 1$. In my opinion, leveraging an approximation-based method would lead players to the approximate equilibrium which has a significantly lower payoff than the exact equilibrium. This way, approximating the solution may impact convergence of MeVa."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679264612,
                "cdate": 1700679264612,
                "tmdate": 1700679264612,
                "mdate": 1700679264612,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IPMnVpZQ3K",
            "forum": "3OzQhhPLyW",
            "replyto": "3OzQhhPLyW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6122/Reviewer_c4Mz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6122/Reviewer_c4Mz"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the task of opponent modeling in general sum games. The paper attempts to introduce a LOLA-based method that is able to judge policies over a longer horizon than one-step. The paper introduces a value-based method for a meta-game of optimization. This allows the method to avoid directly modeling the policy space updates for each player. Prior art in LOLA assumes that the opponent is a naive learner and uses a one-step lookahead. The paper implements a self-consistent version of LOLA that relaxes these assumptions. The paper evaluates on small matrix games and a logistic game in order to show how the method may help in scenarios similar to that which LOLA was evaluated in."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Opponent modeling is an important domain of multi-agent learning. There is much potential for opponent modeling iff the community is able to create a scalable method. The idea of any LOLA-based paper is to relax some of the assumptions that restrict its scalability. The paper attempts to make a more scalable method through the use of a value-based method that does not rely on policy updates. However, the evaluation is restricted to simple domains."
                },
                "weaknesses": {
                    "value": "The biggest worry with the method is the reliance on the REINFORCE algorithm. The sample complexity required in order to perform this method may be incredibly high once scaled to sufficiently difficult action and state spaces. Though the authors make a note of the limit of this method in terms of learning with neural networks and suggest a direction to help scale their method in the future. This is also a general worry about the LOLA-based works. My question is, does the improvements in the paper lead LOLA-based methods closer to scaling to a feasible solution? If so, how?\n\nAnother question is how can this method scale beyond two-player multi-agent settings to general multi-agent scenarios. Even an understanding of how this methodology works in three-player games would be quite an interesting topic.\n\nThe results appear to show that M-FOS and MeVa are equivalent in performance. I am confused as to what the takeaway here is. Is it that the performance of MeVa is able to do this without the policy gradient? Why not use the policy gradient as those methods are more scalable?"
                },
                "questions": {
                    "value": "\u201cWe argue that this throws out the baby with the bathwater\u201d Language like this typically is more confusing than helpful. Consider either explaining the metaphor in the context of the paper or removing this line.\n\nPlease see Weaknesses for other questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6122/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6122/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6122/Reviewer_c4Mz"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6122/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802957099,
            "cdate": 1698802957099,
            "tmdate": 1699636662869,
            "mdate": 1699636662869,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C4c50boRRi",
                "forum": "3OzQhhPLyW",
                "replyto": "IPMnVpZQ3K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6122/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review. Please see our responses below:\n\n- Reliance on REINFORCE: MeVa does not rely on REINFORCE in principle. We did not use any REINFORCE gradients for the experiments because these games can be differentiated exactly. However, we did note that our $U$ formulation requires a way of estimating $\\nabla f$, and REINFORCE is an obvious choice. REINFORCE is in no way required though, as one could also use a model $\\hat{f}$ to estimate $f$ and approximate $\\nabla f$ by $\\nabla \\hat{f}$ (this is what the $V$ formulation with $\\gamma=0$ would provide).\n- *Do the improvements in the paper lead LOLA-based methods closer to scaling to a feasible solution?* Yes and no. Our method requires significantly more data and computation upfront to learn the value function. However we solve two issues with LOLA that could very well keep it from scaling to neural networks:\n  - LOLA's extrapolation rate $\\alpha$ needs to be large. Large learning rates don't work with neural networks. MeVa's discount rate $\\gamma$ provides a different way of looking further ahead than just enlarging the learning rate, which all else being equal should be helpful for neural network policies.\n  - LOLA's inconsistency (the assumption of naivety) is a manifestation of the exact issue it sought to address in naive learning (inconsistency due to the assumption of stationarity). In a sense, we feel that MeVa is the logical conclusion of the idea of LOLA.\n- The generalization of our method to more than two agents is straightforward: each agent learns their own meta-value approximation and follows its gradients. How the practical implementation will scale computationally and datawise with the number of agents is another matter, although we have no reason to believe it will scale particularly poorly.\n- Yes, M-FOS and MeVa are equivalent in performance, and if one had to choose, one would choose M-FOS for its scalability. However, M-FOS' performance on the coin game is not stellar, so there is value in exploring a variety of solutions to the same problem.\nOne interesting takeaway is that our restriction to local updates did not hurt performance -- MeVa is just as well able to dynamically exploit naive and LOLA learners as M-FOS is.\n- \"We argue that this throws out the baby with the bathwater\" -- we will remove this line.\n\nThanks again for your time!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699933767906,
                "cdate": 1699933767906,
                "tmdate": 1699933767906,
                "mdate": 1699933767906,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Gcy1rTAjcK",
                "forum": "3OzQhhPLyW",
                "replyto": "C4c50boRRi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6122/Reviewer_c4Mz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6122/Reviewer_c4Mz"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications. I will keep my score as is."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609372225,
                "cdate": 1700609372225,
                "tmdate": 1700609372225,
                "mdate": 1700609372225,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "79O2F07jYJ",
            "forum": "3OzQhhPLyW",
            "replyto": "3OzQhhPLyW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6122/Reviewer_VzC1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6122/Reviewer_VzC1"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an algorithm that allow agents to model learning processes in the extended future with a value function. This generalizes previous work, namely LOLA, which only models the myopic one-step ahead learning dynamics.\nIt is pointed out that there are previous works, and other generalizations of LOLA, that also learn meta-value functions.\nUnlike other attempts at genearlizing LOLA, the claim is that this algorithm is \"self-consistent\" meaning that \"the algorithm does not assume the opponent is naive and it is aware of its own learning process as well\".\nResults are shown on logistic and matrix games, showing that meta values outperform LOLA uniformly across the 4 environments and that meta-values tend to be competitive with M-FOS.\nUnfortunately, head-to-head results between meta-values and M-FOS are not presented due to computational restrictions.\n\n# Decision\n\nWhile I like several things about this paper and find the proposed algorithm promising, I think the paper has too many issues to warrant acceptance. I am not sure whether these can be adequately addressed in a rebuttal, but I am tentatively rating the paper below the acceptance threshold."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- An overall interesting idea that applies important ideas from meta-learning, single-agent reinforcement learning and multi-agent RL. It is also well motivated by the successes of LOLA and other algorithms that they build on. And the theoretical foundations, while not formally rigorous, are convincing."
                },
                "weaknesses": {
                    "value": "- The paper makes a few statements that are vague and which can be true, but require more specificity. Please see the detailed comments.\n- Experimental evaluation seems limited from my point of view, but perhaps this is typical for multi-agent work? The policy spaces are small, and even in these small problems the authors allude to computational concerns for head-to-head results between M-FOS and meta-values. I would also like to see error bars on the performance. Overall, I consider this a weakness, but less so than the first weakness which is the primary factor in my decision."
                },
                "questions": {
                    "value": "- Section 1: (Clarity around Proposed improvements to LOLA): You state that your approach is self-consistent and explain that it \"does not assume the opponent is naive and that it is aware of its own learning as well\". I find this point unclear and/or redundant.\n\n  In my understanding, the first claim is that the policy convergence of your algorithm does not depend on the opponent following a particular algorithm (specifically, an algorithm that is not learning aware). This is interesting, but can be stated more clearly.\n\n  I think the second claim is that your algorithm is aware of its own learning process. But this is also true of LOLA to a first-order approximation, correct?.\n\n  The second contribution states that other approaches to estimating the meta-value using extrapolation using naive learning. Again, the framing around naive vs non-naive learning is confusing and it may help to state explicitly what this means earlier than section 2.1 if it is a focal point of section 1.\n\n- Section 1 (Clarity around Extrapolation): You refer to the term \"extrapolation\" but do not define it and I do not think it is common terminology. I assume extrapolation refers to the policy being followed in the bootstrapping step, but I am not sure.\n\n\n- Section 2.2: You introduce LOLA and a few variants before introducing the proposed algorithm. There seems to be more than a few connections between those previous approaches and the meta-value function approach, and I think the paper would benefit from circling back and relating the proposed algorithm to previous work. The results section, for example, only shows the performance benefit of the proposed approach without a clear demonstration of why or how the extended predictions provided by the value function are beneficial.\n\n- Section 2.3 (M-FOS Description): Contrasting the description of M-FOS here with the earlier one, I do not see how M-FOS can be simultaneously \"solving the inconsistency\" and \"not learning, but acting with learning awareness\". If the value function does solve the inconsistency then learning an arbitrary meta-policy from this value function should be seen as \"learning with learning awareness\" rather than merely acting.\n\n- Section 3 (In place of implicit gradients) At the end of section 3 you remark that once the meta-value function is trained, you can substitute the gradient of the approximation with the implicit gradient of the real meta-value funciton. What is not obvious to me is where the gradient of the implicit meta-value function was used i nthe first place.\n\n\n- Section 4.1 \"This variant is more strongly grounded in the game and helps avoid the detachment from reality that plagues bootstrapped value functions\"\n  Related to my last point, I do not see the proposed advantage. I do not know what it means for something to be more grounded in a game, nor how this reformulation achieves that.\n\n- Section 4.3 (Exploration): I understand that there is a balance between too much noise for learning and too little noise for exploration. The proposed approach, flipping the sign of the final layer, and the accompanying explanation does not make sense to me. Of course perturbing the output layer would change the behavior of the inner policy, but this seems like a very crude source of noise in comparison to the conventional approach of small gaussian noise added to all parameters.\n\n  I am also confused as to why there are interleaved perturbed and unperturbed rollouts. If the unperturbed rollouts are used to update V, then does that mean the exploration procedure is closer to \"exploring starts\"\n\n- Section 3 and 4 (Overall): This would all be much clearer if you outlined what exactly is the meta-MDP for the meta-value function. It seems to be not episodic, is that correct?\n\n\n# Minor Comments\n- Section 2: You should explicitly state that you are studying two-player differentiable games in the first sentence for clarity.\n\n- Section 4.1 (Bellman Equation for U): It would be good to show more details how you arrived at this bellman equation as it was not immediately obvious to me (Expanding V(x + \\alpha \\nabla V(x)) and substiuting U(x) within the recurison). I also do not see why this would make any difference in learning the value function, as something similar can be done in a single agent RL setting. It is not clear what advantage the correction formulation provides."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6122/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823531752,
            "cdate": 1698823531752,
            "tmdate": 1699636662751,
            "mdate": 1699636662751,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FgQnilynz9",
                "forum": "3OzQhhPLyW",
                "replyto": "79O2F07jYJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6122/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for a very thorough review. We believe several of the issues raised are the result of misunderstandings, we hope to clear them up below.\n\n- Section 1: (Clarity around proposed improvements to LOLA):\n\n  The point is unclear rather than redundant. In the context of LOLA, \"consistency\" means that the optimization process in the simulated updates is the same as that in the true updates. In LOLA, the simulated update is a naive one, while the true update is a LOLA update. In naive learning, the simulated update is no update at all, while the true update is a naive update.\n\n  LOLA exhibits two kinds of inconsistency:\n  - The simulated update is naive when the true update is a LOLA update.\n  - The simulated optimization process only optimizes the opponent and not the agent itself (equivalently: the agent itself is simulated to not learn at all).\n\n  We swept this second kind under the rug in the name of simplicity: LOLA as described by Eqn (2) only exhibits the first kind of inconsistency, while LOLA proper (given in Appendix B) exhibits both. Reviewer rws5 also appeared to be confused by this, so we will make it explicit, ideally reworking the equations to be written from a single player's perspective like in MetaMAPG. Apologies for the confusion.\n\n  We would moreover like to clarify that consistency in this sense is not related to convergence of the algorithm. It also does not mean invariance to the opponent's algorithm; a consistent surrogate explicitly needs to account for the opponent's algorithm. It may perhaps be construed as an equivariance.\n- Section 1 (Clarity around Extrapolation): Extrapolation is used in the sense of forecasting the future (of the optimization process). We will make sure to clarify this in the paper as it is indeed a pretty overloaded term.\n- Section 2.2: Good point, we will add more discussion for both the Logistic game and the matrix games.\n- Section 2.3 (M-FOS Description): You say \"learning an arbitrary meta-policy from this value function should be seen as \"learning with learning awareness\" rather than merely acting\"; we think this mixes up levels of learning. Learning an arbitrary meta-policy would be \"learning to act with learning awareness\", whereas learning a local meta-policy is \"learning to learn with learning awareness\". We will clarify this in the paper, as it is a subtle point.\n- Section 3 (In place of implicit gradients): it is used to update the policies $x$ in the inner learning process. We use $V$ as a surrogate for $f$; it is a game on which naive learning is naive-learning-aware.\n- Section 4.1 (Grounding): the practical difference between the $U$ and $V$ formulations is that the $U$ formulation provides useful gradients from the start. Whereas $\\nabla V$ relies entirely on the model, $\\nabla f + \\gamma \\nabla U$ sort of \"defaults\" to the naive gradient $\\nabla f$ and learns to modify it eventually. In practice it appears the gradients $\\nabla V$ (and $\\nabla U$) start out nearly zero, and falling back to the naive gradient allows the system to discover a (minimal) variety of policies. In the process of fitting these policies, the model starts producing more useful gradients.\n\n  One failure mode that we have observed with $V$ is a tendency for policies to become overly cooperative. This is a consequence of the TD error $f(x) + \\gamma V(x') - V(x)$ (where $x'=x+\\alpha \\nabla V(x)$) having minimal weight on the ground truth term $f(x)$ and comparatively large weight (effectively $\\frac{\\gamma}{1-\\gamma}$) on the bootstrapped return estimate $V(x')$. With $V$, the $f(x)$ term in the TD error is the only way in which ground truth enters the system. With $U$, the ground truth additionally enters the system through the use of $\\nabla f$ in the optimization process. This is the main sense in which we say $U$ is more strongly grounded in $f$.\n\n  Evidently, we need to discuss this more explicitly in the paper.\n- Section 4.3 (Exploration): There may be a misunderstanding -- we do not apply the noise to the inner policy ($x$) but to the meta-policy ($V$). A perturbation to the meta-policy will cause the meta-rollouts ($x$ optimization trajectories) to go in systematically different directions than they otherwise would. By flipping signs on the learned features in terms of which $V$ predicts the value, we hope to make those directions semantically meaningful. Adding gaussian noise to the parameters of $V$ would potentially do a similar thing, however it is unclear how to scale such noise especially when parameters have an extremely wide range of different sensitivities. Multiplicative noise seems more appropriate to us.\n- Section 3 and 4 (what exactly is the meta-MDP): It is indeed not episodic. We will fully specify the meta-MDP in the paper. Most of the pieces are there in the prose in Sec 2.3, except the action space which we left vague because it differs between the prior work and ours (where it is local and depends on the state).\n\nThank you!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699928128081,
                "cdate": 1699928128081,
                "tmdate": 1699928128081,
                "mdate": 1699928128081,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1MEHuhR5Ol",
                "forum": "3OzQhhPLyW",
                "replyto": "79O2F07jYJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6122/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to provide an important clarification regarding the final sentence of your summary:\n\n*Unfortunately, head-to-head results between meta-values and M-FOS are not presented due to computational restrictions.*\n\nThe restrictions are not computational but methodological; essentially it is not clear how to perform the experiment fairly. It is easy to compare LOLA vs LOLA head-to-head because the meta-policy (LOLA) does not have any parameters to be learned upfront. M-FOS and MeVa *do* have parameters, and comparing M-FOS vs LOLA (or MeVa vs LOLA) involves training M-FOS (or MeVa) with optimization trajectories so they can adapt to LOLA's behavior.\n\nComparing M-FOS vs MeVa head-to-head requires training them jointly. M-FOS and MeVa learn meta-policies; let us call the process by which they learn these meta-policies ``meta-meta-policies'' (concretely, naive learning). The purpose of a head-to-head evaluation of meta-policies (e.g. LOLA vs LOLA) is to learn something about the interaction between these meta-policies. However, when trained jointly, these meta-policies are moving targets, and whatever we observe will be confounded by the interactions of the meta-meta-policies.\n\nMoreover, we would wish to choose hyperparameters for each method, in order to give both a fair opportunity. Do we choose the ones that make MeVa look good, or the ones that make M-FOS look good? Or do we choose the ones that make both look equally good? At the risk of carrying the terminology too far, we might call the hyperparameter optimization scheme a meta-meta-meta-policy. It's two-player general-sum games all the way up."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699976915461,
                "cdate": 1699976915461,
                "tmdate": 1699976915461,
                "mdate": 1699976915461,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "i45J8uA15B",
            "forum": "3OzQhhPLyW",
            "replyto": "3OzQhhPLyW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6122/Reviewer_ZYT4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6122/Reviewer_ZYT4"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes the MeVa method designed for two-player zero-sum meta-games. By extending the concept and form of \"looking ahead\" methods, and incorporating a discounted sum over returns, it allows for an extrapolated approximation of the meta-value. Additionally, from a practical standpoint, MeVa employs $U(x)$ to approximate the extrapolated value, which helps avoid the detachment from reality often encountered with bootstrapped value functions. Experimentally, the study analyzes the method's behavior on a toy game and makes comparisons to previous work on repeated matrix games."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* **Novelty**: This work innovatively extends the concept and form of \"looking ahead\" methods, while introducing a discounted sum over returns. This allows for an extrapolated approximation of the meta-value, sidestepping the need for approximating the gradient of the policy.\n* **Presentations**: The paper presents its viewpoints and theoretical discussions in a lucid and straightforward manner, making it easy for readers to grasp its underlying premise and theoretical implications.\n* **Experimental Analysis**: The study compares MeVa with methods like LOLA, HOLA, and COLA. In the meta-games, MeVa demonstrates superior performance.\n* The paper fairly analyzes the limitations and prospects of the proposed method."
                },
                "weaknesses": {
                    "value": "* **Theoretical Guarantees**: Although the method leverages the concept of looking ahead, given its use of meta-value, I believe there should be some theoretical analysis regarding its convergence and computational complexity, etc. However, I did not find such discussions in the paper.\n* **Experiments**: The experiments primarily compare against baseline methods that are not specifically designed for meta scenarios. I believe it would be more informative to design experiments comparing with other meta-specific methods beyond M-FOS, such as Meta-PG, meta-MAPG.\n* **Reproducibility**: The source code is not submitted, making reproducibility uncertain.\n* **The targeted scenarios are somewhat restrictive**:\n    * The scope of the approach seems somewhat limited. As it's currently tailored for two-player zero-sum meta-games, it might be challenging to expand to tasks with more complex state and action spaces.\n    * The paper assumes that one can observe the strategy parameters of the opponent. This assumption might be difficult to uphold in real-world tasks."
                },
                "questions": {
                    "value": "* As previously mentioned, within the MeVa framework, is it possible to analyze the algorithm's convergence properties?\n* Please include benchmark experiments for Meta-PG and Meta-MAPG to provide a more comprehensive comparison.\n* In two-player zero-sum games, might some baseline methods that compute equilibria also be included in the comparison experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6122/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6122/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6122/Reviewer_ZYT4"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6122/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836210047,
            "cdate": 1698836210047,
            "tmdate": 1699636662636,
            "mdate": 1699636662636,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g6oNZuQiyt",
                "forum": "3OzQhhPLyW",
                "replyto": "i45J8uA15B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6122/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review. Please see our responses below.\n\n- Theoretical Guarantees: we do briefly discuss convergence at the end of Section 3.2. We also expected to be able to inherit convergence results from Q-learning. Unfortunately, multi-agent Q-learning is not known to converge even in the discrete/tabular case. The Bellman operator is generally not a contractive map when multiple agents maximize different objectives.\n- Experiments: we will include a comparison to MetaMAPG on the matrix games. This will take a few days to come together. The M-FOS authors (Sec 5.1) note that MetaMAPG does not scale beyond 7 updates, and use a variant based on exact gradients. We will follow them in this regard.\n- Reproducibility: apologies, we should have submitted the code here on OpenReview. It is available at https://github.com/MetaValueLearning/MetaValueLearning .\n- The targeted scenarios are somewhat restrictive:\n  - We present the method in two-player form for simplicity, but the generalization to n players is straightforward: each player learns and optimizes their own meta-value.\n  - We are not sure why you say the method is tailored to zero-sum meta-games; that does not appear to us to be the case. E.g. on IPD, some meta-policies lead to defection whereas others lead to cooperation, with different total payoffs.\n  - It is true that we assume access to opponent parameters. We believe working on top of opponent models is entirely possible, but the science is better done in a controlled setting. Even there, the nonstationarity problem of MARL is far from solved.\n- *In two-player zero-sum games, might some baseline methods that compute equilibria also be included in the comparison experiments?*\nThe only zero-sum game we consider is Iterated Matching Pennies, and it only has a single Nash equilibrium at the uniform policy pair. Naive learning finds it without issue, so it's unclear what these other algorithms would add. Could you elaborate?\n\nWe hope to have addressed some of your concerns. Thank you!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699894448015,
                "cdate": 1699894448015,
                "tmdate": 1699894448015,
                "mdate": 1699894448015,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GQF1fVKkCn",
            "forum": "3OzQhhPLyW",
            "replyto": "3OzQhhPLyW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6122/Reviewer_rsw5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6122/Reviewer_rsw5"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Meta-Value Learning (MeVa), a general framework for learning with learning opponent awareness in MARL. MeVa uses a meta-value method to account for longer-term behaviours of opponents and does not require policy gradients. It is consistent and far-sighted, avoiding the need to explicitly represent the continuous action space of policy updates. Many evaluations are conducted on various games, including the Logistic Game, Iterated Prisoner's Dilemma, Iterated Matching Pennies, and the Chicken Game, demonstrating MeVa's effectiveness in opponent shaping and cooperation for MARL. The method shows its merit in achieving cooperation where self-interest warrants it, without being exploitable by other self-interested agents."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originality:**\n\nMeVa is a novel method based on meta learning for dynamic opponent modelling in MARL. Unlike previous methods, MeVa is consistent, meaning it does not assume its opponent is naive and is aware of its own learning process. MeVa is far-sighted, i.e., it looks more than one step ahead through a discounted sum formulation, which allows it to account for longer-term and higher-order interactions.\n\n**Quality:**\n\nOverall, MeVa is a high-quality method. It extends previous LOLA and other methods with value learning, which is based on value learning and does not require policy gradients anywhere. MeVa can be applied to optimization problems with a single objective.\n\n**Clarity:**\n\nOverall, the writing is good. It introduces comprehensive background and related works, which is easy for readers to follow. \n\n**Significance:**\n\nMeVa brings new insights into the MARL community, including its consistency on learning the full dynamics of the other agents, far-sightedness, value learning and implicit Q-function."
                },
                "weaknesses": {
                    "value": "1. Scalability: The main weakness of MeVa is its scalability, particularly when learning in environments with more agents. The method may struggle to handle large parameter vectors and complex multi-agent interaction when more agents are involved.\n\n2. Writing in the methodology section: This section can be improved by using similar notations from previous works, such as LOLA, Meta-PG and Meta-MAPG. It will make it consistency in notation and easy to follow.\n\n3. Algorithm 1 is hard to follow. It would be great to add more explanations."
                },
                "questions": {
                    "value": "Q1: In page 3,  authors mentioned that \u201cnevertheless there is always a gap where each player assumes it looks one step further ahead than their opponent\u201d. Could you please explain why there is a gap in LOLA?\n\nQ2: Why is Equation (3) a better surrogate?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6122/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6122/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6122/Reviewer_rsw5"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6122/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698979345804,
            "cdate": 1698979345804,
            "tmdate": 1699636662519,
            "mdate": 1699636662519,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VoDIQ29KGb",
                "forum": "3OzQhhPLyW",
                "replyto": "GQF1fVKkCn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6122/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review. We respond to the issues raised below.\n\nWeaknesses:\n1. It is true that the complexity of the problem increases with the number of agents, however it is not clear that this is a weakness of our method, as opposed to an inherent fact of life in MARL.\n2. Another reviewer also raised a concern about the clarity of the notation in Section 2. We will revisit the notation and ensure the meta-MDP is fully defined.\n3. The current algorithm is hard to follow because it was written to have full details regarding the application of the practical techniques. We will move this to an appendix and provide a simplified, readable presentation in the main text.\n\nQuestions:\n1. Our words were confusing and we will rewrite them. To see the gap, consider two LOLA agents A and B training against each other. A assumes B uses naive learning, when in truth B uses LOLA as well. And similarly, B assumes A uses naive learning, when A actually uses LOLA. In a sense, both players think they are smarter than their opponent. This is the gap. The same holds in naive learning, where each agent assumes the other to not be learning at all. And the same holds in HOLA2 when A assumes B uses LOLA and B assumes A uses LOLA, when in fact they are both using HOLA2.\n2. Two agents that follow the gradient of (3) make the correct assumption about how optimization would proceed, namely that both agents follow the gradient of (3). (Our notation in Section 2 is somewhat simplified and does not reflect LOLA's original proposal to only imagine the opponent's update; instead we consider the agents to be aware of their own learning as well. Ignoring the agent's own update would be a different, unrelated source of inconsistency.)\n\nWe hope that clears things up."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699890628501,
                "cdate": 1699890628501,
                "tmdate": 1699890628501,
                "mdate": 1699890628501,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]