[
    {
        "title": "TransCues: Boundary and Reflection-empowered Pyramid Vision Transformer for Semantic Transparent Object Segmentation"
    },
    {
        "review": {
            "id": "tCsBmGNCls",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission705/Reviewer_42YL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission705/Reviewer_42YL"
            ],
            "forum": "e9bEoxNiTJ",
            "replyto": "e9bEoxNiTJ",
            "content": {
                "summary": {
                    "value": "This paper presents a hierarchical architecture for transparent object segmentation. Boundary and reflection cues are incorporated in the module designs. Extensive experiments are conducted on multiple benchmarks, which shows the effectiveness of the proposed model. The paper is overall well-written and nicely structured."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed model achieves state-of-the-art performances on multiple datasets.\n2. Failure cases are well studied.\n3. The paper is overall well-written and nicely structured."
                },
                "weaknesses": {
                    "value": "1. In Fig. 5, does it show that the proposed method is consistently effective for different backbones? This should be better discussed.\n2. In Table 6, it would be nice to show the computation complexity of the two designed modules for analysis.\n3. How to theoretically verify that the proposed method did really make use of reflection cues? This could be better discussed.\n4. It is hard to find any novel operations in the proposed reflection feature enhancement module as it simply combines existing mechanisms. It would be nice to clarify the technical novelty and theoretical contributions of the proposed modules.\n5. There are extensive segmentation methods that introduce boundary-relevant loss designs or other designs. Please consider incorporating some existing boundary-relevant designs for a comparison. This can better show the superiority of your proposed boundary feature enhancement module. \n6. The related work follows that of Trans4Trans. It would be nice to add more recent related state-of-the-art works.\n\nSincerely,"
                },
                "questions": {
                    "value": "Would it be possible to incorporate your model with the RGB-D modalities for an experiment? This could be discussed.\n\nWhen the proposed model works on images without any transparent objects, would it create false positives? This could be assessed.\n\nSincerely,"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission705/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission705/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission705/Reviewer_42YL"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission705/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697345884666,
            "cdate": 1697345884666,
            "tmdate": 1699635997684,
            "mdate": 1699635997684,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uo2gPLQp2I",
                "forum": "e9bEoxNiTJ",
                "replyto": "tCsBmGNCls",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission705/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer 42YL (1/2)"
                    },
                    "comment": {
                        "value": "**Q1** In Fig. 5, does it show that the proposed method is consistently effective for different backbones? This should be better discussed.\n \n > **A1** The relationship between the complexity, size, and performance of the backbone is shown in Figure 5. It is evident that there is a clear trade-off between the three factors. The performance improves as the backbone size increases. This relationship is crucial for users as they need to balance the computational demands with the performance requirements of their specific applications. Our research findings suggest that choosing a larger backbone leads to better performance, which provides a strategic advantage for those aiming to enhance the effectiveness of their models.\n \n **Q2** In Table 6, it would be nice to show the computation complexity of the two designed modules for analysis.\n\n > **A2** The below is the updated version for Table 6. We added to the revised paper.\n > | **Backbone** | **GFLOPs&#8595;** | **MParams&#8595;** | **BFE** | **RFE** | **Stanford2D3D (mIoU) &#8593;** | **Trans10K-v2 (mIoU) &#8593;**  |\n > |--------------|:------------:|:-------------:|:------------:|:------------:|------------------------------------------------------|------------------------------------------------------|\n > | PVTv1-T      | 10.16 | 13.11 | -          | -          | 45.19                                                | 69.44                                                |\n > | PVTv2-B1     | 11.48 | 13.89 | -          | -          | 46.79 +1.6          | 70.49 +1.05          |\n > | PVTv2-B1     | 13.22 | 14.37 | -          | &check; | 48.12 +2.93          | 72.65 +3.21          |\n > | PVTv2-B1     | 19.55 | 14.39 | &check; | -          | 50.22 +5.03         | 74.89 +5.45 |\n > | PVTv2-B1     | 21.29 | 14.87 | &check; | &check; | 51.55 +6.36| 77.05 +7.61 |\n\n\n **Q3** How to theoretically verify that the proposed method did really make use of reflection cues? This could be better discussed.\n\n > **A3** As depicted in Figure 2 and quantified in Table 6, our method effectively extracts regions of reflection, substantially improving overall performance. A potential enhancement to this approach could involve incorporating a binary classifier responsible for identifying the lack of reflections before activating the RFE module. This preliminary classification step has the potential to conserve computational resources by determining when no reflections are present and deciding to bypass the RFE process, thus optimizing the method's efficiency.\n \n\n **Q4** It is hard to find any novel operations in the proposed reflection feature enhancement module as it simply combines existing mechanisms. It would be nice to clarify the technical novelty and theoretical contributions of the proposed modules.\n\n > **A4** The RFE module utilizes an extensive convolution-deconvolution architecture to process input data and generate an improved feature map. The system takes in input features and carefully processes them to generate a sophisticated feature map. The utilization of a layered architecture is of utmost importance, as it effectively handles visual data at various levels of abstraction, effectively navigating the complexities associated with diverse visual occurrences, including reflections. In contrast to models designed for global reflection scenarios, which assume that reflections are present throughout the entire image, our RFE module demonstrates superior performance in detecting local reflections. Implementing a localized technique is of utmost importance in real-world scenarios characterized by irregular and area-specific reflections rather than uniform ones. The RFE module demonstrates high accuracy in detecting glass surfaces within various environmental conditions, primarily by prioritizing the analysis of localized reflections. Hence, this approach offers a more accurate and contextually nuanced method for discerning reflection within the multifaceted fabric of real-world environments."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496280990,
                "cdate": 1700496280990,
                "tmdate": 1700496280990,
                "mdate": 1700496280990,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5m6PCiekc5",
                "forum": "e9bEoxNiTJ",
                "replyto": "ZMbNYaNzWS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission705/Reviewer_42YL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission705/Reviewer_42YL"
                ],
                "content": {
                    "title": {
                        "value": "Comment on A5 and A3"
                    },
                    "comment": {
                        "value": "Regarding the comparison mentioned in A5, if it is possible, please consider adding some numerical results of their performances. The responses in A3 do not fully present theoretical verifications of the exploitation of reflection cues for transparency perception.\n\nThe reviewer would like to thank the authors for their responses and added analyses.\n\nSincerely,"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700628644803,
                "cdate": 1700628644803,
                "tmdate": 1700628644803,
                "mdate": 1700628644803,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gwkXSKHJZ2",
            "forum": "e9bEoxNiTJ",
            "replyto": "e9bEoxNiTJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission705/Reviewer_3ba2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission705/Reviewer_3ba2"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the TransCues, a transformer encoder-decoder network for the segmentation of glass, mirrors, and transparent objects. The main idea of this paper is to model the boundary and the reflection cues. Accordingly, a Boundary Feature Enhancement (BFE) module and a Reflection Feature Enhancement (RFE) module are proposed. The BFE module is implemented based on the ASPP module and the RFE module has an encoder-decoder structure. The paper runs experiments on eight existing datasets, and the comparisons show that the proposed method achieves impressive results, but with different models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper has certain merits. \nAlthough the boundary and reflection cues have been explored in previous works, the paper shows that a better network design that focuses on low-level features may improve the segmentation of mirrors and glass surfaces/objects.\nThe paper provides extensive comparisons on eight benchmarks, which shows an overall picture of this topic.\nThe paper is generally easy to read and understand."
                },
                "weaknesses": {
                    "value": "However, I have some concerns.\nThe first concern is about the results. The paper creates a lot of models, I.e., TransCues -T, -S, -M, -L, -B1, -B2, -B3, -B4, -B5, while some of them are based on PVTv1, and the others are based on PVTv2. During the comparisons, Table 1 uses B4, Table 2 and 5 use B2, Table 3 and 4 use B3, and the Table 6 uses B1. This makes the comparisons very messy, which may not provide meaningful analysis/discussions. What are the criterion of such selections? I note that there are only one Table (Table 13 in the supplemental) includes all nine TransCues models, from which it seems that B1 and B2 outperforms Ours-L with less parameters. How often and why does this happen is not known. \nThe Abstract mentions that the RFE module ``decomposes reflections into foreground and background layers\u2019\u2019, however, in section 3.3, I do not find corresponding designs and the motivations of such designs. Second, section 3.4 uses pseudo ground truth reflection masks, but it is not mentioned how these pseudo labels are created. Third, the paper only discuss RFE with (Zhang et al., 2018) regarding the reflection modeling. The ICCV\u201921 paper ``Location-aware Single Image Reflection Removal\u2019\u2019 detects the strong reflections. Would it be better to use reflection removal methods to generate pseudo labels?\nThe boundary loss seems not a novelty. If so, I suggest to move it onto the supplemental. Otherwise, the paper needs to explain where the novelty is and provides discussions with existing methods. For example, the IJCV\u201922 paper ``Learning to Detect Instance-level Salient Objects Using Complementary Image Labels\u2019\u2019 uses canny operators to enhance the boundary information. The PMD (Lin et al. 2020) also uses ground truth boundary information for the supervision.\nThe feature flow in the RFE module (Figure 7 of supp.) is rather complicated and more explanation is helpful, in order to evaluate its novelty.\nThe placements of RFE and BFE seems casual. I can only guess the reason might be that the authors try to focus the whole network on low-level features. More explanation is helpful.\nThe ablation study only includes the RFE and BFE, while it is not known how much contributions the FEM, FPM and the final MLP have made to the segmentation performance.\nThe model relies on the detection of reflections, while for glass surface/objects segmentation, the question is whether reflections can always be detected, and if not, how does it affects the final results? The paper shows failure cases on the Trans10K-v2, but such cases seem dataset-specific. It is better to show failure cases that caused by the limitations of the proposed model.\n\n\n\n\nBelow are some suggestions.\nUse the symbols (including the use of, e.g., \\mathcal) more consistently.\nThe Position Embedding and the Encoder and Decoder paragraphs in section 3.1 can be shorten."
                },
                "questions": {
                    "value": "Please see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission705/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697517977253,
            "cdate": 1697517977253,
            "tmdate": 1699635997586,
            "mdate": 1699635997586,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zwmqtTIKZ4",
                "forum": "e9bEoxNiTJ",
                "replyto": "gwkXSKHJZ2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission705/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer 3ba2 (1/3)"
                    },
                    "comment": {
                        "value": "**Q1** The first concern is about the results. The paper creates a lot of models, i.e., TransCues -T, -S, -M, -L, -B1, -B2, -B3, -B4, -B5, while some of them are based on PVTv1, and the others are based on PVTv2. During the comparisons, Table 1 uses B4, Table 2 and 5 use B2, Table 3 and 4 use B3, and the Table 6 uses B1. This makes the comparisons very messy, which may not provide meaningful analysis/discussions. What are the criterion of such selections? I note that there are only one Table (Table 13 in the supplemental) includes all nine TransCues models, from which it seems that B1 and B2 outperforms Ours-L with less parameters. How often and why does this happen is not known. \n\n > **A1** The key criterion for our model selection is to ensure fair comparisons. For each experiment, we have carefully selected a our model variant that has similar model size used by other methods, as indicated in the respective tables. Due to the rich adaptation of transformers in practice, previous methods have used several different backbones with different sizes. It would be unfair to compare a large model to a small model as their performance and number of parameters are very different. For deployment in a real use case, one should choose a model that fits their computational budget on their platform. \n\n\n\n **Q2** The Abstract mentions that the RFE module decomposes reflections into foreground and background layers, however, in section 3.3, I do not find corresponding designs and the motivations of such designs. Second, section 3.4 uses pseudo ground truth reflection masks, but it is not mentioned how these pseudo labels are created. Third, the paper only discuss RFE with (Zhang et al., 2018) regarding the reflection modeling. The ICCV\u201921 paper Location-aware Single Image Reflection Removal\u2019\u2019 detects the strong reflections. Would it be better to use reflection removal methods to generate pseudo labels? \n\n > **A2** \n > * Apologies for this confusion and thank you for pointing this out. In Section 3.3, we meant that it is possible to separate the reflection area (foreground) from the non-reflection area (background). The mechanics of this process are depicted in Figure 7, where we detail the workings of the RFE module. We can see that after the last deconvolution layer, we split the features into two parts: the enhanced features (with reflection property)  and the reflection mask area (which is supervised by pseudo ground truth). This bifurcation is pivotal in our model's ability to identify and segregate reflective surfaces from their surroundings accurately. We will revise the writing to improve the clarity on this point. \n > * Section 3.4: The $\\phi()$ function function employs an erosion operation on the semantic mask to isolate potential reflection areas that are crucial for our model's accurate discernment of reflective surfaces (we assume that common categories like window, door, cup, bottle, etc will have reflective appearance). \n > * Regrading to utilizing reflection removal methods, it can help to generate pseudo labels can offer certain advantages. However, it is crucial to acknowledge that these methods primarily focus on mitigating global reflections, which occur when the presence of glass encompasses the entirety of an image. The approach above demonstrates limitations when applied to intricate real-world situations, particularly those involving glass objects distributed across the scene rather than occupying a dominant position. The RFE module in our study presents a comparison of different approaches. It can detect localized reflections and distinguish glass surfaces based on the semantic mask. It is better suited to the diverse and unpredictable conditions found in real-world situations, where reflections are specific to certain areas rather than uniformly distributed over the entire image. As a result, the RFE module provides a more accurate and contextually appropriate approach for identifying reflections in real-world environments."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496019307,
                "cdate": 1700496019307,
                "tmdate": 1700496019307,
                "mdate": 1700496019307,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2tfOrywmft",
            "forum": "e9bEoxNiTJ",
            "replyto": "e9bEoxNiTJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission705/Reviewer_WwQc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission705/Reviewer_WwQc"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an efficient transformer-based segmentation architecture TransCues, which exhibits strong performance in segmenting transparent objects. This capability is attributed to the innovative integration of the Boundary Feature Enhancement module and the Reflection Feature Enhancement module. \nThe authors show solid results on various transparent object segmentation and generic semantic segmentation benchmarks and conducts comprehensive ablation studies on their core design choices."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The content is well-organized and easy to follow. The motivation is well-established and the effectiveness of their solution is verified by extensive experiment. The proposed architecture achieved competitive performance on a wide range of tasks, while maintaining competitive efficiency."
                },
                "weaknesses": {
                    "value": "The authors regard the boundary loss as their contribution, but do not provide an ablation of this module. Similarly, the reflection loss also has not been ablated.\nThe authors claim that their proposed approach is robust to generic semantic segmentation tasks, but do not evaluate on the most widely used semantic segmentation datasets, such as ADE20K and cityscapes.\nThe influence of different pretraining of the backbone is not properly assessed;\nThe authors claim that most semantic segmentation models struggle to distinguish between glass and non-glass regions, but does this assertion still hold true for the state of the art generic semantic segmentation model, such as SAM?"
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission705/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699264345228,
            "cdate": 1699264345228,
            "tmdate": 1699635997501,
            "mdate": 1699635997501,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o2tiQ1OLZK",
                "forum": "e9bEoxNiTJ",
                "replyto": "2tfOrywmft",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission705/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission705/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer WwQc (1/2)"
                    },
                    "comment": {
                        "value": "**Q1&2** The authors regard the boundary loss as their contribution, but do not provide an ablation of this module. The reflection loss also has not been ablated.\n\n > **A1&2** We provided the ablation studies in Section 4.2 (Effectiveness of different modules) in the paper. As shown in Figure 7, the boundary loss and the reflection loss are tied to the network architecture with BFE and RFE modules. Therefore, the ablation studies with these losses are equivalent to ablation studies on the BFE and RFE modules. \n\n **Q3** The authors claim that their proposed approach is robust to generic semantic segmentation tasks, but do not evaluate on the most widely used semantic segmentation datasets, such as ADE20K and cityscapes. \n\n > **A3** Selecting which dataset to perform the experiments has been an important topic of discussion when our team developed this paper. To rigorously assess the effectiveness of our proposed method, we have curated a diverse array of datasets that encompass a broad spectrum of tasks, fields of view (FOV), and object placements, as detailed in Section C.1. We have conducted extensive testing on the Stanford2D3D dataset as this dataset reflects well real-world scenarios, where common objects dominate and transparent objects are less than $1\\%$ of the total dataset.\n >  \n > As suggested by the reviewer, we conducted additional experiments on ADE20K and CityScapes datasets, with the results (**mIoU**) shown below and were added to Section D.4 and Table 19. Note that the results of other methods are taken from [Paperswithcode-ADE20k](https://paperswithcode.com/sota/semantic-segmentation-on-ade20k), [Paperswithcode-CityScapes](https://paperswithcode.com/sota/semantic-segmentation-on-cityscapes) and their manuscripts, and sorted by ascending order of GFLOPs ($512 \\times 512$). \n >\n > As can be seen, our method performs well on both datasets, with mIoU **$47.5\\%$** on ADE20K and **$81.9\\%$** on CityScapes.\n > \n > | Method                                             | GFLOPs&#8595; | MParams&#8595; | ADE20K (**mIoU**) &#8593; |  CityScapes (**mIoU**) &#8593; |\n > |:-------------------------------------------------- |:-------------:|:--------------:|:------:|:------------:|\n > | Trans4Trans-M (PVTv2-B3) **[Zhang et al., 2022]**  |     41.9      |      49.6      |    -   |     69.3     |\n > | Semantic FPN (PVTv2-B3) **[Wang et al., 2022]**    |     62.4      |      49.0      |  47.3  |      -       |\n > | Ours-b3 (PVTv2-B3)                                 |     68.3      |      51.2      |  47.5  |     81.9     |\n > | MogaNet-S (SemFPN)   **[Li et al., 2023]**         |     189       |      29.0      |  47.7  |      -       |\n > | NAT-Mini (UPerNet) **[Hassani et al., 2023]**      |     900       |      50.0      |  46.4  |      -       |\n > | InternImage-T (UPerNet) **[Wang et al., 2023]**    |     944       |      59.0      |  47.9  |     82.5     |\n > \n > **[Wang et al., 2022]** Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. In Computational Visual Media, 2022.\n > \n > **[Zhang et al., 2022]** Jiaming Zhang, Kailun Yang, Angela Constantinescu, Kunyu Peng, Karin M\u00a8uller, and Rainer Stiefelhagen. Trans4trans: Efficient transformer for transparent object and semantic scene segmentation in real-world navigation assistance. In IEEE T-ITS, 2022.\n > \n > **[Li et al., 2023]** Siyuan Li, Zedong Wang, Zicheng Liu, Cheng Tan, Haitao Lin, Di Wu, Zhiyuan Chen, Jiangbin Zheng, Stan Z. Li. Efficient Multi-order Gated Aggregation Network. In arXiv 2211.03295v2, 2023. \n > \n > **[Hassani et al., 2023]** Ali Hassani and Steven Walton and Jiachen Li and Shen Li and Humphrey Shi. Neighborhood Attention Transformer. In CVPR, 2023.\n > \n > **[Wang et al., 2023]** Wang, Wenhai and Dai, Jifeng and Chen, Zhe and Huang, Zhenhang and and others. InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions. In CVPR, 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission705/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495787961,
                "cdate": 1700495787961,
                "tmdate": 1700495787961,
                "mdate": 1700495787961,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]