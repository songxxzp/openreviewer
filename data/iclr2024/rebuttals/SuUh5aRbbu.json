[
    {
        "title": "End-to-end Story Plot Generator"
    },
    {
        "review": {
            "id": "3JpJR1l9M7",
            "forum": "SuUh5aRbbu",
            "replyto": "SuUh5aRbbu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8614/Reviewer_G5uN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8614/Reviewer_G5uN"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an end-to-end model for the task of (text) story plot generation. The authors first replicate a previous model for story generation (DOC) with open-source architectures and show challenges and fixes to overcome some of the challenges in the transition. Then, they use this model to create a large number of story plots that are then used to fine-tune another, 7B model for this task, showing competitive performance (according to GPT-4) with the teacher model. Finally, the authors collect human preferences from plots generated with the same premise and further tune the fine-tuned model with RLHF, resulting in better performance across 5 metrics (again, according to GPT-4)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The task of story plot generation is interesting and allows to disentangle two major phases of content creation often seen in humans: planning and coarse-to-fine generation\n2. The authors show the intricacies of adapting a previous model from closed-source APIs to open-source alternatives that do not have the same capabilities (completion/infilling vs. chat)\n3. The authors can train a 7B model that is relatively fast and seems to generate stories of similar quality as a 13B model that follows the original protocol with 100s of calls.\n4. The paper is well written and easy to follow"
                },
                "weaknesses": {
                    "value": "1. My major concern is the lack of human evaluation. The authors state multiple times that \u201cstory plot is relatively short and thus easy for humans to evaluate\u201d yet perform no human evaluation. Relying solely on a model like GPT-4 for such a complex task is, in my opinion, a major limitation to the soundness of the claims. I hence invite the authors to pair the main evaluations (ie Table 5 and 6) with human evaluation. I will then happily advocate for acceptance.\n2. Another minor limitation is that RLHF results in 5 different models, which are each compared against a single SFT model. Could the authors consider combining the different rewards into a single RLHF model and then ask humans to compare it against SFT according to the metrics Q1-6?\n\n---\nRebuttal: The authors added human evaluation, and said that (2) is a direction for future work."
                },
                "questions": {
                    "value": "1. The Related Work section is comprehensive. To make the paper\u2019s scope broader and linked to work in the computer vision and multimodal communities, I would recommend adding a brief paragraph on work in story generation for image-to-text [1], text-to-image [2] and text-to-video [3] tasks.\n2. Are the results in Table 4 a \u201cvalidation\u201d of the same RLHF training story plots?\n3. In page 2, when you say \u201ca completion model which accepts a suffix\u201d I was not entirely sure what you meant. It then became clear later throughout the paper. I recommend adding a brief explanation that you mean a model that can do text infilling given a text prefix and a text suffix, and cite [4].\n4. Line 3 of Sec 3 should reference Table 7, not Table 1.\n5. First line of Sec 3.2 might make it explicit that the RLHF results in 5 models.\n\n---\n\n[1] Huang et al. Visual Storytellin. NAACL\u201916\n\n[2] Li et al. StoryGAN: A sequential conditional gan for story visualization. CVPR\u201919\n\n[3] Bugliarello et al. StoryBench: A Multifaceted Benchmark for Continuous Story Visualization. arXiv 2308.11606\n\n[4] Bavarian et al. Efficient Training of Language Models to Fill in the Middle. arXiv 2207.14255"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8614/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8614/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8614/Reviewer_G5uN"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8614/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698486566396,
            "cdate": 1698486566396,
            "tmdate": 1700730814744,
            "mdate": 1700730814744,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G2YARJ8bWV",
                "forum": "SuUh5aRbbu",
                "replyto": "3JpJR1l9M7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8614/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8614/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their helpful and insightful comments. Below we respond to the questions respectively.\n\n> My major concern is the lack of human evaluation\u2026 I hence invite the authors to pair the main evaluations (ie Table 5 and 6) with human evaluation. I will then happily advocate for acceptance.\n\nThanks for your suggestion. We added human evaluation results, and the details can be found in the global response.\n\n>  Another minor limitation is that RLHF results in 5 different models, which are each compared against a single SFT model. Could the authors consider combining the different rewards into a single RLHF model and then ask humans to compare it against SFT according to the metrics Q1-6?\n\nTraining a unified model that can produce better story plots in multiple aspects simultaneously would definitely be an interesting future direction, which is also studied in a concurrent work [1]. Note that according to our human evaluation results, the model RLPlot_Q1 achieves better performance than E2EPlot consistently in all the aspects, which shows that even if we only use a reward model for a specific aspect, it might generalize well to other aspects since usually different aspects might be correlated. \n\n> Q1. The Related Work section is comprehensive. To make the paper\u2019s scope broader and linked to work in the computer vision and multimodal communities, I would recommend adding a brief paragraph on work in story generation for image-to-text [1], text-to-image [2] and text-to-video [3] tasks.\n\nThanks for your suggestion. We have added a brief paragraph on these three works in the revision.\n\n> Q2. Are the results in Table 4 a \u201cvalidation\u201d of the same RLHF training story plots?\n\nThe 7000 story pair preference dataset was split into training and validation sets with a ratio of 9:1 for each one of the five reward models. We have made it clear in the revision.\n\n> Q3 .In page 2, when you say \u201ca completion model which accepts a suffix\u201d I was not entirely sure what you meant. It then became clear later throughout the paper. I recommend adding a brief explanation that you mean a model that can do text infilling given a text prefix and a text suffix, and cite [4].\n\nThanks for your suggestion. We have fixed it in the revision.\n\n> Q4. Line 3 of Sec 3 should reference Table 7, not Table 1.\n\nThanks for catching this! We have corrected it in the revision.\n\n> Q5. First line of Sec 3.2 might make it explicit that the RLHF results in 5 models.\n\nThanks for your suggestion. We explicitly mentioned that RLHF results in 5 models in the revision.\n\n\n**References:**\n\n[1] Learning Personalized Story Evaluation, https://aps.arxiv.org/abs/2310.03304."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455839912,
                "cdate": 1700455839912,
                "tmdate": 1700455839912,
                "mdate": 1700455839912,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ndQPMGslZX",
                "forum": "SuUh5aRbbu",
                "replyto": "G2YARJ8bWV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8614/Reviewer_G5uN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8614/Reviewer_G5uN"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my questions. I have increased my score. \nI noticed several concerns raised by other reviewers, with which I'll engage during reviewer discussion."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730686656,
                "cdate": 1700730686656,
                "tmdate": 1700730686656,
                "mdate": 1700730686656,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PSGlhXrFYT",
            "forum": "SuUh5aRbbu",
            "replyto": "SuUh5aRbbu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8614/Reviewer_vgZV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8614/Reviewer_vgZV"
            ],
            "content": {
                "summary": {
                    "value": "This paper attempts to replicate the story outline generation by DOC (Yang et al. 2022a), which makes use of GPT-3. Instead, this paper replaces GPT-3 with the opensource Llama2-13B. In addition, it performs an end-to-end finetuning on Llama2-7B, achieving speedup over repeated API calls. Finally, it performs RLHF on a collected dataset of human feedback.\n\nGiven the low-quality author response, I decided to lower my score to strong reject."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is generally well written, if missing a few technical definitions here and there. \n\nThe speed-up in Table 1 is significant. \n\nThe human comparison data of the 7000 story pairs would be quite interesting, if released."
                },
                "weaknesses": {
                    "value": "For someone who is familiar with DOC and LLMs, the paper does not seem to offer any new insight or new knowledge. Yes, Llama-2 and GPT-3 can do roughly the same things. Yes, we can use a supervisedly finetuned model to replace repeated LLM calls. Yes, we can do RLHF. All these are common knowledge. Hence, it may seem that the paper does not make a real scientific contribution. It may be an interesting engineering effort, but may not qualify as a scientific publication. \n\nThe decision to use GPT-4 as the evalution for final story quality seems dubious. The paper has not offered any evidence that GPT-4 is good at the task. The paper makes the claim that the RLHF stories are better at suspense and surprise. However, do we know if GPT-4 is good at detecting suspense or surprise? \n\nSince the author has spent significant effort to collect human ratings on 7000 story pairs, why not do another 300 pairs? This would create a much more solid evaluation. \n\nHow is story generation different from other forms of structured text generation, such as poetry or argumentative essays? The paper has used specific prompts to handle aspects of stories such as characters or settings. But it is not explicit that if there is any principle behind the writing of these prompts or if they solely rely on trial and error by the paper authors. This goes back to the question: what is it that we can learn from this paper?\n\nMinor comments:\n\nSection 2.1.1 The authors do not define what is meant by \"supports suffix\". I managed to guess the meaning from the context but it created temporary confusion. \n\nThe authors make several claims about how humans supposedly do things. Humans write stories in a specific way (Page 2, Paragarph 2). Humans write from coarse to fine (Challenge 1). However, these claims are unsubstantiated."
                },
                "questions": {
                    "value": "- What novel or surprising scientific insights or findings are reported by this paper?\n- What evidence can support the claim that humans write stories by first planning an outline?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8614/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8614/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8614/Reviewer_vgZV"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8614/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761129197,
            "cdate": 1698761129197,
            "tmdate": 1700749206021,
            "mdate": 1700749206021,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aiqK49WW5r",
                "forum": "SuUh5aRbbu",
                "replyto": "PSGlhXrFYT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8614/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8614/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their helpful and insightful comments. Below we respond to the questions respectively.\n\n> For someone who is familiar with DOC and LLMs, the paper does not seem to offer any new insight or new knowledge. Yes, Llama-2 and GPT-3 can do roughly the same things. Yes, we can use a supervisedly finetuned model to replace repeated LLM calls. Yes, we can do RLHF. All these are common knowledge. Hence, it may seem that the paper does not make a real scientific contribution.\n\nAs reviewer zk9y mentioned, Our paper \u201caddresses a difficult problem, which has a long history in AI and to which the advent of LLM offers new perspectives\u201d, Also, our work \u201ccontains a number of original aspects (experimental approach, evaluation)\u201d and \u201cincludes a transparent account of prompt engineering aspects faced during the development of the work, which might be beneficial to readers\u201d. We discussed this and our contribution in the **Novelty** section in the global response in detail. Also, studying the e2e **plot** generator instead of the whole story generator provides important insight into the story generation problem. See the **Why story plot rather than the entire story** section in the global response for more details.\n\n> Since the author has spent significant effort to collect human ratings on 7000 story pairs, why not do another 300 pairs? This would create a much more solid evaluation.\n\nThanks for your suggestion. We added human evaluation results during the rebuttal to make the evaluation more solid. Please see the global response for details. \n\n> How is story generation different from other forms of structured text generation, such as poetry or argumentative essays? The paper has used specific prompts to handle aspects of stories such as characters or settings. But it is not explicit that if there is any principle behind the writing of these prompts or if they solely rely on trial and error by the paper authors.\n\nThe reason that we use the (premise, settings, characters, outlines) structure of the plot is as follows (also discussed in the response to reviewer zk9y). \n\n- The underlying principle is \u201ccoarse-to-fine\u201d, i.e., to progressively provide more and more detailed information. This coarse-to-fine representation makes end2end autoregressive generation easier.\n- While short, this representation still contains a good amount of information about the story, which is helpful for multiple perspective evaluations (represented by multiple questions). \n- This representation is used by the SOTA story generation work DOC [1] for the planning stage. It can be used to generate high-quality full-length stories. This demonstrates that such a structure can lead to full story generation.\n- The plot structure we chose in this paper is text-based, which could greatly benefit from the powerful text-based LLMs that take the plot as input and produce the whole story (usually with multiple steps such as the second stage of DOC [1]).\n\nOther forms of structured text generation, like poetry or argumentative essays, may not require that many levels of hierarchy. \n\nWe acknowledge that we haven\u2019t further leveraged the special structure of the story yet (e.g., characters) in our generation process, which will be our future work.  \n\n> Section 2.1.1 The authors do not define what is meant by \"supports suffix\". I managed to guess the meaning from the context but it created temporary confusion.\n\nWe have fixed it in the revision to avoid confusion. Thanks for your comment.\n\n\n> The authors make several claims about how humans supposedly do things. Humans write stories in a specific way (Page 2, Paragarph 2). Humans write from coarse to fine (Challenge 1). However, these claims are unsubstantiated.\n\nThe planning-based method is common in many generation tasks (e.g., [3],[4]).  The coarse-to-fine generation has been widely used in previous work (such as event-to-sentence generation in [2], which is mentioned by reviewer zk9y).  Also, it is supported by the comment of reviewer G5uN that \u201cThe task of story plot generation is interesting and allows to disentangle two major phases of content creation often seen in humans: planning and coarse-to-fine generation\u201d.\n\n\n\n> What novel or surprising scientific insights or findings are reported by this paper?\n\nWe discussed this in detail in the global response (see the **Novelty** section in the global response)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455590603,
                "cdate": 1700455590603,
                "tmdate": 1700455590603,
                "mdate": 1700455590603,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "73OtYGjtZZ",
                "forum": "SuUh5aRbbu",
                "replyto": "sjZQtX4soR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8614/Reviewer_vgZV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8614/Reviewer_vgZV"
                ],
                "content": {
                    "comment": {
                        "value": "I remain unconvinced. I fully agree that story generation is a difficult problem. However, the paper does not address any of the difficult aspects of story generation. Instead, it achieves some speed-up. Who ever complained about their stories not being generated fast enough?\n\n> The planning-based method is common in many generation tasks (e.g., [3],[4]). The coarse-to-fine generation has been widely used in previous work (such as event-to-sentence generation in [2], which is mentioned by reviewer zk9y). Also, it is supported by the comment of reviewer G5uN that \u201cThe task of story plot generation is interesting and allows to disentangle two major phases of content creation often seen in humans: planning and coarse-to-fine generation\u201d.\n\nThis is entirely missing the point. Computers doing something in no way implies that humans do this too. A comment of a reviewer cannot be taken as scientific evidence."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627713291,
                "cdate": 1700627713291,
                "tmdate": 1700627713291,
                "mdate": 1700627713291,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "73EuIVUwRY",
            "forum": "SuUh5aRbbu",
            "replyto": "SuUh5aRbbu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8614/Reviewer_zk9y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8614/Reviewer_zk9y"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of automatic generation of story plots intended as a record of story premise, character descriptions, and sequence of plot elements. It purports to use LLMs for plot generation and considers limitations of state of the art systems such as DOC (Yang et al., 2022a)) in particular in their requirement for large number of calls to LLMs. \nThe work implements an end-to-end story plot generator, which replaces Open AI (as used in DOC) with Llama2-13B-chat and is fine-tunable with human feedback. The generator is based on on a two-level hierarchy (where DOC supports different numbers of levels for the hierarchical outline) and operates in a breadth-first and coarse-to-fine manner. The system has been evaluated using an evaluation prompt in GPT4, showing that (after using RLHF and SFT) it outperforms DOC in a majority of cases."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper addresses a difficult problem, which has a long history in AI and to which the advent of LLM offers new perspectives. It contains an appropriate rationale and demonstrate a reasonable knowledge of the state-of-the-art (to the exception of pre-2018 plan-based narrative generation). Experimental design is overall well described, leading to end-to-end model training.\nWhile aiming at replicate and extend the performance of an existing approach (DOC) the work contains a number of original aspects (experimental approach, evaluation). \nThe paper also includes a transparent account of prompt engineering aspects faced during the development of the work, which might be beneficial to readers."
                },
                "weaknesses": {
                    "value": "Although the paper describes generated units as 'plots' it stands in-between substantial previous work of Plan-based plot generation [Riedel and Young, 2010] where plot elements were narrative functions or operators and text-based story generation [Wang and Wan, 2019] (in the paper's references), originating in story completion experiments, up to systems to which the approach is compared that generate plot + narrative text [Yang et al., 2023 (2022a in the paper's references)]. It is thus unclear whether what is presented in the paper, in particular in Figure 7 are plots or storyboards, and this is not just a terminological issue, as it affects the ability to apply structural evaluation methods to plots (see below) as well as creating an unusual setting for users to 'evaluate' the plot as opposed to evaluation methods based on end-story quality or story understanding (e.g. QUEST graphs [Graesser et al., 1992] used in [Christian and Young, 2004]). \nIt would thus be necessary to much better justify the approach compared to end-story text generation (not just completion), or non-DL, non-LLM based plot generation (e.g. Plan-based).  In particular, considering that LLM text generation could be used in conjunction to other plot/backbone generation methods, or that the DOC method is in reality generating both plot and (textual) narrative. \n\nRegarding reward models, there should probably be a discussion of previous approaches in text-based narrative generation, for instance [Ammanabrolu et al., 2019] and [Castricato et al., 2022]. There is also a lack of details on how RLHF has actually been performed (no details in the supplementary materials). \n\nEvaluation techniques are somehow underspecified considering previous work in evaluating narrative generation. The expression of comparative preferences by GPT4 is moderately replicable and appears rather qualitative and not sufficiently related to structural properties of the plot and rigorous definitions of the above properties. \nAlthough most of the work on evaluation based on narrative criteria (suspense, surprise, narrative arc...) has been developed as part of Plan-based narrative generation [Bae and Young, 2013] [Doust and Piwek, 2017] it should be transposable to DL-based (text-based [Yao et al., 2019] - in the paper's references, plot backbone [Polceanu et al., 2021]) or LLM-based work. Visual aspects of Plot structures that reconstruct Aristotelian arcs are of particular interest [Leong et al., 2022], not least because the paper makes reference [Goldfarb-Tarrant et al., 2020] to similar principles for neural-based story generation. In the absence of formal models it seems difficult to rely on GPT-4 with generic evaluation prompts, meaning that plots or storyboards would be better evaluated by industry professionals [Mirowski et al., 2022] (in the paper's references). \nOther related work is not discussed [Xie et al., 2023], although it may have been released too late considering the ICLR deadline.\n\nAmmanabrolu, P., Tien, E., Cheung, W., Luo, Z., Ma, W., Martin, L. and Riedl, M., 2019, August. Guided neural language generation for automated storytelling. In Proceedings of the Second Workshop on Storytelling (pp. 46-55).\nBae, B.C. and Young, R.M., 2013. A computational model of narrative generation for surprise arousal. IEEE Transactions on Computational Intelligence and AI in Games, 6(2), pp.131-143.\nCastricato, L., Havrilla, A., Matiana, S., Pieler, M., Ye, A., Yang, I., Frazier, S. and Riedl, M., 2022. Robust Preference Learning for Storytelling via Contrastive Reinforcement Learning. arXiv preprint arXiv:2210.07792.\nChristian, D.B. and Young, R.M., 2004, July. Comparing cognitive and computational models of narrative structure. In AAAI (pp. 385-390).\nDoust, R. and Piwek, P., 2017, September. A model of suspense for narrative generation. In Proceedings of the 10th International Conference on Natural Language Generation (pp. 178-187).\nGraesser, A.C., Gordon, S.E., Brainerd, L.E.: QUEST: a model of question answering. Comput. Math. Appl. 23, 733\u2013745 (1992)\nLeong, W., Porteous, J. and Thangarajah, J., 2022. Automated sifting of stories from simulated storyworlds. In: Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22 (pp. 4950-4956).\nPolceanu, M., Porteous, J., Lindsay, A. and Cavazza, M., 2021, May. Narrative plan generation with self-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 7, pp. 5984-5992).\nRiedl, M.O., Young, R.M.: Narrative planning: balancing plot and character. J. Artif. Intell. Res. 39, 217\u2013268 (2010)\nXie, Z., Cohn, T. and Lau, J.H., 2023, September. The Next Chapter: A Study of Large Language Models in Storytelling. In Proceedings of the 16th International Natural Language Generation Conference (pp. 323-351)."
                },
                "questions": {
                    "value": "How has RLHF been performed (user population, instructions, criteria...)?\nHow is the system dealing with relationships between characters? With the plot/character duality?\nWhat is the average length of generated plots (counted in plot units or narrative functions)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8614/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8614/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8614/Reviewer_zk9y"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8614/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787825856,
            "cdate": 1698787825856,
            "tmdate": 1699637077819,
            "mdate": 1699637077819,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CiqyF7BthX",
                "forum": "SuUh5aRbbu",
                "replyto": "73EuIVUwRY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8614/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8614/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their helpful and insightful comments. Below we respond to the questions respectively.\n\n> Although the paper describes generated units as 'plots' it stands in-between substantial previous work of Plan-based plot generation\u2026 It is thus unclear whether what is presented in the paper, in particular in Figure 7 are plots or storyboards\u2026\n\nWe agree that the terminology \u2018plot\u2019 does not have a rigorous definition, and there are different choices of the format of the plot. The most suitable definition of the plot in our paper should be \u2018some certain structure that captures most of the essential information of the story, which is often created in the planning stage\u2019. \n\nIn our work, we choose the structure of the plot (premise, settings, characters, outlines)  mainly due to the following reason:\n\n- The underlying principle is \u201ccoarse-to-fine\u201d, i.e., to progressively provide more and more detailed information. This coarse-to-fine representation makes end2end autoregressive generation easier.\n- While short, this representation still contains a good amount of information about the story, which is helpful for multiple perspective evaluations (represented by multiple questions). \n- This representation is used by the SOTA story generation work DOC [1] for the planning stage. It can be used to generate high-quality full-length stories. This demonstrates that such a structure can lead to full story generation.\n- The plot structure we chose in this paper is text-based, which could greatly benefit from the powerful text-based LLMs that take the plot as input and produce the whole story (usually with multiple steps such as the second stage of DOC [1]).\n\n\n> Regarding reward models, there should probably be a discussion of previous approaches in text-based narrative generation, for instance [Ammanabrolu et al., 2019] and [Castricato et al., 2022].\n\nCompared to [Ammanabrolu et al., 2019],  where the reward is based on how close the generated event is to a pre-trained goal, our reward model training is more similar to CARP [Castricato et al., 2022], a contrastively-trained preference model as a reward signal in story generation. Note that our reward model is trained on story plots (using cross-entropy loss on the preference label) instead of the whole story, which makes learning and human labeling easier without losing important signals. We added these contents in Section 2.3 in the revision. \n\n\n> Evaluation techniques are somehow underspecified considering previous work in evaluating narrative generation...it seems difficult to rely on GPT-4 with generic evaluation prompts, meaning that plots or storyboards would be better evaluated by industry professionals. \n\nFor GPT-4 evaluation, we provided the exact prompt format we used in Appendix B on Page 17. Also, thanks for your suggestion for conducting human evaluation. We conducted human evaluation during the rebuttal session and please see the global response for details. Given the short amount of rebuttal time, we won\u2019t be able to find industry professionals to evaluate the story plots, but will open source all generated stories for future reference. \n\n> How has RLHF been performed (user population, instructions, criteria...)?\n\nAfter reward model training, we use standard RLHF objective (e.g., equation (2) in [2]):\n\n$\\mathcal{L}\\_\\phi = \\mathbb{E}\\_{(x,y) \\sim D\\_{\\pi\\_\\phi^{\\text{RL}}}} [r\\_\\theta(x,y) - \\beta \\log( \\pi\\_\\phi^{\\text{RL}} (y|x) / \\pi^{\\text{SFT}} (y|x))]$, \n\nwhere $\\pi\\_\\phi^{\\text{RL}}$ is the learned policy by RLHF, $r\\_\\theta$ is the reward model, $\\pi^{\\text{SFT}}$ is the policy after SFT (our E2EPlot), and $\\beta$ is to control the KL divergence. The prompt $x$ (i.e., the premise) is generated using the OpenPlot pipeline. We added the above formula in Section 2.3 in the revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455292175,
                "cdate": 1700455292175,
                "tmdate": 1700455292175,
                "mdate": 1700455292175,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gfGcGvdjPP",
                "forum": "SuUh5aRbbu",
                "replyto": "73EuIVUwRY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8614/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8614/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> How is the system dealing with relationships between characters? With the plot/character duality? What is the average length of generated plots (counted in plot units or narrative functions)?\n\nOur e2e model treats the story plot (premise, settings, characters, outlines) as a sequence and does not need to explicitly deal with the relationship between characters, which can be regarded as one of the advantages of the e2e story plot generator, compared to a hand-crafted method. \n \nFor the OpenPlot pipeline (or original DOC [1]), after initializing the characters, when generating bulletin points of the outline, for each point, we will detect the characters that appeared in the sentence, and refine the character portraits by adding details in the current sentence to the initial character portrait.\n\nOn the other hand, we acknowledge that such a modeling relies on LLM\u2019s capacity to model characters and their relationships/interactions, and in many situations, still yields a shallow and boring plot. In the future, we will think about ways to model their relationships in more detail, not by hand-crafting but by leveraging the power of LLMs.   \n\nThe average length of the generated plots is ~1000 tokens.\n\n**References:**\n\n[1] Yang, Kevin, Dan Klein, Nanyun Peng, and Yuandong Tian. \"Doc: Improving long story coherence with detailed outline control.\" arXiv preprint arXiv:2212.10077 (2022).\n\n[2] Ouyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang et al. \"Training language models to follow instructions with human feedback.\" Advances in Neural Information Processing Systems 35 (2022): 27730-27744."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455311316,
                "cdate": 1700455311316,
                "tmdate": 1700455349557,
                "mdate": 1700455349557,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A3ghIPgom1",
                "forum": "SuUh5aRbbu",
                "replyto": "gfGcGvdjPP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8614/Reviewer_zk9y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8614/Reviewer_zk9y"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledgement of Authors Response"
                    },
                    "comment": {
                        "value": "Thanks for your response and clarification, in particular on the reward model, which was appropriate. \nI am however less convinced by other elements of response, in particular on the definition of plot which leads to a specific form of narrative generation difficult to situate with respect to previous work. Similar comments could be made on evaluation techniques, especially for high-level narrative phenomena such as suspense or surprise. I am not implying that your assumptions are technically flawed, but instead that, by being somehow at variance with previous work (DL-based or not) on story generation, they might end up addressing problems of lesser relevance. \nFor this reason, I am not inclined to modify my assessment."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690647784,
                "cdate": 1700690647784,
                "tmdate": 1700690647784,
                "mdate": 1700690647784,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]