[
    {
        "title": "Neuroexplicit Diffusion Models for Inpainting of Optical Flow Fields"
    },
    {
        "review": {
            "id": "DJSqzehUAu",
            "forum": "2gMwe9Duc4",
            "replyto": "2gMwe9Duc4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3351/Reviewer_cyeS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3351/Reviewer_cyeS"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a neuroexplicit diffusion model for the optical flow inpainting task. The method combines domain knowledge (explicit PDE-based formulation) with CNN for the task and demonstrates outperforming other baselines such as CNN-based, GAN-based, and probabilistic diffusion baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Good clarity\n\n  The paper includes sufficient details for understanding the main methods (equations, network architecture details, and implementation details). This helps the reproduction of the method.\n\n- Better accuracy over baselines\n\n  The paper compares its method with several baselines (FlowNetS, WGAIN, EED, and PD) and achieves better accuracy than them."
                },
                "weaknesses": {
                    "value": "- Limited evaluation\n\n  The paper evaluates the method only on one synthetic dataset, Sintel. To ensure the method also works on real-world domains, it would be great to evaluate the method on other datasets such as KITTI, Middlebury, etc. Furthermore, the paper doesn't compare with any previous optical flow inpainting methods (eg., Raad et al, \"On Anisotropic Optical Flow Inpainting Algorithms\"). Achieving better accuracy than baselines is great, but a comparison with previous work would be also necessary to see where the methods stand among the previous works.\n\n  One could also adopt baselines from the depth completion tasks (https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion), train their models on the optical flow tasks, and compare with them.\n\n  The method sounds okay, but due to the limited evaluation, it's difficult to judge if the method is really valuable for the community.\n\n\n- Other applications\n\n  Despite that the proposed method could be generic, the paper demonstrates only optical flow inpainting as an application. Can this method also be applied to other tasks such as depth completion or semantic scene completion? If the paper showed its applicability to such tasks, it could have demonstrated better impact."
                },
                "questions": {
                    "value": "- Transparency?\n\n  What's the meaning of the transparency of the model in the abstract?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3351/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698725964055,
            "cdate": 1698725964055,
            "tmdate": 1699636284978,
            "mdate": 1699636284978,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YpVEQwoOZn",
                "forum": "2gMwe9Duc4",
                "replyto": "DJSqzehUAu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3351/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3351/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank the reviewer for the effort and appreciate the detailed, valuable feedback. \n\n**W1:** *\"The paper evaluates the method only on one synthetic dataset, Sintel. To ensure the method also works on real-world domains, it would be great to evaluate the method on other datasets such as KITTI\"*\n\nWe agree that more results to demonstrate the effectiveness of our approach in a practical application strengthen the paper. The ground-truth optical flow in the KITTI dataset is acquired from the registration of LiDAR scans, and therefore inherently sparse in its nature. Densifying it presents a practically highly relevant use case of our method. We have evaluated all our methods on this task, including the previous state-of-the-art of inpainting optical flow from random masks. We provide the results in the new Section 4.4 and Table 3 in the paper. Our method is on par with the Laplace-Beltrami method in terms of EPE but significantly outperforms most other methods in the FL metric - especially in the most difficult 1% density case - indicating that our results have fewer outliers. It demonstrates the impact of our method on this selected practical application. Please note that our method was not even adapted for the spatially varying mask densities present in this setting. \n\n\n**W2:** *\"Furthermore, the paper doesn't compare with any previous optical flow inpainting methods (eg., Raad et al [1], \"On Anisotropic Optical Flow Inpainting Algorithms\")\"*\n\nWe thank the reviewer for directing our attention to this work. We agree that the mentioned method is indeed the prior state of the art. We have evaluated it and show that we significantly outperform it, thus setting a new state of the art. Please see the section \u201cComparison against the state-of-the-art baseline\u201d in the general comment to all reviewers. \n  \n**W3/W4:** *\"One could also adopt baselines from the depth completion tasks (https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion), train their models on the optical flow tasks, and compare with them. / Despite that the proposed method could be generic, the paper demonstrates only optical flow inpainting as an application. Can this method also be applied to other tasks such as depth completion or semantic scene completion? If the paper showed its applicability to such tasks, it could have demonstrated better impact\"*\n\nWe agree to the reviewer that this is another application domain for our method. So far, we have provided a methodically novel approach and demonstrated state-of-the-art results for the optical flow inpainting task as well as a positive impact on a practical real-world application. We consider adaptation and evaluation of our approach on depth maps as interesting future work, e.g. as an addition for a journal paper. We thank the reviewer for this feedback and plan to carry out evaluations for the suggested additional application in the near future. \n\n**Q1:** *\"What's the meaning of the transparency of the model in the abstract?\"*\n\nWith transparency, we refer to the fact that the inpainting process is purely done by a diffusion process. Restricting the parameters of the diffusion process to theoretical bounds nets stability guarantees for the inpainting layers. Consequently, the combined model inherits all the mathematical foundations of discrete diffusion processes by construction.\nTherefore, it is straightforward to reason about the behavior of the method a priori.\nIt is possible to extract a valid flow field at each step of the inpainting process and judge its quality, due to the fact that we never move the input into a higher dimensional latent space as it would be done with neural inpainting methods.\nThis explainable behavior is desirable over black box models for high risk tasks such as autonomous driving.\n\n\n**References:**\n\n[1] Lara Raad, Maria Oliver, Coloma Ballester, Gloria Haro, and Enric Meinhardt. On anisotropic optical flow inpainting algorithms. Image Processing On Line, 10:78\u2013104, 2020"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739462933,
                "cdate": 1700739462933,
                "tmdate": 1700739799839,
                "mdate": 1700739799839,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bjnLMV1jCj",
            "forum": "2gMwe9Duc4",
            "replyto": "2gMwe9Duc4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3351/Reviewer_3rtj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3351/Reviewer_3rtj"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an end-to-end pipeline for inpainting values of a diffusion process. The model is a hybrid of explicit (solutions to partial differential equations) and neural (U-net) components, where the evolution of the diffusion process is explicitly computed, but guided by learned parameters. The method is demonstrated on inpainting of optical flow fields, where it bests several chosen baselines that are explicit, neural, and neruoexplicit."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The particular combination of learned and explicit diffusion computation is novel.\nThe metrics demonstrate accuracy superior to the baselines.\nThe ablation study in Section 4.3 is informative."
                },
                "weaknesses": {
                    "value": "The approach has only been demonstrated on one niche application -- optical flow. The paper does mention sparse mask inpainting of images several times, which could be another use case to strengthen the paper.  More results would be appreciated too, perhaps on some real-world datasets such as KITTI."
                },
                "questions": {
                    "value": "Figure 1 would be more readable with larger fonts and more separation between the UNet and the D,a arrows. What is the difference between yellow and orange layers in the encoder? The inpainting boxes could be more fleshed out to visualize what they are actually doing (are they solving equation 8?). Where do the iterations come into play?\n\nHow does the diffusion tensor D connect to equation 8.\n\nSection 3.1 mentions using average pooling to obtain the coarse version of the sparse flow field. Won't that grossly underestimate the flow field due to all the 0 values? Are those ignored somehow? Are the flow values also scaled down by 2 in each downsampling step, so that they are valid offsets for the coarser image size (similar for upsampling)?\n\nTable 1 could be augmented with train/inference timings, parameter count, and number of iterations. The Figure 3 could be removed and that space used for additional results.\n\nIn Figure 2 left, it would be helpful to put the x axis ticks exactly where the samples are. There are only 4 sample sizes, and marking e.g. 0 on the x axis is really not informative.\n\nIn Figure 2 right, what does the vertical line down the middle indicate? Is that some ideal mask density threshold?\n\nThis sentence is hard to parse: \"When evaluated on a density of 10%, the network trained on 5% density can even reach a very close EPE on to the network that was optimized on this density (0.28 vs. 0.29).\" Does this intend to state that the network trained on 5% density has EPE of 0.29, while the network trained on 10% density has EPE of 0.28, when both are evaluated on 10% density dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3351/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3351/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3351/Reviewer_3rtj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3351/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758724256,
            "cdate": 1698758724256,
            "tmdate": 1699636284906,
            "mdate": 1699636284906,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Aiw3vBbSPX",
                "forum": "2gMwe9Duc4",
                "replyto": "bjnLMV1jCj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3351/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3351/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1:"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the effort and appreciate the detailed, valuable feedback. \n\n**W1:** *\"The approach has only been demonstrated on one niche application -- optical flow. The paper does mention sparse mask inpainting of images several times, which could be another use case to strengthen the paper. More results would be appreciated too, perhaps on some real-world datasets such as KITTI\"*\n\nWe agree that more results to demonstrate the effectiveness of our approach in a practical application strengthen the paper. The ground-truth optical flow in the KITTI dataset is acquired from the registration of LiDAR scans, and therefore inherently sparse in nature. Densifying it presents a practically highly relevant use case of our method. We have evaluated all our methods on this task, including the previous state of the art of optical flow inpainting from random masks. We provide the results in the new Section 4.4 and Table 3 in the paper. Our method is on- par with the Laplace-Beltrami method in terms of EPE but significantly outperforms most other methods in the FL metric - especially in the most difficult 1% density case - indicating that our results have fewer outliers. It demonstrates the impact of our method on this selected practical application. Please note that our method was not even adapted for the spatially varying mask densities present in this setting. \n\nGeneral image data differs significantly from the flow field inpainting scenario that we designed our approach for. On one hand, flow fields are piecewise smooth, while natural images tend to contain texture and fine-scale details. While it is reasonable to use our approach for other piecewise smooth or cartoon-like data, significant changes would be required for natural image data. Moreover, the model-based inpainting in our hybrid approach uses the reference image to guide a linear diffusion process. This would not be available in a natural image inpainting scenario and thus would necessitate a different architecture. Here, a nonlinear process would be required which can infer structure directions from the given sparse data only. Overall, this would be a significant departure from the concepts we have proposed in our manuscript. We agree that addressing inpainting of images with neuroexplicit models is a highly interesting research question and will pursue this in future work.\n\nWe thank the reviewer also for the additional valuable improvement suggestions and have addressed all of them in the paper:\n\n\n**Q1:** *\"Figure 1 would be more readable with larger fonts and more separation between the UNet and the D,a arrows. What is the difference between yellow and orange layers in the encoder? The inpainting boxes could be more fleshed out to visualize what they are actually doing (are they solving equation 8?). Where do the iterations come into play?\"*\n\nWe adapted our figure 1 accordingly. Please see the updated version of the paper. \n\n**Q2:** *\"How does the diffusion tensor D connect to equation 8\"*\n\nIn Equation, 6 we introduced the notation $D:=g(S)$, which states that the diffusion tensor is derived from the structure tensor $S$. In Equation 7, we use this notation to introduce the function \n$\\Phi(I,~Ku^k)=g\\Bigl(\\sum_{i=0}^c(KI)_i (KI)_i^\\top\\Bigr)(Ku^k)$. The argument to the function $g$ here is a discrete version of the structure tensor $S$. Consequently, the diffusion tensor is inherently built into the function $\\Phi$ which we reference in Equation 8. The full scheme in Equation 8 denotes one timestep (from k -> k+1) of the diffusion evolution we use to inpaint in the image-driven inpainting blocks in the Figure. Therefore, each of these blocks in the figure performs the number of timesteps we declare in Section 4.1. The first one performs 5 explicit steps, the next one 15, and so on.\n\n**Q3:** *\"Section 3.1 mentions using average pooling to obtain the coarse version of the sparse flow field. Won't that grossly underestimate the flow field due to all the 0 values? Are those ignored somehow? Are the flow values also scaled down by 2 in each downsampling step, so that they are valid offsets for the coarser image size (similar for upsampling)?\"*\n\nWe thank the reviewer for pointing this out. Naive average pooling can indeed not be used due to the sparsity of the available data. We mentioned in the manuscript that we use averages of the flow values. This means that only flow values indicated by the binary mask are averaged. We do not scale the flow values at lower resolutions, since it does not matter if they are valid offsets or not. We are only concerned with validity on the full resolution, and therefore don't have to worry about scale on the lower resolutions. The coarse-to-fine process is purely done to speed up the inpainting process. We have addressed coarse-to-fine also in our answer to Reviewer 1, W3."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739195577,
                "cdate": 1700739195577,
                "tmdate": 1700739872061,
                "mdate": 1700739872061,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jW2eLMKKNt",
                "forum": "2gMwe9Duc4",
                "replyto": "bjnLMV1jCj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3351/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3351/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 2:"
                    },
                    "comment": {
                        "value": "**Q4:** *\"In Figure 2 left, it would be helpful to put the x axis ticks exactly where the samples are. There are only 4 sample sizes, and marking e.g. 0 on the x axis is really not informative.\nIn Figure 2 right, what does the vertical line down the middle indicate? Is that some ideal mask density threshold?\"*\n\nIn the right part of Figure 2, we test how well all methods can deal with previously unseen mask densities. All methods were trained / optimized on a mask density of 5% (indicated by the gray line) and tested on different mask densities. This showed that the explicit and neuroexplicit methods have superior performance when presented with different mask distributions compared to the neural methods (see the increase in EPE of the neural methods when presented with more dense initializations compared to the one they were optimized on).\n\nWe have adjusted the figures to make them more readable and space conserving in the manuscript.\n\n\n**Q5:** *\"This sentence is hard to parse: \"When evaluated on a density of 10%, the network trained on 5% density can even reach a very close EPE on to the network that was optimized on this density (0.28 vs. 0.29).\" Does this intend to state that the network trained on 5% density has EPE of 0.29, while the network trained on 10% density has EPE of 0.28, when both are evaluated on 10% density dataset?\"*\n\nThis is correct, training the model on a specific density only provides a marginal performance benefit due to the good generalization to unknown mask densities of our method."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739229382,
                "cdate": 1700739229382,
                "tmdate": 1700739896033,
                "mdate": 1700739896033,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NM1lD0tqL4",
            "forum": "2gMwe9Duc4",
            "replyto": "2gMwe9Duc4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3351/Reviewer_BStA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3351/Reviewer_BStA"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new approach that combines model-driven and data-driven methods to achieve improved inpainting of optical flow fields. The authors propose a joint architecture that integrates explicit partial differential equation (PDE)-based approaches with convolutional neural networks (CNNs). The paper demonstrates that their model outperforms both fully explicit and fully data-driven baselines in terms of reconstruction quality, robustness, and amount of required training data."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The paper successfully combines the strengths of explicit PDE-based models and CNNs, leveraging the interpretability and generalization capabilities of the former and the learning power of the latter. This integration provides an effective architecture for inpainting optical flow fields.\n\n2. The proposed model achieves superior results compared to both explicit and data-driven baselines. The evaluation demonstrates higher reconstruction quality, robustness, and generalization capabilities, making it an advancement in the field.\n\n2. The neuroexplicit diffusion model requires comparatively fewer learnable parameters and can be trained with significantly less data while still outperforming baselines trained on the full dataset. This aspect addresses the dependency on large-scale datasets, making the model more practical and efficient."
                },
                "weaknesses": {
                    "value": "1. Although the paper compares the proposed model with explicit and data-driven baselines, it would be beneficial to include a comparison with other recent state-of-the-art methods in inpainting for optical flow fields. This would provide a more comprehensive evaluation and enhance the paper's contribution.\n\n2. The paper assumes prior knowledge of diffusion processes and their application in inpainting. I wonder why diffusion-based inpainting is suitable for flow inpainting? Are there any theoretical explanations for this?\nThere are also many other traditional inpainting methods, are they also suitable in this task and do they work well with neural networks? Why or why not?\n\n3. In the ablation study part, I wonder is coarse-to-fine approach important in this method? And is it possible to substitute Diffusion Tensor module with other parameter-free inpainting or propagation methods, to see which one best suits this task?"
                },
                "questions": {
                    "value": "Please address the questions in weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3351/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770226856,
            "cdate": 1698770226856,
            "tmdate": 1699636284827,
            "mdate": 1699636284827,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PNwjailvO1",
                "forum": "2gMwe9Duc4",
                "replyto": "NM1lD0tqL4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3351/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3351/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank the reviewer for the effort and appreciate the detailed, valuable feedback. \n\n**W1:** *\"It would be beneficial to include a comparison with other recent state-of-the-art methods in inpainting for optical flow fields\"*\n\nWe agree that comparing to the state-of-the-art baseline is useful in addition to previous comparisons against different methodologies. We have evaluated the state of the art. Our results show that we consistently outperform all competitors and thus set a new state of the art. Please see the section \u201cComparison against the state-of-the-art baseline\u201d in the general comment to all reviewers and the updated table 1 in the paper for details. \n\n**W2:** *\"I wonder why diffusion-based inpainting is suitable for flow inpainting\"*\n\nIn contrast to other model-based inpainting approaches such as the Absolutely Minimizing Lipschitz Extension (AMLE) and Laplace-Beltrami (LB) approaches, diffusion inpainting is our first choice due to several theoretical and practical considerations.\n\nFirst off, diffusion inpainting is closely related to regularization in model-based variational models for optical flow estimation. The so-called smoothness term in such cost functions is responsible for filling in flow data at regions of low confidence. This has a natural relation to diffusion filters, which is explained in (Weickert and Schn\u00f6rr, 2001 [1]). Several variants of diffusion with edge preservation capabilities have been shown to perform well on the reconstruction of piecewise smooth image content from sparse data in compression (Schmaltz et al., 2014 [2]), including flow fields (Jost et al. 2019 [3]). Edge-enhancing diffusion represents the state of the art among this class of diffusion filters.\n\nAnother advantage of diffusion compared to AMLE or LB is the existence of theoretical results w.r.t. combinations with deep learning. For instance, for diffusion filters as layers in neural networks, provable stability guarantees have been established (Alt et al. 2022 [4]). \n\nThe aforementioned advantages make diffusion inpainting a natural choice for the model-based part of our neuroexplicit model. The experimental results in Table 1 also confirm our results experimentally: Our model outperforms all neural and learning-based competitors. . Nevertheless, other approaches such as AMLE or LB are interesting candidates for additional future research regarding neuroexplict flow inpainting.\n\n\n\n**W3:** *\"Is coarse-to-fine approach important in this method\"*\n\nThe coarse-to-fine approach is critical to speed up the convergence rate of the method, which is determined by the distance across which information has to be transported. Working on the full resolution would require a high number of iterations of the diffusion model and make the optimization within the deep learning framework intractable. \nInpainting results from coarse levels provide a good initialization for finer resolutions of the inpainting problem. This allows to speed up convergence on the next resolution level in a straightforward and efficient way without impacting the quality of the reconstruction. The results from Table 1 confirm that there is no qualitative disadvantage due to the coarse-to-fine scheme, and it enables us to train our models and achieve state of the art. \n\n**References:**\n\n[1] Joachim Weickert and Christoph Schn\u00f6rr. A theoretical framework for convex regularizers in pde-\nbased computation of image motion. International Journal of Computer Vision, 45(3):245\u2013264,\nDecember 2001. ISSN 0920-5691\n\n[2] Christian Schmaltz, Pascal Peter, Markus Mainberger, Franziska Ebel, Joachim Weickert, and An-\ndr\u00e9s Bruhn. Understanding, optimising, and extending data compression with anisotropic diffu-\nsion. International Journal of Computer Vision, 108(3):222\u2013240, jul 2014. ISSN 0920-5691.\n\n[3] Ferdinand Jost, Pascal Peter, and Joachim Weickert. Compressing flow fields with edge-aware\nhomogeneous diffusion inpainting. In Proc. 45th International Conference on Acoustics, Speech,\nand Signal Processing, pp. 2198\u20132202, Barcelona, Spain, May 2020. IEEE Computer Society\nPress.\n\n[4] Tobias Alt, Karl Schrader, Matthias Augustin, Pascal Peter, and Joachim Weickert. Connections\nbetween numerical algorithms for pdes and neural networks. Journal of Mathematical Imaging\nand Vision, 65(1):185\u2013208, jun 2022. ISSN 0924-9907"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739119882,
                "cdate": 1700739119882,
                "tmdate": 1700739932551,
                "mdate": 1700739932551,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]