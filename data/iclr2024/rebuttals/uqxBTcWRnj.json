[
    {
        "title": "Bridging Neural and Symbolic Representations with Transitional Dictionary Learning"
    },
    {
        "review": {
            "id": "M3GWDgpyhl",
            "forum": "uqxBTcWRnj",
            "replyto": "uqxBTcWRnj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2974/Reviewer_mxpA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2974/Reviewer_mxpA"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Transitional Dictionary Learning \u2013 a framework for implicitly learning symbolic knowledge, such as visual parts and relations, through input reconstruction using parts and implicit relations. This is done by employing a game-theoretic diffusion model for input decomposition, leveraging dictionaries learned by the Expectation Maximization (EM) algorithm. Experimental results demonstrate the proposed approach\u2019s efficacy through evaluation in discovering compositional patterns compared to SOTA methods, depicting human alignment with the predictions as well."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper provides a convincing motivation for the proposed methodology. It offers crucial insights into transitional representations, clustering information gain, and the reinforcement learning approach employed to optimize the objective. Overall, the paper exhibits a well-written supported by experimental evidence and a well-formulated mathematical framework. Figure 2, along with Section 4, elucidates the proposed approach and its crucial implementation details for the reader. I believe that this methodology holds significant promise for the research community, particularly in the midst of the surge of VLMs, where interpretable representations can not only serve as effective starting points or initializations, but also provide disentangled inputs for VLMs/LLMs to engage in high-level reasoning. The transfer learning experiments outlined in Table 2 provide strong evidence of the approach's utility beyond the confines of its training domain."
                },
                "weaknesses": {
                    "value": "While the conducted experiments offer valuable insights into the effectiveness of the proposed approach, I would like to encourage the authors to extend their testing to more challenging real-world datasets. This expansion could further underscore the practical utility of the approach. Specifically, incorporating diverse categories of 3D objects from sources like ShapeNet, integrating written language datasets such as EMNIST, and including datasets featuring objects relevant to manipulation tasks would be valuable additions to the paper. Demonstrating the application of the proposed approach in contexts like robot manipulation or affordance prediction would provide tangible benefits for readers."
                },
                "questions": {
                    "value": "Apart from the points mentioned in the Weaknesses section, the paper could benefit from a broader discussion of its potential applications and impact, which would be valuable for the research community. Additionally, a more detailed analysis of the computational resources and time required would be helpful for readers seeking to implement the proposed methodology."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2974/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2974/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2974/Reviewer_mxpA"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2974/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697940478361,
            "cdate": 1697940478361,
            "tmdate": 1699636241732,
            "mdate": 1699636241732,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j3vqS9Ws7q",
                "forum": "uqxBTcWRnj",
                "replyto": "M3GWDgpyhl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mxpA"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your thorough review and insightful feedback. Below, we address each of your comments in detail:\n\n## For the weaknesses:\n\n> \u201cWhile the conducted experiments offer valuable insights into the effectiveness of the proposed approach, I would like to encourage the authors to extend their testing to more challenging real-world datasets. This expansion could further underscore the practical utility of the approach. Specifically, incorporating diverse categories of 3D objects from sources like ShapeNet, integrating written language datasets such as EMNIST, and including datasets featuring objects relevant to manipulation tasks would be valuable additions to the paper. Demonstrating the application of the proposed approach in contexts like robot manipulation or affordance prediction would provide tangible benefits for readers.\u201d\n\nWe are grateful for your suggestion to test our approach on more challenging real-world datasets. We recognize the potential impact and utility of expanding our experimentation to include diverse categories of 3D objects from sources like ShapeNet, as well as datasets like EMNIST and those relevant to robotic manipulation. While the current scope of our work and the limited rebuttal period constrain us from incorporating these extensions immediately, we are committed to exploring these areas in future research phases.\n\n## For your questions:\n\n> \u201cApart from the points mentioned in the Weaknesses section, the paper could benefit from a broader discussion of its potential applications and impact, which would be valuable for the research community. Additionally, a more detailed analysis of the computational resources and time required would be helpful for readers seeking to implement the proposed methodology.\u201d\n\nYour recommendation for a broader discussion on potential applications and impacts is highly appreciated. We have expanded our discussion in Appendix K.2 to encompass a broader range of applications and potential impacts of our research. This addition aims to provide the research community with a clearer vision of the future directions and the transformative potential of our methodology.\n\nWe agree that a detailed analysis of computational resources and processing time would be beneficial for readers interested in implementing our methodology. We are working on it and plan to include a comprehensive efficiency analysis in future updates. This information will indeed offer valuable insights for practical applications and further research.\n\n\n---\n\nWe are confident that these responses address your concerns and clarify the directions of our research. Your suggestions have been instrumental in refining our work and guiding its future trajectory. If you have any more questions or require further clarification, please do not hesitate to reach out. Thank you once again for your valuable input and support."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700342964826,
                "cdate": 1700342964826,
                "tmdate": 1700343103978,
                "mdate": 1700343103978,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JvOcm8la2r",
                "forum": "uqxBTcWRnj",
                "replyto": "j3vqS9Ws7q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2974/Reviewer_mxpA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2974/Reviewer_mxpA"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response"
                    },
                    "comment": {
                        "value": "I am satisfied with the limitations and the broader impact statement and the future works section in the appendix.\n\nThe considerations about the expansion of the work to real work scenarios and datasets in future work are appreciated. \n\nMinor comments: on page 4, top paragraph, please change: \u201daverage\u201d to ``average\u201d"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700352903386,
                "cdate": 1700352903386,
                "tmdate": 1700352903386,
                "mdate": 1700352903386,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JqUpmdVUgO",
            "forum": "uqxBTcWRnj",
            "replyto": "uqxBTcWRnj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2974/Reviewer_KjjP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2974/Reviewer_KjjP"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores unsupervised part segmentation using a neural symbolic approach.  The authors propose Transitional Dictionary Learning for symbolic feature representations for representing the feature embedding as structural information.  This is done via a set of \u2018players\u2019 estimating the visual parts which are combined together for the reconstruction and clustering losses for self-supervised learning of the features.  In addition, a game-theoretic decomposition loss prevents one player from reconstruction everything or overlapping with other players."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written and easy to understand.  There are good explanations for each step of the approach.  The \"Transitional Representation\" section does a really good job of approaching the symbolic and neural representations.  \n\n\nThe method is topical and will be of interest to the ICLR community and the method seems to be novel for how to produce a dictionary of neuro-symbolic part segmentation.  \n\nI really like the overarching goal for self-supervised part segmentation and the method seems to attack the problem directly.  The neural symbolic approach to ML has been of interest for a while and part segmentation is a good problem to apply it towards."
                },
                "weaknesses": {
                    "value": "The biggest disappointment was not doing this on real visual data rather than on LineWorld data.  This is still useful with just LineWorld but showing on realworld data would be much more impressive.  \n\nRunning human evaluations requires an IRB or something similar not mentioned here.   This needs to be stated (anonymously) that you did actually go through someone to ensure the human experiments were done properly.  \n\nFor the \u201cCompositional Representation\u201d related work, please add references to older approaches such as Bag of Words such as:\nL. Fei-Fei and P. Perona, \"A Bayesian hierarchical model for learning natural scene categories,\" 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), San Diego, CA, USA, 2005, pp. 524-531 vol. 2, doi: 10.1109/CVPR.2005.16.\n\nCsurka, Gabriella, Christopher Dance, Lixin Fan, Jutta Willamowski, and C\u00e9dric Bray. \"Visual categorization with bags of keypoints.\" In Workshop on statistical learning in computer vision, ECCV, vol. 1, no. 1-22, pp. 1-2. 2004.\n\nThe citations needs to reference the actual venue such as this one should not just refer to Open Review (be wary of using automated citations):\nYann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62, 2022.\n\nFormular 1 -> Equation 1"
                },
                "questions": {
                    "value": "For Figure 3, could you compare a more conventional approach to compare against to see if this approach is causing it to be separated verse just from the data?  \n\nHave you tried this on more complex data 2D images?\n\nCan you elaborate on exactly what the human criteria were that they were evaluating?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "There was a mention of human evaluation but no mention of responsible research practices such as an IRB or something similar."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2974/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2974/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2974/Reviewer_KjjP"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2974/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698618330635,
            "cdate": 1698618330635,
            "tmdate": 1699636241651,
            "mdate": 1699636241651,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KI7ruwp6SH",
                "forum": "uqxBTcWRnj",
                "replyto": "JqUpmdVUgO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KjjP"
                    },
                    "comment": {
                        "value": "We deeply appreciate your acknowledgement of our research topic and method, our step-by-step demonstration, and for pointing out areas for improvement. We have addressed each of your concerns as follows:\n\n## For the weaknesses:\n\n> \u201cThe biggest disappointment was not doing this on real visual data rather than on LineWorld data. This is still useful with just LineWorld but showing on realworld data would be much more impressive.\u201d\n \nWe acknowledge the significance of applying our method to real-world data. As highlighted in our general response, this aspect is beyond the scope of our current research but is a crucial direction for future work.  For this paper, we focused on a more controlled environment to the establish the foundational aspects of our method.\n\n> \u201cRunning human evaluations requires an IRB or something similar not mentioned here. This needs to be stated (anonymously) that you did actually go through someone to ensure the human experiments were done properly.\u201d\n\n We appreciate your concern regarding the ethical aspects of human evaluation. To clarify, our qualitative study was conducted via Google Vertex AI. This platform ensures anonymity and ethical compliance, as we interact only with the platform, and not directly with human annotators. This setup mitigates any ethical risks related to privacy or direct human subject interaction. Guaranteed by the skilled annotators in the platform,and we annotated 3 times for each sample, the evaluation quality is also ensured.\n\n> \u201cFor the \u201cCompositional Representation\u201d related work, please add references to older approaches such as Bag of Words such as: L. Fei-Fei and P. Perona, \"A Bayesian hierarchical model for learning natural scene categories,\" 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), San Diego, CA, USA, 2005, pp. 524-531 vol. 2, doi: 10.1109/CVPR.2005.16.\n\n> Csurka, Gabriella, Christopher Dance, Lixin Fan, Jutta Willamowski, and C\u00e9dric Bray. \"Visual categorization with bags of keypoints.\" In Workshop on statistical learning in computer vision, ECCV, vol. 1, no. 1-22, pp. 1-2. 2004.\u201d\n\nThank you for suggesting these seminal references. We have now incorporated citations to Fei-Fei & Perona (2005) and Csurka et al. (2004) in our related work section to enrich the historical context of our research.\n\n> \u201cThe citations needs to reference the actual venue such as this one should not just refer to Open Review (be wary of using automated citations): Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62, 2022.\u201d\n\nWe have rechecked our citations to ensure accuracy and adherence to proper referencing standards. The specific citation of this Yann LeCun\u2019s work, being available only on OpenReview, has been retained as is.\n\n> \u201cFormula 1 -> Equation 1\u201d\n\n   We corrected the terminology throughout the paper, replacing all instances of 'Formula' with 'Equation' for consistency and accuracy.\n\n\n## For your questions:\n\n> \u201dFor Figure 3, could you compare a more conventional approach to compare against to see if this approach is causing it to be separated verse just from the data?\u201d\n\n  We have added a visualization from the UPD method in Figure 3 for comparative purposes. The caption has been updated to reflect this addition and provide clarity on the distinctiveness of our method\u2019s results.\n\nTo further answer your question, it cannot be directly caused by data, because each data point is a visual part, which cannot be straightforwardly obtained from input, and needs to be decomposed from it by the model.\n\n> \u201cHave you tried this on more complex data 2D images?\u201d\n\n   As mentioned, our current study does not extend to more complex or real-world 2D images, but we consider this an important area for future exploration.\n\n> \u201cCan you elaborate on exactly what the human criteria were that they were evaluating?\u201d\n\n  Detailed instructions and criteria for our human evaluators are provided in our supplementary materials (Data/qualitative study/Instructions.pdf) and briefly described in Appendix J. We asked annotators to assess if they could feasibly use the model\u2019s suggested strokes to draw characters, with ratings ranging from 'Non-stroke' to 'Good' based on the naturalness and usability of the strokes:\n- Non-stroke: They are not strokes at all; it is impossible to draw the character with them.\n- Unnatural: Can draw with these strokes, but unnatural or uncomfortable\n- Acceptable: The strokes are not ideal enough, but not that unnatural.\n- Good: The strokes are close to those used by humans.\n\nThe instruction file illustrates each rating with examples in detail.\n\n---\n\nWe hope that these responses thoroughly address your points and provide a clearer understanding of our research. Thank you once again for your valuable feedback and insightful questions. Should you have any further queries, please do not hesitate to contact us."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700342923455,
                "cdate": 1700342923455,
                "tmdate": 1700343072629,
                "mdate": 1700343072629,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "C808qWBKRB",
            "forum": "uqxBTcWRnj",
            "replyto": "uqxBTcWRnj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2974/Reviewer_jEsx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2974/Reviewer_jEsx"
            ],
            "content": {
                "summary": {
                    "value": "The paper targets the reconstruction of an input signal $x$ (evaluated with images and point clouds) through a combination of parts in a learning framework. The solution is formulated as an unsupervised dictionary learning problem and solved through EM. The method is evaluated and compared on three datasets including 2D non-overlapping lines, 2D handwritten characters, and 3D shapes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The motivation and the background of the paper are well demonstrated and insightful in Sec. 1&2. The significance of the paper is clear and the arguments are insightful."
                },
                "weaknesses": {
                    "value": "The major weakness of the paper is the bad presentation of Sec. 3&4 that greatly hinders the readers from understanding the paper.\n- The annotation in Sec. 3 is in quite a mess. Scalar value, vector, set, and matrix are not in consistent forms, and multiple critical variables lack clear definition/explanation:\n1) what is 'a' stands for in \"such as Cat(a), Tree(a), Person(a)\"? \n2) What is the relationship between $x_i$ and $x$? Seemingly the pieces of $x_i$ are determined by the masks and are directly combined into a whole instead of the linear addition.\n3) What is the relationship between $r_i$ above Eq. 1, $R_i$ in Eq. 1, and $r_j^i$ below Eq. 1?\n4) How can $theta$ be optimized in Eq. 1 if it does not appear in the two terms? The definition of the decoder g(\u00b7) is not consistent. Does it take $theta$ as a condition or not? Seemingly Eq. 2 is the appropriate form.\n5) The definitions of two crical terms $E_{\\tilde{D}}$ and $d_S$ are unclear.\n6) How is the dictionary $\\tilde{D}$ obtained given the argument \"As we have meaningful $\\tilde{D}$\"?\n7) It seems that the only variable to be optimized is the hidden dictionary $\\theta$. What about the models of $f(x;\\theta)$, $\\hat{g}(r_i;\\theta)$, $g_{\\theta}(R^i)$, and $g_{\\tilde{D}(R)}$?\n\n- The illustration of Fig. 2 does not clearly demonstrate the formulation in Sec. 3 and the solution in Sec. 4:\n1) $f, R, r_i, g, x_i, m_i$ are not clearly labeled in the figure.\n2)Where is the $N_P$ copies of the model in the figure? \n3) What does each patch stands for and what are the relation between the patches and the aforementioned terms in Sec. 3?\n4) Why is there a \"GT loss\" in an unsupervised learning pipeline?\n5) Where is the \"Decomposition Loss\" mentioned in Fig. 2?"
                },
                "questions": {
                    "value": "Though Sec. 1&2 are well-demonstrated with clear motivations, the unsatisfied presentation of Sec. 3&4 makes the formulation and solution hard to follow. The authors are also encouraged to provide qualitative comparison results on Line World and ShapeNet5."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2974/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770824823,
            "cdate": 1698770824823,
            "tmdate": 1699636241580,
            "mdate": 1699636241580,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "h8UtqmnXnV",
                "forum": "uqxBTcWRnj",
                "replyto": "C808qWBKRB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jEsx (Part 1/2)"
                    },
                    "comment": {
                        "value": "We greatly appreciate your acknowledgment of our research problem demonstrated in Sections 1 and 2, and the significance of our paper. We are also highly grateful for your comprehensive review and constructive, concrete feedback on Sections 3 and 4. We have revised our manuscript significantly, addressing each of your points for clarity and coherence:\n\n\n## For the weaknesses:\n\n> \u201cThe major weakness of the paper is the bad presentation of Sec. 3&4 that greatly hinders the readers from understanding the paper.\u201d\n\nWe have substantially revised Sections 3 and 4 for better clarity and coherence, with changes highlighted in blue in the revised manuscript. These revisions are intended to address the issues you raised and improve overall readability.\n\n> \u201cThe annotation in Sec. 3 is in quite a mess. Scalar value, vector, set, and matrix are not in consistent forms, and multiple critical variables lack clear definition/explanation:\u201d\n\nWe meticulously reviewed and standardized our notation, adopting a consistent sub/super-script convention detailed at the start of Section 3: ***\u201cSuperscript $\u00b7^i$ denotes $i$-arity, superscript $\u00b7^{(i)}$ with brackets indicates the $i$-th sample in a dataset, and subscript $\u00b7_i$ refers to the $i$-th visual part in a sample.\u201d*** This clarification aims to resolve any ambiguity in our notation.\n\n\n\n> \u201cwhat is 'a' stands for in \"such as Cat(a), Tree(a), Person(a)\"?\u201d\n\nWe clarified that \u2018a\u2019 represents a logical variable, revising examples to use $x_i$ (e.g., Cat($x_i$)) directly for greater clarity.\n\n> \u201cWhat is the relationship between $x_i$ and $x$? Seemingly the pieces of  are determined by the masks and are directly combined into a whole instead of the linear addition.\u201d\n\n$x_i$ is a visual part of image $x$, it is true that under our linear assumption, $x=\\sum_{i=1}^{N_P} x_i=\\sum_{i=1}^{N_P} m_ix$, the parts can be decided by masks, that is the reason why we introduce the game theoretic mechanisms to avoid collapsed solutions, i.e. directly output an all-1 mask. \n\nWe rewrote paragraph 3 in Section 3.1 to introduce the linear assumption in detail first, followed by a discussion on symbol grounding, in order to reduce such ambiguity.\n\n> \u201cWhat is the relationship between $r_i$ above Eq. 1, $R_i$ in Eq. 1, and $r^i_j$ below Eq. 1?\u201d\n\nWe solved this kind of ambiguity by the sub/superscript convention discussed above. $R^{(i)}$ is the transitional representation or neural logical variables (each variable is an visual part) for $i$-th sample in the dataset, $r_i$ is the representation for the i-th part in $R$, $r^{(i)}_j$ is the representation for the $j$-th part in $i$-th sample.\n\n> \u201cHow can $theta$ be optimized in Eq. 1 if it does not appear in the two terms? The definition of the decoder $g(\u00b7)$ is not consistent. Does it take $theta$ as a condition or not? Seemingly Eq. 2 is the appropriate form.\u201d\n\nYes, the theta (dictionary) should be used in both $f$ and $g$, while $f$ and $g$ (the diffusion model) should also have their own parameters to be optimized. \n\nWe included theta (dictionary) in both the $f$ and $g$ and provided additional clarification on how $R$ is derived from $f$ in Equation (1).\n\n> \u201cThe definitions of two crical terms $E_{\\tilde{D}}$ and $d_S$ are unclear.\u201d\n\nWe expanded the explanation of the expectation term and the ideal metric distance in the revised paper by rewriting paragraph 5 in Section 3.1, we introduced details and examples to clarify these critical concepts.\n\n> \u201cHow is the dictionary $\\tilde{D}$ obtained given the argument \"As we have meaningful $\\tilde{D}$\"?\u201d\n\nWe fixed this ambiguous sentence by rewriting Section 3.1 paragraph 5, it is not we have meaningful dictionaries, but we only need to consider meaningful dictionaries. We also updated the corresponding sentence to ***\u201cAs we only need to consider meaningful dictionaries\u2026\u201d***\n\n\n> \u201cIt seems that the only variable to be optimized is the hidden dictionary $\\theta$. What about the models of $f(x;\\theta)$, $\\hat{g}(r_i;\\theta)$, $g_\\theta(R^i)$, and $g_{\\tilde{D}}(R)$\u201d\n\nWe clarified that the parameters of the decomposition model ($f$ and $g$) are also optimized, alongside the dictionary, to resolve any confusion and added this sentence: ***\u201cwe omit the parameters of decomposition model f and g that also need to be optimized in this target for simplicity.\u201d***."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700342341188,
                "cdate": 1700342341188,
                "tmdate": 1700342360952,
                "mdate": 1700342360952,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i2dApApnNy",
                "forum": "uqxBTcWRnj",
                "replyto": "C808qWBKRB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jEsx (Part 2/2)"
                    },
                    "comment": {
                        "value": "> \u201cThe illustration of Fig. 2 does not clearly demonstrate the formulation in Sec. 3 and the solution in Sec. 4:\u201d\n\n Figure 2 has been updated to better align with the formulation in Section 3, including annotated mathematical notations and more details about the player models.\n\n> 1. \u201c are not clearly labeled in the figure. 2)Where is the  copies of the model in the figure?\u201d\n\nWe added labels to the figure to clarify them.\n\n> 2. \u201cWhat does each patch stands for and what are the relation between the patches and the aforementioned terms in Sec. 3?\u201d\n\nWe explained the patches as visual parts corresponding to the set of visual parts {$x_i$}$_{i=1}^{N_P}$.\n\n> 3. \u201cWhy is there a \"GT loss\" in an unsupervised learning pipeline?\u201d\n\nGT loss is to avoid collapsed outputs as discussed above. It is unsupervised as it does not rely on any label. It introduced additional inductive biases to remove collapsed solutions from its search space.\n\n> 4. \u201cWhere is the \"Decomposition Loss\" mentioned in Fig. 2?\u201d\n  \nDecomposition loss is written at the end of Section 4.1, we bolden it, we also renamed the Reconstruction loss to reconstruction error as it is a part of decomposition loss to avoid such confusion.\n\n\n## For your questions:\n\n\n> \u201cThough Sec. 1&2 are well-demonstrated with clear motivations, the unsatisfied presentation of Sec. 3&4 makes the formulation and solution hard to follow. The authors are also encouraged to provide qualitative comparison results on Line World and ShapeNet5.\u201d\n\nThe concerns about Sections 3 and 4 are addressed above. We acknowledge the importance of providing qualitative comparisons on Line World and ShapeNet5. We will include these additional insights in the final version of our paper.\n\n---\n\nWe have made these revisions to directly address each of your points to ensure that our paper is as clear and informative as possible. Thank you again for your invaluable input. We hope our updates can address your concerns and help you re-evaluate our paper. We are open to further discussions and feel free to ask us any questions you have."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700342732885,
                "cdate": 1700342732885,
                "tmdate": 1700342732885,
                "mdate": 1700342732885,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WbOKeGwBIP",
            "forum": "uqxBTcWRnj",
            "replyto": "uqxBTcWRnj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2974/Reviewer_6UkK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2974/Reviewer_6UkK"
            ],
            "content": {
                "summary": {
                    "value": "This paper looks at a way to merge symbolic and DNN representations. The authors propose a transitional representation that contains high-fidelity details of the input and and also provides structural information about the semantics of the input. An Expectation-Maximization loop is used to optimize the parameters, where the Expectation step is used to optimize the hidden dictionary of parts, and maximize the overall likelihood of the dataset. To control the arity, techniques such as online clustering and random sampling are used. The authors conduct unsupervised segmentation on three abstract compositional visual object datasets and show superior accuracy compared to unsupervised clustering baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Neuro-symbolic reasoning is a timely topic for research, and joint optimization of reconstruction and predicate logic appears to be an interesting idea. The method utilizes a dictionary of entities, and 1-any and 2-ary predicates as a neck to train the semantic distance during reconstruction. The works incorporates several interesting ideas such as Expectation Maximization, game-theoretic loss function and online prototype clustering to make the system work."
                },
                "weaknesses": {
                    "value": "- The paper is a hard to read and the language is confusing. Technical concepts such as \"hidden dictionaries of symbolic knowledge\" are introduced early on without much explanation. \n- Experiments are limited to tiny, mostly binary datasets such as \"ShapeNet5\", which is basically a subset of 5 categories from the ShapeNet dataset. It is not clear if the methods would generalize to noisy real-world data, such as training using noisy, incomplete instances where the parts are not all visible. \n- Although the paper provides a reasonably well-curated list of neuro-symbolic approaches, the evaluations do not compare against any of the recent approaches. Instead the comparison is against clustering baselines.\n- The paper reads as a mishmash of several different ideas that are used together, but not integrated coherently. Therefore having a ablation studies to show the value of each module would be crucial. However, the evaluations do not provide a clear understanding of the contribution of each component to the overall methodology. \n- Ultimately, the task of reconstructing and explaining shapes simultaneously might be quite ambiguous as depicted in figure 4, and might not generalize to natural datasets, These aspects are not addressed in the paper."
                },
                "questions": {
                    "value": "1. Are the predicates shared among different classes? Do predicates always correspond to semantic attributes? It would help to visualize the learnt 1-any and the 2-ary predicates. \nHow does the method compare to other neuro-symbolic baselines? The current set of baselines are essentially unsupervised clustering methods.\n2. \n2. Please provide a clear set of ablation studies which show the benefits drawn from each component. How can the system be simplified without affecting the overall accuracy.\n3. It would be good to have a limitations section that discusses when this method wouldn't work. How do predicates such as left_of and larger (examples from the paper) operate in case of multi-view settings, where these terms become ambiguous."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2974/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816373787,
            "cdate": 1698816373787,
            "tmdate": 1699636241500,
            "mdate": 1699636241500,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0ejl2Yy2q4",
                "forum": "uqxBTcWRnj",
                "replyto": "WbOKeGwBIP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6UkK (Part 1/2)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your acknowledgment of our research topic and our idea, and your constructive feedback and comprehensive review. Our responses to your concerns are as follows:\n\n## For the weaknesses: \n\n> \u201cThe paper is a hard to read and the language is confusing. Technical concepts such as \"hidden dictionaries of symbolic knowledge\" are introduced early on without much explanation.\u201d \n\nAs mentioned in the response to all reviewers, we have updated our technical writing in the revised version. Specifically, in the introduction, we updated the corresponding sentence to: \n***\u201cTDL uses an Expectation Maximization (EM) algorithm to iteratively update dictionaries that store hidden representations of symbolic knowledge, through an online prototype clustering on the visual parts decomposed from the inputs by a novel game-theoretic diffusion model using the current dictionaries\u201d***\n\nIt uses only terms like \u201chidden representations\u201d which are widely accepted in ICLR community. It also directly expresses the relation of EM algorithm, game-theoretic decomposition, and the prototype clusterin to avoid confusion. We also checked the entie paper to avoid using new terms without explanation.\n\n> \u201cExperiments are limited to tiny, mostly binary datasets such as \"ShapeNet5\", which is basically a subset of 5 categories from the ShapeNet dataset. It is not clear if the methods would generalize to noisy real-world data, such as training using noisy, incomplete instances where the parts are not all visible.\u201d\n\nOur experimental setup is delicately designed to exclude visual feature confoundings. This approach substantiates our main claim that TDL can learn compositional and reusable visual predicates from high-dimensional input. Extension to noisy real-world data is recognized as an important future direction that is out of the main scope of this paper.\n\n> \u201cAlthough the paper provides a reasonably well-curated list of neuro-symbolic approaches, the evaluations do not compare against any of the recent approaches. Instead the comparison is against clustering baselines.\u201d\n\nThe neuro-symbolic approach is a broad concept, it can cover almost all kinds of tasks while baseline picking is task-specific. In our task, unsupervised learning of concepts and relations (predicates) from visual data, the unsupervised part segmentation methods are the most relevant. To the best of our knowledge, we do not see a suitable, directly comparable neural symbolic baseline, especially for this problem. \n\n> \u201cThe paper reads as a mishmash of several different ideas that are used together, but not integrated coherently. Therefore having a ablation studies to show the value of each module would be crucial. However, the evaluations do not provide a clear understanding of the contribution of each component to the overall methodology.\u201d\n\nWe improved our writing to make the structure of our method clear. Our method is basically an EM algorithm implemented by two components, clustering and a decomposition model (i.e. the game-theoretic diffusion model). We provided the ablation study in Appendix E, \u201cBase\u201d vs \u201d+Cluster\u201d, the model with and without clustering, we further provided an ablation study on the commonly used optimizations for the decomposition model. \n\nInside the decomposition model, its loss terms are all necessary to guarantee the right function (right convergence) of the model, the reconstruction error is the basis, and the other GT loss terms are designed to avoid collapsed outputs which are further discussed in Appendix D, SMLD loss is to train the scoring based diffusion model we used. They cannot be further decomposed into an ablation study. \n\n> \u201cUltimately, the task of reconstructing and explaining shapes simultaneously might be quite ambiguous as depicted in figure 4, and might not generalize to natural datasets, These aspects are not addressed in the paper.\u201d\n\nWe modified the caption of Figure 4 from:\n***\u201cComparison between our method and baselines on the OmniGlot test set. Our method can learn interpretable strokes compared to the baselines that failed to give effective strokes.\u201d***\n to:\n***\u201cExamples from OmniGlot test set. Our method generates multiple interpretable strokes to reconstruct the input hand-written characters. As a comparison, the baseline methods segment the input into colored parts that are not valid strokes revealing a failure in learning compositionality.\u201d***\n\nIt introduced more details about the difference between our method and baselines, and why our method can be regarded as successful in learning reusable predicates.\n\nAs discussed earlier, It is natural to extend the basic TDL proposed to natural data, it firstly requires considering the noise as discussed, and secondly the additional information, including the visual features like edges, shadows, and textures, which are normally related to the use of pre-trained backbones."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700341456595,
                "cdate": 1700341456595,
                "tmdate": 1700341456595,
                "mdate": 1700341456595,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c00GoU2wRl",
                "forum": "uqxBTcWRnj",
                "replyto": "WbOKeGwBIP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6UkK (Part 2/2)"
                    },
                    "comment": {
                        "value": "## For your questions:\n\n> 1.1 Are the predicates shared among different classes? \n\n\nYes, as an unsupervised learning method, the model is unaware of \u201cclasses\u201d.  In our experiments, the learned predicates correspond to visual parts and combinations (e.g., armrest, leg), applicable across different object types (e.g. table, chair), It is also demonstrated in our ShapeGlot transfer learning to unseen classes experiment.\n\n\n> 1.2 Do predicates always correspond to semantic attributes? It would help to visualize the learnt 1-any and the 2-ary predicates. \n\nWe rewrote Section 3.1 paragraph 5 to reduce the confusion related to this question. To answer your question, the correspondence of predicates to semantic attributes is not guaranteed, as it is subjective like there exist different fonts to write the same character with the same meaning. The learned transitional representation (i.e. predicates) is an \"average\" of those fonts that have the minimal possible distance to all of them by approximating their commonality, the reusable and compositional, as demonstrated in the objective target in Section 3.2. And each font is a \"concrete meaningful dictionary\" mentioned in Section 3.2. Such an \"average\" dictionary is not necessarily highly meaningful but should be close enough as we assume the meaningful ones are compositional and reusable.\n\nWe agree that visualizing the latent predicates is helpful, right now we do not have a decoder that can directly decode the predicate as an image, we are working on it and try to update it in the final version.\n\n> 1.3 How does the method compare to other neuro-symbolic baselines? The current set of baselines are essentially unsupervised clustering methods.\n\nAs discussed above in Weaknesses 3, to the best of our knowledge, we cannot find a neuro-symbolic baseline for our specific task. \n\n> 2. Please provide a clear set of ablation studies which show the benefits drawn from each component. How can the system be simplified without affecting the overall accuracy.\n\nAddressed in Weaknesses 4 above. Further simplification might involve merging loss terms or integrating the game theory mechanism into a multi-body diffusion process, which we consider beyond this paper\u2019s scope.\n\n> 3. It would be good to have a limitations section that discusses when this method wouldn't work. How do predicates such as left_of and larger (examples from the paper) operate in case of multi-view settings, where these terms become ambiguous.\n\nWe've added a new section in Appendix K.1 discussing the limitations. Particularly, the model's output may be ambiguous in scenarios where the problem itself is ambiguous to humans. For example, A is on the left of B in image 1, while A is on the right in image 2. Decide whether A is on the left or right will be an undecidable problem, the model will also give an ambiguous output. Similar to Figure 8, bottom left, when the model is given an invalid combination of parts, it cannot decide a clear relation.\n\nRelations like \"larger\" are different as it is related to reasoning (i.e. using knowledge/rule of perspective) which is not involved in TDL as a representation method. We added the discussion of the unsupervised rule learning in Appendix K.2 under the view of TDL.\n\n---\n\nWe hope our replies can address your concerns and help you further evaluate our paper, and we are happy to solve any additional questions you may have and further improve our paper. Thank you again for your invaluable feedback."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700341549056,
                "cdate": 1700341549056,
                "tmdate": 1700341549056,
                "mdate": 1700341549056,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]