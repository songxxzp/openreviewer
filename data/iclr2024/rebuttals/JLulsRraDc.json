[
    {
        "title": "Bridging the Gap Between Foundation Models and Heterogeneous Federated Learning"
    },
    {
        "review": {
            "id": "6oj91mNO6R",
            "forum": "JLulsRraDc",
            "replyto": "JLulsRraDc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1488/Reviewer_U93Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1488/Reviewer_U93Y"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a comprehensive and timely approach to integrating FMs with FL, a topic of significant relevance in the era of privacy-preserving AI. It provides a comprehensive experimental setup, employing diverse benchmarks and datasets across both NLP and computer vision tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The quantitative analysis is thorough, covering multiple performance metrics and offering a comparative evaluation against baseline federated learning models."
                },
                "weaknesses": {
                    "value": "The paper suffers from organizational and presentational issues, making it somewhat challenging to navigate."
                },
                "questions": {
                    "value": "Abstract:\n- Some claims in the Abstract are a bit vague. For example, the authors mention RaFFM shows \"significant superiority in resource utilization efficiency.\" What metrics are used to measure this superiority? How \u201csignificant\u201d are the improvements?\n- Further, the authors claim that the performance is \"on par with traditional FL methods applied to full-sized FMs.\" This statement would benefit from quantification, if possible. Is it a 1% difference in performance, or is it negligible?\n- The abstract mentions that the framework is effective \"across tasks in both natural language processing and computer vision domains.\" This is a broad claim. Please specify if there any limitations or specific conditions under which this is true.\n\nIntroduction:\n- The problem statement could be more explicit and clearer. The authors mention the challenges of integrating FMs into FL but do not clearly delve into why this integration is crucial. For example, \u201cGiven the superior strengths of FMs in few-shot transfer learning, they appear well-suited for non- IID FL environments.\u201d This sentence: a) assumes that Foundation Models have \"superior strengths\" in few-shot transfer learning without providing evidence or citations to support this claim. This is a strong statement that requires substantiation. b) The sentence implies a logical connection\u2014that because FMs are good at few-shot transfer learning, they are well-suited for non-IID FL environments. However, it does not explain why this would be the case. The logical leap is not self-evident and needs justification.\n- \u201cfine-tuning FMs typically requires approximately seven times the resources compared to inference.\u201d This is an interesting point but is presented without reference or further elaboration.\n- Consider improving the logical flow and cohesion when transitioning from the problem statement to the proposed solution.\n- While the authors discuss the technical aspects, the broader impact of this work is not adequately addressed. For example, how will RaFFM contribute to the field of FMs/FL?\n- The key contributions could be more specific (despite some descriptions in the paragraph before the bullet points). For example, what are \"specialized FM compression algorithms\"? How are they specialized, and why is this significant? Also, claims such as \"enhanced resource utilization,\" \u201csignificant reduction in communication overhead\u201d could benefit from clearer quantification and specification, if possible.\n\nBackground:\n- The discussion about FL somewhat lacks depth and may benefit from improved cohesion as it feels disjointed. For example, the authors mention \u201ca representative FL algorithm is FedAvg\u201d but do not explain why it is representative or how it works.\n- The authors mention that FL is a preferred choice in sectors like healthcare but do not elaborate on why this is the case. A sentence (or citation) or two providing context could be beneficial.\n- Phrases like \"often lead to training failures\" and \"poor model convergence and performance\" are vague. What constitutes a \"training failure\"? How poor is \"poor performance\"? Consider adding some citations to support these claims at least.\n- Similar problem to the problem statement. The authors mention that there is a gap between traditional model training and FL, particularly in heterogeneous FL-edge environments. However, the nature of this gap is not clearly articulated. Is it a technological gap, a performance gap, or something else?\n- The term \u201cresource-hungry\u201d has been mentioned several times so far but it is somewhat ambiguous. Is it computational resources, memory, or something else? Furthermore, in the context of the paper, it seems \u201cresource-hunger\u201d would be more fitting, given it has been properly explained?\n\nMethodology:\n- This section reads a bit like a mix of existing solutions and proposed solution (especially the opening part of the first subsubsection \u201cSALIENT PARAMETER PRIORITIZATION\u201d). This lack of clear demarcation can lead to confusion for the reader and dilutes the focus of the section. Consider maybe moving some of the discussion on model compression/scaling, the review of existing solutions, and how the proposed work differs or improves upon existing ones."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1488/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698456012303,
            "cdate": 1698456012303,
            "tmdate": 1699636077868,
            "mdate": 1699636077868,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WI6FYSA98P",
                "forum": "JLulsRraDc",
                "replyto": "6oj91mNO6R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "content": {
                    "title": {
                        "value": "General response for reviewer U93Y"
                    },
                    "comment": {
                        "value": "## General response for reviewer U93Y\nDear reviewer U93Y, \n\nThank you for your valuable and constructive feedback. In response to your comments and suggestions, we have carefully revised the manuscripts to enhance clarity and address potential miscommunication. We have incorporated citations to support our arguments, clarified points raised in your comments, and added further motivations in the latest version of our paper.  \n\nAdditionally, inspired by suggestions from other reviewers, we have conducted supplementary experiments to enrich our research. These include: \n\n\n- Experiment on the Necessity of Specialized Salient Parameter Prioritization (Link 2 in supplementary material):  \n\n- Experiment Comparing with Additional Baselines (Link 3 in supplementary material)  \n\n- Post-training High-performance Resource-aware Model Deployment (Link 4 in supplementary material) \n\n  \n\nIf there are any additional questions, please do not hesitate to let us know. We are committed to providing any necessary supplemental support material and explanations."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1488/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700199189904,
                "cdate": 1700199189904,
                "tmdate": 1700199189904,
                "mdate": 1700199189904,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "23m7Lu1HlN",
                "forum": "JLulsRraDc",
                "replyto": "6oj91mNO6R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Question on Abstract"
                    },
                    "comment": {
                        "value": "### General response to Question on Abstract \n\nThank you for your feedback on the abstract. In response, we have significantly revised the abstract to provide a clearer and more detailed overview of our work. The revised abstract now includes specific quantitative data supporting our claims, addresses the key points you raised, and integrates relevant examples and motivations to provide a comprehensive overview of our work. \n\n\n### [Question 1] What metrics are used to measure this superiority? How \u201csignificant\u201d are the improvements? \n\n \n\nWe appreciate your advice on providing clarity regarding the measurement metrics. In response, we have updated the abstract with specific quantitative measures as follows to avoid unclarity: \n\n\"RaFFM demonstrates up to 2.3 times improvement in resource utilization efficiency and saving up to 225.02 GB in communication costs when optimizing the FLAN-T5 Base model.\u201d \n\nAdditionally, here are explanations of the evaluation metrics (detailed evaluations are available in  **Experiment section**) : \n\n**Communication Cost Measurement**: The network traffic between edge clients and the server is quantified in Gigabytes (GB) and Megabytes (MB). Detailed data and analysis can be found in Section 4.4, Table 4, and Figures 2(c) and 2(d) of the Experiment section. \n\n**System Resource Requirements**:  We quantify the system peak computational memory (RAM) requirements, in Gigabytes (GB), necessary to deploy and optimize a given FM across the entire edge-FL system. \n\n**Resource Utilization**: This metric evaluates the average proportion of memory utilized during both the deployment and training phases, compared to the total computational memory available on edge devices. \n\n \n\n### [Question 2] Unclarity statements of performance are on par with traditional FL methods applied to full-sized FMs. \n\n \n\nThank you for highlighting the need for greater clarity regarding our comparative performance statements. We have taken your suggestion into account and have revised our abstract accordingly to provide a more quantifiable presentation. \n\nIn the revised abstract, we now include the following statement to offer specific metrics: \n\n\u201cIn our experiments with the Vision Transformer (ViT) model, RaFFM's resource-aware deployment achieves a training acceleration of approximately 1.76 times compared to full-size FM deployments, with a minimal accuracy loss of 0.49%.\u201d \n\nOn our original statements, the key information we want to convey is that RaFFM markedly enhances the training speed of FMs in edge-FL scenarios while incurring minimal performance loss compared to deployments using full-sized models. We followed your advice and added the quantitative example, which further underscores and clarifies our statements. \n\n \n\n \n### [Question 3] Clarification on the statement effective \u201cacross tasks in both natural language processing and computer vision domains.\"  \n\n \n\nWe appreciate you for highlighting the need for specificity in our claims. Accordingly, we have revised our manuscript, we now detailed the scope and limitations of our framework\u2019s effectiveness in NLP and CV domains. In the revised abstract, we now clarify: \n\n\u201cRaFFM demonstrates its effectiveness in a range of tasks within natural language processing, including question-answering and sentiment analysis, and in computer vision, particularly in image classification tasks.\u201d \n \n\nFurthermore, the **Experiment section** of our manuscript provides an in-depth analysis of these tasks. For NLP, we have thoroughly tested RaFFM across a spectrum of tasks like question answering, sentence classification, and sentiment analysis. In the CV domain, our focus has been on image classification tasks, where we have analyzed RaFFM\u2019s strengths and weaknesses compared to baseline methods."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1488/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700199290043,
                "cdate": 1700199290043,
                "tmdate": 1700283601724,
                "mdate": 1700283601724,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CuqkzQN3kW",
                "forum": "JLulsRraDc",
                "replyto": "6oj91mNO6R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Question on Introduction \u2013 Part 1"
                    },
                    "comment": {
                        "value": "### General response to Question on Introduction \n\n \n\nThank you for your constructive feedback on improving the introduction of our manuscript. We have made several modifications based on your suggestions, including the addition of citations to support our arguments, clarification of the points you raised in your review comments, and inclusion of further motivations, etc. \n\n \n\n### [Question 1] The problem statement \u201cFMs are superior in few-shot transfer learning and FL\u201d could be more explicit and clearer, and lack of reference support. \n\n \n\nWe greatly appreciate your feedback on making our problem statement more explicit and providing supporting references. In response, we have revised our manuscript to offer a clearer explanation and added relevant citations to substantiate the strengths of FMs in few-shot transfer learning, as well as their potential in non-IID FL. \n\n \n\nSpecifically, we have included citations [1] [2] to support our assertion regarding the efficacy of FMs in few-shot transfer learning. Additionally, we have referenced [3] to illustrate the potential advantages of FMs in non-IID FL environments. \n\nMoreover, our experiments (see Table 3 on manuscripts) demonstrate the effective application of the FM model (ViT) in few-shot learning tasks within image classification. This empirical evidence further bolsters our claim regarding the suitability of FMs in these contexts. \n\n \n\n \n \n\n \n\n**Reference**  \n\n \n\n[1] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan et al. \"Language models are few-shot learners.\" Advances in neural information processing systems 33 (2020): 1877-1901. \n\n[2] Hu, Shell Xu, Da Li, Jan St\u00fchmer, Minyoung Kim, and Timothy M. Hospedales. \"Pushing the limits of simple pipelines for few-shot learning: External data and fine-tuning make a difference.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9068-9077. 2022. \n\n[3] Zhuang, Weiming, Chen Chen, and Lingjuan Lyu. \"When foundation model meets federated learning: Motivations, challenges, and future directions.\" arXiv preprint arXiv:2306.15546 (2023). \n\n \n### [Question 2] Why do fine-tuning FMs typically require significantly more resources compared to inference? \n\n \n\nThank you for highlighting the importance of clarity on why fine-tuning FMs typically requires significantly more resources compared to inference. We have expanded our manuscript to include a more detailed explanation and a relevant reference to support this point. \n\n \n\nHere is a brief explanation: Fine-tuning FMs is more resource-intensive than inference because it necessitates maintaining the entire computational state of the model in memory for optimization purposes. This process involves not only storing the full model weights but also retaining gradients, activation states, and optimizer momentum states, all of which contribute to increased memory usage. This explanation is further detailed in the tutorial by Stern [1]. \n\n \n\nIn our experimental setup, with a mini-batch size of 4, we observed that fine-tuning consumes approximately seven times more resources than inference. This disparity in resource consumption becomes even more significant with larger batch sizes. \n\n \n \n\n**Reference** \n\n[1] Stern, J. (2022, August 18). A comprehensive guide to memory usage in pytorch. Medium. https://medium.com/deep-learning-for-protein-design/a-comprehensive-guide-to-memory-usage-in-pytorch-b9b7c78031d3 \n\n \n\n \n\n### [Question 3]: Lack of broader impact \n\n \n\n \n\nThank you for emphasizing the importance of addressing the broader impact of our work. Based on your suggestion, we have expanded the discussion on this aspect within the main body and the conclusion section of our manuscript. \n\n \n \n\n \nAdditionally, the broader impact of RaFFM can be summarized as follows: \n\n**1. Addressing Deployment Challenges of FMs in FL**: RaFFM specifically tackles the challenges associated with deploying large-scale FMs in resource-heterogeneous edge-FL systems. \n\n**2. Introducing novel FM compression algorithms for edge-FL clients**:  \n\nWe have developed specialized FM compression algorithms tailored for edge-FL systems. These algorithms enable dynamic compression of FMs to meet the resource constraints of individual edge clients, thereby facilitating more efficient and effective model deployment. \n\n**3. Limitations and future research directions**: We also acknowledge the limitations in the Conclusion section. Notably, certain foundation models may remain unsuitable for deployment on resource-constrained edge devices, even after compression. We also highlight a need for advancements in hardware and algorithmic strategies, pointing towards potential directions for future research."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1488/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700199475972,
                "cdate": 1700199475972,
                "tmdate": 1700200197404,
                "mdate": 1700200197404,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qo3tgX5Nfg",
                "forum": "JLulsRraDc",
                "replyto": "6oj91mNO6R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Question on Introduction - Part 2"
                    },
                    "comment": {
                        "value": "### [Question 4] Further clarification on the key contributions in detail. \n\n \n\nThank you for requesting further details on the key contributions of our work. We have made revisions to our manuscript to provide clearer explanations, which are briefly summarized below: \n \n\n**1. Specialized FM compression algorithm**: Our model compression algorithm is designed to transformer-based FMs and tailored to the unique constraints of edge clients, enabling efficient deployment of models in a heterogeneous environment. It dynamically scales FMs to suit the computational capabilities of each edge device, with more powerful devices receiving larger models and resource-constrained devices receiving smaller models. Hence, we refer to it as the specialized FM compression algorithm for edge-FL. \n\n**2. Enhance resource utilization**: In edge-FL scenarios, local clients have varied resource capacities. Traditional approaches often lead to under-utilization of high-capacity devices due to the need to accommodate the limitations of the least capable devices. RaFFM addresses this inefficiency by deploying appropriately scaled models to each device, thereby optimizing the overall system efficiency. \n\n**3. Significant Reduction in communication overhead**: RaFFM's strategy of using scaled-down models for less capable clients leads to a considerable reduction in communication costs. For instance, our experimental results (Table 4, Section 4.4, and Figure 2) show that with the FLAN-T5-base model, RaFFM achieved a reduction of 227.02GB in communication overhead to reach the target F1 score. This quantifies the significant reduction in communication overhead facilitated by our approach."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1488/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700199507183,
                "cdate": 1700199507183,
                "tmdate": 1700200203912,
                "mdate": 1700200203912,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yjzVuYA4Zf",
                "forum": "JLulsRraDc",
                "replyto": "6oj91mNO6R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer U93Y \ud83d\ude0a,\n\nI hope this message finds you well. We are writing to follow up on the revisions we submitted in response to your valuable feedback. We have made concerted efforts to address each of your concerns and suggestions comprehensively.\n\nAs the open discussion period is approaching its deadline on November 22nd, we understand that you have many commitments and appreciate the time and effort you dedicate to the review process. If any further questions or clarifications are needed, please do not hesitate to let us know. We are more than willing to provide any additional information that can assist in your review.\n\nThank you once again for your insightful comments and valuable guidance. Your feedback is crucial in helping us refine our work, and we await your thoughts on our response.\n\n\nBest regards \ud83d\ude04,\n\nAnonymous authors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1488/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670652577,
                "cdate": 1700670652577,
                "tmdate": 1700670652577,
                "mdate": 1700670652577,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3FprlAOQcZ",
                "forum": "JLulsRraDc",
                "replyto": "yjzVuYA4Zf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1488/Reviewer_U93Y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1488/Reviewer_U93Y"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for the response and clarification."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1488/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704990150,
                "cdate": 1700704990150,
                "tmdate": 1700704990150,
                "mdate": 1700704990150,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "38p0bPvPwz",
            "forum": "JLulsRraDc",
            "replyto": "JLulsRraDc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1488/Reviewer_Y3SH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1488/Reviewer_Y3SH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a resource-aware federated foundation models training framework called RaFFM. RaFFM assigns different sub-models to clients for local training based on neuron saliency and client computational resource constraints, which allows all clients in the FL to train FMs under the scenario of limited computational resources and uneven distribution."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1: The problem of how to train FMs in FL scenarios with limited and unevenly distributed computational resources studied in the paper is a practical one, which is partially alleviated by the proposed approach.\nS2: The proposed method can save the communication overhead while guaranteeing the performance of the global model and is effectively combined with PEFT methods for various FMs."
                },
                "weaknesses": {
                    "value": "W1: The method proposed in the paper to select neurons to be retained based on saliency lacks novelty. First, the use of the L1-norm measure of neuron parameter saliency is just a simple use of an existing method [1]. Second, the strategy of ranking the saliency to select neurons is also commonly used in the field of model pruning [2]. Third, the paper does not make it clear why it is necessary to apply a special significance prioritization strategy to the transformer.\nW2: Comparisons with relevant baseline methods are lacking in the paper. Methods that extract submodels from the original model for local training have been investigated, such as in [3].\nW3: The description of the experimental setup of the paper is not clear, for example: 1) The scenario in the paper is client computing resource heterogeneity, and the experimental part does not introduce the distribution of the sub-model size held by each client; 2) The hyper-parameter settings in the experimental part are not listed, which makes it difficult to reproduce the experiments; 3) The distribution of the data between the clients is not introduced, and there is a lack of experiments on the part of the heterogeneity of the data.\n\n[1] Li H, Kadav A, Durdanovic I, et al. Pruning Filters for Efficient ConvNets[C]//International Conference on Learning Representations. 2016.\n[2] Hu H, Peng R, Tai Y W, et al. Network trimming: A data-driven neuron pruning approach towards efficient deep architectures[J]. arXiv preprint arXiv:1607.03250, 2016.\n[3] Niu Y, Prakash S, Kundu S, et al. Federated Learning of Large Models at the Edge via Principal Sub-Model Training[J]. arXiv preprint arXiv:2208.13141, 2022."
                },
                "questions": {
                    "value": "See Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1488/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1488/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1488/Reviewer_Y3SH"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1488/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634950759,
            "cdate": 1698634950759,
            "tmdate": 1699636077779,
            "mdate": 1699636077779,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I3BeJWGBiQ",
                "forum": "JLulsRraDc",
                "replyto": "38p0bPvPwz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "content": {
                    "title": {
                        "value": "General response to Reviewer Y3SH"
                    },
                    "comment": {
                        "value": "### General response to Reviewer Y3SH \n\n \n\nDear reviewer Y3SH, \n\nWe appreciate your insightful comments.  We have taken your feedback into careful consideration and have made the following enhancements: first, we emphasized the novelty of our proposed transformer-based salient parameter prioritization (SPP) algorithm. Secondly, we have conducted additional experiments to further validate the effectiveness of our SPP algorithm compared to existing rank-based pruning algorithms in transformer models. Furthermore, suggested by the reviewer, we have conducted additional experiments comparison with PruneFL and PriSM. Additionally, we have also emphasized the distinctions between RaFFM and network pruning, referenced the suggested papers, and provided a detailed experimental setup. \n\n \n\nThe follow-up experiments are available in the supplementary materials, where we provide comprehensive information, step-by-step instructions, and Jupyter notebook tutorials to verify our results: \n\n- Experiment on the Necessity of Specialized Salient Parameter Prioritization (Link 2 in supplementary material):  \n\n- Experiment Comparing with Additional Baselines (Link 3 in supplementary material)  \n\n- Post-training High-performance Resource-aware Model Deployment (Link 4 in supplementary material) \n\n  \n\nIf there are any additional questions, please do not hesitate to let us know. We are committed to providing any necessary supplemental support material and explanations."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1488/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700198562571,
                "cdate": 1700198562571,
                "tmdate": 1700198562571,
                "mdate": 1700198562571,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CRv9JUJsLQ",
                "forum": "JLulsRraDc",
                "replyto": "38p0bPvPwz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[Question 1] Motivation and contribution of salient parameter prioritization"
                    },
                    "comment": {
                        "value": "### [Question 1] Motivation and contribution of salient parameter prioritization  \n\n \n\nThank you for your insightful comments. We value this opportunity to clarify the novelty and specific contributions of our work. We have included the references you suggested ([1], [2], [3]) in our revised manuscript to acknowledge the established work in this field. \n\nFirstly, we wish to clarify that our use of the L1 norm as a saliency metric is part of a broader framework. We have developed a specialized salient parameter prioritization (SPP) algorithm specifically for transformer-based Foundation Models (FMs), as detailed in Section 3. This approach is integrated with the other components of RaFFM and addresses the specific challenges of applying FMs in FL environments. \n\nWhile we recognize that identifying salient parameters is an established method in the domain of network pruning. However, our proposed SPP is more than just measuring saliency, prioritization implies weight reordering, and the reordering benefits of the scaled FMs always retain the most salient parameters. Our novel contribution of SPP lies in retaining the inherent attention saliency of transformer-based FMs after model scaling, particularly in heterogeneous resource FL contexts. \n\nBecause transformers, particularly multi-head attention layers, are designed to capture positional and sequential information. An example is Vision Transformer (ViT), where adding positional embeddings significantly improve the performance. Traditional pruning-based parameter prioritization methods, if applied directly, would destroy the inherent attention characteristic in FMs. To circumvent this, we propose our specialized SPP to preserve the essential capability of multi-head attention layers. \n\n \n\nSummary of our novel contribution in SPP: \n\n- Maintain the integrity of pre-trained knowledge in FMs . (Demonstrate by the supplementary materials link 2 and Theorem 1 in Appendix, details refer Question 2 response) \n\n- Specialization for Transformers: Tailoring the salient parameter prioritization specifically for multi-head attention-based transformer models. \n\n- Prototype for Heterogeneous FL model aggregation: Strategically placing salient parameters at the forefront for efficient extraction and aggregation in heterogeneous model FL."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1488/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700198658003,
                "cdate": 1700198658003,
                "tmdate": 1700283453438,
                "mdate": 1700283453438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8uxur50Frk",
                "forum": "JLulsRraDc",
                "replyto": "38p0bPvPwz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[Question 4] Difference between Network Pruning"
                    },
                    "comment": {
                        "value": "### [Question 4] Difference between Network Pruning \n\n\nNetwork pruning: Network pruning typically involves reducing the complexity of a pre-trained model by eliminating less significant neurons or connections. This process often occurs post-training and requires subsequent fine-tuning to restore or enhance the model\u2019s performance. A key limitation, especially relevant in the context of federated learning at the edge, is the computational intensity of this post-pruning fine-tuning process. When dealing with edge clients with limited computational resources and local data, this approach can be particularly challenging. \n\nSalient Parameter Prioritization in RaFFM: RaFFM\u2019s Salient Parameter Prioritization (SPP) extends beyond mere measurement of saliency. It encompasses strategic weight reordering, ensuring that the scaled Foundation Models (FMs) retain the most crucial parameters. Significantly, SPP maintains the attention mechanism's saliency within scaled FMs.  SPP enables RaFFM to collaboratively and dynamically optimize scaled FMs at the edge of resource constraints, based on their local resources. The outcome is a scalable FM capable of generating multiple high-performance subnetworks that meet these constraints. A notable advantage of RaFFM is its ability to scale an FM post-federated learning without necessitating additional fine-tuning. \n\n \n\nTo practically demonstrate RaFFM's efficacy, we provide a demonstration in the supplementary materials (link 4). This demo illustrates RaFFM's capability to generate multiple high-performance subnetworks post-training without the need for extra fine-tuning. It exemplifies RaFFM's efficiency in adapting FMs for diverse and resource-constrained FL environments while preserving their performance and integrity, particularly with transformer architectures."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1488/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700199003641,
                "cdate": 1700199003641,
                "tmdate": 1700283698356,
                "mdate": 1700283698356,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IzdXfhOmbG",
                "forum": "JLulsRraDc",
                "replyto": "38p0bPvPwz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer Y3SH \ud83d\ude0a,\n\nI hope this message finds you well. We are writing to follow up on the revisions we submitted in response to your valuable feedback. We have made concerted efforts to address each of your concerns and suggestions comprehensively.\n\nAs the open discussion period is approaching its deadline on November 22nd, we understand that you have many commitments and appreciate the time and effort you dedicate to the review process. If any further questions or clarifications are needed, please do not hesitate to let us know. We are more than willing to provide any additional information that can assist in your review.\n\nThank you once again for your insightful comments and valuable guidance. Your feedback is crucial in helping us refine our work, and we await your thoughts on our response.\n\n\nBest regards \ud83d\ude04,\n\nAnonymous authors"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1488/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670686987,
                "cdate": 1700670686987,
                "tmdate": 1700670686987,
                "mdate": 1700670686987,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "t2TxCXQcfz",
            "forum": "JLulsRraDc",
            "replyto": "JLulsRraDc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1488/Reviewer_erSL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1488/Reviewer_erSL"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a method to optimise training of large models with Federated Learning. The designed a salient parameter prioritisation and a submode extraction method that is tailored for foundation models and they show that this method can help us train larger foundation models even within resource-restricted clients."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well written and easy to understand. \n- The transformer-focused salient parameter prioritisation is an interesting idea and a good contribution of this paper \n-  The evaluation is quite thorough, showing results on a number of benchmarks and settings. \n- The authors evaluated the communication cost and the memory footprint of their approach. It was great to see these results being included. \n- Overall, thee results show good improvements over the state of the art."
                },
                "weaknesses": {
                    "value": "The main weakness of this paper is the overall novelty factor. As the authors mention, there are a lot of works that recently introduced sub-model training to optimise device resources for training larger models with FL. While this method is tailored for transformer-based foundation models, the main differences to existing works are somewhat limited. Having said that, there are contributions such as the saliency metric."
                },
                "questions": {
                    "value": "Some areas where the authors could improve:\n\n- Maybe the authors can motivate a bit more on the motivation to train larger Foundation models with federated learning. Typically we might train those on public tasks. Is FL  mostly targeting the fine-tuning part ? Overall, an expanded motivation would be great. \n\n- Maybe some discussion about privacy could be good to have. For example, how well does this method work with Differential privacy (noise), gradient clipping etc. I don't think there is a need to show these results, but maybe consider discussing these aspects. \n\n- While speedups of ~2x were shown for smaller FM (e.g, bert base), and reduced memory cost,  I was wondering if this is enough to train on resource constrained devices (e.g., mid-range mobile phones). In figure 3, memory in the order of 200GB is shown. I was wondering if this can be broken down to better show the memory requirements during training for end-devices. Maybe provide some numerical results wrt to the time it would require to compute a single round for a range of end-user devices."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1488/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1488/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1488/Reviewer_erSL"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1488/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698750798427,
            "cdate": 1698750798427,
            "tmdate": 1699636077708,
            "mdate": 1699636077708,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MkwpOPhPoa",
                "forum": "JLulsRraDc",
                "replyto": "t2TxCXQcfz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1488/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### General response to Reviewer erSL \n\n \n\nDear reviewer erSL,  \n\nWe are grateful for your supportive and insightful comments.  \n\nIn line with your comments, we have revised our original manuscript. The motivation has been clarified, particularly highlighting the broader impacts from a security perspective of integrating Foundation Models (FMs) into Federated Learning (FL). Additionally, we have augmented our experimental results with quantitative measurements that illustrate the performance of our algorithms in end-user FL scenarios. \n\nAdditionally, Following constructive suggestions from other reviewers, we have also conducted three additional experiments: \n\n- Experiment on the Necessity of Specialized Salient Parameter Prioritization (Link 2 in supplementary material):  \n- Experiment Comparing with Additional Baselines (Link 3 in supplementary material)  \n- Post-training High-performance Resource-aware Model Deployment (Link 4 in supplementary material) \n\n  \n\nIf there are any additional questions, please do not hesitate to let us know. We are committed to providing any necessary supplemental support material and explanations. \n\n \n\n \n\n### [Question 1] Clarification on Motivation to Train Larger Foundation Models with Federated Learning \n\n \n\nThank you for your remarks regarding the motivation behind using FL to train larger FMs. We have revised our manuscript to provide a clearer motivation on the abstract, introduction, and conclusion section. \n\nIndeed, our primary focus is on the fine-tuning of FMs within FL scenarios, specifically addressing the unique challenges that arise when fine-tuning these models in a collaborative edge-FL environment. \n\nGiven the substantial size of FMs and their rich pre-trained knowledge, our proposed RaFFM method prioritizes resource efficiency and the preservation of this pre-trained knowledge. Additionally, RaFFM is designed to support post-FL model deployment, enabling the generation of a significant number of scaled models that cater to various resource constraints encountered at the edge. \n\n \n### [Question 2] Broader Impact on Privacy and Security \n\n \n\n \n\nThank you for highlighting the importance of discussing the broader impact of our solution on privacy and security. In response, we revised our manuscripts correspondingly and added related discussion on the Conclusion section.  \n\nWhile our primary focus has been on addressing the challenges of resource heterogeneity in applying FMs to FL, we recognize the implications for privacy and security. Incorporating FMs into FL can indeed have a positive impact on FL's security. Due to their pre-training on large corpora of public datasets, FMs have the potential to significantly mitigate threats such as inference attacks. As highlighted in the work [1], amidst the growing concern for statistical notions of privacy, the application of FMs may contribute to achieving perfect secrecy for certain sensitive tasks, given their rich pre-trained knowledge. \n\n \n\n**Reference** \n\n \n\n[1] Arora, Simran, and Christopher R\u00e9. \"Can Foundation Models Help Us Achieve Perfect Secrecy?.\" arXiv preprint arXiv:2205.13722 (2022).,  \n\n \n \n\n### [Question 3] Quantitative Measurement of RaFFM\u2019s Speed Up on End-User Devices \n\n \n\nThank you for your valuable suggestions regarding the need for quantitative measurements of RaFFM\u2019s efficiency on end-user devices. In response, we have conducted additional experiments and provided specific data to demonstrate RaFFM\u2019s performance in this context, as shown in Table 1.  \n\n \n \n\nWe focused on optimizing the BERT-base model for the SST-2 dataset in an FL scenario involving 100 clients, with a participation rate of 10% among local clients. Recognizing that actual training time can vary due to several uncontrollable factors such as device temperatures and electricity voltage, we chose to measure energy consumption as a more stable indicator of efficiency. \n\n \nOur experiments were conducted across five different edge system settings, categorized as Xsmall, Small, Medium, Large, and Xlarge (detailed settings information can be find in our supplementary materials link 5). were designed to represent a range of end-user devices, from resource-constrained to more powerful systems. As shown in Table 1, by tracking the peak energy usage for each setting, we provide a more accurate and reliable measure of RaFFM's efficiency and its adaptability to diverse hardware environments. \n\n \n\n**Table 1. Peak Energy Usage at Edge System** \n\n| System Setting | Model     | Dataset | Peak Energy Usage | \n|----------------|-----------|---------|-------------------| \n| Xsmall         | BERT-Base | SST-2   | 0.5 kWh           | \n| Small          | BERT-Base | SST-2   | 2.2 kWh           | \n| Medium         | BERT-Base | SST-2   | 5.55 kWh          | \n| Large          | BERT-Base | SST-2   | 16.2 kWh          | \n| Xlarge         | BERT-Base | SST-2   | 27.6 kWh          |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1488/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700198462533,
                "cdate": 1700198462533,
                "tmdate": 1700198462533,
                "mdate": 1700198462533,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XSdbYtTm6G",
                "forum": "JLulsRraDc",
                "replyto": "MkwpOPhPoa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1488/Reviewer_erSL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1488/Reviewer_erSL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. \n\nWith response to the peak energy usage: I was thinking that 0.5kWh (e..g, 500watt/h) for the Xsmall case might be restrictive for mobile end-user devices (most mobile devices have batteries of just 5000mah --> 20wh --> 0.02kwh). Can you clarify on this number ? Is this summed over 100 clients ?"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1488/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229441473,
                "cdate": 1700229441473,
                "tmdate": 1700229441473,
                "mdate": 1700229441473,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]