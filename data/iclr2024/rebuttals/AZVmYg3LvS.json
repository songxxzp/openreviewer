[
    {
        "title": "Improved Function Space Variational Inference with Informative Priors"
    },
    {
        "review": {
            "id": "VDRp6Fpk4Z",
            "forum": "AZVmYg3LvS",
            "replyto": "AZVmYg3LvS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6987/Reviewer_QnS6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6987/Reviewer_QnS6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new function space variational inference for classification problem. The existing informative function prior may lead to high entropy to both in and out distribution data points. The idea is to assign lower entropy to in-distribution data and higher entropy to out-of-distribution data. The authors have designed a specific function prior and variational posterior to control the entropies. The experiments show somewhat promising results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well presented with good visualization to motivate the targeted problem. The designed prior and variational distribution are interesting and reasonable."
                },
                "weaknesses": {
                    "value": "1.\tWhy was the last layer set as BNN layer? Why not set the whole network as BNN? \n2.\tThe experimental results are only marginal, which is my main concern. Is that because the only last layer is BNN? Why not try more BNN layers that can demonstrate more difference between new prior with previous informative prior? \n3.\tSince the prior is changing during the training, how to ensure the convergence of the procedure? \n4.\tSince the method is specially designed for the classification task, I suggest the author to revise the title and introduction accordingly to highlight the classification task. \n5.\tWhat is the role of (12)? \n6.\tSome symbols are not defined, like N^q"
                },
                "questions": {
                    "value": "Please see the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6987/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698559097703,
            "cdate": 1698559097703,
            "tmdate": 1699636817541,
            "mdate": 1699636817541,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nwYbqzJK8z",
                "forum": "AZVmYg3LvS",
                "replyto": "VDRp6Fpk4Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6987/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6987/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer QnS6"
                    },
                    "comment": {
                        "value": "### Q1. Why was the last layer set as BNN layer? Why not set the whole network as BNN?\n\n> We request you to confirm our response regarding the **Reasons for taking the Last-layer BNNs** in the general review.\n\n### Q2. The experimental results are only marginal. Is that because the only last layer is BNN? Why not try more BNN layers that can demonstrate more difference between new prior with previous informative prior? \n\n> We request you to confirm our response regarding the **Marginal Performance improvement, possibly due to the Last-layer BNN** in the general review.\n\n\n### Q3. Since the prior is changing during the training, how to ensure the convergence of the procedure?\n\n> Our function-space prior does not vary during training. Please refer to our response in the **Procedure of the proposed algorithm** above and Algorithm 1 on page 12 for details.\n\n\n### Q4. Since the method is specially designed for the classification task, I suggest the author revise the title and introduction accordingly to highlight the classification task.\n\n> We have extended our method for the regression task and included regression results. Please see our response in the **Extension for regression tasks**. We will reflect this in the final manuscript.\n\n\n\n\n\n### Q5. What is the role of Eq. (12)?\n\n> The function-space prior requires auxiliary inputs, as discussed in our response regarding the **distinct property of the function-space prior as an input-dependent prior**. However, selecting additional datasets for the auxiliary inputs could be cumbersome if those datasets are not readily available. Therefore, for each training input $x_i$, we consider the $h(\\cdot)$ from Eq. (12) to be used for constructing the function-space prior. It's important to note that the $h(\\cdot)$ from Eq. (12) using $x_i$ is different from $h(x_i)$.  Therefor, this results in a function-space prior derived from $h(\\cdot)$ of Eq. (12), which is distinct from the one derived from $h(x_i)$.\n\n\n\n### Q6. Some symbols are not defined, like N^q\n\n> The $N^q$ represents the number of the labels corresponding to $q$ for the training set, i.e., $N^q= \\vert \\ \\\\{y_n \\ | \\, y_n=q, y_n \\in \\mathcal{D}\\\\} \\ \\vert$, for the training set $\\mathcal{D}=\\\\{(x_n, y_n)\\\\}_{n=1}^{N}$ and $y_n \\in \\\\{1,..,Q\\\\}$. For other errors, we will correct them in the final manuscript."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410430646,
                "cdate": 1700410430646,
                "tmdate": 1700445255945,
                "mdate": 1700445255945,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3ZPyRWdu02",
                "forum": "AZVmYg3LvS",
                "replyto": "nwYbqzJK8z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6987/Reviewer_QnS6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6987/Reviewer_QnS6"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' responses! \n\nI understand the difficulty of adopting full BNNs for large networks, but the authors could at least try 3 to 10 BNN layers rather than one. The results in the explanation of the marginal improvement are still marginal to me. The extension to regression is weird to me. In order to use the proposed method, the authors have to discretize the continuous outputs. I can understand why the entropies work for classification tasks, but I do not understand why there are classification entropy properties in continuous outputs. Hence, I am going to keep my socre."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650838323,
                "cdate": 1700650838323,
                "tmdate": 1700650838323,
                "mdate": 1700650838323,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "M0R9KAUSqx",
            "forum": "AZVmYg3LvS",
            "replyto": "AZVmYg3LvS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6987/Reviewer_hyeW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6987/Reviewer_hyeW"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of inferring posteriors of Bayesian neural nets with function-space priors more effectively than the existing variational inference approach that uses the first-order Taylor approximation."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The studied transfer learning setup with Vision Transformers is interesting and fits well to the purpose of doing Bayesian inference.\n\n * The paper presents results from a comprehensive set of experiments."
                },
                "weaknesses": {
                    "value": "* The problem setup does not make much sense to me. The introduction says:   **\u201cWe build an informative function space prior by using the empirical Bayes approach along with the parameters of hidden values and the last layer weight parameters which are obtained iterations during early stages of training\u201d**. I wonder how the approach is then different from having an uninformative prior after all. If the information comes from the data, it is technically not a prior. It appears that the paper makes its main point by differentiating from cases where the priors are just so strong that they unnecessarily restrict the model capacity.\n\n * The paper significantly lacks clarity. Apart from having extremely many typographical errors, it has statements without sharp enough meanings. Among the many, one example is: **\u201cDenote auxiliary inputs, which are far from training points and are placed closely with the training sets, respectively\u201d**. I have no idea what it means for a training point to be close to a training set. Likewise: **\u201ch(.) from the q-th component empirical parameters of hidden feature \u2026\u201d** What is an empirical parameter? I also have no clue about what is going on in pages 5 and 6 after spending considerable time trying to read them. Even the purpose of all these complications such as introducing adversarial hidden features do not look to me justified.\n\n * It is a clear weakness that after motivating Bayesian inference with lots of effort, the paper ends up using it only on the penultimate layer. Those layers are typically linear, where even closed-form Bayesian linear regression would work and the learned model weights would give a degree of interpretability. Why should one use function-space Bayesian inference if the prior will come from the weights learned in another data set and only the penultimate layer will be Bayesianized?\n\n * I do not think the reported results demonstrate the benefit of the proposed approach clearly enough. All models in all experiments perform very closely to each other. The results reported in Figure 2 are mixed: (a) and \u00a9 are favorable for the central message of the paper while (b) and (d) are just the opposite.\n\n* The take-home message given in the last sentence of Section 5.1 is obvious and comes from the nature of using an arbitrary regularizer. I wonder why one needs even an experiment for that.\n\n--- POST REBUTTAL ---\n\nThe author response does not give any concrete answer to any of the issues I raised above. I keep my score unchanged."
                },
                "questions": {
                    "value": "Only T-FVI or all the three baselines? If first, why not others?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6987/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6987/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6987/Reviewer_hyeW"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6987/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698664384493,
            "cdate": 1698664384493,
            "tmdate": 1700757367120,
            "mdate": 1700757367120,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n2t6Ov96RQ",
                "forum": "AZVmYg3LvS",
                "replyto": "M0R9KAUSqx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6987/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6987/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer hyeW"
                    },
                    "comment": {
                        "value": "### Q1. I wonder how the approach is then different from having an uninformative prior after all. If the information comes from the data, it is technically not a prior.\n\n> The proposed function-space prior is adaptively constructed based on auxiliary inputs, as described in **The Distinct Property of the Function-Space Prior as an Input-Dependent Prior**. We devise the function-space prior in such a way that if the auxiliary inputs appear more similar to the in-distribution dataset, the proposed function-space prior would exhibit higher a peaked probability due to its construction, as outlined in the initial phase of the **Procedure of the proposed algorithm**. The regularization of this prior would adaptively influence the model parameters depending on the auxiliary input (for example, $A$: the subset of CIFAR 100 sharing the label of CIFAR 10, and $B$: the remaining datasets of CIFAR 100). This is definitely distinct to the uninformative prior applying the same regularization to both $A$ and $B$.\n\n> Regarding the comment about the prior using the dataset, we partially agree with it because the empirical Bayesian method [1] sets the parameters of the prior distribution by training on the datasets. We adopt a similar approach to establish the function-space prior.\n\n> [1] https://en.wikipedia.org/wiki/Empirical_Bayes_method\n\n\n###  Q2. \u201cDenote auxiliary input, I have no idea what it means for a training point to be close to a training set. Likewise: \u201ch(.) from the q-th component empirical parameters of hidden features \u2026\u201d What is an empirical parameter?\n\n> In light of the feedback from other reviewers, we acknowledge that our manuscript seems difficult to understand. We kindly request you to confirm the responses from all reviewers above. Then, we recommend revisiting sections 2, 3, and 4 for a clearer understanding. Also, we will enhance the clarity of our manuscript for easier understanding.\n\n\n### Q3. When applying the last-layer, the corresponding model would be Bayesian linear regression with interpretable parameters. Why should one use function-space Bayesian inference if the prior will come from the weights learned in another data set and only the penultimate layer will be Bayesianized?\n\n> Although our proposed approach models only the last layer to be the Bayesian layer, the regularization via the function-space KL divergence of Eq. (6) has an impact on the overall parameter learning, including the weight parameters of the $L-1$ deterministic layers $\\\\{f^{l} \\\\}_{l=1}^{L-1}$. \n\n> Note that from a Bayesian perspective, the weight parameters of  L-1 deterministic layers $\\\\{f^{l}\\\\}_{l=1}^{L-1}$ can be also understood as the mean parameters of the variational distribution for the BNN having a fixed variance (not trainable) with a very small value.\n\n\n\n### Q4. The results reported in Figure 2 are mixed.  Figure 2.(a) and (C) are favorable for the central message of the paper while Figure 2.(b) and (d) are just the opposite.\n\n> We would like to emphasize that the primary message from Figure 2 is that the proposed method results in more reliable uncertainty estimation for both the in-training set (lower NLL) and out-of-training test set (higher AUROC) across various regularization parameters \u03bb. Therefore, it demonstrates that the proposed method effectively mitigates the trade-off issue associated with the existing input-independent function-space prior.\n\n\n### Q5. The take-home message given in the last sentence of Section 5.1 is obvious and comes from the nature of using an arbitrary regularizer. I wonder why one even needs an experiment for that.\n\n> We agree that controlling the regularization hyperparameter $\\lambda$ can alleviate the trade-off issue for the input-independent prior. However, we propose a more effective solution by incorporating the input-dependent function-space prior. Note that the NLL and AUROC of the proposed method (R-FVI) consistently outperform the baseline (T-FVI) for all varying regularization hyperparameters in Figure 2 (a) and (b).\n\n\n### Q6. Only T-FVI or all the three baselines? If first, why not others?\n\n> Since we aim at tackling the issue of input-independent function-space prior for T-FVI in this work, we set the T-FVI as our main baseline."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410234865,
                "cdate": 1700410234865,
                "tmdate": 1700411507523,
                "mdate": 1700411507523,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8SBY2kVHe1",
            "forum": "AZVmYg3LvS",
            "replyto": "AZVmYg3LvS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6987/Reviewer_3j4p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6987/Reviewer_3j4p"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates a new approach to specify informative priors for improve variational inference in function space on classification tasks. In particular, the paper relies on the functional variational inference (f-VI) framework proposed by Rudner et al. (2022) but replaces the uniform (uninformative) functional prior with an informative functional prior. To this end, the authors reconsider the role of functional prior in the perspective of Bayesian Model Averaging (BMA), and then propose a new functional prior relying on the empirical Bayes approach (using the training data to specify the prior). This prior is aimed at avoiding common pathology of the uniform functional prior, which encourages the model to be uncertain on both the training and out-of-training data. In addition, the authors propose a new functional variational distribution that is aligned with this new functional prior.  The proposed method is validated on toy data and popular benchmarks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper aims at tackling an important problem for Bayesian deep learning which is designing a good prior for Bayesian neural networks.\n- The code is anonymously provided. However, there are no instructions to use the code. Thus, it is difficult to verify the provided code.\n- The idea is interesting and well-motivated which is to design a new prior promoting high uncertainty for out-of-distribution data but low uncertainty for in-distribution data."
                },
                "weaknesses": {
                    "value": "- The writing should be improved. There are many grammar typos such as the use of \u201ca\u201d and \u201can\u201d. Some parts of the paper are difficult to read, especially Section 4. There are some confusions of notations. Please refer these to in the box of Questions.\n- In Section 4, although the authors motivated the paper from the view of Bayesian Model Averaging, and claimed that *\u201cwe may design a function space prior that does not explicitly encourage generating high-entropy predictions for each predictive probability but the average prediction (via BMA) would still have high-entropy when encountered with an OOD input\u201d*, it is not clear how the proposed prior can achieve this.\n- The proposed method is somehow ad-hoc without well-elaborations. For example, in Eq (9), why do the empirical mean and covariance are averaged from those obtained from all pre-training iterations? How did the authors come up with the equation (13), the parameter $\\hat{p}(\\cdot)$ for the variational distribution?\n- The authors ignored a very related work from Izmailov et al. (2021). The narrative of this work also relies on Bayesian model averaging. This work also considers designing a novel prior that is robust to out-of-distribution data by using the empirical Bayes approach. The authors should cite, discuss, and compare experimentally with this work.\n- Experimental results on image benchmarks (Sec 5.2) show that the performance gain from the proposed prior (R-FVI) is very marginal compared to the uniform prior (T-FVI). To show clearly the effect of the prior, the authors should ablate different training sizes and temperature values for the posterior on these benchmarks.\n\nReferences:\n\nIzmailov et al. Dangers of Bayesian Model Averaging under Covariate Shift. NeurIPS 2021."
                },
                "questions": {
                    "value": "- In Equation (8), what is $N^q$ how to define it? Do we have to compute the means and shared covariance over the dataset?\n- In Equation (9): what is $T$? It should be consistent with the sentence before the Equation (8).\n- From Figures 3c and 3f, it seems that the proposed method always induces a much higher disagreement ratio on both in- and out-of-distribution data compared to the uniform prior. Is this good? On in-distribution data, the entropy should be low.\n- In the paragraph \"Context inputs from adversarial hidden features\". How do we define $r$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6987/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6987/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6987/Reviewer_3j4p"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6987/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698695666402,
            "cdate": 1698695666402,
            "tmdate": 1699636817295,
            "mdate": 1699636817295,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m6hR4ko64c",
                "forum": "AZVmYg3LvS",
                "replyto": "8SBY2kVHe1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6987/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6987/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 3j4p"
                    },
                    "comment": {
                        "value": "### Q1. Regarding our claim that the proposed function-space prior allows the average prediction (via BMA) to have high-entropy for an OOD input, it is not clear  how the proposed function-space prior achieves this.\n\n> We request you to confirm our response regarding the **How the proposed methods achieve the BMA property using the inductive bias** in the general review.\n\n\n### Q2.  Our method is somehow ad-hoc without well-elaboration for Eq (9) and Eq (13).\n\n> We request you to confirm our response regarding the **Elaboration of the proposed algorithm** in the general review.\n\n\n### Q3. The authors ignored a very related work from Izmailov et al. (2021) relying on Bayesian model averaging and a novel prior. \n\n> Based on our understanding, in the referenced study [1], the authors addressed the issue of poor uncertainty estimation in BNNs on the covariate shift dataset when the BNNs, using a zero-mean Gaussian prior distribution, are trained using SGHMC (oracle Bayesian inference yielding an almost true posterior). To address this limitation, they identified shortcomings in the posterior distribution arising from the zero-mean Gaussian prior and proposed a novel empirical covariance for the prior distribution of weight parameters.\n\n>  We acknowledge some similarities with [1], where both approaches use training datasets to construct the prior distribution for BNNs with the aim of improving uncertainty estimation. However, our work specifically focuses on resolving the trade-off issue caused by the input-independent function-space prior. Therefore, we refine the function-space prior and variational distribution to achieve reliable uncertainty estimation for both the in-training distribution and out-of-distribution dataset. This objective distinguishes our work from [1], which primarily concentrated on addressing the covariate shift dataset. We will elaborate on this discussion in the main manuscript.\n\n>  [1] Dangers of Bayesian Model Averaging under Covariate Shift - NeurIPS 21\n\n\n### Q4. Regarding the marginal performance gain in Section 5.2. \n\n> We request you to confirm our response in the **Marginal Performance improvement, possibly due to the Last-layer BNN**\n\n\n\n\n### Q5. What is $N^q$, how to define it?, \n\n> The $N^q$ represents the number of the labels corresponding to $q$ for the training set, i.e., $N^q= \\vert \\ \\\\{y_n \\ | \\ y_n=q, y_n \\in \\mathcal{D} \\\\} \\ \\vert$, for the training set $\\mathcal{D} =\\\\{(x_n, y_n)\\\\}_{n=1}^{N}$ and $y_n \\in \\\\{1,..,Q\\\\}$.\n\n### Q6. Do we have to compute the means and shared covariance over the dataset for Eq. (8)?\n\n> We can utilize a subset of the full training set for computing the mean and shared covariance. However, this introduces another consideration\u2014specifically, how to choose the subset of the training set. If the subset is selected in a way that introduces bias, it can give rise to additional issues for the function-space prior and the latent variable $z$ in the variational distribution. Consequently, we opt for using the entire training set. Since the means and shared covariances are updated in a batch manner, there is no need for additional iterations to compute the mean and shared covariance.\n\n\n\n### Q7. what is $T$ ?,  It should be consistent with the sentence before the Equation (8).\n\n> The $T$ represents the number of pre-determined iterations. For example, if $\\mathcal{T}=\\\\{145, 150, 155, 160 \\\\}$, then $T=4$. We apologize for the error in this notation. We will clarify it in the final manuscript."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409442539,
                "cdate": 1700409442539,
                "tmdate": 1700418252597,
                "mdate": 1700418252597,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rLpRMAKYMr",
                "forum": "AZVmYg3LvS",
                "replyto": "gS59oEqDAJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6987/Reviewer_3j4p"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6987/Reviewer_3j4p"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer 3j4p"
                    },
                    "comment": {
                        "value": "Thanks the Authors for the rebuttal. However, the rebuttal did not address satisfactorily all my concerns. I believe that the manuscript should be improved significantly and needs another round of review. Therefore, I would keep the current score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639199206,
                "cdate": 1700639199206,
                "tmdate": 1700639199206,
                "mdate": 1700639199206,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nxMcKnXsAG",
            "forum": "AZVmYg3LvS",
            "replyto": "AZVmYg3LvS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6987/Reviewer_1KC1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6987/Reviewer_1KC1"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on improving function-space Bayesian Neural Networks (BNNs) by addressing some of the key challenges they face in terms of dealing with significative prior distributions. Applied in a classification setting, the authors propose an informative function space prior that encourages sample functions to have a certain predictive probability and varying degrees of disagreements based on input status. They also tackle the issue of computing KL divergences in function space by using an adversarial hidden feature and refining the variational function space distribution. Experimental results show that their approach outperforms other inference methods on the CIFAR 100 dataset, demonstrating its effectiveness for large-scale models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The topic is quite interesting from the point of view of enlarging the contributions to the function-space approach to modern probabilistic machine learning. The topic of using function-space BNNs is relevant and promising, and further research such as this is very welcome.\n* The proposal for function-space variational distribution introduces a categorical latent variable that represents the uncertainty in using a specific feature based on its empirical distribution. This allows for better understanding and interpretation of the model's behavior.\n* The authors use multi-dimensional probit approximation (MPA) to obtain an approximate marginalization over a Gaussian distribution for obtaining $\\hat{p}(\\cdot)$. This technique helps to efficiently compute and approximate complex distributions, making it feasible to implement this approach."
                },
                "weaknesses": {
                    "value": "* The article writing does not contribute to the overall appreciation of the work being done and should be thoroughly revised. I strongly encourage the authors to do an integral check on the text for improvements. This is quite noticeable, even the abstract should be revised to correct typos and improve the overall text flow and comprehension. A lot more care and effort have to be put in this regard.\n* The proposed method relies on a last-layer approximation, which is not thoroughly discussed enough. While this approach can be employed with the right arguments, the authors do not make the efforts necessary to justify this choice or the consequences it may entail in the proposed technique.\n* While the paper presents an improved function-space variational inference method, it does not provide extensive evaluation results or comparisons against other existing methods on benchmark datasets or real-world applications to demonstrate its superiority over alternative approaches. I think stronger experimental work is needed to further motivate the usage of the proposed approach. The contribution is itself interesting, but further experimental results would bolster the proposal (e.g. regression experiments, applying this method to specify the prior in other function-space inference methods to check the potential improvements, etc.).\n* The proposal made in the article is strictly limited to BNNs, while other methods such as the one in [1] or Rodr\u00edguez-Santana et al. (2022) can be seen as \"generalist approaches\" where the function-space formulation can be done for many other models (not just BNNs, which are only particular cases).\n\n*Note:* I condition my review score on the fact that some of these issues get fixed in the final draft version. Otherwise, I may be inclined to lower the score. \n\n\n(see \"**Questions**\" for the references)"
                },
                "questions": {
                    "value": "* In the initial paragraph of the introduction, when mentioning function-space BNNs I would include the reference [1].\n* Why would you argue that the main goal of function-space BNNs is \"directly assigning prior distributions to the outputs of neural networks\"? I would argue this can be done without function-space formulation, and that the main interest lies directly on the properties of the function space itself. I would even argue that the approach does not necessarily become more \"user-friendly\" due to the difficulties intrinsic to function space. I would appreciate more insight on these points.\n* Given the BMA approach and the nature of the contribution in Rodr\u00edguez-Santana et al. (2022), how do these relate to each other? I think further discussion here could improve to make a more comprehensive overall picture.\n* Does the last-layer approximation play a role in the final performance metrics? What results are achieved if this restriction is not applied and instead a full Bayesian NN is used?\n\n---\n### **Notes:**\n\n* As the authors mention: \"(Flam-Shepherd et al., 2017; Karaletsos & Bui, 2020; Tran et al., 2022), it has been less clear to specify the interpretable function space prior for the classification task\" I would expect this contribution to try to either expand on this formulation or present a general contribution for classification problems (although one also could say that works such as Tran et al. 2022 could serve to that purpose). The formulation of the article up to Section 3 makes the reader think the authors are presenting a general approach both for regression and classification, while in reality they only do the latter. Thus, I would encourage the authors to be clear with these intentions from the beginning. Moreover, since the conversion to regression does not seem too far off from the method present, I strongly \n* Results for the presented method in table 2 are highlighted, while there are other methods in ECE and AUROC that are competitive (with equal performance). This should be corrected.\n\n### **Minor corrections:**\n\n* Please correct typos. Just in the abstract there are some of them, such as the capitalized \"Recent\", the \"the this function space\" sentence, \"thought the uniform function space...\" should be revised. Further examples can be found all through the text, such as \"lineariztion\" or \"Jacobin matrix\".\n* Maintain consistency, e.g. if you are using \"function-space\" on the title I would expect to keep the \"-\" throughout the text. On the same line, remove the red-colored subindex in page 3 (unless you use justify its usage further). Also, there are some inconsistencies also in singular and plural expressions, such as \"since the posterior distribution of the weight parameters p(\u0398|D) are not tractable in general\". Please, correct the text carefully.\n* Reference for Rodr\u00edguez-Santana et al. (2022) is missing the \\' in \"\u00ed\" \n\n### **References:**\n[1] Ma, C., Li, Y., and Hern\u00e1ndez-Lobato, J. M. (2019). \u201cVariational implicit processes\u201d. In: International Conference on Machine Learning, pp. 4222\u20134233."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6987/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699461116831,
            "cdate": 1699461116831,
            "tmdate": 1699636817172,
            "mdate": 1699636817172,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DonOx453WO",
                "forum": "AZVmYg3LvS",
                "replyto": "nxMcKnXsAG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6987/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6987/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to  Reviewer 1KC1"
                    },
                    "comment": {
                        "value": "### Q1.  Authors do not make the efforts necessary to justify this last-layer approximation. \n\n> We explain why the last-layer approximation was considered in **Reasons for taking the Last-layer BNN** in the responses to all reviewers. We will also describe this in our final manuscript as well.\n \n### Q2. Does the last-layer approximation play a role in the final performance metrics? What results are achieved if this restriction is not applied and instead a full Bayesian NN is used ?\n\n> We request you to confirm our response in the **Marginal Performance improvement, possibly due to the Last-layer BNN**  in the responses to all reviewers above.\n\n\n\n### Q3. Comment about insufficient extensive evaluation results (evaluation on regression task) in **Weakness** \n\n> We request you to confirm our response in the **Response to all reviewers for \"\"Extension for regression tasks\"\"** above.\n\n\n### Q4. Why would you argue that the main goal of function-space BNNs is \"directly assigning prior distributions to the outputs of neural networks? \n\n> When considering one of the Bayesian perspectives, which involves incorporating prior knowledge into the model, we believe that providing a more directly interpretable function-space prior for BNNs is a desirable direction for practitioners. We acknowledge that this perspective can be subjective and thus would like to moderate our claim regarding the main goal. Additionally, we agree with your assertion that addressing the characteristics of the function space and overcoming the challenges of function-space inference are indeed meaningful directions for improving function-space inference.\n\n>However, if we focus solely on the characteristics of the function space and technical research, we might overlook the advantages of the Bayesian approach\u2014especially its ability to enhance performance with limited data by leveraging prior knowledge. Therefore, we believe that research on BNNs that can incorporate intended prior knowledge is crucial. Exploring ways to make the function-space prior more user-friendly is the key direction in this context.\n\n\n\n### Q5. Connection to generalistic approach; Given the BMA approach and the nature of the contribution in Rodr\u00edguez-Santana et al. (2022), how do these relate to each other?\n\n> Based on our understanding, Sparse Implicit Process (SIP) [1] primarily contributes to improving the scalability and flexibility of the Implicit Process (IP), known for constructing an implicit function-space distribution to model data. SIP achieves this improvement by incorporating the inducing input idea and adversarial Variational Inference. Our research, on the other hand, focuses on BNNs, considered as an example of IP. We aim to enhance function-space Variational Inference for BNNs, inspired by the ideal role of function-space predictive sample functions.\n\n> Regarding the connection to these works, we believe that these studies offer interesting insights into the construction of a function-space prior. SIP forwards Gaussian noise and data through a neural network to obtain random functions. It then uses the empirical distribution (averaged mean and covariance of the random functions) to form and learn the implicit prior distribution on function space. In contrast, our study utilizes the paths of weight parameters during training to construct an input-dependent function-space prior with different predictive entropy for in-distribution and out-of-distribution datasets. This difference seems to originate from the fact that, while SIP focuses more on obtaining the implicit function-space prior for flexible modeling, our work concentrates more on the adaptive function-space prior for Bayesian Model Averaging (BMA) prediction. This represents our perspective on the connection between our work and SIP. If there are different opinions, feel free to share them. It could contribute to a more comprehensive discussion of the papers.\n\n> [1] Function-space Inference with Sparse Implicit Processes - ICML 22"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408491608,
                "cdate": 1700408491608,
                "tmdate": 1700408658390,
                "mdate": 1700408658390,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eK4uEz91YX",
                "forum": "AZVmYg3LvS",
                "replyto": "Op2fFKEduq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6987/Reviewer_1KC1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6987/Reviewer_1KC1"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the authors"
                    },
                    "comment": {
                        "value": "First of all, I wanted to thank the authors for the very detailed response and the amount of extra work put forth in the rebuttal process. I think the submitted work strongly benefits from these extra points and I deem the submission to be much more complete with the information available currently. I consider this updated version of the work a very different paper with respect to the original submission (and certainly, an improved version of it).\n\nThe authors have addressed many of my questions and concerns in their comments, and I appreciate the clarifying statements for these points. On the other hand, I wish the authors conducted a more thorough experimental phase comparing to those other methods I mentioned in the review, namely, VIP and SIP, to better showcase the properties of the proposed method. In particular, the current method should beat generalist approaches in tasks related to BNNs, and that should also strengthen the experimental part of the submission and would provide further heuristic support for the approach. This same comment applies both to the classification and the regression tasks, on which the authors have much to gain from simple extensions of current work.\n\nIn general, I am now more inclined than before to a positive review of the submitted work. However, given the interesting ideas and the amount of work already put forth in the submission, I consider the authors could provide a much-improved version with some very simple extensions and a bit of re-writing of the original text. Therefore, even though a bit conflicted by these facts, I  am inclined to maintain my original score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675318570,
                "cdate": 1700675318570,
                "tmdate": 1700675318570,
                "mdate": 1700675318570,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]