[
    {
        "title": "Pi-DUAL: Using privileged information to distinguish clean from noisy labels"
    },
    {
        "review": {
            "id": "s0OISmAZQy",
            "forum": "8oYjW8QxuC",
            "replyto": "8oYjW8QxuC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5831/Reviewer_h1hP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5831/Reviewer_h1hP"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a dual structure for learning with noisy labels by separating the training process into regular feature learning and privileged information learning. The regular feature learning module is responsible for the final inference. The effectiveness of the algorithm was validated on three datasets: CIFAR=1-H, CIFAR-N, and ImageNet-PI."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The structure of this paper is clear, making it easy for readers to follow.\n2. It's intriguing to note that the no-PI network of the Dual structure outperforms previous PI-related works.\n3. The results presented in the paper attest to the efficacy of the proposed method."
                },
                "weaknesses": {
                    "value": "1. While PI is a concept introduced in prior works, this article doesn't offer significant innovations to it. The paper points out that PI-based methods underperform compared to no-PI-based methods. However, it fails to delve deep into the underlying principles causing this discrepancy. The conclusions seem to be drawn mainly from some experimental verifications rather than in-depth analysis.\n\n2. The experimental comparisons are not exhaustive. Several state-of-the-art methods mentioned in Table 1, such as dividemix, weren't comparatively analyzed in the experiments. Given that Pi-Dual incorporates additional information to tackle label noise, comparing it with the current best methods is crucial to gauge the algorithm's effectiveness.\n\n3. Certain ablation studies were not conducted, like the choice of the model backbone and parameters of the additional PI-related modules."
                },
                "questions": {
                    "value": "Please refer to the Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5831/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827560154,
            "cdate": 1698827560154,
            "tmdate": 1699636615531,
            "mdate": 1699636615531,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BRDGPd8MQN",
                "forum": "8oYjW8QxuC",
                "replyto": "s0OISmAZQy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5831/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5831/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h1hP (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to review our paper and for all the valuable feedback. We address their comments below:\n\n**Innovations to PI**\n\nWe respectfully disagree with the reviewer in that the innovations of Pi-DUAL are minor. As appreciated by the other reviewers, the goal of our paper is to build a simple and effective method that requires the minimal changes to existing training pipelines. In this regard, while Pi-DUAL is not the first in utilizing PI features to combat label noise, Pi-DUAL provides a solution that is both simpler and more efficient. Compared to TRAM, it has the following key distinctions:\n\n1. **Network structure**: TRAM uses one model but with two heads, one for training and one for testing. In contrast,  Pi-DUAL uses three distinct subnetworks with clear roles during training and inference: the prediction network for fitting clean target distribution, the noise network for explaining away label noise, and the gate network for identifying noisy samples. These three roles can not be accounted for in the architecture of TRAM.\n2. **Learning objective**: While the learning objective of TRAM is a weighted sum of two objectives (one for fitting the noisy target distribution, and another for transferring the knowledge to the no-PI head), the learning objective of Pi-DUAL is a simple cross-entropy loss over the noisy targets. For a better illustration of the differences between Pi-DUAL and TRAM, we conducted an ablation which trains Pi-DUAL with the loss function of TRAM, and show that this produces much worse results. This experiment has been added to Section C.8 in Appendix.   \n\n| Dataset \\ Method |    CE    |  Pi-DUAL | Pi-DUAL with TRAM loss |\n|:----------------:|:--------:|:--------:|:----------------------:|\n|     CIFAR-10H    | 51.1\u00b12.2 | 71.3\u00b13.3 |        61.6\u00b14.8        |\n|     CIFAR-10N    | 80.6\u00b10.2 | 84.9\u00b10.4 |        81.6\u00b10.9        |\n|    CIFAR-100N    | 60.4\u00b10.5 | 64.2\u00b10.3 |        59.0\u00b10.6        |\n\n3. **Modeling of label noise**. TRAM does not explicitly model label noise. Instead, it learns the conditional distribution of noisy labels with respect to both x and a, to eventually predict with an implicit marginalization over a. On the other hand, Pi-DUAL explicitly models per-instance-level label noise in its architecture, which permits it to identify noisy labels and to effectively reduce the negative impacts from noisy labels. In contrast to TRAM, which performs approximate marginalization over the PI,  the prediction network of Pi-DUAL learns the conditional distribution of clean labels with respect to only x. With this explicit modeling of label noise, Pi-DUAL delivers not only better performance, but also a much better interpretability as to how PI helps explain away label noise (e.g., how the noise network overfits to the wrong labels in Figure 2, and how the gate network identifies wrong labels in Figure 3).\n\nWith these non-trivial improvements, Pi-DUAL outperforms TRAM++ on all 5 benchmarks achieving up to 8.1 accuracy points more than TRAM++ on ImageNet-PI. Pi-DUAL also delivers much better performance in identifying label noise post-training due to the explicit modeling of label noise."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216857777,
                "cdate": 1700216857777,
                "tmdate": 1700216857777,
                "mdate": 1700216857777,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i8uA913mvp",
                "forum": "8oYjW8QxuC",
                "replyto": "s0OISmAZQy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5831/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5831/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h1hP (Part 2)"
                    },
                    "comment": {
                        "value": "**Reasons why PI-methods underperform No-PI methods**\n\nWe appreciate the comment of the reviewer, but we note that the overall goal of our work is to provide a better approach to utilize PI features, and not a deep analysis of prior work. In the paper, we have provided ample empirical and theoretical evidence illustrating the behavior of Pi-DUAL and we have tried to be as thorough as possible in our ablations. Pi-DUAL closes the gap between PI and no-PI methods even in datasets with low-quality PI features, while exhibiting much better scalability to no-PI methods. \n\nHowever, to answer the reviewer, we would like to emphasize that one of our main findings is that the relative performance between PI and No-PI methods depends heavily on the quality of PI features. For example, CIFAR-10H has fine-grained PI features, and the performance of TRAM++ outperforms the No-PI methods by 15.5 points. On the other hand, the PI feature quality for CIFAR-10N and CIFAR-100N are much worse as they are averaged over a batch of samples, which significantly reduces the amount of information on each sample carried by PI. \n\nTo quantitatively illustrate the importance of PI quality in the performance of PI methods, we provide a new experiment where we randomly corrupt a percentage of PI features in the train set and train Pi-DUAL with the corrupted PI features (details in Appendix C.9). We copy the results here for reference: \n\n| PI Corruption | No corrupt | 25% corrupt | 50% corrupt | 75% corrupt | 100% corrupt |\n|:-------------:|:----------:|:-----------:|:-----------:|:-----------:|:------------:|\n|    Accuracy   |  71.3\u00b13.3  |   66.4\u00b12.7  |   63.9\u00b10.7  |   52.6\u00b14.0  |   47.2\u00b13.7   |\n\nAs we can see, as the quality of the PI gets worse, the performance of Pi-DUAL degrades significantly, highlighting the importance that good PI has on these methods.\n\n**Comparison to state-of-the-art**\n\nFor a fair comparison, we only included in Table 2 the methods which use only one model and that are trained for a single stage. We did not compare to methods like Divide-Mix, because it incorporates techniques from semi-supervised learning, which substantially increases the computational costs (e.g., it uses two networks, mixup augmentation, longer training epochs, etc.). It is worth noting that prior works in label noise literature also do not compare directly to DivideMix, but compare DivideMix to versions of their methods which use semi-supervised learning techniques. For example, the authors for ELR (Liu et al., 2020) compare DivideMix with ELR+, while the authors for SOP (Liu et al., 2022) compare DivideMix with SOP+ (rather than SOP). ELR+ and SOP+ are respectively more sophisticated versions of the original methods augmented with semi-supervised learning techniques, making the comparison more fair in terms of both costs and complexity.\n\nTherefore, in response to the reviewers request for a comparison to Divide-Mix and other semi-supervised methods, and for a full validation of our proposed method and a fair comparison, we follow the common practice, and propose an augmented version of Pi-DUAL, Pi-DUAL+, and compare it to other semi-supervised learning methods. Pi-DUAL+ adds to Pi-DUAL two regularization techniques, label smoothing and prediction consistency regularization from UDA (Xie et al., 2020), where the latter one was a common choice for augmenting label noise training methods (Cheng et al., 2020, Liu et al., 2022). We have added the details for implementation of Pi-DUAL+ in Appendix C.5, and copy the results for Pi-DUAL+ below for reference, comparing it with several prior semi-supervised pipelines for combating label noise:\n\n|                |  CIFAR-10H |  CIFAR-10N | CIFAR-100N |\n|----------------|:----------:|:----------:|:----------:|\n|       CE       | 51.10\u00b12.20 | 80.60\u00b10.20 | 60.40\u00b10.50 |\n|   Divide-Mix   | 71.68\u00b10.27 | 92.56\u00b10.42 | 71.13\u00b10.48 |\n|    PES(semi)   | 71.16\u00b11.78 | 92.68\u00b10.22 | 70.36\u00b10.33 |\n|      ELR+      | 54.46\u00b10.50 | 91.09\u00b11.60 | 66.72\u00b10.07 |\n|     CORES*     | 57.80\u00b10.57 | 91.66\u00b10.09 | 55.72\u00b10.42 |\n|      SOP+      | 66.02\u00b10.06 | 93.24\u00b10.21 | 67.81\u00b10.23 |\n| Ours: Pi-DUAL+ | 83.23\u00b10.26 | 93.31\u00b10.21 | 67.99\u00b10.08 |\n\nIn this table, the baseline results are obtained using the code of the original papers if they are not available in the [public leaderboard](http://www.noisylabels.com). It is worthwhile noting that, on CIFAR-10H, Pi-DUAL+ outperforms the rest of the methods by a margin of over 11%, while it performs on par with the state-of-the-art methods on CIFAR-10N and CIFAR-100N. Importantly, this is because Pi-DUAL+ benefits from the high quality PI features in CIFAR-10H, while for CIFAR-10N and CIFAR-100N, , the PI is of poor quality, which explains why Pi-DUAL+ does not have a performance boost that is as large as the one in the case for CIFAR-10H. We hope it can help in delivering a thorough comparison with the state-of-the-art."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216921953,
                "cdate": 1700216921953,
                "tmdate": 1700228782795,
                "mdate": 1700228782795,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qf4wynP9wZ",
                "forum": "8oYjW8QxuC",
                "replyto": "s0OISmAZQy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5831/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5831/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h1hP (Part 3)"
                    },
                    "comment": {
                        "value": "**Architectural ablation studies**\n\nWe thank the reviewer for bringing the missing ablation studies on the model architectures to our attention, and we hope the following additional ablations help complete our understanding of the method.\n\n> *Ablations for the model backbones*\n\nIn the paper we use a WideResNet-10-28 for all experiments on CIFAR. Here we provide the results when replacing the architecture with a ResNet-34 instead. ResNet-34 is a common choice for model backbones in CIFAR-level experiments in the noise label literature (Cheng et al., 2020, Liu et al., 2020, 2022). We have added this ablation in Appendix C. 6.1. The results using ResNet-34 comparing Pi-DUAL and CE baseline are reported in the following table:\n\n|                  | WideResNet-10-28 |          | ResNet-34 |          |\n|------------------|------------------|----------|:---------:|:--------:|\n| Dataset \\ Method |        CE        |  Pi-DUAL |     CE    |  Pi-DUAL |\n|     CIFAR-10H    |     51.1\u00b12.2     | 71.3\u00b13.3 |  51.4\u00b12.1 | 69.3\u00b13.1 |\n|     CIFAR-10N    |     80.6\u00b10.2     | 84.9\u00b10.4 |  80.7\u00b11.0 | 84.5\u00b10.4 |\n|    CIFAR-100N    |     60.4\u00b10.5     | 64.2\u00b10.3 |  56.9\u00b10.4 | 62.2\u00b10.3 |\n\nFrom the table, we observe that Pi-DUAL maintains its performance improvement over CE with a different model backbone, exceeding the performance of CE baseline by a notable margin in all three datasets.\n\n> *Ablations for PI network structure*\n\nFollowing the reviewer\u2019s advice, we have added ablations on the width and depth of PI-related modules in Appendix C. 6.2. \n\n1. *Ablations for the width for PI-related modules*: In the paper we set the width of the PI-related modules (for both noise network and gate network) by default to 1024 for CIFAR-10H, and 2048 for all other experiments, without tuning. Here we provide the results for different widths for the PI networks on three CIFAR datasets. \n| Dataset \\ Width |    512   |   1024   |   2048   |   4096   |\n|:---------------:|:--------:|:--------:|:--------:|:--------:|\n|    CIFAR-10H    | 72.8\u00b12.9 | 71.3\u00b13.3 | 71.4\u00b13.6 | 71.2\u00b13.8 |\n|    CIFAR-10N    | 83.8\u00b10.4 | 83.6\u00b10.7 | 84.9\u00b10.4 | 85.3\u00b10.2 |\n|    CIFAR-100N   | 62.3\u00b10.2 | 63.7\u00b10.4 | 64.2\u00b10.3 | 64.4\u00b10.2 |\n\nFrom the table we observe that the performance of Pi-DUAL benefits from larger network width for the PI-related modules\n\n2. *Ablations for the depth for PI-related modules*: In the paper we set the depth of the PI-related modules to 3 by default for all experiments, without tuning. Here we provide the results for different depths for the PI networks on three CIFAR datasets. Note that the width of the PI-related modules are set to the default value when tuning the depth.\n| Dataset \\ Depth |     2    |     3    |     4    | \n|:---------------:|:--------:|:--------:|:--------:|\n|    CIFAR-10H    | 68.9\u00b12.0 | 71.3\u00b13.3 | 70.1\u00b13.5 | \n|    CIFAR-10N    | 83.4\u00b10.4 | 84.9\u00b10.4 | 85.2\u00b10.3 | \n|    CIFAR-100N   | 59.4\u00b11.0 | 64.2\u00b10.3 | 64.1\u00b10.2 | \n\nFrom the table we observe that the depth of the PI-related modules have to be at least of 3 layers to maximize the performance of Pi-DUAL. We thank the reviewers for leading us to these interesting findings."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217152153,
                "cdate": 1700217152153,
                "tmdate": 1700223405796,
                "mdate": 1700223405796,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "phW2OkNy6h",
            "forum": "8oYjW8QxuC",
            "replyto": "8oYjW8QxuC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5831/Reviewer_VkrZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5831/Reviewer_VkrZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces PI-DUAL, a novel approach designed to address label noise by leveraging privileged information (PI). Privileged information is only accessible during the training phase. In essence, PI-DUAL employs neural networks to separately model the features of $x$ and the privileged information $a$, and it employs an additional network to determine the weights for the logits of $x$ and $a$. \n\nTo demonstrate the advantages of PI-DUAL, the authors conduct a series of comprehensive studies to investigate the training dynamics of PI-DUAL and to provide valuable theoretical insights. The proposed method is characterized by its simplicity, and experimental results exhibit promising performance when compared to other baseline methods.\""
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method is both simple and technically sound. Notably, it represents the first PI-based method to explicitly model label noise.\n\n- The studies conducted on PI-DUAL are thorough. The authors perform a range of experiments, including an exploration of training dynamics, an evaluation of detection performance, and an investigation of the impact of PI length. These experiments collectively contribute to a comprehensive assessment of the effectiveness of PI-DUAL.\n\n- While the theoretical insights are based on the linear layer scenario, their analyses appear to be reasonable."
                },
                "weaknesses": {
                    "value": "- The primary distinction between PI-DUAL and TRAM [R1] lies in PI-DUAL's approach of separately modeling features and privileged information (PI), as opposed to using two heads on top of the feature vector $x$. While this approach provides some technical novelty, it may be considered somewhat limited.\n\n- I find the experimental results to be somewhat perplexing. It is reasonable that PI-DUAL doesn't need to be compared to two-stage methods like DivideMix or other semi-supervised pipelines. However, the test accuracy presented in Table 2 exhibits a notable gap compared to the results reported in the original papers or public leaderboards [R2]. Additionally, the test accuracy for TRAM on CIFAR-10N, as presented in the original paper, was 71.8, but in Table 2, it's only 64.9. While I acknowledge that differences in training settings may account for this, it would be beneficial to conduct specific experiments to compare each method under their optimal configurations.\n\n- Typos:\n    - In Section 3.2: Change \"Here, $\\gamma_{\\phi}$ denotes\" to \"Here, $\\gamma_{\\psi}$ denotes.\"\n\n[R1] Transfer and Marginalize: Explaining Away Label Noise with Privileged Information\n\n[R2] You can access the relevant information at http://www.noisylabels.com."
                },
                "questions": {
                    "value": "See *Weaknesses above*"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5831/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698857108002,
            "cdate": 1698857108002,
            "tmdate": 1699636615428,
            "mdate": 1699636615428,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TCIINwNBW6",
                "forum": "8oYjW8QxuC",
                "replyto": "phW2OkNy6h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5831/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5831/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VkrZ (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to review our paper and for all the valuable feedback. We address their comments below:\n\n**Differences between Pi-DUAL and TRAM**\n\nWe respectfully disagree with the reviewer in that the innovations of Pi-DUAL with respect to TRAM are minor and correspond only to a change in architecture. As appreciated by this reviewer, the goal of our paper is to build a simple and effective method that requires minimal changes to existing training pipelines. In this regard, while both TRAM and Pi-DUAL utilize PI features to combat label noise, Pi-DUAL provides a solution that is both simpler and more efficient. Compared to TRAM, it has the following key distinctions:\n\n1. **Network structure**: TRAM uses one model but with two heads, one for training and one for testing. In contrast,  Pi-DUAL uses three distinct subnetworks with clear roles during training and inference: the prediction network for fitting clean target distribution, the noise network for explaining away label noise, and the gate network for identifying noisy samples. These three roles can not be accounted for in the architecture of TRAM.\n2. **Learning objective**: While the learning objective of TRAM is a weighted sum of two objectives (one for fitting the noisy target distribution, and another for transferring the knowledge to the no-PI head), the learning objective of Pi-DUAL is a simple cross-entropy loss over the noisy targets. For a better illustration of the differences between Pi-DUAL and TRAM, we conducted an ablation which trains Pi-DUAL with the loss function of TRAM, and show that this produces much worse results:\n\n| Dataset \\ Method |    CE    |  Pi-DUAL | Pi-DUAL with TRAM loss |\n|:----------------:|:--------:|:--------:|:----------------------:|\n|     CIFAR-10H    | 51.1\u00b12.2 | 71.3\u00b13.3 |        61.6\u00b14.8        |\n|     CIFAR-10N    | 80.6\u00b10.2 | 84.9\u00b10.4 |        81.6\u00b10.9        |\n|    CIFAR-100N    | 60.4\u00b10.5 | 64.2\u00b10.3 |        59.0\u00b10.6        |\n\n3. **Modeling of label noise**. TRAM does not explicitly model label noise. Instead,  it learns the conditional distribution of noisy labels with respect to both x and a, to eventually predict with an implicit marginalization over a. On the other hand, Pi-DUAL explicitly models per-instance-level label noise in its architecture, which permits it to identify noisy labels and to effectively reduce the negative impact from noisy labels. In contrast to TRAM, the prediction network of Pi-DUAL learns the conditional distribution of clean labels with respect to x alone. With this explicit modeling of label noise, Pi-DUAL delivers not only better performance, but also a much better interpretability as to how PI helps explain away label noise (e.g., how the noise network overfits to the wrong labels in Figure 2, and how the gating network identifies wrong labels in Figure 3).\n\nWith these non-trivial improvements, Pi-DUAL outperforms TRAM++ on all 5 benchmarks achieving up to 8.1 accuracy points more than TRAM++ on ImageNet-PI. Pi-DUAL also delivers much better performance in identifying label noise post-training thanks to the explicit modeling of label noise."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216472644,
                "cdate": 1700216472644,
                "tmdate": 1700216472644,
                "mdate": 1700216472644,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jn4xNGMn7a",
                "forum": "8oYjW8QxuC",
                "replyto": "phW2OkNy6h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5831/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5831/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VkrZ (Part 2)"
                    },
                    "comment": {
                        "value": "**Comparison with results in literature**\n\nWe absolutely agree with the reviewer that all methods should be compared under their optimal configurations and this has precisely been our intention in all our experiments. All our experiments, including the reimplementation of other methods, are built on the open-source uncertainty baselines codebase (Nado et al., 2021) and follow as much as possible the benchmarking practices standardized by Ortiz-Jimenez et al. (2023).\n\nAs detailed in Appendix B, we follow the same hyper-parameter tuning pipeline for all methods, namely we perform a grid search over their main hyper-parameters using a noisy validation set randomly split from the train set. In this regard, we want to highlight that, since Pi-DUAL only introduces one additional hyper-parameter compared to the cross-entropy baseline, it has the lowest tuning budget compared to the other methods that all have  at least two extra hyper-parameters.\n\nMoreover, we want to clarify that there are no discrepancies between the results we report and those  from the literature, as the reviewer points out. Specifically:\n- On CIFAR10-H, our results for TRAM agree with those released by the original TRAM authors in their follow-up paper (Table 1 in Ortiz-Jimenez et al. 2023). The performance gap between Collier et al. (2022) and Ortiz-Jimenez et al. (2023) is due to the fact that they use two different versions of  CIFAR10-H. Collier et al. (2022) use the \u201cuniform\u201d version while Ortiz-Jimenez et al. (2023) use the harder \u201cworst\u201d version; see discussion in Sec. 2.1 of Ortiz-Jimenez et al. (2023). All our experiments are trained using the latter version of the dataset.\n- In all our tables, to ensure a fair comparison, we report results of the standard ELR and SOP methods which operate without any additional semi-supervised learning techniques. We note that the [leaderboard](www.noisylabels.com) reports separate results for ELR (our baseline) and ELR+ (including semi-supervised learning). For SOP however, we have found that the leaderboard mistakenly reports the results for SOP+ (including semi-supervised learning) instead of SOP, as one can see by comparing to the original SOP paper (cf. Table 4 of Liu et al. (2022)). **Taking this into account, we highlight that our results thus perfectly agree with those reported in the literature, and even exceed them due to better hyper-parameter tuning** (e.g. our ELR baseline is 5 points better on CIFAR100N than the one in the leaderboard).\n\nWe hope these clarifications will make the reviewer increase their confidence in our results. We fully understand the initial confusion of the reviewer due to the benchmarking disparities in the literature. Our goal was precisely to ensure a fair comparison of the different methods under the same settings to be able to draw solid conclusions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216566898,
                "cdate": 1700216566898,
                "tmdate": 1700216656705,
                "mdate": 1700216656705,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CsHPOQArLT",
                "forum": "8oYjW8QxuC",
                "replyto": "phW2OkNy6h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5831/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5831/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VkrZ (Part 3)"
                    },
                    "comment": {
                        "value": "**Comparison to semi-supervised methods**\n\nWe also agree with the reviewer that a comparison of Pi-DUAL with semi-supervised learning methods would be unfair and that is why we did not consider it in our initial manuscript. Prior works in the noisy-label literature also do not directly compare to DivideMix, but compare DivideMix to versions of their methods which use semi-supervised learning techniques. For example, the authors for ELR compare DivideMix with ELR+, while the authors for SOP compare DivideMix with SOP+ (rather than SOP). ELR+ and SOP+ are respectively more sophisticated versions of the original methods augmented with semi-supervised learning techniques, making the comparison fairer in terms of both costs and complexity.\n\nHowever, in response to the request of Reviewer h1hP to add a comparison to Divide-Mix and other semi-supervised methods, we have designed an extension of Pi-DUAL, referred to as Pi-DUAL+. It adds to Pi-DUAL two regularization techniques, namely label smoothing and the prediction-consistency regularization from Xie et al., 2020. The latter is a common choice to add semi-supervision to methods of the noisy-label literature (Liu et al., 2020, Cheng et al., 2020). The results for Pi-DUAL+ are shown below and are compared with several prior semi-supervised approaches :\n\n|                |  CIFAR-10H |  CIFAR-10N | CIFAR-100N |\n|----------------|:----------:|:----------:|:----------:|\n|       CE       | 51.10\u00b12.20 | 80.60\u00b10.20 | 60.40\u00b10.50 |\n|   Divide-Mix   | 71.68\u00b10.27 | 92.56\u00b10.42 | 71.13\u00b10.48 |\n|    PES(semi)   | 71.16\u00b11.78 | 92.68\u00b10.22 | 70.36\u00b10.33 |\n|      ELR+      | 54.46\u00b10.50 | 91.09\u00b11.60 | 66.72\u00b10.07 |\n|     CORES*     | 57.80\u00b10.57 | 91.66\u00b10.09 | 55.72\u00b10.42 |\n|      SOP+      | 66.02\u00b10.06 | 93.24\u00b10.21 | 67.81\u00b10.23 |\n| Ours: Pi-DUAL+ | 83.23\u00b10.26 | 93.31\u00b10.21 | 67.99\u00b10.08 |\n\n\nIn this table, the baseline results are obtained using the code of the original papers if they are not available in the [public leaderboard](http://www.noisylabels.com). We can see that on CIFAR-10H, Pi-DUAL+ outperforms the rest of the methods by a margin of over 11%, while it performs on par with the state-of-the-art methods on CIFAR-10N and CIFAR-100N. This is because Pi-DUAL+ benefits from high-quality PI on CIFAR-10H, while for CIFAR-10N and CIFAR-100N, the available PI is known to be of poor quality (Ortiz-Jimenez et al., 2023). We have added this table, as well as a detailed description of Pi-DUAL+ in our paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216602457,
                "cdate": 1700216602457,
                "tmdate": 1700228803949,
                "mdate": 1700228803949,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8JdJboNALL",
                "forum": "8oYjW8QxuC",
                "replyto": "CsHPOQArLT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5831/Reviewer_VkrZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5831/Reviewer_VkrZ"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I thank the authors for their detailed response to address my concerns. I am now confident in the reported accuracy and find the comparison to be fair. Regarding the disparity between Pi-DUAL and TRAM, I concur with the authors that not only does the network structure design differ, but so does the learning objective. While I acknowledge the novelty of Pi-DUAL, I am of the opinion that its technical novelty may not be as substantial as that of TRAM. Therefore, I maintain my score at 6, leaning towards acceptance."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650130359,
                "cdate": 1700650130359,
                "tmdate": 1700650130359,
                "mdate": 1700650130359,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7fcgp9cwrr",
            "forum": "8oYjW8QxuC",
            "replyto": "8oYjW8QxuC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5831/Reviewer_sbuM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5831/Reviewer_sbuM"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the innovative \"Pi-DUAL\" architecture, which effectively uses privileged information (PI) to distinguish between clean and erroneous labels, offering a crucial solution to label noise. Pi-DUAL demonstrates substantial performance enhancements in several benchmark tests and attains a new state-of-the-art accuracy.  It also excels in identifying noise samples post-training, surpassing other methods.  The ablation study is complete and conducted on all benchmarks for better presentation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "First, from the originality point of view, this paper presents a novel architecture, i.e., a noise labeling architecture guided by privileged information (PI), which enables the model to distinguish clean labels and mislabels more clearly. Second, they implement a bidirectional gated output logic structure that decomposes the output logic into a predictive term based on regular input features and a noise-adapted term influenced only by PI. Finally, a PI-driven gating mechanism adaptively chooses between the predictive term and the noise-adaptation term to handle clean and mislabeled learning paths, respectively. \nSecond, from a significance perspective, the results are impressive, and the improvement of the proposed methods is significant.  Combined with the novelty, I think overall, this is a sound paper introducing an effective method."
                },
                "weaknesses": {
                    "value": "I list several questions that may be helpful.\n1. What hardware requirements are needed for Pi-DUAL training for large datasets? If a training cost analysis is provided, I think it can be more useful for deploying your method in more scenarios.\n2. How does Pi-DUAL perform in terms of security and privacy protection? For example, is there a risk that privileged information may be compromised? If there is no PI exists, what the performance will be? \n3. Are there any parameter tuning for Pi-DUAL for different levels of label noise and datasets? What is your hyper-parameter chosen strategy?"
                },
                "questions": {
                    "value": "Please see above weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5831/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698983929130,
            "cdate": 1698983929130,
            "tmdate": 1699636615312,
            "mdate": 1699636615312,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dmb6kpMlgD",
                "forum": "8oYjW8QxuC",
                "replyto": "7fcgp9cwrr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5831/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5831/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sbuM"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to review our paper and for all the valuable feedback. We address their comments below:\n\n**Computational cost of Pi-DUAL**\n\nIn our experiments, we found that Pi-DUAL does not have extra computational requirements compared for hardware to the standard training baseline. The added cost of processing the PI features (which typically have much fewer dimensions) is indeed negligible with respect to processing the whole images. In this sense, training and inference using Pi-DUAL take roughly the same time as training with standard cross-entropy, TRAM++ or HET. For our large-scale datasets (i.e., ImageNet-PI), we used a TPU V3 with 8 cores for all our experiments.\n\n|                        |   CE   | TRAM++ | Pi-DUAL |   HET  | SOP | ELR |\n|:----------------------:|:------:|:------:|:-------:|:------:|:---:|:---:|\n|  Number of parameters  |   26M  |   32M  |   36M   |   58M  | >1B | >1B |\n| Training time per step | 0.510s | 0.541s |  0.566s | 0.575s |  -  |  -  |\n\nWe use the parameter count as a proxy for the memory cost. Since Pi-DUAL models the per-instance level of noise using the PI features, it does not require introducing parameters that scale with the number of samples or classes as SOP and ELR need to. In fact, the extra parameters needed in our experiments to account for the noise and gating networks are comparable to those of TRAM++ and even fewer than the ones introduced by the no-PI method HET.\n\nWe have added these comments to the Appendix D of our new version.\n\n**Privacy concerns of using PI**\n\nWe thank the reviewer for bringing this topic to our attention and we will make sure to discuss it in our camera-ready work. However, we note that none of our results require the use of personally identifiable annotator IDs. In fact, cryptographically safe IDs in the form of hashes work perfectly fine as PI. In this regard, we do not think that there are serious concerns about possible identity leakages stemming from our work if the proper anonymization protocols are followed. In addition, we have added an ethics statement to the paper regarding the privacy and safety protection requirements of Pi-DUAL.\n\n**Performance without PI**\n\nAs mentioned throughout the paper, we would like to emphasize that Pi-DUAL has been explicitly designed to leverage PI features. In this regard, its good performance hinges on the availability of good PI features such as in the case of ImageNet-PI or CIFAR-10H (Ortiz-Jimenez et al. 2023).\n\nHowever, as mentioned by the reviewer, it is still interesting to study the performance of Pi-DUAL when no PI is available, as we show in Table 4 in the paper (last row). In these cases, we can always construct some randomly-generated PI to train PI-DUAL, following the  procedure of Ortiz-Jimenez et al. (2023). As shown in Table 4 (and copied below for reference), Pi-DUAL with the randomly-generated PI still outperforms the cross-entropy baseline on the CIFAR datasets, but its performance is much worse than when using the real PI of the datasets.\n\n|                        |  CIFAR-10H |  CIFAR-10N | CIFAR-100N | ImageNet-PI (high noise) | ImageNet-PI (low noise) |\n|:----------------------:|:----------:|:----------:|:----------:|:------------------------:|:-----------------------:|\n|           CE           |  51.1\u00b12.2  |  80.6\u00b10.2  |  60.4\u00b10.5  |         68.2\u00b10.2         |         47.2\u00b10.2        |\n|    Pi-DUAL (full PI)   |  71.3\u00b13.3  |  84.9\u00b10.4  |  64.2\u00b10.3  |         71.6\u00b10.1         |         62.1\u00b10.1        |\n| Pi-DUAL (only rand PI) |  53.5\u00b12.2  |  83.7\u00b11.3  |  61.8\u00b10.3  |         68.4\u00b10.1         |         47.0\u00b10.4        |\n\n\n**Hyperparameter tuning strategy**\n\nAll our experiments, including the reimplementation of other methods, are built on the open-source uncertainty baselines codebase (Nado et al., 2021) and follow as much as possible the benchmarking practices standardized by Ortiz-Jimenez et al. (2023). \n\nAs detailed in Appendix B, we follow the same hyper-parameter tuning pipeline for all methods in which we perform a grid search over their main hyper-parameters using a noisy validation set randomly split from the train set. We do this to make sure that all methods are compared under their optimal and fair settings. In this regard, we want to highlight that, because Pi-DUAL only introduces one additional hyper-parameter being tuned compared to the cross-entropy baseline (i.e., the length of the random PI), it has the lowest tuning budget in comparison to every other method. All other methods introduce at least two additional hyper-parameters to be tuned and therefore we dedicate at least twice as much compute to tune them than the compute used to tune Pi-DUAL. Despite these additional hyper-parameter tuning resources, the performance of Pi-DUAL compares favorably to these methods.\n\nOverall, we see the simplicity of tuning Pi-DUAL compared to other noisy label methods as another strength of the method."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5831/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216292050,
                "cdate": 1700216292050,
                "tmdate": 1700216292050,
                "mdate": 1700216292050,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]