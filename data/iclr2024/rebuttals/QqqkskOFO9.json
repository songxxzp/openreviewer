[
    {
        "title": "Rethinking Actor-Critic: Successive Actors for Critic Maximization"
    },
    {
        "review": {
            "id": "3aPxmLGKpn",
            "forum": "QqqkskOFO9",
            "replyto": "QqqkskOFO9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9227/Reviewer_ojfo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9227/Reviewer_ojfo"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel reformulation of the actor by employing a sequence of sub-actors, which solves the problem of non-convex and high-dimensional, and non-stationary during action-value optimization landscape training. The logical assumption stated by the authors is an ensemble of successive actor-critic modules can collectively navigate the action-value landscape more proficiently than a single, monolithic actor. The authors demonstrate improvement over continuous and large discrete action space reinforcement learning tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written and well structured. The idea of successive actor modules for \"pruning\" all actions with Q-values lower than the baseline is interesting and (to the best of my knowledge) novel. The experimental setup (especially on large discrete action space RL tasks and more discontinuous variants of continuous RL tasks) appears rigorous."
                },
                "weaknesses": {
                    "value": "1.The paper's central assumption feels reasonable, and the experiment seems to confirm it. But there is no theoretical proof.\n2.The key parts of the successive actor-critic modules adopt both \u2018deep-set\u2019 and \u2018FiLM\u2019 methods, but lack a description of potential advantages and an explanation of alternative methods.\n3.From the structure of the proposed method, it can fluidly integrated into other widely adopted RL algorithms. In the experiment, TD3 was selected as the baseline. Is it possible to add other RL methods to the ablation study to illustrate the applicability of the method.\n4.FIG. 1 is a diagram illustrating the core ideas of the paper. Can 'tractable' be explained from the perspective of real data in the experimental part? FIG. 4 seems intended to explain, but is insufficient."
                },
                "questions": {
                    "value": "1.Algorithm 1, line 17 has a prominent '{' symbol. What is the difference between $(s_t|A)$ and $(s_t,A)$ in formulas $a=\\pi_{i}(s_t|A)$ and $\\pi_{\\phi_i}(s_t,A)$.\n2.Legend should be added to FIG. 7, although it is related to FIG. 3.\n3.Although there is not enough time to go through the source code carefully, it is recommended that the method abbreviation be consistent in the code(called FLAIR) and the paper(called SAVO)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9227/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698118772722,
            "cdate": 1698118772722,
            "tmdate": 1699637160971,
            "mdate": 1699637160971,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "syjPIyN8tB",
                "forum": "QqqkskOFO9",
                "replyto": "3aPxmLGKpn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review! Here are our responses."
                    },
                    "comment": {
                        "value": "We thank you for your constructive feedback. We appreciate your positive comments on the approach and experimental performance. We address your concerns below:\n\n### FiLM, DeepSet Design Choices\n- In Appendix C.3 and C.4, we added the descriptions of Deepset along with other design choices of list-summarizers as well as Feature-wise Linear Modulation (FiLM). In short, Deepset effectively aggregates the information in the list input while FiLM helps to condition on preceding list-input. Those modules contribute to better training the list of actor-critics in SAVO.\n- We also added ablations in Appendix.D.2 & D.3 to compare (i) FiLM v/s no-FiLM and (ii) the choice of DeepSet v/s LSTM v/s Transformers to aggregate the information from previous selected actions. The results validated our choice of DeepSet as it is more scalable to an increasing number of actor-critic pairs. Also, we acknowledge that this design choice depends on the environment and the use-case as some tasks in continuous domains show that LSTM / Transformers outperform Deepset.\n\n### Visual insights on central claim\n`[New Figure.3]` The optimization landscape of Q-value concretely represents the landscape where the N-D action space represents the independent variable and the 1-D Q-value represents the dependent variable. For visualization, we map the action space to 2-D when N > 1. Fig.3 demonstrates that the Q-function becomes more easy to globally optmize with every iteration of our successive actor-critics. This is because the locally optimal peaks below past actions\u2019 Q-values are pruned away from the optimization landscape. As we can observe:\nThe number of local optima are reduced in $Q_0 \\to Q_1 \\to Q_2$, as most action values are shifted up in the Q-value space. This enables successive actors $\\pi_0 \\to \\pi_1 \\to \\pi_2$ to focus on more optimal regions of the true Q-value space.\nThe actions $a_1$ and $a_2$ found by $\\pi_1$ and $\\pi_2$ are indeed more optimal than $\\pi_0$ whose actions were only locally optimal peaks, but globally suboptimal\n\n### Clarifications of Baseline Selection\nWe choose TD3 because of its unified applicability to both action-representation-based discrete action spaces and continuous action space. Particularly, the Wolpertinger algorithm [Dulac et al 2015] is based on the DDPG algorithm and is the most commonly used base algorithm for large discrete action spaces. While we agree with the reviewer\u2019s suggestion that combining SAVO with SAC would be good to show wider applicability of our method, we think this extension might offer more flexibility in the design choices. Thus, for this project, we decided to stick to the simple architecture of TD3 as described above.\n\n### Notational fixes\nWe have fixed all the suggested typos, missing legends, and source code naming in our revision. Thank you for your detailed suggestions.\n\nPlease do let us know if you have any leftover concerns or questions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740593313,
                "cdate": 1700740593313,
                "tmdate": 1700741807058,
                "mdate": 1700741807058,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "C7ojHhGBV3",
            "forum": "QqqkskOFO9",
            "replyto": "QqqkskOFO9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9227/Reviewer_oxP1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9227/Reviewer_oxP1"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied an interesting problem that the training of the policy network often cannot effectively optimize the learned value function. This could lead to sub-optimal learning performance and ineffective exploration. To address this issue, this paper proposed a new ensemble technique that utilizes a sequence of separately trained actor-critic pairs to gradually refine/restrict the action space being considered. The newly proposed algorithm has been experimented on multiple benchmark problems with both discrete and continuous action spaces. The performance results showed that the new algorithm can achieve better overall performance across all the benchmark problems."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It is a well-known problem in the literature that the trained policy network in an actor-critic algorithm may often fail to optimize the learned value function. This inconsistency could potentially weaken the performance of the learning process. This paper developed an interesting new algorithm to address this issue. The effectiveness of the new algorithm is also evidenced by promising experiment results."
                },
                "weaknesses": {
                    "value": "While the idea of using a series of successive actor-critic pairs to gradually refine/restrict the action space is interesting, however, this also means that the action selection decision from the policy network may be highly sensitive to the minor nuances of the learned critic. This often introduces bias to the learning process, resulting in degraded learning stability and restricted exploration. Hence, the downside of using multiple successive actor-critic pairs should be extensively examined in this paper. It is important to know why successively restricting the action space based on the trained critic will not affect the learning stability with a solid theoretical foundation. It is also important to know why this actually helps to improve the effectiveness of exploration, rather than restricting exploration, as claimed by the authors.\n\nSince the newly proposed algorithm uses an ensemble of actor-critic pairs, it is related to ensemble actor-critic algorithms. Hence, in section 2, it seems necessary for the authors to review existing ensemble actor-critic methods and clearly highlight the key novelty of the new algorithm, compared to existing ensemble algorithms. Furthermore, the experiment study should include more state-of-the-art ensemble baselines, in order to clearly show the advantages of the new algorithm over existing ensemble algorithms.\n\nSome parts of the new algorithm design seem to lack technical clarity. In particular, it is not clear to me how deep-set is used to produce Z as a concatenation of previously selected actions and state. It is also unclear how FiLM is used to enable a policy to choose its actions that are conditional on Z. Meanwhile, The motivations and rationales of using deep-set and FiLM should be clearly explained and strongly justified.\n\nThe authors stated that their new ensemble technique can be applied to many different actor-critic algorithms. Given this statement, it is not clear why they focus primarily on applying their new technique to TD3 alone. To demonstrate the wide applicability of the new technique, the authors should study its possible application to other algorithms, such as SAC.\n\nI don't quite understand some mathematical formulas presented in this paper. For example, I don't know how to find the optimal action a' based on the primary critic, as part of the final training objective, which is further conditional on $\\Pi$. The formula for policy gradient in the final training objective also misses some brackets such as }. Meanwhile, it remains questionable why the policy gradient formula is valid, i.e. what kind of gradient is being calculated and why the gradient allows the trained policy network to maximize the expected return. I think more detailed and thorough theoretical analysis is necessary to justify the validity and effectiveness of the proposed training objective.\n\nSome statements in this paper are not easy to understand. For example, what does it mean by \"navigate the action-value landscape more proficiently\" on page 1? What does it mean by \"distribute the optimization load over to the critic\" on page 5? What does it mean by \"slower than the original inefficiencies of the actor\" on page 5? What does it mean precisely for the optimization landscape of Q to be more tractable? If the number of local optima of Q's landscape can be reduced, to which extent can such reduction be actually achieved?\n\nThe English presentation of this paper may need to be improved. The authors are highly recommended to conduct more rounds of proof-reading of their paper to substantially improve the presentation clarity and quality.\n\n======\n\nThank the authors for responding to my questions. the response has addressed some of my concerns. Meanwhile, I have some further doubts regarding the response:\n\n1. I understand that the new algorithm does not follow the conventional ensemble strategies. However, for the sake of showing the true strength of the new algorithm, its benefits in terms of exploration, policy gradient estimation, value function learning etc. may need to be further expanded, compared to previously proposed ensemble methods.\n\n2. Line 13 of Algorithm 1 is based on the estimated Q-function. Due to the estimation noise/error, I don't believe it is guaranteed that the new action selection method is definitely better than the case of using a single actor. To a large extent, I feel the intuitive discussion (including some illustrative figures) about the stability issue or the direct advantage of identifying the best possible action is not sufficiently convincing. It would be great if the authors can present a more thorough theoretical analysis to justify these major claims.\n\n3. I am not so sure how the policy gradient should be calculated for the new form of actor-critics. Specifically, does Q_i in the new form satisfy any Bellman equation and why? Why can the effective state space be expanded with actions chosen by other actors? What is the new definition of policy gradient on the basis of the expanded state space, especially when the action component of the state space is affected by the learning process of other actors (hence the expanded state space may no longer satisfy the Markov property)? Perhaps more detailed mathematical derivation could help to clarify these concerns."
                },
                "questions": {
                    "value": "Why will successively restricting the action space based on the trained critic not affect the learning stability and improve the exploration effectiveness?\n\nWhy is the new policy gradient formula valid, i.e. what kind of gradient is being calculated and why the gradient allows the trained policy network to maximize the expected return?\n\nPlease refer to the previous section regarding questions on the clarity of some statements."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9227/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9227/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9227/Reviewer_oxP1"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9227/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698536765682,
            "cdate": 1698536765682,
            "tmdate": 1700802761481,
            "mdate": 1700802761481,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WvGRoA4o9d",
                "forum": "QqqkskOFO9",
                "replyto": "C7ojHhGBV3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review! Here are our responses."
                    },
                    "comment": {
                        "value": "We express gratitude for your valuable feedback and constructive input. Your positive remarks regarding the approach and experimental performance are acknowledged and appreciated. Below, we provide responses to the concerns you raised.\n\n### New Ensemble Baseline\nNovelty over ensemble methods: Ours is not a typical ensemble method, where usually the constituents of the ensemble are symmetric. In fact, ours is a sequential ensemble, where successive actor-critic pairs optimize different optimization problems, building up on the past actor-critic pairs.\nOur added visual result in `Fig.3` shows how this successive optimization eases the Q-function optimization landscape, which indeed helps the successive actors to find more globally optimal actions than before, with respect to the current Q-function\n\n### Visual demonstration of improving optimization landscape\n- `[New Figure.3]` The optimization landscape of Q-value concretely represents the landscape where the N-D action space represents the independent variable and the 1-D Q-value represents the dependent variable. For visualization, we map the action space to 2-D when N > 1. Fig .3 demonstrates that the Q-function becomes more easy to globally optmize with every iteration of our successive actor-critics. This is because the locally optimal peaks below past actions\u2019 Q-values are pruned away from the optimization landscape. As we can observe:\n- The number of local optima are reduced in $Q_0 \\to Q_1 \\to Q_2$, as most action values are shifted up in the Q-value space. This enables successive actors $\\pi_0 \\to \\pi_1 \\to \\pi_2$ to focus on more optimal regions of the true Q-value space.\nThe actions $a_1$ and $a_2$ found by $\\pi_1$ and $\\pi_2$ are indeed more optimal than $\\pi_0$ whose actions were only locally optimal peaks, but globally suboptimal.\n\n\n### Learning stability\n`Action selection decision`\nThis is true for all actor-critic methods, especially those learned with deep learning. However, the key goal of any RL agent is to take actions that maximize its expected return in the environment, which is why actor-critic algorithms learn actors that can find actions to maximize the Q-values. This does not create any *bias* in learning of the actors, because their true objective is to maximize the critic. While the Q-function itself can be biased during deep RL training, it is still the best estimate of the objective for the actor. Our method does not degrade the primary critic\u2019s function which is already based on stabilizing tricks adopted from TD3, like twin critics, delayed updates, and soft-action evaluation. Thus, stability of training of the primary critic is not negatively affected at all by our method. And the actor learned by our method is guaranteed to be better than a single actor, because of the max operator in Line 13 of Algorithm 1, which ensures the action is always at least as good as the one suggested by a single actor.\nPlease also refer to combined response for more details.\n\n`Exploration`\n- Exploration for each successive actor $\\pi_i$ still follows the standard OU Noise based exploration from DDPG, TD3. The successive actors provide an action that has a higher Q-value than a single actor would have provided, but the exploration term is still added over the actions while acting in the environment. We have modified the Algorithm 1 to clarify this important detail.\n- Furthermore, the sample efficiency improvements across 14 tasks (Fig.4 using RLiable as AC suggested) empirically justifies that our method never hurts exploration in the environment.\n\n\n### Validity of policy gradient objective\nEach successive actor-critic pair solves its own deterministic policy gradient objective, where only the effective state space is modified. Conventional actor-critics take the form $\\pi(s, a)$ and $Q(s, a)$. Whereas, each of our successive actor-critics take the form $\\pi_i(\\{s, a_{0:i-1}\\}, a)$ and $Q_i(\\{s, a_{0:i-1}\\}, a, a)$. Thus, only the effective state space of the MDP is modified, and we can directly apply any actor-critic algorithm to train these actor-critics (we use TD3).\n\nPlease do let us know if you have any leftover concerns or questions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740831696,
                "cdate": 1700740831696,
                "tmdate": 1700742120464,
                "mdate": 1700742120464,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nC0P1WHpWt",
                "forum": "QqqkskOFO9",
                "replyto": "C7ojHhGBV3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respnose continued"
                    },
                    "comment": {
                        "value": "### Technical explanation and analysis of FiLM, DeepSet\n- In Appendix C.3 and C.4, we added the descriptions of Deepset along with other design choices of list-summarizers as well as Feature-wise Linear Modulation (FiLM). In short, Deepset effectively aggregates the information in the list input while FiLM helps to condition on preceding list-input. Those modules contribute to better training the list of actor-critics in SAVO.\n- We also added ablations in Appendix.D.2 & D.3 to compare (i) FiLM v/s no-FiLM and (ii) the choice of DeepSet v/s LSTM v/s Transformers to aggregate the information from previous selected actions. The results validated our choice of DeepSet as it is more scalable to an increasing number of actor-critic pairs. Also, we acknowledge that this design choice depends on the environment and the use-case as some tasks in continuous domains show that LSTM / Transformers outperform Deepset.\n\n\n### Clarifications\n`Baseline Selection of TD3`\nWe choose TD3 because of its unified applicability to both action-representation-based discrete action spaces and continuous action space. Particularly, the Wolpertinger algorithm [Dulac et al 2015] is based on the DDPG algorithm and is the most commonly used base algorithm for large discrete action spaces. While we agree with the reviewer\u2019s suggestion that combining SAVO with SAC would be good to show wider applicability of our method, we think this extension might offer more flexibility in the design choices. Thus, for this project, we decided to stick to the simple architecture of TD3 as described above.\n\n`How to find the optimal action a' based on the primary critic`\nWe improved the clarity of writing in the approach section and added the following qualitative result to support our claim;\n``[New Figure.3]`` The optimization landscape of Q-value concretely represents the landscape where the N-D action space represents the independent variable and the 1-D Q-value represents the dependent variable. For visualization, we map the action space to 2-D when N > 1. Fig.3 demonstrates that the Q-function becomes more easy to globally optmize with every iteration of our successive actor-critics. This is because the locally optimal peaks below past actions\u2019 Q-values are pruned away from the optimization landscape. As we can observe:\nThe number of local optima are reduced in $Q_0 \\to Q_1 \\to Q_2$, as most action values are shifted up in the Q-value space. This enables successive actors $\\pi_0 \\to \\pi_1 \\to \\pi_2$ to focus on more optimal regions of the true Q-value space.\nThe actions $a_1$ and $a_2$ found by $\\pi_1$ and $\\pi_2$ are indeed more optimal than $\\pi_0$ whose actions were only locally optimal peaks, but globally suboptimal.\n\n\n`Language clarifications`\nAs per the reviewer\u2019s suggestion, we have improved the language-ambiguous stataments:\n\"navigate the action-value landscape more proficiently\" \u2192 \u201cglobally optimize the Q-value landscape\u201d\nAn actor is trained with gradient ascent over the Q-value landscape and can get stuck in local optima, which we resolve.\n\"distribute the optimization load over to the critic\" \u2192 \u201cutilize the critic to evaluate multiple actions\u201d.\n\"This can make the learning procedure even slower than the original inefficiencies of the actor\" \u2192 \u201cA larger action space in the joint-actor-critic model present another optimization challenge, making this solution infeasible\u201d."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741274403,
                "cdate": 1700741274403,
                "tmdate": 1700742302962,
                "mdate": 1700742302962,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QsWuHquv9I",
            "forum": "QqqkskOFO9",
            "replyto": "QqqkskOFO9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9227/Reviewer_MBmv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9227/Reviewer_MBmv"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a continuous actor value optimization method to address the issue that traditional single actor-critic algorithms are prone to failing into local optima, in order to improve sampling efficiency and final performance in large discrete action spaces and continuous action spaces. The effectiveness of the proposed method is ultimately demonstrated through experiments."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper presents a novel reformulation of the actor. I think that addressing the challenge of better maximization value function by \"pruning\" an optimization landscape is an interesting work."
                },
                "weaknesses": {
                    "value": "1) The writing expression is not sufficiently clear and the logic is confusing, especially in Introduction and Related Work sections. It is difficult to understand the structure of the article. The contribution is not clear, and it is not suitable to use a large space to introduce the experimental environment.\n2) This paper lacks many vital technical explanations, including an introduction to deep-set and FiLM layer, the motivation behind their usage, and analysis of their effects.\n3) The experimental results are not sufficiently reliable. The baselines are outdated. There is no mention of hyperparameter sensitivity or setting experiments."
                },
                "questions": {
                    "value": "1) In Relate Work, the introduction of prior work is outdated. Please supplement it with the latest relevant work.\n2) The work presented in this paper seems to fall under the domain of ensemble methods. It may be necessary to supplement it with relevant work and introduce ensemble-based value optimization algorithms as additional baselines.\n3) In Algorithm 1, the \"state s\" in lines 8 and 10 need clarification.\n4) Does the proposed method in this paper suffer from the problem of action values overestimation?\n5) Why only select 3 seeds in some experiments, such as Figure 4?\n6) Is the modification of the experimental setup fair? Other baselines may not have been designed specifically to address the problem presented in this paper.\n7) The experiment in the Appendix only has the Easy environments, what about the other Hard environments?\n8) Due to the utilization of ensemble methods, I am concerned about the computational efficiency of the algorithm. Please supplement the experiments or provide an analysis."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9227/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9227/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9227/Reviewer_MBmv"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9227/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698659264199,
            "cdate": 1698659264199,
            "tmdate": 1699637160755,
            "mdate": 1699637160755,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uvwOBCSrEI",
                "forum": "QqqkskOFO9",
                "replyto": "QsWuHquv9I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Added several new experiments and clarifications"
                    },
                    "comment": {
                        "value": "We thank you for your constructive feedback that helped improve our paper significantly. We address your concerns below:\n## Technical explanation and analysis of FiLM, DeepSet\nPlease refer to the combined response\n## Added latest baselines\nThanks for your suggestion. Please refer to the combined response. We have added 4 new baselines: CEM, Greedy-AC, Greedy-AC-TD3, Ensemble.\n## Added more experimental benchmarks\n> \u201cThe experimental results are not sufficiently reliable.\u201d \"The experiment in the Appendix only has the Easy environments, what about the other Hard environments?\u201d\n\n`[Figure 2]` We added more continuous control environments in the paper, and report results across 11 environments.\n`[Figure 4(c)]` We also report the results using the RLiable library (also suggested by AC) to demonstrate that our method outperforms the baselines and ablations reliably across the environments.\n## Computational Efficiency\n\u201cI am concerned about the computational efficiency of the algorithm. Please supplement the experiments or provide an analysis.\u201d\n`[Figure 4 (d)]` Thank you for this suggestion. We have added a complexity v/s performance analysis that shows the computational requirement increase is negligible. Please refer to the combined response.\n## Supplemented discussion in Related Work\n`[Section 2]` Added discussion on evolutionary methods and ensemble methods, and also restructured the related work section significantly.\n## Clarifications\n> \u201cThere is no mention of hyperparameter sensitivity or setting experiments.\u201d\n- Appendix E.2 and E.3 already provides a discussion of hyperparameter sensitivity and setting details across all baselines.\n- To reiterate, the sensitive hyperparameters were found to be learning rates and network sizes of actors and critics, and were searched for each baseline.\n- `[Figure 4 (c)]` A key hyperparameter of our approach is $K$, the number of actor-critic pairs. Figure.4 shows the improvement in performance as $K$ increases in continuous control tasks. However, we chose $K$=3 across environments as that is enough to show significant gain with our method.\n- `[Appendix D2, D3]` We also add experiments to show different design choices are viable: FiLM v/s No-FiLM for conditioning, and DeepSet v/s LSTM v/s Transformer for past action summarization.\n\n> \u201cIn Algorithm 1, the \"state s\" in lines 8 and 10 need clarification.\u201d  \n- We correct the typo in lines 8-10, as $s$ \u2192 $s_t$. $s_t$ is the state observed in Line 5.\n\n> \"Does the proposed method in this paper suffer from the problem of action values overestimation?\u201d\n- Since our algorithm is based on TD3, the problem of overestimation is reduced by the presence of twin critics and delayed updates. We clarify this in Algorithm 1 to alleviate the concern of overstimation. To further justify, in the Q-learning update of primary critic in Line 19 of Algorithm 1, our method only modifies the search of the Q-value maximizing action. This is a key requirement of the DPG algorithm and the training objective of DPG, DDPG, TD3, etc. Works like DDPG and TD3 alleviate the issues of overestimation of Q-functions, and our implementation being based on TD3, also benefits from those solutions. All the baselines and ablations also equivalently benefit from this, and do not suffer from overestimation any more than TD3.\n\n> \u201cWhy only select 3 seeds in some experiments, such as Figure 4?\u201d\n- All analysis results have been updated with 5 seeds. No trend is affected.\n\n> \u201cIs the modification of the experimental setup fair? Other baselines may not have been designed specifically to address the problem presented in this paper.\n- To exemplify the problem of challenging Q-function landscapes, we curate hard benchmarks of continuous control tasks, but we also reported results on the original easy benchmarks with relatively simple optimization landscapes, in Figure 2. SAVO outperforms baselines in both the easy and hard versions of the tasks, but more significantly in the hard versions. This shows that our algorithm\u2019s contribution is more robust to challenging optimization landscapes, that would be more common as RL scales up to harder problems. This is already evident in our results in the discrete action space environments, where the optimization landscape is already challenging because of the presence of only few valid points in the action representation space, that correspond to actual discrete actions.\n\n- The baselines from various prior works (Figure 2) are indeed incapable of addressing the problems of Q-function optimization presented in this paper. That is the key claim that we are making. All the implementations of the baselines are fair and based on the same code skeleton (code is attached in supplementary), and we clearly demonstrate that it is because of the better optimization of the Q-function by SAVO\u2019s successive actors than baseline actors like single actors, Sampling-augmented actors, CEM Actors, and Ensemble of Actors."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741416733,
                "cdate": 1700741416733,
                "tmdate": 1700741830400,
                "mdate": 1700741830400,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g5H7U7PzDT",
            "forum": "QqqkskOFO9",
            "replyto": "QqqkskOFO9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9227/Reviewer_ZeCv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9227/Reviewer_ZeCv"
            ],
            "content": {
                "summary": {
                    "value": "The paper claims that actor often finds actions that cannot maximize the critic and this leads to sample inefficient training and suboptimal convergence. The paper proposes an algorithm that roughly works as follows. First, in addition to a primary critic, another K actors and critics are initialized, and they are queried in order: an actor\u2019s input depends on all previous actors\u2019 outputs; second, the action with highest primary critic value is executed; third, K updates applied to the K actor-critic pairs; last, the primary critic is updated by its own maximum action. Experiments are conducted to verify their claims."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The presentation is reasonably clear. \n\n2. The proposed problem regarding the actor often cannot align well with the maximum action worths studying, it looks interesting to me."
                },
                "weaknesses": {
                    "value": "The critical claims are not well-supported: 1) why the proposed method can help find maximum action; 2) the connection between finding maximum action and improved sample efficiency; 3) the actual benefit of the proposed algorithm, is it from finding the maximum, or ensemble, or exploration? 4) experiments are not well-designed; 4) highly related works are missing. \n\nTo support the claim of the paper, the following experiments need to be done: \n1. add experiments to verify the proposed method does find action with a higher action value; current version directly jumps into sample efficiency, leaving the critical claim unverified; \n2. The connection between finding the maximum action and improved sample efficiency is not supported, please justify;  would it introduce overestimation that hurts learning? \n3. please add ensemble-based exploration method for comparison, as it is known that ensemble would provide benefits of enhancing sample efficiency. Another purpose of adding ensemble is to verify if the main benefit really comes from finding the maximum action or from exploration, If it is the letter, then the pitch of the paper should be modified and another set of baselines aiming at better exploration should be compared. \n4. Any comments on the convergence of such an algorithm? I am a bit concerned that the update of an actor depends on all previous actors output could result in high non-stationarity. This would make the training difficult. \n5. The proposed algorithm seems to have much higher computation cost, which weaken the practical utility. \n\nPotential flaws of the experiment design. \n1. In the algorithm, it seems at each time step, the algorithm update both policy and critic parameters K times, do the authors do the same thing for baselines? \n2. Please add baselines as suggested by below missing related works. \n\nThere are several missing references that are highly relevant: \n\n1. A model reference adaptive search method for global optimization by Jiaqiao et al.\n2.  Q-learning for continuous actions with cross-entropy guided policies by Riley et al.\n3.  Greedy Actor-Critic: A New Conditional Cross-Entropy Method for Policy Improvement by Samuel et al\n4. CEM-RL: Combining evolutionary and gradient-based methods for policy search by Alois et al. \n5. Wire fitting algorithm by Baird et al. the title is likely RL with high-dimensional continuous actions. \n\nAmong these, 2,3,4 are highly relevant and should be also compared. Please explain what the differences are between your work and those existing ones and comment on the significance of such difference. I consider this one of the critical weaknesses of this work."
                },
                "questions": {
                    "value": "see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9227/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789002538,
            "cdate": 1698789002538,
            "tmdate": 1699637160641,
            "mdate": 1699637160641,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KacHekuivL",
                "forum": "QqqkskOFO9",
                "replyto": "g5H7U7PzDT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9227/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9227/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Important Baselines from Related Work added"
                    },
                    "comment": {
                        "value": "We thank you for your constructive feedback that helped improve our paper significantly. We address your concerns below:\n## Important Baselines from Related Work added:\nWe appreciate the reviewer\u2019s suggestions and pointing out the relevant work and baselines from prior work. We have added a discussion and comparison to all the suggested baselines:\n\n### Ensemble actors\n[`Figure 2`, `Section 2.3`] As per your suggestion, we implement an ensemble-actors approach where we train multiple actors with the same critic, but explore using the critic maximizing action found by the actors. This approach helps disentangle SAVO\u2019s ability to prune the optimization landscape from the improved exploration due to the presence of multiple actors. Please refer to combined response for further details, and Section 2.3 for a discussion and references of past work.\n\n### Evolutionary Actors\n[`Figure 2`, `Section 2.4`] As per your suggestion, we add:\n- **CEM-actor baseline**: Algorithms like QT-Opt, CGP, CEM-RL, and GRAC employ CEM as the actor in online RL training. We implement QT-Opt style CEM, where the critic is still trained with our TD3-augmented tricks. These algorithms involving CEM require infeasible amounts of repetitive evaluations of the Q-function and do not scale well to high-dimensional action spaces (Yan et al., 2019). Note that CGP, CEM-RL and GRAC can be considered as suboptimal versions of a good CEM actor in the QT-Opt, because they already use CEM as a guide to train their actors. Thus, we only implement and compare against the QT-Opt style CEM baseline.\n- **Greedy-actor** critic baseline: emulates CEM by sampling from a high-entropy action proposal policy, evaluating these actions with the Q-function, and training the actor to greedily follow the best actions. However, since this approach also depends on gradient ascent and samples actions around the mean actions of a single-actor, thus limiting its ability to find the globally optimal action.\n- **Greedy-TD3**: We implement another version of Greedy-actor-critic in TD3 style training, where the critic now benefits from TD3 update tricks, and the exploration is performed with OUNoise added to the mean action of the stochastic greedy policy.\n\n## Computational Cost Analysis\n`[Figure 4(d)]` Please refer to combined response.\n\n## Clarifications\n\n> \u201cThe connection between finding the maximum action and improved sample efficiency is not supported\u201d\n- We bring attention to Figure 4(a) and Figure(b) where we already show that SAVO results in an improved Q-value of its actions, which translates into a sample efficiency improvement when evaluating SAVO with all its actors v/s only a single actor.\n\n> \u201cWould it introduce overestimation?\u201d\n- No. SAVO only improves the actor\u2019s capability to find the Q-value maximizing action, which is the key goal of the base algorithm of DPG / DDPG / TD3.\n- There is still exploration on top of the action found by SAVO, so the exploration is not hindered.\n\n\n\n> \u201cIt seems at each time step, the algorithm update both policy and critic parameters K times?\u201d\n- No. We clarify that, as shown in Lines 16 and 17 of Algorithm 1, each individual $Q_i$ and $\\pi_i$ is only updated once every iteration. $K$ denotes the number of actor-critic pairs, as written in the first line of Algorithm 1. So, the for loop in line 15 is to denote all the actor-critic pairs. To further clarify, all the baselines and ablations are based on the same algorithm design and implemented in the same code file, for fairness.\n\n### [References]\n- Yan et al. Learning probabilistic multi-modal actor models for vision-based robotic grasping. ICRA 2019."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9227/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740826964,
                "cdate": 1700740826964,
                "tmdate": 1700742735693,
                "mdate": 1700742735693,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]