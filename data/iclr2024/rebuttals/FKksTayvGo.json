[
    {
        "title": "Denoising Diffusion Bridge Models"
    },
    {
        "review": {
            "id": "fFrFuCqNzV",
            "forum": "FKksTayvGo",
            "replyto": "FKksTayvGo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2089/Reviewer_BgL3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2089/Reviewer_BgL3"
            ],
            "content": {
                "summary": {
                    "value": "The authors describe diffusion generative model as a time reversal of a Brownian bridge forward process, where the Brownian bridge end points are paired samples from a coupling of two marginal distributions. The generative model is learnt as a time reversal of a Brownian bridge, amortized across pairs by learning the score conditioned on an end point.\n\nThe authors show strong empirical results and adapt parameterization of networks from current state of the art diffusion models.\n\nThis is closely related to existing diffusion model, diffusion bridge models and bridge matching methods.\n\nEssentially this is a regular denoising diffusion model where the forward and backward process is conditioned on the terminal point. This is also a conditional bridge matching approach where the reverse bridge is matched."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors show excellent empirical performance of the method and adapt network parameterization from current state of the art diffusion models to bridges.\n- The authors extend the time reversal approach of [1] to the generative setting by amortizing the time reversal across multiple pairs of points from the marginal distributions. Whereas [1] uses general and in particular nonlinear forward process, this work uses a Brownian bridge where the bridge conditioned on end points can now be sampled in closed form and the forward h-transform is also known. Although this is not useful for the applications considered in [1], it is useful for generative modelling where one does not care about the reference process.\n- The work is well explained and is essentially a continuous time version of [2], which does not detail the time reversal interpretation very well and does not condition on the end marginal in the network, but does in the h-transform.\n- Ultimately this is a conditional bridge matching approach which seems to empirically have a better coupling than non conditioned bridges.\n\n[1] Heng et al. Simulating Diffusion Bridges with Score Matching, 2021 \\\n[2] Li et al BBDM: Image-to-Image Translation with Brownian Bridge Diffusion Models, 2022"
                },
                "weaknesses": {
                    "value": "**Novelty**\nUltimately this work trains a conditional diffusion model through score matching, and conditioning on the terminal state x_T from a given coupling. The difference to regular diffusion models is simply conditioning the forward process as well as the backward process with additional information from a coupling. This can also be viewed as conditional bridge matching, by matching the reverse bridge.\n\nAlthough I very much like the connection to bridge matching, time reversal and diffusion bridges, this is not discussed to a satisfactory standard.\n\n**Lack of discussion to prior work**\n\nThe authors' work severely lacks a detailed related works section. Many related works and contributions have been briefly commented in passing. Without proper acknowledgement prior work, the authors' contributions appear inflated. \n\n1) Lacking reference [2], this appears to be the same idea but for discrete interpretation of diffusion models / bridge, derived through a variational approach similar to DDPM rather than time reversal - but which are well known to be equivalent. [2] does not condition on the end marginal in the network, but does in the h-transform.\n\n2) Lack of discussion to bridge matching prior work\n\nThere is a lack of discussion to [1,4,5,6,9], which the authors are aware of given these papers are cited. Although phrased differently they appear highly related to this work. [1,4,6,9] similarly sample from a coupling and train a network for the drift of the of the backward or forward diffusion process (which also involves a h transform), but not conditioned on a one of the marginal points. This work considers learning the the drift of the backward process by learning the score (the score and h-transform (forward drift) make up the reverse drift, as shown in [5]), but instead conditioned on the starting point, x_T. I imagine this work is essentially a conditional bridge matching approach.\n\nAs discussed above, [5] is highly related in that the high level approach is the same: train a network between two points by reversing a diffusion bridge. The differences lie in [5] focusing on general / nonlinear forward process for sampling bridges rather than generative modelling. [5] requires simulating the forward diffusion as nonlinear forward bridges cannot be sampled in closed form. [5] is for a fixed pair rather than amortized across pairs. Indeed, to further illustrate the similarities **Equation 6 within Theorem 1 of this work appears to coincide with the equation written below Equation (4) in Section 2.1 of [5].**. \n\n3) Misrepresenting prior work\n\nThe authors claim \"A related work (Somnath et al.,2023) similarly establishes a diffusion bridge that translates between two distributions and has seen success in protein differentiable domain, but the training objective requires sampling an entire SDE trajectory for computation\".\n\nAs far as I am aware this is not true. I do not understand how the authors came to this conclusion.\n\nMinor:\n-  is it not clear why [3] is being cited for the Schrodinger bridge interpretation of diffusion models. The approach of [3] is not a Schrodinger bridge between marginals but two diffusion models back to back with a Gaussian between, there are significantly more relevant works. If anything this is misleading the reader.\n\n- \"A recent work (Liu et al., 2023) considers a special case of SB\" . I2I SB [9] does not actually result in a Schrodinger Bridge, neither does the same method detailed in Aligned SB [1]. They both perform bridge matching with respect to Brownian bridge reference diffusion but for a data driven coupling. Given the data driven coupling does not correspond to the coupling from Brownian motion, the bridge matching procedure returns a Markovian projection and hence breaks the coupling. See discussion in [7] \"the Schr\u00f6dinger Bridge is the unique path measure which satisfies the initial and terminal conditions, is Markov and is in the reciprocal class of Q, see (L\u00e9onard, 2014b).\" One needs to iterate on the coupling and drift in order to obtain a SB. \n\nThis is a tangential remark as one does not need an optimal diffusion in order to perform conditional generation if it can be supervised with given paired samples from a coupling. However there appears to be many errors in papers being published in this area and I feel this should be addressed.\n\n**Claim of generalizing flow matching/ bridge matching**\n\nThe connection of diffusion bridges to flow matching has been established and detailed in [7], detailing the limit as the noise coefficient goes to 0. The original flow matching paper [8] is even derived using result from [4] on bridge matching, so this is widely known. Note: [7] was public well before submission but not published.\n\nHowever, given the score function in this work is conditioned on x_T, the learnt backward diffusion is non-Markovian, hence retains the initial marginal coupling and would not recover rectified flow / schrodinger bridge by iterative Markovian fitting as in [7] or in rectified flow. So I would argue this work does not subsume flow matching / drift matching.\n\n**Summary**\nWhilst I like the core idea of this paper and believe there is a contribution, I believe \"contextualization relative to prior work\" is lacking.\n\n[1] Somnath et al 2023, Aligned Diffusion Schr\u00f6dinger Bridges \\\n[2] Li et al 2022, BBDM: Image-to-Image Translation with Brownian Bridge Diffusion Models \\\n[3]  Su et al. 2022, Dual Diffusion Implicit Bridges for Image-to-Image Translation \\\n[4] Peluchetti 2022, Non-Denoising Forward-Time Diffusions \\\n[5] Heng et al. Simulating Diffusion Bridges with Score Matching \\\n[6] Liu et al., 2022, Let us Build Bridges: Understanding and Extending Diffusion Generative Models \\\n[7] Shi et al Diffusion Schr\u00f6dinger Bridge Matching 2023 \\\n[8] Lipman et al 2022, Flow Matching for Generative Modeling \\\n[9] Liu et al 2023, I2SB: Image-to-Image Schro \u0308dinger Bridge"
                },
                "questions": {
                    "value": "How does this work relate to bridge matching? Can the same result not be achieved through conditional bridge matching?\n\nIs it true this work is a continuous time version of [2]?\n\nAm I right in thinking that Theorem 1 equation (6) is the same as that derived in Section 2.1 of [1]?\n\n[1] Heng et al. Simulating Diffusion Bridges with Score Matching, 2021\n[2] Li et al BBDM: Image-to-Image Translation with Brownian Bridge Diffusion Models, 2022"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I have concerns about lack of discussion of highly related and cited work. Whilst I hope this is not intentional, without proper discussion of prior work, the contributions appear inflated."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2089/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2089/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2089/Reviewer_BgL3"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2089/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698599967970,
            "cdate": 1698599967970,
            "tmdate": 1700672320024,
            "mdate": 1700672320024,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nTEens9HaH",
                "forum": "FKksTayvGo",
                "replyto": "fFrFuCqNzV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your detailed and insightful review (Part 1)."
                    },
                    "comment": {
                        "value": "We thank the review for pointing out our paper\u2019s lack of discussion of related works. We greatly appreciate the thoughtful suggestions. Due to space limitation of the main text, we could not properly discuss the large body of related works in detail. According to suggestions from you and reviewer H8Nr, we have included a revised version of related works in the new supplementary files and we intend to replace the corresponding section in main text with the revised version. Please be sure to take a look.\n\nWe first briefly summarize the mentioned references and contextualize our contributions before addressing individual concerns from the reviewer.\n- [1] learns to bridge two distributions by matching Doob\u2019s h-function and proposes a simulation-free training procedure for training. [4] proposes a similar approach which learns a generative model by constructing a mixture of bridges where the intermediate $x_t$ can be marginally sampled. Both works propose to generate via forward-time simulation with the trained model. Our method instead adopts a reverse-time perspective. The key benefit from this perspective is the ability to integrate key diffusion model techniques into the bridge framework, such as an accurate higher-order sampler and network preconditioning. Since these previous works are developed with other perspectives, it is less obvious how one would integrate these diffusion model design choices and achieve empirical improvement. We also extend beyond the Brownian bridge construction (as used in [1,2,6,7], which is a special case of VE bridge) and show that both VE and VP bridge can achieve competitive results than state-of-the-art. \n\n- [2] proposes to connect two image domains via discrete Brownian bridges, and proposes to reverse the bridge for generation. This is a discrete-time special case of the VE bridge we consider, and our method also shows success on VP bridges that work well in continuous time. In addition, our sampler takes much fewer steps for best generation results.\n\n- [5] adopts a reverse-time perspective of diffusion bridges pinned on both ends and proposes score-matching for inference time simulation. Our method considers a more general case where both ends are drawn from distributions rather than fixed, and our results are developed for this case. In addition, [5] requires simulation of an entire trajectory for score-matching training, while our method extends directly from diffusion models and naturally results in a simulation-free training procedure\n- [6] is also built on learning Doob\u2019s h-function, similar to [1]. However, they also propose to perform forward time simulation during training for matching against the Doob\u2019s h-function, which we avoid.\n- [7] proposes bridge-matching and Iterative Markovian Fitting, an iterative procedure for solving the Schrodinger bridge problem. This procedure involves an inner loop of optimization and simulation, with the exception of the first iteration where the intermediate $X_t$ can be marginally sampled given endpoints. Our method is different in that our method is completely simulation-free, and directly extend diffusion models for the bridge construction. We further discuss connections in the main response.\n- [8] is a normalizing-flow based generative model which relies on deterministic ODEs for generation. Our method is instead based on SDEs, and we also show the introduced stochasticity in our sampler is integral for empirical success. In the noiseless limit of our method with the VE bridge, our method reduces to the method introduced in [8].\n- [9] is an image-to-image method based on a special case of SB which is tractably computable and also results in a simulation-free algorithm. Our method is functionally similar but theoretically different. Different from [9], our method can theoretically subsume other classes of models and directly adapt successful design choices to improve quality and speed. We also empirically outperform them on image translation.\n\nWe now address our the reviewer\u2019s concern below.\n\n> Is our model a continuous-time version of (Li et al., 2022)?\n\nOur method is a superset of the method proposed in (Li et al., 2022). Their work proposes a simple Brownian bridge formulation in discrete time and seeks to reverse the discrete-time bridge using ancestral sampling adopted from diffusion models. In continuous-time, their Brownian bridge formulation is a special case of our VE bridges. Conversely, our VP bridges, which generalize VP diffusions, are separate and are also shown to work well empirically."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379801517,
                "cdate": 1700379801517,
                "tmdate": 1700379801517,
                "mdate": 1700379801517,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w0vsPZqgWh",
                "forum": "FKksTayvGo",
                "replyto": "fFrFuCqNzV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your detailed and insightful review (Part 2)."
                    },
                    "comment": {
                        "value": "> Discussion with bridge-matching.\n\nWe want to note that our work was concurrently developed with bridge matching (Shi et al., 2023) and some property of our method may seem similar. Bridge matching directly aims to solve Schrodinger Bridge problem and find the minimum cost bridge through Iterative Markovian Fitting, which relies on an inner loop for optimizing bridge approximation and simulating the bridge trajectory (with the exception of the first iteration where they marginally sample intermediate $X_t$ ). It is shown to be a superset of Flow-Matching. Somnath et al. (2023) also adopts a learning procedure similar to the first iteration of bridge matching, and learns to fit Doob\u2019s h-function. We also acknowledge that a similar procedure is developed by Peluchetti, (2022) which shows a model can learn a mixture of bridges to map from one distribution to the next.\n\nOur framework is related in that we also build from Doob\u2019s h-transform for bridging between distributions and in theory propose to match scores of intermediate distributions. However, we note that we do not try to solve the Schrodinger Bridge problem to find the optimal coupling but try to directly use a preexisting diffusion schedule to establish this coupling. This allows us to reuse previously successful design choices for diffusion models and generalize them for the bridge case. This coupling (Eq. 8) is also written in light of diffusion models which sheds light on its connection with the underlying diffusion process. Both Shi et al. (2023) and Somnath et al. (2023) empirically consider Brownian bridge for connecting two distributions. We go beyond this simple choice and investigate an alternative VE bridge and a VP bridge design. The VE bridge uses different interpolation velocity inspired by EDM (Karras et al., 2022), which is crucial for our higher-order sampler. The VP bridge directly builds on the VP diffusion model (which few prior works consider), and we similarly empirically show its success on the same tasks. Since our method is directly built on diffusion models, we generalize successful designs such as high-order sampler, better time-discretization, and different noise-scheduling to show that a bridge framework is similarly powerful equipped with these generalized design choices. Therefore, our framework is both theoretically and empirically motivated, which we hope contributes to the larger body of works to push bridge-based frameworks further.\n\n\n\n\n> Discussion with [1,4,5,6,9] as cited.\n\nWe have included their discussion in our write-up in the new supplementary. It is true that these works all seek to establish bridges from one distribution to another. [1,4,6] are built on Doob\u2019s h-transform and share the high-level idea that one can match the Doob\u2019s h-function given intermediate sample $X_t$, and the learned network can guide one towards high-likelihood regions during inference. [1,4] consider marginally sampling any intermediate $X_t$ while [6] consider simulating a path for loss calculation. [9] is built on a special case of Schodinger Bridge when one distribution is reduced to Dirac delta distribution, although empirically it is shown to work well and results in a similar algorithm for marginally sampling $X_t$ for score-matching. [5] is also highly related in that, as kindly pointed out, it reverses the diffusion bridge trajectory via score-matching. Different from more recent approaches, it simulates a forward trajectory for obtaining intermediate samples $X_t$. \n\nOur work is similar in theory to these mentioned prior works as we are also built on learning bridges via h-transform. Different from them, our work is theoretically developed by directly extending diffusion models in the hope that it can extend the empirical success of diffusion models (both in terms of quality and efficiency) on high-dimensional image synthesis tasks. The reverse-time formulation is crucial for generalizing previous successful designs to push the empirical limit on high-dimensional image translation with the bridge framework. Interestingly, the reverse-time perspective allows us to connect with different generative frameworks such as diffusion models and Flow-Matching/Rectified Flow.\n\n> Misinterpreting (Somnath et al.,2023) \n\nWe appreciate the reviewer pointing this out. This mistake has been corrected.\n\n> Should cite other more related works than [3].\n\nThank you for pointing this out. We will more prominently discuss other more related works as suggested.\n\n> I$^2$SB is not SB?\n\nPrecisely speaking, according to Liu et al. (2023), the class of SB they consider is a \u201ctractable SB with the Dirac delta boundary\u201d (Corollary 3.2), which assumes special structure on data distribution for the algorithm to be tractably handled. We do agree that Aligned SB do not result in SB and is doing bridge matching (as the first iteration specified in the algorithm in Shi et al. (2023))."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700380050795,
                "cdate": 1700380050795,
                "tmdate": 1700380050795,
                "mdate": 1700380050795,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9TjcRko70S",
                "forum": "FKksTayvGo",
                "replyto": "fFrFuCqNzV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your detailed and insightful review (Part 3)."
                    },
                    "comment": {
                        "value": "> Does DDBM subsume Flow-Matching/Rectified Flow?\n\nThe additional input of $x_T$ is a helpful design choice that guides generation towards the data distribution. This is mostly a practical decision. However, note that our score-matching objective is sound without injecting $x_T$ as additional condition, in which case the reverse-time SDE is Markovian. Since the target score as described exactly reduces to that of Flow-Matching (the OT formulation) and Rectified Flow, our framework reduces these special cases. We have noted this point in our additional write-up. \n\n> Is Theorem 1 different from Section 2.1 in (Heng et al. 2021)?\n\nSection 2.1 in Heng et al. 2021 presents time reversal for a single diffusion bridge (pinned on both ends). Our Theorem 1 generalizes it to tackle $x_0$ drawn from a distribution. In addition, we present the probability-flow ODE version (which is crucial for our ODE sampler) that produces the same marginal distribution as the SDE."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700380104574,
                "cdate": 1700380104574,
                "tmdate": 1700380104574,
                "mdate": 1700380104574,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pD6Zs2oFc9",
                "forum": "FKksTayvGo",
                "replyto": "RuNUyq0ff8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2089/Reviewer_BgL3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2089/Reviewer_BgL3"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for these comments, I agree with a lot and have many of the same questions / concerns."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583936478,
                "cdate": 1700583936478,
                "tmdate": 1700662110548,
                "mdate": 1700662110548,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "45HnXv0EZA",
                "forum": "FKksTayvGo",
                "replyto": "fFrFuCqNzV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We have addressed the concerns in Peluchetti's comment."
                    },
                    "comment": {
                        "value": "We have addressed the concerns in Peluchetti's comment. Let us know if anything else can be addressed."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662065726,
                "cdate": 1700662065726,
                "tmdate": 1700662065726,
                "mdate": 1700662065726,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w89d2XFpSt",
                "forum": "FKksTayvGo",
                "replyto": "45HnXv0EZA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2089/Reviewer_BgL3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2089/Reviewer_BgL3"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response and engagement - I appreciate the thorough discussion.\n\nI will properly read the discussion / new draft and update my score appropriately."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662789885,
                "cdate": 1700662789885,
                "tmdate": 1700662789885,
                "mdate": 1700662789885,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4VYIjBFh5t",
                "forum": "FKksTayvGo",
                "replyto": "fFrFuCqNzV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2089/Reviewer_BgL3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2089/Reviewer_BgL3"
                ],
                "content": {
                    "comment": {
                        "value": "Minor comments:\n- **\"We want to note that our work was concurrently developed with bridge matching (Shi et al., 2023)\"** \\\nWhen I refer to bridge matching; I really mean the work of Peluchetti; Qiang Liu etc. Shi et al perform iterative bridge matching to get to the SB. Similar to iterated flow matching gets to rectified flow (reflow)\n\n- **\"Precisely speaking, according to Liu et al. (2023), the class of SB they consider is a \u201ctractable SB with the Dirac delta boundary\u201d (Corollary 3.2), which assumes special structure on data distribution for the algorithm to be tractably handled. We do agree that Aligned SB do not result in SB and is doing bridge matching (as the first iteration specified in the algorithm in Shi et al. (2023)).**\n\nI think Liu et al and Aligned SB have almost identical losses in the main text. Liu et al 23 also has an extension conditioning on a marginal point too. The brownian bridge between points from an OT coupling with respect to brownian reference measure is SB; but Liu et al use a data driven coupling which is not brownian optimal, hence not a SB. Indeed, the coupling between diracs is trivial so one could argue it is a mixture of SB between diracs, but this is not a SB between the continuous support marginal measures. This is out of scope of this review, but I think it's important to mention here and not necessarily in the paper - there are lots of errors being propagated in this literature.\n\nActually even some of the authors of Liu et al 23 recognise this is not a SB in recent work [10] \"In Somnath et al.\n (2023); Liu et al. (2023), it remains debatable whether the training dataset pairs inherently represent the solution to the static Schro \u0308dinger Bridge equation. In particular, this assumption is not ensured in real-world applications and can be easily violated even in low-dimensional cases as shown in Figure 1. Hence, the bridge matching procedure does not preserve the original coupling in general.\"\n\n\n-----------------\n\nLess Minor:\n\n- **[8] is a normalizing-flow based generative model which relies on deterministic ODEs for generation. Our method is instead based on SDEs, and we also show the introduced stochasticity in our sampler is integral for empirical success. In the noiseless limit of our method with the VE bridge, our method reduces to the method introduced in [8]**\n\nAgain I think there is actually a big distinction that this is not the case given your method conditions on x_T - this is completely fine but I think it's important to make it clear.\n\nFor example, iterated versions of your method similar to Shi et al Iterative Markovian Fitting will not result in a SB but retain the initial coupling (whether independent or data driven). Very recent work appearing after submission details this distinction [10]\n\nI actually think one of the contributions of your work is indeed that you retain the coupling; this could be useful.\n\n- **Section 2.1 in Heng et al. 2021 presents time reversal for a single diffusion bridge (pinned on both ends)**\nHeng et al first presents the time reversal with a single marginal (x_0) pinned. To me this coincides to what is here? In particular the relationship between conditioned score; unconditioned score and h-transform. Then Heng et al shows if you initialize your reverse diffusion from x_T, the resulting process is a bridge from x_T to x_0. \n\n[10] Augmented Bridge Matching https://arxiv.org/abs/2311.06978\n\n-----------------\n\n**Summary**\n- Overall I think it's a good piece of work; for performance alone it is valuable to the community. \n- The discussion of prior work has been addressed to a satisfactory degree, which was my main concern. I do think it would be better to have it in the main text. I sympathise that there is a space restriction.\n- I think Peluchetti has some valid points - I appreciate his comment but also that it is not his place to push too much. \n- It is still a mystery whether the performance gap between forward bridge matching vs reverse bridge matching comes from as it does seem they are equivalent; perhaps learning the score as oppose to drift is easier? (the difference is only the h-transform). Perhaps there are better parameterisations of forward bridge matching that work equally well and the empirical results of this work paves the way for that.\n- I also still believe the connection between the forward/ reverse bridges through the unconditional score; conditional score and h-transform was already established in Heng et al 2021 which should be acknowledged (if it is the same); though the motivation was different and experiments less impressive. This work is a lot clearer and more suitable for the ML community for generative modelling.\n- I disagree that this work generalizes FM; BM etc which do not retain data driven couplings. I think the stronger selling point for this work is that it retains the coupling used in training at deployment time rather than breaking it as is done in prior work like I2I SB\n\nI have raised my score."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672084245,
                "cdate": 1700672084245,
                "tmdate": 1700673016932,
                "mdate": 1700673016932,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8CaoqRXCZ2",
                "forum": "FKksTayvGo",
                "replyto": "fFrFuCqNzV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2089/Reviewer_BgL3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2089/Reviewer_BgL3"
                ],
                "content": {
                    "comment": {
                        "value": "I want to stress, that although I like Shi et al 2023 and they have some very nice and relevant insights; that paper does not invent bridge matching but instead iterated bridge matching similar to reflow. The paper under review here does not perform iterative bridge matching. There are significantly more relevant papers on bridge matching than Shi et al.\n\nIn addition, although I feel that papers should fully acknowledge all related work whether published or not. I understand this work under review here was also submitted to neurips concurrently to Shi et al. 2023. Furthermore; although Shi et al 2023 was accepted to neurips - the proceedings are not yet released or at least not at the time of submission. Hence under ICLR guidelines the authors may be forgiven for not fully discussing or comparing to it.\n\nQ: Are authors expected to cite and compare with very recent work? What about non peer-reviewed (e.g., ArXiv) papers? (updated on 7 November 2022)\n\nA: We consider papers contemporaneous if they are published (available in online proceedings) within the last four months. That means, since our full paper deadline is September 28, if a paper was published (i.e., at a peer-reviewed venue) on or after May 28, 2022, authors are not required to compare their own work to that paper. Authors are encouraged to cite and discuss all relevant papers, but they may be excused for not knowing about papers not published in peer-reviewed conference proceedings or journals, which includes papers exclusively available on arXiv. Reviewers are encouraged to use their own good judgement and, if in doubt, discuss with their area chair. \nhttps://iclr.cc/Conferences/2023/ReviewerGuide"
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741125352,
                "cdate": 1700741125352,
                "tmdate": 1700741260284,
                "mdate": 1700741260284,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "imq0RfrzwR",
            "forum": "FKksTayvGo",
            "replyto": "FKksTayvGo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2089/Reviewer_2Uac"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2089/Reviewer_2Uac"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new class of generative models, Denoising Diffusion Bridge Models (DDBM), that can define a generative path between any 2 paired distributions. This is in contrast to current diffusion models where the path is always from Gaussian to a data distributions. DDBMs are a more generic representation, and they allow solving tasks like image-to-image translation (which is not trivial to do with regular diffusion models). DDBMs work by building a stochastic bridge between a paired samples. DDBMs share several attributes with diffusion models, which allows reusing many of the techniques already available in this field. These new models can be trained in a similar fashion as score matching, and the authors present extensive theoretical backing to allow for a loss formulation and transport between distributions via a reverse SDE and flow ODE. The authors show results in image-to-image tasks with reasonable results. The authors also show competitive results when running tasks currently done by diffusion models, mainly text-to-image generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors tackle an important and challenging problem, mainly how do we use our current generative pipelines (diffusion) to solve more general generative problems. The authors do a good job at presenting a sounds alternative. I highlight the following strengths:\n- The formulation of DDBMs seems solid. The authors present a good comparison between this method and score matching and flow matching. In fact, the authors show that DDBMs are a generalization of these methods. \n- The authors formulate the model in a way that shares many commonalities with current diffusion models. I appreciate this effort for two reasons. First, it makes understanding and adopting them easier given the familiarity they have with current methods. Second, it reuses many of the formulations already developed in things like score matching, which should lead to better training. \n- The authors go to good length to present and explain all relevant mathematical formulations. These look sound, although I did not fully explore all the details.\n- The authors show favorable results in image-to-image tasks and comparable results in text-to-image generation. Given that these come from the same model formulation (DDBMs), this combination of results is a strength of the formulation.\n- The paper is well written and structured."
                },
                "weaknesses": {
                    "value": "I have 2 concerns I believe are minor, but would like to hear from the authors:\n- The authors only validate the work with image-to-image translation. In this domain, ControlNet is the dominant solution. I value that the 2 models might not be competing solutions. Can the authors clarify why they didn't compare against ControlNet in the image-to-image task?\n- With generalized flow matching models we can in theory move between any distributions. What is the advantage of DDBMs compared to flow matching? I found some reasoning in the paper but didn't fully understand, therefore I appreciate a brief explanation from the authors.\n\nOverall I believe this are minor issues and I hope the authors provide answers during rebuttal."
                },
                "questions": {
                    "value": "- Why didn't the authors compare against ControlNet?\n- What is the advantage of DDBMs when compared to FM given that both can in theory model flow between arbitrary distributions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2089/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767307369,
            "cdate": 1698767307369,
            "tmdate": 1699636141308,
            "mdate": 1699636141308,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iRQlGe8fOz",
                "forum": "FKksTayvGo",
                "replyto": "imq0RfrzwR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review and appreciation."
                    },
                    "comment": {
                        "value": "Thank you very much for your appreciation of our work! We want to address the remaining concerns below.\n\n> Why not comparing with ControlNet?\n\nYou are correct. Our framework is orthogonal to ControlNet, which is an architectural design at its core. Our framework can also use ControlNet architecture and start from pretrained diffusion weights for the image-to-image tasks. However, due to limited resources and time, we did not proceed to larger scale experiments using ControlNet. Nevertheless, we do plan to scale up our framework using more popular architectures in the future.\n\n> What is the advantage of DDBM compared to Flow-Matching?\n\nIn theory they both aim to learn a model to move between distributions. However, we designed our framework with the practical motivation about how can can maximally transfer empirical success of diffusion models to the bridge framework, which has seen limited empirical success in these high-dimensional image translation tasks. The framework is developed to allow reuse/generalization of many prior successful implementation choices such as the preconditioning in EDM and higher-order sampler, which Flow-Matching lack. With these choices, our framework can achieve better performance in quality and speed."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379572136,
                "cdate": 1700379572136,
                "tmdate": 1700379572136,
                "mdate": 1700379572136,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pMMAmW6S3g",
            "forum": "FKksTayvGo",
            "replyto": "FKksTayvGo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2089/Reviewer_H8Nr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2089/Reviewer_H8Nr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a means of adapting standard denoising diffusion models, which parameterize the score in the Ornstein Uhlenbeck process to map a Gaussian to a data distribution, so that it can be more properly used to connect arbitrary densities, e.g. for image-to-image translation.\n\nThe main tool at play is Doob's h-transform, which they use to derive an SDE that has a learnable score conditioned on some endpoint $x_T$. They write a variant of the score matching loss that accommodates this. \n\nFollowing this, they address the usual topics of a SBDM paper, which is how to re-write the score in a way that allows you to predict $x_t$, how to consider the signal to noise ratio in the diffusion path (which is not a very meaningful name in the case of the ODE), how to write the probability flow, and what sampler to use.\n\nThey benchmark the method on various image-to-image translation tasks, quantifying image quality metrics with a consideration for the number of function calls necessary to achieve said quality.\n\n\n\n--generalized time reversal is not proven to converge to the correct distribution. Equation 13 should be shown to be justified. \n\n\n\n-- can only do this given a pair"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "In the specific context of denoising (score-based) diffusion models (DDPM),  this paper introduces the useful perspective from the bridge literature of how to alleviate the limitation of DDPM as a generative model in that it could only connect a data distribution to a Gaussian. \n\nThe experimentation is thorough which shows that the method is performant on the image tasks presented. The ablation study is useful (even if a bit contrary to their claims in the abstract of wanting to move away from \"cumbersome methods like guidance\"). The authors usefully summarize how many of the tricks that make diffusions highly performant fit into their extension."
                },
                "weaknesses": {
                    "value": "Let me begin by saying, thank you for the effort you put into this. The experiments are very thorough, and the doob-h formulation is sound.\n\nUnfortunately, this paper is written in a way that highlights some misconceptions. It is unclear what the authors see as the contribution of this paper. It has become clear from the wider literature than just the narrow perspective of the same score-based diffusion equation how to do image-to-image translation. In fact, the works that this paper already cite do this, even for both the ODE and SDE between arbitrary densities. \n\nA quick summary of related work that should be properly addressed in this paper, and the authors should highlight what actually sets this method apart:\n- [1,2,3] propose a means of learning an ODE to do connect any two densities, e.g. for your image-to-image translation in finite time $ t \\in [0,1]$ without bias. In fact [1] does this image-to-image translation experiment there directly.\n- [4] shows how to do this for **both** an ODE and an SDE (with the score), also in finite time. It also shows how to avoid the added complexity of doob, and how SBDM is a subcase.\n- [5, 6] shows the influence of changing the coupling $p(x_0, x_1)$ between the densities $p(x_0)$, $p(x_1)$\n- [7] also shows how to use either and ODE or SDE to connect the densities under varying coupling. \n\nThe related worked section, in addition to the intro, should be thoroughly reworked to not overlook these clear contributions. While it is certainly beneficial to have a clear connection to how to do this with SBDM using doob-h and experiments of it, the paper is currently written so as to suggest that there is a clear need to come up with tools to do this as if many works have not addressed it. The reviewer is sympathetic to the fact that this field moves very fast, but also believes that overselling a concept by overlooking other work is detrimental to the field. \n\nWith regards to the statement that the ODE based methods [1,2,3] \"tend to severely underperform when compared to diffusion models,\" there is no evidence of this presented, and in fact the purpose of those papers is to present evidence of the contrary, e.g. see [3]. If the authors would like to make this statement, they should demonstrate it in experiments. It is of the reviewers mindset that the differentiating factor between most simulation-free transport generative models is just whether or not the model was conditionally trained, e.g. as $s(x,y,t)$ vs $s(x,t)$. Many papers report \"unconditional sampling\" FIDs by training a conditional model and using a null-token e.g. $y=-1$ to sample unconditionally, which improves their score.\n\nThere is perhaps a misinterpretation of what optimal transport is, and what role it may play in generative models that are constructed from continuous-time transport plans. The authors refer to \"VE (OT)\" bridges without providing a clear definition of what they mean by OT. Here are the two ways to learn an optimal transport between distributions:\n\n- 1. Choose the optimal coupling $p(x_0, x_1)$, with \"schedule\" $x_t = (1-t)x_0 + tx_1$. This is hard to do, and why people rely, e.g. on Sinkhorn.\n- 2. For independent coupling $p(x_0)p(x_1)$, learn a process $x_t = f_{\\theta}(t,x_0,x_1)$ which gives a time dependent velocity/score and density $v_t, p_t(x)$ that induces a minimum action (or least transport cost) in terms of Benamou-Brenier transport cost.\n\n- In the abstract, the authors reference \"OT-Flow-Matching\", though the work [3] does not propose to solve OT. At most ,they use a relationship about McCann displacement maps (which is related to OT from a Gaussian to a Gaussian) to motivate choosing a straighter conditional probability path -- they are not doing OT. \n\n\n-Throughout the text, the authors need to make sure all acronyms are defined.\n\n- The introduction of the weighting factor in equation (13) is not shown in the text to preserve an exact/unbiased transport. The authors should justify this or state that it introduces a bias, even if a beneficial one.\n\n\n*The reviewer is not opposed to adjusting their score, but there are many organizational and presentational aspects of this work that suggest it is not fit for contribution to the larger body of work at ICLR at this stage. Please find questions below that could help better demonstrate the utility of the method (in controlled experiments or new ones that e.g. exploit an interesting coupling) as well as the remarks above that need addressing.*\n\n[1] Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. *Xingchao Liu, Chengyue Gong, Qiang Liu*, Sept 2022.\n\n[2] Building Normalizing Flows with Stochastic Interpolants. *Michael S. Albergo and Eric Vanden-Eijnden*, Sept 2022.\n\n[3] Flow Matching for Generative Modeling. *Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, Matt Le*, Oct 2022.\n\n[4] Stochastic Interpolants: A Unifying Framework for Flows and Diffusions. *Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden*, March 2023.\n\n[5] Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport. *Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, Yoshua Bengio*, Feb 2023.\n\n[6] Multisample Flow Matching: Straightening Flows with Minibatch Couplings. *Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, Ricky T. Q. Chen*, April 2023.\n\n[7] Simulation-free Schr\u00f6dinger bridges via score and flow matching. *Alexander Tong, Nikolay Malkin, Kilian Fatras, Lazar Atanackovic, Yanlei Zhang, Guillaume Huguet, Guy Wolf, Yoshua Bengio*, July 2023."
                },
                "questions": {
                    "value": "- The authors write down the doob-$h$ transformed SDE for arbitrary coupling, but it's not clear in the experiments that they consider any coupling besides $p(x_0, x_1) = p(x_0)p(x_1)$. Can the authors consider any experiments which make use of a more interesting coupling?\n- An important ablation should be based on the sampler the authors used. Did the authors retrain the models they compared against and fixed e.g. the sampling strategy? They discuss a higher-order Euler-Maruyama sampler, but it is unclear if all the comparison to other methods used the same integrator. Many of the methods listed could use a different integrator for the same trained model, and it's unclear to the reviewer what was held fixed across comparisons and what wasn't. The reviewer points this out to stress that a paper is not a method -- the method presented in the paper should be recognizably connected to the conclusions of the experiments. If an auxiliary factor influenced the outcome, e.g. like choosing a better sampler, this diminishes the message about the method itself."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2089/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2089/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2089/Reviewer_H8Nr"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2089/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802515506,
            "cdate": 1698802515506,
            "tmdate": 1700673894790,
            "mdate": 1700673894790,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sm96dO0nHR",
                "forum": "FKksTayvGo",
                "replyto": "pMMAmW6S3g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your detailed and insightful comments. (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you very much for your detailed review and suggestions. We greatly appreciate your time and expertise. In addition to clarifying a few points of concern, we also wish to note that we have a reworked version of related works in the new supplementary docs. We wish to properly contextualize our framework within a larger body of works as mentioned.\n\nWe want to start by briefly summarizing the referenced works and addressing important differences to highlight our contributions.\n\n- [1] proposes to learn an OT mapping between two different distributions by approximating the time-invariant velocity field, while our method is not based on OT. [1] is based on an ODE, while we have an SDE for distribution mapping, which in the noiseless limit, reduces to that in [1] in a special case. Our work enables a higher-order sampler and output parameterization that are not directly usable for [1].\n- [2,3] are flow-based and aim to learn transport maps to push forward a prior distribution to data distribution, and utilizes deterministic ODEs for generation. Our theory is developed using SDE and we introduce higher-order sampler with additional stochasticity, which are also not directly usable for these works. The resulting method can further subsume the case in [3] developed with OT displacement map.\n- [4] is a general theory that directly constructs a bridge using an interpolation map and avoids the use of Doob\u2019s h-function. Our method is based on Doob\u2019s h-function from which a direct interpolation map can be constructed for marginal sampling. Furthermore, the training objective is quite different as ours is based on denoising score-matching without any regularization, while theirs is not. Our construction also shows stronger tie with diffusion models, which allow extending existing designs for the new bridge construction. These include reusing noise schedules, speicalized higher-order sampler, and network preconditioning. It\u2019s not clear if these choices can be directly applied to [4].\n- Extending [3], [5,6] are similarly flow-based methods that show success on exploiting potential couplings between the two distributions via minibatch simulation-free OT plan. Again, our method is fundamentally based on SDE and does not aim to solve OT. Our simulation free sampling is directly constructed from VP and VE diffusion processes. [3,5,6] all consider brownian bridge paths as as the interpolation map, while we show that this straight interpolation map is a special case of VE bridge. VP bridge exhibits curved interpolation paths. Nevertheless, when implemented correctly, we show both bridges can perform well in practice.\n- [7] constructs a simulation-free training plan using brownian bridge as the interpolation map, and aims to match against both flow vector field and bridge score. Our method does not only consider brownian bridge as the bridge construction, and shows success for other types of bridge in practice. We only consider a denoising score matching loss without additional flow-matching loss as proposed. A set of practical design choices are also proposed to further improve quality and speed of our bridge model.\n\nIn the following sections we address the reviewer\u2019s concerns one by one.\n\n> What is the contribution?\n\nOur framework aims to show that the bridge-based frameworks, when developed from the perspective of diffusion models, can also achieve competitive empirical results for both high-dimensional image-to-image and unconditional generation, which previous bridge-based frameworks have seen limited success on. The practical motivation inspires the theoretical development in directly extending diffusion models, which naturally results in a simulation-free bridge construction. Furthermore, different from previous works, our method allows for constructing bridges beyond the Brownian bridge (as mostly used in previous works such as [5,6,*]) and shows promising empirical success. Brownian bridges are a special case of VE bridges we consider. Empirically speaking, we design a set of practical choices, such as higher-order sampler with stochasticity, improved network preconditioning, by directly adapting those from prior successful designs, which are not directly applicable to prior bridge-based works.\n\nOur core contribution (in one sentence) is \u201cconnecting bridge-based frameworks with diffusion models to adapt existing diffusion model techniques for improved empirical performance\u201d. We believe that this is important since prior bridge methods have seen limited success on tasks that they should be better suited for."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379390685,
                "cdate": 1700379390685,
                "tmdate": 1700379390685,
                "mdate": 1700379390685,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ibxflzbQyQ",
                "forum": "FKksTayvGo",
                "replyto": "pMMAmW6S3g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your detailed and insightful comments. (Part 2)"
                    },
                    "comment": {
                        "value": "> Properly addressing previous works\u2019 contributions\n\nWe thank the reviewer for the references and we appreciate the reviewer pointing out the lack of properly addressing previous works. We have thoroughly incorporated them in our new related works in supplementary. Please see our comments to all reviewers in the main response comment up above. \n\n> Statement regarding ODE-based methods\u2019 performance. \n\nFor ODE-based methods like the papers [1, 2, 3] mentioned above, we apologize for the misleading statement (which unfairly discredits these papers as being fundamentally unable to compete with diffusion). \n\nHowever, we wanted to emphasize that, although these methods have seen some improvements over the last year, they have yet to see the empirical success that diffusion models are accustomed to. In particular, the only case where ODE methods have consistently outperformed SDE methods (that we are aware of) is in the CIFAR10 dataset, but even then most of the improvements have stemmed from core work in the diffusion model/probability flow ODE space (e.g. EDM). We believe that this gap is fundamentally caused by a difference in research volume: diffusion models have seen many more small improvements (e.g. sampling, network parameterization, noise schedule)\n\nAs such, we have updated this statement to be \u201cODE methods have not achieved the same empirical success as diffusion models\u201d.\n\n\n> Misinterpreting OT? \n\nWe want to stress that our framework is not solving OT, and the VE bridge resembles OT plan in that they share a similar coupling in the form of $(1-t)x_0+t x_1$ in the noiseless limit (with factor changed as $t^2$ instead of $t$ for our case). We have deleted the \u201cOT\u201d in brackets after \u201cVE\u201d in the corresponding section to avoid the potentially misleading meaning. The OT-Flow-Matching we use in our text is only short for the Flow-Matching case in [3] where the author designs their conditional probability path by OT plan. We agree that this is a slight misnomer, and our updates intend to fix any possible misinterpretation issues.\n\n> Bias introduced in the weighting factor\n\nIndeed, the weighting factor introduced causes the process to no longer follow the same marginal distribution as the bridge process if $w\\neq 1$. We empirically find that this factor is useful because it allows us to interpolate between probability-flow ODE of unconditional diffusion model and that of a bridge model (by setting $w=0.5$ vs $w=1$), which is beneficial for unconditional generation with the ODE sampler generalized from EDM.\n\n\n> Different coupling between x_1 and x_0.\n\nNote that for image translation tasks we assumed distribution coupling of the form $p(x_0, x_1)=p(x_0|x_1)p(x_1)$ (e.g. colored image samples depends on edge map) while for unconditional generation tasks we consider $p(x_0, x_1)=p(x_0)p(x_1)$. In both cases, we construct VP or VE bridges (coupling derived from underlying VP and VE processes), and we find both cases work well in practice for translation and unconditional generation.\n\n      \n\n> Questions on sampler used in experiments.\n\nWe present the ablation on our sampler (and preconditioning effect) in Table 3. We do not retrain our model for sampler ablation studies. We observe that our higher-order sampler performs better than the EDM ODE integrator out-of-the-box. \n\nWhen comparing with other methods, our motivation is to \u201cstrongman\u201d the baselines. In particular, we need to compare against the best configuration for all models because the practical design choices have been key for empirical success. In particular, these extend beyond the loss function and SDE/ODE formulations.\n\nWe therefore use our best configuration to compare against the best of other methods on standard benchmarks, and we present additional ablation studies on these design choices to see the effect on our model\u2019s performance. We also note that it is also not always feasible to exactly compare only the methods. For example, OT-based methods like Rectified Flow for flow-based methods do not work well with the additional stochasticity introduced in our sampler, and even our baseline ODE-sampler does not work well with such methods because of the special time-discretization. We have tried applying this to Rectiflied Flow but fails completely in generating quality images. Therefore, reusing some designs for baselines may be detrimental to the baseline methods.\n\n[*] Liu et al., 2022, Let us Build Bridges: Understanding and Extending Diffusion Generative Models"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379517609,
                "cdate": 1700379517609,
                "tmdate": 1700379517609,
                "mdate": 1700379517609,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MeINafRO3Q",
                "forum": "FKksTayvGo",
                "replyto": "ibxflzbQyQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2089/Reviewer_H8Nr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2089/Reviewer_H8Nr"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response, some clarifications needed."
                    },
                    "comment": {
                        "value": "Thanks to the authors for their thorough response back. There are some things that I find unsatisfactory about the responses, and hopefully you have some time to clarify it. \n\n - It seems that your characterization of some alternative works is not befitting, especially with regards to the very thorough public comments made by Peluchetti. Indeed, it is unclear to the reviewer, as the public comment says, what statements like \"our model is equally valid\" means in the context of solving the measure transport problem, when the public comment shows that the proposed method does not do that. \n\n- To the reviewer, it is really poor form to just stuff citations in the supplementary material. The purpose of a submission is to present work *in the context of the literature to add to our shared knowledge*. While I appreciate you taking the time to assemble a thorough related work, it is not a bad thing for your work to be similar to others, indeed it can be a *good* thing! It means multiple perspectives arising to similar conclusions, which is what we hope to do (I think!). As such, as a reviewer, I don't aim to suggest you need to differentiate your work as better or above other perspectives. Just contextualize it :)\n\n- The characterization I think of [4] is incorrect -- indeed, that paper seems be showing that you don't need any special coupling to build a bridge, as you are requiring here. Moreover, it can be done with arbitrary linear or nonlinear processes. As Peluchetti pointed out, needing Doob-h and conditioning the score model on x_t does not seem to engender flexibility. What do you mean when you say this allows for a more flexible model? \n\n- This work [4] does not specify anything about relying on an ODE, as you've written in the additional supplemental material. In fact, it seems to be saying that both the SDE and the ODE are available from the same model, with the learning being totally independent of choice. What does it mean to say: \"Different from these methods, our model is SDE-based, directly builds from diffusion models, and uses a different denoising bridge score-matching loss than this class of models.\"? All of these works build off the notion of diffusion, however the reviewer doesn't understand why this is a blessing -- indeed, it's why Doob-h and such complicate the validity of the flow map, no? \n\n- I believe that this paper and the Peluchetti's discussion need to be properly addressed in the text. Particularly Peluchetti's derivation regarding the solution to the transport equation. If you can do that, I will bump my score, but there seems to be some technical inconsistencies in the work, and to be honest I don't understand some of the language justifying the method. \n\n\n- With regards to the remark about mis-interpreting OT: I am a bit confused by the response, as $x = (1-t)x_0 + t x_1$ is not a coupling, that's a function. A coupling would be how you draw $x_0, x_1 \\sim p(x_0, x_1)$. If you had the OT, the function you wrote above, which does interpolation, would be $x = (1-t)x_0 + t*T(x_0)$ where $T$ is the OT map such that $T(x_0) = x_1$. \n\n\n- Finally, the remark about experiments. You write that \"For example, OT-based methods like Rectified Flow for flow-based methods do not work well with the additional stochasticity introduced in our sampler.\" This confuses me as well. Whatever sampler you use, you need to show that the objects in that sampler actually correspond to the objects you learn. If you learned the solution to a probability flow, e.g. a velocity field $v(x,t)$ that gives the map $\\dot X_t(x) = v(t, X_t(x))$ with $X_0(x) = x$, then you can't go ahead and plug this into an SDE, you'd need a different vector field, for example, the one presented in [4], equation 2.33. \n\n\nI cannot raise my score until some of the above is addressed. While the supplemental related work is thorough and greatly appreciate the author's efforts to do this, there are some remaining inconsistencies that I think do not merit publication yet (but are some which I think are potentially addressable!). Thanks again for your efforts and the thoroughness of your response."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596403438,
                "cdate": 1700596403438,
                "tmdate": 1700596403438,
                "mdate": 1700596403438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7awwTyBBau",
                "forum": "FKksTayvGo",
                "replyto": "pMMAmW6S3g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional response (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer again for the patience and suggestions. We want to address the remaining concerns.\n\n\n> It seems that your characterization of some alternative works is not befitting, especially with regards to the very thorough public comments made by Peluchetti. Indeed, it is unclear to the reviewer, as the public comment says, what statements like \"our model is equally valid\" means in the context of solving the measure transport problem, when the public comment shows that the proposed method does not do that.\n\nWe have additionally clarified that in the case of noiseless limit, the magnitude of the score $\\nabla_{x_t} \\log q(x_t | x_T)$ (Eq. 10) goes to infinity because $\\sigma_t \\rightarrow 0$, so we can no longer match this term directly with score-matching. Although the presented score-matching model cannot handle the noiseless limit case, we can design new losses to predict the combined $-g^2(t)(1/2 s(x_t,t,y,T) - h(x_t,t,y,T))$, which we\u2019ve shown in the appendix reduces to $x_T - x_0$ (special case 2). Our framework subsumes this in the sense that the ODE drift terms becomes the same terms, e.g.  $x_T - x_0$. In addition, thanks to the generalized parameterization of networks in Section 4, we can design our score model to be $s(x_t, x_T, t; \\theta) = A x_t +B  x_T + C V(x_t, t; \\theta)$ where $V$ is the neural network with parameter $\\theta$ and simply choose $A=B=0$, $C=1$ and match it against $x_T - x_0$ with loss $\\mathbb{E}_{x_t,t,x_0,x_T}[ \\lVert s(x_t,x_T, t; \\theta) - ( x_T - x_0) \\rVert^2] $.\n\nThe above to loss then reduces to $\\mathbb{E}_{x_t,t,x_0,x_T}[\\lVert V(x_t, t; \\theta) -  (x_T - x_0) \\rVert^2]$ \n\nwhere $s$ and $V$ are parameterized by $\\theta$ and the loss becomes loss of OT-Flow-Matching and Rectified Flow. \n\n\n\n\n> To the reviewer, it is really poor form to just stuff citations in the supplementary material. The purpose of a submission is to present work in the context of the literature to add to our shared knowledge. While I appreciate you taking the time to assemble a thorough related work, it is not a bad thing for your work to be similar to others, indeed it can be a good thing! It means multiple perspectives arising to similar conclusions, which is what we hope to do (I think!). As such, as a reviewer, I don't aim to suggest you need to differentiate your work as better or above other perspectives. Just contextualize it :)\n\nUnfortunately, due to ICLR rules, we are only allowed to submit a 9 page paper (increasing this would otherwise disqualify our paper). Furthermore, we are also running out of time to address all reviewer comments due to the shortened discussion period (where we can update the main pdf) and conflicting deadlines, meaning that it would be extremely untenable for us to cut the main manuscript to fit in our enhanced discussion. We included the discussion in the appendix to give us the freedom to write down all of our thoughts while also avoiding damaging the flow of the main paper. We apologize if this came off as \u201cstuffing\u201d citations in the supplementary material; our goal was simply to find space to address all reviewer concerns about prior work before fully integrating the enhanced discussion.\n\nGenerally, we agree that our work fits into this (extremely) broad field of diffusion models with bridge additions. In particular, we agree that most of our developed theory can be (relatively easily) rederived from existing frameworks, as most papers in this field build off of the same few constructions. In fact, this is actually similar to how Schrodinger Bridge and Entropic Optimal Transport are fundamentally the same thing despite their different formulations. However, the difference with our paper (that we must stress) is the ability to connect with the existing diffusion model literature, which is necessary for achieving good empirical results. As such, it\u2019s worth emphasizing why the existing literature has largely been unable to scale to these types of problems that we consider in our paper."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661890274,
                "cdate": 1700661890274,
                "tmdate": 1700662881241,
                "mdate": 1700662881241,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gKPO2qdWym",
                "forum": "FKksTayvGo",
                "replyto": "pMMAmW6S3g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional response (Part 2)"
                    },
                    "comment": {
                        "value": "> The characterization I think of [4] is incorrect -- indeed, that paper seems be showing that you don't need any special coupling to build a bridge, as you are requiring here. Moreover, it can be done with arbitrary linear or nonlinear processes. As Peluchetti pointed out, needing Doob-h and conditioning the score model on x_t does not seem to engender flexibility. What do you mean when you say this allows for a more flexible model?\n\nWe don\u2019t believe that we stated that our model is more flexible than [4] (if we did so, then we apologize for the miscommunication). In particular, as we mention explicitly in our subsequent response point, our more focused development is more critical for our goal (of extending existing diffusion model frameworks).\n\nIt is also perhaps not fitting to talk about [3] and [4] together in the same sentence as [3] because [3] uses interpolants for constructing normalizing flow, which gives deterministic dynamics. We have put them in a separate sentence and we stress that we do not claim our framework is more flexible.\n\n> This work [4] does not specify anything about relying on an ODE, as you've written in the additional supplemental material. In fact, it seems to be saying that both the SDE and the ODE are available from the same model, with the learning being totally independent of choice. What does it mean to say: \"Different from these methods, our model is SDE-based, directly builds from diffusion models, and uses a different denoising bridge score-matching loss than this class of models.\"? All of these works build off the notion of diffusion, however the reviewer doesn't understand why this is a blessing -- indeed, it's why Doob-h and such complicate the validity of the flow map, no?\n\n\nThe reviewer makes a good point that works like [4] and our work (and generally many of the works since the advent of ScoreSDE) build off of the core fundamentals of the original diffusion models papers (with perhaps some modifications like the Doob-h transform, presentation, etc\u2026). Our statement was primarily meant to characterize our work as particularly suitable with the empirical frameworks developed for diffusion models (DDPM, EDM, VDM). In particular, by directly introducing our framework with SDEs, a connected score matching loss, and similar noise schedules as existing work, we can better adapt ubiquitous design choices. However, for a work like [4], although there is undoubtedly a much larger degree of generalizability (ie arbitrary interpolation strategies between arbitrary distributions), these additional mathematical formulations make it harder to see the connections with the existing literature and, as such, adapt the existing empirical design choices.\n\n\n\n> Remaining confusion on mis-interpreting OT.\n\nWe want to emphasize that we are simply borrowing the terminology from Flow Matching. Our method does not solve OT, but we are somewhat compelled to reuse this terminology as the naming of Flow Matching \u201cOT\u201d path is extremely popular. Generally, Flow Matching calls this the \u201cOT\u201d path as it is the optimal transport path between a dirac delta distribution and a Gaussian, even though the full probability path is not OT.\n\n\n> The remark about using proper samplers for different methods.\n\nThe additional stochasticity example may be inapplicable in the context of Rectified Flow. Our point was that each sampler is tailored to a specific model. For example, diffusion model samplers [A,B,C] cannot be directly used with flow models as diffusion samplers heavily rely on model-specific definitions such as log SNR, noise schedule, ODE step-schedule, etc. and are not general-purpose integrators. Even though many are ODE-based, it would require significant redesign to work on other flow-based models. We therefore refrain from doing so.\n\n\n\n\n[A] Lu, Cheng, et al. \"Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.\" \n\n[B] Lu, Cheng, et al. \"Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models.\"\n\n[C] Karras, Tero, et al. \"Elucidating the design space of diffusion-based generative models.\""
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661937184,
                "cdate": 1700661937184,
                "tmdate": 1700662409102,
                "mdate": 1700662409102,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l6vucZeBeR",
                "forum": "FKksTayvGo",
                "replyto": "gKPO2qdWym",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2089/Reviewer_H8Nr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2089/Reviewer_H8Nr"
                ],
                "content": {
                    "title": {
                        "value": "Final clarification"
                    },
                    "comment": {
                        "value": "Ok, thanks again folks. I'm satisfied with your response regarding Peluchetti's comments.\n\n However, I really think it is valuable to not just propagate misconceptions because they are popular (regarding OT). Choosing $x_1 - x_0$ as the conditional learning problem has almost *nothing* to do with OT, unless as you say you reduce it to the tautological case of a point mass to a Gaussian...I think we should all strive (for posterity's sake) to make accurate statements in our work.\n\n> The reviewer makes a good point that works like [4] and our work (and generally many of the works since the advent of ScoreSDE) build off of the core fundamentals of the original diffusion models papers (with perhaps some modifications like the Doob-h transform, presentation, etc\u2026). Our statement was primarily meant to characterize our work as particularly suitable with the empirical frameworks developed for diffusion models (DDPM, EDM, VDM). In particular, by directly introducing our framework with SDEs, a connected score matching loss, and similar noise schedules as existing work, we can better adapt ubiquitous design choices. However, for a work like [4], although there is undoubtedly a much larger degree of generalizability (ie arbitrary interpolation strategies between arbitrary distributions), these additional mathematical formulations make it harder to see the connections with the existing literature and, as such, adapt the existing empirical design choices.\n\nThis wasn't really my point. My point was that the connection between the two densities *need not rely on SDEs at all* and by forcing it to you have to tie your sampling to your noise schedule in these ways. But I understand wanting the continuity of readership etc. In reality the noise schedule stuff is actually totally independent of the learning problem, but I understand the continuation in ideas.\n\nThe real clarification I was trying to make with [4] is now elucidated in your remark about samplers:\n\n>  Our point was that each sampler is tailored to a specific model. For example, diffusion model samplers [A,B,C] cannot be directly used with flow models as diffusion samplers heavily rely on model-specific definitions such as log SNR, noise schedule, ODE step-schedule, etc. and are not general-purpose integrators. Even though many are ODE-based, it would require significant redesign to work on other flow-based models. We therefore refrain from doing so.\n\nThis is not completely true, and is the point I was making about the takeaway from [4]. Indeed, the diffusion coefficient in the stochastic sampling need not be related to the process connecting the densities. \n\nThank you for being available for responses. I will raise my score to a 6. I think there are some broad re-writings of the exposition here that could be useful, but you've been thorough in your engagement and have clarified a number of points, particularly wrt to the public comments.."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673846192,
                "cdate": 1700673846192,
                "tmdate": 1700673846192,
                "mdate": 1700673846192,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mvN3tHh4pn",
            "forum": "FKksTayvGo",
            "replyto": "FKksTayvGo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2089/Reviewer_zUzA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2089/Reviewer_zUzA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a novel formulation on diffusion-based generative model, i.e., denoising diffusion bridge models (DDBMs). This formulation further inspire applications in important tasks such as image-to-image translation. Detailed theoretic derivation and empirical results are provided."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The proposed method and formulation is novel and can potentially empower some pivotal applications such as image-to-image translation.\n2. The paper is well-written and easy-to-follow.\n3. Sufficient qualitative and quantitative results are provided to validate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. More results on larger-scale generation would be better (e.g., DDBM with Stable Diffusion).\n2. It would be better if the author could compare the DDBMs with some more advanced image-to-image translation algorithm such as controlnet."
                },
                "questions": {
                    "value": "1. Is there any advantage of DDBMs over some previous image-to-image translation methods such as controlnet?\n2. It would be better to show the variation ability of DDBMs in terms of the generation results. For example, when translating edges into an image, there exists various solutions. Are DDBMs able to generate these various results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2089/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2089/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2089/Reviewer_zUzA"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2089/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698858128485,
            "cdate": 1698858128485,
            "tmdate": 1699636141154,
            "mdate": 1699636141154,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NcGuHMhMKa",
                "forum": "FKksTayvGo",
                "replyto": "mvN3tHh4pn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2089/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review and appreciation."
                    },
                    "comment": {
                        "value": "We thank the reviewer for your appreciation of our work. We hope the following answers address your concerns.\n\n> Is there any advantage of DDBMs over some previous image-to-image translation methods such as controlnet?\n\nWe want to note that our framework is orthogonal to ControlNet as ControlNet is primarily an architectural design. Our framework is theoretically different from (and based on) diffusion models, and we can also use ControlNet with pretrained diffusion weights as our starting point for training our bridge framework. One of the motivations for our work is to develop a bridge-based framework such that it is easy to transfer many successful design choices over to improve the empirical results of bridge-based frameworks, which have been met with limited empirical success. We have experimented with those from EDM, but ControlNet is certainly another choice one can adopt. However, due to limited resources and time we did not proceed to larger-scale experiments with ControlNet.\n\n> It would be better to show the variation ability of DDBMs in terms of the generation results. For example, when translating edges into an image, there exists various solutions. Are DDBMs able to generate these various results?\n\nYes indeed. This is one of the motivations for introducing stochasticity to the bridge sampler, as the conditional generation is multimodal. We indeed observe some variations in image translation when we set different seeds, and this variation comes from the introduced stochasticity in our hybrid sampler. We show some variations in the additional pdf page in the supplementary."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379242049,
                "cdate": 1700379242049,
                "tmdate": 1700379242049,
                "mdate": 1700379242049,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8IP444olXF",
                "forum": "FKksTayvGo",
                "replyto": "NcGuHMhMKa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2089/Reviewer_zUzA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2089/Reviewer_zUzA"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply. These have resolved my concerns, and I will keep my score of 8."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2089/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730135211,
                "cdate": 1700730135211,
                "tmdate": 1700730135211,
                "mdate": 1700730135211,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]