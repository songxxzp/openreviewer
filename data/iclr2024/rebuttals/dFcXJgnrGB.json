[
    {
        "title": "PlaSma: Procedural Knowledge Models for Language-based Planning and Re-Planning"
    },
    {
        "review": {
            "id": "XOoiXMT8gR",
            "forum": "dFcXJgnrGB",
            "replyto": "dFcXJgnrGB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8605/Reviewer_bpqq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8605/Reviewer_bpqq"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to leverage knowledge distillation to train smaller LMs to replicate the procedural plan generation abilities of LLMs.This serves as a cost-effective alternative to achieve the same performance as LLMs using smaller LMs. The authors generate a novel dataset, called CoPlan which includes goal-based and constrained planning tasks, as well as counterfactual replanning tasks. When trained on the CoPlan dataset, the smaller (distilled) models showcased comparable performance to their LLM counterparts. Empirical and ablation experiments further demonstrate the same."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Overall, the paper is well-written and has a smooth flow, which makes it easy to follow.\n\n2. The data collection process used to generate CoPlan is novel and would be useful for researchers to collect high-quality data with minimal human involvement.\n\n3. Some of the results shown in the paper were interesting, although not entirely surprising."
                },
                "weaknesses": {
                    "value": "1. Novelty: The main contribution of this work -- to train a small LM to imitate an LLM by using the LLM as a teacher to train the small LM, is not novel. It has already been demonstrated in [A] (and has not been cited here) for a variety of tasks including complex reasoning If considered in the context of [A], the novelty here is limited to its extension to planning. The use of beam search-based planning is also very similar to [B], however, given its recency, I have discounted it in my evaluation. \n\n2. Missing Key Experiments: While the authors motivate the use of knowledge distillation and compare their distilled models with that of the teacher model, the comparisons with the original (undistilled) model seem to be missing. Without this, it is hard to gauge the performance enhancement from distillation.\n\n[A] Orca: Progressive Learning from Complex Explanation Traces of GPT-4, Mukherjee et al., 2023\n\n[B] SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge, Hazra et al., 2023\n\n--------------------------------------------------------------\n\nEdit 1:\n\nI'm discarding the Novelty contention given that [A] is considered \"contemporaneous\" according to ICLR guidelines. The authors have also answered the aspect of the missing experiment. Hence, I'm raising my score."
                },
                "questions": {
                    "value": "1. How is CoPlan different from ProScript? Barring the size factor, is it the counterfactual and replanning subset that is novel? Or is there a difference in the diversity of data too?\n\n2. Can you explain the use of the term \"symbolic\" in \"symbolic procedural knowledge distillation\"? What is \"symbolic\" here?\n\n3. It would be interesting to see when distillation leads to overfitting. What factors (model size of small LM, amount of training data) does it depend on? This would help motivate the generation of a larger dataset (compared to existing ones like ProScript).\n\n4. Minor Comment: The use of the term \"task\" in Sec 2.2 is ambiguous. The authors should clarify upfront that the three tasks are the three different settings that they investigate."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8605/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8605/Reviewer_bpqq",
                        "ICLR.cc/2024/Conference/Submission8605/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8605/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698706748778,
            "cdate": 1698706748778,
            "tmdate": 1700391265467,
            "mdate": 1700391265467,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hJYpPF6E4K",
                "forum": "dFcXJgnrGB",
                "replyto": "XOoiXMT8gR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8605/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8605/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bpqq"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed review and positive comments on our novel data collection, interesting results, and well-written paper. We are in the process of updating our pdf, meanwhile please find our response below:\n\n### W1. Novelty and Comparison with Orca:\n\nThanks for pointing out Orca that we overlooked. Indeed there is a recent line of work on building general-purpose small models which warrant a discussion and reference. While our work shares similar spirit with Orca, we find the key distinction in (1) our goal is to develop a specialized small model for procedural/counterfactual planning and replanning with potential application to embodied domain, and (2) Orca is focused on learning from GPT-4 explanations (Chain of Thought) to improve models capabilities. Nonetheless, building specialized models on top of them can be explored in future works as we only worked on models that were accessible at the time of submission.\n\nWe will add this discussion in a new extended related work in the Appendix.\n\nThanks for bringing [B] to our attention. We believe such contemporaneous works indicate the timeliness of our contributions as they reflect the needs and interests of the research community.\n\n### W2. Re Missing Key Experiment:\n\nIndeed we have tried to get an undistilled T5 model to work on the planning task. However, trying several prompt formats (such as \u201cHow to <goal>?\u201d, Provide steps to achieve the <goal>\u201d, etc.), we couldn\u2019t get any meaningful outputs from this baseline to run human evaluation on or apply guided decoding. Most outputs were just repeating the given inputs or empty strings. However, we successfully ran a few-shot variant of undistilled T5 (with some post-processing to format the outputs). Below is the human evaluation result for this baseline on the same 250 randomly sampled test examples (Curie and PlaSma scores are compiled from Table 1):\n\n|                         | Coverage | Order | Overall |\n|-------------------------|----------|-------|---------|\n| Fewshot T5 (undistilled) | 2.63     | 3.05  | 2.63    |\n| Fewshot Curie (teacher) | 3.75     | 4.27  | 3.75    |\n| PlaSma (best)           | 4.53     | 4.77  | 4.58    |\n\nResults of Few-shot undistilled T5 is significantly behind those of fewshot Curie (teacher) and all our model variants.\n\n### Q1. How CoPlan is different from ProScript?\nWe would like to clarify that ProScript is a small-scale **manually-curated** dataset of scripts, whereas CoPlan is (1) model generated, (2) larger scale, (3) includes counterfactual replanning, and finally (4) it broadly covers a diverse set of everyday, real-world activities both at low-level (actionable) goals and high-level long-term goals. We provided a detailed diversity analysis of goals and conditions of CoPlan in **Appendix A.1 and A.2**.\n\n### Q2. What is \"symbolic\"?\n\u201cSymbolic\u201d (in symbolic knowledge distillation) refers to human-readable textual formats rather than the transfer of obscure/soft model weights as in standard distillation (Hinto et al. 2015). Thanks for pointing this out, we will include this explanation in the updated version.\n\n### Q3. Re \"motivation for generating larger data\" and impact of \"model size of small LM\", \"amount of training data\":\n\n- Our **experiment in Table 2** investigates the utility of larger CoPlan that is obtained through symbolic distillation in the presence of manually-curated ProScript. There we compare a distilled model trained only on CoPlan with a T5-11B model trained only on proscript. Results indicate that we do benefit from a larger dataset.\n\n- We studied the impact of (student) model sizes on performance (Fig. 3 and Table 1) where we show that larger students perform relatively better but we can bridge the scale gap with multi-task distillation and our proposed verifier-guided decoding.\n\n- We also conducted experiments on the impact of training data size on test/validation losses. However, we ended up not including the latter in the paper as we thought it might not fit well with the scope. Below are the results:\n\n| Portion of Training data | Validation Loss | Test Loss |\n|--------------------------|-----------------|-----------|\n| 20%                      | 1.41            | 2.27      |\n| 40%                      | 1.34            | 2.15      |\n| 60%                      | 1.26            | 2.10      |\n| 80%                      | 1.23            | 2.06      |\n| 100                      | 1.19            | 2.01      |\n\nBased on decreased validation/test loss, we don't observe signs of overfitting. Nonetheless, a more comprehensive scaling law contribution in the context of distillation could be done in the future.\n\nWe will include this in the Appendix of our updated pdf.\n\n### Q4. Typo for the term \"task\"\nThanks,  we fixed this to \u201cthree different task settings\u201d in our updated pdf."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700247169357,
                "cdate": 1700247169357,
                "tmdate": 1700247169357,
                "mdate": 1700247169357,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K0qTJ5scaN",
                "forum": "dFcXJgnrGB",
                "replyto": "XOoiXMT8gR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8605/Reviewer_bpqq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8605/Reviewer_bpqq"
                ],
                "content": {
                    "title": {
                        "value": "Good Question Reviewer EJrs"
                    },
                    "comment": {
                        "value": "That's a good question and while I was aware of [A], I had not bothered to verify that. Orca [A] is quite old now given the fast-paced timeline (and is also somewhat popular), however, it seems the authors still haven't open-sourced the data (which also explains why I couldn't find a peer-reviewed version). Nevertheless, the framework is quite detailed in their paper, based on which there are (at least) a couple of open-sourced replications of Orca going back to June 2023 (see OpenOrca: https://huggingface.co/datasets/Open-Orca/OpenOrca and Dolphin) with impressive results. As you see, given the follow-up works, the idea of training smaller LMs to reason using knowledge from LLMs is not particularly new.\n\nAs for [B], as I said, I've not taken it into account given its recency, despite its overlap."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700339826795,
                "cdate": 1700339826795,
                "tmdate": 1700341484011,
                "mdate": 1700341484011,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Pr0iTP990o",
                "forum": "dFcXJgnrGB",
                "replyto": "hJYpPF6E4K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8605/Reviewer_bpqq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8605/Reviewer_bpqq"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledging author response"
                    },
                    "comment": {
                        "value": "Thank you authors for taking the time to write detailed clarifications. You have answered most of my questions well, however, I'm still somewhat skeptical about the core contribution of the work. I'll update my score after discussing it with other reviewers. For now, I don't have any further questions."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700341431771,
                "cdate": 1700341431771,
                "tmdate": 1700341431771,
                "mdate": 1700341431771,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e2vkQyl1Tg",
                "forum": "dFcXJgnrGB",
                "replyto": "K0qTJ5scaN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8605/Reviewer_bpqq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8605/Reviewer_bpqq"
                ],
                "content": {
                    "title": {
                        "value": "Re: Good Question Reviewer EJrs"
                    },
                    "comment": {
                        "value": "Dear SACs, ACs, Reviewers, Authors,\n\nUpon revisiting the reviewer guidelines, I found that ICLR considers papers as contemporaneous if it was \"published\" on or after May 28. Since Orca is still a preprint, and since novelty has been my main contention, I'm inclined to drop it and raise my score. The authors are still encouraged to add some comparisons in their manuscript."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700391129086,
                "cdate": 1700391129086,
                "tmdate": 1700391129086,
                "mdate": 1700391129086,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GDfmZLE9Y2",
            "forum": "dFcXJgnrGB",
            "replyto": "dFcXJgnrGB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8605/Reviewer_EJrs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8605/Reviewer_EJrs"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the use LLMs for generating NL instructions for a given task, called procedural plans. The paper proposes that smaller LLMs trained specifically for generation of such plans can outperform the models used as teacher, and be on par of larger models.\n\nAlgorithmically, the contributions are three:\n- PlaSma: small model specialized in generating NL instructions.\n- PlaSma+: uses PlaSma and an additional model for biasing the output towards higher validity. The paper refers to this bias as a \u201cconstraint\u201d.\n- CoPlan dataset\n\nThe key transversal issues are both data generation and evaluation.\n\nThe dataset (CoPlan) is generated using a combination of seed prompts, large models, and human validation.\n\nThe experimental setup is reasonable these days: use proprietary GPT as teacher and for generating data; use T5 variants as small models; BERT-variations for classification tasks.\n\nThe evaluation is more complicated. The paper reports good human evaluation results in one dataset as the plans cannot be tested. (The  appendix reports usual BLEU and ROUGE scores, perhaps for pacifying some reviewers, but for natural situations there are so many possible wordings that that might very misleading). For VirtualHome, they report an interesting success rate.\n\nThe key question is whether the smaller model is just mimicking the teacher\u2019s behaviour. However, the paper reports that the student might outperform the teacher significantly, especially if it has enough capacity. The bias-towards verification model has a higher impact in models with lower capacity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Interesting problem as instructions are a key possible application of LLM. Sensible to scale and cost.\n- Good description of the methodology in all aspects.\n\t- In particular the data generation vs curation.\n- Sensible complexity of the tasks: goal, conditions, verification.\n- A secondary model specialized in higher correctness is a good idea while focusing at lower capacity."
                },
                "weaknesses": {
                    "value": "- The dataset ProScript is not well explained\n\t- It is hard to qualify the complexity of the instructions.\n\t- So, the results in Table 2 are hard to understand because we don\u2019t know about the relative complexity of the task and the diversity of tasks.\n- I suggest reducing the tone of the phrase \u201cwe introduce the task of counterfactual planning\u201d. A quick search in google scholar for \u201cplan revision\u201d reported, for instance, these papers: \n\t- Ow, P. S., Smith, S. F., & Thiriez, A. Reactive plan revision. AAAI 1998.\n\t- Williams, K., & Burdick, J. Multi-robot boundary coverage with plan revision. In Proceedings 2006 IEEE International Conference on Robotics and Automation.\n- Abuse of some terms\n\t- The \u201cstep verifier\u201d is **not** verifying, but adding a bias. For instance, the paper mentions that in the case of embodied agents, that verification can be taken over by a safety module. In that scenario, with reasonable \\alpha that follows the LLM when the so-called verification is not saying anything relevant, the aggregation of Eq (3) cannot prevent cathastrophic errors that are considered very attractive by the LLM. Perhaps a better name would be \u201cquality bias\u201d or anything saying bias.\n\t- Same applies to the notion of \u201csymbolic knowledge destilation\u201d, but we are probably too late for this one. I find it problematic that in AI we associate NL with symbolic, as the word is overloaded with a huge body of work in AI ranging from logic to graphical models. It should be more clear to call it something like \u201cinstruction distillation\u201d."
                },
                "questions": {
                    "value": "- Except for PlaSma-Mul, what does PlaSma mean when measured in different tasks. Can you elaborate on how that manifests in the experiments?\n\t- For instance, in Table 1, the PlaSma model is trained in the planning task, so there are precisely 6 models there. Right?\n\t- Those models are completely different from the ones reported in Fig 4, correct?\n- Please describe the ProScript in-depth and discuss why this is a good dataset for studying this problem.\n- Please discuss what other datasets could have been used, and explain why some possibilities are inconvenient. \n\t- This should be added to the related work section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8605/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698877319523,
            "cdate": 1698877319523,
            "tmdate": 1699637076827,
            "mdate": 1699637076827,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8L02WCRxcB",
                "forum": "dFcXJgnrGB",
                "replyto": "GDfmZLE9Y2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8605/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8605/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EJrs"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed review and positive endorsement of our interesting research problem, our proposed methodology, the comprehensiveness of the task under study, and compelling results on VirtualHome. Please find our response below:\n\n\n### Re ProScript details:\nProscript is a manually-curated dataset of ~6k scripts at varying levels of granularity, for a wide range of everyday activities (e.g., `bake a cake`, `audition for a musical`, `go sailing`, etc.). Given the diversity and granularity of goals in ProScript, we believe it provides a good venue to study procedural knowledge in language models available through distillation.\n\nTable 2  includes both evaluation on the ProScript as well as our collected CoPlan dataset.  The goal is to investigate the utility of CoPlan that is obtained through \u201cgeneration from an LLM\u201d in the presence of manually-curated ProScript.\n\n### Re phrase \"introduce the task of counterfactual planning\":\nWhile plan revision has been explored in classical AI (with restricted action vocabularies) and Robotics, here we are investigating counterfactual and constrained (re)planning capabilities of contemporary language-based agents. As LMs are gaining more attention in robotics and physically embodied application, it becomes imperative to enhance their capabilities by integrating counterfactual planning and contained planning mechanisms.\n\nWe thank the reviewer for bringing this to our attention, and we will revise this phrase accordingly in our updated version.\n\n### Re \"step verifier\" is not verifying:\nWe agree with the reviewer. While during training the verifier acts as a binary classifier to identify the validity of a next-step, we are using this as a bias/regularizer term during decoding the plan (Eq. 3). Thank you for pointing this out. We will update our paper to reflect this.\n\n### Re \"Symbolic\" in Symbolic Knowledge distillation:\nWe totally agree with the reviewer that \u201csymbolic\u201d became an overloaded word in AI. However, we use this term in reference  to the previous work (West et al. 2020, \u201cSymbolic Knowledge Distillation: from General Language Models to Commonsense Models\u201d).\n\n### Q1. what does PlaSma mean when measured in different tasks?\nThank you for bringing this to our attention. Yes, except for PlaSma-Mul, the models (listed under PlaSma) in Table 1 are completely different from those in Figure 4. This distinction also applies to the models featured in the left and right plots of Figure 4. We will ensure clarity by having a different naming convention.\n\n### Q2. Describe ProScript:\nPlease see our response above. We will add this to our update pdf soon.\n\n### Q3. Other datasets:\nFor training purposes, we use our large-scale collected CoPlan dataset.\n\nFor evaluation purposes, in the general domain, we use ProScript which was the only human-written dataset of this kind we were aware of. In the embodied domain, we use RobotHow/VirtualHome (Puig et al. 2018) for conducting our experiments. There are other embodied text-based environments such as Alfworld (Shridhar et al. 2020) which are similar in nature to VirtualHome. We will discuss this in our extended related work section. \n\nNote that we also used DeScript (Wanzare et al. 2016) which is a small-scale manually-written dataset as our seed examples during data generation. \n\nPuig et al. 2018. \u201cVirtualhome: Simulating household activities via programs\u201d\n\nWanzare et al. ACL 2016. \u201cA Crowdsourced Database of Event Sequence Descriptions for the Acquisition of High-quality Script Knowledge.\u201d\n\nShridhar et al. 2020, ALFWorld: \u201cAligning Text and Embodied Environments for Interactive Learning.\u201d"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245443364,
                "cdate": 1700245443364,
                "tmdate": 1700245443364,
                "mdate": 1700245443364,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q7t3UYtslR",
                "forum": "dFcXJgnrGB",
                "replyto": "GDfmZLE9Y2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8605/Reviewer_EJrs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8605/Reviewer_EJrs"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you. This answer some of my questions. I\u2019m looking forward to see the description of ProScript, including a justification of why is that a good dataset for the research question. This is a key issue: If the plans are very short, and the sample of problems is not diverse enough, the planning problem becomes a classification problem: decide which one to retrieve.\n\nIf new plans are required for novel situations, but the plans are basically composing two pieces of existing plans, the problem can be solved with two classifiers. Imagine a baseline that does directly that: choose possible few segments of a plan, and then use a ranker over potential combinations. Such baseline might have a good performance in a bad dataset for testing planning algorithms. That\u2019s why is very risky to test planning with a passive data, without a simulator. Then only alternative is that the dataset is diverse.\n\nAn important factor are amount to overlapping of the plans in the train set, but also respect to the test set. In this case, even if the plans are **not symbolic**, it should be possible to compare the text of the \u201csteps\u201d of the plans. That\u2019s true for both datasets.\n\nFurthermore, given the simple structure of the sentences in both datasets, the overlapping of the sentences is not enough. It\u2019d be more informative to see how the text compare with each other by ignoring **nouns**, while keeping at least the verbs and perhaps the rest of the sentence. Why? Well, the plans for these two goals is almost the same: \u201ccarry a glass from the bedroom to the kitchen\u201d, \u201ccarry a plate from the bedroom to the kitchen\u201d. This actually qualifies my argument about how plans in a dataset might be easier.  If a LLM memorize the plan for glass to the kitchen, then plate to the kitchen is just editing an existing plan.\n\nSuch an analysis might explain why some datasets are harder than others.\n\nI understand some papers in procedural planning are being accepted in top venues without such analysis. That\u2019s a shame. I attribute that to a weak background in combinatorial and symbolic AI, as well as lack of awareness of the literature. We are not here to repeat those mistakes, so let\u2019s show that the problem is interesting and cannot be solved by memorizing the plans and doing small editions.\n\nIn summary, \n1. please explain ProScript in detail,\n2. Analyze both datasets to disprove the hypothesis zero: planning can be done just by memorizing the plans and making few combinations or adjustments.\n\nNew:\n- in the main body, Please explain briefly the three tasks used for Multi-Task distillation. Then in the appendix explain the dataset in detail, and any hyper-parameter so we can reproduce the experiment. Justify any decision.\n- Please confirm that PlaSma-mul is not fine-tuned at all in the other datasets.\n\nOthers:\n- the expression \u201coriginal planning task\u201d is confusing. It might be better just to say ProScript.\n- In each table where PlasSa is reported, please mention the dataset used for fine-tuning. \n- About the use of \u201csymbolic\u201d, I don\u2019t think that 2020 reference is enough to use such a confusing expression, but I\u2019m not requesting a change about that terminology. I do request to acknowledge the confusion about the term, and refer to work in symbolic AI. This paper accepted to AAAI 2022 might be useful: https://arxiv.org/abs/2109.09904\n\nPS: I haven\u2019t read the other responses, so perhaps part of my new questions are answered there. Apologies in that case."
                    },
                    "title": {
                        "value": "Reject that planning in the datasets is trivial/easy"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700316949952,
                "cdate": 1700316949952,
                "tmdate": 1700317008633,
                "mdate": 1700317008633,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mqhFrvFylf",
                "forum": "dFcXJgnrGB",
                "replyto": "q7t3UYtslR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8605/Reviewer_EJrs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8605/Reviewer_EJrs"
                ],
                "content": {
                    "comment": {
                        "value": "By the way, about this answer to Q3:\n\n> For evaluation purposes, in the general domain, we use ProScript which was the only human-written dataset of this kind we were aware of.\n\nThese seems to imply that human-written is better. Please justify that statement. I\u2019d argue that purely human-written might lack diversity depending on the protocol, as human annotators have incentive to write as soon as possible, and schemas for additional incentive might have limited effect. \n\nThe key issue is not the human-writing but how the situations are generated. For instance. We can ask a human to provide some traces of the game tic-tac-toe. Or we can sample boards of tic-tac-toe and ask them to provide an answer. The later case would have more diversity of goals, but perhaps no diversity of plans, but that might be ok.\n\nSo, when explaining ProScript, please describe the data generation as part of the argument of why ProScript is interesting to the question of the paper. \n\n**The contribution of using smaller models might be tainted if that\u2019s exploiting an artifact of the datasets. So, the dataset description and analysis is crucial to prove the point. Otherwise, I\u2019m not convinced and I might reduce my scores.**\n\nI hope this is not surprising. My intention is that the paper proves the point."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700317485116,
                "cdate": 1700317485116,
                "tmdate": 1700317485116,
                "mdate": 1700317485116,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eZXtBYFoMM",
                "forum": "dFcXJgnrGB",
                "replyto": "hJYpPF6E4K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8605/Reviewer_EJrs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8605/Reviewer_EJrs"
                ],
                "content": {
                    "comment": {
                        "value": "Question for the reviewer: are the references [A] and [B] accepted or they are just pre-prints?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700317688556,
                "cdate": 1700317688556,
                "tmdate": 1700317688556,
                "mdate": 1700317688556,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E1Cl72rXJl",
                "forum": "dFcXJgnrGB",
                "replyto": "GDfmZLE9Y2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8605/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8605/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(continued) Response to Reviewer EJrs' comment"
                    },
                    "comment": {
                        "value": "### Re Can a classifier solve the problem? \n- The original proScript paper proposed a **prediction task** to be: given a goal and an unordered list of steps, predict a set of ordered steps. (Note that in this setting the model has access to the ground-truth list of steps which is NOT the case with the end-to-end plan **generation task**.)\n- They implemented a two-step approach baseline where they trained a **binary classifier** (RoBERTa-large) to predict the precedence between pairs of steps, followed by building an ordered list of steps (plan) by aggregating the predicted relations across all pairs of steps. Scores by the classifier are used as weights to create an adjacency matrix which is then automatically converted into ordered steps (plan).\n- This classification baseline achieved an **F1 score of 61.20%** which is far behind **human performance of 89.28%**. This suggests that the dataset is not trivial for a classification model even with full access to ground-truth steps. In the generation task (our focus) where the model **ONLY** has access to the goal, the task becomes even more challenging.\n\n\nWe include all this discussion on the complexity of the datasets in the Appendix of our updated pdf.\n\n\n### Re details of crowdsourcing proScript:\nFirst, given a scenario (e.g., bake a cake), each crowd-worker is required to describe five to seven core events (to balance the cognitive load and cost) that are essential for the scenario with the estimated time it takes to complete each event. In the second question, crowdworkers confirm the set of steps and they are asked to create a flowchart by connecting the steps possibly in partial order. When crowd-workers make a submission, a validation function is executed to check if the created flowchart is a valid dag and does not contain any shortcut edge. To have micro and microscopic scenarios, they iteratively picked events and used them as an additional source of finer-grained scenarios. For example, \u201cturn on the oven\u201d is a new fine-grained scenario derived from \u201cbake a cake\u201d.\nThis results in 6,414 (train=3,252) valid scripts that include 311,502 pairs of events.\n\n\n### Re training data used for finetuning single and multitask models:\nWe trained all (single task) PlaSma models on their respective subsets of the CoPlan dataset. PlaSma variants (besides Plasma-Mul) in Table 1 are trained on the goal-based planning subset of CoPlan. Similarly, PlaSma models in the left plot of Figure 4 are trained on the constrained planning subset of Coplan, and so on. PlaSma-Mul, on the other hand, is jointly trained on all CoPlan subsets: goal-based planning, constrained planning, and counterfactual replanning. We will briefly explain this in the main text and move hyperparameters in the appendix. \n\n\n### Clarification of task settings:\nWe would like to clarify that in the paper we used \u201cgoal-based planning\u201d and \u201coriginal planning task\u201d interchangeably. These two are contrasted with the new proposed task settings: \u201cconstrained planning\u201d and \u201ccounterfactual replanning\u201d.\n\n\n\nWe thank the reviewer for their thoughtful comments and discussion."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516512350,
                "cdate": 1700516512350,
                "tmdate": 1700516661858,
                "mdate": 1700516661858,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RZI9Gwzklz",
            "forum": "dFcXJgnrGB",
            "replyto": "dFcXJgnrGB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8605/Reviewer_h13v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8605/Reviewer_h13v"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a framework designed to improve the procedural knowledge and planning capabilities of small language models. This is achieved through symbolic procedural knowledge distillation and a verifier-guided step-wise beam search algorithm. The authors have conducted experiments to compare student models of varying sizes with their teacher model, and have utilized human evaluations to assess the generated plans in terms of sequence, completeness, and overall quality. The findings indicate that smaller models can reach or even surpass the performance of larger models by employing the PLASMA framework."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-written and well-structured.\n- Equipping small language models to come up with procedural knowledge at the same level as large language models is an important direction from an engineering perspective given the accessibility, carbon footprint, and cost of large language models."
                },
                "weaknesses": {
                    "value": "- Although human evaluations were conducted, the executability conditions for the plans in the domains used in these experiments seem to be loose. It would be beneficial to evaluate the models in domains which have hard executability conditions (like the domains used in International Planning Competitions), where the correctness can be objectively determined, to more accurately gauge the language planning abilities of the proposed method.\n- A comparison with GPT-4, in addition to GPT-3, could provide additional insights into the effectiveness of the method.\n- The potential for increased bias due to the distillation from larger language models is mentioned in the limitations section but remains a concern."
                },
                "questions": {
                    "value": "- If smaller language models can be effectively paired with human input or external verifiers for improved planning, why is distillation from a larger model necessary? This question is particularly relevant given that the domains discussed in the paper appear to be amenable to human verification."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed.",
                        "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)"
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8605/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8605/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8605/Reviewer_h13v"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8605/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698923587663,
            "cdate": 1698923587663,
            "tmdate": 1700736382630,
            "mdate": 1700736382630,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CDzZylfcUd",
                "forum": "dFcXJgnrGB",
                "replyto": "RZI9Gwzklz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8605/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8605/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h13v"
                    },
                    "comment": {
                        "value": "Thank you for your helpful review and positive feedback on the importance of our research to the community and our well-structured paper. We are in process of updating our pdf, meanwhile please see our response below:\n\n### Re W1: evaluate the models in domains which have hard executability conditions:\n\n- We indeed conducted an extrinsic evaluation in a domain with hard executability conditions, i.e., **VirtualHome (Section 3.3)**. This also helps to investigate the application of PlaSma and its generalization to the embodied agent planning domain. Given our strong results in the VirtualHome environment, we expect that PlaSma can serve as a strong foundation model and be easily transferred to other embodied domains with minimal domain adaptation.\n- On the general domain (i.e., on CoPlan); however, we follow the common practice of Likert scale human evaluation [1-3] across multiple dimensions. However, we made sure these dimensions are aligned with plan ability in an embodied domain. For example, **temporal ordering** is related to **executability** (e.g., `grasping` should be done before `picking up`), and **coverage/completeness** is related to success/correctness. \n\nWe hope this clarifies the reviewer concern regarding our evaluation.\n\n[1] Recent Advances in Neural Text Generation: A Task-Agnostic Survey. (Tang et al., 2023)\n\n[2] Human evaluation of automatically generated text: Current trends and best practice guidelines (Van Der Lee et al., CSL 2021)\n\n[3] The use of rating and Likert scales in Natural Language Generation human evaluation tasks: A review and some recommendations (Amidei et al. iNLG 2019)\n\n### W2: comparison with GPT-4:\nOur data was collected using GPT-3 Curie (smaller and less powerful than GPT-4) as the teacher due to cost constraints. We do expect to get better results for our distillation if using GPT-4 as our teacher. Nonetheless, as suggested by the reviewer, we ran a human evaluation comparing GPT-4, GPT-3.5, our best PlaSma model and its teacher on 50 instances (total of 200):\n\n|                         | Coverage | Order | Overall |\n|-------------------------|----------|-------|---------|\n| Fewshot Curie (teacher) | 3.53     | 4.37  | 3.58    |\n| PlaSma (best)           | 4.31     | 4.68  | 4.23    |\n| GPT-3.5 (Davinci-003)   | 4.78     | 4.84  | 4.81    |\n| GPT-4                   | 4.78     | 5.00     | 4.81    |\n\nAs we observe, the trend remains the same, with GPT-4 surpassing its predecessor (GPT-3.5) only in the `ordering` dimension. We will add this comparison in the **Appenidx F** of our updated pdf.\n\n### W3: Re potential increased bias:\nWe acknowledge potential biases, and  have discussed  them in the limitations section, warranting the need for further investigation in  future research.\n\n### Q1: If smaller language models can be effectively paired with human input or external verifiers...?\n\nSmaller models without distillation are not competent to perform the planning task and thus do not benefit from being augmented with an external verifier or human input. In fact, in our initial experiments, we tried to get an undistilled T5 model to work on the planning task. However, trying several prompt formats (such as \u201cHow to <goal>?\u201d, Provide steps to achieve the <goal>\u201d, etc.), we couldn\u2019t get any meaningful outputs from this baseline to run human evaluation on or apply guided decoding. Most outputs were just repeating the given inputs or empty strings. However, we successfully ran a few-shot variant of undistilled T5 (with some post-processing to format the outputs). Below is the human evaluation result for this baseline on the same 250 randomly sampled test examples:\n\n|                         | Coverage | Order | Overall |\n|-------------------------|----------|-------|---------|\n| Fewshot T5 (no distillation) | 2.63     | 3.05  | 2.63    |\n| Fewshot Curie (teacher) | 3.75     | 4.27  | 3.75    |\n| PlaSma (best)           | 4.53     | 4.77  | 4.58    |\n\nResults of Few-shot undistilled T5 is significantly behind those of fewshot Curie (teacher) and all our model variants (**Table 1**)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238816864,
                "cdate": 1700238816864,
                "tmdate": 1700238816864,
                "mdate": 1700238816864,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TEJZw0WUFy",
                "forum": "dFcXJgnrGB",
                "replyto": "CDzZylfcUd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8605/Reviewer_h13v"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8605/Reviewer_h13v"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking your time and writing the response. Most of my concerns have been addressed and I have updated the score of the paper. My concern regarding the evaluation on domains where the correctness can be objectively determined as opposed to humans evaluating it (which might not be accurate to gauge the planning abilities) still remains."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736363803,
                "cdate": 1700736363803,
                "tmdate": 1700736363803,
                "mdate": 1700736363803,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oVMAwaIfbS",
            "forum": "dFcXJgnrGB",
            "replyto": "dFcXJgnrGB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8605/Reviewer_drVQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8605/Reviewer_drVQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a distillation procedure and an inference-time decoding algorithm to enable relative small language models for planning and replanning with performance close or surpassing its larger language teacher models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\u2022 Proposed a paradigm to distill procedural planning knowledge from large language models to enable smaller languages to do planning, and it seems to be working. \n\t\u2022 Within the paradigm, a cost-effective human-in-loop LLM generated data curation procedure is also proposed to create the COPLAN dataset.\n\t\u2022 Proposed a  guided decoding procedure with a LLM-based (RoBERTa) step verifier to  guide the beam-search during planning steps decoding generation. The guidance help to further regulate the validity of the steps."
                },
                "weaknesses": {
                    "value": "\u2022 The LLM-to-Planning-Model teacher-student paradigm for planning is not well motivated. Cost, performance (from specialization), controllable procedure, better-integration with downstream tasks (e.g. decoding/execution) and so on? It is more about better understanding of the key capabilities of existing techniques and combining them to solve the critical problems. For example, if it is more about specializing common knowledge embedded in LLMs to do planning, then smaller LLM might not be the right solution --- the same proposed paradigm can be combined with LLM of the same size or even larger LLMs for superior planning capabilities. What are the real problems and the corresponding means could be better sorted out? \n\t\u2022 The truth contribution and their relevancy might be hidden in the paper title and the current way of writing.  The proposal is composed of three parts (1) planning data generation from LLMs with human-in-loop curation, (2) teacher-student distillation training, (3)  language model decoding with step verifier.  There are less texts regarding teacher-student distillation. This might indicate that the teacher-distillation importance is over-estimated. With the planning data generation and verifier-guided decoding generation, there might other ways to enhance planning abilities, e.g. finetuning the original LLMs to specializing into planning domains. If the distillation step is an importance component in the ingredients, please detail it and discuss more."
                },
                "questions": {
                    "value": "1. There are good ideas within the paper. The writing could be improved to make these good idea clear and stand-out. For example, how to train the step-verifier from human-written plans along with more formal analysis of impact of the step-verifier. \n\t2. Please define the loss functions formally with teacher-student distillation and verifier-training.\n\t3. For the step verifier, \"we design perturbations \u2026 ordering, semantic completeness, topicality and fluency\", please provide detailed analysis of these data-side steps regarding their intuition and formal properties if possible. How does a single verifier score reflect all these criteria? Any special design to achieve them with a simple RoBERTa based classifier?\n\t4. Regarding the evaluation metrics, please provide more details of the AMT human steps. Are coverage, order, over quality complete to evaluate a plan? Any comparison or correlation on the human evaluation metrics and the bleu numbers and the Emobided Environment's metrics? If not well-correlated, any proposal on automatic evaluating plans? Also how to relate and align human evaluation, bleu-style sequence matching metrics, embodied environment testable metrics and real-world execution measurable metrics?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8605/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699339447755,
            "cdate": 1699339447755,
            "tmdate": 1699637076584,
            "mdate": 1699637076584,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ieQWOLZZ4G",
                "forum": "dFcXJgnrGB",
                "replyto": "oVMAwaIfbS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8605/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8605/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer drVQ"
                    },
                    "comment": {
                        "value": "Thank you for the detailed review and positive comments on our cost-efficient distillation paradigm and our verifier-guided decoding algorithm. We are working on improving the writing of the paper to better showcase the ideas/contributions as suggested by the reviewer. Meanwhile, please find our responses below:\n\n### Re importance of distillation:\n\nFinetuning LLMs requires updating models\u2019 parameters which is not only costly but often inaccessible for the broader community. To clarify, our goal is not to make superior large models, but to distill procedural knowledge of LLMs into smaller models.\nReducing the scale and cost of strong models via teacher-distillation is the key in developing open-sourced LMs that are accessible to all, facilitating fine-tuning and seamless adaptation to various domains and custom use cases. Given the large-scale training dataset used for PlaSma, we hope it can serve as a foundation model that can be quickly adapted to specific domains with minimal additional annotation (like we demonstrate by adapting Plasma to VirtualHome).\nMoreover, we could not augment most of the LLMs with our decoding-time algorithm due to limited access to the model's log probabilities.\n\nThanks for the comment. We will update our pdf to include this.\n\n### Q1. Re details of training the verifier and perturbation strategies:\nWe included implementation details of the verifier in **Appendix B.3** due to space limit. We will move these materials to the main text as we recognize the potential inconvenience: \n\n- For training, we reuse 3k human-written plans from the existing ProScript dataset [2] to automatically create 47K positive and negative pairs of (plan-so-far, next-step). \n- Our perturbation strategies to generate pseudo-negative examples include: **(1) Reordered steps**: Conflicting logical order results from inaccurate causal or temporal dependencies in a plan. Thus, we apply both near and distant reordering by randomly reordering two consecutive and two distant steps. **(2) Repetitive steps**: Degeneration i.e., generating repetitive text is commonly observed in language models (also observed in the planning domain [1]). Similarly, we include both near and distant repetition by repeating the immediate previous step and distant previous step as a pseudo-negative next-step. **(3) Missing steps**: Another common mistake made by language models is missing necessary steps, leading to (semantically) incoherent plans. To simulate this behavior, we randomly select a non-immediate step as a pseudo-negative next-step. Below we show symbolic examples of perturbed pairs:\n\nGoal: g\n\nOracle Plan (steps): s1 s2 s3 s4 s5\n\n\nWe have the following perturbations: \n\nReordering: (g + s1 + s3, s2) \u2192 0\n\nRepetitive: (g + s1 + s2 + s3,  s2)  \u2192 0\n\nMissing: (g + s1 + s2, s4) \u2192 0\n\n\nNote that our perturbation strategies follow general rules applicable to any new domain with some task-specific adaptation.\n\n- Using the constructed positive and negative pairs of (plan-so-far, next-step), the verifier is trained using the standard binary Cross Entropy loss to identify the validity of the candidate next-step. It achieved an F1 score of 78% on a held-out test set.\n\n### Re formal analysis of impact of the step-verifier:\nIn Table 1, we include several ablations to show the contribution of multitasking, **step-verifier**, and scale. For example, the comparison between PlaSma and PlaSma+ across different model sizes shows non-negligible performance boosts as a result of the step verifier in our proposed tree-based decoding.\n\nIs the reviewer seeking a specific formal analysis? Thanks.\n\n\n### Q2. Re defining loss functions:\nAll models are trained with the standard Cross Entropy (CE) loss function with an early stop based on the validation loss. We will update our manuscript soon to include the loss used for distillation and verifier training.\n\n### Q3. Verifier perurtabation strategies\nPlease see our response to Q1"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237040310,
                "cdate": 1700237040310,
                "tmdate": 1700668794081,
                "mdate": 1700668794081,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eTTk9jTZBe",
                "forum": "dFcXJgnrGB",
                "replyto": "oVMAwaIfbS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8605/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8605/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to  Reviewer drVQ (Continue)"
                    },
                    "comment": {
                        "value": "### Q4.1 Are human evaluation dimensions enough?\nGiven the open-ended nature of CoPlan goals having varying levels of actionability (not all of them are grounded), we follow the common practice of Likert human evaluation across multiple dimensions [1-3]. Notably, we make sure we evaluate plans on dimensions that are related to actual plan ability in an embodied domain. For example, temporal ordering is related to executability (e.g., grasping should be done before picking up), and coverage/completeness is related to success/correctness. Nonetheless, we believe that extrinsic evaluation is crucial; for this reason, we use **VirtualHome** to investigate the application of PlaSma and its generalization to the embodied agent with hard executability **(Section 3.3)**. Given our strong VH results, we expect that PlaSma can serve as a strong foundation model and be easily transferred to other embodied domains with minimal domain adaptation.\n\n### Q4.2. Correlation between human and BLEU scores: \nWe indeed compute the correlation between BLEU metric and human scores. We find that BLEU has very weak correlations to human scores of coverage, ordering an overall quality, with a **Pearson correlation of 7.7%, 5.9%, and 5.6%**. This verifies the fact that n-gram-based metrics may not provide an informative measure of performance (as also suggested by previous works [3,4])\n\n### Q4.3: Proposal for better automatic evaluation:\nWe appreciate the interesting questions raised by the reviewer regarding human and automatic evaluation alignment. We also acknowledge the need for better automatic evaluation of plans, ideally those that take into account the causal, temporal, and completeness of steps towards achieving a goal. However, these are out of the scope of the current project and we hope future works can explore this direction.\n\n[1] Huang et al. \u201cLanguage models as zeroshot planners: Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agent.\u201d ICML 2022.\n\n[2] Sakaguchi et al. \u201cproScript: Partially ordered scripts generation\u201d. In Findings of the Association for Computational Linguistics: EMNLP 2021.\n\n[3] Novikova et al. \u201cWhy We Need New Evaluation Metrics for NLG\u201d EMNLP 2017\n\n[4] Celikyilmaz et al. \u201cEvaluation of Text Generation: A Survey\u201c"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8605/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237176664,
                "cdate": 1700237176664,
                "tmdate": 1700670799012,
                "mdate": 1700670799012,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]