[
    {
        "title": "SCALE: Synergized Collaboration of Asymmetric Language Translation Engines"
    },
    {
        "review": {
            "id": "u4A9R9O4vR",
            "forum": "yisfNWUEsD",
            "replyto": "yisfNWUEsD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5552/Reviewer_xCiG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5552/Reviewer_xCiG"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the SCALE framework, which integrates specialized Translation Models (STM) with Large Language Models (LLMs) such as GPT-4, utilizing triplet examples for few-shot learning. Each triplet incorporates an additional element: a translation generated by the STM, supplementing the standard source and target sentences. The study demonstrates significant enhancements in translating from various low-resource languages into English (X\u2192En). Furthermore, the paper conducts ablation studies to show the individual contributions of each SCALE component to the overall performance improvement."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is easy to understand and well written.\n- The idea is simple but effective. The SCALE framework shows substantial improvement for low-resource languages when translating into English.\n- The paper did a good ablation study to show the essential role of each component in the SCALE framework."
                },
                "weaknesses": {
                    "value": "Major weaknesses:\n- I like the idea; however, the paper focuses on translation into English and does not demonstrate the efficacy of translating from English to these low-resource languages. I suspect that SCALE has limitations in translating from English, given that refinement is core to SCALE. Although GPT-4 excels in English, it falls short in low-resource languages, suggesting that SCALE may not be effective for En\u2192X.\n- SCALE seems confined to low-resource languages, as LLM already performs excellently in translating between high-resource languages, such as German and French. The paper should make it clearer for readers how SCALE fares in the context of translating high-resource languages.\n- In presenting \"pivoting\" (Sec. 4.2) and \"updating\" (Sec. 4.3), the paper restricts its experiments to LAO\u2192En and Xhosa\u2192En, respectively, which doesn't sufficiently substantiate this aspect. Conducting more comprehensive experiments would provide stronger support for the concept.\n\nMinor weaknesses:\n- The paper's assertion that SCALE \"mitigates both language bias and parallel data bias\" seems peculiar to me. It feels intuitive rather than conclusively factual. Defining \"language bias\" and \"parallel data bias,\" followed by a quantitative analysis, might be necessary for clarity.\n- The choice of a less advanced model like GPT-2 for evaluating GPT-4's fluency strikes me as odd. While employing GPT-2 might yield reasonable comparative results for less sophisticated generation models, but I do not think it is proper here."
                },
                "questions": {
                    "value": "I would be appreciate if the authors can address my concerns above!"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5552/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5552/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5552/Reviewer_xCiG"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5552/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698614759713,
            "cdate": 1698614759713,
            "tmdate": 1700521812890,
            "mdate": 1700521812890,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "W9K7jS1UP5",
                "forum": "yisfNWUEsD",
                "replyto": "u4A9R9O4vR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5552/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5552/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe sincerely appreciate your time and insightful comments on our paper. Your commendation of our idea is encouraging, and we are eager to address the concerns you raised. Further, we kindly request that you reconsider your score should our responses help alleviate some of your concerns.\n\nConcern #1: The efficacy of the En\u2192X translation setting\n\nResponse: We appreciate your insightful comment and agree that evaluating the efficacy of En\u2192X translation is crucial. To address this, we have expanded our experiments to include translations from English to six low-resource languages, using the same experimental setup as that of SCALE-refine. The results, gauged by COMET-22 and COMETKiwi, are presented below, and all corresponding outputs have been appended to the supplementary zip files for your reference. These additional experiments lead us to several observations:\n\n1. SCALE exemplifies its proficiency in translating from English to a diverse range of languages, spanning various scripts and linguistic families.\n2. While SCALE shows improvement over few-shot GPT-4 and NLLB, the magnitude of this improvement is less pronounced compared to the X\u2192En setting. This indicates that the present limitation for LLM translators mainly lies in the generation of the target language, rather than the comprehension of the source language, as also suggested in [1].\n3. Due to the limited transparency of the GPT-4 model, it is challenging to attribute these inconsistencies to specific factors like tokenization, data coverage, or the proportion of languages in the training data. Regardless, we are striving to identify a potential indicator, specifically the number of native speakers, to illustrate the extent of a language's resources. As illustrated in the accompanying table, the performance of few-shot GPT-4 diminishes with the number of native speakers, while our framework, SCALE, consistently and effectively mitigates this language bias, outperforming few-shot GPT-4 and the supervised NLLB model in 6/6 directions.\n\nRecognizing the significance of these findings, we would include this discussion in the final version of our paper. \n\n|                   |   Javanese    |     Tamil     |    Urdu     |    Amharic    |     Khmer     |    Nepali     |\n| --------------- | :-----------: | :-----------: | :---------: | :-----------: | :-----------: | :-----------: |\n| # Native Speakers |     98 M      |    84.12 M    |   71.29 M   |     25 M      |     18.4M     |     16 M      |\n|  few-shot GPT-4   |   83.9/75.8   |   83.5/80.8   |  80.0/80.4  |   77.1/73.4   |   74.3/74.7   |   80.1/86.4   |\n|     NLLB-3.3B     |   86.4/76.4   |   86.5/82.9   |  80.7/80.4  |   84.4/80.7   |   76.3/77.8   |   77.0/82.0   |\n|       SCALE       | **86.6/77.5** | **87.8/84.7** | **82/81.7** | **84.7/81.7** | **79.6/80.3** | **82.6/87.8** |\n\n---\nConcern #2: Clear motivation for low-resource setting.\n\nResponse: We acknowledge your concern about the perceived focus of SCALE on low-resource languages and apologize if our manuscript did not communicate this aspect clearly. Your suggestion to clarify the motivation of our experimental design is well taken. \n\nWhile it is indeed true that current LLMs demonstrate exceptional performance in high-resource settings, our curiosity was piqued regarding the performance of SCALE in high-resource languages. We carried out experiments in three languages: German, Czech, and Chinese, under the same settings as SCALE-refine. We utilized COMET-22 and COMETKiwi to measure the translations. As shown below, the results reveal that SCALE offers consistent improvements even for high-resource languages, albeit less significantly than for low-resource languages, and it exhibits robust performance even when paired with a less performant STM (especially in en-zh direction). This is remarkable, given the already strong performances of models such as GPT-4 and NLLB. All the generated output has been updated to the supplementary zip files for your reference.\n\nWe assure you that these insights will be integrated into the revised version of our manuscript to provide a more comprehensive understanding.\n\n|                |     en-de     |     de-en     |     en-cs     |     cs-en     |     en-zh     |     zh-en     |\n| ------------ | :-----------: | :-----------: | :-----------: | :-----------: | :-----------: | :-----------: |\n| few-shot GPT-4 |   88.5/84.4   | **90.0**/84.6 |   92.0/86.8   |   88.4/85.0   | 88.8/**84.7** |   87.3/84.3   |\n|   NLLB-3.3B    |   86.9/82.9   |   89.1/84.2   |   90.1/84.8   |   88.4/85.5   |   78.0/70.9   |   86.1/83.7   |\n|     SCALE      | **89.0/85.1** | 89.9/**84.7** | **92.4/87.1** | **89.2/86.0** | **89.1/84.7** | **87.8/84.9** |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5552/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367423785,
                "cdate": 1700367423785,
                "tmdate": 1700367423785,
                "mdate": 1700367423785,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XKyU8sxsVL",
                "forum": "yisfNWUEsD",
                "replyto": "u4A9R9O4vR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5552/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5552/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xCiG (continued)"
                    },
                    "comment": {
                        "value": "Concern #5: The selection of GPT-2 for experiments.\n\nResponse: We understand your concerns about our choice of using GPT-2 in our experiments. The use of GPT-2 in this context is solely as an evaluator to measure the fluency of translations, a role it has successfully fulfilled in recent studies such as [2]. Despite being a model from four years ago, GPT-2 still serves as an effective evaluator for assigning perplexity scores to English sentences.\n\nTo address your concern and ensure the reliability of our results, we also conducted the same experiments using Llama-2-7B and Llama-2-13B. The results of these tests are shown below. While Llama2 gives different absolute values of perplexity of STM, GPT-4, and SCALE, the relative order remains identical to that obtained with GPT-2, thereby reinforcing our confidence in the original findings.\n\nThis is the original results produced by GPT-2-XL:\n\n|       | Assamese  | Armenian  |  Amharic  |   Xhosa   |  Uyghur   |   Khmer   |  Nepali   |  Sindhi   |\n| ----- | :-------: | :-------: | :-------: | :-------: | :-------: | :-------: | :-------: | :-------: |\n| NLLB  |   45.00   |   34.02   |   45.29   |   42.11   |   37.51   |   34.49   |   39.90   |   40.80   |\n| GPT-4 | **34.61** |   36.11   |   38.83   |   34.70   |   32.47   |   32.73   |   34.75   | **31.82** |\n| SCALE |   36.35   | **31.01** | **33.98** | **33.26** | **32.34** | **30.79** | **34.09** |   32.97   |\n\nThis is the results produced by Llama-2-7B:\n\n|       | Assamese  | Armenian  |  Amharic  |   Xhosa   | Uyghur |   Khmer   |  Nepali   |  Sindhi   |\n| ----- | :-------: | :-------: | :-------: | :-------: | :----: | :-------: | :-------: | :-------: |\n| NLLB  |   27.08   |   21.39   |   26.88   |   25.72   | 24.53  |   22.70   |   23.74   |   25.32   |\n| GPT-4 |   21.77   |   21.18   |   23.64   |   21.26   | 20.50  |   20.63   |   21.33   | **19.90** |\n| SCALE | **21.68** | **18.94** | **21.20** | **20.82** | **20.29**  | **19.63** | **20.24** |   20.37   |\n\nThis is the results produced by Llama-2-13B:\n\n|       | Assamese  | Armenian  |  Amharic  |   Xhosa   |  Uyghur   |   Khmer   |  Nepali   |  Sindhi   |\n| ----- | :-------: | :-------: | :-------: | :-------: | :-------: | :-------: | :-------: | :-------: |\n| NLLB  |   27.41   |   21.37   |   26.73   |   26.01   |   24.74   |   22.90   |   23.68   |   25.42   |\n| GPT-4 | **22.01** |   21.53   |   23.75   |   21.01   |   20.58   |   20.68   |   21.23   | **20.07** |\n| SCALE |   22.11   | **18.84** | **21.21** | **20.63** | **20.49** | **19.80** | **20.12** |   20.41   |\n\n[1] Raunak, Vikas, et al. \"Leveraging GPT-4 for Automatic Translation Post-Editing.\"\u00a0*arXiv preprint arXiv:2305.14878*\u00a0(2023).\n\n[2] Hendy, Amr, et al. \"How good are gpt models at machine translation? a comprehensive evaluation.\"\u00a0*arXiv preprint arXiv:2302.09210*\u00a0(2023)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5552/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367542069,
                "cdate": 1700367542069,
                "tmdate": 1700367624963,
                "mdate": 1700367624963,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gdMOrZMKqC",
                "forum": "yisfNWUEsD",
                "replyto": "u4A9R9O4vR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5552/Reviewer_xCiG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5552/Reviewer_xCiG"
                ],
                "content": {
                    "title": {
                        "value": "Thank authors for rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the authors for providing comprehensive results. This indeed makes me better understand how SCALE works on en->xx and high-resource languages. It surprises me that SCALE also works well on en->xx (though less pronounced). Although this method is not very helpful for high-resource languages, I believe this paper should be interesting to researchers interested in low-resource translation. Thus, I have raised my score. The authors seem to conduct extra experiments for four new low-resource languages (only Amharic and Nepali overlap with the paper), not the original eight low-resource languages, i.e., Assamese (asm Beng), Armenian (hye Armn), Amharic (amh Ethi), Xhosa (xho Latn), Uyghur (uig Arab), Khmer (khm Khmr), Nepali (npi Deva), and Sindhi (snd Arab). Why is this?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5552/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700521796089,
                "cdate": 1700521796089,
                "tmdate": 1700521939108,
                "mdate": 1700521939108,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K2ZUdWmqXU",
                "forum": "yisfNWUEsD",
                "replyto": "u4A9R9O4vR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5552/Reviewer_xCiG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5552/Reviewer_xCiG"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response. I am inclined to maintain my score for the following reasons:\n\n- The scope of the initial paper primarily focuses on xx->en translations in low-resource languages. While the authors have presented results for high-resource languages and en->xx translations, their effectiveness appears less pronounced.\n- I would be more inclined to rate the paper based on the initial submission. I acknowledge that some of results were shown in the rebuttal, but the manuscript could be substantially improved further with these extra experiments, e.g.,  en->xx translation results, results for high-resource languages, and a more comprehensive ablation study.\n\nNonetheless, this work should be of interest to those engaged in LLM machine translation and research in low-resource languages."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5552/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681746554,
                "cdate": 1700681746554,
                "tmdate": 1700681850957,
                "mdate": 1700681850957,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rCV6L39Uy9",
            "forum": "yisfNWUEsD",
            "replyto": "yisfNWUEsD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5552/Reviewer_tFyE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5552/Reviewer_tFyE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework SCALE, to incorporate the translation ability of a specialized model into a LLM. The idea is to provide the results of a STM as extra input to the LLM, and let LLM generate new translations considering the input. This idea is quite simple and effective. It also works well in providing updating translations for refinement, pivoting and updating purposes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper demonstrated a nice way of incorporating STM into LLMs.\n\nThe method is useful in three different senarios (refinement, pivoting and updating) in almost the same way.\n\nAccording to the evaluation, the results are better than both the STM and LLMs."
                },
                "weaknesses": {
                    "value": "My main concerns are in the analysis part:\n\nFor the analysis in 5.1, although the perplexity of a GPT2-XL decreases after SCALE refinement, there is no evidence that the results of larger LMs decreases as well. It is highly likely that the original results are more preferred by GPT4 than the refined results.\n\nIt is not straightforward to me why the STM results are with higher NM score than GPT4, but based on these results, SCALE achieves results with lower NM than GPT4.\n\nIt is not even clear for me whether the NM or USW should be higher or lower. According to the analysis, GPT4 produces results that are more literal (which is bad ), while STM have results more figurative. But it seems to me that GPT4 understand source languages better than STM, which is more likely to generate figurative translations.\n\nBesides, since the LLMs learn how to use the latent variable Z by the examples, it might be extremely important to choose proper demonstrations. It will be useful to check the effects of different demonstrations."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5552/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698684340133,
            "cdate": 1698684340133,
            "tmdate": 1699636570349,
            "mdate": 1699636570349,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gxQd5mqvvf",
                "forum": "yisfNWUEsD",
                "replyto": "rCV6L39Uy9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5552/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5552/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your comprehensive feedback and for investing your time in reviewing our paper. We appreciate your insightful comments and understand your concerns. Allow us to provide some clarifications.\n\nFirstly, we would like to point out that the purpose of our Analysis section is to delve into the translation characteristics of the LLM, STM, and SCALE, rather than to present overall quality metrics such as COMET or BLEU. Regarding the perplexity score, we did not aim to decrease the perplexity of the GPT2-XL model. Instead, our intention was to utilize GPT2-XL as an evaluator to assess the fluency of the translations generated by these three translation systems. The conclusion drawn was that when STM-generated output is used as in-context demonstrations, GPT-4 has the potential to generate more fluent translations (similar pattern is also found when using Llama-2 as evaluator as shown in our response to Reviewer xCiG).\n\nWe understand your confusion regarding the connection between fluency and quality. However, we abstained from establishing a direct correlation as LLM-based translations often exhibit higher fluency, irrespective of the quality of the translation. For a more detailed analysis, we refer to Section 5.2 in [1].\n\nMoving on to the NM and USW score, this metric is designed to measure the literalness of the translation output. Interestingly, the higher NM score observed for STM resulted in a lower NM score for SCALE. A conceivable explanation for this is that in the triplet in-context learning, the role of the STM-generated output is not to instruct the LLM to generate specific words, but rather to constrain the paraphrastic expression space, thereby helping the LLM to achieve a higher translation quality by comparing $Z$ and $Y$ in the triplet demonstrations. As mentioned in our paper, SCALE tends to maintain the original structure of the LLM output, whilst incorporating word selection cues from the STM.\n\nWith respect to your question about the selection strategies for in-context learning samples, we did conduct some preliminary studies to evaluate their impact. Surprisingly, the performance of SCALE did not show a significant variation with the selection strategy. The three strategies we used were (1) first-k, (2) random, and (3) maximum-coverage, as suggested in [2]. No significant performance differences were observed among these three methods. We value your suggestion and will ensure this aspect is included in the final draft of our paper.\n\nWe hope that our clarifications have satisfactorily addressed your concerns. We kindly request you to reconsider your score in light of these explanations. We welcome any further questions or concerns you may have and assure you that we are committed to improving our work based on your valuable feedback.\n\nThank you once again for your time and effort.\n\n[1] Hendy, Amr, et al. \"How good are gpt models at machine translation? a comprehensive evaluation.\"\u00a0*arXiv preprint arXiv:2302.09210*\u00a0(2023).\n\n[2] Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2023.\u00a0[In-context Examples Selection for Machine Translation](https://aclanthology.org/2023.findings-acl.564). In\u00a0*Findings of the Association for Computational Linguistics: ACL 2023*, pages 8857\u20138873, Toronto, Canada. Association for Computational Linguistics."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5552/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367255296,
                "cdate": 1700367255296,
                "tmdate": 1700367255296,
                "mdate": 1700367255296,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6cs5hV3ira",
            "forum": "yisfNWUEsD",
            "replyto": "yisfNWUEsD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5552/Reviewer_RsWP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5552/Reviewer_RsWP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework to combine Large Language Models (LLMs) and Specialized Translation Models (STMs). Specifically, The authors first sample translation candidate from a STM. Then they combine 1) source sentence, 2) STM's translation and 3) ground-truth reference into a triplet. By providing 10 triplets as demonstrations, LLMs learn through in-context learning and refine STM's translation. Further experiments show its superiority in low-resource translation and continual learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This work is comprehensive, with both the main experiment and the analysis experiment effectively conveying the author's perspective. The writing is clear and well-structured."
                },
                "weaknesses": {
                    "value": "1. The idea is relatively straightforward. The SCALE framework and experiments are more engineering-oriented, lacking scientific insight.\n\n2. The baselines used for comparison are weak. There are many previous works, such as back translation, pretraining, and other improvements for low-resource languages, which may require fewer resources and perform better.\n\n3. Inference cost. The SCALE model involves two types of decoding:  STM decoding and LLM decoding. It is important to examine the computational cost associated with the inference process of the SCALE model."
                },
                "questions": {
                    "value": "1. Why not conduct translation experiments on languages with slightly higher resources, such as German, French, or Chinese? In these languages, the improvements of SCALE may be limited."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5552/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835554541,
            "cdate": 1698835554541,
            "tmdate": 1699636570162,
            "mdate": 1699636570162,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JEVLyNa5N0",
                "forum": "yisfNWUEsD",
                "replyto": "6cs5hV3ira",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5552/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5552/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nFirstly, we would like to express our sincere gratitude for your time and effort in reviewing our paper. We appreciate your thoughtful feedback and are eager to address your concerns and questions.\n\nConcern #1: Lack of scientific insight\n\nResponse: We acknowledge your viewpoint, but we stand firmly to dispute your claim that our work is predominantly engineering-oriented and lacks scientific insight. We wish to assert with utmost certainty that our work is not confined to the realm of engineering but makes significant strides in contributing to the scientific literature in these crucial ways:\n\n1. Originality: We have pioneered an approach that seamlessly integrates two distinct translation paradigms within a single framework. This is a first in the field and enriches the existing body of knowledge by demonstrating how these two asymmetric translation models can work in synergy through the mechanism of triplet in-context learning.\n2. Rigorous Experimentation: Our research encompasses stringent experiments that authenticate our approach. These tests extend beyond mere engineering prowess to involve scientific investigation, presenting empirical proof of how triplet in-context learning can bolster the performance of translation models in a variety of scenarios, including refinement, pivoting, and updating.\n3. In-depth Analysis: We have conducted a thorough scrutiny of the translation traits of STM, LLM, and SCALE, delineating the subtle dissimilarities among these three systems. Furthermore, we have undertaken a sweeping analysis of SCALE to validate each critical design in our framework, enhancing our comprehension of SCALE in terms of its robustness and latency cost.\n4. Far-reaching Implications: The outcomes of our research have profound implications for the design of future translation systems. They equip practitioners with strategic insights on when to employ large language models, specialized models, or a blend of the two. Additionally, our study tackles a pivotal question in machine translation literature in the era of Large Language Models: Should we continue to train specialized translation models, or should we wholeheartedly adopt LLM-based translation? Our paper breaks new ground by proposing a feasible solution that amalgamates these two systems within a single framework.\n\nWe are confident that this articulates the scientific merit of our work. We will make it a point to highlight these aspects more prominently in our revised manuscript to forestall any potential misconceptions.\n\n---\nConcern #2: Weak baseline\n\nResponse: We would like to clarify that we have selected three types of baselines for our study: supervised models (NLLB, M2M100), few-shot LLMs (XGLM, GPT-3.5, and GPT-4), and one commercial translation system (Microsoft Translator). To our knowledge, these models are among the strongest and most representative of their respective categories. In particular, NLLB and MS Translator have already employed techniques such as back-translation and pretraining, and NLLB remains the state-of-the-art model on the Flores datasets.\n\nMoreover, our SCALE framework is not restricted to certain translation models. We believe that SCALE would also benefit from other strong baseline models by employing it as STM. If you could provide any additional models you believe we overlooked, we would greatly appreciate it. This would allow us to further validate and enhance the applicability of our SCALE framework.\n\n---\nConcern #3: Inference cost. \n\nResponse: We appreciate your valid concerns regarding the computational cost associated with the inference process of the SCALE model.\n\nIn Section 5.4 of our paper, we have indeed addressed this concern. We have considered two additional costs brought about by SCALE: the STM decoding and the extended context for LLM inference. Our conclusion is that the latency incurred can be primarily attributed to the extended context window due to the transformer's quadratic time complexity. For more details, we kindly refer you back to our paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5552/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367016002,
                "cdate": 1700367016002,
                "tmdate": 1700367200595,
                "mdate": 1700367200595,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9GPt3RwK0F",
                "forum": "yisfNWUEsD",
                "replyto": "6cs5hV3ira",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5552/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5552/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Question: Experiments on high-resource languages\n\nResponse: In our experiment design, we primarily evaluate SCALE in the challenging low-resource setting. This decision was informed by current research findings that indicate Large Language Models already perform comparably with supervised models on high-resource languages, yet struggle with low-resource languages \\[1][2]. Therefore, we believe it is more meaningful and challenging to focus on improving LLMs in this lower-resource setting.\n\nTo resolve some of your concerns, we still would like to test SCALE performance in the high-resource setting. We carried out experiments in three languages on the Flores datasets: German, Czech, and Chinese, under the same settings as SCALE-refine. We utilized COMET-22 and COMETKiwi to measure the translations. As shown below, the results reveal that SCALE offers consistent improvements even for these high-resource languages, albeit less significantly than for low-resource languages as you predicted, and it exhibits robust performance even when paired with a less performant STM (especially in en-zh direction). This is remarkable, given the already strong performances of models such as GPT-4 and NLLB. All the generated output has been updated to the supplementary zip files for your reference.\n\nWe assure you that these insights will be integrated into the revised version of our manuscript to provide a more comprehensive understanding.\n\n|                | en-de         | de-en         | en-cs         | cs-en         | en-zh         | zh-en         |\n| -------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |\n| few-shot GPT-4 | 88.5/84.4     | **90.0**/84.6 | 92.0/86.8     | 88.4/85.0     | 88.8/**84.7** | 87.3/84.3     |\n| NLLB-3.3B      | 86.9/82.9     | 89.1/84.2     | 90.1/84.8     | 88.4/85.5     | 78.0/70.9     | 86.1/83.7     |\n| SCALE          | **89.0/85.1** | 89.9/**84.7** | **92.4/87.1** | **89.2/86.0** | **89.1/84.7** | **87.8/84.9** |\n\nWe hope that our responses have addressed your concerns adequately. We would greatly appreciate it if you could consider revising your score in light of these clarifications. Please feel free to ask any further questions or voice any other concerns you might have.\n\n[1] Garcia, Xavier, et al. \"The unreasonable effectiveness of few-shot learning for machine translation.\"\u00a0*International Conference on Machine Learning*. PMLR, 2023.\n\n[2] Vilar, David, et al. \"Prompting palm for translation: Assessing strategies and performance.\"\u00a0*arXiv preprint arXiv:2211.09102*\u00a0(2022)."
                    },
                    "title": {
                        "value": "Response to  Reviewer RsWP (continued)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5552/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367028750,
                "cdate": 1700367028750,
                "tmdate": 1700367209809,
                "mdate": 1700367209809,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]