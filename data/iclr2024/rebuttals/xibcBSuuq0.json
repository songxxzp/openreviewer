[
    {
        "title": "Do not Start with Trembling Hands: Improving Multi-agent Reinforcement Learning with Stable Prefix Policy"
    },
    {
        "review": {
            "id": "WZgHO4PFTI",
            "forum": "xibcBSuuq0",
            "replyto": "xibcBSuuq0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9370/Reviewer_4uBa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9370/Reviewer_4uBa"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of trembling hands in multi-agent systems, namely the negative effect that exploration has on the coordination between agents. This effect is particularly evident when $\\espilon$-greedy policies are used as template policies. This work proposes a method to compute a template policy to be followed, instead of greedy policies, and offers some empirical evidence that the proposed method can be competitive on some experimental settings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The trembling hands problem plays a central role in multi-agent coordination and the idea to follow specifically designed policies instead of more standard policies might lead to original results."
                },
                "weaknesses": {
                    "value": "Unfortunately, the limitations of this work are extensive and I believe structural. The exposition is the main factor concerning this feedback, both from the point of view of the rhetorics and for the clarification of the original contributions. Here is a (non-exhaustive) list of points:\n- In the abstract it is claimed that [you] \" find that $\\epsilon$- greedy policies can be deemed...\", it is unclear how and why this was not already known. In its second part unclear. How do you compute such policies? What do you mean by \"plan an existing optimal policy\"? The description of what was done is unclear to me, and how this was done is absent.\n- The related works section addresses the background rather than the related works, and the background is insufficient in the exposition to provide tools to understand what will be done later. Trembling Hands Nash Equilibria are never defined, for example. This leads to the fact that in the proposed method, it was unclear to me what portions of the whole regime are proper contributions of the work and what are not. \n- The Theoretical Analysis is absent, meaning that in the way it is done is mostly unclear what it should suggest. \n- The Experimental Evaluation suggests some cases of competitiveness but does not compare the methods from a computational point of view, which I believe would help understand the pros and cons of the proposed method. Finally, it was not clear to me how the hyper-optimization of the Sota algorithms used as baselines was done, both in the standard case and in the SDD-augmented case.\n- A scientific analysis of the limitations would be needed.\n\nFinally, some English phrasing is wrong and some typos are present (for example there should be an $\\epsilon$ at the 9th line of the first page I believe)"
                },
                "questions": {
                    "value": "Unfortunately, the limitations seem extensive, and I believe a refactoring of the work is needed, I hope the comments suggest the portions of the work to be addressed, but I am open to further provide insights and discuss."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9370/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9370/Reviewer_4uBa",
                        "ICLR.cc/2024/Conference/Submission9370/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9370/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698596523366,
            "cdate": 1698596523366,
            "tmdate": 1700682941355,
            "mdate": 1700682941355,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "19fZSgLugy",
                "forum": "xibcBSuuq0",
                "replyto": "WZgHO4PFTI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9370/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9370/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to Weaknesses"
                    },
                    "comment": {
                        "value": "Thank you for your comments and suggestions. After reading the summary as well as the weaknesses, I think there might be some gaps between your understanding of our paper and the contents we want to express. From my perspective, you are interested in the topic that how our approach solves the trembling hands' problems. However, we find the $\\epsilon$-greedy methods in MARL are similar to the THPNE problem in game theory. We aim to solve the exploration-exploitation dilemma taken by the epsilon-greedy algorithm. Therefore, the paper is about the sequence of MARL problems instead of the one-step THPNE problem.  Despite of that, I will try my best to respond to your comments.\n\n> In the abstract it is claimed that [you] \" find that $\\epsilon$- greedy policies can be deemed...\", it is unclear how and why this was not already known. In its second part unclear. How do you compute such policies?  What do you mean by \"plan an existing optimal policy\"?\n\nBased on the $\\epsilon$-greedy algorithm, agents choose the best action with the probability of 1-$\\epsilon$ and a random action otherwise. In game theory, players who make decisions with trembling hands will choose actions greedily with a probability of $p$ and other (random) actions with $1-p$. In such a way, the $\\epsilon$-greedy method and the trembling hands' decision-making are correlated. Meanwhile [1] also states the relationship between $\\epsilon$-greedy and the trembling hands process.\n\nIn this paper, the stable prefix policy can be considered as a trajectory template. We establish a trajectory tree based on multiple historical interactive trajectories. In each episode, agents are in the initial state at the first time step. The trajectory tree will generate a potential optimal route of states from that initial state by greedily selecting the subsequent states with the largest value, which is the process of 'plan an existing optimal policy'.\n\n> The related works section addresses the background rather than the related works, and the background is insufficient in the exposition to provide tools to understand what will be done later.\n\nIn this paper, we are focusing on the MARL exploration problem rather than the solution to the THPNE problem. Thus, we introduce MARL and some NE definitions in the Related Work section and introduce MARL basic notations and settings in the Background section. These contents might be sufficient for introducing the subsequent MARL methods.\n\n> The Theoretical Analysis is absent, meaning that in the way it is done is mostly unclear what it should suggest.\n\nWe provide some theoretical analysis of the convergence guarantee and computational complexity of our algorithm in section 4.4. The THPNE problem is used for introducing the exploration dilemma, so we did not provide the theoretical analysis of the THPNE, which, we think, is not quite relevant to MARL problems in this paper.\n\n> it was not clear to me how the hyper-optimization of the Sota algorithms used as baselines was done, both in the standard case and in the SDD-augmented case.\n\nThese Sota algorithms aim at solving the MARL problem instead of the THPNE problem, so none of them introduce their methods to solve that. Based on their algorithms, our approach does not focus on the problem either.\n\n> some English phrasing is wrong and some typos are present (for example there should be an $\\epsilon$ at the 9th line of the first page I believe)\n\nWe will double-check the expression methods and typos in this paper. However, at the 9th line of the first page, we want to express that agents choose the best actions with the largest values. Thus, agents choose actions greedily and there should not be an epsilon here.\n\nIn the remaining time we would like to address the remaining concerns that you have that prevent you from accepting the paper. We would appreciate it if you could engage with us on the remaining concerns, as we want to address them.\n\n\n\n[1] Lee, Kiyeob, et al. \"Reinforcement learning for mean field games with strategic complementarities.\" International Conference on Artificial Intelligence and Statistics."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542905116,
                "cdate": 1700542905116,
                "tmdate": 1700542905116,
                "mdate": 1700542905116,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sg3DGbF9U3",
                "forum": "xibcBSuuq0",
                "replyto": "WZgHO4PFTI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9370/Reviewer_4uBa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9370/Reviewer_4uBa"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the Authors for the clarifications. I will proceed by points. What I was trying to point out is that the exposition of the work is the main concerning point to me, and I hope my comments will help:\n- In the abstract it is claimed that [you] \" find that - greedy policies can be deemed...\", it is unclear how and why this was not already known. In its second part unclear. How do you compute such policies? What do you mean by \"plan an existing optimal policy\"?\nIn general what I was trying to say is that the abstract is not clear in what will be done, and it is imprecise in some parts as well. As an example of the general, unprecise exposition, I commented that in the abstract it is said that \"[you] find that epsilon-greedy can be deemed as the concept ...\". As you suggested as well, this result was not \"found\" in this work, it is a known fact. This is not a big issue, but the comment was to suggest that the phrasing of the work might be substantially revisited.\n- The related works section addresses the background rather than the related works, and the background is insufficient in the exposition to provide tools to understand what will be done later.\nI agree with the authors saying that \"[they] are focusing on the MARL exploration problem rather than the solution to the THPNE problem\". For this reason indeed, I suggested inserting in the related works some comments on how the problem of exploration in MARL is managed, rather than a few lines on MARL algorithms that do not tackle the problem of exploration, and a section on the Trembling Hands Problem, that is not the focus of the paper. In this sense, the related works section is rather unrelated to the work. \n- The Theoretical Analysis is absent, meaning that in the way it is done is mostly unclear what it should suggest.\nIndeed a theoretical analysis of the THPNE was not requested at all. What I was trying to say is that:\nTheorem 1 is by [2]. Assumption 1 is not discussed, how likely is it to be verified? Theorem 2 is by [1]\nWhat I mean is that the authors should better discuss how the proposed approach maps to pre-existing theoretical results, otherwise this section is rather poor and unjustified as a \"Theoretical Analysis\".\n\n-  some English phrasing is wrong and some typos are present (for example there should be an at the 9th line of the first page I believe).\nI am still a little confused here, mostly by the fact that the revised version has indeed $1-\\epsilon$, so I really don't understand the answer \"there should not be an epsilon here\".\n\nGenerally, then, I think the global exposition of the work still hinders the overall quality, and I don't see substantial changes done to address that. Nonetheless, I am positive about raising my score in view of the other's comments. \n\n\n[1] Koenig & Simmons (1993)\n[2] Uchendu et al. (2023)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670786045,
                "cdate": 1700670786045,
                "tmdate": 1700688664870,
                "mdate": 1700688664870,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YPH7Hkbnpq",
                "forum": "xibcBSuuq0",
                "replyto": "Io53lsJCf0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9370/Reviewer_4uBa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9370/Reviewer_4uBa"
                ],
                "content": {
                    "title": {
                        "value": "Some Final Comments"
                    },
                    "comment": {
                        "value": "First of all, I am glad that the initial misunderstanding was solved. I believe the concerns I raised were mostly solved. \nI am aware that this work has more of an experimental contribution, but I would propose the following modifications to the \"Theoretical Analysis\" Section to reach what I believe would be more of a sound statement.\n\nWhat the authors show is that:\n - there exists an MDP instance with exponential sample complexity when $\\epsilon$-greedy is used.\n - provided the Assumption 1, Theorem 2 could be applied to show polynomial sample complexity upper bound.\n\nWhat I suggest for the current version is to just use the second portion, namely :\n- provided the Assumption 1, Theorem 2 could be applied to show polynomial sample complexity upper bound and explain what in your setting corresponds to the \" appropriate choice of training and evaluation process\", as it was done in the original paper.\n\nThe reason to avoid the first claim is that the link between a lower bound for $\\epsilon$-greedy and an upper bound of the proposed method is vacuous since you are comparing different algorithms. $\\epsilon$-greedy has exponential lower bounds yet it does not require any assumption, and I am not confident excluding that there might exist another MDP where your assumption is not satisfied and the exponentiality returns\n\nGenerally, then, I would call the section \"Sample Complexity Analysis\", since it is too limited to be called a Theoretical Analysis.\n\n\nMore generally:\n - avoid claiming that \"[you] showed that\", since this would be an over-statement. The correct phrasing would be something like  \"We linked our algorithm to the framework in ..\"\n- when using another work's theorem, please use the format \"Theorem i [ Cite, theorem k]\" where the first number is in your ordering and the second links to the theorem in the original paper. This is needed for theoretical reproducibility."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731822326,
                "cdate": 1700731822326,
                "tmdate": 1700731822326,
                "mdate": 1700731822326,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s7gouy797H",
                "forum": "xibcBSuuq0",
                "replyto": "WZgHO4PFTI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9370/Reviewer_4uBa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9370/Reviewer_4uBa"
                ],
                "content": {
                    "title": {
                        "value": "Comments for the Area Chair"
                    },
                    "comment": {
                        "value": "Provided that the Authors refactor the section \"Theoretical Analysis\" as suggested, most of my concerns have been solved but I think the is still room for improvement. I am mostly worried about other's reviewer's doubts about the empirical corroboration, which can be shared, but I think I would need to know Reviewer RkkR's impression to give a final judgment."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732376267,
                "cdate": 1700732376267,
                "tmdate": 1700737635513,
                "mdate": 1700737635513,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zDGIRGC8oM",
            "forum": "xibcBSuuq0",
            "replyto": "xibcBSuuq0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9370/Reviewer_3kTw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9370/Reviewer_3kTw"
            ],
            "content": {
                "summary": {
                    "value": "In order to alleviate the Trembling Hands Nash Equilibrium solution caused by the $\\varepsilon$-greedy method in multi-agent reinforcement learning, this paper proposes a Stable Prefix Policy (SPP). SPP can rebalance the exploration and exploitation process when the policy of agents is close to the optimal policy during the training process. The specific method is to implement a Monte-Carlo Trajectory Tree (MCT$^2$) to preserve the structure of previous trajectories, which can plan the existing optimal trajectory template. When agents follow this template during rollouts, the target value is assembled with other target values with the same trajectories. When the agents drop out from the template, the $\\varepsilon$-greedy method is activated afterward. SPP can be applied to any value decomposition framework, and experimental results in SMAC and MPE show that it can improve the performance of the basic algorithm."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper introduces the concept of the trembling hands into cooperative multi-agent reinforcement learning, which is reasonable and novel. The two didactic tasks in the introduction section fully demonstrate that the Trembling Hand Perfect Nash Equilibrium does exist in multi-agent tasks, which provides sufficient reasons for the proposal of the Stable Prefix Policy.\n2. This paper implements MCT$^2$, which can plan an existing optimal trajectory (EOT) based on the trajectories in the replay buffer. SPP calculates the target value for TD update by comparing the actual trajectory of the agent with EOT, which is indeed a very novel approach.\n3. Key resources (proofs, code, and replay videos) are available, and sufficient details are described such that an expert should be able to reproduce the main results.\n4. The experimental results are thoroughly analyzed. For example, The dropout time step ratio in Figure 7 illustrates the working mechanism of SPP and is intuitive."
                },
                "weaknesses": {
                    "value": "1. The proposed method is based on the premise that agents should be capable of finding a policy toward success from historical interactions. In other words, SPP relies heavily on the performance of the underlying algorithm.\n2. The trembling hands is a concept in multi-agent games, but this paper only provides solutions in cooperative scenarios (Dec-POMDP problems). At the same time, SPP is only applied to value decomposition methods.\n3. MCT$^2$ introduces more hyperparameters, which increases the difficulty and workload of hyperparameter tuning.\n4. The proposed method was only evaluated on SMAC (the description of the experimental results in MPE is skimpy and unconvincing). SMAC is a popular multi-agent experimental platform but has been pointed out to have many shortcomings [1]. More and more researchers in the MARL community advocate conducting experiments in multiple different domains to evaluate the proposed algorithm comprehensively [2].\n\n**Reference**\n\n[1] Ellis, Benjamin et al. SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning. 2022.\n\n[2] Gorsane, R. et al. Towards a Standardised Performance Evaluation Protocol for Cooperative MARL. 2022."
                },
                "questions": {
                    "value": "1. What is the value of the hyperparameter $t_{inter}$? How does its value affect the performance?\n2. The target value $y_t$ in vanilla QMIX is $y_t = r_t+\\gamma\\max_{a^{t+1}}Q_{tot}(s^{t+1}, a^{t+1} )$, which is related to $s^{t+1}$. Why is $y^t$ still related to $s^t$ in Eq. (3)?\n3. Is there any theoretical basis to prove that $Q^t_{assem}$ is more accurate than the original $Q_{tot}$?\n4. I think that in some scenarios, the SPP variant may be more likely to fall into a local optimal solution. Suppose that in such a scenario, agents can easily access the state corresponding to the suboptimal solution, while the state corresponding to the global optimal solution is in the opposite direction and relatively difficult to access (for example, further away from the initial position of the agents). The SPP variant may directly give up early exploration and find it difficult to converge to the global optimal solution. Of course, the above issue can be alleviated by adjusting $c_{ucb}$, but this requires sufficient prior knowledge."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9370/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9370/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9370/Reviewer_3kTw"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9370/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698651891023,
            "cdate": 1698651891023,
            "tmdate": 1700549348040,
            "mdate": 1700549348040,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gluaQG7n0c",
                "forum": "xibcBSuuq0",
                "replyto": "zDGIRGC8oM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9370/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9370/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to Weaknesses"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed review of our paper. We are motivated that you found the new perspective interesting and inspiring. After reading the reviews, we address your concerns below. Please let us know if further clarification is needed.\n\n> The proposed method is based on the premise that agents should be capable of finding a policy toward success from historical interactions. In other words, SPP relies heavily on the performance of the underlying algorithm.\n\nOur SPP approach divides the decision-making process into two stages: pre-decision based on planning and post-decision with action noise. Where the pre-decision is given through UCT planning, it is not necessarily required that the historical interactions contain a successful policy, which means that even if there are no successful policies in the historical trajectory, UCT planning is still able to select the regions with the highest value or exploration potential at the moment based on the existing interactions. In contrast to relying heavily on the base policy, our SPP approach takes advantage of this favorable property of UCT planning and instead improves the performance of the base policy. Our experimental results on SMAC benchmarks can fully validate this.\n\n> The trembling hand is a concept in multi-agent games, but this paper only provides solutions in cooperative scenarios (Dec-POMDP problems). At the same time, SPP is only applied to value decomposition methods.\n\nThank you very much for your comments. For the perspective of better demonstrating the performance improvement of our SPP method compared to the original MARL method, we choose to test our method on the most popular benchmarks with MARL algorithms in the current MARL community, which mostly require computing the joint utility function to reach a better equilibrium of the game, and thus focus on solving the problem of credit allocation for cooperative MARL. Technically, our SPP method works for those scenarios with suboptimal equilibria due to trembling hands. We will actively look for MARL benchmarks other than cooperative scenarios for testing and welcome your further suggestions.\n\n> SMAC is a popular multi-agent experimental platform but has been pointed out to have many shortcomings [1]. More and more researchers in the MARL community advocate conducting experiments in multiple domains to evaluate the proposed algorithm comprehensively [2].\n\nWe also fully agree with this comment that the SMACv1 environment lacks randomness in initializing the task settings. SMACv2 in contrast provides the configuration settings including the number of agents and enemies, the initialized positions, and the probability of generating an agent type. However, as far as we are concerned, this configuration provides too much randomness (I'd like to call it unstable). We believe SMACv2 is more suitable to test the migration capability and few-shot scenario adaptation ability of an algorithm. Despite that, we tested our approach on three scenarios of the SMACv2 environment. The items in the table are the average winning rate among 5 seeds and their variance.\n\n| 5_vs_5       | 0.5M           | 1M             | 1.5M           | 2M             |\n| ------------ | -------------- | -------------- | -------------- | -------------- |\n| MAPPO        | 8,67$\\pm$4.82  | 21.39$\\pm$7.91 | 28.77$\\pm$6.56 | 35.94$\\pm$9.24 |\n| QMIX         | 20.15$\\pm$4.59 | 32.68$\\pm$5.80 | 41.48$\\pm$5.74 | 50.21$\\pm$3.12 |\n| Our Approach | 19.07$\\pm$5.62 | 38.97$\\pm$9.32 | 45.11$\\pm$8.34 | 52.24$\\pm$6.73 |\n\n| 10_vs_10     | 0.5M           | 1M             | 1.5M            | 2M              |\n| ------------ | -------------- | -------------- | --------------- | --------------- |\n| MAPPO        | 3.54$\\pm$2.50  | 11.84$\\pm$1.85 | 16.98$\\pm$2.18  | 30.85$\\pm$7.77  |\n| QMIX         | 11.09$\\pm$2.24 | 20.56$\\pm$4.48 | 31.54$\\pm$6.75  | 42.29$\\pm$12.41 |\n| Our Approach | 6.70$\\pm$2.87  | 24.48$\\pm$8.78 | 37.75$\\pm$10.35 | 43.45$\\pm$9.80  |\n\n| 10_vs_11     | 0.5M          | 1M            | 1.5M          | 2M             |\n| ------------ | ------------- | ------------- | ------------- | -------------- |\n| MAPPO        | 0.76$\\pm$0    | 0.83$\\pm$0.91 | 0.74$\\pm$0.47 | 1.84$\\pm$1.29  |\n| QMIX         | 0.57$\\pm$0.44 | 4.29$\\pm$2.61 | 11.6$\\pm$3.62 | 14.95$\\pm$2.05 |\n| Our Approach | 0.34$\\pm$0.75 | 4.51$\\pm$1.92 | 10.3$\\pm$4.54 | 12.67$\\pm$5.26 |\n\nAccording to the results, our proposed method can follow the performance of the baseline algorithm in the so-called unstable environments in 2M time steps."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541925851,
                "cdate": 1700541925851,
                "tmdate": 1700541925851,
                "mdate": 1700541925851,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KOQQMCODuP",
                "forum": "xibcBSuuq0",
                "replyto": "zDGIRGC8oM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9370/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9370/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to Questions"
                    },
                    "comment": {
                        "value": "> What is the value of the hyper-parameter $t_{inter}$? How does its value affect the performance?\n\nThe $t_{inter}$ value is the number of warm-up episodes. We need state data to train a KMeans classifier and this state data is collected within the $t_{inter}$ episodes. Additionally, the classifier and the MCT$^2$ are reconstructed every $t_{inter}$ episode to overcome the influence of policy shifts. In this paper, this hyper-parameter is empirically set as 500.\n\n> The target value $y^t$ in vanilla QMIX is $y^t=r^t+\\gamma \\max_{a_{t+1}}Q_{tot}(s^{t+1},a^{t+1})$, which is related to $s^{t+1}$. Why is $y^t$ still related to $s^t$ in Eq. (3)?\n\nSorry for the confusion caused by our mistakes. The $y^t$ is related to $s^{t+1}$ in Eq.(3) (currently Eq.(4) in the revised version). The revised formula is\n$$\ny^t=r^t + \\gamma[\\mathbb{1}(c^{t+1}=\\phi(s^{t+1}))\\cdot Q_{assem}^{t+1}(s^{t+1}) +(1-\\mathbb{1}(c^{t+1}=\\phi(s^{t+1})))\\cdot Q_{tot}(\\tau,a^{t+1})]\n$$\nand we have updated it in the paper.\n\n> Is there any theoretical basis to prove that $Q_{assem}^t$ is more accurate than the original $Q_{tot}$?\n\nIn this paper, similar states belong to a prototype cluster with a value, so similar states share the same value. The $Q_{assem}^t$ is the average value of $Q_{tot}$. In such a way, the expectation of $Q_{assem}^t$ is the same as the expectation of original $Q_{tot}$. The $Q_{assem}^t$ provides lower variance.\n\n> I think that in some scenarios, the SPP variant may be more likely to fall into a local optimal solution. Suppose that in such a scenario, agents can easily access the state corresponding to the suboptimal solution, while the state corresponding to the global optimal solution is in the opposite direction and relatively difficult to access (for example, further away from the initial position of the agents). The SPP variant may directly give up early exploration and find it difficult to converge to the global optimal solution. Of course, the above issue can be alleviated by adjusting cucb, but this requires sufficient prior knowledge.\n\nThank you very much for your comments, we would love to discuss this scenario with you. You say that states oriented to suboptimal solutions are very easy to reach while states oriented to global optimal solutions are very difficult to collect and that the vast majority of data collected by algorithms focusing on exploration in this scenario are suboptimal, and we believe that obtaining an accurate value estimation of the global optimal state that is distinguishable from the suboptimal data is also a very difficult task to solve in this case. Our approach includes an annealing-like process to target such scenarios, in addition to the possibility of tuning the parameters of the UCT to encourage exploration. During annealing the tree in $\\text{MCT}^2$ is destroyed and reconstructed, at which point the MARL algorithm resumes single-step exploration.\n\nIn the remaining time we would like to address the remaining concerns that you have that prevent you from accepting the paper. We would appreciate it if you could engage with us on the remaining concerns, as we want to address them."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542202439,
                "cdate": 1700542202439,
                "tmdate": 1700542202439,
                "mdate": 1700542202439,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gGYgNVT4RM",
                "forum": "xibcBSuuq0",
                "replyto": "KOQQMCODuP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9370/Reviewer_3kTw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9370/Reviewer_3kTw"
                ],
                "content": {
                    "title": {
                        "value": "I have raised my score to 5."
                    },
                    "comment": {
                        "value": "Thanks to authors for the reply. Most of my concerns have been solved. But I still hope to see the results in more representative multi-agent domains, such as Google Research Football, and some other environments that are not Dec-POMDPs. After all, the trembling hand is a concept in general multi-agent games. And I think this paper still has a lot of room for improvement.\n\nI have improved my score, but I think the contribution of SPP still needs more convincing experimental results to support it."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549389976,
                "cdate": 1700549389976,
                "tmdate": 1700549389976,
                "mdate": 1700549389976,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zwpI0TjvXj",
            "forum": "xibcBSuuq0",
            "replyto": "xibcBSuuq0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9370/Reviewer_RkkR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9370/Reviewer_RkkR"
            ],
            "content": {
                "summary": {
                    "value": "In order to balance between exploration and exploitation during the training process, the authors encourage the policy to follow the optimal trajectory as planned by a Monte-Carlo Trajectory Tree (MCT\u00b2). The MCT\u00b2 is built upon historical trajectories, wherein states are organized into clusters via KMeans clustering. Within the MCT\u00b2 framework, state values within the same cluster node are concurrently updated. The authors leverage PUCB values to find the optimal path across these clusters. During the rollout, when the actual state (cluster) diverges from the predicted state (cluster), the policy adopts an \u03b5-greedy approach to facilitate exploration.\nExperiments conducted within the SMAC benchmark show that the proposed method accelerates training and can be integrated into various MARL algorithms, including QMIX, QPLEX, and OW_QMIX."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors innovatively apply Monte-Carlo Tree structure into MARL context, leading to increased training speed. The proposed method may be applied to various existing MARL algorithms, thereby potentially contributes to the field of MARL research."
                },
                "weaknesses": {
                    "value": "The experiment results do not conclusively demonstrate the effectiveness of the proposed method. In Figure 8, the performance of the proposed policy closely mirrors that of the original QMIX implementation. I would suggest the authors to test on more challenging MARL benchmarks, though those benchmarks often require more exploration, which may pose challenges for the proposed method.\n\nAlso, many MARL algorithms already suffer from a lack of exploration. The proposed method, in its pursuit of faster convergence, makes the additional trade-off of further diminishing exploration in favor of exploitation. This strategy necessitates careful consideration due to the potential consequences it may have on the algorithm's overall effectiveness."
                },
                "questions": {
                    "value": "- In Section 6, the authors claim that the proposed method can be applied to the critic training in Actor-Critic MARL alrogithms. Can you briefly describe how to implement the proposed method in, say, MAPPO? And what is the performance improvement when applying to MAPPO?\n- In the matrix game presented in Section 1, should the $epsilon$ for player 1 be 0.1?\n- Can the proposed method be applied to scenarios with continuous action spaces?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9370/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699609932967,
            "cdate": 1699609932967,
            "tmdate": 1699637178595,
            "mdate": 1699637178595,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wN6VTYRaM6",
                "forum": "xibcBSuuq0",
                "replyto": "zwpI0TjvXj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9370/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9370/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respond to Weaknesses"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed review of our paper. After reading the reviews, we address your concerns below and update a revision of our paper. Please let us know if further clarification is needed.\n\n> In Figure 8, the performance of the proposed policy closely mirrors that of the original QMIX implementation. I would suggest the authors test on more challenging MARL benchmarks.\n\nIn this paper, we provide our main experimental results in Figure 5 which contains the challenging subtasks of the SMAC environment. Among the subtasks, the 5m\\_vs\\_6m, 3s\\_vs\\_5z, and 2c\\_vs\\_64zg tasks are hard tasks. Not all the baseline algorithms can achieve good results. Additionally, the subtasks including MMM2, 3s5z\\_vs\\_3s6z, and 6h\\_vs\\_8z are super-hard tasks. Experimental results on these tasks within 2M time steps can show the better performance of our approach. In contrast, Figure 8 shows the other tasks in the SMAC environment and most of them are easy tasks in which the original QMIX as well as other baseline algorithms can achieve acceptable results. This might be the reason why our proposed policy closely mirrors that of the original QMIX.\n\n> The proposed method, in its pursuit of faster convergence, makes the additional trade-off of further diminishing exploration in favor of exploitation. \n\nOur method suggests dividing the decision-making of the existing MARL methods into two phases: our Stable Prefix Policy and vanilla policy. SPP balances the exploration and exploitation during the trajectory planning process with UCT, and we further apply Dirichlet noise to the planning phase, which gives more explorations. SPP explores more globally and centrally compared to existing popular MARL methods."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700539486840,
                "cdate": 1700539486840,
                "tmdate": 1700539486840,
                "mdate": 1700539486840,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R6dnqxuXrA",
                "forum": "xibcBSuuq0",
                "replyto": "zwpI0TjvXj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9370/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9370/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Resond to Questions"
                    },
                    "comment": {
                        "value": "> In Section 6, the authors claim that the proposed method can be applied to the critic training in Actor-Critic MARL algorithms. Can you briefly describe how to implement the proposed method in, say, MAPPO? And what is the performance improvement when applying to MAPPO?\n\nIn the Actor-Critic MARL algorithms, actor networks are responsible for training the action distributions conditioned on observations and the critic networks train the values of states. Applying our SPP method to the Actor-Critic method includes (i)assembling the values with the same cluster nodes and (ii)removing the actors' explorations by taking the maximum of the distributions and using the trajectory templates generated by the planning process to determine whether to interrupt the process and resume single-step explorations like non-SPP-augmented methods. Our approach contributes to the performance improvement of the Actor-Critic method in two ways: on the one hand, assembles the values with the same cluster nodes so that the values for training are more accurate; on the other hand, it determines whether the elimination of the exploration of the first half of the strategy can lead to more fine-grained improvements in the second half of the strategy by planning, which has already balanced exploration and exploitation by UCT.\n\nTo focus on the efficiency aspects, we chose three subtasks to show the performance of the MAPPO and our SPP-augmented MAPPO within 2M time steps. The experimental results indicate that our method can improve the performance of Actor-Critic MARL methods. The items in the table are the average winning rates (%) among 5 seeds and their variances.\n\n| MMM2      | 0.5M          | 1M              | 1.5M            | 2M              |\n| --------- | ------------- | --------------- | --------------- | --------------- |\n| MAPPO     | 4.46$\\pm$9.97 | 17.27$\\pm$30.11 | 34.27$\\pm$35.44 | 42.65$\\pm$37.72 |\n| SPP+MAPPO | 9.34$\\pm$5.54 | 46.81$\\pm$19.63 | 51.03$\\pm$14.95 | 62.42$\\pm$14.98 |\n\n| 5m_vs_6m  | 0.5M           | 1M              | 1.5M            | 2M              |\n| --------- | -------------- | --------------- | --------------- | --------------- |\n| MAPPO     | 6.57$\\pm$1.47  | 17.83$\\pm$16.47 | 45.38$\\pm$15.48 | 49.58$\\pm$23.32 |\n| SPP+MAPPO | 36.58$\\pm$6.54 | 53.32$\\pm$11.27 | 57.57$\\pm$11.19 | 60.65$\\pm$10.56 |\n\n| 3s5z_vs_3s6z | 0.5M          | 1M             | 1.5M            | 2M              |\n| ------------ | ------------- | -------------- | --------------- | --------------- |\n| MAPPO        | 8.00$\\pm$1.79 | 1.96$\\pm$3.92  | 9.17$\\pm$17.84  | 13.09$\\pm$21.71 |\n| SPP+MAPPO    | 5.63$\\pm$0.97 | 7.29$\\pm$12.63 | 15.02$\\pm$26.02 | 20.80$\\pm$35.85 |\n\n\n\n\n\n> In the matrix game presented in Section 1, should the epsilon for player 1 be 0.1? \n\nYour concerns are much appreciated. We double-check the corresponding section. With $\\epsilon$ set to 0.2, the player will have a 0.8 probability of going into the greedy mode and a 0.2 probability of going into a mode where the action is chosen randomly; however, since there is also a 0.1 probability that the original greedy action will be chosen in random mode, the value of $\\epsilon$ should be taken as 0.2, at which point the greedy action will take the value of 0.1\n\n> Can the proposed method be applied to scenarios with continuous action spaces? \n\nThe original intent of our paper was to address the problem of trembling hands that exists in value-based MARL methods. However, our SPP method contains major components: the UCT planning[1], Q-value assembling and cancelation of action noise are technically applicable to continuous action spaces. We have found that our approach is surprisingly able to improve the performance of Actor-critic based, so we are reasonably confident that our SPP approach is also capable of applying to continuous action spaces. We will actively search for suitable benchmarks for testing and welcome your further suggestions.\n\nIn the remaining time we would like to address the remaining concerns that you have that prevent you from accepting the paper. We would appreciate it if you could engage with us on the remaining concerns, as we want to address them.\n\n\n\n[1] Hubert, Thomas, et al. \"Learning and planning in complex action spaces.\" International Conference on Machine Learning."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700539627256,
                "cdate": 1700539627256,
                "tmdate": 1700539941522,
                "mdate": 1700539941522,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oJAiM8jDQ1",
                "forum": "xibcBSuuq0",
                "replyto": "R6dnqxuXrA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9370/Reviewer_RkkR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9370/Reviewer_RkkR"
                ],
                "content": {
                    "comment": {
                        "value": "Sorry for the late reply. I appreciate authors for the detailed response addressing my concerns. However, there are a few additional queries I have regarding the paper.\n\n- Final Performance Comparison. While the use of SPP appears to accelerate training during the early phase, I am interested in the final performance of MARL algorithms with and without SPP. Can you provide the final performance comparison of QMIX and MAPPO on SMAC? \n- SPP's Performance Beyond SMAC Scenarios. Your experiments suggest that most scenarios in SMAC are not challenging enough and therefore weaken your results. Can you test SPP in other environments? \n- Your response mentioned that \"SPP explores more globally and centrally compared to existing popular MARL methods.\" Could you further explain how it explores globally and centrally at the same time? \n\nOverall, I would like to keep my score as it stands now."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9370/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709326171,
                "cdate": 1700709326171,
                "tmdate": 1700709326171,
                "mdate": 1700709326171,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]