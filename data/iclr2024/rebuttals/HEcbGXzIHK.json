[
    {
        "title": "Episodic Memory Theory for the Mechanistic Interpretation of Recurrent Neural Networks"
    },
    {
        "review": {
            "id": "P0qaIClC5b",
            "forum": "HEcbGXzIHK",
            "replyto": "HEcbGXzIHK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2776/Reviewer_VSBc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2776/Reviewer_VSBc"
            ],
            "content": {
                "summary": {
                    "value": "The authors employed a linear RNN to execute a task called VARIABLE BINDING. They utilized linear algebra to illustrate the process of extracting outputs from the network."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The specific sections of the article that describe what was done are relatively clear."
                },
                "weaknesses": {
                    "value": "There are three main drawbacks:\n\n1. The current analysis is overly limited. It applies solely to a single-layer RNN with linear dynamics. The assertion that any general RNN can be treated as a linear RNN in A.4 is fundamentally incorrect. In reality, a general RNN may not behave near a fixed point, resulting in significant higher-order terms. Furthermore, the analysis pertains to a task, the VARIABLE BINDING TASK, which is notably distinct from a translation task.\n\n2. The current analysis lacks novelty. The article primarily employs linear projection techniques to examine the components of RNN weights, a methodology that has been in use for a considerable period.\n\n3. The presentation lacks clarity. Figures 1 and 2 are difficult to comprehend, and their intended message is unclear. Additionally, the mention of GSEMM seems unnecessary since the current work solely involves a simple linear RNN. There is a lack of a concise summary of the core mechanisms of the RNN."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "none"
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2776/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2776/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2776/Reviewer_VSBc"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2776/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698324386636,
            "cdate": 1698324386636,
            "tmdate": 1699636220617,
            "mdate": 1699636220617,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u77ITUFtGb",
                "forum": "HEcbGXzIHK",
                "replyto": "P0qaIClC5b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2776/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2776/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comments.\n\n\"The current analysis is overly limited. ...\"\n\nOur analysis is restricted to linear RNNs, but linearization approaches allow researchers to study complex non-linear dynamical systems like RNNs. What we described in A.4 is only one approach to linearization typically used in the literature. Alternate approaches, like the Koopman theory, allow a more complicated linearization while maintaining applicability in regions farther from fixed points. Our theory extends to these spaces, although we find in the paper that the variable binding tasks are encoded in a simple inner product (the finite-dimensional real vector inner product) space.\n\n\u201c, the analysis pertains to a task, the VARIABLE BINDING TASK, which is notably distinct from a translation task\u201d \n\nThe class of VARIABLE BINDING TASKS is analogous to a deterministic version of s-gram tasks used in NLP. We only restrict the composition function to be linear. Future work will explore more complex non-linear composition functions.\n\n1.\tThe current analysis lacks novelty. The article primarily employs linear projection techniques to examine the components of RNN weights, a methodology that has been in use for a considerable period.\n\nIn our restriction of the VARIABLE BINDING TASKS, we find that a linear basis transformation reveals the operator. To our knowledge, an explicit operator that enables memory storage mechanisms in RNNs has not yet been proposed and experimentally validated. We further showed that the operator is not just a theoretical construct but emerges naturally after training. We also want to note that Fig 4,5 has not been extracted from prior RNN analyses.\n\nSimple tasks enable researchers to probe the mechanisms of complex dynamical systems like RNNs. For instance, simple tasks like 3-bit flip flop, Frequency cued sine wave, Context-dependent integration are used to probe various dynamical behaviors of RNNs [3]. Our tasks are more complex than these as the dynamical behavior for VARIABLE BINDING is very high dimensional and inscrutable (Appendix Figure 5 shows how high dimensional and diverse the Jacobian spectrum is after linearizing around the origin). \n\nAmong the VARIABLE BINDING tasks, repeat copy ($T_1$) has been used frequently to analyze the memory storage behavior of RNNs [1]. Recently, the operator we proposed for repeat copy was also found in connection to traveling waves in RNNs [2], suggesting the theory's applicability is broader than the VARIABLE BINDING tasks. \n\n\n[1] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. ArXiv, abs/1410.5401, 2014\n[2] Thomas Anderson Keller, Lyle E. Muller, Terrence J. Sejnowski, and Max Welling. Traveling waves\nencode the recent past and enhance sequence learning. ArXiv, abs/2309.08045, 2023. URL https://api.semanticscholar.org/CorpusID:262013982\n[3] Maheswaranathan, Niru et al. \u201cUniversality and individuality in neural dynamics across large populations of recurrent networks.\u201d Advances in neural information processing systems 2019 (2019): 15629-15641 .\n\n\u201cAdditionally, mentioning GSEMM seems unnecessary since the current work solely involves a simple linear RNN. There is a lack of a concise summary of the core mechanisms of the RNN.\u201d\n\nGSEMM is not unnecessary, as this connection to neurocomputational memory models enabled interpreting the weights of the RNN as stored memories and their interactions. To our knowledge, this is the first connection between Hopfield-like memory models studied in theoretical neuroscience and Recurrent Neural Networks. \n\n\"There is a lack of a concise summary of the core mechanisms of the RNN.\"\n\nSection 5 and Figure 2 summarize the main parts of the theory. Please let us know if any particular aspects of the figures and exposition are unclear."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2776/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700531101285,
                "cdate": 1700531101285,
                "tmdate": 1700531101285,
                "mdate": 1700531101285,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tzb0u1LdtX",
                "forum": "HEcbGXzIHK",
                "replyto": "u77ITUFtGb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2776/Reviewer_VSBc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2776/Reviewer_VSBc"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed answer, while I believe there is still much room for improvement."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2776/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644660141,
                "cdate": 1700644660141,
                "tmdate": 1700644660141,
                "mdate": 1700644660141,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Sy7WcexPdp",
            "forum": "HEcbGXzIHK",
            "replyto": "HEcbGXzIHK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2776/Reviewer_PqoU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2776/Reviewer_PqoU"
            ],
            "content": {
                "summary": {
                    "value": "This paper develops the Episodic Memory Theory (EMT), where a circuit mechanism is presented to illustrate how a linear RNN recursively stores and composes hidden variables. The authors show that, under specially designed algorithmic tasks called *variable binding*, the hidden neurons and the learned parameters of a trained linear RNN can be illustrated by *variable memories* $\\Psi$, which are a group of interpretable bases. They also design an operator $\\Phi$ to form a circuit computation of variable memories $\\Psi$. Finally, they propose a power iteration-based algorithm to find the bases $\\Psi$ via the learned RNN parameters. In the experiment, the authors show the variable memories $\\Psi$ could reveal the information stored in the hidden states of a linear RNN. They also provide examples of how such bases $\\Psi$ enable human interpretability of learned RNN parameters."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Proposes a novel basis (variable memory $\\Psi$) that, for the first time, reveals hidden neurons actively involved in information processing in a linear RNN.\n\n* Using the basis to interpret the learned parameters of a linear RNN in a  human-friendly way."
                },
                "weaknesses": {
                    "value": "* Only a Repeat Copy task is shown to reveal the stored information in hidden neurons. As the authors mentioned in section 7.3, there are some cases in which the computed basis is converged, but it cannot give interpretable representations. In other words, under which tasks do we expect this framework to fail? \n\n* Typos: The first sentence below Equation 6 should be \"Figure 2A\"."
                },
                "questions": {
                    "value": "* > This deviation from the theory is a result of the sensitivity of the basis definition to minor errors in the pseudo-inverse required to compute the dual.\n\n* What is the meaning of \"dual\"? is this related to the conjugate transpose computation $EE^{*}$ in Algorithm 1?\n\n* Could you please discuss in detail the difficulties of applying the proposed theory to nonlinear RNNs? In Appendix A.4, you have shown that a nonlinear RNN has a similar form of the linear system as linear RNNs instead of a different $W_{hh}$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2776/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2776/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2776/Reviewer_PqoU"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2776/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698642177004,
            "cdate": 1698642177004,
            "tmdate": 1699636220534,
            "mdate": 1699636220534,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UMrgLKKBjx",
                "forum": "HEcbGXzIHK",
                "replyto": "Sy7WcexPdp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2776/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2776/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comments and for pointing out the typo.\n\n\u2022\tOnly a Repeat Copy task is shown to reveal the stored information in hidden neurons. As the authors mentioned in section 7.3, there are some cases in which the computed basis is converged, but it cannot give interpretable representations. In other words, under which tasks do we expect this framework to fail?\n\nWe have added a section in the Appendix A.6 for a more formal analysis of the error in approximating variable memories. In short, the algorithm assumes specific interactions of the variable memory are negligible. This assumption is valid for the cases of repeat copy and compose copy we showed in the paper, but for some $f$ where this assumption is broken, the algorithm fails. We want to note that the failure of the algorithm does not mean that the theoretical mechanisms are not present (the convergence of the theoretical $\\Phi$ in Table 1 still stands). This, however, means that it is not trivial to formulate an algorithm to find such a human interpretable basis for a general class of tasks.\n\n\u2022\tThis deviation from the theory is a result of the sensitivity of the basis definition to minor errors in the pseudo-inverse required to compute the dual.\n\nWe were incorrect on this point in the paper. More recent analysis has revealed that the deviation results from the errors accumulated when approximating the variable memories by power iteration. A section has been added to Appendix A.6 that formally explains the approximation error of the variable memory algorithm.\n\n\u2022\tWhat is the meaning of \"dual\"? is this related to the conjugate transpose computation \ufffd\ufffd\u2217 in Algorithm 1?\n\n\"dual\" is a misnomer. We meant the projection operator that extracts the components in the variable memory space into the standard basis. This is updated in the manuscript.\n\n\u2022\tCould you please discuss in detail the difficulties of applying the proposed theory to nonlinear RNNs? In Appendix A.4, you have shown that a nonlinear RNN has a similar form of the linear system as linear RNNs instead of a different \ufffd\u210e\u210e.\n\nThe proposed algorithm (not the theory) does not work for non-linear RNNs when any of the two assumptions fail \u2013 (1) The geometry of the representation is not easily captured by a linear basis, in which case a more complex set of basis vectors needs to be taken, (2) If the RNN dynamics are far from the fixed point, which means that the Taylor series expansion in Appendix A.4 needs to account for the higher order terms, even if the correction from the Jacobian is taken into account. (1) and (2) are not mutually exclusive; sometimes failure of (2) implies the failure of (1)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2776/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530799349,
                "cdate": 1700530799349,
                "tmdate": 1700530799349,
                "mdate": 1700530799349,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UIYSodbaX3",
            "forum": "HEcbGXzIHK",
            "replyto": "HEcbGXzIHK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2776/Reviewer_xCsT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2776/Reviewer_xCsT"
            ],
            "content": {
                "summary": {
                    "value": "The authors frame RNNs as episodic memory retrievers and use this to devise a circuit mechanism that could carry out sequential memory tasks. They show that this circuit mechanism seems to appear in trained networks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper attempts to bring several different topic areas together, which is admirable.\n\nThe potential applications listed for this work would be useful if achievable. \n\n The work is thorough."
                },
                "weaknesses": {
                    "value": "I struggled to read this paper at several points. It is pulling concepts from many different fields and also I believe trying to introduce new ones. I'm also not well versed in the specific notation used. I don't want to down-score work for being too interdisciplinary, but as the paper stands now I don't know if there is a large community who would be able to understand and benefit from it as a whole. \n\nThe tasks (insofar as they are described) seem like weak, or at least very specific, tests of variable binding. For the authors to make claims about variable binding in general, they would need to show tasks that do more than just require sequential repeats of the input. \n\n\n\nA substantial issue for me is that I am confused about the elements that the authors label as being novel here. Most of them are, at least at a broad level, well-represented in the neuroscience-inspired RNN literature. For example, the authors say in the discussion:\n\nwe provide \"a novel perspective on Recurrent Neural Networks (RNNs), framing them as dy-\nnamical systems performing sequence memory retrieval\". The original Ellman model itself uses RNNs for a form of sequence memory retrieval, but also several more recent works study serial working memory with RNNs such as: https://direct.mit.edu/neco/article/30/6/1449/8400/A-Theory-of-Sequence-Indexing-and-Working-Memory and https://psycnet.apa.org/record/2006-04733-001\n\n\"We introduced the concept of \u201cvariable\nmemories,\u201d linear subspaces capable of symbolically binding and recursively composing informa-\ntion.\"  The notion of storing different items in different linear subspaces has also been explored: https://www.science.org/doi/10.1126/science.abm0204\n\n\"We presented a new class of algorithmic tasks that are designed to probe the variable binding\nbehavior of RNNs. \" As represented by the above studies on serial working memory, this class of tasks is not new. \n\n\"for the first time, revealed hidden neurons actively involved in in-\nformation processing in RNNs. \" This is obviously not the first time people have studied how neurons process information in RNNs (see the work of Omri Barak and David Sussillo, e.g.). \n\nOn the whole I also don't see the specific value in claiming that this analysis is related to episodic memory. Sequential memory, yes. But there is nothing specifically episodic about the motivation for the analyses and the tasks represent serial working memory."
                },
                "questions": {
                    "value": "What are the tasks? One is described in the main text and another mentioned in the appendix. All 4 should be described in the main text. \n\nI thought u(t) was the input vector, which is 0 for t>s, yet Eqn 2 shows the evolution of u(t) for t>s. \n\nThe authors say:\n\n\"The mechanistic interpretability seeks to reverse-engineer neural net-\nworks to expose the underlying mechanisms enabling them to learn and adapt to previously unen-\ncountered conditions\"\n\nand\n\n\" This assumption limits the mod-\nels\u2019 applicability to mechanistic interpretability, which requires the symbolic binding of memories\ntypically available only during inference.\"\n\nWhy are they focusing on mechanistic interpretability for such a limited behavior? As I understand it MI can be used to explain any behavior of a neural network."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2776/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698677073805,
            "cdate": 1698677073805,
            "tmdate": 1699636220465,
            "mdate": 1699636220465,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OZGIaWtqKw",
                "forum": "HEcbGXzIHK",
                "replyto": "UIYSodbaX3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2776/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2776/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments and for pointing us to the relevant literature. \n\n\"The tasks (insofar as they are described) seem like weak, or at least very specific, tests of variable binding. ...\"\n\nWithout the linear assumption, the variable binding tasks can represent any dynamic system with a historical context of s states or, in NLP terms, an s-gram model. Probablistic versions of s-gram models have been used in the literature before; we take a restricted $f$ for better analytic tractability.\n\nVariable binding is a very diverse phenomenon, and tackling all of its aspects in a single paper is intractable. Our class of tasks is a simple restriction that can be used to probe some mechanisms of variable binding. Although simple, these tasks reveal the mechanisms of RNNs and contribute to developing a better, more complete theory of variable binding. \n\nWe also note that simple tasks are used liberally in the literature to improve our understanding of RNN computations. Starting with small-scale tasks on standard architectures yields alternative benefits, including interpretable results and potential generality of conclusions.\n\nRegarding the impact of the theory beyond what we showed, we want to bring to attention a recent work studying traveling waves in RNNs that found the operator for $T_1$ independently (https://arxiv.org/pdf/2309.08045.pdf). Our work accommodates more general cases that $T_1$. \n\n\u201ca novel perspective on Recurrent Neural Networks (RNNs), framing them as dy- namical systems performing sequence memory retrieval\u201d\n\nPreviously, RNNs were used to study memory, but to our knowledge, memory models were not used to explore the behavior of RNNs, and we showed in the paper that this view has benefits.\n\nWe showed that RNNs are discrete-time analogs of a sequence memory model GSEMM (a newer class of Hopfield Networks with precisely defined memories and inter-memory interactions). To our knowledge, this is the first mathematically rigorous connection between RNNs and neurocomputational memory models. In this paper, this connection enabled reinterpreting the learned weight matrix of RNNs as stored memories and their interactions. Future work can flesh out the relationship between RNNs and the energy function in memory models."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2776/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530494168,
                "cdate": 1700530494168,
                "tmdate": 1700530494168,
                "mdate": 1700530494168,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MhIPMOfZhk",
                "forum": "HEcbGXzIHK",
                "replyto": "UIYSodbaX3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2776/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2776/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "\u201cThe notion of storing different items in different linear subspaces has also been explored\u201d\n\nWe thank the reviewer for the relevant literature. Empirical works have investigated the linear subspaces we presented, mainly from the neuroscience community. To our knowledge, there is no work formalizing these linear spaces, how they interact, and how the two (space and interactions) solve the task at hand. Using the new formalism, we made theoretical predictions on what class of operators could solve the tasks and validated them with experimental results. Note that our theory goes beyond the task T_1 that is analogous to the delayed sequence reproduction task considered in the linked paper on sequence working memory.\n\n\u201cThis is obviously not the first time people have studied how neurons process information in RNNs (see the work of Omri Barak and David Sussillo, e.g.).\u201d\n\nOur work builds on the Jacobian spectrum analysis approach of Omri Barak and David Sussilo. This connection is discussed in the Related Works section.  To our knowledge, Jacobian spectrum research has not explicitly delved into the phenomenon of variable binding and defined a notion of storing variables in subspaces of the hidden state. \n\n Further, the tasks used in the previous studies using spectral interpretations (3-bit flip flop, Frequency-cued sine wave, Context-dependent integration) are intentionally low-dimensional, making it easy to plot and draw conclusions. The variable binding tasks in our paper exhibit very high-dimensional behaviors with complicated spectrum distribution (Figure 5 in the appendix), which means that spectral analysis will not yield easily understandable results. We showed in the paper that all these complicated spectral distributions have a straightforward interpretation with variable memories and their interactions albeit restricted to the variable binding tasks in Section 4. We have made changes to clarify these points in the manuscript.\n\n\"What are the tasks? \"\n\nWe have amended the document with the matrix representation of the composition function for these tasks with a figure above Table 1. They all differ only by the composition function $f$ acting on all the variables.\n\n\"I thought u(t) was the input vector, which is 0 for t>s, yet Eqn 2 shows the evolution of u(t) for t>s.\"\n\nThank you for pointing this out. Corrected: Eqn 2 is supposed to be y(t) \u2013 the network output rather than u(t).\n\n\"Why are they focusing on mechanistic interpretability for such a limited behavior? As I understand it, MI can be used to explain any behavior of a neural network.\"\n\nThere are different levels of explainability that MI is used for. Sussilo proposed a very general method, which can be applied to any behavior of neural networks. However, this approach makes interpreting the variable binding tasks in our work challenging. Specifically, the high dimensional nature of the variable binding tasks meant no straightforward interpretation of the Jacobian spectrum. \n\nOur method is specific to the class of variable problems described in Section 4. It provides a better understanding of the RNN behavior beyond what the Jacobian spectrum allows in these tasks. With this specificity, we better understand RNN behavior on these tasks but lose some generality to other tasks. There is a clear gap between what the general method enables in these tasks that the current approach fills. Although the tasks we consider are still simple, repeat copy has been used to test the memory capabilities of recurrent neural networks [1] and more recently found in connection with traveling waves in RNNs [2].\n\nWe have clarified these points in the updated paper.\n\n\u201cOn the whole I also don't see the specific value in claiming that this analysis is related to episodic memory. Sequential memory, yes. But there is nothing specifically episodic about the motivation for the analyses and the tasks represent serial working memory.\u201d\n\nThis analysis is episodic in the sense that it uses an episodic memory model from computational neuroscience and the formalisms (stored memories, inter-memory interactions) it provides to perform the analysis. The definition is in line with GSEMM, and episodic memory as described in neuroscience [3] - which describes the ability of neural networks to store and process temporal and contiguous memory sequences.\n\n[1] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. ArXiv, abs/1410.5401, 2014\n\n[2] Thomas Anderson Keller, Lyle E. Muller, Terrence J. Sejnowski, and Max Welling. Traveling waves\nencode the recent past and enhance sequence learning. ArXiv, abs/2309.08045, 2023. URL https://api.semanticscholar.org/CorpusID:262013982\n\n[3] Umbach, Gray et al. \u201cTime cells in the human hippocampus and entorhinal cortex support episodic memory.\u201d Proceedings of the National Academy of Sciences of the United States of America 117 (2020): 28463 - 28474."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2776/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530563099,
                "cdate": 1700530563099,
                "tmdate": 1700530673725,
                "mdate": 1700530673725,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iqebsAwgFK",
                "forum": "HEcbGXzIHK",
                "replyto": "MhIPMOfZhk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2776/Reviewer_xCsT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2776/Reviewer_xCsT"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their additional clarifications. Unfortunately I still think the paper needs work before it can be of use to an interdisciplinary audience. \n\nSome specific responses:\n\"Variable binding is a very diverse phenomenon, and tackling all of its aspects in a single paper is intractable. Our class of tasks is a simple restriction that can be used to probe some mechanisms of variable binding.\"\nIf this is the case then the paper should be much clearer about it and mention the specific features it is trying to probe as the main contribution.\n\n\"Previously, RNNs were used to study memory, but to our knowledge, memory models were not used to explore the behavior of RNNs\"\nI don't understand this distinction. RNNs are used as memory models. So interpreting them as memory models is core to many previous studies. \n\n \"RNNs are discrete-time analogs of a sequence memory model GSEMM (a newer class of Hopfield Networks with precisely defined memories and inter-memory interactions). To our knowledge, this is the first mathematically rigorous connection between RNNs and neurocomputational memory models. \"\n The hopfield network simply is a (binary) RNN. So there has always been a connection between RNNs and neurocomputational memory models. Therefore this framing of the contribution makes it hard to see what this specific work is achieving.\n \nRegarding mechanistic interpretability, I understand that the authors are doing something specific in this paper, but the paper makes more general claims about MI (listed above) that aren't consistent with how the term is used in the literature. \n\nEpisodic memory has a specific meaning in neuroscience/psychology in that it is autobiographical. In these simple models there is no ability to model specifically autobiographical memory. I understand that the previous literature used this term for sequence memory, but it is best to not propagate that error."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2776/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665289026,
                "cdate": 1700665289026,
                "tmdate": 1700665289026,
                "mdate": 1700665289026,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s15zVHY7pK",
            "forum": "HEcbGXzIHK",
            "replyto": "HEcbGXzIHK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2776/Reviewer_Ub1M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2776/Reviewer_Ub1M"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a mathematical formulation on linear (and potentially non-linear) RNNs to analytically track the storage of all relevant memories in a human interpretable manner. The authors frame this as \"Episodic Memory Theory\" or EMT. The authors propose a variable binding mechanism, and found that when training RNNs on repeat-copy and compose-copy tasks, the solutions exhibit the theoretically predicted behavior."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The mathematical formulation on interpreting internally-stored variables in RNNs is novel and insightful. This work provides an important new tool for the mechanistic deconstruction of RNNs. The most impressive result of the paper is that trained RNNs converge to some intended mechanism, suggesting that the mechanism the authors have found is very likely the most optimal solution for the cost function."
                },
                "weaknesses": {
                    "value": "I have several major issues with this work, as detailed below.\n\n(1) While the mathematical formulation is meaningful, I believe this is not episodic memory (and this proposed method is not a theory of episodic memory). The model and mathematical framework may be inspired by episodic memory models, but the task, objective and entire narrative is largely unrelated to episodic memory. RNNs receiving variable inputs, performing computations based on those variable inputs, and subsequently producing an output is straightforward decision-making or information-processing in many cases, including this work. This is further supported by Figure 1, where performing an addition operation is merely a simple computation that does not even require any memory storage or retrieval. Knowing how to add is not episodic memory. I am open to discussion if the authors still feel it is correctly defined.\n\n(2) The way the entire paper is structured is unnecessarily confusing. I can summarize the work in the following manner:\n- Consider an input vector with $d$ dimensions that spans $s$ timesteps, with a total of $sd$ input elements\n- The number of neurons in the RNN needs to be greater than $sd$ otherwise superposition effects will occur\n- The authors perform a change of basis such that the first $sd$ elements of the latent activity within the RNN now represent the (human interpretable, one-to-one mapped) input sequence with some shifting. This is referred to as \"variable memory\" by the authors (which is well-named and easily understood if not convoluted by the hard-to-parse narrative leading to its definition).\n\nRight now a reader requires knowledge in RNNs and their applications in neuroscience, as well as some experience in mathematical tools commonly found in modern physics to fully understand the work, when in reality this paper could be written to a pure RNN audience without the Dirac and Einstein notations (at least in the main text), by simply stating that information about the latent is being carefully tracked by a basis transformation and formulating the equations in that context.\n\n(3) The effects described in Figure 3 and 4A are specific to the repeat-copy task. More complex tasks (especially those with non-periodic solutions) may result in weight matrices that are not interpretable or offer any additional insight. Similarly, the compose-copy task, which is surprisingly never defined in this paper (but can be inferred from the Figure 4B to be generating the sequence one unit at a time), is the only task that will give rise to such interpretable values. In general, the authors summarize this class of tasks as $f$ in equation (2). The intended narrative is that the authors are using a class of tasks to elegantly highlight the feature of the proposed method, but my impression is that the method will not be producing anything meaningful beyond this class of tasks."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2776/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2776/Reviewer_Ub1M",
                        "ICLR.cc/2024/Conference/Submission2776/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2776/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698698827200,
            "cdate": 1698698827200,
            "tmdate": 1700807190800,
            "mdate": 1700807190800,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8pYFvKT8Dp",
                "forum": "HEcbGXzIHK",
                "replyto": "s15zVHY7pK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2776/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2776/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for deeper insights and for constructive feedback to improve the clarity of the work.\n\n(1)\tThe definition of Episodic Memory is stipulative: that is, we took an episodic memory model (GSEMM) and showed that RNNs are equivalent to its discretization. Here, episodic memory is defined as the ability of a neural system to store episodes (or distinct sequences of memories). In the context of the paper, we showed it was useful to interpret RNNs using this notion of episodic memory from the memory modeling literature. There are, of course, other definitions of episodic memory. For instance, episodic memory is defined originally as the subjective recall of personal experiences, focusing on the human experience rather than the underlying neural network.  Given the variety of definitions for episodic memory, we don\u2019t think it is fruitful to formalize a single definition encapsulating all aspects of episodic memory. Further, such a precise definition is not necessary for the technical contributions of the paper. We take the point that the addition example in Fig 1 is misleading. We have replaced that example with the generation of the Fibonacci series, which necessitates an explicit variable memory store.\n\n(2)\tThank you for the comments to improve the clarity, we have modified our document clarifying the points of confusion and removing the notation. The intention was that the model of variable binding is defined in the more general vector space \u2013 the Hilbert space. We don\u2019t assume any geometry (the definition of the inner product) on the neural representations to formulate the theory. The theory will work irrespective of the geometry of the neural networks representing the solution (Appendix A.3). It so happens that the tasks and RNNs we considered in the paper all represent the solution in a space where the inner product is the vector inner product, and hence, linear algebra can be used. When formulating a theory of neural computation, the rich representational diversity of neural networks needs to be considered, and this notation enables that. We accept that the notation is not standard in the general literature. We take the point that the new notation needs to be better motivated. Given that we found only a simple geometry in these tasks, we have deferred introducing it to future work (and removed the notation from the main document in the updated version). Still, given the utility of the notation to form principled theories for how RNNs work, we don\u2019t see why it cannot be used to inform future developments.\n\n(3)\tFigure 3, 4A are results of running the approximation algorithm which can have errors (added a section in the Appendix with formal error analysis). The effect described in the paper is present across all the 4 tasks we considered in the paper (Table 1). Since the variable memory algorithm is approximate, sometimes correct bases cannot be obtained by power iteration, which is why only some tasks can be supported by the empirical algorithm. Figures 3, 4 show the result of running the algorithm to approximate the variable memories, which may have errors when applied in specific tasks. We only show 3 for repeatCopy as the variables there are easier to comprehend than the other composition functions.  \n\n\"The intended narrative is that the authors are using a class of tasks to elegantly highlight the feature of the proposed method, but my impression is that the method will not be producing anything meaningful beyond this class of tasks.\"\n\nWe want to bring into attention a recent work, that found the operator to solve repeat copy (Fig 4A) in connection to the presence of traveling waves in RNNs (https://arxiv.org/pdf/2309.08045.pdf), suggesting a broader applicability of the theory. Starting with small-scale experiments on standard architectures yields alternative benefits, including interpretable results and potential generality of conclusions. Future work will extend it to more general tasks of interest to the machine learning community. \n\nThere is always the added caveat in analyzing RNNs that behaviors like chaos and quasi-periodicity cannot be captured analytically. This is a disadvantage for any analysis of RNNs and not unique to our approach. That said, it should still be interesting that RNNs capture the inherent periodicity of the problems they solve in their dynamical behavior without introducing any additional implicit biases.\n\nWe added the 4 task functions as a figure above the Table 1. All these tasks differ only in the definition of $f$, the function computing the final variable memory."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2776/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530325169,
                "cdate": 1700530325169,
                "tmdate": 1700530325169,
                "mdate": 1700530325169,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]