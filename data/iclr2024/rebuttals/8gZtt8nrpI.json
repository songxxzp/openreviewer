[
    {
        "title": "Diffusion Models With Learned Adaptive Noise Processes"
    },
    {
        "review": {
            "id": "gW7nd6LEo7",
            "forum": "8gZtt8nrpI",
            "replyto": "8gZtt8nrpI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6378/Reviewer_aHz6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6378/Reviewer_aHz6"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a method to learn the parametric diffusion noise schedule by jointly optimizing model parameters and diffusion parameters. In addition, the authors propose a learning method for conditional diffusion via a latent distribution."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed approch that learns an adaptive diffusion noise schedule is somewhat novel.   \n\n2.  The paper is well-written and well-organized."
                },
                "weaknesses": {
                    "value": "1.  The authors argue a novel approch that learns conditional diffusion via auxiliary latent variables.  \n However,  the relationship and difference compared with (Wang et al. 2023) is not clearly discussed. \n\n\n2. The advantage of the proposed approch via auxiliary latent variables is not well supported.   In Figure 1 (a), it seems that MuLAN w/o auxiliary latent variable performs worse than the standard VDM.   \n\n\n3. The empirical results can not support the claimed advantage of the proposed method MULAN .  In Table 1, it seems that the proposed method MULAN   performs worse than i-DODE\u2217 (Zheng et al., 2023)."
                },
                "questions": {
                    "value": "Q1.  The empirical results are not convincing enough to demonstrate the advantage of the proposed method.  Could the authors provide additional empirical evidence to support the claim?\n\nQ2.  It seems that the proposed method incurs additional time complexity. Could the authors provide additional running time comparison with baselines?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6378/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698733338290,
            "cdate": 1698733338290,
            "tmdate": 1699636705605,
            "mdate": 1699636705605,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "192pO9cQTc",
                "forum": "8gZtt8nrpI",
                "replyto": "gW7nd6LEo7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6378/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6378/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer aHz6"
                    },
                    "comment": {
                        "value": "We want to thank the reviewer for their constructive feedback. We address each concern below.\n\n**Concern 1**: The strength of the empirical results. Can the authors provide additional empirical evidence?\n\nWe have improved our experimental results, and **we now achieve a new state-of-the-art** in density estimation on CIFAR10 and ImageNet. The table below summarizes our results.\n\n| Method | CIFAR10 bpd | ImageNet bpd |\n|-------------|-------------|-------------|\n| PixelCNN (Van den Oord et al., 2016) | 3.03 | 3.83 \nImage Transformer (Parmar et al., 2018) | 2.90 | 3.77\nScore SDE (Song et al., 2020) | 2.99 | -\nImproved DDPM (Nichol & Dhariwal, 2021) | 2.94 | -\nVDM (Kingma et al., 2021) | 2.65 | 3.72 \niDODE (Zheng et al., 2023) | 2.56  | 3.69\nMuLAN (ours; initial version) | 2.60 \u00b1 1e-3 | 3.71 \u00b1 1e-3\nMuLAN (**ours; rebuttal version; state-of-the-art**) | **2.55** \u00b1 1e-3 | **3.67\\*** \u00b1 1e-3\n\n*\\* Partial result obtained on 70% of the data due to time constraints.*\n\nOur updated method adds two architectural improvements: velocity reparameterization (as in Zheng et al., 2023) and truncated normal dequantization (Dinh et al., 2017; Salimans et al., 2017; Ho et al., 2019; Zheng et al., 2023). The recent iDODE paper contains additional innovations (e.g., importance sampled training) that could further improve the performance of MuLAN. Lastly, for both iDODE and MuLAN with velocity parameterization, we report an importance-sampled estimate of test NLL with K=20 samples, as in the i-DODE paper (sec A.1., eqn. 32).\n\n**Concern 2**: Explaining the relationship between Infodiffusion (Wang et al., 2023) and MuLAN.\n\nIn brief, auxiliary-variable diffusion is a technique introduced by Wang et al., 2023. MuLAN makes use of this technique in conjunction with several other components. The resulting method is substantially different from InfoDiffusion and solves a different problem: density estimation versus representation learning.\n\nSpecifically, the key novel component of MuLAN is a learned adaptive noise process. A widely held assumption is that the ELBO objective of a diffusion model is invariant to the noise process (Kingma et al., 2021). We **dispel this assumption**: we show that when input-conditioned noise is combined with (a) multivariate noise, (b) a novel polynomial parameterization, and (c) auxiliary variables, a learned noise process yields an improved variational posterior and a tighter ELBO. This approach sets a new state-of-the-art in density estimation.\n\nThe table below summarizes the relationship between MuLAN and other methods.\n\n\n\n| | Learned noise | Multivariate noise | Input Conditioned noise | Auxiliary latents | Noise parameterization |\n|---|---|---|---|--- |---|\nVDM (Kingma et al., 2021) | **Yes** | No |No | No | Monotonic neural network |\nBlurring Diffusion Model (Hoogeboom et al., 2022) | No | **Yes** | No | No | Frequency scaling |\nInfoDiffusion (Wang et al., 2023) | No | No | No | In denoising process | Cosine schedule |\nMuLAN (ours) | **Yes** | **Yes** | **Yes** | In nosing & denoising processes | Polynomial, sigmoid\n\n**Concern 3**: MuLAN without auxiliary variables performs worse than VDM.\n\nThis may be a misunderstanding. MuLAN without auxiliary variables __performs the same__ as VDM: in Fig 1a, the blue and gray lines overlap almost perfectly. This is expected: when the noise is not adaptive (on the data via auxiliary variables), there is no improvement.\n\nPerhaps the reviewer meant to ask why MuLAN without multivariate noise is worse than VDM? First, recall that when noise is univariate, the ELBO is invariant to the choice of noise process (Kingma et al., 2021). However, auxiliary variables require applying the ELBO twice: this yields a training objective that bounds the marginal log-likelihood less tightly than in VDM. By itself, this lowers performance; however this performance drop is more than compensated by learning the noise process.\n\n**Concern 4**: Understanding the time complexity of the method.\n\nWhen trained on 8 A100 GPUs, VDM achieves a training rate of 11.6 steps/second, while Mulan trains slightly slower at 10.1 steps/second due to the inclusion of an additional encoder network. However, despite this slower training pace, VDM requires 30 days to reach a BPD of 2.65, whereas Mulan achieves the same BPD within a significantly shorter timeframe of 10 days."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6378/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687203154,
                "cdate": 1700687203154,
                "tmdate": 1700713269192,
                "mdate": 1700713269192,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GzsxQuHtuG",
            "forum": "8gZtt8nrpI",
            "replyto": "8gZtt8nrpI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6378/Reviewer_U1Ra"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6378/Reviewer_U1Ra"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors propose using an instance-dependent multivariate Gaussian noise scheduling and auxiliary latent variables to improve the likelihood estimation. Their method demonstrates strong performance in terms of negative log-likelihood (**NLL**) and convergence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Overall, this paper is clearly written and easy to follow. The idea is simple, narrowing the gap between marginal log-likelihood and its evidence lower bound (**ELBO**) by specifying a more flexible family of approximate variational posteriors, which is a standard approach in variational inference. The relevant derivations in the paper are also straightforward.\n2. The paper studies the effect of an adaptive multivariate Gaussian noise schedule on the likelihood estimation performance, which can be potentially combined with other techniques to improve the metric. The proposed method, MULAN, is also found advantageous to previous SOTA generative modeling methods in likelihood estimation and convergence. The ablation study underscores the indispensable synergy between the method's core components: the auxiliary variable and the multivariate Gaussian noise scheduling."
                },
                "weaknesses": {
                    "value": "1. The intuition behind using a non-identical pixel-wise Gaussian noise schedule from a frequency perspective (e.g., texture and shape, which are mainly perceptual) is not convincing. It is known that likelihood generally does not correlate with sample quality and visual appearance [1].\n2. As mentioned in the paper, the proposed method itself does not contain much novelty. The use of multivariate non-isotropic Gaussian noise scheduling and the introduction of an auxiliary variable in diffusion models are not new [2][3]. The mathematical derivations presented in this paper largely mirror previous work.\n3. The introduction of an auxiliary variable does not necessarily agree with the objective of narrowing the posterior gap. In Section 3.3.2, the right-hand side of the first inequality, i.e., Equation (7), to my understanding, is the same as the ELBO of a variational autoencoder. If the actual objective of MULAN is based on the second inequality, then the ELBO w.r.t. $(x, z)$ would act as a bottleneck of the ELBO w.r.t. $(x_0, x_{1..T}, z)$.\n4. The experiment results are rather not impressive. As mentioned in the paper, the authors implement their method based on the VDM codebase and adopt the same settings for the most part. VDM is almost the strongest model excluding i-DODE and MULAN (this work) in the main table (Table 1). Although the improvement of the proposed method seems significant compared with other methods, it is far less impressive relative to the result by VDM (the method it is built upon), considering the extra degrees of freedom.\n5. Typos:\n\tAppendix C.1 prexisting -> pre-existing\n\n[1] Theis, Lucas, A\u00e4ron van den Oord, and Matthias Bethge. \"A note on the evaluation of generative models.\" arXiv preprint arXiv:1511.01844 (2015).\n\n[2] Hoogeboom, Emiel, and Tim Salimans. \"Blurring diffusion models.\" arXiv preprint arXiv:2209.05557 (2022).\n\n[3] Wang, Yingheng, et al. \"InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models.\" arXiv preprint arXiv:2306.08757 (2023)."
                },
                "questions": {
                    "value": "1. How is BPD calculated? Is it calculated by the stochastic VLB or ODE-based exact likelihood computation methods? If the reported metric of MULAN is obtained by the former one, what is the variance of it? And what effect does the choice of log-SNR parameterization have on the variance? I think the variance of stochastic VLB matters in this case when it is used to compare the proposed method with others including VDM. VDM explicitly minimizes VLB variance with a learned noise schedule whereas MULAN does not. \n2. Does the authors try to analyze the auxiliary context variables? Do they have interpretable meaning? If so, it might also be a way to do representation learning and controllable generation. In the paper, the auxiliary variables are also referred to as the context and are said to \"encapsulate high-level information\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6378/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6378/Reviewer_U1Ra",
                        "ICLR.cc/2024/Conference/Submission6378/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6378/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783976311,
            "cdate": 1698783976311,
            "tmdate": 1700448664672,
            "mdate": 1700448664672,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dkzDGvJlsG",
                "forum": "8gZtt8nrpI",
                "replyto": "GzsxQuHtuG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6378/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6378/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to weaknesses for reviewer U1Ra"
                    },
                    "comment": {
                        "value": "We want to thank the reviewer for their constructive feedback. We address each concern below.\n\n**Concern 1**: The experimental results are not impressive\n\nWe report new experimental results, and **we now achieve a new state-of-the-art in density estimation on CIFAR10 and ImageNet**. The table below summarizes our results.\n\n| Method | CIFAR10 bpd | ImageNet bpd |\n| --- | --- | --- |\n|PixelCNN (Van den Oord et al., 2016) | 3.03 | 3.83|\n|Image Transformer (Parmar et al., 2018) | 2.90 | 3.77|\n| Score SDE (Song et al., 2020) | 2.99 | - |\nImproved DDPM (Nichol & Dhariwal, 2021) | 2.94 | -\n| VDM (Kingma et al., 2021) | 2.65 | 3.72\n|i-DODE (Zheng et al., 2023) | 2.56 | 3.69\nMuLAN (ours; initial version) | 2.60 \u00b1 1e-3 | 3.71 \u00b1 1e-3\nMuLAN (**ours; rebuttal version; state-of-the-art**) | **2.55** \u00b1 1e-3 | **3.67\\*** \u00b1 1e-3\n\n\\* *Partial result obtained on 70% of the data due to time constraints*\n\nOur updated method adds two architectural improvements: velocity reparameterization (as in Zheng et al., 2023) and truncated normal dequantization (Dinh et al., 2017; Salimans et al., 2017; Ho et al., 2019; Zheng et al., 2023). The recent iDODE paper contains additional innovations (e.g., importance sampled training) that could further improve the performance of MuLAN. Lastly, for both iDODE and MuLAN with velocity parameterization, we report an importance-sampled estimate of test NLL with K=20 samples, as in the i-DODE paper (sec A.1., eqn. 32).\n\nNote also that **the magnitude of our improvement over VDM or i-DODE is significant**: it is comparable to the progress made by most papers on these benchmarks from 2018-2021.\n\n**Concern 2**: Understanding the novelty of the paper relative to recent work\n\nMuLAN is the **first** method to introduce a learned adaptive noise process. A widely held assumption is that the ELBO objective is invariant to the noise process (Kingma et al., 2021). We __dispel this assumption__: when input-conditioned noise is combined with (a) multivariate noise, (b) a novel polynomial parameterization, and (c) auxiliary variables, a learned noise process yields a tighter ELBO and a new state-of-the-art in density estimation. While (a), (c) were proposed in other contexts, we leverage them as subcomponents of a novel algorithm.\n\nThe table below summarizes the relationship between MuLAN and other methods.\n\n| | Learned noise | Multivariate noise | Input Conditioned noise | Auxiliary latents | Noise parameterization |\n| --- | --- | --- | --- | ---  | --- |\nVDM (Kingma et al., 2021) | **Yes** | No |No | No | Monotonic neural network |\nBlurring Diffusion Model (Hoogeboom et al., 2022) | No | **Yes** | No | No | Frequency scaling |\nInfoDiffusion (Wang et al., 2023) | No | No | No | In denoising process | Cosine schedule |\nMuLAN (ours) | **Yes** | **Yes** | **Yes** | In noising & denoising processes | Polynomial, sigmoid\n\n\n\n**Concern 3**: The motivation of our work, specifically the intuition behind using adaptive noise from a frequency perspective, is not convincing.\n\nThe main motivation of our work comes from __variational inference__: we view the noising process as a variational posterior, and we learn it to obtain a tighter ELBO (see Section 3.1). This in turn yields state-of-the-art density estimation.\n\nNote that the intuition to which the reviewer refers is not the motivation for our work. We also do not claim that likelihood correlates with image quality. The paragraph in question only provides an example of different datasets that might benefit from different noise characteristics.\n\n**Concern 4**: The introduction of auxiliary variables does not lower the posterior gap.\n\nPlease note that our results demonstrate empirically that introducing both auxiliary variables and learned noise most likely __improves the posterior gap__: these methods yield significantly better ELBO values on CIFAR10 and ImageNet.\n\nHowever, if we were to only introduce auxiliary variables without adaptive noise, we would indeed get a worse posterior gap. This is confirmed by our ablation study (orange line in Figure 1a). However, this performance drop is more than compensated by learning the noise process (red line in Figure 1)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6378/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709095036,
                "cdate": 1700709095036,
                "tmdate": 1700713398661,
                "mdate": 1700713398661,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6EqpAsa8bx",
                "forum": "8gZtt8nrpI",
                "replyto": "GzsxQuHtuG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6378/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6378/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer U1Ra's questions"
                    },
                    "comment": {
                        "value": "**Question 1**: How was the BPD calculated? Is it VLB or ODE-based? What is the variance of the BPD?\n\nWe used both VLB and ODE-based methods to compute BPD. \n\nIn the VLB-based approach, we employ Eqn. 9. We use T = 128 in Eqn. 10, discretizing the timesteps [0, 1] into 128 bins. For the ODE-based approach, we follow the exact evaluation procedure in i-DODE: we extract the underlying ODE for the diffusion process and calculate the likelihood using ODE-based exact methods. We also use their importance-weighted bound with K importance samples (note that K=1 corresponds to the ELBO, and K>1 provides a tighter bound on the likelihood).\n\nBelow, we report BPD values (mean and 95% Confidence Interval) for MuLAN on CIFAR10 (8M training steps) and ImageNet (2M training steps) using both the VLB-based approach, and the ODE-based approach with K=1 and K-20 importance samples.\n\n| Approach | CIFAR10 BPD | ImageNet BPD |\n| --- | --- | --- |\nVLB-based  | 2.59 \u00b1 1e-3 | 3.71 \u00b1 1e-3\nODE-based (K=1) | 2.59 \u00b1 3e-4 | 3.71 \u00b1 1e-3\nODE-based (K=20) | 2.55 \u00b1 3e-4 | 3.67 \u00b1 1e-3 *\n\n\\* *Partial result obtained on 70% of the test data due to time constraints.*\n\n**Question 2**: Did the authors analyze the auxiliary latent space? Was there any interpretable meaning?\n\nWe analyzed the effects of the auxiliary variables on the noise process. While we did observe variation in the noise schedule for different auxiliary variables (Figure 2), we did not find any human-interpretable patterns. We defer the human-interpretable analysis to future work. We did not attempt to analyze the ability of the auxiliary variables to perform representation learning; however, that problem has been extensively studied by Wang et al., 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6378/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709180439,
                "cdate": 1700709180439,
                "tmdate": 1700712897725,
                "mdate": 1700712897725,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0Ibr9A39kQ",
            "forum": "8gZtt8nrpI",
            "replyto": "8gZtt8nrpI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6378/Reviewer_oSn9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6378/Reviewer_oSn9"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors provided a theoretical argument that creating a noise schedule for each input dimension and conditioning it on the input yields improved likelihood estimation, which means noise with different covariance matrices can be applied to the inputs. Furthermore, the authors introduced a novel method to condition the noise schedule on the input via a latent distribution. Empirical experiments are made to demonstrate the effectiveness and efficiency of the new proposed model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well written. The presentation is good and the reference list is complete. \n2. The experiments in this paper is quite solid."
                },
                "weaknesses": {
                    "value": "1. I recommend the authors show some generated images as well as the comparison with other existing models, so that we can see the improvement more clearly. \n2. The theory proposed by the authors only showed us the pipeline of this model. For the reason why polynomial noise scheduling is better than the existing constant/linear/exponential noise scheduling still remains unclear. If it is difficult to obtain a solid theorem, I think it necessary to explain it more. \n3. The idea proposed is not so impressive in my opinion, but it is not a serious weakness since the authors have done solid experiments and made the polynomial noise scheduling model come true."
                },
                "questions": {
                    "value": "1. Do you use pretrained score estimator, or you trained your own? Since the polynomial noise scheduling is originally proposed by you, there are no pretrained score estimators to use I guess. Is it right?\n2.There are no more additional questions. The authors only need to answer my questions in the \"weakness\" section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6378/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698880466021,
            "cdate": 1698880466021,
            "tmdate": 1699636705344,
            "mdate": 1699636705344,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IbOWl8u62m",
                "forum": "8gZtt8nrpI",
                "replyto": "0Ibr9A39kQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6378/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6378/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer oSn9"
                    },
                    "comment": {
                        "value": "We want to thank the reviewer for their constructive feedback. We address each concern below.\n\n**Concern 1**: Understanding the novelty of the paper relative to recent work. \n\nMuLAN is the **first** method to introduce a learned adaptive noise process. A widely held assumption is that the ELBO objective is invariant to the noise process (Kingma et al., 2021). We __dispel this assumption__: when input-conditioned noise is combined with (a) multivariate noise, (b) a novel polynomial parameterization, and (c) auxiliary variables, a learned noise process yields a tighter ELBO and a new state-of-the-art in density estimation. While (a), (c) were proposed in other contexts, we leverage them as subcomponents of a novel algorithm.\n\nThe table below summarizes the relationship between MuLAN and other methods.\n| | Learned noise | Multivariate noise | Input Conditioned noise | Auxiliary latents | Noise parameterization |\n| --- | --- | --- | --- | ---  | --- |\nVDM (Kingma et al., 2021) | **Yes** | No |No | No | Monotonic neural network |\nBlurring Diffusion Model (Hoogeboom et al., 2022) | No | **Yes** | No | No | Frequency scaling |\nInfoDiffusion (Wang et al., 2023) | No | No | No | In denoising process | Cosine schedule |\nMuLAN (ours) | **Yes** | **Yes** | **Yes** | In noising & denoising processes | Polynomial, sigmoid\n\n**Concern 2**: The inclusion of samples from the model.\n\nWe have added the samples to the paper in the appendix G.\n\n**Concern 3**: Explaining why a polynomial noise schedule performs empirically better\n\nThe reason why a polynomial function works better than a sigmoid or a monotonic neural network as proposed by VDM is rooted in Occam\u2019s razor. In Appendix D2, we show that a degree 5 polynomial is the simplest polynomial that satisfies several desirable properties, including monotonicity and having a derivative that equals zero exactly twice. Simpler noise processes (e.g., scalar, exponential) are not sufficiently expressive to achieve these properties. More expressive models (e.g., monotonic 3-layer MLPs) are more difficult to optimize, hence perform worse.\n\n**Question 1**: Were the models trained from scratch?\n\nYes, all the models were trained from scratch.\n\n**Addendum**: New experimental results\n\nWe report new experimental results, and we now achieve a new state-of-the-art in density estimation on CIFAR10 and ImageNet. The table below summarizes our results.\n\n| Method | CIFAR10 bpd | ImageNet bpd |\n|-------------|-------------|-------------|\n| PixelCNN (Van den Oord et al., 2016) | 3.03 | 3.83 \nImage Transformer (Parmar et al., 2018) | 2.90 | 3.77\nScore SDE (Song et al., 2020) | 2.99 | -\nImproved DDPM (Nichol & Dhariwal, 2021) | 2.94 | -\nVDM (Kingma et al., 2021) | 2.65 | 3.72 \niDODE (Zheng et al., 2023) | 2.56  | 3.69\nMuLAN (ours; initial version) | 2.60 \u00b1 1e-3 | 3.71 \u00b1 1e-3\nMuLAN (**ours; rebuttal version; state-of-the-art**) | **2.55** \u00b1 1e-3 | **3.67\\*** \u00b1 1e-3\n\n\n*\\* Partial result obtained on 70% of the data due to time constraints.*\n\nOur updated method adds two architectural improvements: velocity reparameterization (as in Zheng et al., 2023) and truncated normal dequantization (Dinh et al., 2017; Salimans et al., 2017; Ho et al., 2019; Zheng et al., 2023). For both iDODE and MuLAN with velocity parameterization, we report an importance-sampled estimate of test NLL with K=20 samples, as in the iDODE paper (sec A.1., eqn. 32)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6378/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709732316,
                "cdate": 1700709732316,
                "tmdate": 1700713318017,
                "mdate": 1700713318017,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3l1kvbLRKy",
            "forum": "8gZtt8nrpI",
            "replyto": "8gZtt8nrpI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6378/Reviewer_PeVN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6378/Reviewer_PeVN"
            ],
            "content": {
                "summary": {
                    "value": "The paper suggests a method for teaching diffusion models to adapt their noise schedules in order to increase the ELBO (Evidence Lower BOund). The authors observe that if the noise schedule is expanded to include multiple variables, the ELBO for diffusion models will change depending on the noise schedule. This variation allows for the noise schedule to be optimized at the same time as the diffusion model parameters to enhance likelihood. The authors also explore instance-conditional diffusion along with auxiliary variables. They discovered that using these multivariate noise schedules combined with auxiliary variables enables the training of diffusion models that not only surpass previous benchmarks in terms of likelihood but also converge more quickly."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper under review brings to light that the Evidence Lower BOund (ELBO) for continuous-time diffusion models, as described in the Variational Diffusion Model (VDM) paper, remains unchanged across various noise schedules only when the noise is univariate. It presents a novel finding that for multivariate noise schedules, the ELBO transforms into a line integral and varies with different noise schedules. This is insightful and could pave the way for further research in diffusion models.\n\n* While VDM sets a challenging benchmark in terms of log-likelihoods, the paper in question surpasses these results, which is very impressive.\n\n* Although the use of auxiliary variables in diffusion models isn't a new concept, the approach of conditioning noise schedules on such variables, as shown in this paper, is a valuable contribution that enhances model likelihoods.\n\n* The clarity of the writing and the effective presentation of the paper are commendable."
                },
                "weaknesses": {
                    "value": "* The concept of multivariate noise schedules is intriguing; however, it appears that it does not function effectively by itself and requires auxiliary variables for better performance. This raises the question of whether the combined learning of noise schedules and the diffusion model is advantageous.\n\n* The potential improvements for Variational Diffusion Models (VDM) through the use of auxiliary latent variables remain unclear. It would be helpful to understand the significance of these variables in enhancing the likelihood. Although the authors have provided ablation studies for MuLAN without multivariate aspects, it's uncertain whether this is directly comparable to VDM with an auxiliary variable due to possible differences in noise schedule parameterization.\n\n* The authors justify the learning of noise schedules based on the manual adjustment of such schedules in high-resolution image diffusion models. Yet, they focus on maximizing the Evidence Lower Bound (ELBO) for their learning method, while current diffusion models tend to optimize a different objective that emphasizes perceptual quality of samples. Whether their method is applicable or beneficial when the goal is not ELBO is not clear.\n\n* Additionally, the discussion of related works could be more comprehensive. The paper frequently refers to continuous-time diffusion models but often overlooks citation [1], from which such models originate. The authors could provide a more thorough background for readers by acknowledging concurrent works [2] and [3], which propose the same ELBO for continuous-time diffusion models. This inclusion would add value to the context in which VDM is discussed.\n\nReferences:\n\n[1] Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S. and Poole, B., 2020. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456.\n\n[2] Song, Y., Durkan, C., Murray, I. and Ermon, S., 2021. Maximum likelihood training of score-based diffusion models. Advances in Neural Information Processing Systems, 34, pp.1415-1428.\n\n[3] Huang, C.W., Lim, J.H. and Courville, A.C., 2021. A variational perspective on diffusion-based generative models and score matching. Advances in Neural Information Processing Systems, 34, pp.22863-22876."
                },
                "questions": {
                    "value": "I would like to hear the authors' thoughts on the weaknesses identified above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6378/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699257543957,
            "cdate": 1699257543957,
            "tmdate": 1699636705198,
            "mdate": 1699636705198,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qPOatA1Rcl",
                "forum": "8gZtt8nrpI",
                "replyto": "3l1kvbLRKy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6378/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6378/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer PeVN"
                    },
                    "comment": {
                        "value": "We want to thank the reviewer for their constructive feedback. We address each concern below.\n\n**Concern 1**: Multivariate noise requires auxiliary variables to be useful. This raises the question of whether the combined learning of noise schedules and the diffusion model is useful.\n\nThe combined learning of the noise schedules and the diffusion model is beneficial, as illustrated in Fig. 1. Neither a multivariate noise schedule nor an auxiliary variable scalar noise schedule individually leads to improvement; however, when these are combined, MuLAN yields a significantly better BPD than VDM. \n\nNote also that we could not learn the noise separately from the diffusion model in MuLAN: they are coupled via the auxiliary variables, which are found in both.\n\n**Concern 2**: The improvement over VDMs through the use of auxiliary variables is unclear. What is the role of these variables in improving log-likelihood?\n\nAuxiliary variables by themselves do not improve the log-likelihood of VDMs: we show this in Figure 1. Auxiliary variables require applying the ELBO twice: this yields a training objective that bounds the marginal log-likelihood less tightly than in VDM. By itself, this lowers performance (Figure 1; orange line). However this performance drop is more than compensated by learning the noise process (Figure 1; red line). Thus, obtaining good log-likelihood requires using the entire set of components introduced in MuLAN (including noise that is multivariate, adaptive, and learned).\n\n**Concern 3**: The motivation comes from the manual adjustment of noise schedules in diffusion models. Does mulan lead to improved performance when the obj. Isn\u2019t ELBO?\n\nThe main motivation of our work comes from variational inference: we view the noising process as a variational posterior, and we learn it to obtain a tighter ELBO (see Section 3.1). This in turn yields state-of-the-art density estimation.\n\nNote that the manual adjustment of noising schedules in high-resolution diffusion models is not the motivation for our work. We also do not claim that likelihood correlates with image quality. The passage in question is only a citation to relevant concurrent work.\n\nLastly, the goal of this paper is to study how noise schedule affects the likelihood. We leave to future work the question of whether a learnable noise schedule is also beneficial when the objective function isn\u2019t ELBO. Since there is evidence that manual adjustment of the noise schedule helps with improving the perceptual quality of the images (Chen, 2023; Hoogeboom et al., 2023), methods inspired by MuLAN hold promise there.\n\n**Concern 4**: Lack of a citation to Song et al., 2020.\n\nWe thank the reviewer for bringing this to our attention. We have added the reference to our paper.\n\n**Addendum**: New experimental results\n\nWe report new experimental results, and we now achieve a new state-of-the-art in density estimation on CIFAR10 and ImageNet. The table below summarizes our results.\n\n\n| Method | CIFAR10 bpd | ImageNet bpd\n| --- | --- | --- |\nPixelCNN (Van den Oord et al., 2016) | 3.03 | 3.83 \nImage Transformer (Parmar et al., 2018) | 2.90 | 3.77\nScore SDE (Song et al., 2020) | 2.99 | -\nImproved DDPM (Nichol & Dhariwal, 2021) | 2.94 | -\nVDM (Kingma et al., 2021) | 2.65 | 3.72 \niDODE (Zheng et al., 2023) | 2.56  | 3.69\nMuLAN (ours; initial version) | 2.60 \u00b1 1e-3 | 3.71 \u00b1 1e-3\nMuLAN (**ours; rebuttal version; state-of-the-art**) | **2.55** \u00b1 1e-3 | **3.67\\*** \u00b1 1e-3\n\n\n*\\* Partial result obtained on 70% of the data due to time constraints.*\n\nOur updated method adds two architectural improvements: velocity reparameterization (as in Zheng et al., 2023) and truncated normal dequantization (Dinh et al., 2017; Salimans et al., 2017; Ho et al., 2019; Zheng et al., 2023). For both iDODE and MuLAN with velocity parameterization, we report an importance-sampled estimate of test NLL with K=20 samples, as in the iDODE paper (sec A.1., eqn. 32)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6378/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712501138,
                "cdate": 1700712501138,
                "tmdate": 1700713330989,
                "mdate": 1700713330989,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]