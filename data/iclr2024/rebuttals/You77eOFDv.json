[
    {
        "title": "RefConv: Re-parameterized Refocusing Convolution for Powerful ConvNets"
    },
    {
        "review": {
            "id": "irHs0aW5Q3",
            "forum": "You77eOFDv",
            "replyto": "You77eOFDv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission714/Reviewer_qHhh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission714/Reviewer_qHhh"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a novel re-parameterization method named Re-parameterized Refocusing, which can establish connections across the channels of the learned conv kernel.\nExperiments show that the proposed method can improve the performance of many convnets in various tasks, such as image classification and segmentation, without introducing any computation cost in inference phase."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method is novel and effective.\n2. The experiments are solid."
                },
                "weaknesses": {
                    "value": "None"
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission714/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722825515,
            "cdate": 1698722825515,
            "tmdate": 1699635998680,
            "mdate": 1699635998680,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "ohJ1SgSHDa",
            "forum": "You77eOFDv",
            "replyto": "You77eOFDv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission714/Reviewer_Z4v5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission714/Reviewer_Z4v5"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel approach to neural network training, specifically by bifurcating the weight training process into two distinct stages. However, I believe the validation of the method's effectiveness is not adequately comprehensive. Given the current state of the paper, my recommendation would be to not accept it in its present form."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The concept presented in the paper captures the interest.\n\n\nFigure 1 is exceptionally clear and effectively conveys the central concept of the method proposed in the paper.\n\n\nBecause the parameters are seamlessly integrated, the proposed method does not incur additional costs during the inference phase.\n\n\nThe paper employs techniques such as visualization to offer numerous valuable insights."
                },
                "weaknesses": {
                    "value": "The proposed method incurs a higher training cost compared to the original approach. My concern does not lie with the cost itself; rather, I am questioning the accuracy and reliability of the validation process employed.\n\n\nThe authors believe that their method can indirectly connect information from different channels of the input, which is clearly a mistake. Let x = [x1, x2, ..., xc]; y = [y1, y2, ..., yc]. It is obvious that y1 does not contain the content from x2 to xc. If there is any, please prove it using the notation I provided.\n\n\n\nI am quite familiar with ImageNet, and I have concerns about the data presented in Table 1. I would like the authors to refer to the data from timm. The authors might argue that their values are lower than the standard libraries in timm because they only trained for 100 epochs, but I consider this a drawback. If the standard training procedure from timm was used, perhaps the authors' method would not show any gain. It is conceivable that the authors' method is essentially equivalent to providing more extensive training to an originally under-trained model, albeit with a longer training time. If a model is adequately and standardly trained, the authors' method should be unnecessary.\n\n\nIn the first experiment of Section 4.4, the authors should train all models to full convergence (for instance, more than 500 epochs) before making comparisons. Stepping back, when the authors retrain this model, do they use twice the training epochs?\n\n\n\nThe second experiment in Section 4.4 is incorrect. The authors should not simply use a small learning rate to fine-tune; instead, they should follow timm\u2019s practice of training from scratch for 500 epochs until convergence."
                },
                "questions": {
                    "value": "See #Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission714/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782462699,
            "cdate": 1698782462699,
            "tmdate": 1699635998598,
            "mdate": 1699635998598,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qpEZv4cgHK",
                "forum": "You77eOFDv",
                "replyto": "ohJ1SgSHDa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission714/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank the reviewer for appreciating the conceptual novelty and insights of our paper. We have revised the paper according to the constructive feedbacks. \n\nWeakness 1 (connecting the channels)\n\nGiven x = [x1, x2, ..., xc]; K=[K1, K2, ..., Kc]; y = [y1, y2, ..., yc], it is true that K1 can not **directly** extract the features of x2 to xc. However, in the pre-training stage, each Ki is trained to extract the feature patterns of xi, that is to say, each Ki learns certain representations corresponding to xi. Let function R represent the pre-training process that produces the channel weights given the features of a channel, we denote that by Ki:=R(xi) because yi=conv2d(Ki, xi) in the pretraining phase (conv2d is the convolution operation). RefConv makes each kernel channel learn a combination of every channel through the trainable Refocusing Transformation T, namely, New_Ki = T(K1, K2, ..., Kc) = T(R(x1), R(x2), ..., R(xc)), thus RefConv essentially makes the transformed kernel learn the combination of representations of each input feature channel xi. With such a kernel New_K, we have new_yi = conv2d(New_Ki, xi) = conv2d(T(R(x1), R(x2), ..., R(xc)), xi), which **indirectly** establishes the connections between different channels. \n\nWe explained this mechanism in the original paper (the middle part of Page 2). [With a properly designed Refocusing Transformation, we can relate the parameters of a specific kernel channel to the parameters of the other kernel channels, i.e., make them refocus on the other parts of the model (rather than the input features only) to learn new representations. As the latter are trained with the other feature channels, they encode the representations condensed from the other feature channels, so that we can indirectly establish connections among the feature channels, which cannot be realized directly (by the definition of DW conv).]"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643034080,
                "cdate": 1700643034080,
                "tmdate": 1700643034080,
                "mdate": 1700643034080,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vr32875tXh",
            "forum": "You77eOFDv",
            "replyto": "You77eOFDv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission714/Reviewer_3dDh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission714/Reviewer_3dDh"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a technique called Re-parameterized Refocusing Convolution, which is based on the idea of structural re-parameterization, i.e., incorporating more learnable parameters into the model during training and training them for better performance. These parameters are merged into the original model's parameters during inference to achieve the goal of not introducing additional inference costs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The approach in this paper can be viewed as \"convolution B of convolution A\", where A is the convolution parameter trained by the pre-training process and kept frozen once trained.B is the convolution parameter that continues to be trained.\n\nThe method in this paper has a slight advantage over several other structural reparameterization and weight reparameterization methods in terms of results.\n\nI think the conclusion of the final analysis, \"Re-parameterized refocusing reduces redundancy between channels\", demonstrates well the changes that the methods in this paper can make to a pre-trained convolutional model.\n\nThe experiments included a variety of convolutional models."
                },
                "weaknesses": {
                    "value": "Observe that the ImageNet experimental results have about 1% performance improvement on many models, but also a lot more Params.\n\nThe network architectures that come into play are generally early CNN models such as ResNet, DenseNet, MobileNet family, etc. For modern convolutional architectures such as SlaK, RepLKNet, HorNet, etc., the effect is currently unknown."
                },
                "questions": {
                    "value": "1\tThe refocusing technique seems to be one that can be iterated. Can the refocusing technique in this paper continue to be iterative? I.e., after doing one refocusing exercise, then the next one. Will the results continue to improve?\n\n2\tFor the base weight W_b, one of the points claimed by the refocusing technique is the possibility of establishing links between its individual channels. Why is this necessary? Each channel of the base weight kernel has its own role, so to link them?\n\n3\tDuring refocus training, the result after convolution of the base weight W_b with its previous features can be seen as a new \"feature\". The transform weight W_t can be seen as trainable to process this new \"feature\". This process is equivalent to fine-tuning the convolution after \"injecting\" new parameters. I would like to ask if any experiments with other models (e.g. new convolutional networks) have found that this degrades performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission714/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699527051549,
            "cdate": 1699527051549,
            "tmdate": 1699635998502,
            "mdate": 1699635998502,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PuTqBL0dBS",
                "forum": "You77eOFDv",
                "replyto": "vr32875tXh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission714/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank the reviewer for the constructive comments. \n\nWeakness 1 (more params in training)\n\nAlthough RefConv introduces extra params, these params are conducted on the basis weights instead of the feature maps, thus the extra Flops and memory cost introduced in training are negligible as stated in Sec.3.2 and quantitively shown in Table 1.\nMoreover, these extra params can be equivalently transformed after training, thus no extra params are introduced in inference.\n\nWeakness 2 (modern architectures)\n\nWe also test the effectiveness of RefConv on two modern architectures, ConvNeXt [1] and FasterNet [2], the improvements are 0.96% and 1.15%, as shown in Table 1.\\\n[1] A convnet for the 2020s. (CVPR 2022)\\\n[2] Run, don\u2019t walk: Chasing higher flops for faster neural networks. (CVPR 2023)\n\nQuestion 1\n\nAs requested, we conduct refocusing technique for the second time on the trained MobileNetv2 with RefConv, the accuracy is slightly  enhanced from 72.35% to 72.43%. Then the accuracy remains close to 72.43% when more iterations of refocusing technique are conducted.\\\nThus the performance improvement that RefConv brings may have an upper limit, which may be because conducting the refocusing technique once has been capable enough to effectively reduce the channel redundancy and learn new representations.\n\nQuestion 2\n\nDW conv was originally proposed to reduce the model size and FLOPs but at the cost of representational capacity. Consider a two-channel input feature map whose shape is (2, H, W), assume the conv layer is a two-channel DW conv whose kernel shape is (2, 1, K, K). The first channel of output only relates to the first channel of the input, and the second channel of output only relates to the second channel of input. But if the kernel is a regular (dense) conv, the kernel shape will be (2, 2, K, K), so that each of the output channel relates to all the input channels. From this perspective, we can see that a regular conv's representational is higher than a DW conv, i.e., it comes at a cost to let \"each channel of the base weight kernel have its own role\". \n\nAs evidence, in practice, if we only consider the performance, regular conv is always better than DW conv (namely, the accuracy will become higher when simply replacing DW conv with regular conv). Therefore, it's expected that implicitly connecting the channels of the DW conv can narrow the representational gap between the DW conv and the regular conv.\n\nWe explain how RefConv works. Taking the aforementioned two-channel DW conv for example, we denote its input, output, and kernel by X, Y, and W, respectively, so that using pytorch-style pseudo code, this layer can be represented by\\\nY = conv2d(X, W)\n\nRefConv can link the channels by re-parameterizing the kernel and such a re-parameterization is realized with a convolution on W. So that\\\nY = conv2d(X, conv2d(W_b, W_r))\\\nNote that conv2d(W_b, W_r) can be seen as a re-parameterization of the original W. Since conv2d(W_b, W_r) is a dense conv, each channel of the output directly relates to each channel of W_b, so that each output channel of Y **indirectly** relates to each channel of W_b. Note that we omit the \"identity mapping\" (Eq.1 in the paper) here for simplicity.\n\nExcept for the improvements in performance, in this paper, we have also provided evidence that linking the channels brings benefits. In a DW conv, since there are no connections between the channels of DW conv, it is easy to result in redundancy. The evidence is shown in Section 4.5. In contrast, RefConv reduces the redundancy, as shown in Figure 4. This is because by linking the channels with W_r, the re-parameterized kernel W_t is encouraged to learn new representations through the learnable combination of the channels of W_b, which were originally independent.\n\nQuestion 3\n\nSo far we have not encountered the performance degradation.\\\nAs discussed above, in the procedure of refocus training, the basis weights W_b dose not conduct convolution on the feature maps, it actually act as the input\"features\" of the refocusing transformation with W_r. Then the output generated by the refocusing transformation is the transformed weights W_t, which replaces the W_b and conducts the convolution on the feature maps. To conclude, in refocus training, W_b does not serve as a kernel to convolve on anything, W_r conducts refocusing transformation on W_b and generates W_t, and W_t is the kernel of convolution on the input feature maps and generates the output feature maps.\\\nWe reckon that RefConv is more than just simply fine-tuning the original conv layer. It uses learnable transformations to establish the connections between convolution kernels, learning new representations through combining the learned representations by the basis kernels, thus strengthening the model representational capacity.\\\nMoreover, due to the use of \"identity mapping\" (Eq.1 in the paper), the representational capacity of W_t should be no lower than the original basis kernel."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700021115996,
                "cdate": 1700021115996,
                "tmdate": 1700021115996,
                "mdate": 1700021115996,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FdsVEWbghY",
                "forum": "You77eOFDv",
                "replyto": "PuTqBL0dBS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission714/Reviewer_3dDh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission714/Reviewer_3dDh"
                ],
                "content": {
                    "comment": {
                        "value": "The response partly addresses my concerns. \n\nThe effect of RefConv on more recent CNNs is still unclear for me.\n\nAs the authors only provided the results of FasterNet-S and ConvNeXt-T, at the very least, larger models from the same family should be reported. \n\nAdditionally, I suggest that the authors consider including more modern CNNs to demonstrate that RefConv is a general method, rather than effective only on some \"classic\" CNNs. \n\nAlso, I recommend using [1] as the baseline for ResNet. \n\nGiven the current rebuttal, I keep my rating unchanged.\n\n[1] ResNet Strikes Back: An Improved Training Procedure in Timm."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665946732,
                "cdate": 1700665946732,
                "tmdate": 1700665946732,
                "mdate": 1700665946732,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]