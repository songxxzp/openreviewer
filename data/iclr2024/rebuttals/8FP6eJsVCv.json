[
    {
        "title": "Explanation Shift: How Did the Distribution Shift Impact the Model?"
    },
    {
        "review": {
            "id": "YURBZ0zI2A",
            "forum": "8FP6eJsVCv",
            "replyto": "8FP6eJsVCv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1220/Reviewer_3HUZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1220/Reviewer_3HUZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a model for detecting data shift by modeling and quantifying the shift in explanations rather than model performance shift or data distribution shift directly. \u201cExplanations\u201d are defined in terms of a feature-level contribution to the (relative) model output, e.g. Shapley values. The distribution of explanations between the training and the evaluation/new data sets are quantified to get a measure of the explanation shift. The measure of shift is based on a two-sample test where a classifier predicts whether the explanation come from the training or new distribution."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea to focus on changes in explanation distribution allows for detecting shift even when there is no effect on model predictions (P(f(D))=P(f(D^{new})), and without making assumption about the type of shift to detect. This formulation allows for detecting covariate, prediction, concept, and novel group shift as long as the explanation values change. \n\nThe above is well supported by experiments of multiple types of shifts and compared to several shift-detecting baselines as well as alternative distributions in the two-sample test. \n\nExperiments and analytical examples showcase the settings where alternative shift detection approaches fail and explanation shift detection can succeed."
                },
                "weaknesses": {
                    "value": "Computing explanation vectors with Shapley values limits the number of features that can be considered or, if using TreeShap for efficiency, limits the models that can be used to tree-based. \n\nResults are sensitive to choice of prediction model and detector model"
                },
                "questions": {
                    "value": "Is there some way to know which predictor and detector model should be used based on prior assumptions about the data itself or the expected shift, if any, that you are trying to detect? Do you have any insights into when different models agree/disagree, for example?\n\nTop of page 8 referencing Figure 4 says the PR18 is the most disparate. Should this be KS18 as it results in the largest AUC for the shift detector? Or what is meant by \u201cdisparate\u201d here? Can you comment on the difference between the results shown in Figure 4 and those from Figures 6-8 in Appendix E? It seems employment, travel time, and mobility follow similar patterns, but differ from Figure 4. \n\n\nMinor comments  \nIs the line for g_\\phi=Input, f_\\theta=Log missing in Figure 3 middle? Or is it overlapping with the XGBoost line?  \n\n\u201cvii\u201d on page 7 below Figure 2 should be \u201cB7\u201d."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1220/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799701854,
            "cdate": 1698799701854,
            "tmdate": 1699636048457,
            "mdate": 1699636048457,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sj6cLhsCzr",
                "forum": "8FP6eJsVCv",
                "replyto": "YURBZ0zI2A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1220/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1220/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, many thank for the reviewer and its valuable and insightful comments. Please consider the main points of our rebuttal above and our specific points that follow now:\n\n> Results are sensitive to choice of prediction model and detector model\n\nYes, different explanation shift $g$ will perform differently on different model $f$. The pipeline is susceptible to both. In the Appendix Table 7, we have compared several estimators $f$ and detectors $g$\n\n\n> Question 1. Is there some way to know which predictor and detector model should be used based on prior assumptions about the data itself or the expected shift, if any, that you are trying to detect? Do you have any insights into when different models agree/disagree, for example?\n\nNot to the best of our knowledge. Note that this question is similar to how we could know what was the best ML estimator prior to fitting.\n\n\n>  Question 2. Top of page 8 referencing Figure 4 says the PR18 is the most disparate. Should this be KS18 as it results in the largest AUC for the shift detector? Or what is meant by \u201cdisparate\u201d here? \n\nYes, this is a typo, which we will fix\u2014many thanks for noting. \n\n\n> Question 3. Can you comment on the difference between the results shown in Figure 4 and those from Figures 6-8 in Appendix E? It seems employment, travel time, and mobility follow similar patterns, but differ from Figure 4.\n\nOur main analysis in this experimental design is that, given similar data, the choice of what to predict is relevant. Depending on what the model has learned, there are different results. As the reviewer has correctly noticed, the most disparate state is not always PR. This shows an example of when the explanations learned representation varies but changes on the input data are mostly constant. We will further refine the paragraph at the end of section 5.2.2 to make this clearer."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1220/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699952481241,
                "cdate": 1699952481241,
                "tmdate": 1699962299759,
                "mdate": 1699962299759,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HtNj2fVu8M",
            "forum": "8FP6eJsVCv",
            "replyto": "8FP6eJsVCv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1220/Reviewer_QGE3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1220/Reviewer_QGE3"
            ],
            "content": {
                "summary": {
                    "value": "The authors tackle the problem of detecting whether distributions shift between some labelled source dataset and an unlabelled target dataset. In particular, they focus on detecting shifts that affect the behavior of a particular model trained on the source data. Instead of comparing distributions of features directly between source and target, they propose a method based on comparing the distribution of _explanations_ generated using Shapley Values for the given model. This comparison is done using a binary classifier trained to predict the domain from the explanations. They compare their method with a variety of baselines on synthetic and real data, finding that their method has greater sensitivity to data shifts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is generally easy to read and well-written.\n- The paper provides a nice formulation of the problem.\n- The paper tackles an important real-world problem.\n- The authors provide theoretical analyses for a few simple synthetic cases."
                },
                "weaknesses": {
                    "value": "1. I am not convinced that the results demonstrate the empirical superiority of the proposed method relative to the baselines. The authors only compare to the baselines in Figure 2. Here, there are several other methods that are competitive with explanation shift. In addition, the authors do not show confidence intervals in this figure. I also contest that \"good indicators should follow a progressive steady positive slope\", as if the goal is distribution shift detection, the only thing that should matter is the outcome of the hypothesis test.\n\n2. All of the datasets considered are tabular datasets. How would the authors adapt their method to images (where X could be pixels) and text (where X is a sequence of tokens and f is an LLM)? It seems like Shapley values would be less meaningful and harder to compute in these settings.\n\n3. In the real-world datasets, it is unclear what the ground truth should be, and so it is hard to say whether the proposed method is behaving as intended. For example, how do we know in Section 5.2.2 that the distribution in CA18 is actually different than CA14, in a way that affects model performance?\n\n4. One important aspect of distribution shift detection is to isolate the shift to particular distributions (i.e. particular shifts in Definitions 2.1-2.5). This is underexplored in most of paper. The authors do explore this by examining the feature importances in Section 5.2.2, but this seems quite ad-hoc and should be characterized further. For example, how would this behave theoretically in the covariate shift case in Example 4.1?\n\n5. The authors should consider examining the scenario where the number of samples on the target domain is limited, both empirically and theoretically. How does the power of the two-sample test scale with the number of target domain samples?\n\n6. The authors have missed several important prior works [1-2], which should be baselines.\n\n7. (minor) There are many typos in the paper, including \"taks\" in Section 4.1, \"datasests\" in Section 5.1, \"Sensitivy\" in Figure 2, and an extra bracket in Efficiency Property.\n\n[1] Towards Explaining Distribution Shifts. ICML 2023.\n\n[2] A Learning Based Hypothesis Test for Harmful Covariate Shift. ICLR 2023."
                },
                "questions": {
                    "value": "Please address the weaknesses above, and the following questions:\n1. How does the runtime of the algorithm compare to the baselines? I believe that Shapley values may be time consuming to compute especially when there are a lot of features.\n\n2. Have the authors tried any other tests to distinguish between the two explanation distributions, other than the classifier based approach?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1220/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824915799,
            "cdate": 1698824915799,
            "tmdate": 1699636048367,
            "mdate": 1699636048367,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v1LhJi5oJi",
                "forum": "8FP6eJsVCv",
                "replyto": "HtNj2fVu8M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1220/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1220/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, many thanks for the comments, please consider th points of our main rebuttal above and our specific points that follow now:\n\n>Item I.1 I am not convinced that the results demonstrate the empirical superiority of the proposed method relative to the baselines. The authors only compare the baselines in Figure 2. Here, there are several other methods that are competitive with explanation shift. In addition, the authors do not show confidence intervals in this figure. I also contest that \"good indicators should follow a progressive steady positive slope\", as if the goal is distribution shift detection, the only thing that should matter is the outcome of the hypothesis test.\n\nAnswer A.1: The reviewer may note that we also compare baselines B4, B5, and B6 in Figure 3. Wrt our evaluation, we fully agree with the reviewer that the approach's effectiveness is key. This raises the question of how best to measure effectiveness. We have decided to construct experiments in Figures 2 and 3 in a way such that we can control the level of distribution shift by continuously increasing $\\rho$, the co-variance (Fig 2), and the level of OOD data (Fig 3). A good metric for distribution shift should be able to mirror this control parameter. By the design of our experiment, step changes or fluctuations point towards less reliable metrics for recognising distribution shifts. We will make this experimental design clearer in the text. Also, see the extra evaluation above (2. quantifiable metrics).\n\n\n> I.2 All of the datasets considered are tabular datasets. How would the authors adapt their method to images (where X could be pixels) and text (where X is a sequence of tokens and f is an LLM)? It seems like Shapley values would be less meaningful and harder to compute in these settings.\n\nOur method is limited to tabular data. While this constitutes a limitation to our approach, we posit that understanding the distribution shift of tabular data is of problem hugely relevant for both research and practice. \n\n\n > I.3 In the real-world datasets, it is unclear what the ground truth should be, and so it is hard to say whether the proposed method is behaving as intended. For example, how do we know in Section 5.2.2 that the distribution in CA18 is actually different than CA14, in a way that affects model performance?\n\nA.3 As described in section 2.2 (Impossibility of model monitoring), (Garg 2021)\u2019s theorem shows that it is not possible to relate distribution shift to a change in model performance, particularly for tabular data where concept shift cannot be detected by properties of the visual domain. Therefore, our approach focuses on the change in how a given model works on data.\n\n\n> I.4  One important aspect of distribution shift detection is to isolate the shift to particular distributions (i.e. particular shifts in Definitions 2.1-2.5). This is underexplored in most of paper. The authors do explore this by examining the feature importances in Section 5.2.2, but this seems quite ad-hoc and should be characterized further. For example, how would this behave theoretically in the covariate shift case in Example 4.1?\n\nA1.4 We fully agree with the reviewer, in the last paragraph of section 2.1 we have stated: \"_In practice, multiple types of shifts co-occur together and their disentangling may constitute a significant challenge that we do not address here._\"\". We have only analyzed particular shifts on synthetic data where we can fully characterize the type of shift. \n\n\n > I.5 The authors should consider examining the scenario where the number of samples on the target domain is limited, both empirically and theoretically. How does the power of the two-sample test scale with the number of target domain samples?\n\nA1.5: It is correct that our recognition procedure is a binary classifier and the reliability of the classifier depends on the number of samples on which it is trained. Previous work has studied the limitations of Classifier Two Sample Tests in the particle physics domain, in which they suggest using bootstrap permutation tests to get a better handle on the test's accuracy.\n\nWe've taken this methodology. In our own experiments (check out Section 5.2.2), we have included bootstrap permutation testing between the in-distribution (CA14) bootstraps and other states bootstraps.\nFurthermore, in our experiment in section 5.2.1 where an increasing fraction of data is from OOD, we that from a small fraction onwards our metrics picks up the changes in the model due to distribution shift. \n\n[3] Model-independent detection of new physics signals using interpretable SemiSupervised classifier tests, Chakravarti, Purvasha and Kuusela, Mikael and Lei, Jing and Wasserman, Larry, The Annals of Applied Statistics, 2023"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1220/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699966004673,
                "cdate": 1699966004673,
                "tmdate": 1699966004673,
                "mdate": 1699966004673,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Xz7HXQNxCW",
            "forum": "8FP6eJsVCv",
            "replyto": "8FP6eJsVCv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1220/Reviewer_Kdfx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1220/Reviewer_Kdfx"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to use distribution shifts in \"model explanations\" such as Shapley values to attribute/identify distribution shifts across domains. The problem is phrased as that of running two-sample tests/conditional independence tests on explanations generated in the training domain and the target domain. The conditional independence test is operationalized using classifier two sample tests. \n\nExample cases motivate how univariate (feature-level) two-sample tests will not pick up conditional covariate shifts, and that for an optimally trained model with uninformative features in the training set, a univarite feature-level two sample test will detect distribution shift, while the explanations will not have a shift. \n\nOther example cases showing that: explanation shift does not always imply a shift in the predictive distribution (this example is not clear to me as terms are not well defined). \n\nFinally a negative result showing the concept shift cannot be indicated by an explanation shift is presented. \n\nEmpirical evaluation consists of synthetic data analysis: Here, sensitivity of classifier two-sample tests and metrics of evaluating distribution shifts are considered. This evaluation suggests NDCG may be unstable as a test-statistic for evaluating distribution shifts.\n\nReal-world analysis: UCI Adult Income demonstrates AUCs of the two-sample test proposed in the paper with multiple choices of model families used to train the original classifer as well as the explanation classifier. \n\nSpatio-temporal shifts are evaluated using this data along with interpretation of the linear coefficients of the explanation shift detector model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The overall paper is well written. Understanding how distribution shifts affect ML model performance is important. \n\n2. The hypothesis that distribution shifts in explanations generated for a model could give a hint about overall distribution shifts and their impact on model performance is a good one and should be explored"
                },
                "weaknesses": {
                    "value": "1. The authors provide some interesting case studies and examples of the utility of Shapley based explanations in identifying distribution shifts. One aspect of this discussion that could be done better is trying to emphasize what additional assumptions could be required so that explanations such as Shapley can indeed be used for detecting, say, concept shift. This is not an impossible task for other methods, see for example: Liu et al [1]. See also experiments in this paper. \n\n2. The example on \"uninformative features\" is valid but unrealistic, shortcut learning is real in ML and hoping a model is trained optimally is kind of unrealistic. Also may be best state explicitly that $X_1 \\perp X_2$.\n\n3. The overall empirical evaluation, in my view is weak. May be at least discuss how general the approach will be (can it be used on other data-modalities, beyond tabular data?) \n\n[1] Liu, Jiashuo, Tianyu Wang, Peng Cui, and Hongseok Namkoong. \"On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets.\" arXiv preprint arXiv:2307.05284 (2023)."
                },
                "questions": {
                    "value": "1. What is $\\alpha_i$ in Example 4.3? I am not sure this example is valid based on what I think $\\alpha_i$ will be in this example. Can authors clarify?\n\n2. Why aren't methods that don't actually need a causal graph such as [2] compared to in the paper? \n\n3. In fact it is possible to also compare to other methods of Budhatoki et al, Zhang et al that the authors cite with minimal assumptions that $X \\to Y$ or $Y \\to X$ to highlight limitations of these methods. \n\n4. I see a discussion on using LIME as a possible explanation for addressing the same, however, I am not sure whether crucial properties of Shapley are necessary for this method to succeed. I think additional discussion on which explanations have the desirable properties for use in detecting explanation shifts will make the paper stronger. \n\n[2] Namkoong, Hongseok, and Steve Yadlowsky. \"Diagnosing Model Performance Under Distribution Shift.\" arXiv preprint arXiv:2303.02011 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1220/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699140947124,
            "cdate": 1699140947124,
            "tmdate": 1699636048300,
            "mdate": 1699636048300,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dPqENtt0rU",
                "forum": "8FP6eJsVCv",
                "replyto": "Xz7HXQNxCW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1220/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1220/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, many thanks for the comments, please consider the main points of our rebuttal above and our specific points that follow now:\n\n> I.1  The authors provide some interesting case studies and examples of the utility of Shapley-based explanations in identifying distribution shifts. One aspect of this discussion that could be done better is trying to emphasize what additional assumptions could be required so that explanations such as Shapley can indeed be used for detecting, say, concept shift. This is not an impossible task for other methods, see for example: Liu et al [1]. See also experiments in this paper.\n\nA.1: Paper [1] can deal with concept shift because the authors assume that labels $D^{new}_Y$ are given. In our problem scenario, we make the realistic assumption that $D^{new}_Y$ are unknown. Thus, we address a different problem. Also cf. our Main Point 1 at the beginning of our rebuttal.\n\n\nWe will contrast our work against [1] in our discussion. This will further clarify and strengthen our work. \n\n> I.2  The overall empirical evaluation, in my view is weak. May be at least discuss how general the approach will be (can it be used on other data-modalities, beyond tabular data?)\n\nA.2: \u200b\u200bWe have systematically varied models, model parametrizations, and input data distributions. As indicated in Main Point 2 (at the beginning of the rebuttal), we have designed carefully controlled experiments (which are novel by themselves). Additionally, the appendix contains supplementary experimental results, providing details on synthetic data experiments, extending to further natural datasets, showcasing a broader range of modelling choices, and different explanation mechanisms. Thus, we believe that one of the paper's strengths is an extensive empirical evaluation.\n\nAs the reviewer correctly notes, the Classifier Two Sample Test on the explanations distributions is limited to tabular data and is not able to detect concept shift (we have stated this in the discussion). For non-tabular data, it needs further modifications that are out-of-scope for the paper.   Characterizing distribution shift on tabular data constitutes an economically hugely important problem that still lacks appropriate treatment.\n\n\n\n> I.3 Question 1 What is $\\alpha_i$ in Example 4.3? I am not sure this example is valid based on what I think will be in this example. Can authors clarify?\n\nA.3: The Shapley values are a product of a weighting (here: alpha_i for feature i) multiplied by the actual value of the feature (here: x_i). Thus,Shapley values explain a model f by a linear model with coefficients $\\alpha_i$. We will further clarify that the Shapley values can be directly calculated for a linear model and IID data by this formula (Aas, 2021)\n\nThe specific example shows that there is a feature that is not used by the model and is shifted. In this case, we have a distribution shift, that does not affect the model f nor the explanation shift, and any \u201cdistribution shift\u201d methods based on input data will flag it. \n\n\n> I.4 Question 2 Why aren't methods that don't actually need a causal graph such as [2] compared to in the paper?\n[2] Namkoong, Hongseok, and Steve Yadlowsky. \"Diagnosing Model Performance Under Distribution Shift.\" arXiv preprint arXiv:2303.02011 (2023).\n\nA.4: We do not compare to this paper for two reasons:\nThis paper works in a different problem setup. It requires labels $D^new_Y$, whereas we assume that these are not available (cf. answer A.1; Main Point 1)\nWe will be happy to acknowledge the recent work that is reported in the two arxiv papers. However, given that they have not been published at a conference yet, we think that their absence in our discussion should not be held against our submission.\n\n\n> I.5: Question 3: The overall empirical evaluation, in my view is weak. May be at least discuss how general the approach will be (can it be used on other data-modalities, beyond tabular data?)\n\nA.5 Classifier Two Sample Test, as we have done for tabular data, is not directly applicable to text or images. We have focused on tabular data since it constitutes an economically hugely important domain that brings up unique challenges. We have stated this in the discussions, but we will further clarify. \n\n\n> I.6 Question 4 I see a discussion on using LIME as a possible explanation for addressing the same, however, I am not sure whether crucial properties of Shapley are necessary for this method to succeed. I think additional discussion on which explanations have the desirable properties for use in detecting explanation shifts will make the paper stronger.\n\nA.6: The theoretical properties that we require from a feature attribution method are efficiency and uninformativeness (cf section 2.3). We use and compare Shapley values and Lime because these approaches (and their implementations) fulfil or at least approximately fulfil these properties."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1220/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699954450109,
                "cdate": 1699954450109,
                "tmdate": 1699962277097,
                "mdate": 1699962277097,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oIfuMtZxUL",
            "forum": "8FP6eJsVCv",
            "replyto": "8FP6eJsVCv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1220/Reviewer_hRuz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1220/Reviewer_hRuz"
            ],
            "content": {
                "summary": {
                    "value": "The work proposes a method to detect distribution shifts that matter to a model. The central proposal is to study changes in output of an explanation method (like SHAP) between the shifted datasets. This change is quantified using classifier two-sample test, the output of which is used to detect distribution shifts that matter. Through extensive experiments, the proposed test based on explanation outputs is argued to be more sensitive than multiple baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Paper is written well with all relevant background explained in main text.\n- The idea of detecting relevant distribution shifts through their effect on explanations is original and interesting.\n- Work provides intuitive examples to show which distribution shifts can be detected by explanation shifts.\n- It tackles a significant problem. Interpretability of distribution shifts, including their effect on model behavior, is an under-studied problem."
                },
                "weaknesses": {
                    "value": "- Motivation for considering shifts in explanation is not convincing. This, I believe, is because the goal is not stated concretely to then motivate the solution. The stated goal of investigating interaction between distribution shifts and learned model needs to be further characterized in quantifiable forms. \nI agree that not every type of distribution shifts are important to detect. The ones that change model behaviour in some meaningful way are important. However, it is unclear to me why explanations, that too Shapley values, is a meaningful or useful summary of model behavior to consider. Say, we care about AUC or any metric of choice to summarize model behavior, we can directly detect shifts in this metric using the same classification pipeline as proposed. If the reason for using explanations is for the interpretability of the shift in model behavior, the problem and solution needs to be stated differently.\nI think that some of this is addressed in the Appendix by pointing to related work that uses explanations for model debugging, drift detection, or other tasks. This could be used in Introduction to more clearly motivate the approach. \n- Evaluation setup can be improved in terms of quantifying evaluation metrics like sensitivity. \nThe magnitude of AUC does not matter that much I believe. Authors are right in asking for sensitivity in the method but it is not directly evaluated. For instance in Figure 2, it is unclear between (ours) and (B7) methods in the left figure and between (ours) and (B2) which one is better. All seem sensitive in the sense that different correlation coefficients result in different AUCs.\n- Presentation of experiment can be improved. The case study on ACS data in Sec 5.2.2 is a good place to showcase how the question in the paper title (how the shift impacted the model) is addressed by the method. The findings are stated in a matter-of-fact way. These could be contextualized in the data making the significance of the method and its results more apparent. The questions asked through the experiments could be described at the start of Sec 5 and what we learned from the results could be described more directly at the end of the section."
                },
                "questions": {
                    "value": "1. Please clarify the problem statement more formally. What aspects of the model behavior under distribution shifts (interaction between distribution shift and learned model) is of interest? \n\n2. Please motivate the approach (focus on explanations) in terms of the problem statement. How does Shapley values capture the desired aspects of the model behavior?\n\n## Minor (no response is requested)\n\nThe success of the method depends on how powerful the classifier in the two-sample test is, since a powerful enough classifier can detect relevant shifts just from a combination of input data and predictions without requiring explanations. Explanation method, in the proposed approach, helps in creating better and more relevant features for use by the classifier. Therefore, consider describing how to choose the classifier and the explanation method.\n\nExplain C2ST (which I believe is classifier two-sample test?) in Experiments on Page 7.\n\nExplain the reasoning for choosing novel group shift in Sec 5.2.1 and discuss the results in more detail.\n\nGeopolitical -> geographical in Section 5.2.2\n\nOn the use of Shapley values, the claim that they account for co-variate interactions (compared to univariate tests) depends on the payoff function (interventional vs observational) used in computing Shapley values (reference Kumar et al. 2020 Problems with Shapley-value-based\u2026 https://proceedings.mlr.press/v119/kumar20e.html). Discussion in Appendix H could be highlighted in main text.\n\nFor completeness, please define the crossed \\sim notation. Does this mean that the population-limits of the  empirical distributions are not the same?\n\nThe definition of perf(D) is potentially missing a division by number of data points in Sec 2.1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1220/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699171696928,
            "cdate": 1699171696928,
            "tmdate": 1699636048229,
            "mdate": 1699636048229,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hl4ZfSsJjb",
                "forum": "8FP6eJsVCv",
                "replyto": "oIfuMtZxUL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1220/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1220/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, many thanks for the comments, please consider the main rebutall and the specific rebutal:\n\n> I.1 Motivation for considering shifts in explanation is not convincing. This, I believe, is because the goal is not stated concretely to then motivate the solution. The stated goal of investigating interaction between distribution shifts and learned model needs to be further characterized in quantifiable forms. I agree that not every type of distribution shifts are important to detect. The ones that change model behaviour in some meaningful way are important. However, it is unclear to me why explanations, that too Shapley values, is a meaningful or useful summary of model behavior to consider.\n\nA.1: Main Points 1 and 2 at the beginning of our rebuttal address these concerns. To summarize: AUC or likewise metrics cannot be applied in our problem setting, which is frequent in the real world, where new data $D_x^{new}$ comes without labels $D_y^{new}$.\n\nAs (Garg 2021) have shown, correctly estimating the performance $\\text{perf}(D^{new})=\\sum_{(x,y)\\in D} \\ell_\\text{eval}(f_\\theta(x),y)$ is impossible in the presence of distribution shift. Their results imply that AUC and likewise methods are inapplicable for discovering distribution shift when $D_y^{new}$ is not given.\n\nGiven the model predictions $f_\\theta(x)$, we use the Shapley values, as they attribute the contribution of input features to the prediction (more information in section 2.3).\n\nWe will use your question and our response to make the introduction clearer. Thank you.\n\n> I.2  Evaluation setup can be improved in terms of quantifying evaluation metrics like sensitivity. The magnitude of AUC does not matter that much I believe. The authors are right in asking for sensitivity in the method but it is not directly evaluated. For instance in Figure 2, it is unclear between (ours) and (B7) methods in the left figure and between (ours) and (B2) which one is better. All seem sensitive in the sense that different correlation coefficients result in different AUCs.\n\nA.2 In order to clarify and provide a quantifiable metric, we have extended the experimental evaluation in the main rebuttal. We say baselines are underperformant because they achieve a lower correlation with $\\rho$ or are independent of the model used.\n\n\n> I.3 Presentation of experiment can be improved. The case study on ACS data in Sec 5.2.2 is a good place to showcase how the question in the paper title (how the shift impacted the model) is addressed by the method. The findings are stated in a matter-of-fact way. These could be contextualized in the data making the significance of the method and its results more apparent. The questions asked through the experiments could be described at the start of Sec 5 and what we learned from the results could be described more directly at the end of the section.\n\nMany thanks for the suggestion; we fully agree with the reviewer. These minor edits will improve the paper's clarity. We will take them into account.\n\n\n> I.4 Q1  Please clarify the problem statement more formally. What aspects of the model behavior under distribution shifts (interaction between distribution shift and learned model) is of interest?\n\nA.4 Section 4 covers all the different types of shifts: Uninformative shifts vs Informative Shifts. Among the latter, problems in practice have to deal with Covariate shifts, Prediction shifts, and Concept shifts. Discovering concept shift (if not domain assumptions can be made, nor labeled data can be obtain), which is why we are interested in covariate and prediction shifts. These are all defined in section 4 and related to explanation shifts.\n\n\n> I.5 Q2  Please motivate the approach (focus on explanations) in terms of the problem statement. How do Shapley values capture the desired aspects of the model behavior?\n\n The Shapley values, weigh the influence of feature values towards a prediction, \n$\\sum_{j \\in N} S_j(f, x^{\\star}) = f(x^{\\star}) - E[f(X)])$(Efficiency Property - cf. Section 2). \n\nThe strength lies in the decomposition of predictions, offering insight into how each feature contributes to the overall outcome. This attribute is valuable when dealing with high-dimensional distributions, providing a more informative distribution than prediction distributions.\n\nBy employing feature attribution explanation techniques, specifically Shapley and LIME, we exploit this efficiency property. These techniques facilitate a granular understanding of how shifts in specific feature distributions impact the model. \nhe decomposition of the prediction across the contribution of each features  allows for a higher dimensionality distribution than the predictions distributions. Therefore the explanation distribution compasses always more information."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1220/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699966303751,
                "cdate": 1699966303751,
                "tmdate": 1699966303751,
                "mdate": 1699966303751,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0F1rmvFdxd",
                "forum": "8FP6eJsVCv",
                "replyto": "hl4ZfSsJjb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1220/Reviewer_hRuz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1220/Reviewer_hRuz"
                ],
                "content": {
                    "title": {
                        "value": "After author response"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for the detailed response. It partly addresses my concerns. However the ones on problem statement are not resolved. For instance, if detecting a particular aspect of model behavior like sensitivity to covariate or prediction shifts is the focus then this should be highlighted in Introduction and Setup. The properties of explanation outputs that help in such detection, better than prior approaches, should be discussed to motivate the approach. These I feel are substantial changes to the presentation. Thus, I am keeping the score at 5 marginally below threshold. \n\nI now understand that the problem settings of interest do not have labelled data in target, hence accuracy metrics can't be computed. In this case, explanation outputs seems like one alternative. Thanks for quantifying performance of the method in the new experiments."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1220/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643365005,
                "cdate": 1700643365005,
                "tmdate": 1700643365005,
                "mdate": 1700643365005,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]