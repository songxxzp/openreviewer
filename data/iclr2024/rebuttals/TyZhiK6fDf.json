[
    {
        "title": "Co-Learning Empirical Games & World Models"
    },
    {
        "review": {
            "id": "M375W8DP6b",
            "forum": "TyZhiK6fDf",
            "replyto": "TyZhiK6fDf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3944/Reviewer_c2GE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3944/Reviewer_c2GE"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new PSRO variant called Dyna-PSRO which combines the game-theoretic framework of PSRO that trains a population of policies over an environment, with a world model that allows for transfer of training information across the population."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- In my opinion the paper addresses a valid problem in the PSRO literature. The need to re-learn policies from scratch in PSRO is an efficiency bottleneck and there exist other works that have also tried to tackle this problem (i.e. NeuPL). Therefore, because PSRO generates a large amount of training data, it makes sense to try and re-use this data in a smart way in order to generate new policies. \n- Overall, the paper is well-presented and well-executed. It is generally well written and the experiment section is extensive and well-thought out in presenting the benefits of Dyna-PSRO."
                },
                "weaknesses": {
                    "value": "- The primary concern I have with the paper is in relation to the discussion of strategic diversity. This concern is two-fold:\n    1) There exists an extensive array of literature related to diversity within the PSRO framework which has not been discussed by the paper. For example [1][2][3][4][5]. In these works multiple diversity measures are recruited to measure the diversity over a PSRO population, and I would be interested if the authors could comment on specifically why they have chosen their variant of diversity measure over the others in the literature. \n    2) The paper seems to suggest that strategic diversity amongst the population is important. Therefore, why do the authors not perform any form of strategic diversity optimisation, in order to make the population policies more diverse than if they are learned in the classic PSRO way? For example, there are no results in the work that show that there exists much strategic diversity amongst the PSRO policies, and it is quite common from my own experience with PSRO that individual policies can end up strategically very similar without any diversity optimisation.\n\nReferences: \n\n[1] Policy Space Diversity for Non-Transitive Games - Yao et al. 2023\n\n[2] Open-ended learning in symmetric zero-sum games - Balduzzi et al. 2019\n\n[3] Modelling behavioural diversity for learning in open-ended games - Perez-Nieves et al. 2021\n\n[4] Towards unifying behavioural and response diversity for open-ended learning in zero-sum games - Liu et al. 2021\n\n[5] A unified diversity measure for multi agent reinforcement learning - Liu et al. 2022"
                },
                "questions": {
                    "value": "It would be great if the authors could respond to the points that I have raised in the weaknesses section. Primarily:\n\n1) Strategic diversity vs. PSRO diversity metrics\n\n2) Strategic diversity optimisation\n\n3) Strategic diversity evidence\n\nI will be happy to raise my score if the authors can address these, as I think generally this is a strong paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3944/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3944/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3944/Reviewer_c2GE"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3944/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698673296872,
            "cdate": 1698673296872,
            "tmdate": 1699636355347,
            "mdate": 1699636355347,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zBrSUS1K8P",
                "forum": "TyZhiK6fDf",
                "replyto": "M375W8DP6b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3944/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your kind words on the quality of the presentation and execution of this work. We address your questions about strategic diversity, paraphrased here, below.\n\n> \u201cWhy have the authors chosen their measurement of strategic diversity over related works?\u201d\n\nOur reference to \u201cstrategic diversity\u201d is from the perspective of learning a world model. As this is a well-defined supervised learning problem, for our purposes strategic diversity amounts to wanting the training dataset to reasonably cover the test dataset. We do not mean to suggest that we are introducing a new method of measuring diversity. Instead, we\u2019re merely framing and motivating it from the perspective of world model learning in a multiagent system. Hence, we do not claim this as a contribution nor include an extensive literature review on this topic.\n\nThe reviewer offers several citations related to how considering strategic diversity may aid in strategy exploration (the process of selecting which new policy to include within an empirical game). Often these metrics of diversity ask how different is a candidate policy from a set of fixed policies. These metrics can be over the policies themselves (behavioral diversity), or how the policy strategically relates to the set of policies (response diversity). The cited works sometimes adopt ostensibly the same definition of diversity (at least in part) when they refer to behavioral diversity. The quality of these metrics is evaluated by how well they aid in improving the quality of an empirical game. \n\nThe use of strategic diversity in the cited work is thus orthogonal to the consideration of world-model learning addressed here. This raises an interesting question: what is the impact of Dyna-PSRO when augmented to include diversity in its strategy exploration subroutine? We did not consider that here as  Dyna-PSRO is already a rather complex algorithm, and varying strategy exploration approaches would have complicated evaluation of our primary hypotheses.\n\nSpeculating, one would expect to see that the Dyna-PSRO+diversity methods variant would continue to be experientially cheaper than PSRO+diversity due to its inherent transfer learning. However, we expect that the difference between Dyna-PSRO+diversity and PSRO+diversity to be smaller than Dyna-PSRO and PSRO. This is because +diversity methods are using their experiential budget more effectively on game solving. This raises the related question as to how these diversity metrics aid in learning world models, and how well the world models trained through Dyna-PSR+diversity generalize when compared to Dyna-PSRO. These questions should be investigated in future work, but notably don\u2019t undermine any results within this manuscript.\n\n> \u201cWhy did the authors not try and optimize/increase the diversity of the policy set used to train the world models?\u201d\n\nThis is a fair question. We chose to not optimize for diversity in our \u201cstrategic diversity\u201d experiments and instead chose to randomly sample policies from runs of PSRO. We did this to better emulate a \u201cpractical comparison\u201d that we would expect to occur within a run of PSRO. Recall, our primary interest is making algorithmic design decisions that would benefit Dyna-PSRO from this isolated experiment. This method contrasts with that of an artificially constructed set of diverse policies. Any results gained from these policies may not be realized within a run of PSRO as the optimized diversity serves as a potentially large confounder. We would speculate that the benefits of diversity would be greatly exacerbated by optimizing for it within the set of policies used to train the world model. How these results would translate to Dyna-PSRO would require further experiments and would likely depend on augmenting Dyna-PSRO with diversity in its strategy exploration method (see Q1)."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699828710982,
                "cdate": 1699828710982,
                "tmdate": 1699828710982,
                "mdate": 1699828710982,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cPMIcmznxH",
            "forum": "TyZhiK6fDf",
            "replyto": "TyZhiK6fDf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3944/Reviewer_Y4nV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3944/Reviewer_Y4nV"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the benefits of co-learning *transition dynamics and\nexpected reward signal* (using \"world models\") and *game models from which the\nbest response is inferred from strategy profiles of other players and estimated\npayoff matrices* (using \"empirical games\"). A few experiments are run to\nindicate that these paradigms can benefit from integration with each other, and\na final set of experiments demonstrate performance increases when incorporating\nco-learned aspects of world-model and empirical games to a baseline model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper provides an original contribution to learning in repeated games by\ncombining ideas in \"world-models\" and \"empirical games\", and provides experimental results.\n\nWhile unexplored by the paper, the algorithmic elements proposed, by estimating\npayoffs for *other* players, also has relevance to inverse game theory."
                },
                "weaknesses": {
                    "value": "The empirical results of the paper could have benefited from a clearer\nexposition, and some of these results are not as strong as claimed.\n\n\n## Exposition\nStylistically, each experiment should be justified with a clear hypothesis in\nmind, rather relying on the reader to reverse-engineer the hypothesis and how\nthe experiments support the central claims.\n\nMy own reverse-engineering is as follows: The experiments are motivated as a way\ndo demonstrate the benefits of co-learning world models and empirical games.\nThis is intuitively justified by:\n1. The benefits to the world model anticipated by increased exploration\n   (strategy diversity induced by the empirical game model). [Figure 2]\n2. The benefits to the empirical game model in calculating best responses when\n   allowed access to a world model. [Figures 3, 4]\n\n## Claim 1\n\n### Figure 2\nA world-model is trained from game trajectories generated by random play or\nPSRO-generated policies with restricted strategy spaces. This is reasonable for\nassessing Claim 1, above, but the results are not as convincing as one would\nhope (a clear trend between diversity of strategy-space samples and accuracy is\nnot established, nor are clear trends regarding accuracy *restricted to the\nstrategies used for training*). Moreover, the provided interpretation lacks\ncoherency: If class imbalance is causing problems, then perhaps simpler game and\nsetting should be used to demonstrate the desired claim.\n\nAs a minor point also regarding Figure 2: The way in which the results are\ncommunicated is somewhat confusing. For example, why use the matrices to\nrepresent combinations of strategies if we only considering symmetric strategy\nprofiles? Just give sets of strategies, e.g., $\\\\{1\\\\}, \\\\{2\\\\}, \\\\{1, 2\\\\}, \\\\{2, 3\\\\},\n\\\\{1, 3\\\\}, \\\\{1, 2, 3\\\\}$.\n\n## Claim 2\nThe experiments that address Claim 2, above attempt to do so by considering two\nforms of \"planning\". The motivation is that the use of world-models can benefit\nthe selection of best-responses to estimated payoff matrices and restricted\nstrategy profiles (ie, empirical games), where the world-model is used to train\na reinforcement learning model to select best-responses.\n\n### Figure 3\nA pre-trained world-model (learned from samples with a restricted strategy\nprofile) is used to pre-train an RL model for best-response in a \"planning\"\nphase, and this RL model is compared to a non-pre-trained baseline during real\nplay (Real play involves an opponent using a strategy omitted from the data seen\nby the pre-trained world model). This experiment is reasonably constructed and\njustifies the claim that the world model benefits the learned RL best-response\nmodel through background planning.\n\n### Figure 4\nI do not understand the distinction between Decision-Time planning as used in\nthis paper and Markov-Chain Monte-Carlo (MCMC) to obtain better estimates of the\nvalue function. Ostensibly, the RL model used for best-response planning is\nalready deploying the maximum-estimated-value action, no? In any case, the\nfigure appears to suggest that Decision-Time planning alone markedly *degrades*\nperformance, but this is ignored in the text. I do not see justification for the\nclaim that \"world models offer the potential to improve response calculation\nthrough DT planning,\" based on this figure. How does the figure support this claim?\n\nAs a minor point regarding Figure 4, the x-axis and alignment of the loss curves\non the right panel should be adjusted to agree with the analogous presentation\nin Figure 3, as is done in the left panel.\n\n## Main Experiments\nWhile a marked improvement over PRSO, Dyna-PRSO could be compared against\nadditional baselines in an ablation study --- i.e., against models that\nincorporate only the world model or only incorporate the \"Dyna\" RL component to\nsolve the empirical games component --- to more rigorously establish the claims\nof the paper."
                },
                "questions": {
                    "value": "Please address the questions regarding Figure 4, asked above, regarding\n- The difference between DT and MCMC.\n- How Figure 4 supports the claim that DT improves response calculation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3944/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3944/Reviewer_Y4nV",
                        "ICLR.cc/2024/Conference/Submission3944/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3944/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794796895,
            "cdate": 1698794796895,
            "tmdate": 1700162417027,
            "mdate": 1700162417027,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4rfh88eNZ6",
                "forum": "TyZhiK6fDf",
                "replyto": "cPMIcmznxH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3944/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to review our manuscript. Below we have responded to the individual comments or questions that you have raised throughout your review.\n\n### Summary\nBased on this summary, the reviewer may be assuming an overly narrow view of the use of empirical games. They write that empirical games are \u201cgame models from which the best response is inferred from strategy profiles of other players and estimated payoff matrices\u201d. Empirical games are estimated models of the game of interest and may be studied and analyzed as one may do with a standard game. The PSRO algorithm uses empirical games to compute solutions (e.g., Nash equilibrium), both to drive strategy exploration and as a final result. (Best-response inference in PSRO is actually performed using dRL, outside the game model.)\n\n### Exposition\nThe reviewer then goes on to suggest improving the exposition of the text by associating with each experiment an explicit hypothesis. These hypotheses were laid in the abstract: \u201cWe investigate the potential gain from co-learning these elements: a world model for dynamics and an empirical game for strategic interactions. Empirical games drive world models toward a broader consideration of possible game dynamics induced by a diversity of strategy profiles. Conversely, world models guide empirical games to efficiently discover new strategies through planning.\u201d It seems to us unfair to suggest that a reader must reverse engineer the hypotheses of the paper. We agree that we could and should more clearly and centrally state the hypotheses, and collate them better with experimental evidence. \n\n### Claim 1\n> the results are not as convincing as one would hope (a clear trend between diversity of strategy-space samples and accuracy is not established, nor are clear trends regarding accuracy restricted to the strategies used for training).\n\nThe results may not be as strong as we would have preferred, but we argue they are sufficiently positive to validate this as a promising direction of work. We found several trends:\nIncluding the random policy negatively impacted observation accuracy.\nIncluding the random policy positively impacted reward recall.\nThe inclusion of more policies correspondingly reduces the observation loss (cross-entropy).\n3.64\u00b10.24, 5.33\u00b10.89, 4.25\u00b10.75, 1.99\u00b10.23, 3.76\u00b10.67, 1.83\u00b10.24, 1.45\u00b10.20 (following the order in the paper). \nWe will include this additional result in the appendix.\nMore importantly, this experiment offered insights into world model training, which we could deploy within Dyna-PSRO. \n\n> If class imbalance is causing problems, then perhaps simpler game and setting should be used to demonstrate the desired claim.\n\nWe agree that had we chosen to study simpler games or force artificial diversity into our set of policies (see reviewer c2GE) that we could construct overwhelmingly positive results. We chose not to do this because we suspect that its results would not generalize to the practical cases of interest. \n\n\n### Claim 2\n> I do not understand the distinction between Decision-Time planning as used in this paper and Markov-Chain Monte-Carlo (MCMC) to obtain better estimates of the value function.\n\nDecision-time planning is a classical reinforcement learning technique where planning is focused on a specific state to inform its action selection. Within this specific work, we take a very vanilla approach and do a width-and-depth-limited lookahead search. Given some candidate state s, we sample k actions, and use predict successor states/rewards for each of these k actions. Note that we\u2019re not sampling stochastically from the transition dynamics in this work as we\u2019re assuming a deterministic world model. We repeat this process for a fixed depth. The estimate of the action values from the original candidate state is then the maximum reward + bootstrap (using the value function to estimate the continued return from the final state in the search process) of each subtree of the search process. \n\nThere are a number of ways that this differs from MCMC. The first is, that we are assuming deterministic transitions so have no need to sample over transition spaces. The second is that we do sample the full return in our planning procedure as one would with a Monte Carlo method, and instead leverage our value function to bootstrap this estimate. \n\nIt is possible to use Monte Carlo methods for Decision-Time planning in Dyna-PSRO for correspondingly compatible classes of games. The benefit of using the world model is its predicted partial returns should offer a more accurate estimate of the value function. This does assume inaccuracies in the value function and corresponding accuracies in the world model."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699828609237,
                "cdate": 1699828609237,
                "tmdate": 1699828609237,
                "mdate": 1699828609237,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xb9hMb9RzE",
                "forum": "TyZhiK6fDf",
                "replyto": "cPMIcmznxH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3944/Reviewer_Y4nV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3944/Reviewer_Y4nV"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the detailed reply to my review.\n\nThank you for clarifying better that PSRO solves for Nash equilibria using dRL\noutside the game model, and I retract my criticism that a more thorough ablation\nstudy was not performed in the main experiment.\n\nMy chief criticism was not the overarching hypotheses of the paper, but rather\nthat it was not immediately obvious to me how each experiment's outcomes\nsupported the overarching hypotheses. That is, the overarching hypothesis should\nprovide a clear expectation for what results from the experiments should be (the\nexperiment-specific hypothesis), which can be quickly compared to the actual\nresults. As it is, I find the connection between the experimental results and\nthe core hypothesis to be somewhat convoluted in presentation.\n\nOverall, I am happy to improve my rating for contribution from 2 to 3, and my\noverall rating from 3 to 5.\n\nI will respond to the provided feedback below.\n\n### Figure 2\n\n> The random policy negatively impacted observation accuracy.\n> Including the random policy positively impacted reward recall.\n\nI now better understand the results communicated by Figure 2. I still find the\nway these results are presented to be lacking in simplicity and clarity, but I\nacknowledege that they are stronger than I initially realized.\n\n## Claim 2\n\n> We\u2019re not sampling stochastically from the transition dynamics in this work as we\u2019re assuming a deterministic world model.\n\nThank you for clarifying.\n\n## Figure 4\n\n> The caveat of this result, as discussed in the paper, is that this background planning should be used in conjunction with decision-time planning.\n\nLet me know if this analogy is apt: DT is like liquid soap, and background-planning is like water.\n\nIn a study to determine the most effective way to wash one's hands, we compare\n+ rubbing dry hands together.\n+ rubbing hands together under water.\n+ rubbing hands together, with soap on them, under water.\n+ We don't consider the use of liquid soap without water.\n\nRather make the claim that \"multi-component liquid use is unlikely to harm\nhand-washing, and may help help when applied effectively\", it would be far more\nprecise to claim \"Results indicate that water alone can be more effective at\ncleaning hands than dry rubbing, while soap used with water does the best.\"\n\nIt's not that the claim you've made is wrong, per-se, but it is rather weak and,\nagain, strains the relationship between the experimental results and the central\nhypotheses / results you communicate."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700162488591,
                "cdate": 1700162488591,
                "tmdate": 1700162488591,
                "mdate": 1700162488591,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eQ8kYU6xNu",
            "forum": "TyZhiK6fDf",
            "replyto": "TyZhiK6fDf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3944/Reviewer_qJSG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3944/Reviewer_qJSG"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors consider both world dynamics and strategic interactions among agents when considering strategies for games. They explore the benefits of co-learning world models for dynamics and empirical games for strategic interactions. The authors introduces a new algorithm called Dyna-PSRO, which combines these two elements and demonstrates better performance compared to a baseline algorithm (PSRO) in partially observable general-sum games, particularly in terms of lower regret and fewer required experiences. Their approach proves advantageous in scenarios where collecting player-game interaction data is costly."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, I think the idea of exploring the combined training using empirical games and world models  is an interesting idea. The paper is well-written and the design of the experiment seems very reasonable in terms of three different games."
                },
                "weaknesses": {
                    "value": "This is probably the theoretician in me, but it would be nice to see some theoretical guarantees measuring how much of an improvement Dyna-PSRO is over PSRO. I understand that this might be difficult to obtain given that it might be game-dependent."
                },
                "questions": {
                    "value": "Q1) I see you selected two versions of Harvest and Running with Scissors for testing. Is there any reason behind these choices? \n\nQ2) This is an honest question: Were you surprised by the results? By combining the two (empirical game and world model) you would expect that the training should be no worse, no? However, the paper measures, at least empirically, how much does combining the two improves the SumRegret and the return?\n\nQ3) Is there anything that can be said about how many less experiences are needed in the Dyna-PSRO vs. the standard PSRO?\n\nQ4) What are the specifications of the system used to run the experiments?\n\n\nMinor typos/comments\n\nI'm assuming NE in the first paragraph of Section 2 refers to Nash equilibria, but it should be spelled out once for the sake of completeness.\n\nSection 3 could be spelled out a bit better in terms of definitions. There are some variables undefined like $\\Delta(\\mathcal{R})$ and $\\mathcal{A})$ which should be spelled out for the sake of completeness, though I know they are of common use."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3944/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698906291669,
            "cdate": 1698906291669,
            "tmdate": 1699636355179,
            "mdate": 1699636355179,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "huohui6zwy",
                "forum": "TyZhiK6fDf",
                "replyto": "eQ8kYU6xNu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3944/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review and your kind words including that the paper is \u201cwell-written\u201d and that the \u201cdesign of the experiments seems very reasonable\u201d. We address your individual comments and questions below.\n\n> \u201c... it would be nice to see some theoretical guarantees \u2026\u201d\n\nWe agree that theoretical results would be advantageous. However, due to the complexity of studying games in general, we were unable to construct any bounds that improved upon PSRO. This is primarily due to the convergence of learning-based game-solving algorithms hinging on using epsilon best-response subroutines. In idealized settings, one could construct games where model-based RL (Dyna-PSRO) is guaranteed to produce a smaller epsilon BR than model-free (PSRO). However, it\u2019s not understood how these small epsilon errors contribute to the overall algorithm's long-term progress toward game-solving. This is because small epsilon noise in the best response can help PSRO explore new policies in a way that could further improve the empirical game. We agree that this is a super interesting research direction, and any progress toward it would be a great advancement. \n\n**Q1:** \u201cHow did you select the games for evaluation?\u201d\n\nWe selected these games for several reasons. The four reasons behind choosing Harvest were: (1) we had access to both simple (categorical) and more complex (RGB images) variants of the game allowing for in-depth analysis, (2) its popularity in the field of multiagent reinforcement learning, (3) we had experiences running experiments on the game, and (4) it is general-sum meaning it represents an important class of games. As for Running With Scissors, we both similarly had experience training agents in this game as well as it being a strictly competitive (zero-sum) non-transitive game. Thereby, representing a different and important class of games than Harvest.\n\n**Q2:** \u201cWere you surprised by your results?\u201d\n\nWe were not generally surprised that Dyna-PSRO outperformed PSRO, because including transfer learning should at least preserve performance. It is possible for transfer learning to hurt performance, but because we had studied the co-benefit problems independently and extensively before deploying them in Dyna-PSRO, we had confidence that it was unlikely to hurt performance in the worst case. We were, however, surprised by how large of an improvement that was possible without employing more advanced planning algorithms. \n\n**Q3:** \u201cIs there anything that can be said about how many fewer experiences are needed in the Dyna-PSRO vs. the standard PSRO?\u201d\n\nThis is a really good question and one we\u2019re still thinking about. As we mentioned in Q1, the community currently doesn\u2019t fully understand the relationship between the best-response quality and how many rounds of empirical game expansion are required. As a result, only last-iterate convergence guarantees could be readily made and they would be straightforward adoptions of guarantees from the model-based reinforcement learning literature (requiring strong assumptions about the nature of the game). Speculating, we would expect games with simpler dynamics and a larger strategic space to benefit more from Dyna-PSRO. However, games with exceptionally complex dynamics may see little benefit as learning an effective world model is difficult. \n\n**Q4:** \u201cSpecifications of the systems used in the experiments?\u201d\n\nIn Appendix B we specify the compute settings for our experiments. Copied below for convenience. \u201cGPUs are used for training world models, and policies within Dyna-PSRO. Two types of GPUs were used throughout this work interchangeably: TITAN X and GTX 1080 Ti. All other computation was completed using CPUs. Each response calculation had additional CPUs corresponding to the number of experience generation arenas described in Appendix C. Experiments were run on internal clusters\u201d\n\nThank you for the editorial suggestions, we agree that fully defining these terms will help with the completeness and clarity of the work. We will incorporate these changes in the final version of the paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699828517050,
                "cdate": 1699828517050,
                "tmdate": 1699829380729,
                "mdate": 1699829380729,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fQ0JvAsrIa",
                "forum": "TyZhiK6fDf",
                "replyto": "huohui6zwy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3944/Reviewer_qJSG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3944/Reviewer_qJSG"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my concerns."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700154550822,
                "cdate": 1700154550822,
                "tmdate": 1700154550822,
                "mdate": 1700154550822,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NqKbPlkW4Q",
            "forum": "TyZhiK6fDf",
            "replyto": "TyZhiK6fDf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3944/Reviewer_sm74"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3944/Reviewer_sm74"
            ],
            "content": {
                "summary": {
                    "value": "In this paper the authors present an approach to simultaneously learn a world model and empirical game. The basic concept is that the empirical game benefits the world model by ensuring more diverse training data and the world model benefits the empirical game by allowing for simulated planning. Their results demonstrate that their combined approach can outperform approaches that only learn an empirical game."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is quite original. To the best of my knowledge it represents the first major effort to combine world model and empirical game learning. The quality of the work in terms of describing the approach itself and the evaluation setup is also sufficient. There are no issues with the clarity of the paper in terms of explaining the core of the approach, though some details could be improved. Finally, in terms of significance, the paper can clearly improve the performance of agents attempting to play multiplayer games of a certain type, which is likely to be of interest to researchers working on empirical games or multiagent settings generally."
                },
                "weaknesses": {
                    "value": "The current paper draft has one minor weakness and one major weakness from my perspective. \n\nThe minor weakness is the relative lack of clarity on the approach in the paper itself. The appendix implementation details help but there's still some things that are unclear. As an example of what I mean, the authors state: \"This schedule starts out training as a variation of teacher forcing, and slowly transitions to fully auto-regressive.\" It's clear from this statement what is happening at a high level, but not the exact setup for how the models are trained. This kind of imprecision is unfortunately common in the current draft concerning the authors' approach. \n\nThe major weakness(es) of the paper are the results. Specifically, they do not fully seem to support the authors' stated claims. Section 3.1 seems to actually contradict the claims that empirical game learning can benefit world model learning, since the approach is outperformed in terms of reward prediction by a random sampling approach. Further, the games that are employed throughout the paper are rather simple in comparison to the common games used to evaluate world models (e.g. Pong, Doom, Pacman, Cheetah Run, etc.). I understand that the authors are focused on multiagent settings, but it might have been helpful to construct a setting more like a traditional world model environment, which would have also allowed for a comparison against a World Model baseline. As it is, my takeaway from the results are that world models benefit empirical games but not necessarily the other way around. This is still a contribution, but a more limited one with less general significance. It also does not match authors' stated claims and contributions."
                },
                "questions": {
                    "value": "1. What is the process from start-to-finish of training your approach (at a high level)? \n2. Do the authors disagree with my interpretation that the results do not support that empirical games benefit world models? If so, why?\n3. Why not employ a more complex evaluation game?\n4. Why not include a world model baseline?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3944/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3944/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3944/Reviewer_sm74"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3944/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699050010554,
            "cdate": 1699050010554,
            "tmdate": 1699982511540,
            "mdate": 1699982511540,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0v74i3Crhl",
                "forum": "TyZhiK6fDf",
                "replyto": "NqKbPlkW4Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3944/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your kind words on the originality, quality, and interest of our work. We address your comments and questions below (paraphrased).\n\n> \u201cLack of clarity in the approach. For example, action-conditioned scheduled sampling.\u201d / Question 1\n\nWe apologize for the lack of clarity and would be more than happy to elaborate on any specific details that any reviewer found unclear. We agree that this is an area for improvement in the manuscript as the algorithms being used are complex and challenging to convey in limited pages. Despite this, we hope that our effort to include pseudocode, hyperparameter tables, algorithmic descriptions, graphical descriptions, and a promise to release code help alleviate these concerns in the long-term. We will continue to work on improving the clarity of the text as suggested.\n\nIn regards to action-conditioned schedule sampling, we believe that clarity may have been compromised due to separating details across sections C.2.1 and C.2.2 and to a missing high-level description of the World Model training process. Thank you for pointing out this missing detail. The world model is trained as a sequence-to-sequence supervised learning problem. This motivated our analogies to \u201cteacher forcing\u201d and \u201cauto-regressive\u201d training regimes. To expand, as requested in Question 1, we train our world models in the strategic diversity section on a fixed dataset. For each gradient step, we sample a mini-batch of sub-trajectories of length 20. Part (5 timesteps) of the sub-trajectories are used to burn-in/initialize the recurrent state of the world model. Notably, this means that we\u2019re not performing backpropagation through these timesteps. For the remaining timesteps, we perform action-conditioned scheduled sampling (C.2.1, Algorithm 1) to train our model. Essentially, for the remaining timesteps we take the previous world model's predictions and randomly feed them as input to the model (auto-regressively) as opposed to using the true next timestep from the dataset. We define an epsilon-schedule (C.2.2) that describes how this random sampling occurs throughout training. At the beginning of training, we are performing teacher forcing: giving the world model ground truth inputs as it predicts the full sub-trajectory. After many updates, we start randomly not using ground-trough timesteps within a trajectory, but instead feed in the models previous output as input. By the end of training we are only giving the world model the burn-in timesteps and all inputs are auto-regressive. This allows the world model to learn to make accurate predictions given any artifacts induced by the world model. We will update the text of the paper to make this all clearer.\n\n> \u201cResults do not fully seem to support the authors\u2019 stated claims.\u201d / Question 2\n\nIt is not accurate to say that \u201crandom sampling approach[es]\u201d would outperform any of the world models we trained. The class imbalance in rewards, favoring unrewarding states, means that random sampling would actually perform quite poorly in terms of reward-prediction accuracy. It is true, however, that predicting only the most frequent class, 0 reward, will register as having a very strong reward-prediction accuracy. This doesn\u2019t indicate a problem with our trained world models, but with using accuracy to evaluate the quality of world models. These metrics don\u2019t measure their effectiveness in the downstream task of interest: planning. We briefly mention this in Section 3.1\u2019s discussion, but the nuance is omitted due to paper length requirements. As a stopgap to measuring planning performance in isolation, we considered further metrics such as recall and saw small positive benefits. Furthermore, in Section 3.2 and the appendix we perform planning experiments with both the {1,2,3} world model and the {1} world model. When compared (background: Fig3 v Fig16, decision-time: Fig4 v Fig19), we found that the more strategically diverse world model led to much more successful policy training. Moreover, if you examine the world models observation cross-entropy loss across the same diversity order in the paper you observe: 3.64\u00b10.24, 5.33\u00b10.89, 4.25\u00b10.75, 1.99\u00b10.23, 3.76\u00b10.67, 1.83\u00b10.24, 1.45\u00b10.20. This is another instance of seeing a positive correlation between strategic diversity in world model performance. These arguments do not directly appear in the body of the paper and we will revise the manuscript to make these results clear. In total, we believe that all of this evidence adequately supports the strategic diversity claim in the paper and that empirical games benefit world models.  We would also like to stress that the strategic diversity result is meant only as a point of novelty (as mentioned in the introduction), and is not meant to be a major claim or contribution of the paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699828407423,
                "cdate": 1699828407423,
                "tmdate": 1699829068515,
                "mdate": 1699829068515,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7TntmIzsJj",
                "forum": "TyZhiK6fDf",
                "replyto": "0v74i3Crhl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3944/Reviewer_sm74"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3944/Reviewer_sm74"
                ],
                "content": {
                    "title": {
                        "value": "Re: Official Comment by Authors Q1+Q2"
                    },
                    "comment": {
                        "value": "Thanks for your detailed response to Q1. That helped improve my understanding of the approach considerably, and I think this sort of walkthrough would benefit the paper. \n\nFor Q2, I apologize for oversimplifying the situation. I understand that the proposed approach outperforms in terms of rewards prediction. I also definitely agree that there seems to be a strong improvement in terms of decision-time planning. However, the background results seem fairly comparable to me. I definitely commiserate over the difficulty of finding good metrics for this problem, but overall it seems to me that while the proposed approach improves the decision-time planning performance, it does not necessarily lead to \"a broader consideration of possible game dynamics\". Thus I still would argue that some of the claims are not supported."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699981970767,
                "cdate": 1699981970767,
                "tmdate": 1699981970767,
                "mdate": 1699981970767,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KqbArXE8Ng",
                "forum": "TyZhiK6fDf",
                "replyto": "6UURXgdBPm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3944/Reviewer_sm74"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3944/Reviewer_sm74"
                ],
                "content": {
                    "title": {
                        "value": "Re: Official Comment by Authors Q3+Q4"
                    },
                    "comment": {
                        "value": "For the Q3 response, I would disagree that these games are not partial observable. This is clearly true of Pacman as modelled by NVIDIA's GameGan, but is also true of the common VizDoom and Pong environments due to stochastic elements (enemy movement, spawning, etc.). I definitely agree that the partially observable elements are more complex in the paper's environments than VizDoom and Pong, but the comparison to Pacman is less clear. Beyond the partial observability issue, I'd be curious to see how the proposed approach models complex observable dynamics present in these games. This would help determine the generalizability of the approach. Is the approach only relevant to games with complex partially observable dynamics or to other games?\n\nFor Q4, the speculation in the answer is well-grounded, but still speculation. Particularly given the results comparing the random baseline to the proposed approach for background planning and accuracy, a more traditional world model would have been an appropriate additional baseline to help further clarify the results. \n\nOverall, given the change in Q1 I will update my recommendation, but only by one step as I feel there are still issues in Q2-Q4."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699982472231,
                "cdate": 1699982472231,
                "tmdate": 1699982472231,
                "mdate": 1699982472231,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "d1OKFkgFeF",
            "forum": "TyZhiK6fDf",
            "replyto": "TyZhiK6fDf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3944/Reviewer_SAe1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3944/Reviewer_SAe1"
            ],
            "content": {
                "summary": {
                    "value": "The authors provide a method for simultaneously learning world models (i.e. models of the transition dynamics) and empirical games (i.e. estimates of the per-player payoff). The main benefits of this are twofold: (1) by incorporating PSRO-style policy generation, one obtains a more diverse range of strategy dynamics, which then leads to more diverse data on which to train the world model and (2) by reusing a single learned world model within the PSRO training loop, one can achieve greater efficiency in subsequent generations of PSRO. The resulting algorithm (Dyna-PSRO) is shown to outperform PSRO in terms of the sum of regret against all policies generated by either method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The high-level motivation for this work is good. Given the recent success of world-modelling in single-agent RL (e.g. the Dreamer series), one might well imagine that world modelling can be used to good advantage for multi-agent RL. Moreover, the strategic diversity offered by PSRO may well lead to better exploration of the state space, thus aiding world model construction. The \"win-win\" here is intuitively sensible, and to my knowledge has not previously been investigated in the literature.\n\n- The related work section is reasonably thorough (although some recent citations are missed, see \"Weaknesses\"). \n\n- The Dyna-PSRO algorithm is well-described and the results in Figure 5 are reasonably convincing, with some caveats in the \"Weaknesses\" section below."
                },
                "weaknesses": {
                    "value": "- I find it hard to understand why PSRO is a reasonable algorithm for the Harvest game, and the authors do not provide a strong argument or qualitative evidence here. At a high-level, the Harvest game reduces to a social dilemma, and therefore the Nash solution is not the desirable one (for it means that everyone will defect at the resources will be exhausted, leading to low individual and collective return). I suspect that PSRO and Dyna-PSRO are finding a variety of different solutions that succeed in defection. Indeed, low regret against exploitative opponents will exactly correspond to defection. Instead, what one should be looking for in this context are policies which incentivize others to cooperate (along the lines of opponent modelling, for instance). Now, world modelling does, in principle, help here (see this recent paper: https://arxiv.org/pdf/2305.11358.pdf). But without clear measurements of the individual return, collective return and qualitative analysis of the policies, the reader cannot judge whether the world model yields better or worse outcomes in Harvest. This leads to a key question:\n\n(*) Do the PSRO or Dyna-PSRO agents find cooperative solutions in Harvest? If not (as the case seems to be from the discussion on page 5), what is the argument for using PSRO / Dyna-PSRO in this environment? \n\n- The order in which the paper is presented is confusing. The main algorithm, Dyna-PSRO, is not introduced until very late in the paper. Since this is the main result, the authors would do better to introduce this first, and then present the additional sections as ablations or analyses. \n\n- The results do not compare to existing strong baselines. For instance there are many existing papers that produce agents with good performance on Harvest and Running with Scissors (e.g. https://arxiv.org/pdf/1906.01470.pdf, https://arxiv.org/pdf/2102.02274.pdf, https://arxiv.org/abs/1803.08884). To what extent does this new method outperform the baselines? \n\n- The results are very hard to interpret. In Figure 5, why do the Dyna-PSRO curves terminate before the PSRO curves? In Figures 3 and 4, what is the difference between the left-hand plot and the right-hand plot? The Figure captions in Section 3.2 require significant clarification, because it is extremely hard for the reader to assess the results in this section at present. \n\n- There are several choices in the \"strategic diversity\" section which seem arbitrary and for which the authors have not provided motivation. For instance, why are two PSRO policies used? What is meant by \"the PSRO policies were arbitrarily sampled\"? Why are the PSRO policies subsampled, and what is meant by this? Why is sampling a different policy from PSRO a good test of generalization (as opposed to having a held-out policy for generalization trained with a method from the literature, which would seem like a better test, in my view)? \n\n- There are some missing citations e.g. to the Dreamer line of work (https://arxiv.org/pdf/2301.04104v1.pdf and citations therein), the MuZero line of work (https://arxiv.org/abs/2111.00210 and citations therein), and the aforementioned paper on world modelling in the Harvest game (https://arxiv.org/pdf/2305.11358.pdf)."
                },
                "questions": {
                    "value": "See \"Weaknesses\". \n\nOverall, I think that the motivation for the paper is strong and the Dyna-PSRO algorithm has merit. However, the authors must be more careful to measure the outcomes of social dilemma environments in terms of metrics that make intuitive sense (e.g. collective return) rather than simply measuring deviation from Nash. I hope that they are able to take on board the feedback above to improve the paper, and thus to rigorously demonstrate the benefits of world modelling in PSRO over existing baselines for both zero-sum and general-sum interactions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3944/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699219074087,
            "cdate": 1699219074087,
            "tmdate": 1699636354974,
            "mdate": 1699636354974,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SRzveImW9k",
                "forum": "TyZhiK6fDf",
                "replyto": "d1OKFkgFeF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3944/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to review our work. We are glad to hear that you found the motivation good, the algorithms well-described, and the results reasonably convincing. We address your specific comments and questions below (paraphrased). \n\n> \u201cDo PSRO or Dyna-PSRO find cooperative solutions in Harvest? If not, why use them?\u201d\n\nOur perspective is that PSRO is a method for finding solutions (e.g., Nash equilibria) for games, and we evaluate our methods based on their computational performance in finding a game solution. In some settings one might prefer to find some solutions (e.g., \u201ccooperative\u201d or high welfare) rather than others; how to address that in PSRO is a topic of much current research but not what we address in this paper. We would argue that being able to find any solution of a complex game is often quite interesting and valuable.\n\nActually, any equilibrium solution for Harvest will balance cooperative and competitive elements, and we do (anecdotally) observe this in the solutions found by our algorithms. This was qualitatively observed as the agents established a pattern of harvesting the orchard in a circular pattern. Moreover, we found it rare that a policy would take the competitive \u201ctag\u201d action due to it being difficult to effectively learn. We agree that by mentioning in section 3.1 that the \u201cPSRO policies are highly competitive, tending to over-harvest,\u201d  may be misleading. This discussion comment was meant only to provide some intuitive insight about this very small subsample of policy behavior, and not commentary about solutions found by the full game-solving algorithm (PSRO or Dyna-PSRO). \n\nAn interesting question to explore in the future is how alternative solution concepts or MSSs affect the advantages and disadvantages of PSRO and Dyna-PSRO.\n\n> \u201cOrganization of the paper is confusing.\u201d\n\nWe chose to organize the paper in this way because we view section 3, which contains the referred \u201cablations or analyses\u201d, as a methods section that is essential to understanding any presented results with respect to Dyna-PSRO. The experimental details found within section 3 are directly used by section 4, and any results on Dyna-PSRO would be impossible to interpret without those details.\n\n> \u201cDoes not compare to existing baselines.\u201d\n\nThe reviewer is correct to note that there exist many works that have studied these games as they appear in MeltingPot. What separates this line of work from them is their goal: the cited work focuses on primarily either studying the game itself or on simultaneously learning policies. Their primary interest isn\u2019t in solving the game for any specific solution (e.g., Nash). To this end, these works likely have extremely strong policies in specific strategic settings, that may perform poorly with other coplayers (i.e., against policies that they were not trained with). This is not because either method is better or worse, but because the goals of these works are different. The PSRO line of work is interested in developing general game-solving algorithms that are notably flexible on solution concepts. Therefore, we think making direction comparisons is unclear for such disparate algorithms, and unfair as they are solving meaningfully different problems.\n\n> In Figure 5, why do the Dyna-PSRO curves terminate before the PSRO curves?\n\nThis is because Dyna-PSRO has a longer walltime than PSRO, so we were more limited in how long we were able to run the algorithm. Note that in \u201cHarvest: Categorical\u201d Dyna-PSRO has converged on the x-axis, and that in the other two games, Dyna-PSRO outperforms the other two games when PSRO is allowed to run for many more iterations. \n\n> In Figures 3 and 4, what is the difference between the left-hand plot and the right-hand plot?\n\nThe difference between the two plots is in the x-axis. The left-hand plot measures each learning algorithm's performance against  \u201cReal Experiences\u201d used during training; whereas, the right-hand plot measures against both \u201cReal and Planned Experiences.\u201d These terms are introduced in Section 3.2.1 (paragraph one, bolded terms), where real experiences are data from interaction with the real game, and planned experiences are data generated from interaction with the world model. The left-hand plot shows the comparison of learners against the objective of interest since we\u2019re assuming generating real experiences is the bottleneck in game solving. The right-hand plot offers additional insight into the respective algorithms by further comparing them by the amount of additional planned experiences they use. This offers insights into the training dynamics of these learners, and how they tend to take more wall time (as they need many more gradient steps) to run than the baseline methods. We will update the figure captions in this section to make this more clear."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699828293804,
                "cdate": 1699828293804,
                "tmdate": 1699828820991,
                "mdate": 1699828820991,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BgpMiSDtDu",
                "forum": "TyZhiK6fDf",
                "replyto": "d1OKFkgFeF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3944/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> \u201cHow did the authors arrive at their methodological choices in the strategic diversity section?\u201d\n\n  a) Why are two PSRO policies used? Because each additional policy resulted in combinatorially more results and compute requirements to perform. \n\n  b) What is meant by \u201cthe PSRO policies were arbitrarily sampled\u201d? We randomly sampled, without replacement, the policies from the set of policies generated by PSRO. \n\n  c) Why are the PSRO policies subsampled and what is meant by this? From the three policies, per seed, we train our world models on, we take different subsamples of them to train different world models. For example, subsampling only the first policy {1}, or the first and third policy {1, 3}. This was done to show the impact of having more policies contribute to the training data of a world model.\n\n  d) Why is sampling a different policy from PSRO a good test of generalization? It\u2019s unclear whether the specific nature of the learning algorithm that is used to generate the held-out policy is meaningful for this test of generalization. The more pertinent question is how different is the held-out policy from the ones in the train set. In our rebuttal to reviewer c2GE we provide data showing the similarity between the policies, which we believe justifies our choice in using another PSRO policy as a test for generalization. \n\n> \u201cMissing citations\u201d\n\nWe will add the suggested citations to the final version of the manuscript, thank you for the suggestions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699828357473,
                "cdate": 1699828357473,
                "tmdate": 1699883628829,
                "mdate": 1699883628829,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wL3ACRq5pi",
                "forum": "TyZhiK6fDf",
                "replyto": "BgpMiSDtDu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3944/Reviewer_SAe1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3944/Reviewer_SAe1"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response. Unfortunately I find it fairly unconvincing for the following reasons:\n\n1. \"Optimising for Nash\". There are very likely a large number of Nash equilibria in this game, treated as a temporally extended interaction. As the folk theorem tells us, subgame perfect Nash equilibria are in fact very plentiful in repeated games. The problem in social dilemmas is not to find an arbitrary Nash equilibrium, but rather to find Nash equilibria which are beneficial for both individuals and the group. So I disagree with the authors that finding a Nash equilibrium is necessarily a useful contribution in a social dilemma. \n\n2. \"Comparison to baselines\". I disagree with the authors that it is scientifically valid to not compare with baselines. If, as the authors claim, their method is better than baselines at cooperating with novel co-players, then they should demonstrate this empirically. On the other hand, if the authors wish to demonstrate that their world modelling method is useful independently of its benefit for improving the state of the art, then they should show this on real-world data. I don't believe that it is valid to both use artificial domains and to not compare against baselines. \n\n3. \"Figures 3 and 4\". Many thanks for your explanation of Figures 3 and 4, this now makes sense. However, it raises the question of why the real-environment reward is so low in the simulated experience training phase, and why the model reward drops at the start of the real environment training phase. This would suggest to me that there was some significant distribution shift between the model and the real environment, which is not analysed in detail. \n\n4. \"Comparison to https://arxiv.org/pdf/2305.11358.pdf\". The authors have not distinguished the ways in which their work has significant impact beyond this prior work to my satisfaction.\n\nI hope that the authors are able to take this feedback on board to improve the paper. If there were able to more clearly articulate and empirically validate their research question, then I believe that a future version of the paper could be impactful. However, I cannot recommend acceptance in its current form."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668717998,
                "cdate": 1700668717998,
                "tmdate": 1700668717998,
                "mdate": 1700668717998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "svr1VemVxC",
                "forum": "TyZhiK6fDf",
                "replyto": "d1OKFkgFeF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3944/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their reply, and continuing to offer suggestions for improving our manuscript. We address the individual questions and comments below. \n\n> Empirical-Game Learning (EGL) Versus Multiagent Reinforcement Learning (MARL)\n\nWe believe the primary point of confusion between us and the reviewer is that they are projecting the problem space of MARL onto our work which is within EGL. In MARL, one is often interested in agents that are simultaneously learning and key problems are how to learn any effective joint policy and what behaviors emerge throughout this process. In EGL, one is interested in modeling an underlying game in a way that affords analytic game reasoning. The primary deliverable is an empirical game that can be solved for as a proxy for the real game. The key problems are how to build the game model such that effectively captures the strategic landscape of the underlying game. The challenges, considerations, and approaches between these two fields is largely disparate. A MARL approach does not produce an artifact that can have game reasoning performed on. Their optimization objectives are different: MARL optimizes for return, and game-solving algorithms optimize for a specified solution concept.  MARL performs only one application of DRL to compute policies, whereas, game-solving algorithms apply _many_ DRL to build a population of policies. This makes their direct comparison unclear. \n\n\n(continues in \"Comparison as baslines\"). \n\n\n> Optimizing for Nash\n\nWe agree with the reviewer that there is often many equilibria, and that high-welfare equilibria are particularly interesting in social dilemma games. Finding high-welfare equilibrium is an equally valid problem, but not the one addressed in this work. The primary interest of this work is singularly to reduce the cost, measured in experiences, of performing learning-based game-solving algorithms. Our solution is notably agnostic to the solution concept of interest, and could readily be applied to high-welfare equilibria. This is why we compare against both Harvest and Running With Scissors, as they represent different game classes of interest (one would not be interested in welfare in Running With Scissors). However, finding _any_ equilibria in these more complex games is an active area of research. Moreover, finding equilibria for _specific_ equilibria such as high welfare equilibria is also an activate area of research, and is orthogonal to this study. Never do we claim to be making a contribution to the analysis of social dilemmas, or attempt to solve for the problem of learning high-welfare equilibrium as the reviewer suggests. \n\n\n> Comparison as baselines\n\nWe do not think it is fair to characterize our rebuttal as saying that \"it is scientifically valid to not compare with baselines\". Our rebuttal points out that the suggested baselines are unfit, because they take a meaningfully different problem. Moreover, our work contains a multitude of baselines that are more in line with the problem studied in this paper: how to effectively use planning to reduce the computational cost of game solving. We believe the reviewer is conflating the multiagent reinforcement learning problem of _learning any joint policy_ with the problem studied here of _general game solving_. For examples of similar game-solving algorithm papers see (small sample here):\n- Lanctot, et al.. A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning. NeurIPS'17\n- Balduzzi, et al.. Open-ended Learning in Symmetric Zero-sum Games. ICML'19.\n- McAleer, et al.. Anytime PSRO for Two-Player Zero-Sum Games. arxiv'22.\n- Marris, et al.. Multi-Agent Training beyond Zero-Sum with Correlated Equilibrium Meta-Solvers. ICML'21.\n- McAleer, et al.. Pipeline PSRO: A Scalable Approach for Finding Approximate Nash Equilibria in Large Games. NeurIPS'20.\n- Yao, et al.. Policy Space Diversity for Non-Transitive Games. arXiv'23.\n- Smith, et al.. Strategic Knowledge Transfer. JMLR'23.\n- Liu, et al.. Towards Unifying Behavioral and Response Diversity for Open-ended learning in Zero-sum games. NeuRIPS'21.\n- McAleer, et al.. XDO: A Double Oracle Algorithm for Extensive-Form Games. NeurIPS'21.\n- Perez-Nieves, et al.. Modelling behavioral diversity for learning in open-ended games. ICML'21.\n\nThese are a sample of papers that similarly to ours tackle the problem of designing learning-based game-solving algorithms. As is standard, these papers similarly do not compare against multiagent reinforcement learning algorithms, because the problem being solved is different. Instead, they compare against their own self-ablations or other game-solving algorithms which are appropriate baselines, as we argued in our rebuttal. This same practice is adopted in our work."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679669389,
                "cdate": 1700679669389,
                "tmdate": 1700679901719,
                "mdate": 1700679901719,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8SFlhZ9sBx",
                "forum": "TyZhiK6fDf",
                "replyto": "d1OKFkgFeF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3944/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3944/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Figures 3 and 4\n\nAs we mention in the paper, there is a significant distribution shift between the model and real environment. This is a well known phenomena in world-models that do not operate in latent state space. We include extensive results in the strategic diversity section highlighting the limitations our of world model, which directly contribute to this point (as discussed in the paper). As errors compounding over time is well known, we known and have communicated the reason for this failure. If there is a particular analysis the reviewer thinks would be useful we are happy to try to compile it.\n\n\n> Comparison with workshop paper\n\nThe suggested paper is concurrent work. It shares two commonalities with our manuscript: (1) its applies model-based reinforcement learning, and (2) it uses Harvest as an evaluation domain. It is **different** in almost every other aspect:\n- Problem statement:\n\t- Workshop: focused specifically on the emergent behavior of agents in social dilemmas. \n\t- Manuscript: focused on the development of general game-solving algorithms\n- Method (high-level):\n\t- Workshop: applies multiagent reinforcement learning to learn a single joint strategy. \n\t- Manuscript: applies empirical game theory to learn an empirical game and solution.\n- Method (low-level):\n\t- Workshop: \n\t\t- Computes policies using concurrent learners.\n\t\t- Uses online reinforcement learning.\n\t\t- Uses latent world models.\n\t- Manuscript:\n\t\t- Computes an _empirical game_, using RL as a subroutine with only ever one concurrent learner.\n\t\t- Uses offline reinforcement learning.\n\t\t- Uses a non-latent world model.\n\n\n> \"more clearly articulate and empirically validate their research question\"\n\nThe key research question addressed in this paper is to reduce the computational cost of learning-based game-solving algorithms through transferring learned world knowledge. We also consider as a minor point the reciprocal benefit of game-solving on learning world models. The major point of the research is shown in Figure 5, and the minor points in Figure 2-4. This is summarized in the last paragraph of the introduction copied here: \"The main points of **novelty** of this paper are as follows: (1) empirically demonstrate that world models benefit from the strategic diversity induced by an empirical game; (2) empirically demonstrate that a world model can be effectively transferred and used in planning with new other-players. The **major contribution** of this work is a new algorithm, Dyna-PSRO, that colearns an empirical game and world model finding a stronger solution at less cost than the baseline, PSRO.\""
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3944/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679675573,
                "cdate": 1700679675573,
                "tmdate": 1700680256905,
                "mdate": 1700680256905,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]