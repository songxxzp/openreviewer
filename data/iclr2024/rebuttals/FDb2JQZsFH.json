[
    {
        "title": "Attention-based Iterative Decomposition for Tensor Product Representation"
    },
    {
        "review": {
            "id": "fXuaeHlkeR",
            "forum": "FDb2JQZsFH",
            "replyto": "FDb2JQZsFH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7169/Reviewer_Qfs2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7169/Reviewer_Qfs2"
            ],
            "content": {
                "summary": {
                    "value": "The authors try to apply the Attention mechanism in the tensor product representation models. They also showed that the proposed AID block can be easily incorporated into many existing networks. Experiments show the advantages of introducing the AID block in previous network architectures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors proposed a new Attention based module for TPR. The proposed module can be combined with existing structures such as TPR-RNN, FWM and Linear Transformers.\n2. The authors conducted extensive experiments including ablation studies to show the advantages of the AID module and influences of hyperparameters.\n3. Code for all experiments is provided."
                },
                "weaknesses": {
                    "value": "The authors mentioned that one advantage of TPR is to represent symbolic structures. I am wondering if this was demonstrated in experiments. I am not familiar with these tasks, but I did not find descriptions about this issue in experiments."
                },
                "questions": {
                    "value": "How is the scalability and complexity of the proposed AID module?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7169/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825175589,
            "cdate": 1698825175589,
            "tmdate": 1699636850325,
            "mdate": 1699636850325,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hXAxp9Wr3d",
                "forum": "FDb2JQZsFH",
                "replyto": "fXuaeHlkeR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7169/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their helpful comments. Also, we updated the manuscript to reflect review comments and marked revision with red color. Our responses to your concerns are as follows:\n\n> The authors mentioned that one advantage of TPR is to represent symbolic structures. I am wondering if this was demonstrated in experiments. I am not familiar with these tasks, but I did not find descriptions about this issue in experiments.\n\nTo address your inquiry, we first would like to explain TPR. TPR is a general method that represents the symbolic structure of data in vector spaces. Also, it provides symbolic information through the decoding process. To gain more insight into how TPR represents the symbolic properties of data, let us consider a scenario where two objects (a red rectangle and a blue circle) are presented in a room, and our objective is to determine the circle\u2019s color. Initially, the two objects are encoded into TPR form by superimposing embedding representations for each object, expressed as $T= d_{\\text{red}} \\otimes d_{\\text{rectangle}} + d_{\\text{blue}} \\otimes d_{\\text{circle}}$. Afterward, it decodes the color information of the circle from TPR with an inner product, $T \\cdot d_{\\text{circle}} = d_{\\text{blue}}$. In this example, object colors are *fillers* and object shapes are *roles* and the *unbinding operators* in the TPR components.\n\nBecause of those TPR\u2019s characteristics, the assessment of how accurately the models represent the symbolic structure in the TPR form hinges on their capacity to generate appropriate *role* and *filler* representations. Moreover, the specific task requirements determine these *roles* and *fillers*. To investigate the ability of models to generate appropriately structured representations, we designed a synthetic task named the Systematic Associative Recall task. This task entails the clear mapping of generative factors to specific TPR elements, such as associating $x$ with *role* and $y$ with *filler*. Our quantitative and qualitative analyses in Section 4.3.1 (Originally, Section 3.3.1) reveal that the AID generates structural representations that better conform to TPR conditions than the baseline model.\n\n\n***\n\n> How is the scalability and complexity of the proposed AID module?\n\nThe iterative attention mechanism of the AID enables it to scale well with the number of TPR components. This scalability allows the AID to be adapted to any TPR-based model. However, an increment in the number of components potentially impacts our routing strategy (learning of initial components), closely related to the training stability. Also, since the AID module demands additional computational complexity $\\mathcal{O}( N_\\text{input} N_\\text{com} D_\\text{com} )$ per iteration at each time step, this increment results in a slowdown of the overall model operations."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363683824,
                "cdate": 1700363683824,
                "tmdate": 1700363683824,
                "mdate": 1700363683824,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ybrb6rxqhn",
                "forum": "FDb2JQZsFH",
                "replyto": "hXAxp9Wr3d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7169/Reviewer_Qfs2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7169/Reviewer_Qfs2"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors\u2019 response. I am not familiar with this field. However, this paper presents some interesting applications of TPR and is well written. The experimental results seem valid. Therefore, I will keep my rating and a low confidence level."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703904105,
                "cdate": 1700703904105,
                "tmdate": 1700703904105,
                "mdate": 1700703904105,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "X6PgNqbNZg",
            "forum": "FDb2JQZsFH",
            "replyto": "FDb2JQZsFH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7169/Reviewer_bYLE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7169/Reviewer_bYLE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an Attention-based Iterative Decomposition (AID) module that uses a competitive attention mechanism to decompose sequential input features into structured representations (roles, fillers, and unbinding operators) to improve systematic generalization for Tensor Product Representation (TPR) based models.\nThe AID module is flexible enough to integrate with existing TPR-based models such as TPR-RNN, Fast Weight Memory, and Linear Transformer.\nThe experiments support the improvements, show AID produces more compositional and well-bound structural representations, and exemplify applications with large-scale real-world data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- It is important to decompose sequential input to structured representations for systematic generalization, and the AID module enhances the performances for TPR-based models.\n\n- The module design is simple and clean, so it may be expected to keep the advantage in general cases.\n\n- It integrates with a wide range of TPR-based models in flexible ways."
                },
                "weaknesses": {
                    "value": "(1) The WikiText-103 task shows the AID module performs well in a large-vocabulary language modeling task, but it seems not to be a systematic generalization task."
                },
                "questions": {
                    "value": "(2) Do the intermediate TPR components always keep TPR conditions (the three key conditions required by TPR)?\nFor example, in integrating with TPR-RNN, the input features to the AID module $x_t$ are a set of word vectors, which may be in any form.\nDoes the AID module convert the input features to TPR?\n\n(3) TPR has its properties, such as the separation of roles and fillers.\nDoes the AID module use TPR properties in the module design, e.g., use role for attention key?\n\n(4) Though the AID module is designed to enhance TPR-based models, is it also informative to compare it with non-TPR-based models in experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7169/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829304263,
            "cdate": 1698829304263,
            "tmdate": 1699636850207,
            "mdate": 1699636850207,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sWkPYvI5yt",
                "forum": "FDb2JQZsFH",
                "replyto": "X6PgNqbNZg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7169/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their helpful comments. Also, we updated the manuscript to reflect review comments and marked revision with red color. Our responses to your concerns are as follows:\n\n> (1) The WikiText-103 task shows the AID module performs well in a large-vocabulary language modeling task, but it seems not to be a systematic generalization task.\n\nWe think our description of the purpose of the experiment on the WikiText-103 may not be good enough. We updated the WikiText-103 part of the manuscript for a better illustration of our intention. [First paragraph of Section 4.4 (originally, Section 3.4)]\n\nOur AID achieves larger improvements on various systematic generalization tasks. In addition to these improvements, we extend our evaluation to assess the effectiveness of the AID on a more practical task, the WikiText-103 task, which may not be explicitly designed to evaluate the systematic generalization capability but is a fundamental problem. In this task, we aim to show the effectiveness of the AID on TPR-based models even when the task is not for systematic generalization. The WikiText-103 results show the potential of the AID for enhancing performance even on large-scale real tasks.\n\n***\n\n> (2) Do the intermediate TPR components always keep TPR conditions (the three key conditions required by TPR)? For example, in integrating with TPR-RNN, the input features to the AID module\u00a0are a set of word vectors, which may be in any form. Does the AID module convert the input features to TPR?\n\n(*Do the intermediate TPR components always keep TPR conditions?*) **No, the intermediate TPR components do not always keep TPR conditions.** Given the relationship between accuracy and adherence to TPR conditions, we believe Fig. 5(c) indirectly addresses your inquiry. This case is akin to making predictions based on the intermediate TPR components. Notably, when the number of attention iterations is limited, the outcomes demonstrate lower performance, indicating a potential failure to adhere to TPR conditions.\n\n(*Does the AID module convert the input features to TPR?*) **Yes.** More precisely, the AID module indeed plays a role in converting the input features into structured representations that serve as the constituents of TPR, such as *role* and *filler*.\n\n\n\n***\n\n> (3) TPR has its properties, such as the separation of roles and fillers. Does the AID module use TPR properties in the module design, e.g., use role for attention key?\n\n**Our AID maintains the separation of *roles* and *fillers*, a key TPR property, in the module design.** As you mentioned, TPR operates by explicitly separating the information at the representation level into distinct symbols. The AID links each symbolic meaning to different slots during iterative attention to maintain this representational separation. These slots compete while being updated individually and independently throughout the competitive attention process.\n\n\n***\n\n> (4) Though the AID module is designed to enhance TPR-based models, is it also informative to compare it with non-TPR-based models in experiments?\n\nOur purpose for the comparison to non-TPR-based models is to measure the systematic generalization capability of TPR-based models. We compare TPR-based memory networks (TPR-RNN and FWM) to state-of-the-art memory networks in the bAbI task. The experimental results indicate that TPR-based models show better systematic generalization performance than others. Furthermore, combined with those TPR-based models, our AID not only enhances the systematic generalization capability but also achieves state-of-the-art results."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363628162,
                "cdate": 1700363628162,
                "tmdate": 1700363628162,
                "mdate": 1700363628162,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6fEBoJxK23",
                "forum": "FDb2JQZsFH",
                "replyto": "sWkPYvI5yt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7169/Reviewer_bYLE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7169/Reviewer_bYLE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the answers and updates. I like to keep the score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700376404793,
                "cdate": 1700376404793,
                "tmdate": 1700376404793,
                "mdate": 1700376404793,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vSHk766QK5",
            "forum": "FDb2JQZsFH",
            "replyto": "FDb2JQZsFH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7169/Reviewer_bmNb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7169/Reviewer_bmNb"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to improve Tensor Product Representation (TPR) for systematic generalization tasks. The authors propose an Attention-based Iterative Decomposition (AID) module, which is plug-and-play and can be easily integrated into existing TPR models. AID is conceptually similar to Slot Attention, but with special designs tailored towards the task. Experimental results show that AID consistently improves existing TPR methods across a broad range of tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The considered challenge, roles/fillers decomposition, is indeed very similar to the object binding problem in object-centric learning (OCL). Therefore, it is intuitive to apply the SOTA OCL module Slot Attention here.\n- The experimental evaluations are thorough. AID shows consistent and non-marginal improvement in all the tasks.\n- The ablation and adapted designs from the original Slot Attention are insightful."
                },
                "weaknesses": {
                    "value": "My background is in OCL so I am unfamiliar with these tasks and baselines. One concern I have is all the tasks (except the WikiText-103 one) are very simple. I understand that areas in the early stage experiment on simple data. However, for example for the CLEVR VQA task, people can train a Slot Attention model to extract object-centric features, and then attach a small Transformer head to predict the question's answer. According to my own experience, such a naive baseline can already achieve nearly perfect accuracy (on the original CLEVR dataset, not Sort-of-CLEVR). Therefore, it is hard for me to assess the importance of this paper.\n\nAlso, what is the difference in model size and computation cost of baselines with and without AID? For example, on the WikiText-103 task, the authors mention that they do not insert AID in every layer due to computation concerns. I wonder how will the baselines perform if they have more parameters."
                },
                "questions": {
                    "value": "The Orthogonality Analysis in Sec. 3.1.1 shows that AID also helps extract more orthogonal *roles*. I am curious why this is the case. In my own experience with Slot Attention, the object-centric features (slots) are usually entangled, as there is no loss to force them to be orthogonal. Any insights here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7169/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699051970665,
            "cdate": 1699051970665,
            "tmdate": 1699636850051,
            "mdate": 1699636850051,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YdqY1HCg3l",
                "forum": "FDb2JQZsFH",
                "replyto": "vSHk766QK5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7169/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their helpful comments. Also, we updated the manuscript to reflect review comments and marked revision with red color. Our responses to your concerns are as follows:\n\n> My background is in OCL so I am unfamiliar with these tasks and baselines. One concern I have is all the tasks (except the WikiText-103 one) are very simple \u2026 Therefore, it is hard for me to assess the importance of this paper.\n\nIn response to your inquiry, we highlight that our AID is a general drop-in module that can be adapted to any TPR-based model and enhances its systematic generalization capability in various domains. The AID seems similar to Slot Attention in certain aspects because both employ competitive attention. However, even though Slot Attention can be a solution to solve the CLEVR task, it cannot directly tackle the SAR task, the bAbI task, and the WikiText-103 task. This is likely because Slot Attention is designed to extract visual object representations. In contrast, our AID generates structured representations regardless of the domain. Combined with TPR-based models, it can solve various systematic generalization tasks. In the experiment, we show the effectiveness of the AID to enhance the systematic generalization capability of TPR-based models.\n\n***\n\n> Also, what is the difference in model size and computation cost of baselines with and without AID? For example, on the WikiText-103 task, the authors mention that they do not insert AID in every layer due to computation concerns. I wonder how will the baselines perform if they have more parameters.\n\nWe have detailed the difference in parameter counts and have included our findings with a comparison to more parameterized baselines in Appendix E.\n\nIn response, we conducted experiments with a more parameterized baseline on all the tasks. In the WikiText-103 task, on the advice of Reviewer 8A4n, we increased the size of the feed-forward network in the attention part of the baseline model. The size increase is applied to the exact same positions of the model where AID is adopted, for a fair comparison with our AID-assisted network architecture. In other tasks, we adopted a different methodology, increasing either the hidden or head size of the baseline models. In the Table below, the experimental results indicate that our improvements do not merely come from the number of increased parameters in the models.\n\n| **WikiText-103 task** | Valid | Test | # params |\n| --- | --- | --- | --- |\n| Linear Transfomer | 36.473 | 37.533 | 44.02M |\n| Linear Transfomer (more params) | 36.452 | 37.306 | 44.22M |\n| Linear Transfomer (+ AID) | **36.159** | **37.151** | 44.16M |\n| Delta Network | 35.640 | 36.659 | 44.03M |\n| Delta Network (more params) | 35.468 | 36.639 | 44.23M |\n| Delta Network (+ AID) | **35.361** | **36.253** | 44.18M |\n\n\n| **bAbI task** | $D_\\text{LSTM}$ | *w/o sys diff* | *w/ sys diff* | Gap | # params |\n| --- | --- | --- | --- | --- | --- |\n| FWM | 256 | 0.79 | 2.85 | 2.35 | 0.73 M |\n| FWM (more params) | 512 | 0.75 | 2.16 | 1.41 | 1.89 M |\n| FWM (+ AID) | 256 | **0.45** | **1.21** | **0.76** | 1.23 M |\n\n\n| **Sort-of-CLEVR task** | $N_\\text{heads}$ | $D_\\text{heads}$ | *Unary* | *Binary* | *Ternary* | # params |\n| --- | --- | --- | --- | --- | --- | --- |\n| Linear Transformer | 8 | 32 | 82.5 | 78.3 | 60.0 | 0.68 M |\n| Linear Transformer (+ AID) | 8 | 32 | **98.9** | 78.0 | 61.0 | 0.83 M |\n| --- | --- | --- | --- | --- | --- | --- |\n| Linear Transformer | 4 | 64 | 69.3 | 75.5 | 56.4 | 0.68 M |\n| Linear Transformer (+ AID) | 4 | 64 | **98.9** | **78.6** | **63.7** | 0.83 M |\n| --- | --- | --- | --- | --- | --- | --- |\n| Linear Transformer | 8 | 64 | 57.5 | 59.7 | 53.2 | 2.55 M |\n| --- | --- | --- | --- | --- | --- | --- |\n| Linear Transformer | 4 | 128 | 57.9 | 59.9 | 52.2 | 2.55 M |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363453495,
                "cdate": 1700363453495,
                "tmdate": 1700363453495,
                "mdate": 1700363453495,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qYqxmgYtUF",
                "forum": "FDb2JQZsFH",
                "replyto": "vSHk766QK5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7169/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> The Orthogonality Analysis in Sec. 3.1.1 shows that AID also helps extract more orthogonal\u00a0*roles*. I am curious why this is the case. In my own experience with Slot Attention, the object-centric features (slots) are usually entangled, as there is no loss to force them to be orthogonal. Any insights here?\n\n**The orthogonality is closely associated with the properties of TPR.** TPR operates by explicitly separating the information at the representation level into distinct symbols, such as *role* and *filler*. It is constituted by the tensor product of *roles*\u00a0vectors and *fillers*\u00a0vectors to represent the symbolic structure of data. In the decoding phase, the *roles* act as keys, facilitating the retrieval of associated *fillers* through inner product operations. Because of these TPR operations, TPR-based models should generate orthogonal *roles* to accurately recall *filler* information from their TPR-formed connectionist representation. During training, those models learn these TPR\u2019s properties to perform correct TPR operations in a supervised manner for solving tasks. Our AID is structured to learn the compositional nature of data and generates structural representations that better conform to TPR conditions than the baseline model."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363535972,
                "cdate": 1700363535972,
                "tmdate": 1700363535972,
                "mdate": 1700363535972,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7L8QSDHeue",
                "forum": "FDb2JQZsFH",
                "replyto": "qYqxmgYtUF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7169/Reviewer_bmNb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7169/Reviewer_bmNb"
                ],
                "content": {
                    "title": {
                        "value": "Re: Rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for the rebuttal and additional experiments. However, I still do not see a clear difference on the technical side compared to Slot Attention. I will maintain my current rating of 5. Since I am unfamiliar with the field, I have downgraded my confidence score and let other reviewers decide."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692357926,
                "cdate": 1700692357926,
                "tmdate": 1700692357926,
                "mdate": 1700692357926,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SI1YfgSzBF",
            "forum": "FDb2JQZsFH",
            "replyto": "FDb2JQZsFH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7169/Reviewer_rEt9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7169/Reviewer_rEt9"
            ],
            "content": {
                "summary": {
                    "value": "The work proposes using iterative attention for learning Tensor Product Representations (TPR), meant to improve their systematic generalization capability, as measured through experiments over textual and visual reasoning tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- **Idea**: TPRs and attention fit well together: identifying and extracting the role and filler components seems like a natural application of attention and so the integration between them makes a lot of sense to me.  \n- **Evaluation**: Experiments are conducted on multiple datasets including both textual and visual modalities as well as both synthetic and realistic data (bAbI, Sort-of-CLEVR, WikiText and the Systematic Associative Recall (SAR) task). The experiments investigate using the attention module to extend several related models (TPR-RNNs, Fast Weight Memory, and Linear Transformers). Both quantitative (through e.g. DCI, downstream performance) and a bit of qualitative analysis (visualization of similarity between the representations of the TPR components). Overall these support the approach\u2019s flexibility.\n- **Clarity**: The presentation is good and the paper is clearly written and well-organized. The introduction and model sections do a good job motivating the idea and presenting the necessary background and preliminaries. The overview figure is very helpful. Detailed description is provided for each of the 3 inspected models and the 4 tasks. The supplementary is also good, providing implementation details and ablation experiments."
                },
                "weaknesses": {
                    "value": "- **Novelty**: The iterative attention decomposition works very similarly to slot attention, reducing the technical contribution of the paper. The paper introduces the idea as a novel attention-based module, not making it clear enough that effectively this strongly relies on slot attention. The comparison to slot attention appears only at the very end of the paper. Since the approach integrates together existing ideas, it will make sense in this case that the related work section will appear earlier on, before the model section.\n- **Empirical Results**: The improvements for WikiText (perplexity) and disentanglement (DCI) are relatively low. On the other hand, we see larger improvements on bAbI and Sort-of-CLEVR.   \n- **Related Works**:  A more detailed comparison to the prior related works, in particular to \u201cEnriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization\u201d that also integrates attention and TPRs. It is cited by the paper but more discussions on similarities and differences would be helpful."
                },
                "questions": {
                    "value": "- **Qualitative Evaluation**: It would be particularly useful for this work to have more qualitative evaluation for both bAbI and sort-of-CLEVR. What do the different TPR components actually attend to? Does their behavior make sense over specific instances? What mistakes do they tend to make? What type of mistakes are made by the baselines and eliminated by the new approach? How do they behave over examples with unseen names (systematic generalization cases)? This type of analysis can significantly help in demonstrating the actual impact of integrating attention into TPRs, beyond the overall accuracy metrics.  For bAbI, a more detailed breakdown of the performance by question type or story length will also be helpful."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7169/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699529187466,
            "cdate": 1699529187466,
            "tmdate": 1699636849930,
            "mdate": 1699636849930,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yZCAE8POJd",
                "forum": "FDb2JQZsFH",
                "replyto": "SI1YfgSzBF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7169/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their helpful comments. Also, we updated the manuscript to reflect review comments and marked revision with red color. Our responses to your concerns are as follows:\n\n> **Novelty**: The iterative attention decomposition works very similarly to slot attention, reducing the technical contribution of the paper. The paper introduces the idea as a novel attention-based module, not making it clear enough that effectively this strongly relies on slot attention. The comparison to slot attention appears only at the very end of the paper. Since the approach integrates together existing ideas, it will make sense in this case that the related work section will appear earlier on, before the model section.\n\nThanks for your suggestion. We have relocated the related work section to precede the method section. Also, we revised the second section of the related work to highlight our major distinction compared to the Slot Attention. [Second paragraph of Section 2 (originally, Section 4)]\n\nIn certain aspects, AID seems similar to the Slot Attention method, because it employs the competitive attention for *role/filler* decomposition. However, in contrast to the original iterative attention which assumes the permutation-invariant slots, in our TPR decomposition problem, each slot representation cannot be equally linked to elements in TPR functions (e.g., should identify the appropriate slot for *filler*). Therefore, we introduced a trainable **Routing** ****mechanism to our iterative attention method to correctly integrate slot-based attention with the TPR framework. Our method enables the network to systematically associate each initial component to a specific structural component of TPR. **Through this Routing strategy, the AID can successfully generate the structural representations for TPR with iterative attention.**\n\n**Our AID is a drop-in module specifically designed to enhance the systematic generalization capability of the existing TPR-based models.** Also, we introduce seamless integration of the AID with various types of TPR-based models. Combined with these models, the AID shows effective generalization performance improvements.\n\n\n\n***\n\n> **Empirical Results**: The improvements for WikiText (perplexity) and disentanglement (DCI) are relatively low. On the other hand, we see larger improvements on bAbI and Sort-of-CLEVR.\n\nWe think our description of the purpose of the experiment on the WikiText-103 may not be good enough. We updated the WikiText-103 part of the manuscript for a better illustration of our intention. [First paragraph of Section 4.4 (originally, Section 3.4)]\n\nAs you mentioned, our experiments show larger improvements on the SAR task, the bAbI task, and the Sort-of-CLEVR task that demand clearly systematic generalization capability from models. Our findings indicate that the AID substantially helps TPR-based models enhance their systematic generalization. In addition to these improvements, we extend our evaluation to assess the effectiveness of the AID on a more practical task, the WikiText-103 task, that may not explicitly demand systematic generalization capability but is an important problem. In this task, we aim to show the effectiveness of the AID on TPR-based models beyond systematic generalization tasks rather than aiming at achieving better performance with a larger gap. The WikiText-103 results show the potential of the AID for enhancing performance even on large-scale tasks.\n\nAs other reviewers pointed out, one concern might be whether the improvement is just from more parameters. We conducted experiments with a more parameterized baseline on the WikiText-103 task to examine this. In the Table below, the experimental results indicate that our improvements are not merely attributable to increased parameters.\n\n\n| **WikiText-103 task** | Valid | Test | # params |\n| --- | --- | --- | --- |\n| Linear Transfomer | 36.473 | 37.533 | 44.02M |\n| Linear Transfomer (more params) | 36.452 | 37.306 | 44.22M |\n| Linear Transfomer (+ AID) | **36.159** | **37.151** | 44.16M |\n| Delta Network | 35.640 | 36.659 | 44.03M |\n| Delta Network (more params) | 35.468 | 36.639 | 44.23M |\n| Delta Network (+ AID) | **35.361** | **36.253** | 44.18M |\n\nThe DCI framework provides quantitative metrics to evaluate the relationship between generative factors and representations. Despite the relatively lower improvement in DCI results, the AID generates more disentangled representations than the baseline model. One plausible explanation for this observation could be the task complexity. While the SAR task is relatively simple in disentangling individual items into representations, it further demands that the generated representations satisfy the TPR conditions to solve the task, which poses a challenge for the baseline model."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363276856,
                "cdate": 1700363276856,
                "tmdate": 1700363276856,
                "mdate": 1700363276856,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "acPJ5qf8Gs",
                "forum": "FDb2JQZsFH",
                "replyto": "SI1YfgSzBF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7169/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Related Works: A more detailed comparison to the prior related works, in particular to \u201cEnriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization\u201d that also integrates attention and TPRs. It is cited by the paper but more discussions on similarities and differences would be helpful.\n\nThanks for your feedback. In response, we have expanded the discussion within the related work section. [Second paragraph of Section 2 (originally, Section 4)] As you pointed out, our work has similarities with [1] in that both explore the integration between attention and TPRs. However, [1] is specifically designed for an abstractive summarization and relies on a pre-defined *role* embedding dictionary. Compared to [1], **the AID is a task-independent drop-in module that can be adapted to any TPR-based model. Also, it is designed to address a more fundamental problem, the decomposition of data into the appropriate *role* and *filler* simultaneously.**\n\n[1] Jiang, Y., Celikyilmaz, A., Smolensky, P., Soulos, P., Rao, S., Palangi, H., ... & Gao, J. (2021). Enriching transformers with structured tensor-product representations for abstractive summarization.\u00a0*arXiv preprint arXiv:2106.01317*.\n\n\n***\n\n> Qualitative Evaluation: It would be particularly useful for this work to have more qualitative evaluation for both bAbI and sort-of-CLEVR \u2026 This type of analysis can significantly help in demonstrating the actual impact of integrating attention into TPRs, beyond the overall accuracy metrics.\n\nThanks for the constructive feedback. In response to your query, **we performed orthogonal analysis on the bAbI task**, as did in the SAR task. We have included the experimental results (including Figs. 10, 11, 12, and 13) in Appendix F. Our findings indicate that the AID shows consistent correlation patterns regardless of a systematic difference between data while the baseline shows an undesired pattern (e.g., a decreased correlation for question-relevant words) when processing unseen names. These findings explain why FWM fails and AID succeeds in tackling the *sys-bAbI* task.\n\nMore specifically, we consider the following two sentences where the desired answer is \"kitchen\" for the orthogonal analysis.\n\n- (*w/o sys-diff*) sandra moved to the office. afterward she journeyed to the kitchen. daniel went to the hallway. then he journeyed to the kitchen. where is sandra?\n\n- (*w/ sys-diff*) julie moved to the office. afterward she journeyed to the kitchen. bill went to the hallway. then he journeyed to the kitchen. where is julie?\n\nFigs. 10 and 11 show the similarity between *roles* across the input sequence. FWM and AID exhibit a **high correlation when the sentence subjects are identical**, ****suggesting that word-level TPR-based models might learn to represent symbolic structures sentence-by-sentence. Notably, FWM shows a high intra-sentence word correlation, while AID shows a high correlation at sentence terminations (indicated by '.'). As highlighted in the yellow box comparison, **FWM, when confronted with unseen subjects (*w/ sys-diff* case), shows a decreased correlation between relevant sentences and an increased correlation among irrelevant ones.** On the other hand, **AID maintains consistent results irrespective of systematic differences.**\n\nFurthermore, we explore similarity patterns between *roles* and *unbinding operators*, as done in [2]. We utilize the *roles* generated at each time step of the input sequence and the *unbinding operators* generated at \"?\" for each of the read heads ($N_r=3$). Figs. 12 and 13 reveal that both models exhibit a high correlation at the end of each sentence (\".\"). As seen from the yellow box, the **FWM struggles to link the \".\" of the question-related sentences in the *sys-diff* case, which may explain the prediction of an incorrect answer** (\"office\"). In contrast, **the AID shows consistent patterns and accurately predicts the correct answer**.\n\nFor the Sort-of-CLEVR task, the baseline model implementation (CNN encoder and the multiple attention layers) makes it hard to analyze the qualitative analysis. So, we only conducted the qualitative analysis of the bAbI task in this review period.\n\n[2] Schlag, I., Munkhdalai, T., & Schmidhuber, J. (2020). Learning associative inference using fast weight memory.\u00a0*arXiv preprint arXiv:2011.07831*.\n\n\n***\n\n> For bAbI, a more detailed breakdown of the performance by question type or story length will also be helpful.\n\nWe have also included the detailed bAbI results in the Appendix (please see Table 14)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363343041,
                "cdate": 1700363343041,
                "tmdate": 1700363343041,
                "mdate": 1700363343041,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "13CbIJhL4a",
            "forum": "FDb2JQZsFH",
            "replyto": "FDb2JQZsFH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7169/Reviewer_8A4n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7169/Reviewer_8A4n"
            ],
            "content": {
                "summary": {
                    "value": "Background information: TPRs are an approach for representing compositional structure in vector space; they work by encoding a compositional structure via pairs of fillers - the components of the structure - and roles - the positions of the fillers in the structure. For instance, in the sentence \u201ccats chase dogs\u201d, the fillers could be the words \u201ccats\u201d, \u201cchase\u201d, and \u201cdogs\u201d, and the roles could be \u201csubject\u201d, \u201cverb\u201d, and \u201cobject\u201d, respectively. Each filler and each role is represented with a vector, and these vectors are then combined via tensor products and matrix addition to produce a representation for the whole compositional structure.\n\nWhat this paper does: The authors introduce an approach called Attention-based Iterative Decomposition (AID) designed to generate role and filler representations for models based on Tensor Product Representations (TPRs). TPRs require the input to be broken down into fillers and roles (both represented as vectors), and this is what AID is designed to do; it can be plugged into any TPR-based system as a way to produce the fillers and roles, which can then be processed in the way they normally are for TPRs. AID starts with an initial proposal for the values of the role and filler vectors. These values are then iteratively updated; at each iteration, each TPR component (i.e., role or filler) attends to the input elements, and the TPR components compete with each other for which component attends to which input element. The result of the attention process at each iteration is a new proposal for the role and filler vectors, which is then the input to the next iteration, until the iteration finishes and the final role and filler vectors are produced. The authors then run experiments where they test 3 TPR-based systems from prior work on 4 tasks that involve systematic generalization. They find that adding the AID module improves compositional generalization across tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- S1: The paper addresses an important problem - namely, how to get neural networks to produce effective compositional representations.\n- S2: The proposed AID module is intuitive and can act as a drop-in module in any TPR-based system, meaning that it will be straightforward for other authors to adopt.\n- S3: AID shows very strong performance in the experiments, often substantially increasing accuracy over previous approaches.\n- S4: The experiments are extensive, providing compelling evidence for the strength of the approach.\n- S5: In addition to the experiments based on accuracy, there are also analyses of the structure of the learned representations, which deepen the analyses and lend insight into the ways in which the AID module is enhancing the representations."
                },
                "weaknesses": {
                    "value": "- W1: I believe there is a potential confound of number of parameters. That is, if I understand correctly, AID adds more parameters to the model. Therefore, it\u2019s possible that the improvements created by AID are due to having more parameters rather than due to the effectiveness of the strategy. For most of the experiments, the difference in performance is so large that it\u2019s probably not solely due to number of parameters, but for the Wikitext experiment, the improvement that AID brings is pretty small, so it does seem like a more important concern there. The most convincing way to address this concern would be have the same number of parameters in the model version that has AID and the model version that doesn\u2019t; this could be achieved by, for example, making the feedforward size a bit smaller in the AID version than the non-AID version.\n- W2: I believe that the paper mischaracterizes the binding problem. The binding problem is the question of how different attributes of a structure can be appropriately bound together; for example, given an image with a red square and a blue triangle, how can a system appropriately associate (that is, bind) colors and shapes in order to represent the fact that you have a red square and a blue triangle, rather than a blue triangle and a red square? There is a separate problem that I\u2019ll call the \u201cdecomposition problem\u201d (I don\u2019t think this is a standard term, but it will be useful for this review), which is how to decide what the attributes of a structure are. The paper seems to use the term \u201cbinding\u201d or \u201cbinding problem\u201d when in fact what it talks about is the decomposition problem. Specific places where this occurs are in the abstract (\u201cbecause of the incomplete bindings\u201d, \u201ccan effectively improve the binding\u201d), the second paragraph of the intro (\u201cthese works still have a binding problem\u201d), and the first section of related work (\u201cBinding problem\u201d). The reason why I think that this work is not really about binding is that the part of the TPR formalism that does binding is the step where tensor products are used to combine fillers and roles; the AID module does not alter that portion of the formalism, which is why, properly speaking, I believe it is really about decomposition rather than binding. I would recommend updating the wording to clarify this point. \n- W3: I think the paper is not as careful as it should be at distinguishing facts (things that have been empirically demonstrated), goals (things that the authors want to achieve), and plausible guesses (things that we think are likely to be true but can\u2019t be certain of). I would recommend rewording the paper to be more careful about these points; as it stands, some points are presented as facts when I believe they are in fact goals or plausible guesses, and this could potentially mislead readers about how clearly these points have been demonstrated. Here are the specific points that stood out to me:\n    - The intro says \u201cthese works still have a binding problem \u2026 because the decomposition mechanism they employed relies on a simple MLP, which is known to be not effective in capturing the compositional nature of data.\u201d I think that this is plausible but not something that can definitively be stated as a fact; a way to more clearly state what is known vs. not known would be \u201cwe find that these approaches still show some difficulties on compositional generalization, likely because the decomposition mechanism they employed relies on a simple MLP, which may not be sufficiently structured to learn the compositional nature of the data.\u201d Specific motivations for these edits: adding \u201clikely\u201d to signal that this explanation is plausible but can\u2019t be definitively said to be the cause; add \u201cmay\u201d for a similar reason; changing \u201ccapture\u201d to \u201clearn\u201d, because an MLP can capture anything (it\u2019s a universal function approximator), so the actual difficulty would be when it needs to learn something.\n    - At the start of section 2, I think the word \u201ceffectively\u201d should be removed from \u201cwe illustrate how the AID module effectively decomposes\u201d. This section doesn\u2019t show that the AID is effective - that is not demonstrated until later, when there are empirical results. Similarly, near the top of page 4, I would remove the word \u201ceffectively\u201d again; I don\u2019t think this work demonstrates that competitive attention on its own is effective at decomposing (as opposed to competitive attention being effective when used in combination with the rest of AID, such as the appropriate initial_components).\n    - At the start of section 3, and at the end of \u201cDisentanglement analysis\u201d under 3.1.1, I would recommend removing the word \u201cconsequently\u201d. That word asserts a causal connection that has not been demonstrated (we know that the model gets better disentanglement and better task performance, but we can\u2019t be certain that one causes the other); a more valid way to phrase this would be \u201cThese results demonstrate the AID module\u2019s efficacy in capturing underlying factors during TPR component generation, which may explain why the AID improves task performance.\u201d\n    - Near the top of page 7, it says that AID generates \u201cmore accurate representations.\u201d I don\u2019t think that \u201caccurate\u201d is the right word here; a better phrasing might be \u201crepresentations that better conform to the formal requirements that Smolensky established for ideal TPRs\u201d\n- W4: Some aspects of the experimental setup were not clear to me. First is what the input features are; see Q1 below. Second is that I found it somewhat difficult to understand exactly how the tasks worked; this concern could be addressed by providing some examples of the tasks (ideally in the main paper, or in an appendix if there isn\u2019t room). For example, it\u2019s not clear to me what the inputs are in the SAR task - is it just one x and one y? Or a sequence of x\u2019s and y\u2019s, and if so how are they arranged - x y x y x y, or x x x y y y? And how is the model presented with an x?\n\nOverall, I really enjoyed this paper, but I do think that these concerns currently decrease the paper\u2019s understandability as well as the confidence that we can place in its results. If these concerns are addressed, I would be open to raising my score.\n\nUPDATE: The author response has sufficiently addressed my concerns. Therefore, I have raised my score from 5 to 8; previously I said 5 because my concerns prevented me from recommending acceptance, but with these concerns addressed I believe that the paper's helpful contributions are now effectively highlighted, enabling me to happily recommend acceptance."
                },
                "questions": {
                    "value": "- Q1: What are the input features? I was unable to figure this out. Specifically, is each input feature one object (such as one word)? Or is it one element within a vector representation? From most of the paper I was assuming that it was one object. However, I normally associate the word \u201cfeature\u201d with an element of a vector representation. Also, if the input features are objects, that makes me confused about why N_inputs is considered a hyperparameter in Figure 5, since that\u2019s really a property of the task rather than a hyperparameter. One thing that could help to clarify this is to show an actual example from an actual task in Figure 1, so that we can see what wach input feature is in the context of that task.\n- Q2: Near the top of page 4, it says that producing initial_components from the concatenated input features assigns symbolic meanings to each component, such as roles and fillers. I don\u2019t understand how it achieves this. I can understand why this would be useful (because it would provide a better/optimized starting point), but I don\u2019t see how it pushes each component to have a particular symbolic meaning such as \u201croles\u201d or \u201cfillers\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7169/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7169/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7169/Reviewer_8A4n"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7169/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699566277873,
            "cdate": 1699566277873,
            "tmdate": 1700633894168,
            "mdate": 1700633894168,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tZeRzEOHKT",
                "forum": "FDb2JQZsFH",
                "replyto": "13CbIJhL4a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7169/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their helpful comments. Also, we updated the manuscript to reflect review comments and marked revision with red color. Our responses to your concerns are as follows:\n\n> W1: I believe there is a potential confound of number of parameters. That is, if I understand correctly, AID adds more parameters to the model \u2026 this could be achieved by, for example, making the feedforward size a bit smaller in the AID version than the non-AID version.\n\nThank you for your feedback. In response to your query and Reviewer bmNb's, **we performed experiments with parameter-increased models on the WikiText-103 task.** In our experiments, we increased the size of the feed-forward network in the attention part of the baseline model. The size increase is applied to the exact same positions of the model where AID is adopted, for a fair comparison with our AID-assisted network architecture. In the Table below, **the experimental results show that improvements obtained with AID do not merely come from the number of increased parameters in the models**.\n\n| **WikiText-103 task** | Valid | Test | # params |\n| --- | --- | --- | --- |\n| Linear Transfomer | 36.473 | 37.533 | 44.02M |\n| Linear Transfomer (more params) | 36.452 | 37.306 | 44.22M |\n| Linear Transfomer (+ AID) | **36.159** | **37.151** | 44.16M |\n| Delta Network | 35.640 | 36.659 | 44.03M |\n| Delta Network (more params) | 35.468 | 36.639 | 44.23M |\n| Delta Network (+ AID) | **35.361** | **36.253** | 44.18M |\n\nMoreover, in additional experiments with other tasks, we applied a different method to increase the model parameters, increasing either the hidden or head size of the baseline models. **These experiments also show that the performance enhancements with AID are not simply due to the increased model parameters, rather the network architecture ( multi-layer vs attention-based slots ) could have contributed more**. We updated these results in Appendix E of the manuscript.\n\n| **bAbI task** | $D_\\text{LSTM}$ | *w/o sys diff* | *w/ sys diff* | Gap | # params |\n| --- | --- | --- | --- | --- | --- |\n| FWM | 256 | 0.79 | 2.85 | 2.35 | 0.73 M |\n| FWM (more params) | 512 | 0.75 | 2.16 | 1.41 | 1.89 M |\n| FWM (+ AID) | 256 | **0.45** | **1.21** | **0.76** | 1.23 M |\n\n| **Sort-of-CLEVR task** | $N_\\text{heads}$ | $D_\\text{heads}$ | *Unary* | *Binary* | *Ternary* | # params |\n| --- | --- | --- | --- | --- | --- | --- |\n| Linear Transformer | 8 | 32 | 82.5 | 78.3 | 60.0 | 0.68 M |\n| Linear Transformer (+ AID) | 8 | 32 | **98.9** | 78.0 | 61.0 | 0.83 M |\n| --- | --- | --- | --- | --- | --- | --- |\n| Linear Transformer | 4 | 64 | 69.3 | 75.5 | 56.4 | 0.68 M |\n| Linear Transformer (+ AID) | 4 | 64 | **98.9** | **78.6** | **63.7** | 0.83 M |\n| --- | --- | --- | --- | --- | --- | --- |\n| Linear Transformer | 8 | 64 | 57.5 | 59.7 | 53.2 | 2.55 M |\n| --- | --- | --- | --- | --- | --- | --- |\n| Linear Transformer | 4 | 128 | 57.9 | 59.9 | 52.2 | 2.55 M |\n\n\n\n***\n\n\n> W2: I believe that the paper mischaracterizes the binding problem \u2026 I would recommend updating the wording to clarify this point.\n\nThank you for your valuable input. Initially, we used the term \"binding problem\" based on its multifaceted description in [1], which outlines three key perspectives: (a) segregation, involving the decomposition of low-level sensory data into distinct entities; (b) representation, referring to the separation of information at a representational level; and (c) composition, which pertains to the formation of new inferences. TPR explicitly provides a representational separation to neural networks in the form of *role* and *filler*. Based on this property, TPR-based models have shown significant improvements in systematic generalization, but still have the potential risk if they fail to find correct structured representations from data. Therefore, our focus was primarily on the \"segregation\" aspect of the binding problem. However, as you pointed out, we recognize that the term \"binding problem\" could mislead readers. To clarify our focus and address your concerns, **we change our expression of \"binding problem\" to \"decomposition problem\".** As you suggested, this change aims to avoid misunderstandings of the reader for our target problem.  Furthermore, the term \"decomposition\" is also closely aligned with the concept of \"r/f decomposition\" mentioned in the foundational TPR paper [2]. Following this change in terminology, we accordingly updated **every sentence in our manuscript, whenever \u201c**binding problem**\u201d is used to** mean the \"segregation\" aspect of the problem [Abstract, Second paragraph of Section 1, First paragraph of Section 2 (Originally, Section 4)]\n\n[1] Greff, K., Van Steenkiste, S., & Schmidhuber, J. (2020). On the binding problem in artificial neural networks.\u00a0*arXiv preprint arXiv:2012.05208*.\n\n[2] Smolensky, P. (1990). Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1-2), 159-216."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362978249,
                "cdate": 1700362978249,
                "tmdate": 1700363032273,
                "mdate": 1700363032273,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gTTWmE7ELS",
                "forum": "FDb2JQZsFH",
                "replyto": "13CbIJhL4a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7169/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "***\n\n> W3: I think the paper is not as careful as it should be at distinguishing facts (things that have been empirically demonstrated), goals (things that the authors want to achieve), and plausible guesses (things that we think are likely to be true but can\u2019t be certain of) \u2026 Here are the specific points that stood out to me\n\nIn response to your feedback, we have rephrased the highlighted sentences. The revised sentence is as follows:\n\nWe remove the word \"effectively\". [First paragraph of Section 3 (originally, Section 2), Fourth paragraph of Section 3.2 (originally, Section 2.2)]\n\n**Sentence 1)** However, these works still have a binding problem \u2026 the compositional nature of data.\n\n$\\rightarrow$ **Rewording 1)** However, we find that these approaches still encounter challenges in achieving compositional generalization. This is likely attributable to their reliance on a simple MLP for decomposition, which may not be structured to learn the compositional nature of data. [Second paragraph of Section 1]\n\n**Sentence 2)** These results demonstrate \u2026, consequently improving task performance.\n\n$\\rightarrow$ **Rewording 2)** These results demonstrate the AID module\u2019s efficacy in capturing underlying factors during TPR component generation, which may explain why the AID improves task performance. [Third paragraph of Section 4.1.1 (originally, Section 3.1.1)]\n\n**Sentence 3)** From all these results, it is clear that the AID module learns to decompose data into meaningful components and generate more accurate representations than the baseline model.\n\n$\\rightarrow$ **Rewording 3)** From all these results, it is clear that the AID module learns to decompose data into meaningful representations that better conform to TPR conditions than the baseline model. [Fifth paragraph of Section 4.1.1 (originally, Section 3.1.1)]\n\n\n***\n\n> W4: Some aspects of the experimental setup were not clear to me \u2026 And how is the model presented with an x?\n\nThank you for pointing out the need for a clearer task description in our manuscript. Recognizing this, we have taken steps to enhance the reader's understanding. Specifically, **we have added an illustrative figure (Fig. 6) to visually depict the SAR task process.** Additionally, **a detailed explanation of how the SAR task operates has been included in Appendix A.1.** For further clarification, as follows. At each training iteration, generative factors ($x$ and $y$) are sampled from word sets $X$ and $Y$ to construct the input sequence. The sampled $x$ and $y$ values are randomly paired, creating combinations of one $x$ and one $y$ each. These pairs are then embedded into a vector space and concatenated with flags, which are scalar values signaling the start of the discovery and inference phases. In the discovery phase, models sequentially receive these concatenated representations. During the inference phase, the model is presented only with $x$ values (considered as *role* information) and is tasked with predicting the corresponding $y$ values (considered as *fillers*)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363134753,
                "cdate": 1700363134753,
                "tmdate": 1700363134753,
                "mdate": 1700363134753,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zZeYwfPKWL",
                "forum": "FDb2JQZsFH",
                "replyto": "13CbIJhL4a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7169/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7169/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Q1: What are the input features? \u2026 One thing that could help to clarify this is to show an actual example from an actual task in Figure 1, so that we can see what each input feature is in the context of that task.\n\nThank you for your feedback. We have updated Figure 1 to provide a clearer understanding. We set up the input feature differently based on which type of baseline model the AID is integrated with. TPR-RNN generates TPR components for each sentence, and thus we designate each word as an individual input feature in TPR-RNN. On the other hand, FWM and Linear Transformer generate the components for each word (or token). To maintain the operations of baseline models, we simply partition the word (or token) vector into $N_\\text{inputs}$ sub-vectors of dimension $D_\\text{inputs}$ and designate each sub-vector as an individual input feature in FWM and Linear Transformer. Therefore, the input features are word vectors in the sentence-level model while partitioned sub-vectors in the word(or token)-level model.\n\n***\n\n> Q2:  Near the top of page 4, \u2026  but I don\u2019t see how it pushes each component to have a particular symbolic meaning such as \u201croles\u201d or \u201cfillers\u201d\n\nThank you for highlighting the need for a more comprehensive explanation. We acknowledge that the description provided in **the fourth paragraph of Section 4.2 (Originally, Section 3.2)** did not sufficiently explain how the initial_components of our model are assigned symbolic meanings. To address this, **we have revised the text to enhance clarity and understanding.**\n\nWe point out that the iterative attention mechanism, in its pure form, generates permutation-invariant components, posing a challenge in directly assigning the representations as elements in TPR functions (e.g., should identify the appropriate slot for *filler*). To navigate this challenge, we propose a trainable routing strategy for integrating attention and TPR. This method systematically associates each initial component with a specific symbolic meaning. As illustrated in Fig. 1, for instance, the first component is designated as *role1* and the third as *filler*. Specifically, these initial_components are obtained by applying a feed-forward network to the concatenated input features and are optimized to facilitate a more effective decomposition during training. **Through this routing strategy, the AID learns to generate the structural representations for TPR.**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363165260,
                "cdate": 1700363165260,
                "tmdate": 1700363165260,
                "mdate": 1700363165260,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SA1iYXA7VC",
                "forum": "FDb2JQZsFH",
                "replyto": "zZeYwfPKWL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7169/Reviewer_8A4n"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7169/Reviewer_8A4n"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response; raised score"
                    },
                    "comment": {
                        "value": "Thank you for your detailed reply! These responses sufficiently address the main concerns that I had: The results with increased parameter counts are convincing; the term \"decomposition problem\" is clear; and the other wording changes are also clear. Therefore, I have raised my score from 5 (\"marginally below acceptance threshold\") to 8 (\"accept good paper\")."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7169/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634016125,
                "cdate": 1700634016125,
                "tmdate": 1700634016125,
                "mdate": 1700634016125,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]