[
    {
        "title": "\"What Data Benefits My Classifier?\" Enhancing Model Performance and Interpretability through Influence-Based Data Selection"
    },
    {
        "review": {
            "id": "IlPuYhuR4w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6213/Reviewer_ZqCd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6213/Reviewer_ZqCd"
            ],
            "forum": "HE9eUQlAvo",
            "replyto": "HE9eUQlAvo",
            "content": {
                "summary": {
                    "value": "This paper diverges from the mainstream focus on enhancing model architectures and learning algorithms on fixed datasets. Instead, it tackles an essential yet overlooked issue: understanding how a fixed convex learning model (or a convex surrogate for a non-convex model) benefits from data by interpreting the feature space. Specifically, this paper proposes to use influence estimation models to interpret the classifier's performance through the lens of data features. Furthermore, it introduces data selection methods based on influence to enhance model utility, fairness, and robustness. Through extensive experiments on both synthetic and real-world datasets, the effectiveness of the proposed method is validated. Additionally, the method proves effective not only in conventional classification scenarios but also in more challenging situations, such as distribution shifts, fairness poisoning attacks, utility evasion attacks, online learning, and active learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The research topic is realistic and important. In the era of big data, the analysis of \"more important data points\" is significant. \n- Experimental results are great. In a series of tasks, the proposed method can achieve the best performance."
                },
                "weaknesses": {
                    "value": "- The motivation of this paper is not clear and not strong. \n- Technical contributions of the proposed method are limited. \n- Writing is not unsatisfactory. Many times, readers are unable to understand the author\u2019s true intentions. \n\nMore details about the weaknesses can be checked below."
                },
                "questions": {
                    "value": "- At the beginning, this paper claims it is related to data valuation,  data influence, and data efficiency. Essentially, this paper studies the problem of \"coreset selection\", which is not a new problem in machine learning. Coreset selection surely is related to the above topics. Therefore, it seems that there is no need to introduce so much redundant content in the main paper. \n- The motivation is not clear. It has been fully studied to use the influence function to analyze the importance of data points. This paper follows this line. However, after checking this paper, I am confused about the proposed method of this paper, as the paper just combines the influence estimation and decision tree. Also, why do we need this tree?\n- This paper uses a lot of space to introduce the previous versions of influence functions (Section 2). However, it is not clear that the difference between previous work and this work mathematically.\n- Could the paper provide more high-level intuitions about the formulas of the overall regression tree prediction and hierarchical shrinkage regularizes?\n- For the method in Section 3.2, what is its time/space complexity?\n- Figure 3 and the illustrations in the appendix are not informative. Could the paper supplement more descriptions for them?\n- Could the paper discuss the difference between this paper and the work [1]?\n\n----\n[1] Shuo Yang et al. Dataset Pruning: Reducing Training Data by Examining Generalization Influence. ICLR 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6213/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6213/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6213/Reviewer_ZqCd"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6213/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697263786738,
            "cdate": 1697263786738,
            "tmdate": 1700448868676,
            "mdate": 1700448868676,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sCGbWwmduP",
                "forum": "HE9eUQlAvo",
                "replyto": "IlPuYhuR4w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZqCd [1/3]"
                    },
                    "comment": {
                        "value": "We would like to sincerely thank the reviewer for their review, we are appreciative of their efforts, time, and consideration. Below we provide answers to the concerns raised:\n\n* _**Essentially, this paper studies the problem of \"coreset selection\", which is not a new problem in machine learning. Coreset selection surely is related to the above topics. Therefore, it seems that there is no need to introduce so much redundant content in the main paper.**_\n\n    We would like to emphasize here that coreset selection and our work differ in multiple ways and our main motivation is not aligned with the motivation for coreset selection. We provide more details on these differences below:\n\n    **Motivation and Research Question**: Coreset selection [1,2] is primarily associated with condensing and reducing the size of the training dataset for accelerating training and making training more efficient (for e.g. in deep learning) without sacrificing performance. In contrast, our work is motivated by assessing what type of data is generally beneficial/detrimental for the model by _interpreting the feature space_, and then using _data trimming/selection_ approaches to minimally modify the training set so as to improve performance with respect to _multiple functions of interest_. In our paper currently we had already cited both [1] and [2] (coreset selection approaches) and discussed them at a high-level, but we can describe these differences of motivation in comparison with our work in more detail in the revision.\n\n    **Other Empirical Justification**: To further reinforce the differences in motivation, it can be seen that for coreset selection [1,2] the training data is reduced to be just about 20-30% of the original training dataset size (that is trimming of up to 80%). This is in stark contrast to our work, where we are minimally trimming the training set-- for example, in Figure 2 we only trim up to 5%. Note that we trim minimally in all experiments of our paper, and do not consider removing a large set of samples, as data efficiency is not our main concern.\n    \n    **Differences in Generality**: Different coreset selection methods generally need to be proposed for each function of interest, as it is non-trivial to apply a coreset selection method for accuracy to fairness [3]. This is also in stark contrast to our work, where we seek to have general and simple approaches that can easily be applied to multiple functions of interest (fairness, accuracy, robustness, etc.) in a trivial way. This makes our approaches more general than coreset selection, which would be more specific. This is also observable by the widespread application of our methods to problem scenarios that extend beyond conventional classification, such as online learning and active learning. For instance, it is not immediately clear how coreset selection could be applied to active learning.\n\n    ___\n\n* _**Could the paper provide more high-level intuitions about the formulas of the overall regression tree prediction and hierarchical shrinkage regularizes?**_\n\n    We apologize for any lack of clarity and will make these descriptions clearer in the main text. \n\n    **Overall Regression Tree Prediction**: Since the predictions for a particular node are dependent on where it lies in the tree, the tree model prediction can be denoted as a telescoping sum. We start with the average prediction response at the root node and then keep summing over the individual prediction response at each node (calculated as differences in the average prediction response between successive nodes). This provides us with the prediction at a particular query leaf in the tree. \n\n    **Hierarchical Shrinkage Regularization**: Without hierarchical shrinkage the tree might have some leaves that noisily influence estimates as they have only a few samples. This can affect both interpretability of the tree as well as predictive capability. To counteract this, hierarchical shrinkage is a powerful yet simple approach. Here, we replace the average prediction response over a leaf with a weighted average of the mean or average responses over the leaf and each of its ancestors. The weights depend on the number of samples in each leaf, and are controlled by the regularization parameter $\\lambda$ specified at run-time. Mathematically, this is observable with the slight modification to the summation over individual predictive responses at nodes where we divide the predictive response by the number of samples the node contains as well as $\\lambda$, the regularization parameter.\n    \n    ___"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238580351,
                "cdate": 1700238580351,
                "tmdate": 1700238580351,
                "mdate": 1700238580351,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "50QHfb4qvM",
                "forum": "HE9eUQlAvo",
                "replyto": "IlPuYhuR4w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZqCd [2/3]"
                    },
                    "comment": {
                        "value": "* _**This paper uses a lot of space to introduce the previous versions of influence functions (Section 2). However, it is not clear that the difference between previous work and this work mathematically.**_\n\n     - The difference between previous work (such as Koh and Liang's work) is that those provide an approach for computing influence at the sample-level whereas we are trying to derive general \"rule sets\" at the feature-level for how features affect influence. For example, in Koh and Liang's work, they can obtain influence $I(x_i)$ for a sample $x_i$ (or change in output with sample-level perturbations). In our work, we use a large set of such samples (and their labels) and their influence values, to train an interpretable estimator, such as CART. This can then be used to derive a general interpretation for influence using the feature space-- for example some rules for Accuracy from our 2-D feature space in Figure 1 are: Feature 1 $>= 1.9$ and Feature 2 $< 0.1$ lead to high influence while Feature 1 $< 1.9$ and Feature 2 $< 0.1$ lead to low influence.\n\n    - This sort of feature-level interpretation of performance gain via influence has not been undertaken in the literature previously. This can be useful in many scenarios, some of which we outline below but are not limited to only these: \n\n        **(1)** When new data needs to be added that we do not have influence scores for yet, we can use the influence estimator to obtain influence scores simply via inference. For Koh and Liang's approach, they would still need to train the model once with all samples under consideration every time a new sample arrives. \n\n        **(2)** In algorithmic data recourse scenarios [4] such as to describe why an individual was denied a loan, it can be used to provide an interpretable justification for why a decision was made simply by describing the tree rules that were either met or violated by the data sample.\n\n        **(3)** In active learning, traditional influence computation (such as using Koh and Liang's work) is not viable since there is no access to the sample labels. Previous work such as ISAL [5] (one competitive method in our paper) have still utilized influence in these scenarios, but these do not work as well, since they use the base model itself to generate pseudo-labels for prediction. Our approach using the decision tree serves as an effective and much better alternative, as is evident in the active learning experiments of our paper (Section 5.5 - Figure 6E), where we compare with ISAL. \n\n        **(4)** In some sense, we are bridging feature-interpretation approaches (such as Shapley values and LIME) with sample-interpretation approaches (such as Data Valuation and Influence), and hence, can accommodate any applications that these are used for.\n    ___\n\n\n* _**I am confused about the proposed method of this paper, as the paper just combines the influence estimation and decision tree. Also, why do we need this tree?**_\n\n    Please refer to our response above for our answer to this question. Thank you. If the reviewer raised extra concerns, we are happy to address them.\n\n    ___\n\n\n* _**For the method in Section 3.2, what is its time/space complexity?**_\n\n    Our algorithms are computationally efficient and can scale with large datasets. We provide analytical details on the time complexity below.\n\n    **Algorithm 1 Time Complexity**: For calculating computational complexity, we need to analyze the two steps of our influence estimation process. First, the influence computation of training samples and a given base model requires $\\mathcal{O}(npm)$ time where $n,p$, and $m$ are number of training samples, model parameters, and test samples respectively. Please refer to Section 3 of Koh and Liang's paper for how influence can be computed in $\\mathcal{O}(npm)$ using stochastic estimation or conjugate gradients. Second, the cost of training time of the influence estimation tree in the worst case is $\\mathcal{O}(n^2 d)$ where $d$ is the number of features. Thus, the overall complexity is $\\mathcal{O}(npm) + \\mathcal{O}(n^2 d)$. Note that the quadratic dependence of the second term can be made linear in $n$ simply be using more sophisticated tree-building methods [6] which have a time complexity of $\\mathcal{O}(nd)$. Since the overall time complexity will then be linear: $\\mathcal{O}(n(pm + d))$, the approaches proposed are amenable to large scale datasets (CelebA in our experiments has 104K samples). Furthermore, our approaches work with very high-dimensional data as well. As shown in experiments on the image (CelebA) and NLP (Jigsaw Toxicity) datasets we consider in the paper, we utilize embeddings obtained by deep learning models as a preprocessing step prior to our approach. This significantly reduces the dataset feature size, and at the same time allows for a rich and powerful feature space that can be used to train our models/methods."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238624985,
                "cdate": 1700238624985,
                "tmdate": 1700278630850,
                "mdate": 1700278630850,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GLt5Z0dlLl",
                "forum": "HE9eUQlAvo",
                "replyto": "BVk5pDs5Lh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6213/Reviewer_ZqCd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6213/Reviewer_ZqCd"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for your detailed feedback that addresses my concerns properly. I thus decide to increase my score. \n\nThe remained concern is that could the paper provide a comparison with previous works about time/space complexity."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700354921273,
                "cdate": 1700354921273,
                "tmdate": 1700354921273,
                "mdate": 1700354921273,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6GRyBG0sHT",
                "forum": "HE9eUQlAvo",
                "replyto": "D7dSLlK2Tj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6213/Reviewer_ZqCd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6213/Reviewer_ZqCd"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for your answers. I become more positive about this submission."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448845120,
                "cdate": 1700448845120,
                "tmdate": 1700448845120,
                "mdate": 1700448845120,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KeL7EPROT9",
            "forum": "HE9eUQlAvo",
            "replyto": "HE9eUQlAvo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6213/Reviewer_MEKL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6213/Reviewer_MEKL"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an influence-based trimming approach to uncover which samples and features contribute positively/negatively to the specified utility function. The authors perform experiments with various utility functions (fairness, accuracy, etc) on several datasets: adult, German, drugs, and celebA, among others."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- From my understanding, the authors are trying to not only get the best data samples useful for the model but also identify the best features of the data to use. They use regression trees to help in feature selection and use the influence function for the sample selection. \nNeither influence estimators for sample valuation (for different utilities: fairness, accuracy, data poisoning, etc..) nor CART as a feature selector is a new concept, but the combination is a useful endeavor and an interesting perspective. \n\n- Authors carry out several experiments on several datasets and investigate the performance of their method on several applications, and also compare their work with TMC Shapley."
                },
                "weaknesses": {
                    "value": "- When I read feature space, I think of d-dimensions where the variables (features) live. The authors' writing was a bit confusing to me because from the abstract to the introduction, I thought their influence estimation-based method was identifying features from the feature space with positive/negative influence on the model utility (accuracy, fairness, robustness, and so on). \nHowever, at the beginning of the background section and throughout the experiment results, the authors focus on only the contribution of the training samples to the utility function. \nI think the authors should be a bit more clear in the writing or presentation.  Although section 3 is fairly written, I would recommend that authors revisit abstract+sections 1-3.\n\n- Since the authors focus on features and samples, it would have been informative to see the difference in selected/excluded features and samples and the consequential contribution to the utility with and without the authors' method. \n\n- Although influences functions are not affected by retraining-related complexity, they have a high incremental complexity due to the computation of the Hessian matrix for each x_{i} valuation, which might worsen (beyond retraining) when n is large. \nAdditionally, using CART as a sub-module further increases model complexity.\nI would have appreciated looking at the code specific to section E.1 in the appendix (I couldn't find it in the shared code base)\n\n\n- Not entirely sure, probably it's the figure, I find the almost constant utility values with random deletion somewhat unrealistic. \nCould the authors also explain Figure 2C?\nThe scale for accuracy on some figures in 2 is not intuitive. Is it possible for authors to adopt similar scales for similar utilities across datasets?\n\n- Experimental results. \n  - Figure 2 Specific questions: I find the almost constant utility values with random deletion somewhat unrealistic.  Could the authors also explain Figure 2C?\n  - Figure 10 in the appendix.  If you're removing low-value samples, I wouldn't expect TMC-Shapley to behave like that, accuracy would increase with the removal of low-value samples.  If you're trimming high-value examples, then this graph would make sense but would mean influence-based trimming is performing poorly.\n  - Instead of TMC-Shapely and random, it would have been more informative to see how the proposed approach compares with other influence estimation-based approaches, including vanilla (without CART) influence estimation.\n  - The scale for accuracy on some figures in 2 is not intuitive. Is it possible for authors to adopt similar scales for similar utilities across datasets?\n\n\n- Minor: \n\n  - While the focus on convex loss is understandable, it might lead to sub-optimal influence value estimation due to the model parameters not being at a stationary point or the model not converging. This might then be a net negative and misleading data value estimation.\n  - It looks like the authors do one utility at a time. Due to often competing utilities,  for example, key features and samples for fairness might not necessarily be the same for accuracy, and in most cases might have a negative influence. It would be interesting to see an interplay of various utilities. \n  - Although authors use several datasets, all of them are binary settings. Value computation increases with classes, so I am curious to know if this is the reason authors only focused on binary settings or if there is another reason behind this design choice.\n  - The authors' paper was 32 pages instead of 9"
                },
                "questions": {
                    "value": "While I think the authors propose an interesting perspective, the presentation of the paper needs some improvement. \nI have raised my main concerns in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6213/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6213/Reviewer_MEKL",
                        "ICLR.cc/2024/Conference/Submission6213/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6213/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792152568,
            "cdate": 1698792152568,
            "tmdate": 1700448751486,
            "mdate": 1700448751486,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KZurtxW6Z4",
                "forum": "HE9eUQlAvo",
                "replyto": "KeL7EPROT9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MEKL [1/3]"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their time, effort, and consideration in reviewing our work, we are very grateful. We provide additional details to answer questions raised:\n\n* _**I think the authors should be a bit more clear in the writing or presentation. Although section 3 is fairly written, I would recommend that authors revisit abstract+sections 1-3.**_\n\n    Thank you for the concrete suggestion for improving our paper. We will work to improve the writing and readability, and add a section on _Problem Formulation_ that clarifies the presentation of the problem.\n\n    ___\n\n* _**Since the authors focus on features and samples, it would have been informative to see the difference in selected/excluded features and samples and the consequential contribution to the utility with and without the authors' method.**_\n\n    If we are correct in understanding the reviewer's suggestion, you would like to see the difference in performance and utility when our approach is used and when it is not (that is, the performance of the original model). This is currently observable in Figure 2 for each subfigure when the x-axis value = 0 (0% samples trimmed or excluded). Note that this will be a constant value throughout as there is little variance in the output of logistic regression. If the reviewer intended something else, we can provide additional details during the discussion phase. \n\n    ___\n\n\n* _**Although influences functions are not affected by retraining-related complexity, they have a high incremental complexity due to the computation of the Hessian matrix for each xi valuation, which might worsen (beyond retraining) when n is large. Additionally, using CART as a sub-module further increases model complexity. I would have appreciated looking at the code specific to section E.1 in the appendix (I couldn't find it in the shared code base)**_\n\n    - We apologize for not including the Appendix E.1 code with the provided codebase as we primarily used the original code of TMC-Shapley (Ghorbani et al). However, we have added this experiment in the following code repository: <https://anonymous.4open.science/r/ICLR_Rebuttal_Experiments> for the reviewer to go through. This also has implementations of additional baselines which we discuss in a subsequent response. Next, we discuss analytical details on time complexity of our algorithms. \n\n    - The tree-based influence model of Algorithm 1 is computationally efficient and can scale with large datasets. It has a training time of $\\mathcal{O}(npm) + \\mathcal{O}(n^2 d)$ in the worst case where $n$, $p$, $m$, and $d$ are the number of training samples, the number of model parameters, the number of test samples, and the number of features, respectively. The first term is for the influence computation step (please refer to Section 3 of Koh and Liang's paper for how influence can be computed in $\\mathcal{O}(npm)$ using stochastic estimation or conjugate gradients) and the second is for training and constructing the tree model. The biggest factor here is the quadratic dependence on the number of training samples, which can be made linear in $n$ simply by using more sophisticated tree-building methods [1] that have a time complexity of $\\mathcal{O}(nd)$. This would result in an improved overall time complexity of $\\mathcal{O}(n(pm + d))$. Thus the model can scale easily to larger datasets on modern hardware. This is also evident in our experiments in the paper on the CelebA dataset that has 104K samples and runs computationally efficiently even with a standard tree-building algorithm such as CART (it takes 2.455 seconds to run on average on a Linux server with an Intel Xeon 2.2GHz CPU).\n\n    ___\n\n* _**The scale for accuracy on some figures in 2 is not intuitive. Is it possible for authors to adopt similar scales for similar utilities across datasets?**_\n\n    We understand the reviewer's concern regarding the figure scales; unfortunately, employing same scales across the different subfigures for a function of interest would make it challenging to observe the differences in the trends across different datasets. For example consider accuracy-- for Adult the values vary from 0.826 to 0.832, for Bank they vary from 0.80 to 0.90, and for Jigsaw Toxicity they vary from 0.735 to 0.744. Thus, using the same scale (say 0.735 to 0.90) would make all the trends for datasets except Bank look like a constant line. This is because the metric values are highly dataset-dependent and comparative analysis necessitates individual scales for each dataset. \n\n\n    ___"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238294007,
                "cdate": 1700238294007,
                "tmdate": 1700278545244,
                "mdate": 1700278545244,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yfto4ebqbu",
                "forum": "HE9eUQlAvo",
                "replyto": "KeL7EPROT9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MEKL [3/3]"
                    },
                    "comment": {
                        "value": "* _**It looks like the authors do one utility at a time. Due to often competing utilities, for example, key features and samples for fairness might not necessarily be the same for accuracy, and in most cases might have a negative influence. It would be interesting to see an interplay of various utilities.**_\n\n    Thank you for the great suggestion, we can do this using our current obtained results. By observing the joint overlapping influence regions (or influence tree rules) for multiple functions of interest we can assess what samples contribute positively or negatively to both the functions of interest being considered (fairness, utility, etc.). In the current version of our paper, we did not do this since visualization for this is somewhat non-trivial and messy; however, in practice this is easy to do using our approach. **To exemplify this, we provide new analysis for the synthetic toy dataset of Section 4, for both accuracy + fairness in Figure 25 (Section R1, Page 32) of our revised paper PDF, and for accuracy + robustness in Figure 26 (Section R2, Page 32) of our revised paper PDF**. It can be seen that there are certain samples that are beneficial to remove for both accuracy and robustness, but for fairness and accuracy there is no overlap. Through this, we would like to emphasize that making joint improvements along multiple functions of interest is a dataset-dependent property and at times it might not be possible to make improvements along both functions (for e.g. the fairness-accuracy tradeoff [3]).\n\n    ___\n\n* _**Although authors use several datasets, all of them are binary settings. Value computation increases with classes, so I am curious to know if this is the reason authors only focused on binary settings or if there is another reason behind this design choice.**_\n\n    The choice for utilizing binary classification datasets are based on three factors. **First**, a number of approaches and work in fairness explicitly consider binary classification [3-5] as it is easier to interpret outcomes, and since we wanted to compare with these fairness baselines, we sought to utilize binary datasets throughout the paper. For example for works in both Section 5.1 (fairness distribution shift) and Section 5.2 (fairness poisoning attacks) datasets have binary classes. **Second**, we believe and hope that since our approaches are general, the binary case can be empirically extended to the multi-class case in future work without too much of a challenge. **Third**, a minor consideration was to have _neater_ looking trees when visualized. In the binary case, the class label can only take on two values, but in the multi-class case it would take on many different combinations and interplay with features in numerous ways leading to more complex trees. \n\n\n___\n___\n\n**References:**\n1. Su, Jiang, and Harry Zhang. \"A fast decision tree learning algorithm.\" AAAI 2006.\n2. Northcutt et al. \"Confident learning: Estimating uncertainty in dataset labels.\" Journal of AI Research 2021. \n3. Li, Peizhao, and Hongfu Liu. \"Achieving fairness at no utility cost via data reweighing with influence.\" ICML 2022.\n4. Roh et al. \"Improving fair training under\ncorrelation shifts\". ICML 2023.\n5. Roh et al. \"Fairbatch: Batch selection for\nmodel fairness.\" ICLR 2021."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238444061,
                "cdate": 1700238444061,
                "tmdate": 1700276334764,
                "mdate": 1700276334764,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EiqfDwNEmd",
                "forum": "HE9eUQlAvo",
                "replyto": "KeL7EPROT9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6213/Reviewer_MEKL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6213/Reviewer_MEKL"
                ],
                "content": {
                    "title": {
                        "value": "General reponse to authors."
                    },
                    "comment": {
                        "value": "I would like to express my gratitude to the authors for addressing the concerns raised by me and other reviewers.\n\nAfter reviewing the rebuttal responses and the anonymous repository, I conducted experiments to verify specific responses on runtime, TMC-Shapely and random deletion results, and data split questions.\n\nUpon analyzing the Bank data from the authors' anonymized repository (https://anonymous.4open.science/r/ICLR_Rebuttal_Experiments/), I discovered that the data separation did not align with the expected train:80/val:20 split. The train data had a shape of (100, 51), while the test data had a shape of (6098, 51).\n\nI ran several experiments with an error rate of 0.1, including a single run with seed 0 and merged runs of seeds [0, 1, and 2]. The results showed that removing the high-value TMC-Shapely data points decreased the accuracy (graphs labeled hightolow) while removing the low-value TMC-Shapely data points increased performance (graphs labeled lowtohigh). The results were consistent across both cases.  \n\nWhen I used random deletion, the utility values were not linear. Lastly, the single run took 288.0850269794464 seconds, while the three runs altogether took 1477.8369567394257 seconds. \nIn the single run (data_val_rebuttal_v2.ipynb/rebuttal_results2), I used computed TMC-Sahpley, random, and LOO data values, which means the time would have been even smaller. For the 3 runs (data_val_rebuttal.ipynb/rebuttal_results), I computed TMC-Shapley, G-Shapley, random, and LOO data values.\n\nThe results and all the code are in the anonymized repository (https://anonymous.4open.science/r/iclr-85E3).\nThe repository contains the data valuation scripts, experimentation notebooks, PKL files, and PDFs. \n\nLastly, it looks like from this  ``[0.025, 0.05, 0.075, 0.1, 0.125]`` authors delete ``[int(i*len(x_train)) for i in l] = [2, 5, 7, 10, 12]`` data samples (not percentages as indicated in the figure). In my opinion, the percentage of data deleted in the experiments (up to 5% in other figures) is very small to properly assess/compare the algorithms. Additionally, in cases like the random method where the deletion of high-value data points might show the expected trend (worse utility), the deletion of low-value data points might not do as well, as expected. So it might be intuitive to see data deletion (both ways) to properly assess the effectiveness of the algorithm.\n\nI am happy to engage with the authors on these observations and or if I made some mistakes (it's possible)."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700365459847,
                "cdate": 1700365459847,
                "tmdate": 1700387076867,
                "mdate": 1700387076867,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BHGU14c0hF",
                "forum": "HE9eUQlAvo",
                "replyto": "9wOMOTsic4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6213/Reviewer_MEKL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6213/Reviewer_MEKL"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors for their detailed responses, addressing concerns, and correcting misunderstandings. I have decided to increase my score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448691233,
                "cdate": 1700448691233,
                "tmdate": 1700448691233,
                "mdate": 1700448691233,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9NDSAzvXzd",
            "forum": "HE9eUQlAvo",
            "replyto": "HE9eUQlAvo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6213/Reviewer_MiDf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6213/Reviewer_MiDf"
            ],
            "content": {
                "summary": {
                    "value": "This paper utilizes influence functions to assess what data samples improve utility (smaller loss), fairness (DP and EOP), and adversarial robustness for a given convex classifier by interpreting which sample features contribute positively or negatively to certain performance metrics, and design a data selection strategy accordingly."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tConsider many aspects of model performance beyond accuracy; especially the fairness.\n2.\tExperiments are thorough and the presentations of the experimental results are sound and clear."
                },
                "weaknesses": {
                    "value": "1.\tLimitation on model class: The authors provide a discussion on why the influence function evaluations are limited to convex classifiers, possible remedies, and recently applications to deep neural networks. However, \n2.\tTheoretical analysis: the estimation of the influence function is based on the trees with hierarchical shrinkage regularization. However, there is no analysis on the credibility, time complexity of the proposed Algorithm 1 and Algorithm 2. It seems that these algorithms are not scalable to large-scale datasets. \n3.\tThe utility, fairness and adversarial robustness are important performance metrics for a classifier; however, there is a lack of a unifying story to connect all three and therefore the discussion and experiments may seem distracted\n4.\tFeature explanation is a key aspect in this paper; however, the connection of feature explanation using the influence function with existing explainable AI literature is lacking."
                },
                "questions": {
                    "value": "1.\tShould not the influence estimator has the same architecture of the classifier?\n2.\tFor the fairness experiments in Section 5.1, would the authors justify the choice of the fairness intervention baselines?\nFor other questions, please refer to the Weaknesses. I will consider raising the scores if the authors could adequately address my questions in the rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6213/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6213/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6213/Reviewer_MiDf"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6213/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698852806363,
            "cdate": 1698852806363,
            "tmdate": 1700670997158,
            "mdate": 1700670997158,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uO0BMPnk30",
                "forum": "HE9eUQlAvo",
                "replyto": "9NDSAzvXzd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MiDf [1/2]"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their detailed review of our work, and are grateful for their time and effort. Below we provide answers to the weaknesses and questions raised:\n\n* **Response to Weaknesses**:\n\n    - > Limitation on model class: The authors provide a discussion on why the influence function evaluations are limited to convex classifiers, possible remedies, and recently applications to deep neural networks. However,\n\n        It seems that perhaps this question was accidentally cut-off. If the reviewer can provide more details, we will aim to provide a targeted response and justification during the discussion phase.\n    \n    ___\n\n\n    - > Theoretical analysis: the estimation of the influence function is based on the trees with hierarchical shrinkage regularization. However, there is no analysis on the credibility, time complexity of the proposed Algorithm 1 and Algorithm 2. It seems that these algorithms are not scalable to large-scale datasets.\n\n        Both our proposed algorithms are very computationally efficient and can scale with large datasets. We provide more analytical details on time complexity below.\n\n        **Algorithm 1 Time Complexity**: For calculating computational complexity, we need to analyze the two steps of our influence estimation process. First, the influence computation of training samples and a given base model requires $\\mathcal{O}(npm)$ time where $n,p$, and $m$ are number of training samples, model parameters, and test samples respectively. Please refer to Section 3 of Koh and Liang's paper for how influence can be computed in $\\mathcal{O}(npm)$ using stochastic estimation or conjugate gradients. Second, the cost of training time of the influence estimation tree in the worst case is $\\mathcal{O}(n^2 d)$ where $d$ is the number of features. Thus, the overall complexity is $\\mathcal{O}(npm) + \\mathcal{O}(n^2 d)$. Note that the quadratic dependence of the second term can be made linear in $n$ simply be using more sophisticated tree-building methods [1] which have a time complexity of $\\mathcal{O}(nd)$. Since the overall time complexity will be linear, the approaches proposed are amenable to large scale datasets (CelebA in our experiments has 104K samples). Furthermore, our approaches work with very high-dimensional data as well. As shown in experiments on the image (CelebA) and NLP (Jigsaw Toxicity) datasets we consider in the paper, we utilize embeddings obtained by deep learning models as a preprocessing step prior to our approach. This significantly reduces the dataset feature size, and at the same time allows for a rich and powerful feature space that can be used to train our models/methods.\n\n        **Algorithm 2 Time Complexity**: The time complexity of Algorithm 2 is easier to calculate. Note that the influence values provided once at input require $\\mathcal{O}(npm)$ to calculate as mentioned before. Then the sorting step requires $\\mathcal{O}(n log(n))$ in the worst case. The other operations (lines 8-9) are linear vector operations, leading to a worst case time complexity of $\\mathcal{O}(n(pm + log(n))$. This is not a computational bottleneck for large datasets as the time complexity is linear or logarithmic for each factor.\n\n    ___\n\n    - > The utility, fairness and adversarial robustness are important performance metrics for a classifier; however, there is a lack of a unifying story to connect all three and therefore the discussion and experiments may seem distracted\n\n        Thank you for the great suggestion, we can do this using our current obtained results. By observing the joint overlapping influence regions (or influence tree rules) for multiple functions of interest we can assess what samples contribute positively or negatively to both the functions of interest being considered (fairness, utility, etc.). In the current version of our paper, we did not do this since visualization for this is somewhat non-trivial and messy; however, in practice this is easy to do using our approach. **To exemplify this, we provide new analysis for the synthetic toy dataset of Section 4, for both accuracy + fairness in Figure 25 (Section R1, Page 32) of our revised paper PDF, and for accuracy + robustness in Figure 26 (Section R2, Page 32) of our revised paper PDF**. It can be seen that there are certain samples that are beneficial to remove for both accuracy and robustness, but for fairness and accuracy there is no overlap. Through this, we would like to emphasize that making joint improvements along multiple functions of interest is a dataset-dependent property and at times it might not be possible to make improvements along both functions (for e.g. the fairness-accuracy tradeoff [2]).\n    \n\n    ___"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238095226,
                "cdate": 1700238095226,
                "tmdate": 1700278480728,
                "mdate": 1700278480728,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1ExGAbyrRD",
                "forum": "HE9eUQlAvo",
                "replyto": "eUcY1QehC5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6213/Reviewer_MiDf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6213/Reviewer_MiDf"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors for their response. For my question that was accidentally cut off, I found the answer to it in the response to other reviewers. As the authors have addressed my concerns, I decided to raise my score."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670956030,
                "cdate": 1700670956030,
                "tmdate": 1700670956030,
                "mdate": 1700670956030,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zjahypgdQW",
            "forum": "HE9eUQlAvo",
            "replyto": "HE9eUQlAvo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6213/Reviewer_GBD9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6213/Reviewer_GBD9"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new approach to enhancing the performance of classification models by interpreting and selecting training data through influence estimation models. The authors aim to improve model utility, fairness, and robustness by identifying data that positively impacts these aspects. Extensive experiments on various datasets demonstrate the effectiveness of their methods, which are also applicable to scenarios like distribution shifts and fairness attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. **Important Research Problem**: The authors have targeted an important research problem that focuses on selecting important training data to improve model performance. The research can improve the effectiveness of machine learning models' development that is often overlooked in favor of more complex model architectures or algorithms.\n\n2. **Thorough Experiments**: The authors have conducted thorough experiments to validate their approaches. The use of both synthetic and real-world datasets ensures that the findings are robust and not limited to specific types of data or scenarios. This comprehensive testing framework strengthens the validity of the research conclusions.\n\n3. **Many Applications**: One of the paper's strengths lies in its application to different scenarios. The authors have not only considered conventional classification tasks but have also extended their methodology to address other challenges such as distribution shifts, fairness poisoning attacks, utility evasion attacks, online learning, and active learning. This broad applicability demonstrates the potential impact of the research on various domains and highlights the versatility of the proposed methods."
                },
                "weaknesses": {
                    "value": "1. **Scalability Concerns**: The use of tree-based influence estimation models might indeed pose scalability issues. Tree-based models can become computationally expensive as the size of the dataset increases, especially if the influence estimation requires building trees for many subsets of data or for complex feature interactions. This could limit the method's applicability to big data scenarios or require significant computational resources, which may not always be feasible.\n\n2. **Hard to Adopt Data with High-Dimensional Features**: For example, image data presents unique challenges due to its high dimensionality and the spatial relationships between pixels. Influence functions and feature space interpretations that work well for tabular data may not translate directly to image data."
                },
                "questions": {
                    "value": "- How do the tree-based influence estimation models proposed by the authors scale with very large datasets, and what are the computational costs associated with these models?\n- Could the authors provide insights into the computational complexity of their influence estimation approach, and are there any strategies they recommend for scaling it to big data applications?\n- How does the tree model handle high-dimensional data, such as images, where feature interactions are more complex?\n- Could the authors elaborate on any modifications or extensions to their approach that might be necessary to apply it effectively to image data or other high-dimensional datasets?\n- The work presented focuses on convex models or convex surrogates for non-convex models. Could the authors discuss the potential limitations of this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6213/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699209689643,
            "cdate": 1699209689643,
            "tmdate": 1699636677234,
            "mdate": 1699636677234,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kpho2LZQDz",
                "forum": "HE9eUQlAvo",
                "replyto": "zjahypgdQW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GBD9"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their thoughtful review and for all their time and effort spent in reviewing our work. Below we answer the questions raised:\n\n* _**How do the tree-based influence estimation models proposed by the authors scale with very large datasets, and what are the computational costs associated with these models?**_\n\n    The tree-based influence models are computationally efficient and can scale with large datasets. They have a training time of $\\mathcal{O}(npm) + \\mathcal{O}(n^2 d)$ in the worst case where $n$, $p$, $m$, and $d$ are the number of training samples, model parameters, test samples, and features, respectively. The first term is for the influence computation step (please refer to Section 3 of Koh and Liang's paper for how influence can be computed in $\\mathcal{O}(npm)$ using stochastic estimation or conjugate gradients) and the second is for training and constructing the tree model. The biggest factor here is the quadratic dependence on the number of training samples for constructing the tree, which can be made linear in $n$ simply by using more sophisticated tree-building methods [1], which have a time complexity of $\\mathcal{O}(nd)$. This will result in an overall time complexity of $\\mathcal{O}(n(pm + d))$. Thus the model can scale easily to larger datasets on modern hardware. This is also evident in our experiments in the paper on the CelebA dataset that has 104K samples and runs computationally efficiently even with a standard tree-building algorithm such as CART (it takes 2.455 seconds to run on average on a Linux server with an Intel Xeon 2.2GHz CPU).\n\n    ___\n\n* _**Could the authors provide insights into the computational complexity of their influence estimation approach, and are there any strategies they recommend for scaling it to big data applications?**_\n\n    Our approaches can be used with big data. For calculating computational complexity, please refer to the previous response. As mentioned before, even though the overall time complexity is $\\mathcal{O}(npm) + \\mathcal{O}(n^2 d)$, using more sophisticated tree-building methods [1] we can improve time complexity to $\\mathcal{O}(n(pm + d))$ which is linear in $n$. Furthermore, for very high-dimensional data ($d$ is large), a feature extraction step can reduce time complexity considerably. For the image (CelebA) and NLP (Jigsaw Toxicity) datasets we consider in the paper, we utilize embeddings obtained by deep learning models as a preprocessing step prior to our approach. This significantly reduces the dataset feature size, and at the same time allows for a rich and powerful feature space that can be used to train our models/methods.\n\n    ___\n\n* _**How does the tree model handle high-dimensional data, such as images, where feature interactions are more complex? Could the authors elaborate on any modifications or extensions to their approach that might be necessary to apply it effectively to image data or other high-dimensional datasets?**_\n\n    Our approaches can be used efficiently with high-dimensional image and text datasets simply by employing a feature extraction preprocessing step on the raw data such as by obtaining embeddings from a deep learning model or some other approach. This is how we conduct experiments on CelebA and Jigsaw Toxicity datasets. For e.g., we use the Mini-LM transformer model to get embeddings for the Jigsaw Toxicity dataset and then conduct experiments on it. This significantly reduces the dataset feature size which further reduces the overall worst case time complexity. Additionally, we would also like to mention that past work has found that pixel-level interpretation of images is not akin to human interpretation/reasoning, and hence, prototype-based explanations are generally preferred [2]. Hence, for raw image data, using a feature extraction step for preprocessing is a viable option for improved interpretation.\n\n    ___\n\n* _**The work presented focuses on convex models or convex surrogates for non-convex models. Could the authors discuss the potential limitations of this?**_\n\n    The most fundamental limitation of the convexity assumption is that for non-convex models it is not possible to derive theoretical guarantees for the efficacy of influence functions. However, despite this lack of provable theoretical guarantee, in practice this does not invalidate the use of influence functions in non-convex models. By adding a damping term to the model's Hessian or using convex surrogates, influence functions can obtain satisfactory results. For instance, in our experiments on non-convex models such as MLPs and (preliminary results on) BERT in the appendix, we obtain performance improvements using our proposed approaches for multiple functions of interest. \n\n___\n___\n\n**References:**\n\n1. Su et al. \"A fast decision tree learning algorithm.\" AAAI 2006.\n2. Chen et al. \"This looks like that: Deep learning for interpretable image recognition.\" NeurIPS 2019."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238015126,
                "cdate": 1700238015126,
                "tmdate": 1700278671808,
                "mdate": 1700278671808,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bT7FqF6G4N",
            "forum": "HE9eUQlAvo",
            "replyto": "HE9eUQlAvo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6213/Reviewer_bDe3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6213/Reviewer_bDe3"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents two schemes for 1) identifying the effect of a training point on a function of a model's parameters, and 2) trimming the training set to stem the influence of training instances that have a high negative influence on the test metric of interest. The paper examines the influence of a training point on a test set fairness metric, adversarial robustness, and utility. The first algorithm fits a regression tree using the input and label to the influence function of a metric of interest. The second algorithm then trims the subset of the training set to improve the model. The paper then demonstrates this approach across a variety of metrics including mitigating the effect of unfairness due to distribution shift, adversarial robustness, and the effect of noisy labels in the streaming setting across several datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, this paper provides a comprehensive empirical demonstration of how to improve a performance metric of interest given training samples and model parameters. \n\n- **The breadth of properties**: This paper considers several interesting scenarios ranging from noisy labels, active learning, adversarial robustness to fairness. The comprehensive nature of these settings is quite impressive and commendable.\n- **Compares to adequate baselines**: The paper considers the key baselines that I would've expected and shows improved performance over these baselines.\n- **Compelling results**: I particularly like Figure 2. Over a range of properties and settings, we see that influence-based deletion approach remains quite effective."
                },
                "weaknesses": {
                    "value": "I have a number of confusion about this work that I state here and in the questions section. I would be happy to revise my score in light of feedback from the authors.\n\n- **Details of the approach**: The exact procedure of the trimming portion is not quite clear to me. I think the authors miss discussing retraining. I assume that the authors are referring to a model retrained after a subset of examples are deleted? So in Fig. 2, x axis==zero is a model trained on all data points? Then you trim a percent of the training samples and then retrain a model on the new dataset? If yes, is it the original model that is used for deciding which samples to trim or is the model changing? \n\n- **What is the motivation behind the cart regression procedure?**: As it stands it seems the cart procedure takes as input $(x_i, y_i)$, and the predicts some influence score per example? More details could be useful here. Are the samples used in the training of the cart model a subset of the original training set for which the influence was estimated? Since we know that the influence score measures the effect of up(down)weighting the training sample, alone, we also know that the label should not have any effect on predictive quality of the tree. What is the point of then concatenating the label? It seems like the goal here is to estimate the effect of a feature on the performance metric of interest. I take this judging from Figure 1 where the authors plot performance metric vs features that is colored by influence. If the goal is really to determine the effect of a feature on the performance metric of interest, then how to do that is already in section 2.2 of the original Koh and Liang paper. If the goal is not to measure the effect of the feature on the influence score, then I am not sure I understand the point of this section. Another point here is that in the rest of the paper, the trimming-based approach is really what the authors use and not the cart procedure. If this is the case, I don't think we can that as a contribution of this work. I am asking all these questions as a way to better understand the motivation and goal of fitting the tree to predict the estimated influence score.\n\n- **Related Work**: There is some related work that this paper should be aware of. I list them here: Kong et. al., Resolving Training Biases via Influence-based Data Relabeling, Adebayo et. al. Quantifying and mitigating the impact of label errors on model disparity metrics, Richardson et. al. Add-Remove-or-Relabel: Practitioner-Friendly Bias Mitigation via Influential Fairness, (concurrent) Understanding Unfairness via Training Concept Influence, Sattigerri et. al. Fair infinitesimal jackknife: Mitigating the influence of biased training data points without refitting. All of these papers have a trimming and/or relabelling scheme in them. I am not claiming that this work is not novel/important. I think the insights here are quite useful actually, but it would be helpful for the authors to acknowledge these works, and contextualize their contributions in light of these papers.\n\n- **Tabular Data**: I don't see this as an important weakness; however, most of this work is demonstrated on tabular data. It will be tricky to extend the feature analysis portion, as done in Figure 1 for example, to say images or text."
                },
                "questions": {
                    "value": "Please see the first two bullet points in the weaknesses section for a list of the questions that I have. Thanks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6213/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699631710447,
            "cdate": 1699631710447,
            "tmdate": 1699636677121,
            "mdate": 1699636677121,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Xi3V7vCDnh",
                "forum": "HE9eUQlAvo",
                "replyto": "bT7FqF6G4N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bDe3 [1/3]"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their deep analysis of our work, and their time, effort, and consideration. Below, we answer the questions raised by the reviewer:\n\n* **Details of the approach**: We apologize for any lack of clarity in presenting the approach.\n\n    - > I assume that the authors are referring to a model retrained after a subset of examples are deleted?\n\n        Yes, the reviewer is correct. \n    \n    - > So in Fig. 2, x axis==zero is a model trained on all data points?\n\n        Yes, in Figure 2, x-axis = 0 the model is trained on all data samples.\n\n    - > Then you trim a percent of the training samples and then retrain a model on the new dataset?\n\n        Yes, the reviewer is correct, this is precisely what we do.\n\n    - > If yes, is it the original model that is used for deciding which samples to trim or is the model changing?\n\n        The original unchanged model is used to decide which samples to trim and then we sequentially trim the samples to the desired percentage. \n    \n\n    Thank you for these details. We will make the above points clearer in the text.\n    \n    ___\n\n* **What is the motivation behind the CART regression procedure?**: Thank you for these insightful questions. We would first like to illustrate our motivation/goal and then address these questions one by one. Our goal is to discover a subset of the training samples that benefit the learning model (via influence functions to trim the samples that are a detriment to the model) and interpret where the benefits come from in the feature space (via CART regression). \n\n    - > Are the samples used in the training of the CART model a subset of the original training set for which the influence was estimated?\n\n        Yes, the reviewer is correct, we use an 80-20 split with the original samples (concatenated with their labels) for training the CART model that will then be used to predict the influence value. \n\n    - > Since we know that the influence score measures the effect of up(down)weighting the training sample, alone, we also know that the label should not have any effect on predictive quality of the tree. What is the point of then concatenating the label?\n\n        The label implicitly contributes to the computation of the influence function. Since the influence function estimation utilizes the gradient of the loss term $l$, which takes in as input both $x_i$ (training sample) and $y_i$ (class label), the label also contributes to the influence computation. This is evident in Eqs. (1-3) in our paper and Eq. (2) in Koh and Liang's paper, where they use $z_i$ which is a tuple = $(x_i, y_i)$. This is why we concatenate the label for influence estimation."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237715700,
                "cdate": 1700237715700,
                "tmdate": 1700237715700,
                "mdate": 1700237715700,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "neJmYi3QQm",
                "forum": "HE9eUQlAvo",
                "replyto": "8xAOI4x7zE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6213/Reviewer_bDe3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6213/Reviewer_bDe3"
                ],
                "content": {
                    "title": {
                        "value": "Addresses my concerns"
                    },
                    "comment": {
                        "value": "Thanks to the authors for addressing my concerns. I now see what the point of the CART Regression procedure is. I agree with the authors about the point of doing the global feature analysis and that no previous work has done this. I just don't know that cart regression is the best way to do this kind of analysis. This is because such a procedure will be useful for tabular data, but for images and text, it clearly wouldn't be appropriate. The effectivenesses of your model will depend on how easy it is to interpret the features that is used to train the cart procedure. In addition, you also have to hope that the CART model is high performing and can learn useful features. This is my only remaining issue as it stands, but it is not a disqualifying one, and I think this work is compelling and has addressed my points."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494757106,
                "cdate": 1700494757106,
                "tmdate": 1700494757106,
                "mdate": 1700494757106,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]