[
    {
        "title": "Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy"
    },
    {
        "review": {
            "id": "eIRp4xAIYw",
            "forum": "aLiinaY3ua",
            "replyto": "aLiinaY3ua",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3466/Reviewer_T1LE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3466/Reviewer_T1LE"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Saliency-Diversified Deep Ensembles (SDDE) as a new diversification technique. This method uses saliency maps produced by GradCAM and makes them as different as possible. Specifically, it computes the cosine similarities between the saliency maps and uses their mean as the diversity loss function. SDDE performs better than previous ensemble methods. In addition, it performs well in OOD detection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The proposed method is very simple and easy to understand.\n- It shows good performance on the OpenOOD benchmark."
                },
                "weaknesses": {
                    "value": "- W1. SDDE performs only with CNNs because it GradCAM that is applied to CNN layers. \n- W2. The main hypothesis that ensemble diversity is proportionally related to the diversity of saliency maps, might be invalid for other datasets. The validation process proposed in the paper might not work for other cases.\n- W3. SDDE can be applied to classification algorithms because CAMs are computed on the predicted classes.\n- W4. Table 1 is misleading. The authors adopted additional diversity metrics, but it is obvious that their method looks to have better scores because they specifically added an additional loss function for diversity, which is based on cosine similarity.\n- W5. The paper requires re-writing. The final method named SDDE_{OOD} is presented at the end of the paper, right before Section 6. The authors should describe this final method in Section 4.\n- W6. It looks like MAL (Maximum Average Logit) is one of the main contributions of this paper. However, there is not enough analysis on this."
                },
                "questions": {
                    "value": "What is the total training time of the entire framework when compared to previous approaches? I think it takes more time and has more FLOPs because of the CAMs computation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698622317209,
            "cdate": 1698622317209,
            "tmdate": 1699636299592,
            "mdate": 1699636299592,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YCWea1WjAs",
                "forum": "aLiinaY3ua",
                "replyto": "eIRp4xAIYw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3466/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3466/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express our sincere gratitude for the time you have taken to thoroughly review our manuscript. We likewise value the insightful critiques you have given, which we believe will sharpen the quality of our research. As we respond, we address each of your points. Should any further concerns persist or emerge, we are open for an ongoing discourse.\n\n> **[Q1]**. *SDDE performs only with CNNs because it GradCAM that is applied to CNN layers.*\n> \n\n**[A1]**. We appreciate your comment regarding the application of SDDE solely with CNNs due to the use of GradCAM which is typically applied to CNN layers. However, we would like to highlight that there are saliency maps for transformers [1] and GradCAM, being a versatile method for visualizing feature maps, can be applied to features of any dimensionality.\n\nIn the context of computer vision tasks, which is our focus in this study, CNNs continue to be a stalwart choice [2] due to their strong performance, popularity in the research community, and computational efficiency. They are often favored for their speed compared to alternatives, making them a viable and relevant model to use in our studies with SDDE and GradCAM. Moreover, CNN architectures are a part of OpenOOD benchmark, which we follow in our paper. We hope this clarifies our choice of models.\n\n> **[Q2]**. *The main hypothesis that ensemble diversity is proportionally related to the diversity of saliency maps, might be invalid for other datasets. The validation process proposed in the paper might not work for other cases.*\n> \n\n**[A2]**. Thank you for your insights. We acknowledge the concern regarding the validity of our hypothesis across varied datasets. However, our work is based on the well-tested OpenOOD benchmark, ensuring diverse domain coverage.\n\n> **[Q3]**. *SDDE can be applied to classification algorithms because CAMs are computed on the predicted classes.*\n> \n\n**[A3]**. We agree with the remark about the applicability of SDDE to classification algorithms due to the computation of CAMs on predicted classes. Moreover, in our work we follow existing open-source benchmark for OOD detection (OpenOOD [4]), that is constituted of image classification tasks. Furthermore, we'd like to note that there exist GradCAM-based saliency maps methods adaptable for various tasks e.g. regression, pose estimation [3]. This proves that our approach is not limited by classification-based tasks.\n\n> **[Q4]**. *Table 1 is misleading. The authors adopted additional diversity metrics, but it is obvious that their method looks to have better scores because they specifically added an additional loss function for diversity, which is based on cosine similarity.*\n> \n\n**[A4]**. Thank you for your valuable feedback. We want to clarify that all of the baselines we considered (NCL, APD, DICE) also introduce some form of loss function to diversify their predictions, and we have not modified these baselines with our loss function.\n\nThe main purpose of Table 1 is to provide a comparison among the diversity metrics of different methods. Our method aims to diversify saliency maps, which is different from most existing methods that increase the diversity in the prediction space. While diversification in the prediction space often has a trade-off with performance, our approach has shown that diversifying features can achieve balanced performance. We hope this clarifies our position and sheds light on the rationale underlying our approach."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699978480135,
                "cdate": 1699978480135,
                "tmdate": 1699978480135,
                "mdate": 1699978480135,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vuCwx9xFEV",
            "forum": "aLiinaY3ua",
            "replyto": "aLiinaY3ua",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3466/Reviewer_iYeu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3466/Reviewer_iYeu"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a Saliency-Diversified Deep Ensembles (SDDE) method for classification and OOD detection. Different from previous works which often focus on diversifying the model output, the proposed method aims to diversify the feature space for improving model performance. Specifically, SDDE leverages distinct input features for predictions via computing saliency maps and applying a loss function for diversification."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea of using saliency to enhance the diversity of input features for OOD detection is interesting."
                },
                "weaknesses": {
                    "value": "1.\tThe authors followed the experimental setup and training procedure from the OpenOOD benchmark (Zhang et al., 2023). I am confused as to why they did not also follow the same evaluation setup from the OpenOOD.\n2.\tThe authors miss several state-of-the-art OOD methods [1-4] for comparison. \n[1] Yue Song, Nicu Sebe, and Wei Wang. Rankfeat: Rank-1 feature removal for out-of-distribution detection. NIPS 2022.\n[2] Andrija Djurisic, Nebojsa Bozanic, Arjun Ashok, and Rosanne Liu. Extremely simple activation shaping for out-of-distribution detection. ICLR 2023.\n[3] Jinsong Zhang, Qiang Fu, Xu Chen, Lun Du, Zelin Li, Gang Wang, xiaoguang Liu, Shi Han, and Dongmei Zhang. Out-of-distribution detection based on in-distribution data patterns memorization with modern hopfield energy. ICLR 2023.\n[4] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. ICML, 2022.\n3. In Table 2, why does the proposed method show inferiority on the MINIST dataset while achieving superior performance on the rest of the datasets?"
                },
                "questions": {
                    "value": "Please see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698659812390,
            "cdate": 1698659812390,
            "tmdate": 1699636299508,
            "mdate": 1699636299508,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nZiAyHd9kV",
                "forum": "aLiinaY3ua",
                "replyto": "vuCwx9xFEV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3466/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3466/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate the time and effort you've dedicated to reviewing our paper. As authors, we are equally grateful for the insightful feedback you provided concerning our work. In our subsequent response, we will address all the concerns. Our primary objective is to improve the quality and impact of our research. If any issues persist or new ones arise, we are ready to engage in ongoing dialogue to ensure the manuscript's refinement.\n\n> **[Q1]**. *The authors followed the experimental setup and training procedure from the OpenOOD benchmark (Zhang et al., 2023). I am confused as to why they did not also follow the same evaluation setup from the OpenOOD.*\n> \n\n**[A1]**. We would like to kindly inform you that we have strictly followed the evaluation procedure delineated in OpenOOD v1.5 [1] for conducting our research. The only modification we have implemented is the multi-seed evaluation for ImageNet. We believe this slight adjustment is necessary for the depth and accuracy of our study.\n\n> **[Q2]**. *The authors miss several state-of-the-art OOD methods [1-4] for comparison. [1] Yue Song, Nicu Sebe, and Wei Wang. Rankfeat: Rank-1 feature removal for out-of-distribution detection. NIPS 2022. [2] Andrija Djurisic, Nebojsa Bozanic, Arjun Ashok, and Rosanne Liu. Extremely simple activation shaping for out-of-distribution detection. ICLR 2023. [3] Jinsong Zhang, Qiang Fu, Xu Chen, Lun Du, Zelin Li, Gang Wang, xiaoguang Liu, Shi Han, and Dongmei Zhang. Out-of-distribution detection based on in-distribution data patterns memorization with modern hopfield energy. ICLR 2023. [4] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. ICML, 2022.*\n> \n\n**[A2]**. In our paper, we have proposed a novel ensembling method and focused on comparisons with other ensemble approaches. The mentioned methods, to our understanding, target single-model designs, and have already been implemented in the OpenOOD 1.5 paper. As we run the same evaluation pipeline as in the OpenOOD benchmark, our results can be compared with the original OpenOOD paper. Thus, our approach outperforms the ASH [2] method, which is included in OpenOOD (C10 Near: 75.27% \u2192 92.06%, C10 Far: 78.49% \u2192 94.60%, C100 Near: 78.20% \u2192 92.06%, C100 Far: 80.58% \u2192 94.60%). We hope this elucidates our considerations regarding the methods we've chosen to compare in our study.\n\nWe hope this explanation addresses the queries raised and provides sufficient insight into our methodological choices. Given the clarified points stated above, we cordially request that you reconsider the evaluation of our paper.\n\n**Reference**\n\n[1] Zhang, Jingyang, et al. \"OpenOOD v1. 5: Enhanced Benchmark for Out-of-Distribution Detection.\"\u00a0arXiv preprint arXiv:2306.09301\u00a0(2023).\n\n[2] Djurisic, Andrija, et al. \"Extremely simple activation shaping for out-of-distribution detection.\"\u00a0*arXiv preprint arXiv:2209.09858*(2022)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699976924004,
                "cdate": 1699976924004,
                "tmdate": 1699976924004,
                "mdate": 1699976924004,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vNyWVOanZm",
            "forum": "aLiinaY3ua",
            "replyto": "aLiinaY3ua",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3466/Reviewer_FdHh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3466/Reviewer_FdHh"
            ],
            "content": {
                "summary": {
                    "value": "This paper suggests that deep ensembles are less effective due to the homogeneity of learned patterns. So, the authors try to diversify the saliency maps of the models involved. \n\nBy doing so, the paper claims to attain SOTA results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Good results\n+ Clearly written"
                },
                "weaknesses": {
                    "value": "- As per my understanding, saliency maps should highlight the object regions to help classification. If we make them highlight different regions, as done in Fig.1, it defeats the purpose of saliency maps. I don't agree with the idea that we should diversify saliency maps spatially, to the extent they start highlighting backgrounds. \n-Technical contributions are very limited."
                },
                "questions": {
                    "value": "Why do authors think diversifying saliency maps is the same as diversifying features?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3466/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3466/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3466/Reviewer_FdHh"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698849580634,
            "cdate": 1698849580634,
            "tmdate": 1699636299417,
            "mdate": 1699636299417,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "J3qqPCpMhK",
                "forum": "aLiinaY3ua",
                "replyto": "vNyWVOanZm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3466/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3466/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for reviewing our manuscript and acknowledging its significance. We appreciate your valuable feedback and will address your concerns in detail. Our aim is to improve our work's quality and we are open to further discussion to resolve any remaining issues.\n\n> **[Q1].** *As per my understanding, saliency maps should highlight the object regions to help classification. If we make them highlight different regions, as done in Fig.1, it defeats the purpose of saliency maps. I don't agree with the idea that we should diversify saliency maps spatially, to the extent they start highlighting backgrounds. -Technical contributions are very limited.*\n> \n\n**[A1].** We respect your perspective on the traditional function of saliency maps; however, we would like to offer a different viewpoint supported by our experimental results. Our approach, termed SDDE, might appear unconventional at first glance, but it's crucial to note that it doesn't compromise the performance of individual models. This is clearly demonstrated in Table 1 of our manuscript, where the accuracy of individual models remains intact despite the diversification of saliency maps. We thus conclude that even models, focusing on backgrounds and separate parts of the image, can achieve competitive prediction accuracy.\n\n> **[Q2].** *Why do authors think diversifying saliency maps is the same as diversifying features?*\n> \n\n**[A2].** Thank you for your thoughtful question. To address your query, we conducted an analysis of the relationship between the diversity in saliency maps and the diversity in the features of the models' penultimate layers. This involved measuring the cosine similarities between the saliency maps and these features. The results of this analysis are presented in Figure 8 of Appendix F of our manuscript.\n\nOur findings indicate a clear correlation: as similarities in the feature space increase, so do the similarities in saliency maps. This observation led us to conclude that by diversifying the saliency maps, we indirectly encourage the ensemble models to utilize a broader range of features for their predictions. This inference bridges the gap between the diversification of saliency maps and features in our approach. We understand the importance of this clarification and have thus incorporated the relevant details into our manuscript (Appendix F).\n\nThank you once again for your valuable feedback, which has helped us improve the clarity and depth of our research. We kindly ask you to reconsider your evaluation of our manuscript, taking into account the additional information provided regarding our methods and results. We look forward to your continued input and further discussions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699976257542,
                "cdate": 1699976257542,
                "tmdate": 1699976257542,
                "mdate": 1699976257542,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oAJKjrbq0m",
            "forum": "aLiinaY3ua",
            "replyto": "aLiinaY3ua",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3466/Reviewer_WdEW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3466/Reviewer_WdEW"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents SDDE, an ensembling method for classification and OOD detection. SDDE forces the models within the ensemble to use different input features for prediction, which increases ensemble diversity. Improved confidence estimation and OOD detection make SDDE a useful tool for risk-controlled recognition. SDDE is further generalised for training with OOD data and achieved SOTA results on the\nOpenOOD benchmark."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Originality: The new aspect in the paper is actually that the diversity loss is combined with cross-entropy during training in a single optimization objective.  \n\nQuality: The paper structure seems adequate. The balance between the theory and experiments seems adequate. The proposed method has been examined and compared against the other state of the art technologies. This paper presents rich ablation results. \n\nClarity: The proposed method sounds reasonable and easy to follow. \n\nSignificance: The paper shows that a large number of experiments have been achieved with comprehensive discussion."
                },
                "weaknesses": {
                    "value": "Originality: The diversity loss is combined with cross-entropy during training in a single optimization objective. This additional component sounds like an incremental change. More deep investigation on the incentive of using this combination is required.\n\nQuality: The discussion on the weaknesses of the proposed method seems missing.  \n\nClarity: This paper does not present sufficient explanation to the introduction of the combination of diversity loss and cross-entropy. The introduced strategy sounds like adhoc solution and requires wide discussion on the underlying mechanism. \n\nSignificance: The proposed method does not significantly outperforms the other state of the art technologies. In some of the metrics, the proposed method seems to work well but not all or large metrics."
                },
                "questions": {
                    "value": "1. Why the combination of diversity loss and cross-entropy is the best way to take on board?\n2. To explain the convergence property of the combined solution in the paper.\n3. To provide computational complexity analysis of the compared algorithms."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698953074983,
            "cdate": 1698953074983,
            "tmdate": 1699636299339,
            "mdate": 1699636299339,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5NE9uiwaoZ",
                "forum": "aLiinaY3ua",
                "replyto": "oAJKjrbq0m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3466/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3466/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful feedback regarding our manuscript. We appreciate your comments on our work, and we would like to address your concerns and clarify all the aspects of our approach.\n\n> **[Q1].** *Quality: The discussion on the weaknesses of the proposed method seems missing.*\n> \n\n**[A1].** Thank you for pointing out the importance of discussing limitations of the proposed method. We follow this guideline and cover possible drawbacks of our approach in Section 7 of the submitted manuscript. Furthermore, in order to make this part of our work more distinguishable, we have made a separate section on the limitations (see Section 7 of the updated manuscript).\n\n> **[Q2].** *Significance: The proposed method does not significantly outperforms the other state of the art technologies. In some of the metrics, the proposed method seems to work well but not all or large metrics.*\n> \n\n**[A2].** Thank you for your feedback. While we respect your view, we politely disagree with the assessment. In our paper, we have proposed a novel ensembling method and focused on comparisons with other ensemble approaches. Our results indicate meaningful improvements over current methods, specifically in the area of OOD detection tasks, even on large-scale datasets; please, refer to Tables 3 and 4 Near/Far OOD detection scores: C10 Near: 91.22% \u2192 92.06%, C10 Far: 93.55% \u2192 94.60%, C100 Near: 83.18% \u2192 83.65%, C100 Far: 82.15% \u2192 82.39%, ImageNet Far: 82.85% \u2192 85.98%. We believe the margin of improvement we demonstrate is significant and worthy of consideration. It's our hope that this will be taken into account in the evaluation.\n\n> **[Q3].** *To explain the convergence property of the combined solution in the paper.*\n> \n\n**[A3].** Our research leverages a well-established setup for optimizers and models using SGD and ResNets, which is grounded in a significant body of previous research. With respect to your question on convergence properties, this is still an open problem in the research community and it transcends the scope of our study [4]. While it is a critically important topic, many pragmatic studies, including ours, do not go in-depth into this subject because the focus is primarily on real-world applications and empirical approach. We appreciate your understanding of these constraints and your important role in helping us improve our contributions in this field."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977849142,
                "cdate": 1699977849142,
                "tmdate": 1699977849142,
                "mdate": 1699977849142,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]