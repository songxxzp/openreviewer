[
    {
        "title": "Bayesian Knowledge Distillation for Online Action Detection"
    },
    {
        "review": {
            "id": "sCxmoh8aiF",
            "forum": "8iojQVLLWb",
            "replyto": "8iojQVLLWb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1447/Reviewer_kgXL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1447/Reviewer_kgXL"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a method based on Bayesian knowledge distillation for online action detection. A teacher-student framework is proposed. By distilling the mutual information and distributions of a Bayesian teacher model to an evidential probabilistic student model. The student model can not only make fast and accurate inference, but also efficiently quantify the prediction uncertainty. Experimental results demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Pros:\n1. A method based on Bayesian knowledge distillation is proposed, which makes inference more efficient.\n\n2. Experimental results verify the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "Cons:\n1. This paper seems to simply combine existing knowledge distillation and uncertainty techniques. Knowledge distillation is already proposed in PKD (Zhao et al. (2020)), and uncertainty technique have been used in Uncertainty-OAD (Guo et al. (2022)) for online action detection.\n\n2. More details about the teacher model should be included. The results of the teacher model are also missing.\n\n3. More visual analysis should be included instead of all numerical analysis.\n\n4. Can the performance of the student model boost by increasing the number of model parameters? I wonder when the performance of the student model can exceed that of the existing state-of-the-art methods when the number of parameters is increased."
                },
                "questions": {
                    "value": "See Weaknesses for more details, and limitations should also be included."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1447/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698245865909,
            "cdate": 1698245865909,
            "tmdate": 1699636073494,
            "mdate": 1699636073494,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ld7GoXRE9r",
                "forum": "8iojQVLLWb",
                "replyto": "sCxmoh8aiF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1447/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1447/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answers to reviewer kgXL"
                    },
                    "comment": {
                        "value": "Thanks for your detailed comments, here are our answers to your concerns:\n\n1) Th knowledge distillation in PKD (Zhao et al. (2020)) aims at distilling the knowledge of offline model to an online model. Our knowledge distillation aims at distilling the distribution and mutual information of an online teacher model to an online student model and improving the efficiency. The overall mechanism is different. In addition, our student model can quantify the predictive uncertainty in a single forward pass, which is practical for real applications.\n2) The teacher model is trained in a deterministic way. The Laplace approximation is performed as a post-processing. Code will be made publicly available. For the teacher model, we include the performance below:\n\n| Dataset | ActivityNet (mAP %) | Kinetics (mcAP%) |\n|-----------|------|--------|\n|THUMOS'14|71.6|73.9|\n|TVSeries|88.9|90.6|\n\nThe teacher model has bettern performance than the student model and SOTAs. But it is more computationally expensive, which does not satisfies the objective of our overall goal\n3) We will update the paper with visual analysis. Demos will also be available on our project page \n4) By increasing the number of self-attention layers, our proposed BKD can achieve 72.3% mAP (SOTA: 71.6 %) on THUMOS\u201914 with 26.9M parameters (SOTA: 94.6M). Will update the results and efficiency in the newer version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1447/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597748888,
                "cdate": 1700597748888,
                "tmdate": 1700597748888,
                "mdate": 1700597748888,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wPCvYsiGy6",
                "forum": "8iojQVLLWb",
                "replyto": "Ld7GoXRE9r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1447/Reviewer_kgXL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1447/Reviewer_kgXL"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the response. Despite there are some differences between the proposed method and existing techniques, I still view that the contribution is marginal and cannot meet the high standards of ICLR. Therefore, I will maintain my score rate and hope the authors to revise the manuscript according to the reviewers' comments."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1447/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709104238,
                "cdate": 1700709104238,
                "tmdate": 1700709104238,
                "mdate": 1700709104238,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I3OuCrEVh2",
            "forum": "8iojQVLLWb",
            "replyto": "8iojQVLLWb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1447/Reviewer_i6Lf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1447/Reviewer_i6Lf"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores a Bayesian knowledge distillation (BKD) framework for the online action detection task, which aims at identifying the ongoing action in a streaming video without seeing the future. Specifically, the teacher model is a Bayesian neural network which outputs the mutual information between historical features and ongoing action and predicts the detection uncertainty. The student model learns useful information from the teacher model and constructs a Dirichlet distribution for uncertainty quantification. Competitive performances are achieved on multiple benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ As far as I am concerned, this is the first work which applies distillation on the online action detection task, which may provide some inspiration for the community."
                },
                "weaknesses": {
                    "value": "- Lack of novelty. Although it may be the first work which applies the distillation architecture on online action detection, there is little innovation on components design and theoretical analysis. Teacher-student architecture, Bayesian neural network, and attention network are all very common tools.\n- Misleading/Inappropriate use of evidential deep learning (EDL). EDL is based on the Subjective Logic theory, which is implemented by its unique optimization objective and is accompanied by its own uncertainty calculation method. In the work, the authors construct a simple Dirichlet distribution and then claim they adopt EDL, which is not true.\n- Unfair (or at least incomplete) comparison of computation efficiency and model complexity. Authors claim that the proposed method BKD achieves competitive performance with less model complexity and computational cost, and provide comparison results on Table 2. However, the other methods did not adopt a teacher-student distillation manner as BKD, and the Bayesian neural network which is used as the teacher model by BKD is quite computationally heavy. It is a very natural result for BKD to achieve fast inference at the cost of much larger computation in the training phase via distillation, and the comparison of training speed is not provided. \n- Careless writing. For example, there are two very obvious citation mistakes on the first page of supplementary material."
                },
                "questions": {
                    "value": "1. More discussion about the unique novelty of this work may be provided.\n2. Why do the authors use CE loss and Eq.(10) for model optimization and uncertainty quantification, instead of using the EDL loss and the EDL uncertainty estimation method?\n3. A comparison of training time is necessary for the completeness of experiments.\nFor others please refer to the Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1447/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1447/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1447/Reviewer_i6Lf"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1447/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698725430958,
            "cdate": 1698725430958,
            "tmdate": 1699636073385,
            "mdate": 1699636073385,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "25fW6knK4f",
                "forum": "8iojQVLLWb",
                "replyto": "I3OuCrEVh2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1447/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1447/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answers to reviewer i6Lf"
                    },
                    "comment": {
                        "value": "Thanks for your insightful comments, here are our answers to your concerns:\n\n1) The novelties of this paper mainly lie in: (1) we are the first work leveraging knowledge distillation to improve the inference efficiency of online action detection, which is a big concern for real applications. (2) The student model are trained to identify important features automatically based on the mutual information. (3) We build an evidential student network, which can quantify the predictive uncertainty by a single forward pass.\n\n2) The concept of evidential deep learning is first introduced by [1] to quantify uncertainty for classification problems. By applying a Dirichlet distribution to class probabilities, they treated predictions of a neural network as subjective opinions and learned the function to collect the evidence leading to these opinions from data. The uncertainty estimation is guided by the theory of evidence. Subsequent research has aimed to enhance uncertainty quantification in the framework of [1], particularly by including out-of-distribution data or density models. These developments are comprehensively reviewed in the survey paper [2], which also inspired our use of the term \u201cevidential deep learning\u201d. For a more detailed discussion on this topic, please refer to [2]. We will further clarify evidential deep learning for quantifying classification uncertainty.  \\\n[1] Sensoy, Murat, Lance Kaplan, and Melih Kandemir. \"Evidential deep learning to quantify classification uncertainty.\" Advances in neural information processing systems 31 (2018).\\\n[2] Ulmer, Dennis, Christian Hardmeier, and Jes Frellsen. \"Prior and posterior networks: A survey on evidential deep learning methods for uncertainty estimation.\" Transactions on Machine Learning Research (2023).\n\n3) We agree the training of Bayesian neural network is expensive. Our proposed BKD performs Laplace approximation (LA) after training one deterministic teacher model. So the main computation gain lies in the LA process. Following the comparison in E2E-LOAD, we made a comparison of training cost with other methods below, the mAP is reported on THUMOS\u201914 with Kinetics-pretrained features :\n| Method | mAP (%) | GPU Mem | Time (min/epoch) | # of parameters |\n|----------|------|-------------------------------|--------|-----|\n| LSTR | 59.2 | 8x31.4 |1.5 | 105.9|\n| E2E-LOAD | 72.4 | 8x16.9 |9.6 | 53.5|\n| BKD (ours) | 71.3 | 2x24 |13.2 | 18.7|\n\nIn addition, the inference speed is the first priority of online action detection so the inference FPS is the most important factor. \nFor the unfair comparison concern, the knowledge distillation is one of our contributions. If other methods also leverage knowledge distillation, the performance will drop as the student model usually performs worth than the teacher model. \n\n4) Thanks for your careful reading and pointing out the writing errors, we will proofread the paper and fix all of them.\n\n\nQuestion of using CE loss:\nOur work focuses on evidential deep learning that is designed specifically for classification uncertainty, as detailed in Section 3 of [2]. We use the Dirichlet distribution to efficiently capture uncertainty, allowing it to be determined in a single forward pass of the neural network. This is because Dirichlet distribution is conjugate to the target categorical distribution of y. Since traditional EDL's uncertainty quantification might not effectively distinguish between aleatoric and epistemic uncertainties, we address this through knowledge distillation to minimize the distance between the Dirichlet distribution and the corresponding distribution of the teacher model. By distilling both epistemic and aleatoric information from the Bayesian teacher model, we can employ Eq. 10 as outlined in Section 3.3 of [2] to separate these two types of uncertainty."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1447/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600574339,
                "cdate": 1700600574339,
                "tmdate": 1700631329969,
                "mdate": 1700631329969,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SBuucl7vkn",
                "forum": "8iojQVLLWb",
                "replyto": "25fW6knK4f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1447/Reviewer_i6Lf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1447/Reviewer_i6Lf"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Thank you for the response. However, some issues are still not addressed. For example, the relation between the proposed method and EDL; and the computational burden of the proposed method.\nBesides, I also agree with the opinion of Reviewer 37DN.\nTherefore, I prefer to maintain my original rating."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1447/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641679060,
                "cdate": 1700641679060,
                "tmdate": 1700641679060,
                "mdate": 1700641679060,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bJhf7qSgdt",
            "forum": "8iojQVLLWb",
            "replyto": "8iojQVLLWb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1447/Reviewer_37DN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1447/Reviewer_37DN"
            ],
            "content": {
                "summary": {
                    "value": "This paper tried to handle a practical video detetion problem, online action detection without seeing the future frames. The authors introduced Bayesian knowledge distillation as a teacher network and evidential probabilistic neural network as a student network. The proposed method is evaluated on three benchmark datasets including THUMOS\u201914, TVSeries, and HDD and shows the efficiency. Ablation studies are conducted to prove the efficiency of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Online action detection aims at identifying the ongoing action in a streaming video without seeing the future and it is a practical problem for the video analysis. The authors tried to resolve a real problem with the motivation behind, to make the whole network inference efficient. They tried the teacher and student architecture and introduced Bayesian knowledge distillation as a teacher network and evidential probabilistic neural network as a student network. Comprehensive experimental results on several benchmark datasets are provided."
                },
                "weaknesses": {
                    "value": "The paper appears to be more of an attempt at work rather than containing a substantial amount of insights or analysis. The paper introduced the Bayesian knowledge distillation, (for the first time in knowledge distillation?), however the authors did not provide much insights, such as why it will make the learning efficient etc. The same issue happend to the student network, the authors just introduced the network into the paper without much explainations. From my point of view, I did not understand why this paper should stand out due to two important proposals. Meanwhile, the experimental results shown in the paper are pretty cherry picked, such as MAT (Wang et.al 2023) is compared in different tables. However, in the Figure 4, the authors did not show MAT, on the contrary, the authors showed papers published before 2022. \n\nThe performance in the paper is not impressive, whatever in mAP or FPS."
                },
                "questions": {
                    "value": "The questions are listed in the weakness section. Please address these questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1447/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1447/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1447/Reviewer_37DN"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1447/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698874068456,
            "cdate": 1698874068456,
            "tmdate": 1699636073293,
            "mdate": 1699636073293,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C6oHixk78j",
                "forum": "8iojQVLLWb",
                "replyto": "bJhf7qSgdt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1447/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1447/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answers to reviewer 37DN"
                    },
                    "comment": {
                        "value": "Thanks for you detailed and insightful comments, here are our answers to your concerns:\n1) We did not introduce Bayesian knowledge distillation for the first time. It was initially introduced by Hinton et al. [1], who transferred knowledge from complex ensemble models to a single deterministic model. This idea was further developed by Korattikara et al. [2], who applied it to distill knowledge from Bayesian neural networks into deterministic networks. However, our work incorporates Bayesian knowledge distillation into online action recognition for the first time. While [1] and [2] focus only on distilling the prediction mean from a Bayesian model, our method also distills the uncertainty from the teacher model, focusing on the entire prediction distribution rather than just the mean. \n2) Efficiency concern: We would like to clarify that our approach enhances inference efficiency, not necessarily learning efficiency. The student model, designed to be much smaller than the complex teacher model, ensures efficient inference for predictions. Additionally, the student model shows improved efficiency in uncertainty quantification. A key advantage of our student model is its ability to capture uncertainty, which is distilled from the teacher model during the knowledge distillation process. By employing evidential deep learning, the student model can calculate uncertainty with just a single forward pass, leveraging the Dirichlet distribution's conjugacy with the target categorical distribution of y. This contrasts with the teacher model, which is less efficient in both prediction and uncertainty estimation.\n3) For the comparison with MAT (Wang et.al 2023), we only incorporate the results reported in the paper. We will reproduce their results and include them in the ablation study part. \n4) For the performance concerns, our proposed BKD achieves very close performance compared to SOTAs on all benchmarks. In Table 2 of the paper, BKD achieves 169.6 FPS with 18.7M parameters, which is 46.3 FPS faster than the second fast TRN. Compared to MAT (72.6 FPS, 94.6M parameters), BKD has 0.3% lower performance drop but it has huge FPS improvement and parameters space saving.  \\\n\\\n[1] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. \"Distilling the knowledge in a neural network.\" arXiv preprint arXiv:1503.02531 (2015). \\\n[2] Korattikara Balan, Anoop, et al. \"Bayesian dark knowledge.\" Advances in neural information processing systems 28 (2015)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1447/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631307540,
                "cdate": 1700631307540,
                "tmdate": 1700631307540,
                "mdate": 1700631307540,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jdeDq7Nout",
                "forum": "8iojQVLLWb",
                "replyto": "C6oHixk78j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1447/Reviewer_37DN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1447/Reviewer_37DN"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your replies. However, to be honest, I still think this paper is not above the bar of acceptance. To be, \"Incorporates Bayesian knowledge distillation into online action recognition for the first time\" is not the contributions or shows any novelty. I hope authors could provide more insights, such as why Bayesian knowledge distillation for the action recognition is critical. I still think this paper is pretty engineering."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1447/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632006009,
                "cdate": 1700632006009,
                "tmdate": 1700632006009,
                "mdate": 1700632006009,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ALFcm92MAc",
            "forum": "8iojQVLLWb",
            "replyto": "8iojQVLLWb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1447/Reviewer_f1zi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1447/Reviewer_f1zi"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors present Bayesian knowledge distillation (BKD), a framework for online action detection that is both efficient and generalizable. The authors utilize a teacher-student architecture to improve efficiency. A key aspect of the proposed method is the introduction of a student model based on the evidential neural network. This student model learns feature mutual information and predictive uncertainties from the teacher model. With this design, the student model can not only select important features and make fast inferences, but also accurately quantify prediction uncertainty with a single forward pass. The proposed method was evaluated on three benchmark datasets: THUMOS\u201914, TVSeries, and HDD."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Scope: Online action detection is a crucial task for various applications, including autonomous driving, visual surveillance, and human-robot interaction. This paper addresses challenges such as incomplete action observations and computational efficiency. The focus of the paper is how a model can generalize to unseen environments. Overall, the work is relevant to the ICLR community.\n2. The authors propose Bayesian knowledge distillation (BKD) as a solution for efficient and generalizable online action detection. They utilize a teacher-student architecture for knowledge distillation and leverage the student model to enable efficient inference."
                },
                "weaknesses": {
                    "value": "1. Contribution #1: The authors assert that the proposed Bayesian deep learning model contributes to the task of active online action detection. However, the reviewer has concerns regarding this claim for the following reasons:\n    a. Firstly, the reviewer agrees that inference speed is an important consideration and acknowledges the adoption of a teacher-student model. However, the authors have not provided justification for the limitations of existing teacher-student architectures for online action detection. It remains unclear why the authors chose to leverage evidential deep learning for this purpose. Additionally, there is no comparison of different teacher-student architectures, making it difficult for the reviewer to understand the rationale behind this design choice.\n    b. Secondly, the motivation behind incorporating uncertainty prediction into the design is unclear. While the reviewer acknowledges the need to consider uncertainty due to the inherent unpredictability of the future, the authors have not highlighted the limitations of not modeling uncertainty. For example, a deterministic prediction of the current action may not be necessary, and predictions of potential opportunities for different actions may suffice.\n    c. The authors present an experiment in Figure 6 to demonstrate the use of uncertainty quantification for abnormal action detection, specifically in the context of THUMOS. However, the experiment appears to be relatively simple and may be considered cherry-picked. A comprehensive evaluation of the proposed framework is necessary to validate its effectiveness.\n2. The second claim is that the proposed framework can perform feature selection using mutual information and output Bayesian predictive uncertainties. The reviewer was expecting experimental evidence to support this claim. However, the reviewer did not find sufficient evidence regarding the effectiveness of the architectural design. To substantiate this claim, the reviewer requests experiments that can validate its effectiveness.\n3. Experiments: As mentioned in point 1 (a), the authors primarily focus on demonstrating that the proposed framework performs on par with existing methods on benchmarks for online action detection. However, the reviewer believes that there is a lack of insights regarding the ablation of teacher-student architectures. To establish the value of the proposed evidential deep learning approach, the reviewer requests experiments that can validate this claim."
                },
                "questions": {
                    "value": "The reviewer identified several major concerns in the Weakness section and would like to know the authors' thoughts on these points. Please answer each concern in the rebuttal stage. The reviewer will respond according to the authors' rebuttal in the discussion phase."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1447/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698917454150,
            "cdate": 1698917454150,
            "tmdate": 1699636073219,
            "mdate": 1699636073219,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "amfo66VqjC",
                "forum": "8iojQVLLWb",
                "replyto": "ALFcm92MAc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1447/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1447/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answers to reviewer f1zi"
                    },
                    "comment": {
                        "value": "Thanks for your insightful comments and  questions. Here are our answers to your concerns:\n1) Weakness 1a: Based on our knowledge, we are the first work to adopt knowledge distillation to improve the efficiency of online action detection. (PKD [1] uses knowledge distillation to transfer the knowledge of an offline model to the online model, which is a different mechanism compared to our work). Traditional knowledge distillation methods typically involve single deterministic models for both teacher and student, aiming to align the student's predictions with those of the teacher. However, our approach employs Bayesian knowledge distillation. This technique not only distills the teacher model's predictions but also its associated uncertainty, focusing on the entire prediction distribution rather than just the mean. We leverage evidential deep learning for its efficacy in Bayesian knowledge distillation, particularly for uncertainty quantification efficiency. By transferring the teacher model's knowledge to a Dirichlet-based network, the student model attains not only accurate predictions but also efficient uncertainty estimation. The student model's uncertainty calculation requires just a single forward pass of the network, thanks to the Dirichlet distribution's conjugacy with the target categorical distribution of y. In contrast, the teacher model is less efficient in both prediction and uncertainty estimation. \\\n\\\nWeakness 1b: The student model learns from the teacher model. We aim to transfer as much as the knowledge to the student model, i.e. we aim to distill the distribution of the teacher model to student model. In addition, online action detection is used for many safety-critical appplications such as autonomous drving. Therefore, the uncertainty quantification is important for decision making which another motivation of modeling the uncertainty.  \\\n\\\nWeakness 1c: In Table 7 of the paper, we used different types of uncertainties for abnormal action detection. Since the abnormal action in our experiment setting denotes the classes that are not used for training, the epistemic uncertainty gives the best detection performance, which demonstrates the correctness of our uncertainty quantification. \n\n2) To validate that the proposed framework can leverage mutual information to select important features, we re-implemented the framework without the attention module. The other parts of the framework remain the same. The experiment results are shown as follows:\n\n| Method | THUMOS'14-ANet | THUMOS'14-Kinetics | TVSeries-ANet | TVSeries-Kinetics |\n|----------|------|-------------------------------|--------|-----|\n| BKD (no attention) | 68.2 | 70.1 | 88.0 | 88.9 |\n| BKD | 69.6 | 71.3 | 88.4 | 89.9|\n\nThe results demonstrate the effectiveness of our proposed attention modeling with mutual information.  \\\n\nFor the effectiveness of predictive uncertainty quantification, the results in Table 7 of the paper show that the uncertainties can be used for detecting abnormal actions accurately. \n\n3) For the insights of teacher-student architectures. We implemented different knowledge distillation methods with a same teacher model for comparison. We did grid searches to select the optimal hyper-parameters for fair comparisons. The experiment results are shown as follows:\n| Method | THUMOS'14-ANet | THUMOS'14-Kinetics | TVSeries-ANet | TVSeries-Kinetics |\n|----------|------|-------------------------------|--------|-----|\n| PKD [1] | - | 64.5 | - | 86.4 |\n| Feature-based [2] | 65.2 | 68.1 | 87.0 | 88.4 |\n| Output-based [3] | 66.1 | 68.3 | 87.4 | 88.5 |\n| BKD | 69.6 | 71.3 | 88.4 | 89.9|\n\nThe results shown our BKD obtains better performance than other knowledge distillation methods, which demonstrates the effectiveness of Bayesian distillation.  \\\n\\\n[1] Zhao, Peisen, et al. \"Privileged knowledge distillation for online action detection.\" arXiv preprint arXiv:2011.09158 (2020). \\\n[2] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. \"Distilling the knowledge in a neural network.\" arXiv preprint arXiv:1503.02531 (2015). \\\n[3] Romero, Adriana, et al. \"Fitnets: Hints for thin deep nets.\" arXiv preprint arXiv:1412.6550 (2014)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1447/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635459915,
                "cdate": 1700635459915,
                "tmdate": 1700635548348,
                "mdate": 1700635548348,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xs0NYjHlmt",
                "forum": "8iojQVLLWb",
                "replyto": "amfo66VqjC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1447/Reviewer_f1zi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1447/Reviewer_f1zi"
                ],
                "content": {
                    "title": {
                        "value": "Thanks to the authors for the response."
                    },
                    "comment": {
                        "value": "Thanks to the authors for the response. Thanks for conducting additional results.\n\n1. The response did not answer the question of why we need Bayesian formulation for action recognition. The authors focus on sharing the proposed design, instead of limitations of existing methods.\n\n2. Thanks for sharing additional results. However, the improvements are marginal, which makes it difficult to convince others of the value of the proposed module.\n\n3. There are \"new\" teacher-student frameworks. It is not convincing to convince the audience with the framework proposed in 2014 and 2015."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1447/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728457079,
                "cdate": 1700728457079,
                "tmdate": 1700728457079,
                "mdate": 1700728457079,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]