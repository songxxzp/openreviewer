[
    {
        "title": "WinNet:time series forecasting with a window-enhanced period extracting and interacting"
    },
    {
        "review": {
            "id": "srlijX5GuR",
            "forum": "QhXisLeIqR",
            "replyto": "QhXisLeIqR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3254/Reviewer_7BDu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3254/Reviewer_7BDu"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces WinNet, a CNN-based model tailored for long-term time series forecasting. Traditional Transformer-based approaches, despite their advancements, struggle with computational efficiency and capturing the periodicity of time series data. WinNet seeks to address these issues with its unique architecture:\n\n1. **Inter-Intra Period Encoder (I2PE):** Converts a 1D sequence into a 2D tensor, capturing both long and short periodicities.\n2. **Two-Dimensional Period Decomposition (TDPD):** Models period-trend and oscillation terms, emphasizing the significance of periodicity in time series data.\n3. **Decomposition Correlation Block (DCB):** Exploits the correlations between period-trend and oscillation terms, enhancing prediction capabilities.\n\nA pivotal concept introduced is the \"periodic window,\" derived as the least common multiple of multiple periods obtained via Fourier Frequency Transformation. This enables the model to represent variations of multiple short periods and organizes the sequence into a 2D tensor, wherein each row signifies a short-period trend and each column stands for the long-period trend.\n\nWinNet's innovations result in a simplified structure with a single convolutional layer at its core, which significantly reduces computational complexity. Moreover, this model outperforms various baselines in both univariate and multivariate prediction tasks across multiple domains, as evidenced by experiments conducted on nine benchmark datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Strengths:**\n\n1. **Originality:** \n    - *Periodic Window Concept:* The inception of the periodic window, which is ascertained by the least common multiple of various periods using Fourier Frequency Transformation, offers a groundbreaking method to encapsulate the core characteristics of time series data.\n\n2. **Quality:**\n    - *Ease of Implementation:* WinNet is notably straightforward to implement, and empirical results demonstrate that it surpasses many other methods in a majority of scenarios.\n\n3. **Efficiency:** \n    - *Optimized Parameters and Complexity:* WinNet has fewer parameters and a reduced computational complexity compared to MLP techniques and other prevailing methods.\n\n4. **Clarity:**\n    - *Well-crafted Manuscript:* The paper is articulately penned and provides a seamless reading experience, making it easy for readers to follow and comprehend."
                },
                "weaknesses": {
                    "value": "**Weaknesses:**\n\n1. **Inconsistent Mathematical Notation:** \n    - The authors' presentation of mathematical symbols lacks consistency. As an example, function names should conventionally be displayed in regular typeface instead of italic. Furthermore, vectors and matrices should be represented in bold. Adhering to proper notation is pivotal for ensuring clarity and averting potential misunderstandings.\n\n2. **Graphics and Formatting Issues:** \n    - The image quality in Figure 4 is noticeably poor. Additionally, the authors did not adhere to ICLR's guidelines, as they combined the appendix and main text, which could hamper structured reading and comprehension.\n\n3. **On CNN-based MTS:**\n    - Given the inherent characteristics of time series data, it's a general understanding that Transformer-based methods often underperform compared to Linear and CNN approaches in various scenarios. However, one of the advantages of Transformer methods is their ability to capture interrelationships amongst multi-variables. Evaluating CNN-based methods purely on accuracy might not provide a fair comparison. Moreover, several existing methods, such as Seq-VAE, leverage CNN for MTS tasks. A comparison of WinNet with such methods would have added depth to the evaluation."
                },
                "questions": {
                    "value": "1. **Normalization in MTS:**\n    - Normalization plays a pivotal role in predicting MTS. What kind of normalization technique has been employed within the WinNet framework?\n\n2. **Handling Multivariate Time Series in WinNet:**\n    - How does WinNet approach and manage the relationships between variables in a multivariate time series (MTS)? A more in-depth discussion on this aspect could enhance the paper's clarity.\n\n3. **Hyperparameter Settings in WinNet:**\n    - Could you elaborate on how the hyperparameters for WinNet were determined? Moreover, how do these hyperparameter choices influence the final results? Insights on this could help understand the model's sensitivity and robustness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3254/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697961819850,
            "cdate": 1697961819850,
            "tmdate": 1699636273782,
            "mdate": 1699636273782,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zhDcs5v9yq",
                "forum": "QhXisLeIqR",
                "replyto": "srlijX5GuR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3254/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7BDu"
                    },
                    "comment": {
                        "value": "**Weaknesses**\n\n(1) Inconsistent Mathematical Notation: The authors' presentation of mathematical symbols lacks consistency. As an example, function names should conventionally be displayed in regular typeface instead of italic. Furthermore, vectors and matrices should be represented in bold. Adhering to proper notation is pivotal for ensuring clarity and averting potential misunderstandings.\n\n**Response:** Thank you for your suggestions. We have updated our manuscript about the presentation of mathematical symbols and matrices.\n\n(2) Graphics and Formatting Issues: The image quality in Figure 4 is noticeably poor. Additionally, the authors did not adhere to ICLR's guidelines, as they combined the appendix and main text, which could hamper structured reading and comprehension.\n\n**Response:** Thank you for your suggestions. In this revision, we have combined the main text and appendix into a single PDF to adhere to ICLR\u2019s guidelines.\n\n(3) On CNN-based MTS: ......\n\n**Response:** Thank you for your comments. As shown in Tables 5, 14 and 15, we have compared not only the accuracy with the transformer model, but also the efficiency of the model including training time, inference time and number of parameters. While the Transformer models have the ability to capture the relationships between multiple variables, WinNet uses a channel-independence strategy (following to DLinear [1], RLinear [2]) to eliminate interactions between multiple channels and focuses on capturing seasonality in a single channel, mostly achieving excellent results.\n\nWe have searched for Seq-VAE [3] and find that it is a method applied to sentence generation tasks and is not related to time series forecasting tasks. In addition, the method is a combination of VAE and GAN, and neither the network model nor the CNN is relevant. Would you please provide the above paper in the reference format that can help us find it precisely?\n\nIn the original version, we have conducted comparisons with the CNN methods TimesNet [4] and MICN [5], and provided a comprehensive comparison against the MLP methods. In this revision, we have also added a comparison with RLinear and RMLP, as shown in Tables 1 and 2. It is believed that the performance of WinNet can be validated by the comparisons mentioned above.\n\n**Questions**\n\n(1) Normalization in MTS: Normalization plays a pivotal role in predicting MTS. What kind of normalization technique has been employed within the WinNet framework?\n\n**Response:** We adopt the regularization method of RevIN, which is widely used in the SOTA methods, such as DLinear, PatchTST [6], and RLinear.\n\n(2) Handling Multivariate Time Series in WinNet: ......\n\n**Response:** We adopt a channel-independence strategy that eliminates interactions between multiple variables. This strategy enhances the robustness of the model for multivariate time series with different periodic channels, which demonstrates in the modern methods, including DLinear, PatchTST, and RLinear.\n\n(3) Hyperparameter Settings in WinNet: ......\n\n**Response:** In the WinNet experiments, only three parameters need to be determined, including window_len for MLP, kernel_size for Avgpool2D(\u00b7), and kernel_size for Conv2D(\u00b7). In the original version, we have supplemented the ablation experiments for the window_len as shown in Appendix Table 9. For the Avgpool2D(\u00b7) kernel_size, we combine the periodicity of the sequence to set it as w (window size). Furthermore, we have also conducted the ablation studies for kernel_size of Conv2D(\u00b7) and the results are presented in Table 10.\n\n[1] Zeng Ailing, et al. \u201cAre Transformers Effective for Time Series Forecasting? \u201d. arXiv arXiv:2205.13504v3(2022).\n\n[2] Li Zhe, et al. \u201cRevisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping\u201d. arXiv preprint arXiv:2305.10721 (2023).\n\n[3] Gao Ting, et al. \u201cSeqVAE: Sequence variational autoencoder with policy gradient.\u201d Appl Intell 51, 9030\u20139037 (2021). https://doi.org/10.1007/s10489-021-02374-7\n\n[4] Wu Haixu, et al. \u201cTimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis\u201d. arXiv [arXiv:2210.02186](https://arxiv.org/abs/2210.02186)(2023).\n\n[5] Wang Huiqiang, et al. \u201cMICN: Multi-Scale Local and Global Context Modeling for Long-term Series Forecasting\u201d. https://openreview.net/pdf?id=zt53IDUR1U\n\n[6] Nie Yuqi, et al. \u201cA Time Series is Worth 64 Words: Long-term Forecasting with Transformers\u201d. arXiv [arXiv:2211.14730](https://arxiv.org/abs/2211.14730)(2023)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711527663,
                "cdate": 1700711527663,
                "tmdate": 1700711527663,
                "mdate": 1700711527663,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BUQSo3IiFO",
            "forum": "QhXisLeIqR",
            "replyto": "QhXisLeIqR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3254/Reviewer_8wnu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3254/Reviewer_8wnu"
            ],
            "content": {
                "summary": {
                    "value": "This paper attempts to use the convolutional neural network (CNN) and proposes WinNet for long-term time series forecasting, which is different from most existing works based on Transformer or MLP. Specifically, WinNet first transforms the 1D time series to 2D tensor according to the predefined periodic windows and then performs period-trend and oscillation decomposition. After that, WinNet captures the correlation between period-trend and oscillation terms, based on which convolution and MLP layers are used to get the final prediction. Nine benchmark datasets are used to evaluate the proposed WinNet compared with some baseline methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper attempts to use the convolutional neural network (CNN) for a better long-term time series prediction, which is still seldomly considered."
                },
                "weaknesses": {
                    "value": "1. The blocks in the architecture of WinNet are not introduced clearly in Section 3. e.g.,\n\n -The I2PE block: How to train the MLP; How to calculate the periodic window when using two periods approximately as one period (11/12 or 23/24) in Table 1; why use n = w; Should we do padding for X_1D before performing Equation (1); What is the detailed process of I2PE in Equation (1) (I can guess the details but they should be described clearly)?\n\n -The TDPD block: what is the meaning of w\u00d7w in Equation (2), is it the kernel_size for AvgPool2d; What are the two inputs in \"According to the equation 2, the two inputs can be decomposed into the period-trend and oscillation terms\"?\n\n -The DCB block: What is the process of CI (channel independence strategy) in Equation (3)?\n\n -The Series Decoder block: How to get  X_{i}^{row } and X_{w \u00b7\u2308i/w\u2309\u2212(w\u22121)+(i mod w)}^{col} from X_{output}^{CI} by inter-period convolution and intra-period convolution; What are the details of these two kinds of convolutions; What is the meaning of {w \u00b7\u2308i/w\u2309\u2212(w\u22121)+(i mod w)}?\n\n -Figure 1: What is the process of CA.\n\n2. The proposal WinNet does not compare with SoTA methods and the performance improvement is not significant.\n\n -It is said in the Appendix that some experimental results are taken from the PatchTST and PETformer, but there is no comparison with PETformer. There are also other MLP-based models (RLinear and RMLP), which outperform PatchTST on some datasets and should be compared with.\n\n   Li, Zhe, et al. \"Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping.\" arXiv preprint arXiv:2305.10721 (2023).\n\n -WinNet cannot beat existing models in many cases even based on the results shown in the manuscript.\n\n -Why the results of the exchange dataset are not given in Table 2?\n\n -The architectures for the Ablation Studies are not described clearly. In addition, how about the results by using only inter-period or intra-period branch in Figure 1?\n\n -Which kinds of Time and Memory are considered in Table 6?\n\n3. Some claims are not clear, e.g., it is not clear why \"The correlation between period-trend and oscillation terms can provide the local periodicity in time series\".\n\n4. The code is not available for reproducibility."
                },
                "questions": {
                    "value": "Same to the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3254/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3254/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3254/Reviewer_8wnu"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3254/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698703415017,
            "cdate": 1698703415017,
            "tmdate": 1699636273706,
            "mdate": 1699636273706,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SHep8xi7Kc",
                "forum": "QhXisLeIqR",
                "replyto": "BUQSo3IiFO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3254/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8wnu - Part 1"
                    },
                    "comment": {
                        "value": "**Weaknesses**\n\n(1) The I2PE block: How to train the MLP; How to calculate the periodic window when using two periods approximately as one period (11/12 or 23/24) in Table 1; why use n = w; Should we do padding for X_1D before performing Equation (1); What is the detailed process of I2PE in Equation (1) (I can guess the details but they should be described clearly)?\n\n**Response:** MLP is a linear transformation at the dimension of the sequence length, and we set d_model to 576 (the square of periodic window size). We are referring to the period obtained by Fast Fourier Transformation (FFT), and for the odd number of 11 or 23, we simply add 1 to find the common multiple easily.\n\nIn the experiments, the number and size of periodic windows are both configured to be the same for generating the square matrix, reinforcing the Avgpool2D(\u00b7) operation in the TDPD block. Specifically, the square matrix can balance feature selection between intra-periodic and inter-periodic features during the Avgpool2D(\u00b7) operation, which makes the TDPD block more rational.\n\nWe perform a linear transformation in the I2PE block to change the length of the sequence to the square of the periodic window size. Then our model performs the padding and Avgpool2D(\u00b7) operations to decompose the 2D tensor into the period-trend and oscillation terms. Therefore, it is unable to perform padding before performing Eq. (1).\n\n(2) The TDPD block: what is the meaning of w\u00d7w in Equation (2), is it the kernel_size for AvgPool2d; What are the two inputs in \"According to the equation 2, the two inputs can be decomposed into the period-trend and oscillation terms\"?\n\n**Response:** Yes, wxw is kernel_size for Avgpool2D(\u00b7), and we change it to k to differentiate the window size. After the I2PE block, we get the two inputs (features), inter-period and intra-period, which are fed into the TDPD and DCB blocks, respectively. In the series decoder, we perform direct additive fusion of the two features.\n\n(3) The DCB block: What is the process of CI (channel independence strategy) in Equation (3)?\n\n**Response:** We adopt a channel-independence strategy that eliminates interactions between multiple variables. Specifically, we split both intra-periodic and inter-periodic features at the channel dimension and feed the single-channel matrix into the Concat(\u00b7) operation to compose a two-channel matrix.\n\n(4) The Series Decoder block: How to get X_{i}\\^{row } and X_{w \u00b7\u2308i/w\u2309\u2212(w\u22121)+(i mod w)}\\^{col} from X_{output}\\^{CI} by inter-period convolution and intra-period convolution; What are the details of these two kinds of convolutions; What is the meaning of {w \u00b7\u2308i/w\u2309\u2212(w\u22121)+(i mod w)}?\n\n**Response:** X_{i}\\^{row} and X_{w -\u2308i/w\u2309-(w-1)+(i mod w)}\\^{col} are the corresponding positions of the time steps at the one-dimension features of the intra-period and inter-period. From the two-dimension matrix, we fuse the (i,j) time steps in the intra-period with the (j,i) time steps in the inter-period. We consider that the intra-period and inter-period trend characteristics are preserved between the points on the main diagonal of the matrix, since they satisfy a progressive relationship in the rows and columns. By modeling these points as the main temporal sequence points, the fusion of the summation can be centred on the diagonal points to get both intra-periodic and inter-periodic features, more fully fusing the periodic features of the sequences. As shown in Table 11, We have conducted ablation studies to validate different fusion mode, and the results demonstrate that direct additive fusion can achieve better performance. **(page 16)**\n\nThe intra-period convolutions extract the correlations between period-trend and oscillation terms along the intra-period. The inter-period convolutions extract the correlations among the period. The former concentrates on intra-periodic variations, while the latter focuses on inter-periodic variation. Furthermore, we use parameter-sharing convolution kernels to combine the features of them. We have provided a description about the formula in Series Decoder in Eq. (4). **(page 6)**\n\n(5) Figure 1: What is the process of CA.\n\n**Response:** In the process of CA, we concatenate the single-channel output after the DCB block at the channel dimension for fusion."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710598820,
                "cdate": 1700710598820,
                "tmdate": 1700710648473,
                "mdate": 1700710648473,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yNOn2SUoyU",
                "forum": "QhXisLeIqR",
                "replyto": "BUQSo3IiFO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3254/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8wnu - Part 2"
                    },
                    "comment": {
                        "value": "(6) The proposal WinNet does not compare with .......\n\n**Response:** Thank you for your comments. In the experimental results, we have compared ICLR2023 SOTA models (PatchTST [1], MICN [2], Crossformer [3], TimesNet [4]) and AAAI2023 SOTA model (DLinear [5]). According to your recommendations, we have also added 2 MLP models (RLinear [6] and RMLP), which can be seen in Tables 1 and 2. According to ICLR's REVIEWER GUIDE, papers published on or after 28 May 2023 are not required to compare their work to that paper, so we do not compare with PETformer. In addition, the authors of PETformer [7] do not release the source code and there are no results from the 512 length setting in the paper. Therefore, we have only compared 336 input lengths with PETformer and the results are listed as follows:\n\n|   Methods   | Methods |   WinNet  |   WinNet  |  RLinear  |  RLinear  |    RMLP   |  RMLP | PETformer | PETformer |\n|:-----------:|:-------:|:---------:|:---------:|:---------:|:---------:|:---------:|:-----:|:---------:|:---------:|\n|    Metric   |  Metric |    MSE    |    MAE    |    MSE    |    MAE    |    MSE    |  MAE  |    MSE    |    MAE    |\n|    ETTm1    |    96   | **0.279** |   0.333   |   0.301   |   0.342   |   0.298   | 0.345 |   0.281   | **0.324** |\n|    ETTm1    |   192   | **0.319** |   0.357   |   0.335   |   0.363   |   0.344   | 0.375 |   0.321   | **0.351** |\n|    ETTm1    |   336   | **0.354** |   0.381   |   0.370   |   0.383   |   0.390   | 0.410 |   0.356   | **0.372** |\n|    ETTm1    |   720   | **0.409** |   0.414   |   0.425   |   0.414   |   0.445   | 0.441 |   0.416   | **0.407** |\n|    ETTm2    |    96   | **0.159** |   0.248   |   0.164   |   0.253   |   0.174   | 0.259 |   0.160   | **0.245** |\n|    ETTm2    |   192   | **0.214** | **0.289** |   0.219   |   0.290   |   0.236   | 0.303 |   0.220   | **0.289** |\n|    ETTm2    |   336   | **0.266** |   0.323   |   0.273   |   0.326   |   0.291   | 0.338 |   0.271   | **0.320** |\n|    ETTm2    |   720   | **0.355** | **0.377** |   0.366   |   0.385   |   0.371   | 0.391 |   0.357   |   0.379   |\n|    ETTh1    |    96   |   0.366   |   0.391   |   0.366   |   0.391   |   0.390   | 0.410 | **0.358** | **0.381** |\n|    ETTh1    |   192   |   0.401   |   0.411   |   0.404   |   0.412   |   0.430   | 0.432 | **0.397** | **0.404** |\n|    ETTh1    |   336   |   0.424   |   0.426   |   0.420   |   0.423   |   0.431   | 0.441 | **0.419** | **0.417** |\n|    ETTh1    |   720   | **0.431** | **0.450** |   0.442   |   0.456   |   0.450   | 0.495 |   0.443   |   0.453   |\n|    ETTh2    |    96   |   0.271   |   0.333   | **0.262** | **0.331** |   0.288   | 0.352 |   0.281   |   0.333   |\n|    ETTh2    |   192   |   0.330   | **0.374** | **0.319** | **0.374** |   0.343   | 0.387 |   0.345   |   0.376   |\n|    ETTh2    |   336   |   0.362   |   0.400   | **0.325** |   0.386   |   0.353   | 0.402 |   0.336   | **0.380** |\n|    ETTh2    |   720   |   0.393   |   0.433   | **0.372** | **0.421** |   0.410   | 0.440 |   0.385   |   0.422   |\n|   Weather   |    96   | **0.145** |   0.200   |   0.175   |   0.225   |   0.149   | 0.202 |   0.150   | **0.190** |\n|   Weather   |   192   | **0.188** |   0.246   |   0.218   |   0.260   |   0.194   | 0.242 |   0.194   | **0.232** |\n|   Weather   |   336   | **0.239** |   0.282   |   0.265   |   0.294   |   0.243   | 0.282 |   0.246   | **0.273** |\n|   Weather   |   720   |   0.318   |   0.341   |   0.329   |   0.339   | **0.316** | 0.333 |   0.320   | **0.326** |\n| Electricity |    96   |   0.131   |   0.227   |   0.140   |   0.235   | **0.129** | 0.224 |   0.131   | **0.223** |\n| Electricity |   192   |   0.148   |   0.241   |   0.154   |   0.248   | **0.147** | 0.240 | **0.147** | **0.238** |\n| Electricity |   336   |   0.164   |   0.258   |   0.171   |   0.264   |   0.164   | 0.257 | **0.163** | **0.255** |\n| Electricity |   720   | **0.201** |   0.293   |   0.209   |   0.297   |   0.203   | 0.291 |   0.203   | **0.289** |\n\n[1] Nie Yuqi, et al. \u201cA Time Series is Worth 64 Words: Long-term Forecasting with Transformers\u201d. arXiv arXiv:2211.14730(2023).\n\n[2] Wang Huiqiang, et al. \u201cMICN: Multi-Scale Local and Global Context Modeling for Long-term Series Forecasting\u201d. https://openreview.net/pdf?id=zt53IDUR1U\n\n[3] Zhang Yunhao & Yan Junchi. \u201cCrossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting.\u201d\n\n[4] Wu Haixu, et al. \u201cTimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis\u201d. arXiv arXiv:2210.02186(2023).\n\n[5] Zeng Ailing, et al. \u201cAre Transformers Effective for Time Series Forecasting? \u201d. arXiv arXiv:2205.13504v3(2022).\n\n[6] Li Zhe, et al. \u201cRevisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping\u201d. arXiv preprint arXiv:2305.10721 (2023).\n\n[7] Lin Shengsheng, et al. \u201cPETformer: Long-term Time Series Forecasting via Placeholder-enhanced Transformer\u201d. arXiv arXiv:2308.04791v2(2023)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711236105,
                "cdate": 1700711236105,
                "tmdate": 1700711236105,
                "mdate": 1700711236105,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S7wkz2ljmA",
                "forum": "QhXisLeIqR",
                "replyto": "BUQSo3IiFO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3254/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8wnu - Part 3"
                    },
                    "comment": {
                        "value": "(7) WinNet cannot beat existing models in many cases even based on the results shown in the manuscript.\n\n**Response:** Thank you for your insightful comments. As shown in Tables 16, 17 and 18, the proposed WinNet achieves superior performance with longer input lengths (e.g., 336 and 512), while it obtains inferior results with shorter input lengths (e.g., 96). The possible reasons can be explained as follows. In general, the periodicity of the data can be easily extracted as the input lengths become longer. Specifically, we decompose the period-trend and oscillation terms for both intra-periodic and inter-periodic features by Avgpool2D(\u00b7) operation. However, the input length of 96 is not sufficient for this purpose. Therefore, WinNet achieves inferior performance for some datasets with 96 input lengths.\n\n(8) Why the results of the exchange dataset are not given in Table 2?\n\n**Response:** We have added experimental results about the exchange dataset in Table 2 **(now is 1)**.\n\n(9) The architectures for the Ablation Studies are not described clearly. In addition, how about the results by using only inter-period or intra-period branch in Figure 1?\n\n**Response:** We have updated the specific details of the ablation experiments in the Appendix A.2 **Model architecture**. Moreover, we have also conducted the ablation studies about only intra-period or inter-period branch. The experimental results of ablation studies show that the method using both intra-period and inter-period achieves the best results. The experimental results can be seen in the Appendix Table 12.\n\n(10) Which kinds of Time and Memory are considered in Table 6?\n\n**Response:** The times in Table 6 (now is 5) are the training times calculated for one iteration, and the memory overhead is obtained by the torch.cuda.memory_allocated function. We have also added the inference times, which presents in Tables 5, 14 and 15. These experimental results demonstrate the efficiency of our model against other SOTA models in univariate prediction, as well as multivariate prediction tasks with different number of channels.\n\n(11) Some claims are not clear, e.g., it is not clear why \"The correlation between period-trend and oscillation terms can provide the local periodicity in time series\".\n\n**Response:** As for long-term time series forecasting tasks, we conclude that there is an extremely strong lag correlation between the trend and seasonal terms, as shown in Figure 1 (page 2), and a network model should be designed to extract the correlations, rather than modelling these two terms separately, such as DLinear and MICN.\n\nIn this work, we concatenate the period-trend and oscillation terms in the DCB block at the channel dimension to form a two-channel matrix and then convolve it into a one-channel matrix. The CNN kernel can exactly extract the variation of the two terms within adjacent periods, and the learned parameters are able to perform a proportional aggregation of them, rather than simply adding. Moreover, the aggregation can extract the lag correlation between the two terms. Therefore, a certain local periodicity can be extracted by a single convolution process.\n\n(12) The code is not available for reproducibility.\n\n**Response:** We will release the source code after the manuscript is accepted."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711306794,
                "cdate": 1700711306794,
                "tmdate": 1700711306794,
                "mdate": 1700711306794,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2pWjhjWrjl",
            "forum": "QhXisLeIqR",
            "replyto": "QhXisLeIqR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3254/Reviewer_jnhX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3254/Reviewer_jnhX"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors aim to solve the problems of high computational costs and the missing periodic data capture in forecasting models. To achieve the goal, the authors designed a CNN-based model, WINNET, with one convolutional layer as the backbone. The model includes four parts, Inter-Intra Period Encoder (I2PE), Two-Dimensional Period Decomposition (TDPD), Decomposition Correlation Block (DCB) and Series Decoder. Specifically, I2PE transforms the input 1D sequence into 2D tensor with inter-period and intra-period. TDPD is to obtain the period-trend and oscillation terms. DCB is to study the correlation between the period-trend and oscillation terms. And finally, through Series Decoder, the final prediction results are obtained.\nIn the experiment, the authors evaluate the performance over the real-world datasets both small and large datasets, comparing to several baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality:\nIn this paper, the backbone of the model is mainly a convolution layer, which greatly reduces the computational complexity and improve the efficiency. It is novel and interesting for simplify model structure for time series forecasting tasks. And this try also shows that simple model framework could also effectively perform time series forecasting tasks.\nQuality:\nFrom the perspective of quality, it is high. The authors design a new model and demonstrate its effectiveness through detailed explanations. And the data analysis is thorough, well-executed, and adequately supports the conclusions drawn.\nClarity:\nIn this paper, the introduction provides a clear overview of the research topic and objectives, and the body sections are logically organized. And the language used in this paper is clear and easy to understand. Besides, some key concepts are well explained.\nSignificance:\nThe work in this paper is of great significance. Firstly, WINNET outperforms other forecasting models. And then, WINNET harvests the high computational efficiency for other forecasting models and make full use of the correlation between period trend and oscillation."
                },
                "weaknesses": {
                    "value": "(1) In figure 1, through I2PE block, you can get inter-period and intra-period features. The inter-period features represent the long-period features and the intra-period features represent the short-period features. However, in figure 1, you wrote that the short-period features are inter-period features, and the long-period features are intra-period features.\n(2) In section 3.1, the framework of I2PE is needed.\n(3) In section 3.4, the framework of series decoder is needed.\n(4) In the experimental part, I noticed that for some data sets, WINNET's performance is not the best, not even the second best. Some explanation is needed.\n(5) Please pay attention to typography issues, such as the size of Table 3. Table 4."
                },
                "questions": {
                    "value": "(1) In section 3.1, you set the number of periodic windows is the same as the periodic window size. What is the reason for this setting?\n(2) In the setting of prediction length, the shortest output length is 24. Why not setting the prediction length to the typical prediction length 12?\n(3) From figure 4, we can see that in some cases, the performances of other baselines are better than WINNET, which cannot support the conclusion that WINNET outperforms other baselines. Does WINNET outperform only under some certain T settings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3254/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768067163,
            "cdate": 1698768067163,
            "tmdate": 1699636273633,
            "mdate": 1699636273633,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1Xs7y4uXjb",
                "forum": "QhXisLeIqR",
                "replyto": "2pWjhjWrjl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3254/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jnhX"
                    },
                    "comment": {
                        "value": "**Weaknesses**\n\n(1) In figure 1, through I2PE block, you can get inter-period and intra-period features. The inter-period features represent the long-period features and the intra-period features represent the short-period features. However, in figure 1, you wrote that the short-period features are inter-period features, and the long-period features are intra-period features.\n\n**Response:** Thank you for your correction. We have updated the descriptions in the Figure 1 (now is 2) accordingly. **(page 4)**\n\n(2) In section 3.1, the framework of I2PE is needed.\n\n#### \n\n**Response:** Thank you for your suggestions. We have updated our manuscript and shown the detailed description about the I2PE framework in Eq. (1). **(page 4)**\n\n(3) In section 3.4, the framework of series decoder is needed.\n\n#### \n\n**Response:** We have updated our manuscript and shown the detailed description about the series decoder framework in Eq. (4). **(page 6)**\n\n(4) In the experimental part, I noticed that for some datasets, WINNET's performance is not the best, not even the second best. Some explanation is needed.\n\n**Response:** Thank you for your insightful comments. As shown in Tables 16, 17 and 18, the proposed WinNet achieves superior performance with longer input lengths (e.g., 336 and 512), while it obtains inferior results with shorter input lengths (e.g., 96). The possible reasons can be explained as follows. In general, the periodicity of the data can be easily extracted as the input lengths become longer. Specifically, we decompose the period-trend and oscillation terms for both intra-periodic and inter-periodic features by the Avgpool2D(\u00b7) operation. However, the input length of 96 is not sufficient for this purpose. Therefore, WinNet achieves inferior performance for some datasets with 96 input lengths.\n\n(5) Please pay attention to typography issues, such as the size of Table 3. Table 4.\n\n**Response:** We have updated our manuscript to unify the form of the tables.\n\n####\n\n**Questions**\n\n(1) In section 3.1, you set the number of periodic windows is the same as the periodic window size. What is the reason for this setting?\n\n**Response:** The number and size of periodic windows are both configured to be the same for generating the square matrix, reinforcing the Avgpool2D(\u00b7) operation in the TDPD module. Specifically, the square matrix can balance feature selection between intra-periodic and inter-periodic features during the Avgpool2D(\u00b7) operation. This approach avoids the over-extraction of periodicity from either feature.\n\n(2) In the setting of prediction length, the shortest output length is 24. Why not setting the prediction length to the typical prediction length 12?\n\n**Response:** In general, the output lengths of time series forecasting tasks are configured as {96, 192, 336, 720}, except for the ILI dataset. For the ILI dataset, we follow the SOTA setting and set it to {24, 36, 48, 60}. In accordance with your suggestion, we have conducted an experiment with the output length of 12 and obtained the second-best results, as shown below:\n\n|      | WinNet | WinNet |  PatchTST |  PatchTST | TimesNet | TimesNet |  MICN |  MICN | Crossformer | Crossformer | DLinear | DLinear | FEDformer | FEDformer | Autoformer | Autoformer |\n|------|:------:|:------:|:---------:|:---------:|:--------:|:--------:|:-----:|:-----:|:-----------:|:-----------:|:-------:|:-------:|:---------:|:---------:|:----------:|:----------:|\n| Hori |   MSE  |   MAE  |    MSE    |    MAE    |    MSE   |    MAE   |  MSE  |  MAE  |     MSE     |     MAE     |   MSE   |   MAE   |    MSE    |    MAE    |     MSE    |     MAE    |\n|  12  |  1.626 |  0.803 | **1.375** | **0.733** |   2.347  |   1.094  | 6.084 | 1.925 |    3.227    |    1.191    |  2.556  |  1.181  |   2.051   |   1.017   |    2.686   |    1.144   |\n\n(3) From figure 4, we can see that ......?\n\n**Response:** Thank you for your insightful comments. The proposed WinNet achieves inferior results with shorter input lengths. The WinNet model is more capable of multivariate time series forecasting tasks with longer input lengths (e.g., 336 and 512). The possible reasons can be explained as follows. In general, the periodicity of the sequence can be easily extracted as the input lengths become longer. Specifically, we decompose the period-trend and oscillation terms for intra-periodic and inter-periodic features by the Avgpool2D(\u00b7) operation. However, the input length of 96 is not sufficient for this purpose. Therefore, WinNet achieves inferior performance for some datasets with 96 input lengths.\n\nAs shown in Appendix Figure 9, we have conducted an experimental validation on the ETT datasets, and the experimental results further support the above analyses. **(page 19)**"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710200376,
                "cdate": 1700710200376,
                "tmdate": 1700710200376,
                "mdate": 1700710200376,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ocmOeR2dPZ",
            "forum": "QhXisLeIqR",
            "replyto": "QhXisLeIqR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3254/Reviewer_PKs2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3254/Reviewer_PKs2"
            ],
            "content": {
                "summary": {
                    "value": "This work proposed a simple 2D-CNN framework for time series forecasting tasks, which mainly utilizes the multiscale periodic bias and achieves good forecasting accuracy with computational efficiency."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The numerical results indicate a strong performance when compared to TimesNet, and even demonstrate a slight advantage or equality when compared to PatchTST."
                },
                "weaknesses": {
                    "value": "The storyline of this work appears to lack depth and insights. The inter-intra period encoder (I2PE) block, while not identical to the Timesnet architecture, is closely resemble it. Furthermore, I find it challenging to comprehend why the TDPD block and DCB block can be applied identically in both inter- and intra-period signals. My limited understanding is that the intra-period signal is simply the transpose of the inter-period signal, and I'm unsure of how the proposed winNet addresses the parallel implementation of TDPD and DCB in such a scenario. While the CNN model has potential as the author suggests, the novelty and motivation of this work seem weakly supported. A more comprehensive explanation and study of the design would be beneficial."
                },
                "questions": {
                    "value": "As stated in the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3254/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698800405477,
            "cdate": 1698800405477,
            "tmdate": 1699636273535,
            "mdate": 1699636273535,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4SkVS0svdb",
                "forum": "QhXisLeIqR",
                "replyto": "ocmOeR2dPZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3254/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PKs2"
                    },
                    "comment": {
                        "value": "**Weaknesses**\n\n(1) The storyline of this work appears to lack depth and insights. The inter-intra period encoder (I2PE) block, while not identical to the TimesNet architecture, is closely resemble it.\n\n**Response:** Thank you for your insightful comments. In fact, the design of the I2PE module is inspired by TimesNet [1]. However, compared to TimesNet, the proposed I2PE module is unique in the following ways:\n\n1) We simplify the overheads of the model by choosing a periodic window to cover multiple periods of variation based on the top-k period values.\n\n2) We introduce both intra-periodic and inter-periodic features to perform the TDPD and DCB blocks to extract the changes between and within periods, respectively.\n\nIn contrast, TimesNet uses multiple convolutional kernels to extract features from the top-k periods, which leads to high experimental overheads.\n\n(2) Furthermore, I find it challenging to comprehend why the TDPD block and DCB block can be applied identically in both inter- and intra-period signals. My limited understanding is that the intra-period signal is simply the transpose of the inter-period signal, and I'm unsure of how the proposed winNet addresses the parallel implementation of TDPD and DCB in such a scenario. While the CNN model has potential as the author suggests, the novelty and motivation of this work seem weakly supported. A more comprehensive explanation and study of the design would be beneficial.\n\n**Response:** As the reviewer\u2019s understanding, the inter-period feature is transposed by the intra-period feature. In the experiments, the number and size of periodic windows are both configured to be the same for generating the square matrix, reinforcing the Avgpool2D(\u00b7) operation in the TDPD module. Specifically, the square matrix can balance feature selection between intra-periodic and inter-periodic features during the Avgpool2D(\u00b7) operation, which makes the TDPD block more rational.\n\nIn time series forecasting tasks, we empirically find that there is an extremely strong lag correlation between the trend and seasonal terms, and a network model should be designed to extract the correlations rather than modelling the two terms separately (such as DLinear [2] and MICN [3]). The core idea of our work is to capture correlations between the trend and seasonal terms obtained by a decomposition process. To this end, both the TDPD and DCB blocks are designed to implement the idea, which achieves SOTA results with longer input lengths. We have updated the analysis of lag correlation for time series data as shown in Figure 1. **(page 2)**\n\n[1] Wu Haixu, et al. \u201cTimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis\u201d. arXiv [arXiv:2210.02186](https://arxiv.org/abs/2210.02186)(2023).\n\n[2] Zeng Ailing, et al. \u201cAre Transformers Effective for Time Series Forecasting? \u201d. arXiv arXiv:2205.13504v3(2022).\n\n[3] Wang Huiqiang, et al. \u201cMICN: Multi-Scale Local and Global Context Modeling for Long-term Series Forecasting\u201d. https://openreview.net/pdf?id=zt53IDUR1U"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708941739,
                "cdate": 1700708941739,
                "tmdate": 1700708941739,
                "mdate": 1700708941739,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]