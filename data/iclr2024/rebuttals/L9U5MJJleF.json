[
    {
        "title": "Concept Bottleneck Generative Models"
    },
    {
        "review": {
            "id": "UR4QVyJxkg",
            "forum": "L9U5MJJleF",
            "replyto": "L9U5MJJleF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6823/Reviewer_5NVe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6823/Reviewer_5NVe"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose to introduce a concept-bottleneck layer into deep generative models, including GANs, VAEs, and diffusion models.  The bottleneck is encouraged to encode interpretable concepts using dense annotations.  Furthermore, it is paired with an unsupervised side-channel conveying information that is constrained to be orthogonal to the concepts in the bottleneck.  The resulting generative model, CBGMs, can be steered by controlling the concepts themselves and admits investigating its behavior by intervening on the concepts.  The experiments evaluate steerability, interpretability, debuggability, and generation performance against a selection of conditional generative models.\n\n**Post-rebuttal update**: I have increased my score based on the rebuttal."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originality**: The specific architecture (or family of architectures) proposed here is novel.\n\n**Quality**: The idea is sensible and the writing is good.\n\n**Clarity**: The narrative (excluding the experimental section, which lacks some details, see weaknesses) is very clear.  \n\n**Significance**: At a high level, this paper fills a clear gap in the CBM literature, showing how the concept bottleneck idea can be extended beyond discriminative models."
                },
                "weaknesses": {
                    "value": "**Originality**: The idea of using concepts for steering generative models is by now somewhat familiar tho, as this is what VLMs implicitly do, and what conditional generative models have done for a while (the whole motivation for investigating disentanglement stems precisely from this problem, although I agree disentanglement does not guarantee interpretability).  The idea of integrating VAEs in concept-bottleneck classifiers has also been explored (as mentioned in the related work), but not for the purpose of steering generative models.\n\n**Quality**: The critical issue with this paper is that the experiments are lacking.\n\nFor instance, the experiment in Section 4.2 measures steerability only by turning concepts on (but not off).  It is not clear how to understand steerability:  it is defined as the accuracy of a pre-trained concept detector on generated images after turning on a concept that originally was predicted to be off.  Why the accuracy?  Why not the percentage of images in which the concept is predicted to be on?  What would be the ideal value, 100%?  If so, the numbers reported in the able are (better than the competitors but) quite far from this goal.  Also, the validation accuracy of the concept classifiers used to evaluate steerability is not reported, so how do we know they are high quality?\n\nUnless I missed something, interpretability is only assessed qualitatively (Section 4.3.1, Figure 4).\n\nGeneration quality (Section 4.4) is only measured for a single data set.  Same for debuggability in Section 4.3.\n\nOverall, the choice of research questions is okay, but taken individually the experiments leave something to be desired.  Evidence is provided for individual data sets or even selected examples.  This makes it difficult to assess the true limitations of CB-* models.\n\n**Significance**:  What is not entirely clear to me is what niche of problems CBGMs help with that VLMs cannot to some extent already deal with.  The other issue is that concept annotations -- as readily admitted by the authors -- is difficult to acquire.  This is why, recently, researchers have started defining concepts using VLMs like CLIP.  Mind you, I am not an LLM enthusiast -- but I feel there is something anachronistic about the proposed setup.  Still, I am a fan of concept bottleneck models and I like the idea of broadening their applicability.  The only serious issue is with the experimental evaluation."
                },
                "questions": {
                    "value": "Please see my doubts about the experimental setup in the **Quality** paragraph above.  I'd appreciate if you could clarify what motivated the limited choice of data sets, for instance.  Also, please let me know if I got it wrong and missed results that are in fact reported in the paper.\n\n**Score**: I graded the paper 3/10, but I think of it as a 4/10 (there is no such option in openreview).  I *am* willing to increase the score if solid motivations are provided for the interpretability, debuggability, and FID score of CB-* models on data sets besides those reported here, or any other solid indication that CB-* models hold their promises beyond the few data sets considered here."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6823/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6823/Reviewer_5NVe",
                        "ICLR.cc/2024/Conference/Submission6823/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6823/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698747055926,
            "cdate": 1698747055926,
            "tmdate": 1700722529340,
            "mdate": 1700722529340,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jLjHtTlSlp",
                "forum": "L9U5MJJleF",
                "replyto": "UR4QVyJxkg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6823/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5NVe Part 1"
                    },
                    "comment": {
                        "value": "Thank you for the feedback. We each of your key points here.\n\n**Originality**\n___\nThank you for raising this point, we take it to mean that we should make the case for concept-bottleneck generative models more explicit in the paper. We addressed your originality point in the general comment, and have updated the first paragraph of the introduction to reflect the change. However, we make some additional points here. \n\n- **VLMs:** Current large-scale vision language models do not include intrinsically interpretable components that allow one to control and interpret/debug them. Importantly, when these models make mistakes, there is no way to identify the factors upon which they base generation, which makes them challenging to fix. As we show in Appendix C, a concept-bottleneck layer can be added to a VLM to better understand and fix the model\u2019s mistakes. Our proposal is not to replace VLMs/LLMs with CBGMs; to the contrary, we actually see our work as an initial step towards augmenting these models with intrinsically interpretable components that will make them easier to debug. We return to this point again in response to your concern about significance.\n\n- **Conditional Generation**: Our goal is not solely conditional generation. We seek a generative model with an intrinsically interpretable component that can be used to **simultaneously explain, debug, and steer** the model. Current conditional generation approaches do not come with the ability to explain the model.\n\n- **Glancenets (VAEs in concept-bottleneck):** As you mentioned, Glancenets are not model-agnostic (i.e., they are designed specifically for VAEs) and were not designed to support steerability.\n\n**Quality**\n___\nThank you for your feedback on the experiment section. We clarify here the goal of these sections. \n\n- **Steerability Metric**: The metric is exactly what you described. We start with a high-accuracy classifier trained on real data; we forgot to mention that we ensure that the minimum accuracy of any concept classifier is at least 98% on a held-out test set from the real data. We sample noise latent vectors and then use it to generate images from the model; for each concept we pass the generated image to the concept classifier; we save the latent vector if the classifier predicts the concept to be absent (i.e., the probability of the concept present is less than 0.5) we continue this process until we have 1K samples. For each conditional generation approach (including CB layer), we intervene on a concept and generate new images. We then pass the newly generated images to the classifier again and measure the fraction of inputs for which the indicates greater than 0.5 probability of concept presence, which we report in Table 1. The classifiers were trained on real data, but the steerability metric is on synthetically generated data, which might cause an adversarial effect or the classifier might consider the samples out-of-distribution causing the accuracy to be low. However, since we use the same classifier across all of the models and steerability approaches, we believe that the metric is fair i.e. the model ranking is reliable. The steerability test and metric that we measured is designed to be a difficult test that aligns with what people typically seek in practice. \n\n- **Intrinsic Interpretability and Debuggable Components**: We now clarify the interpretability and debugging experiments. First, the goal of the experiments that we show in the interpretability and debugging section is to demonstrate capabilities that were previously not possible for other generative and conditioning approaches. Specifically, we demonstrate that the output of the CB layer can be used to determine the features that the model relied on. We note that in the literature, quantifying interpretability and debuggability is an open problem that is often assessed via randomized control experiments. Here we argue that such an approach is not needed. The reason is that the output of the concept bottleneck is not a post-hoc explanation of the model, like feature attributions or linear probing approaches. We explicitly design the rest of the generative model to rely on the output of the concept bottleneck. One can think of the output of the CBM as analogous to the weights of a linear regression model. By design, these weights directly reveal feature importance.\n\n- **Assessing Interpretability**: To address your concern, we conduct a correspondence experiment. We generate 50 random samples and intervene on a subset of 2 concepts for these samples. We then inspect these 50 samples to assess whether the intervened samples are reflected in the generated output. This assessment communicates the extent to which the probability histogram can be used as a reliable interpretation. On the CB-VAE trained on Celeb-A 64 by 64, we find an 87 percent correspondence, and 96 percent for a CB-DDPM trained on Color-MNIST. We added this experiment to appendix Section B.4."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700344741909,
                "cdate": 1700344741909,
                "tmdate": 1700344741909,
                "mdate": 1700344741909,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yrt8hjCh3H",
                "forum": "L9U5MJJleF",
                "replyto": "UR4QVyJxkg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6823/Authors"
                ],
                "content": {
                    "title": {
                        "value": "End of discussion approaching"
                    },
                    "comment": {
                        "value": "Dear Reviewer  5NVe,\n\nGiven that the end of the discussion period is approaching, we would like to ask if you have any further concerns or questions, particularly as a follow-up to our response?\n\nThank you in advance!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580435446,
                "cdate": 1700580435446,
                "tmdate": 1700580435446,
                "mdate": 1700580435446,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w7x6V7a98E",
                "forum": "L9U5MJJleF",
                "replyto": "jLjHtTlSlp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6823/Reviewer_5NVe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6823/Reviewer_5NVe"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors 1/2"
                    },
                    "comment": {
                        "value": "Thank you for your detailed reply.\n\n> Thank you for raising this point, we take it to mean that we should make the case for concept-bottleneck generative models more explicit in the paper.\n\nThat would be great.\n\n> Our proposal is not to replace VLMs/LLMs with CBGMs; to the contrary, we actually see our work as an initial step towards augmenting these models with intrinsically interpretable components that will make them easier to debug.\n\nI can agree with this.  It would make sense to make this more explicit in the text.\n\n> Conditional Generation: Our goal is not solely conditional generation. We seek a generative model with an intrinsically interpretable component that can be used to simultaneously explain, debug, and steer the model. Current conditional generation approaches do not come with the ability to explain the model.\n\nAgreed.\n\n> Glancenets (VAEs in concept-bottleneck): As you mentioned, Glancenets are not model-agnostic (i.e., they are designed specifically for VAEs) and were not designed to support steerability.\n\nAlso agreed."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653018820,
                "cdate": 1700653018820,
                "tmdate": 1700653018820,
                "mdate": 1700653018820,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jM6iOQ5Tpr",
                "forum": "L9U5MJJleF",
                "replyto": "jLjHtTlSlp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6823/Reviewer_5NVe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6823/Reviewer_5NVe"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors 2"
                    },
                    "comment": {
                        "value": "> Steerability Metric: The metric is exactly what you described.\n\nThe extended description in your rebuttal clarifies things a lot.\n\n> the minimum accuracy of any concept classifier is at least 98% on a held-out test set from the real data\n\nThis is definitely worth mentioning explicitly.\n\n> One can think of the output of the CBM as analogous to the weights of a linear regression model. By design, these weights directly reveal feature importance.\n\nAgreed (for some notion of importance).\n\n> Assessing Interpretability: To address your concern, we conduct a correspondence experiment. We generate 50 random samples and intervene on a subset of 2 concepts for these samples. We then inspect these 50 samples to assess whether the intervened samples are reflected in the generated output. This assessment communicates the extent to which the probability histogram can be used as a reliable interpretation. On the CB-VAE trained on Celeb-A 64 by 64, we find an 87 percent correspondence, and 96 percent for a CB-DDPM trained on Color-MNIST. We added this experiment to appendix Section B.4.\n\nI see that Section B.4.2 is quite sparse.  Could you elaborate on what concepts were manipulated for the two data sets?"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653655212,
                "cdate": 1700653655212,
                "tmdate": 1700653655212,
                "mdate": 1700653655212,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aBE85UZDsZ",
            "forum": "L9U5MJJleF",
            "replyto": "L9U5MJJleF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6823/Reviewer_YbNz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6823/Reviewer_YbNz"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to introduce a \"concept bottleneck\" layer to generative models, allowing for better control and introspection of the generative process. The is demonstrated to work with several generative models - VAEs, GANs and diffusion models. In all method the encoded latent is fed to a set of \"concept networks\" - one for each conceptual property like colour, gender etc. - which are trained to project the latent on an embedding and predict the presence or absence (learned seperatly) of a specific context (with ground truth data provided as targets, i.e supervised). The generative model is trained in conjuction with the concept layers by using a linear combination of the concept embeddings weighted by their presence (or absence) probabilitliies. as input to the decoder (instead of the original latent). There is an additional orthogonality loss which constrains the concept embeddings to be orthogonal to an \"unknown\" concept embedding.\n\nControllability is achieved by constraining the presence or absence probabilities to a specific value and feeding the resulting combination to the generative process. Interpretability is achieved via inspecting the resulting presence / absence probablilites for different concepts.\n\nThe method is evaluated on several datasets with ground truth concept data and is shown to improve on baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think the paper has an interesting premise and good motivation - interpretability and controllability of generative models are important subjects and structured approaches to this can be useful.\n\nI enjoyed reading the paper and it is largely well presented and written."
                },
                "weaknesses": {
                    "value": "Unfortunately the paper suffers from several weaknesses:\n\n* The concept bottleneck layer requires ground truth data - this is fine for small-ish scale data, but we can't hope to obtain this kind of data for large datasets. This puts some doubt to the usefullness of the model.\n* The model requires a *separate network* for each concept - this is fine as long as there is a small number of concepts but I doubt this is a scalable approach going forward.\n* Because the model is supervised it will only learn about concepts provided through the labels - there could be an argument that concepts should be learned from the data unsupervised as we don't necessarily always know what are the underlying factors.\n* Evaluation is a bit weak in the paper - Table 1 which is arguably the main quantitative result of the paper is not well explained. If I understand correctly these are the prediced concept presence probablilites after the model has been constrained to include them. This is problematic because a) we don't know the accuracy of presence classifier and b) measuring only the probablitiy doesn't tell us if the output actually contains the desired concept (i.e - there are no visualizations). \n* In absolute terms, the results in Table 1. are quite poor - I would expect much higher accuracies."
                },
                "questions": {
                    "value": "* How does the method scale with the number of concepts? does it affect performance of the generative model?\n* Are all concepts equal? are there ones that affect the output of the model more than others?\n* What is the role of the \"unknown\" concept in generation? what happens if you steer it towards \"presence\" (if possible?)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6823/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6823/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6823/Reviewer_YbNz"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6823/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753452981,
            "cdate": 1698753452981,
            "tmdate": 1700649591106,
            "mdate": 1700649591106,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LMYExrbwEx",
                "forum": "L9U5MJJleF",
                "replyto": "aBE85UZDsZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6823/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YbNz Part 1"
                    },
                    "comment": {
                        "value": "Thank you for the detailed reading and feedback. First, we address the weaknesses in the following comments.\n\n**Re: Weaknesses**\n___\n\n- **Ground-truth Annotations**: Indeed, as currently formulated, the CB-generative models require annotations for the entire training set. However, this requirement can be easily relaxed. To directly address your concern, we performed a new set of experiments where we varied the percentage of concept annotation available for CBGMs. We added this experiment in Appendix B.3.3. We find that with about 20% of annotations, we can directly match steerability metrics of a fully annotated training set. This result is not surprising; in the interpretability literature, concept bottleneck models for classification papers (Kim et al. (2018) and Yuksekgonul et al. (2022)) have shown that it is possible to learn a high-performing concept classifier with as few as 200 samples. This experiment suggests that those results translate to the generative model setting as well.\n\n- **Scaling Concept Layer**: Increasing the number of concepts does increase the model parameters as in any concept bottleneck model. However we show in the paper, we can scale the number of concepts from 8 to 40 on the Celeb-A 64 by 64 dataset, and on the LAION dataset, we used 155 concepts. To address your concern we scaled LAION to 750 concepts which we were able to train using only 4 V100 GPUs. To clarify how the CB layer affects the number of parameters of a model we now discuss the total number of parameters that the CB layer adds. A standard CB layer consists of: $2k$ concept networks ($k$ for the positive concept states, and $k$ for the negative concept states), $k$ probability networks, and 1 unknown concept network. Similar to Espinosa et. al. (2022), we parameterize each concept network as a fully connected module; hence, each concept network has $am$ parameters where $a$ is the input dimension to the concept layer for a given instance, and $m$, is the concept embedding size. Consequently, for $2k$ networks, we have $2kam$ parameters for $2k$ concepts and $am$ parameters for the unknown concept network giving: $(2k+1)am$ parameters. Each probability network has $2m$ parameters leading to $2mk$ parameters for $k$ concept vectors, which in total means the CB layer adds: $(2k+1)am + 2km$ parameters to a given architecture if the networks are instantiated as fully connected layers. In practice, we share a single probability network across all concepts, which reduces the total number of parameters required as done by Espinosa et. al. (2022). For example, training CB diffusion models with 155 concepts required 120 million parameters scaling to 750 concepts increased the number of parameters to 250 million. Current hardware allows us to scale to multiple billion parameter models.\n\n- **Unsupervised Concepts:** We have two categories of concepts: known and unknown concepts. First, for the concepts that we want to be able to interpret and control, we need labels. **However, we allow for unknown concepts that are learned completely unsupervised except with the constraint that these concepts be orthogonal to the known concepts.** Supervision for the known concepts is important. As we referenced in the paper, Locatello et. al. (2019) show that one cannot learn, unsupervised, representations that are also disentangled. In addition, there is no guarantee that these representations should be human-understandable. This means that simply allowing of an entirely unsupervised representation will limit both interpretability and the ability to control the model. In fact, this challenge is why approaches like GANSpace resort to a PCA of the linear representations to \u2018search\u2019 for semantically meaningful and disentangled features. Consequently, our approach allows for both known and unsupervised concepts."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700343509014,
                "cdate": 1700343509014,
                "tmdate": 1700343509014,
                "mdate": 1700343509014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vxNJwtAhrA",
                "forum": "L9U5MJJleF",
                "replyto": "aBE85UZDsZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6823/Authors"
                ],
                "content": {
                    "title": {
                        "value": "End of discussion approaching"
                    },
                    "comment": {
                        "value": "Dear Reviewer  YbNz,\n\nGiven that the end of the discussion period is approaching, we would like to ask if you have any further concerns or questions, particularly as a follow-up to our response?\n\nThank you in advance!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580378878,
                "cdate": 1700580378878,
                "tmdate": 1700580378878,
                "mdate": 1700580378878,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ft14HiPd5F",
                "forum": "L9U5MJJleF",
                "replyto": "vxNJwtAhrA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6823/Reviewer_YbNz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6823/Reviewer_YbNz"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your detailed response!"
                    },
                    "comment": {
                        "value": "I appreciate the time taken to address the concerns I and the other reviewers raised. I found the responses mostly satisfactory and I am raising my score to 6 - I still think this work can be further refined and improved, but I do think there is an interesting contribution here which the ICLR community can benefit from."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649548902,
                "cdate": 1700649548902,
                "tmdate": 1700649548902,
                "mdate": 1700649548902,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bg9eUFmhtN",
            "forum": "L9U5MJJleF",
            "replyto": "L9U5MJJleF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6823/Reviewer_9WtF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6823/Reviewer_9WtF"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied concept bottleneck on a number of generative models, like VAE, GAN, and Diffusion. The work shows using concept bottleneck enables steering generative models and debugging better."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of introducing a concept bottleneck layer to generative models is very interesting. While this has been studied in discriminative models, this is the first time that the reviewer see concept bottleneck been used in generation, which provides explanation and control over the model generation.\n2. It shows better steerability than existing methods, like InfoGAN. Based on Table 1, CB-GAN has higher accuracy of the concept classifier. InfoGAN was a distangled GAN that allow control over concept in an unsupervised way.\n3. Experiments are conducted on three types of generative models, which shows the method is general."
                },
                "weaknesses": {
                    "value": "Lacking baseline comparison with SOTA model. Without concept bottleneck, exiting work can also control concept, like text-image diffusion, GANSpace, StyleGAN, and others.\n\nLike one can find a direction in StyleGAN that control specified concept, which is also very effective.\n\nSimilarly, show comparison to stable diffusion, GANSpace."
                },
                "questions": {
                    "value": "1. How does the method tackle unseen concepts? Can this be extended to open-world concepts?\n\n2. How long does diffusion training take? Do you train all generative models from scratch?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6823/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698877036439,
            "cdate": 1698877036439,
            "tmdate": 1699636789047,
            "mdate": 1699636789047,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PEgW3akGO3",
                "forum": "L9U5MJJleF",
                "replyto": "bg9eUFmhtN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6823/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9WtF"
                    },
                    "comment": {
                        "value": "Thank you for the detailed reading and feedback. First, we address the weaknesses and then respond to your questions point-by-point.\n\n**Re: Weaknesses**\n___\n- **Comparison to SOTA baselines**: As we stated in the general comment, our goal is to use an ***intrinsically interpretable component of a generative model to both steer, interpret, and debug the model***. As it stands, none of the baselines (stable diffusion, GANSpace, etc) accomplishes these goals. We now discuss each baseline in turn.\n\n    - **Text-to-image models (e.g. stable diffusion)**: These models allow for fine-grained control with text inputs. However, given a trained text-to-image model, it is still unclear what features the model relies on to generate its output. While these models allow for text-based control, they do not provide any insight into the models; hence, they are difficult to debug. As an example, in recent work, Tong and Jones et. al. (2023) show that text-to-image models can easily ignore critical words in an input prompt. For example, a Stable Diffusion XL model asked to generate: \u201ca woman proposing to a man\u201d instead produces an image of a man proposing to a woman. As we show in Figure 18, a concept bottleneck model inserted in a diffusion model can help identify the features the model bases its generation on, and hence can immediately point to how to address deficiencies with the model.\n\n    - **GANSpace**: As we stated, our goal is not to do post-hoc interpretation but to insert an intrinsically interpretable layer into the generative model. GANSpace uses PCA to identify the key directions/features in the representations of a GAN. The approach can then vary the representations along these directions to vary the output of the model. Our approach differs from theirs in several ways: 1) since we insert an interpretable layer into the model, we do not need to search for the representations in order to control them. 2) It is possible that the model representations might not encode for a feature that one might want to control. For example, in the Celeb-A setting, we might be interested in controlling the presence of Glasses. However, it is possible that none of the most important PCA directions correspond to Glasses. In contrast, our proposal directly encodes the `glasses' concept as part of the model\u2019s latent representation, so we can immediately identify whether the model has been able to learn that concept.\n\n    - **StyleGAN:** We agree that the StyleGAN model has been used to demonstrate impressive steering/control capabilities. Controlling styleGAN typically can be done in different ways, either by conditioning where the concepts are concatenated to the latent vector, classifier guidance, or model editing methods such as GANSpace as discussed previously. Such methods enable steerability (although we found that for steerability CB layer outperforms conditioning and classifier guided methods as shown in Table 1), but it does not allow for interpretability (i.e we do not understand which concepts the model relied on during generation) and debuggability (i.e we can not tell if the model actually learned a certain concept during training). An alternative approach is adding a CB layer to styleGAN, by doing this, we can explicitly learn the desired concept, we eliminate the need to search for weights correlated with concepts as we know ahead of time which parameters will encode for a particular concept, the concept layer allows us to debug and interpret the generative model during and after training. During training, we can identify which concepts the model is currently unable to encode, and post-training, we can use the output of the concept layer to identify the key features responsible for the model\u2019s output. To address your concern, we added a section in the appendix explaining how one can create CB-StyleGAN; please find the full experiment and results in Appendix B.1.1 (in the updated paper).\n\n**Re: Questions**\n___\n- **Unseen Concepts**: As we discuss in the paper, we partition concepts into two categories: known (pre-determined human understandable ones), and unknown concepts. The unknown concepts indeed directly encode for unseen concepts for which the model designer did not specify ahead of time in the known set. Your suggestion about open world concepts is interesting, we did not consider it in our initial formulation. However, we can easily adapt the current formulation to allow for a dynamic set of concepts. We leave an extension to open-world concepts for future work.\n\n- **Diffusion Model**: Training diffusion on LAION required 240 V100 GPU hours, training diffusion on Celeb-A required 112 V100 GPU hours. We have updated the manuscript to include the compute time.\n\n- **Training from scratch:** Yes all models are trained from scratch.\n\n\n**References**\n- Shengbang Tong, Erik Jones, and Jacob Steinhardt. Mass-producing failures of multimodal systems with\nlanguage models. arXiv preprint arXiv:2306.12105, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700343140023,
                "cdate": 1700343140023,
                "tmdate": 1700343140023,
                "mdate": 1700343140023,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q1nJ6vI1pq",
                "forum": "L9U5MJJleF",
                "replyto": "bg9eUFmhtN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6823/Authors"
                ],
                "content": {
                    "title": {
                        "value": "End of discussion approaching"
                    },
                    "comment": {
                        "value": "Dear Reviewer  9WtF,\n\nGiven that the end of the discussion period is approaching, we would like to ask if you have any further concerns or questions, particularly as a follow-up to our response?\n\nThank you in advance!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580253016,
                "cdate": 1700580253016,
                "tmdate": 1700580253016,
                "mdate": 1700580253016,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MSeoWCd5oS",
            "forum": "L9U5MJJleF",
            "replyto": "L9U5MJJleF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6823/Reviewer_vC1Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6823/Reviewer_vC1Y"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a concept bottleneck (CB) based approach to make generative models interpretable. Specifically, it proposes to insert a CB layer into generative models to constraint (most of) their internal representation to a certain concept set, making it quantifiable with respect to the concepts. The key challenge the paper highlight is that features to perform generative modeling may not be always interpretable. To address this, the paper propose to extend previous Concept Embedding Models (CEMs) to additionally encode an \"unknown\" concept that is meant to be orthogonal to any other concepts. The experimental results show that the proposed approach achieves better accuracy on steering concepts compared to other conditional generative modeling approaches, and providing additional features such as interpretability and debugging of the individual models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper addresses an important yet under-explored problem of interpreting generative models.\n- I found the idea of introducing unknown concept vector is interesting and novel. \n- The proposed method is widely applicable for diverse model families, including VAEs, GANs, and Diffusion models. \n- The paper also partially demonstrates the scalability of the method with a LAION subset on Diffusion model, following contemporary practices.\n- The effectiveness of the method is clearly validated through experiments."
                },
                "weaknesses": {
                    "value": "- The paper claims at Abstract and Introduction that the proposed method is model agnostic, but I am not certain on that. For example, it seems to me that applying the CB layer to other types of generative models can be non-trivial, e.g., to normalizing flows and invertible models. \n- Although the paper presents some scalability experiments, the other parts of the experiments can be seen as somewhat limited in their scale. For example, the paper only explores simple GAN architectures not covering modern ones such as StyleGAN-2. I am also wondering why more recent methods such as GAN inversion could not be a baseline. If these point can be addressed, it will be also beneficial to support the model-agnostic aspect of the method. \n- More qualitative comparisons could be added and highlighted for the steerability experiments, given that the quantitative results are dependent on the pre-trained concept classifiers. \n- (minor) The definition of concept orthogonality loss, Eq (5), is written in somewhat confusing manner - e.g., the index j is not used in the definition."
                },
                "questions": {
                    "value": "- The paper mentions Platt and Barr (1987) regarding hyper-parameter optimization. Does it mean that the paper actually applied that method for tuning in the experiments? If so, it may be good to provide an overview on the method in the paper as well. \n- It seems making sure the minimal concept orthogonality loss is crucial for the soundness of the method, otherwise there can be an overlap between the given concepts and the \"unknown\" concept. One immediate ablation one can try is to check whether the loss is minimized is to strictly project the learned unknown context vector to be orthogonal to other vectors and see if there is degradation in performance. Or I think this kind of procedure can be even incorporated into the training phase to guarantee the minimal concept orthogonality loss. Any discussion regarding the actual orthogonality of the learned unknown context vector would be helpful for the readers."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6823/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698961976954,
            "cdate": 1698961976954,
            "tmdate": 1699636788948,
            "mdate": 1699636788948,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tJ6dpu7yoK",
                "forum": "L9U5MJJleF",
                "replyto": "MSeoWCd5oS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6823/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vC1Y"
                    },
                    "comment": {
                        "value": "Thank you for the detailed reading and feedback. First, we address the weaknesses and then respond to your questions point-by-point.\n\n**Re: Weaknesses**\n___\n\n- **Abstract, Introduction, and Normalization Flows**: Thank you for raising this important point. Indeed, we tested three types of generative models in the draft. However, we found your suggestion of normalizing flows and invertible models intriguing, so we tested our proposed approach on a Normalizing flow model trained on Color MNIST, we added this experiment in Section B.1.2 of the Appendix. Our CB layer proposal translates, as is, to such models as well. We show examples generated from the model in Section B.1.2 of the Appendix. We expect future work to test our findings more exhaustively and extend them to other families of generative models. We take your feedback to mean that we should explicitly specify the class of models we focus on, which we have done in the updated manuscript.\n\n- **Not covering modern ones such as StyleGAN2:** As per your request, we added a CB layer to StyleGAN2 in Appendix B.1.1 (in the updated paper). For StyleGAN, the CB layer was added after the mapping network in the generator, an encoder was also added before the generator to ensure that the generated image matches the concepts constrained in the loss, the exact architecture is shown in appendix B.1.1 Figure 7 (a). We trained StyleGAN2 on Celeb-A 64x64 and CUB 128x128. We added images generated by CB-styleGAN2 for both datasets in the appendix Figure 7 (b) and reported FIDs as well. We find that using more complicated architectures improves the quality of the generated image (as expected) and inserting a CB layer does not degrade the generation quality.\n\n- **GAN inversion baseline**: As we mentioned in the general comment, while GAN inversion approaches enable a model to be steered, these approaches do not address the goals that we set forth to tackle in this work. To re-iterate: **we seek an intrinsically interpretable component of a generative model that allows us to steer and interpret the output of the generative model simultaneously.** Meanwhile, GAN inversion approaches do not provide any form of understanding or insight into which features the model bases its generation on. In addition, we require that the approach be intrinsic to the model and not a post-hoc approach. Critically, a GAN inversion approach cannot indicate the key features that a model relies on for its output. Similarly, to the best of our knowledge, most GAN inversion techniques do not explicitly insert an interpretable component into the model. We think the GAN inversion approaches address an important challenge; however, our goals are orthogonal to these challenges.\n\n- **Qualitative Results:**  As you requested, we updated the Appendix to include several additional qualitative steerability experiments from StyleGAN2; see  Figure 11 in the appendix.\n\n- **Concept Orthogonality Loss:** Thank you for noting this issue, $j$ is the sample in the mini-batch, we have updated the main text under Eq 5 to clarify this.\n\n**Re: Questions**\n___\n\n- **Optimization**: Thank you for raising this important point. On the VAE model we compared Platt and Barr to a standard grid search/sweep over a hyper-parameter set. We didn\u2019t observe a substantial improvement in model FID scores for the subset of models trained. We used an open-source Pytorch implementation of the Platt and Barr approach, which does not require any modification for the setting considered here. The approach minimizes a loss function that is subject to bound constraints with arbitrary parameters that weight the importance of these functions. We added this discussion in Section B.2 in the appendix; we also referred to the open-source implementation used.\n\n- **Orthogonality:** We\u00a0agree\u00a0with your assessment that it is crucial for the unknown concept embedding to be orthogonal to known concept embeddings.\u00a0Removing orthogonality between the known and unknown concepts causes steerability degradation, as shown in the ablation Table 3.\u00a0Our current algorithm encourages orthogonality by adding the loss in equation 5 but does not enforce it.\u00a0We tried forcing orthogonality during training (as you suggested) by projecting the concepts embedding to be orthogonal to non-concepts embedding during training using the Gram-Schmidt Process [1];\u00a0we found that this makes training unstable and harms generation.\u00a0We note that there are other schemes that can achieve independence, including KL minimization, mutual information minimization, etc. We leave exploring different orthogonality schemes as future\u00a0work.\n\n[1] Schmidt E.Zur Theorie der linearen und nichtlinearen Integralgleichungen I. Teil: Entwicklung willk\u00fcrlicher Funktionen nach Systemen vorgeschriebener.\u00a0Mathematische Annalen\u00a01907;\u00a063:\u00a0433\u2013476."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700342722913,
                "cdate": 1700342722913,
                "tmdate": 1700345489059,
                "mdate": 1700345489059,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2JSI4nU8jn",
                "forum": "L9U5MJJleF",
                "replyto": "MSeoWCd5oS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6823/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6823/Authors"
                ],
                "content": {
                    "title": {
                        "value": "End of discussion approaching"
                    },
                    "comment": {
                        "value": "Dear Reviewer vC1Y,\n\nGiven that the end of the discussion period is approaching, we would like to ask if you have any further concerns or questions, particularly as a follow-up to our response?\n\nThank you in advance!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6823/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580213251,
                "cdate": 1700580213251,
                "tmdate": 1700580213251,
                "mdate": 1700580213251,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]