[
    {
        "title": "Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers"
    },
    {
        "review": {
            "id": "h0CC9XNhla",
            "forum": "dgmcE0RsTi",
            "replyto": "dgmcE0RsTi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6271/Reviewer_pFUP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6271/Reviewer_pFUP"
            ],
            "content": {
                "summary": {
                    "value": "The paper develops a way to use pre-trained transformers efficiently on long sequences without the need for further pre-training due to architectural changes.\n\nThe method first creates non-overlapping chunks to apply Transformer attention locally only (within chunks). After each layer, it does an elementary form of inter-chunk interaction by updating each bos and eos special tokens (indicating chunk boundaries within each chunk) with the average of newly produced eos and bos tokens (after the earlier local chunk processing layer) respectively. \n\nThe method uses reinforcement learning for token selection for decoding with rewards created to penalize selecting too many tokens and other rewards based on language modeling probabilities. PPO is used in an actor-critic framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. While the literature is saturated with efficient transformers for handling long sequences, this work can be applied to pre-trained models like BART directly without additional pre-training. \n\n2. The performance is quite decent compared to baselines."
                },
                "weaknesses": {
                    "value": "The method could be counted as a form of dynamic pruning technology. There are already several works in that area. For example, transkimmer [1] already has dynamic token pruning. And there are other works in similar directions [2]. The usability of RL for token pruning is not a particularly surprising method, and chunking + local attention is a very standard policy for efficiency gain. More research can be done in contextualizing the work in the literature review by exploring other related works to [1,2] in the citation network. That said, The inter-chunk communication through average is interesting (although quite simple and could be seen as a hack)  and the overall synthesis seems to work well.\n\n\n[1] Transkimmer: Transformer Learns to Layer-wise Skim - Guan et al. ACL 2022\n\n[2] Learned Token Pruning for Transformers - Kim et al. KDD 2022"
                },
                "questions": {
                    "value": "1. Is the chunking method only applied to the encoder? Would there be any problem in applying it to the decoder and decoder-only models (standard LLMs) after appropriate changes (such as causality constraint in averaging for chunk alignment)? \n\n2. Is there a graph for per-iteration speed-up comparisons with baseline?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6271/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723763237,
            "cdate": 1698723763237,
            "tmdate": 1699636686617,
            "mdate": 1699636686617,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oiWp3tzLLt",
                "forum": "dgmcE0RsTi",
                "replyto": "h0CC9XNhla",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Thank you for the valuable reviews, here are our replies.\n\n> The method could be counted as a form of dynamic pruning technology. There are already several works in that area. For example, transkimmer [1] already has dynamic token pruning. And there are other works in similar directions [2]. The usability of RL for token pruning is not a particularly surprising method, and chunking + local attention is a very standard policy for efficiency gain. More research can be done in contextualizing the work in the literature review by exploring other related works to [1,2] in the citation network. That said, The inter-chunk communication through average is interesting (although quite simple and could be seen as a hack) and the overall synthesis seems to work well.\n>\n> [1] Transkimmer: Transformer Learns to Layer-wise Skim - Guan et al. ACL 2022\n>\n> [2] Learned Token Pruning for Transformers - Kim et al. KDD 2022\n\nAs mentioned in the related works, token pruning is a common technique used to reduce the overhead of training and inference. However, most existing methods are only suitable for short sequence tasks and are often not applicable to the case of long sequence inputs. In contrast to previous work, we propose a selection strategy in the context of chunking to choose hidden representations, significantly improving model performance. Although the use of RL for token pruning has been explored, these methods typically only use the final output signal (such as cross-entropy loss in TR-BERT [1]) as a coarse-grained reward for training the selection strategy. In our method, we additionally utilize attention signals to design the more fine-grained reward for each action. Our experiment results demonstrate the effectiveness of our approach.\n\n[1] Ye D, Lin Y, Huang Y, et al. Tr-bert: Dynamic token reduction for accelerating bert inference[J]. arXiv preprint arXiv:2105.11618, 2021.\n\n> Is the chunking method only applied to the encoder? Would there be any problem in applying it to the decoder and decoder-only models (standard LLMs) after appropriate changes (such as causality constraint in averaging for chunk alignment)?\n\nIn our paper, the chunking method is only used for the encoder. The method we proposed in the paper does not adapt directly to the decoder-only model. The key issues are how to conduct the inter-chunk communication in the decoder-only model and how to design rewards using decoder self-attention.\n\n> Is there a graph for per-iteration speed-up comparisons with baseline?\n\nAccording to your suggestion, we provide these graphs for per-iteration speed-up comparison with SLED on all our test sets in the supplementary materials. From these graphs, we can see that compared to the previous chunking method SLED, our method significantly reduces the time overhead. At the same time, the experimental results in the paper also show that the model performance has greatly improved."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568612406,
                "cdate": 1700568612406,
                "tmdate": 1700568612406,
                "mdate": 1700568612406,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9rSc2j1oju",
            "forum": "dgmcE0RsTi",
            "replyto": "dgmcE0RsTi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6271/Reviewer_9Rva"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6271/Reviewer_9Rva"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduced a Simple learning framework with three typical operations: Chunk, Align, and Select to enable pre-trained encoder-decoder language models to process long-context. The chunking process is a variant of batchfying. The aligning process is the forwarding via BART or other LM encoders. The proposed selector network as well as the RL-based training is novel and interesting. The experiments are very solid, which covers almost all long-context datasets as far as I know."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The experiments are comprehensive and the results are robust and strong. The evaluation tasks and datasets coverage is excellent.  I appreciate that the authors conduct experiments on NarrativeQA to demonstrate the method's effectiveness and the NarrativeQA is recognized as the hardest benchmark for long-context transformer.\n\n2. The method is intuitive and easy to understand. Even if the chunking and selection/retrieval is not that novel, the introduced PPO-based training for selector network is interesting and brings some insights."
                },
                "weaknesses": {
                    "value": "1. The computational timecost brought by retrieval or selection based methods are always a significant issue. I did not find such discussions in main context and appendix about the latency part. The author should discuss and measure it to show the tradeoff between performance gain and latency increase.\n\n2. As the selector FFN is randomly initialized but the backbone is pre-trained well, I think in the first 10k training iterations, the training might be unstable if following the alternative updates on a well-trained model and a newly-initialized model. The author provides limited details about this.\n\n3. The ablation study on the chunk length might be interesting and important. A smaller chunk length brings better granularity and a larger chunk length accelerates the inference. Is the selected 512 length the best length for chunks?"
                },
                "questions": {
                    "value": "1. For footnote 1 of Page 3, how you deal with cases that a sentence is longer than a chunk as sometimes the sentence segmentation tool does not work well to split the sentence to desired length?\n\n2. The selector is a small-size 8M FFN. Did you consider to scale up the parameters and also change the FFN to a RoBERTa model with binary classification head? This might be better for the action decision. \n\n3. If I understands the paper well, the selector is not pre-trained and is only randomly initialized FFN. Then you fine-tune the BART and selector together on specific task. Did you try to freeze BART and pre-trained the FFN selector on the same pre-training corpus of BART? Personally, I think if your selector FFN is only 8M parameters, the pre-training might not be helpful. But if the selector size is scaled up, this might be good to the model.\n\n4. Some missing related works: TRIME (Zhong et. al., 2023) and LongMem (Wang et. al., 2023) are related to this method in terms of chunking and memorizing; Landmark Attention (Mohtashami and Jaggi, 2023) is related to this method in terms of S and E tokens."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6271/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6271/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6271/Reviewer_9Rva"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6271/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792424188,
            "cdate": 1698792424188,
            "tmdate": 1699636686429,
            "mdate": 1699636686429,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ns0izUBJCE",
                "forum": "dgmcE0RsTi",
                "replyto": "9rSc2j1oju",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for the valuable reviews, here are our replies.\n\n> The computational timecost brought by retrieval or selection based methods are always a significant issue. I did not find such discussions in main context and appendix about the latency part. The author should discuss and measure it to show the tradeoff between performance gain and latency increase.\n\nAt the inference stage, the average time overhead of the selection process (Select) and the whole process (All), and their ratio (Ratio)  for a single sample are shown below. In addition, it should be noted that in our experiments, we used the V100 which has a relatively slow computation speed. If more advanced GPUs are used, the latency will be significantly reduced. \n\n|                | arXiv  | PubMed  | GovReport | SummScreen | Multi-News |  WCEP  | NarrativeQA |\n| :------------: | :----: | :-----: | :-------: | :--------: | :--------: | :----: | :---------: |\n| Selection (ms) | 157.5  |  70.4   |   293.9   |   103.4    |    74.2    |  9.8   |    112.4    |\n|    All (ms)    | 9661.5 | 13799.1 | 104953.7  |  30424.0   |  17664.2   | 1356.2 |    689.6    |\n|   Ratio (%)    |  1.63  |  0.51   |   0.28    |    0.34    |    0.42    |  0.72  |    16.30    |\n\n> As the selector FFN is randomly initialized but the backbone is pre-trained well, I think in the first 10k training iterations, the training might be unstable if following the alternative updates on a well-trained model and a newly-initialized model. The author provides limited details about this.\n\nActually, we were surprised to find that the transformer\u2019s MLE loss with label smoothing decreases quite normally during the training phase in most cases. We believe this may be because, although the selector is randomly initialized, it only participates in the selection of the transformer\u2019s hidden representations and does not severely disrupt the representation information. Therefore, the training of the transformer is relatively stable. We have submitted the figures of MLE loss changes during the training process to the supplementary materials and included it in the appendix of the paper.\n\n> The ablation study on the chunk length might be interesting and important. A smaller chunk length brings better granularity and a larger chunk length accelerates the inference. Is the selected 512 length the best length for chunks?\n\nIn our experiments, we found that with the same input length, chunk sizes 512 and 1024 have similar model performance, and the model performance significantly decreases when the chunk size is 256. Therefore, in order to process longer sequences while maintaining sound model performance, we set the chunk size to 512.\n\n> For footnote 1 of Page 3, how you deal with cases that a sentence is longer than a chunk as sometimes the sentence segmentation tool does not work well to split the sentence to desired length?\n\nIn fact, although it is rare, we have indeed observed some sentences that are longer than the chunk size. In this case, we will directly discard the part of the sentence that exceeds the chunk size."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568547477,
                "cdate": 1700568547477,
                "tmdate": 1700568547477,
                "mdate": 1700568547477,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K8tC1oVYj4",
                "forum": "dgmcE0RsTi",
                "replyto": "9rSc2j1oju",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (Part 2)"
                    },
                    "comment": {
                        "value": "> The selector is a small-size 8M FFN. Did you consider to scale up the parameters and also change the FFN to a RoBERTa model with binary classification head? This might be better for the action decision.\n\nIn our method, it is feasible to improve the model performance by increasing the scale of the parameterized selector. However, considering the time overhead brought by the selection process, we hope that the increased overhead is small and the system still has good performance. In my opinion, changing FFN to RoBERTa can improve the model performance, but the time overhead of the selection process may be large.\n\n> If I understands the paper well, the selector is not pre-trained and is only randomly initialized FFN. Then you fine-tune the BART and selector together on specific task. Did you try to freeze BART and pre-trained the FFN selector on the same pre-training corpus of BART? Personally, I think if your selector FFN is only 8M parameters, the pre-training might not be helpful. But if the selector size is scaled up, this might be good to the model.\n\nBecause we consider the compatibility of our method with existing pretrained language models as one of the contributions, we did not conduct additional pre-training. Theoretically, the current selector\u2019s model size is relatively small, making it difficult to achieve good generalization through pre-training. However, pre-training a larger scale selector may improve the model effect, but the time latency will also increase.\n\n> Some missing related works: TRIME (Zhong et. al., 2023) and LongMem (Wang et. al., 2023) are related to this method in terms of chunking and memorizing; Landmark Attention (Mohtashami and Jaggi, 2023) is related to this method in terms of S and E tokens.\n\nThank you for bringing these papers to our attention. We appreciate your suggestion and agree that these works are indeed relevant to our study. We will review these papers and discuss their relation to our method in the revised manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568581805,
                "cdate": 1700568581805,
                "tmdate": 1700568581805,
                "mdate": 1700568581805,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gqDiAoKglO",
            "forum": "dgmcE0RsTi",
            "replyto": "dgmcE0RsTi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6271/Reviewer_5A1y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6271/Reviewer_5A1y"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an approach concentrated on the refinement of long-sequence modeling. The proposed method divides a sequence into a series of chunks. This technique meticulously aligns the inter-chunk information during the encoding phases, and subsequently, the most pivotal hidden states are discerningly selected from the encoder to facilitate the decoding process. Experiment results show that in long-text abstractive summarization and reading comprehension tasks, the proposed method outperforms strong baselines of long-sequence processing."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors propose a simple framework that can directly be used on existing PLMs for processing long sequences.\n2. The authors propose a RL method to facilitate the transformer to concentrate more effectively on the crucial encoded hidden states.\n3. Experiments show better results over strong baselines."
                },
                "weaknesses": {
                    "value": "1. Missing related work. Long sequence Transformer is a hot topic, including two main directions: efficient computation or length extrapolation (train-short-test-long). In the realms of long-sequence Transformers, there appears to be a noticeable omission in the exploration of length extrapolation. Here are two related studies:\n   1) A Length-Extrapolatable Transformer\n   2) Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\n2. The proposed RL objectives lack solid motivation. It is well-known that RL objectives are hard to implement due to the variance of reward scores. There lacks necessary evidence to show how these RL objectives work and whether these RL objectives are necessary (compared to traditional MLE loss). \n3. The authors only conduct experiments on BART models.  It remains unclear whether the proposed method still works on more recent models, like LLAMA."
                },
                "questions": {
                    "value": "1. How about the PPL scores on long text modeling? It is a widely-used metric to evaluate the performance of  long-text language modeling.\n2. A minor stylistic observation pertains to the line spacing within the background section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6271/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698851752294,
            "cdate": 1698851752294,
            "tmdate": 1699636686307,
            "mdate": 1699636686307,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cknQVXEIxV",
                "forum": "dgmcE0RsTi",
                "replyto": "gqDiAoKglO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Thank you for the valuable reviews, here are our replies.\n\n> Missing related work. Long sequence Transformer is a hot topic, including two main directions: efficient computation or length extrapolation (train-short-test-long). In the realms of long-sequence Transformers, there appears to be a noticeable omission in the exploration of length extrapolation. Here are two related studies:\n>\n> 1. A Length-Extrapolatable Transformer\n> 2. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\n\nWe thank the reviewer for these provided related studies and will modify the manuscript to include the content for the length extrapolation.\n\n> The proposed RL objectives lack solid motivation. It is well-known that RL objectives are hard to implement due to the variance of reward scores. There lacks necessary evidence to show how these RL objectives work and whether these RL objectives are necessary (compared to traditional MLE loss).\n\nIn our method, we use a transformer as a simulation environment to optimize a reinforcement learning selection strategy for selecting crucial token hidden representations. We design rewards using the attention and output signals of the transformer, and obtain and update states using the transformer\u2019s hidden representation. Due to the lack of ground truth to determine the importance of hidden representations, and the tendency of attention scores to shift as training progresses, we believe that reinforcement learning methods, especially these algorithms like PPO that can maintain the exploration characteristics, can handle this situation well. In addition, since the \u201cChunk\u201d stage transforms long sequences into multiple blocks, selecting from one block at a time step can also be intuitively seen as a Markov Decision Process. In our paper, there are many empirical results that prove the effectiveness of our RL selection strategy. For example, in Table 4, we observed that adding the \u201cSelect\u201d module can greatly improve the performance of the model. In fact, in our early research, we did not have an RL selection module, and the performance of the model at that time was worse than the current strong baselines. In addition, in Figure 3, from the left graph, we can see that with the progress of training, the number of selected hidden representations first drops rapidly, and then converges to around our preset value (2048). From the right graph, we can see that even though the input length at the inference stage is exponentially increased by adding pad tokens, the number of selected hidden representations still grows slowly. This indicates that our selector has the ability to filter out low-contribution representations. Based on these results, we believe that our RL objective is effective and necessary.\n\n> The authors only conduct experiments on BART models. It remains unclear whether the proposed method still works on more recent models, like LLAMA.\n\nOur method is applicable to most pre-trained encoder-decoder models, including BART, PEGASUS, T5, LED and PRIMERA. For decoder-only models like LLAMA, we believe that although our method cannot be applied directly, it still has high reference value.\n\n> How about the PPL scores on long text modeling? It is a widely-used metric to evaluate the performance of long-text language modeling.\n\nFollowing recent long-sequence processing work such as Unlimiformer [1], we use commonly used metrics such as ROUGE and BERTScore in specific downstream tasks for more persuasive model performance comparisons. Thank you for your suggestion, we have added the PPL results on the test set.\n\n|                        | arXiv | PubMed | GovReport | SummScreen | Multi-News | WCEP | NarrativeQA |\n| ---------------------- | ----- | ------ | --------- | ---------- | ---------- | ---- | ----------- |\n| BART$_{base}$          | 3.46  | 3.55   | 3.74      | 3.89       | 3.90       | 3.93 | 3.15        |\n| BART$_{base}$ + SimCAS | 1.93  | 1.72   | 1.83      | 2.38       | 2.11       | 2.18 | 1.55        |\n\n[1] Bertsch A, Alon U, Neubig G, et al. Unlimiformer: Long-range transformers with unlimited length input[J]. arXiv preprint arXiv:2305.01625, 2023.\n\n> A minor stylistic observation pertains to the line spacing within the background section.\n\nWe will review the line spacing in the background section and make necessary adjustments to improve readability and conform to the formatting guidelines. Your attention to detail is much appreciated."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568385197,
                "cdate": 1700568385197,
                "tmdate": 1700568385197,
                "mdate": 1700568385197,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qs65iImqd2",
            "forum": "dgmcE0RsTi",
            "replyto": "dgmcE0RsTi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6271/Reviewer_mi4g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6271/Reviewer_mi4g"
            ],
            "content": {
                "summary": {
                    "value": "This paper induces attention sparsity by selecting the positions of Key and Value in \\textbf{cross-attention}.  The proposed method first chunks a sequence into blocks, then aligns the bos and eos of each block by using the average of them in every block of the next layer, in the last, it filters the positions of each block according to language modeling likelihood. This paper experiments on many summarization datasets and improves the baseline model Bart by a large margin. The main improvement comes from chunking and selecting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper shows that sparsification in cross-attention has a surprising potential for performance improvement, the proposed method improves about 20% over baselines."
                },
                "weaknesses": {
                    "value": "1. The proposed method depends on cross-attention, and we could not introduce it to the encoder-only or decoder-only Transformer model.   If it can outperform existing LLMs, this would not be a weakness.\n\n2. Experiments on other tasks are limited. This paper mainly experiments on summarization tasks, but does not experiment on CNN/DM or Xsum, which are the most compared data. For other tasks, this paper only does Narrative QA.  More experiments on document translation or classification would be an advantage."
                },
                "questions": {
                    "value": "1. Do the baselines sparse self-attention?\n\n2. There is a typo in Figure 3, the blue line should be selected length.\n\n3. Table 4 should add a row of pure baseline, i.e., w/o neither. It is a surprise that we need both chunk and select, or we will lose the baseline.\n\n4. I would like to see a comparison regarding latency."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6271/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699122251784,
            "cdate": 1699122251784,
            "tmdate": 1699636686195,
            "mdate": 1699636686195,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tMoBPathzX",
                "forum": "dgmcE0RsTi",
                "replyto": "qs65iImqd2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6271/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6271/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Thank you for the valuable reviews, here are our replies.\n\n> The proposed method depends on cross-attention, and we could not introduce it to the encoder-only or decoder-only Transformer model. If it can outperform existing LLMs, this would not be a weakness.\n\nIn our experiment, we utilized the encoder-decoder transformer BART as the backbone. This choice was made because, among pre-trained language models of a relatively smaller scale, this type of transformer architecture often exhibits superior performance in downstream tasks. Simultaneously, our method is well-suited to the characteristics of the encoder-decoder transformer, resulting in a low degree of coupling between our method and this type of transformer. For encoder-only and decoder-only transformers, our method may not be applied directly, but it still holds sound reference value for  long-sequence processing of encoder-only model and decoder-only model, and the environment construction of RL-based selection policy.\n\n> Experiments on other tasks are limited. This paper mainly experiments on summarization tasks, but does not experiment on CNN/DM or Xsum, which are the most compared data. For other tasks, this paper only does Narrative QA. More experiments on document translation or classification would be an advantage.\n\nThe CNN/DM and XSum datasets are commonly used for text summarization tasks. However, the average input text length for the former is 766.1, while for the latter it is 430.2. Therefore, standard full-attention models can already handle them well on consumer-grade GPUs, eliminating the need to introduce long sequence processing techniques. Based on your suggestions, we have incorporated additional experiments on the IMDB document classification dataset. The results are presented as follows.\n\n|   Method   | Accuracy |\n| :--------: | :------: |\n|  RoBERTa   |   95.0   |\n|  BIGBIRD   |   95.2   |\n| Longformer |   95.7   |\n|   SimCAS   |   96.1   |\n\n> Do the baselines sparse self-attention\n\nIn the baselines we demonstrated, LED, PRIMERA, and BIGBIRD have implemented sparse self-attention in encoder module.\n\n> There is a typo in Figure 3, the blue line should be selected length.\n\nThank you for pointing out this issue. We will correct this error in the new version.\n\n> Table 4 should add a row of pure baseline, i.e., w/o neither. It is a surprise that we need both chunk and select, or we will lose the baseline.\n\nIn Table 4, we demonstrate the contribution of each part in SimCAS to the performance improvement when the backbone is BART. If both the \u201cChunk\u201d and \u201cSelect\u201d modules are removed, the model will degrade to the standard BART. The relevant experimental results have already been shown in BART+Standard (Table 1 2 3):\n\n> I would like to see a comparison regarding latency.\n\nBased on a V100 and the backone BART$_{base}$, we tested the average inference time (ms) of a single sample in each test set for SLED and SimCAS, and the results are as follows:\n\n| Method |  arXiv  | PubMed  | GovReport | SummScreen | Multi-News |  WCEP  | NarrativeQA |\n| :----: | :-----: | :-----: | :-------: | :--------: | :--------: | :----: | :---------: |\n|  SLED  | 28375.3 | 25323.0 | 153560.5  |  59012.7   |  37253.0   | 3120.6 |   1711.5    |\n| SimCAS | 9661.5  | 13799.1 | 104953.7  |  30424.0   |  17664.2   | 1356.2 |    689.6    |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6271/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568243113,
                "cdate": 1700568243113,
                "tmdate": 1700568243113,
                "mdate": 1700568243113,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]