[
    {
        "title": "Best Arm Identification for Stochastic Rising Bandits"
    },
    {
        "review": {
            "id": "usESnQ2RWn",
            "forum": "nji0ztL5rP",
            "replyto": "nji0ztL5rP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6158/Reviewer_nQcR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6158/Reviewer_nQcR"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of fixed-budget best arm identification (BAI) within stochastic rising bandits. Specifically, it introduces both pessimistic and optimistic estimators for algorithm design. Building upon these estimators, two distinct algorithms emerge: R-UCBE and R-SR, drawing inspiration from UCB-E and SR as presented in Audibert et al. (2010). Regarding theoretical findings, the authors provide guarantees on the error probability of the two algorithms and investigate the minimal time budget $T$ required for the BAI task. Finally, numerical experiments conducted on synthetically generated data as well as a practical online best model selection problem serve to affirm the superiority of the proposed algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is clear and well-organized.\n2. The theoretical guarantee is exhaustive. Both the error probability and the minimum required time budget to accurately identify the optimal arm are taken into account.\n3. The numerical experiments are impressive and comprehensive. The proposed algorithms clearly outperform the baselines."
                },
                "weaknesses": {
                    "value": "1. While Assumption 2.1 appears intuitive, Assumption 2.2 falls short of being satisfactory. Even though the authors present some theoretical findings solely under Assumption 2.1, their interpretability is somewhat lacking.\n\n2. The proposed algorithm closely resembles UCB-E and SR for standard multi-armed bandits, with the primary distinction lying in the estimators. I'm not suggesting this is unacceptable, but it does somewhat diminish the novelty of this work. A promising future direction would involve integrating both estimators into a unified algorithm.\n\n3. Since the expected rewards are bounded in $[0,1]$ and non-decreasing, they must converge to some value. Thus, it is not surprising that the error probability lower bound will be matched by R-SR for large $T$. For non-stationary BAI, the algorithm SR is minimal optimal up to constant factors."
                },
                "questions": {
                    "value": "1. Theorem 6.1: In any case, the algorithm can make a random guess. Therefore, it is not appropriate to state that $e_T(\\boldsymbol{\\nu}, \\mathfrak{A})=1$.\n\n2. Minor issue in Section 2: \\citet should be used in \"As in (Metelli et al., 2022)\".\n\n3. Figure 3: Could you elucidate some intuitions/explanations behind the remarkable performance of R-UCBE in cases where $T$ is small? The error probability approaches zero very quickly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6158/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698224846285,
            "cdate": 1698224846285,
            "tmdate": 1699636668232,
            "mdate": 1699636668232,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IsKZSxEKeD",
                "forum": "nji0ztL5rP",
                "replyto": "usESnQ2RWn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nQcR (1/2)"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for spending time revising our work, for having appreciated our theoretical analysis and our experimental campaign. Below, our response to the reviewer's concerns.\n\n### Weaknesses\n\n> While Assumption 2.1 appears intuitive, Assumption 2.2 falls short of being satisfactory. Even though the authors present some theoretical findings solely under Assumption 2.1, their interpretability is somewhat lacking.\n\nAssumption 2.1 implies that the expected values are *non-dereasing and concave*. Assumption 2.2 provides a more **explicit characterizarion** of the increment function $\\gamma_i(n)$ by enforcing an upper bound of the form $c n^{-\\beta}$ depending on the parameters $(c,\\beta)$. Although our results hold under Assumption 2.1 only (Theorems 4.1 and 5.1), the forms of the error probability $e_T$ and of the minimum time budget $T$ become more interpretable under Assumption 2.2. Indeed, intuitively, **the BAI problem is easier when the expected reward function reaches the stationary behavior faster** (see [[IMAGE](https://drive.google.com/file/d/1vcthAAJtBTKybq1GULnNdUTk5EAo9kvj/view?usp=sharing)]). This \"speed\" (a.k.a. rate) is governed mainly by $\\beta$. The larger is $\\beta$, the faster is the convergence rate. This is visible in Corollary 4.2 and 5.2 where the minimum requested time budget $T$ as well as the error probability decrease as $\\beta$ increase. We will add this comment in the final version.\n\n> The proposed algorithm closely resembles UCB-E and SR for standard multi-armed bandits, with the primary distinction lying in the estimators. I'm not suggesting this is unacceptable, but it does somewhat diminish the novelty of this work. <!--A promising future direction would involve integrating both estimators into a unified algorithm.-->\n\nWhile we agree that our algorithms follow the basic scheme of the ones of (Audibert et al., 2010), we point out that our algorithms follow **well-established principles**: *optimism in the face of uncertainty* (UCB-E) and *arm elimination* (SR) which are the basic building blocks of a large majority of works in bandits published in top-tier venues [1,2,3]. Thus, we do not think this diminishes the novelty of the work. Furthermore, we stress that our main contributions are: ($i$) the challenging **theoretical analysis** that the rested rising nature of the arms requires for both the estimators and the algorithms and ($ii$) the conception of the novel **lower bound** which shed lights on the complexities of the setting. \n \n> Since the expected rewards are bounded in $[0,1]$ and non-decreasing, they must converge to some value. Thus, it is not surprising that the error probability lower bound will be matched by R-SR for large $T$. For non-stationary BAI, the algorithm SR is minimal optimal up to constant factors.\n\nWhile we agree that the expected rewards must converge to some value, it is not guaranteed that they will all converge to **different values**. Suppose that the second-optimal arm converges for $T \\rightarrow +\\infty$ to the same expected reward as the optimal arm. This SRB instance will be very challenging for large values of $T$. This is why we need a characterization of the conditions (Eq. 11 or 13) under which R-SR succeeds in identifying the optimal arm. Furthermore, we remark that the application of standard SR is not guaranteed to yield the same guarantees, since it uses as estimator the **standard sample mean** that does not discard too old samples and might fail to deliver an estimator with a bias that diminishes at the number of samples increases."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977523128,
                "cdate": 1699977523128,
                "tmdate": 1699977523128,
                "mdate": 1699977523128,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AzAUo8Uxot",
                "forum": "nji0ztL5rP",
                "replyto": "usESnQ2RWn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6158/Reviewer_nQcR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6158/Reviewer_nQcR"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate your prompt response.\n\n- I still hold the opinion that Assumption 2.2 is excessively strong and unrealistic, despite the proposed algorithms not requiring it. The absence of Assumption 2.2 renders the theoretical results perplexing and challenging to comprehend. Are there any analogous assumptions in existing literature that might provide a basis for comparison?\n\n- Upon reading all other reviews and their corresponding rebuttals, I find myself somewhat perplexed by the problem setup. Why is there a need to explore BAI in stochastic rising bandits? The definition of the best arm in this context appears intricate, and even if we find the best arm, pulling it is essential for improving its expected reward, which strikes me as unusual. What advantages does BAI offer over regret minimization in the context of stochastic rising bandits?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075375157,
                "cdate": 1700075375157,
                "tmdate": 1700075375157,
                "mdate": 1700075375157,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BkVeJoably",
                "forum": "nji0ztL5rP",
                "replyto": "LgY9RMh3uz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6158/Reviewer_nQcR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6158/Reviewer_nQcR"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comprehensive response. It's great to see that similar assumptions to Assumption 2.2 are already used in existing literature. I suggest the authors expand on the rationale behind Assumption 2.2 in the main text, akin to the approach taken for Assumption 2.1.\n\nLet's direct our attention to the weakness in the setting, which, in my opinion, is more significant. It seems there might be a misunderstanding of my previous point. In fact, my comment is closely related to that of Reviewer CfKv.\n\nThe definition of the 'best arm' in this paper, referred to as arm 1, is based on $\\mu_1(T)$, the expected reward from pulling arm 1 for $T$ times. Why is it meaningful to find such best arm in the fixed-budget setting?  In the CASH example, the authors state, \u201cIn the CASH problem, we are interested in finding the one which attains the best performance at the end of the budget T\u201d and that \u201conce the best option is identified, in a real-world scenario, it will be deployed\u201d. However, successfully identifying the best arm does not equate to attaining the performance of $\\mu_1(T)$. What we obtain is $\\mu_1(\\tau)$, where $\\tau$ represents the number of times arm 1 is pulled up to time $T$. To realize $\\mu_1(T)$, we need to run the tuple (algorithm, hyperparameter configuration) for $T-\\tau$ additional rounds. Hence, this example does not justify the appropriateness of the problem setup. \n\nRegarding the reference paper [2], it actually supports my viewpoint. In the context of stochastic rising bandits, the simple regret minimization problem is natural. Translated into the notation used in this paper, the simple regret is defined as $\\mu_1(T)-\\mu_i(\\tau_i)$, where $i$ is the algorithm's output, and $\\tau_i$ is its number of arm pulls up to time $T$. This is evidently more practical.\n\n---\n\nI am delighted to engage in further discussions with the authors regarding this issue."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700142201680,
                "cdate": 1700142201680,
                "tmdate": 1700142201680,
                "mdate": 1700142201680,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cYECYOovKQ",
                "forum": "nji0ztL5rP",
                "replyto": "vsMgyysdPu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6158/Reviewer_nQcR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6158/Reviewer_nQcR"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I concur that the central focus of this paper is to address the question: which is the model that would have best performed if trained from the beginning with the full training budget T? Consequently, the illustrative examples provided in the paper may require modification for a more accurate representation. Additionally, I acknowledge that the problem studied is meaningful when there are multiple similar training environments, but this seems somewhat limiting.\n\nBased on the preceding discussions, I prefer to retain my current rating, although it could potentially change after the reviewer discussion phase."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235611103,
                "cdate": 1700235611103,
                "tmdate": 1700235611103,
                "mdate": 1700235611103,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "37mNpyqR1a",
            "forum": "nji0ztL5rP",
            "replyto": "nji0ztL5rP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6158/Reviewer_BjA1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6158/Reviewer_BjA1"
            ],
            "content": {
                "summary": {
                    "value": "This paper focus on the stochastic rising bandits, the objective is maximize the success rate of identifying the best arm within fixed budget. The authors propose two algorithms, and one of them is optimal in success rate as the authors further give the lower bound that matches the upper bound. Authors further conduct synthetic experiments to validate the theoretical findings."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The theoretical proofs are strict and easy to follow, the results seem sound to me.\n- The experiments are explicitly introduced with specific details.\n- The guarantee of R-SR is strong, and the analysis on the minimum budget the problem is solvable is crucial to the problem, making it clear on which parts of the problem is unsolvable."
                },
                "weaknesses": {
                    "value": "- My major concern is the insufficient problem motivation. In the introduction, the example introduces the SRB is ``the arm improve performances over time'', but the problem setup of SRB is arms whose performances increase with pulls. I personally feel it is the example of adversarial MAB or non-stationary MAB rather than SRB. The experiments still do not give the real-world applications. In fact it's hard for me to figure out real-world scenario (with the neccessary to model as a SRB) that solves practical problems.\n\n- The problem statement is a little bit unclear. Specifically, it is mentioned that SRB is a special case of SRB, but it is never explained what the word ``rested'' means.\n\n- There should be some discussions about the difficulties of applying existing algorithms (or some trivial variants) to solve SRB. For example, it is only mentioned in the experiments that non-stationary MAB algorithms and adversarial MAB algorithms is outperformed, but it is essential to verify that the increasing structure of SRB is crucial both theoretically and empirically."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6158/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6158/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6158/Reviewer_BjA1"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6158/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698419799524,
            "cdate": 1698419799524,
            "tmdate": 1699636668104,
            "mdate": 1699636668104,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Zp79Xemfah",
                "forum": "nji0ztL5rP",
                "replyto": "37mNpyqR1a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BjA1"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for spending time revising our work, and for having appreciated our theoretical analysis and our experimental campaign. Below, our response to the reviewer's concerns.\n\n> My major concern is the insufficient problem motivation. In the introduction, the example introduces the SRB is \"the arm improve performances over time\", but the problem setup of SRB is arms whose performances increase with pulls. I personally feel it is the example of adversarial MAB or non-stationary MAB rather than SRB. In fact it's hard for me to figure out real-world scenario (with the neccessary to model as a SRB) that solves practical problems.\n\nIn the main paper, we proposed a relevant *motivating practical example*, the **combined algorithm selection and hyperparameter optimization (CASH)**, **a real-world scenario** arising from the AutoML research field that **naturally models as a SRB problem** (see Introduction). Moreover, we devoted Appendix C to discuss **two additional** motivating examples. We refer the Reviewer to these sections (Introduction and Appendix C) for a more detailed discussion. \nWe agree that the sentence \"the arm improve performances over time\" is misleading since it may suggest that the arms evolve *regardless* they are pulled. We will replace it in the final version of the paper with \"the arms improve their performances as it is pulled\" to avoid confusion.\n\n> The experiments still do not give the real-world applications. In fact it's hard for me to figure out real-world scenario (with the neccessary to model as a SRB) that solves practical problems.\n\nIn the experimental campaign, we test our methods with **real-world data** by them to perform a **Best Model Selection** of a set of different machine learning algorithms trained to address the classification task on the IMDB dataset. Such an experiment is reported in Appendix I.3 due to space constraints.\n\n> The problem statement is a little bit unclear. Specifically, it is mentioned that SRB is a special case of SRB, but it is never explained what the word \"rested\" means.\n\nWe think the Reviewer with the sentence  \"it is mentioned that SRB is a special case of SRB\" means \"it is mentioned that SRB is a special case of **rested bandit**\". We remark that **Section 1 (Introduction) does explain the word \"rested\"** precisely at the beginning of the second paragraph, which we report here for the Reviewer's convenience: \"This work focuses on the Stochastic Rising Bandits (SRB), a specific instance of the rested bandit setting (Tekin & Liu, 2012) in which the expected reward of an arm increases whenever it is pulled\".\n\n\n> There should be some discussions about the difficulties of applying existing algorithms (or some trivial variants) to solve SRB. For example, it is only mentioned in the experiments that non-stationary MAB algorithms and adversarial MAB algorithms is outperformed, but it is essential to verify that the increasing structure of SRB is crucial both theoretically and empirically.\n\nWe considered several baselines in our numerical validation, including *UCB-E* (Audibert et al., 2010) and *Successive Rejects* (Audibert et al., 2010), the most famous solutions for fixed-budget BAI in stationary MABs. These solutions are based on **estimators that do not discard old data**, as they are developed for stationary setting, **preventing from obtaining estimator of $\\mu_i(T)$ whose bias decreases** as the number of pulls increases. This prevents the error probability to decrease with $T$. We considered also *Prob-1* (Abbasi-Yadkori et al., 2018) as a baseline from the adversarial setting. However, we point out that in our case the rewards *stochastic and subgaussian* and not *bounded and adversarial*. Since *Prob-1* assumes bounded rewards, its analysis does not apply to our setting.\n\n---\n\nAudibert, J. Y., Bubeck, S., & Munos, R. (2010). Best arm identification in multi-armed bandits. In COLT (pp. 41-53).\n\nAbbasi-Yadkori, Y., Bartlett, P., Gabillon, V., Malek, A., & Valko, M. (2018). Best of both worlds: Stochastic & adversarial best-arm identification. In COLT (pp. 918-949)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699972773198,
                "cdate": 1699972773198,
                "tmdate": 1699972773198,
                "mdate": 1699972773198,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EWcUJprmHK",
            "forum": "nji0ztL5rP",
            "replyto": "nji0ztL5rP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6158/Reviewer_CfKv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6158/Reviewer_CfKv"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of best arm identification in the context of stochastic rising bandits with a fixed budget, aiming to identify the arm with the maximum expected reward in the final round. Two algorithms are proposed to tackle this issue: one is a UCB-typed algorithm, and the other is a successive-reject-typed algorithm. The paper also establishes a sample number lower bound for BAI problem of SRB setting, as well as an error lower bound when the sample number is fixed. The theoretical guarantees obtained show that R-UCBE is optimal but requires additional prior knowledge, while R-SR reduces the dependence on prior knowledge. Empirical results further demonstrate that R-UCBE and R-SR outperform other algorithms in comparison."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This work is clearly written and provides two solid approaches supported by theory and experiments. It also offers lower bounds for the problem, making it a fairly complete piece of work."
                },
                "weaknesses": {
                    "value": "Assuming there is a unique best arm seems somewhat unrealistic, especially after T rounds, when there is a high probability that multiple arms could have the same reward. This can be observed in Figure 2 of the experiment, where several lines easily overlap, clearly demonstrating this point. Moreover, this situation is influenced by the randomness of the algorithm, similar to the paper's mention that \"$i^*(T)$ may change,\" which is also a result of the algorithm's randomness. While similar assumptions are made in classical MAB settings, in those cases, the algorithm does not have an impact on the best arm."
                },
                "questions": {
                    "value": "see the weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6158/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6158/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6158/Reviewer_CfKv"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6158/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698666456136,
            "cdate": 1698666456136,
            "tmdate": 1699636667943,
            "mdate": 1699636667943,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kd1s0PYCzT",
                "forum": "nji0ztL5rP",
                "replyto": "EWcUJprmHK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CfKv"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for spending time reviewing our work, and for having appreciated our clarity and completeness. Below, our answers to the reviewer's concerns.\n\n> Assuming there is a unique best arm seems somewhat unrealistic, especially after T rounds, when there is a high probability that multiple arms could have the same reward. This can be observed in Figure 2 of the experiment, where several lines easily overlap, clearly demonstrating this point. \n\nThe assumption that the optimal arm is unique is **standard** in the bandit best arm identification community. Such an assumption is present in the seminal paper in this setting (Audibert et al., 2010) and in (Lattimore and Szepesv\u00e1ri, 2020, Chapter 33). Moreover, in our setting, it is proved to be **necessary** to make the problem learnable. \nWe fear that the Reviewer might have misunderstood what we mean for \"unique optimal arm\". Indeed, we have to enforce that **at the end of the budget $T$** the arm with the hightest expected reward (i.e., the optimal) is unique and **not that there are no intersection over the full horizon $T$** (and so we admit the same expected reward at some points). Looking at Figure 2, for $T \\in$ {$1,\\dots,300$}, we notice that there are 4 points of intersection. Thus, there are just **4 choices of $T$ over 300** in which the optimal arm is not unique. We believe that this is a light standard assumption.\n\n> Moreover, this situation is influenced by the randomness of the algorithm, similar to the paper's mention that \"$i^*(T)$ may change\", which is also a result of the algorithm's randomness. While similar assumptions are made in classical MAB settings, in those cases, the algorithm does not have an impact on the best arm.\n\nGiven a specific time budget $T$ (which is an input of the fixed-budget BAI problem) and given an instance of the stochastic rising bandit (i.e., the expected rewards $\\mu_i(t)$ functions), the optimal arm $i^*(T) = \\text{argmax}_{i \\in [K]} \\mu_i(T)$ is well-defined and, thus, **algorithm-independent** (being defined through $T$ and $\\mu_i(t)$ irrespective of the used algorithm). We stress that, **exactly like in classical MAB settings, the algorithm does not have an impact on the best arm also in stochastic rising bandits**.\n\n---\n\nAudibert, J. Y., Bubeck, S., & Munos, R. (2010). Best arm identification in multi-armed bandits. In COLT (pp. 41-53).\n\nLattimore, T., & Szepesv\u00e1ri, C. (2020). Bandit algorithms. Cambridge University Press."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699972501910,
                "cdate": 1699972501910,
                "tmdate": 1699972501910,
                "mdate": 1699972501910,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]