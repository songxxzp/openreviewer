[
    {
        "title": "EWoK: Tackling Robust Markov Decision Processes via Estimating Worst Kernel"
    },
    {
        "review": {
            "id": "DnGY1D7vR0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7324/Reviewer_rzcF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7324/Reviewer_rzcF"
            ],
            "forum": "8WH6ZlDad6",
            "replyto": "8WH6ZlDad6",
            "content": {
                "summary": {
                    "value": "The authors introduce a new method to learn robust policies by approximately simulating the worst-case transition probabilities. The method works for KL-divergence based sa-rectangular uncertainty set. A large set of numerical experiments is conducted."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The authors conduct large-scale experiments.\n* It is nice that we can combine the methods proposed in the paper with other (non-robust) RL algorithms."
                },
                "weaknesses": {
                    "value": "* \u201cThe optimal policy\u201d, \u201cThe worst-case transition probabilities\u201d: some there are multiple optimal policies/worst-case kernels, I think that the authors should be more careful with the phrasing. I also don\u2019t know what it means to \u201ctrain\u201d a transition kernel (last sentence of the paragraph after Eq. (7)), and I don\u2019t know what is a \u201cperfect value function\u201d (paragraph before Eq. (14)). I list other inaccurate statements below.\u2028\n\n* The Theoretical results are very weak. Nothing new in Appendix A.1 (proof of Theorem 3.2), it is already in [A].\n\n* The Theoretical results are only for KL divergence.\n\n[A] A. Nilim and L. El Ghaoui. Robust control of Markov decision processes with uncertain transition probabilities. Operations Research, 53(5):780\u2013798, 2005."
                },
                "questions": {
                    "value": "1. Can the EWOK be modified to cope with other uncertainty sets than KL-based uncertainty?\n\n2. For completeness, can you recall how to tune $\\beta_{sa}$ the uncertainty radius, or give a precise reference for this?\n\n3. Paragraph after Eq. (7): it is now well-recognized that the minimization problem from Eq. (7) is also an MDP, see for instance Section 3 in [1] or Section 4 in [2]. Can we use gradient-based method in this adversarial MDP to learn the worst-case transition probabilities?\n\n4. Please properly introduce the parameter $\\omega_{sa}$ and $\\kappa_{sa}$ in Th 3.2.\n\n5. Th 3.5 only states that the value converges to the robust value. It does not state that \u201cthe estimated kernel converges to the worst kernel\u201d. Also why is \\pi not indexed by the iteration, since you update it at every iteration?\n\n[1] Vineet Goyal and Julien Grand-Clement. Robust Markov decision processes: Beyond rectangularity. Mathematics of Operations Research, 2022.\n\n[2] Chin Pang Ho, Marek Petrik, and Wolfram Wiesemann. Partial policy iteration for l1-robust\nMarkov decision processes. The Journal of Machine Learning Research, 22(1):12612\u201312657, 2021"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7324/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7324/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7324/Reviewer_rzcF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7324/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697468692213,
            "cdate": 1697468692213,
            "tmdate": 1699636875707,
            "mdate": 1699636875707,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x1M1YYyTYZ",
                "forum": "8WH6ZlDad6",
                "replyto": "DnGY1D7vR0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7324/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7324/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response (Part I)"
                    },
                    "comment": {
                        "value": "We extend our gratitude for your valuable and constructive feedback. It is truly encouraging to see the recognition of our efforts in large-scale experiments. Below, we address the comments and questions and are happy to engage in further discussions.\n\n>\n\n`Question 1 (Q1):` \u201cThe optimal policy\u201d, \u201cThe worst-case transition probabilities\u201d: some there are multiple optimal policies/worst-case kernels, I think that the authors should be more careful with the phrasing. I also don\u2019t know what it means to \u201ctrain\u201d a transition kernel (last sentence of the paragraph after Eq. (7)), and I don\u2019t know what is a \u201cperfect value function\u201d (paragraph before Eq. (14)). I list other inaccurate statements below.\n\n`Response 1 (R1):` Thank you for pointing out the inaccuracies in our phrasing. In the updated version, we have rectified these to 'An optimal policy' and 'A worst-case kernel'. Concerning 'train a transition kernel', our intent was to refer to the process of learning a parameterized transition model. As for 'perfect robust value function', we aimed to convey that we do not have the true robust value function beforehand in practice. These imprecise statements have been addressed in the updated version.\n\n>\n\n`Q2:` The Theoretical results are very weak. Nothing new in Appendix A.1 (proof of Theorem 3.2), it is already in [A].\n\n`R2`: We agree that Theorem 3.2 is not difficult to derive. We present its proof for the sake of completeness. Our primary contribution lies in two key aspects:\n\n* Conceptually, we show how to use Theorem 3.2 for sampling the worst kernel using the nominal kernel.\n* The core theoretical contribution is Theorem 3.5, which is the heart of our algorithm. The update rule in Equation 14 is coupled, i.e., we update the transition kernel $P_{n+1}$ using the value function w.r.t. the kernel $P_n$. Hence, showing its convergence is non-trivial, necessitating a detailed and technical proof in Appendix A.1. We believe this proof does not reside in [A] but rather constitutes our core results.\n\nThus, we respectfully disagree with the reviewer's assessment of our theoretical contribution as weak.\n\n>\n\n`Q3:` Can the EWOK be modified to cope with other uncertainty sets than KL-based uncertainty?\n\n`R3`: Yes, we believe the high-level idea of EWoK, approximating a worst transition kernel, can be applied in other uncertainty sets than the KL-based one. The key step is to build the connection between a worst transition kernel and the nominal one, and use such connection to obtain samples approximately distributed according to the worst transition probability.\n\n>\n\n`Q4:` For completeness, can you recall how to tune $\\beta_{sa}$ the uncertainty radius, or give a precise reference for this?\n\n`R4`: $\\beta_{sa}$ is more of a parameter for modeling purposes. In our experiments, we do not tune $\\beta_{sa}$ but set $\\omega$ and $\\kappa$ instead.\n\n>\n\n`Q5:` Paragraph after Eq. (7): it is now well-recognized that the minimization problem from Eq. (7) is also an MDP, see for instance Section 3 in [1] or Section 4 in [2]. Can we use gradient-based method in this adversarial MDP to learn the worst-case transition probabilities?\n\n`R5`: This is a good point. Thank you! We concur that the worst kernel can be computed using gradient descent, as described in Algorithm 2 in [A]. However, the convergene of Algorithm 2 to the worst kernel, has the iteration complexity of $O(\\frac{S^3A}{(1-\\gamma)^6\\epsilon^2})$ (as guaranteed by their Theorem 4.4). In comparison, our method (update rule in our Eqn. 14) has the iteration complexity of $O(\\log(\\frac{1}{\\epsilon}))$. We thank the reviewer for pointing out this direction and we have included the above discussion in the updated version.\n\n[A] Qiuhao Wang,Chin Pang Ho, Marek Petrik. Policy Gradient in Robust MDPs with Global Convergence Guarantee"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7324/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699978212635,
                "cdate": 1699978212635,
                "tmdate": 1699978212635,
                "mdate": 1699978212635,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2CaJ8C55jU",
                "forum": "8WH6ZlDad6",
                "replyto": "x1M1YYyTYZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7324/Reviewer_rzcF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7324/Reviewer_rzcF"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed answer to my questions. \n\nFor **Q2** (weak contributions): the authors mention that Theorem 3.5 is their main contribution. I still find it very weak and it follows from existing results in a straightforward way. The proof of Theorem 3.5 is presented in Appendix A and consists in two lemmas:\n\n* Lemma A.4 is just the classical lemma that says that Policy Iteration generates a decreasing sequence of value functions, see Proposition 6.4.1 in Puterman.\n\n* Lemma A.5 says that the robust Bellman operators are monotonic (non-decreasing) functions. This is also known and widely recognized in the community, see for instance Proposition 6.3.2 in Puterman.\n\n* The proof of Theorem 3.5 consists of literally 5 lines of algebra, based on the above two lemmas.\n\nOverall I still find the paper contributions to be very weak. The authors should provide appropriate citations for Lemma A.4 and Lemma A.5 (eventually the Puterman textbook), eventually keeping their proof for completeness. The way it is presented right now makes it look like the authors are the first to derive these results, which is clearly not the case.\n\nFor **Q3** (other uncertainty sets): Thanks for your answer. As far as I understand, your method requires a \"closed-form\" for the worst-case kernel so that you can exploit this to sample. Apart from KL uncertainty set, the only other closed-form for the worst-case transition kernel that I am aware of is for box uncertainty; see for instance Proposition 3 in [A]. Could your method be applied here?\n\n\n[A] Data Uncertainty in Markov Chains: Application to Cost-\nEffectiveness Analyses of Medical Innovations, Joel Goh, Mohsen Bayati, Stefanos A. Zenios, Sundeep Singh, David Moore."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7324/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640344659,
                "cdate": 1700640344659,
                "tmdate": 1700640344659,
                "mdate": 1700640344659,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qHl69xOmGH",
                "forum": "8WH6ZlDad6",
                "replyto": "DnGY1D7vR0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7324/Reviewer_rzcF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7324/Reviewer_rzcF"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your quick response. I do not agree with some of your statements, as I describe below.\n\n* *Difference between nominal MDPs and robust MDPs.* I agree with you that RMDPs and nominal MDPs are different and that in some new settings it is important to reprove properties that may seem obvious for nominal MDPs. However, the setting chosen in this paper is sa-rectangular RMDPs with KL-based uncertainty sets, which has been considered in many other papers. It is thus not necessary to reprove Lemma A.5 (monotonicity of the robust evaluation operator) and the other results from Appendix A.3, it suffices to cite the appropriate references.\n\n* *Lemma A.4 is **not** policy iteration*. I argue that the update (25), which is the focus of Lemma A.4, is *exactly* policy iteration, for the *adversarial MDP*, i.e., for the MDP played by the adversary choosing the transition probabilities $P$. The notion of adversarial MDP is introduced in [1,2,3]. In [3] the authors use policy iteration in the adversarial MDP, see Eq. (7) in [3]. This is exactly the update (25) in your paper.\n\n* *Analogy with the seminal papers.* The results in Iyengar and Nilim El Ghaoui are non-trivial because these authors work with a max-min optimization problem. In the Appendix A.5 of your paper, only the minimization problem is considered, so that this problem falls in the classical MDP theory (though the action set is compact, but this has been studied in prior work as I explain above).\n\n* *Other uncertainty sets.* Thanks for the detailed response.\n\n[1] Vineet Goyal and Julien Grand-Clement. Robust Markov decision processes: Beyond rectangularity. Mathematics of Operations Research, 2022.\n\n[2] Chin Pang Ho, Marek Petrik, and Wolfram Wiesemann. Partial policy iteration for l1-robust Markov decision processes. The Journal of Machine Learning Research, 22(1):12612\u201312657, 2021\n\n[3] GOH, Joel, BAYATI, Mohsen, ZENIOS, Stefanos A., et al. Data uncertainty in Markov chains: Application to cost-effectiveness analyses of medical innovations. Operations Research, 2018, vol. 66, no 3, p. 697-715."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7324/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684244243,
                "cdate": 1700684244243,
                "tmdate": 1700684263211,
                "mdate": 1700684263211,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TJt9XasyfP",
            "forum": "8WH6ZlDad6",
            "replyto": "8WH6ZlDad6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7324/Reviewer_QAuy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7324/Reviewer_QAuy"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies online RL in robust Markov decision processes from the perspective of worst transition kernel estimation, which can help scale RMDP-based methods to high-dimensional domains and is thus of great significance to the advance of robust RL. The authors start from the theoretical side by giving a closed form solution to the worst case transition kernel (Theorem 3.2). Motivated by the explicit expression, an approximation of the worst case transition is proposed. The proposed algorithm is extensively studied on various robust RL experimental setups."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper first characterizes the worst case transition dynamic within KL-constrained uncertainty set by a closed form solution (Theorem 3.2), which is of independent interests for future researches on robust MDPs. \n2. The idea to simulate the worst case transition based on an approximation of the closed form solution is novel.\n3. The method break the curse of scalability of traditional RMDP-based methods with several experimental demonstrations in complex RL domains. The experiments are well organized and convincing."
                },
                "weaknesses": {
                    "value": "1. I think the idea of using a resampling trick to simulate the true worst case transition dynamic (Line 4 of Algorithm 1) is interesting and makes sense given the product form of the solution (Theorem 3.2). However, the intuition behind the empirical choice of the unknown parameter $\\omega_{s,a}$ (Eq. 12) is somehow elusive, even the authors provided Proposition 3.4 to argue. \n2. Minor typos and notation clarity problem in the theory part (Section 3).\n\nPlease see my questions below."
                },
                "questions": {
                    "value": "1. About the Weakness 1 I mentioned, I would appreciate it if the authors could explain more about the empirical choice of the unknown parameter  $\\omega_{s,a}$ (since the empirical average over the samples $s_i^{\\prime}$ from $\\bar{P}(\\cdot|s,a)$ forms an overestimation of $\\omega_{s,a}$ as suggested by Proposition 3.4).\n2. Some typos and notation clarity: (i) in Eq (12) it should be $\\sum_{i=1}^Nv(s_i')$; (ii) the notion of $\\omega_n$ and $\\kappa_n$ is not pre-defined (even with subscript $s,a$). I suggest explicitly giving them a definition (as the parameter associated with the worst case transition kernel when the target function is $v_{P_n}^{\\pi}$)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7324/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698428687934,
            "cdate": 1698428687934,
            "tmdate": 1699636875599,
            "mdate": 1699636875599,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XtBUYuJpwH",
                "forum": "8WH6ZlDad6",
                "replyto": "TJt9XasyfP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7324/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7324/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "Thank you for your helpful and valuable comments and feedback. We are thrilled to hear that you find the idea of our method to be novel and that our experiments are well-organized and convincing. Below, we address the comments and questions.\n\n>\n`Question 1 (Q1):` About the Weakness 1 I mentioned, I would appreciate it if the authors could explain more about the empirical choice of the unknown parameter $\\omega_{sa}$ (since the empirical average over the samples $s_i'$ from $\\bar{P}(\\cdot|s,a)$ forms an overestimation of $\\omega_{sa}$ as suggested by Proposition 3.4).\n\n`Response 1 (R1):` Thank you for this thoughtful question. The empirical approximation of $\\omega_{sa}$ is mainly motivated by implementation considerations that we only have access to samples from the nominal transition kernel. In the future, we will explore more clever ways to approximate it to mitigate overestimation.\n\n\n>\n`Q2:` Some typos and notation clarity: (i) in Eq (12) it should be $\\sum^N_{i=1} v(s_i')$; (ii) the notion of $\\omega_n$ and $\\kappa_n$ is not pre-defined (even with subscript $s,a$). I suggest explicitly giving them a definition (as the parameter associated with the worst-case transition kernel when the target function is $v_{P_n}^\\pi$).\n\n`R2:` Thank you for pinpointing those typos and notation issues! We have fixed them in the newest version."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7324/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700049215224,
                "cdate": 1700049215224,
                "tmdate": 1700049215224,
                "mdate": 1700049215224,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VG7GUjKxZj",
                "forum": "8WH6ZlDad6",
                "replyto": "TJt9XasyfP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7324/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7324/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thank you very much for your valuable feedback. As the discussion period is nearing its end, we kindly ask if you have any additional questions or concerns. Should all the raised issues be satisfactorily addressed, we hope you might consider adjusting the score accordingly. We look forward to your input and insights!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7324/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674834431,
                "cdate": 1700674834431,
                "tmdate": 1700674834431,
                "mdate": 1700674834431,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NW54fyIsgy",
                "forum": "8WH6ZlDad6",
                "replyto": "VG7GUjKxZj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7324/Reviewer_QAuy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7324/Reviewer_QAuy"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your answer to my questions! I have read the responses and I will keep my rating as 6."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7324/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682231795,
                "cdate": 1700682231795,
                "tmdate": 1700682231795,
                "mdate": 1700682231795,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OJ7z2PijLB",
            "forum": "8WH6ZlDad6",
            "replyto": "8WH6ZlDad6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7324/Reviewer_qiNv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7324/Reviewer_qiNv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new approach, called EWOK, to address robust MDPs based on the KL-divergence $(s,a)$-rectangular ambiguity set. Specifically, this paper simulates the transited state to approximate the worst-case transition kernel based on the analytical form of the robust Bellman update. Also, the comprehensive experiment illustrates the robustness and good performance of the EWoK in various RL environments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is easy to follow. While EWoK is provided based on specific KL-divergence $(s,a)$-rectangular ambiguity set, it provides a new perspective to estimate the worst-case transitions.\n\nIn the experiments, the authors compare their algorithm to both the benchmark non-robust algorithm and the commonly used domain randomization method. The results demonstrate the outperformance of the proposed algorithm in multiple RL practical problems."
                },
                "weaknesses": {
                    "value": "One reason for not giving a higher score at this point is that it seems to me that all the results in this particular paper are rather intuitive or expected. It is worth noting that the convergence analysis of $(s,a)$-rectangular RMDPs has already been extensively studied. While Theorem 3.2 provides the explicit form of the worst-case transition kernel for the KL-divergence ambiguity set, other results do not seem particularly surprising. In particular, Theorem 3.5 is a standard result in the analysis of RMDPs, which seems this paper has limited theoretical contributions.\n\nAnother aspect that seems to be lacking is a discussion on how the radius $\\beta_{sa}$ of the ambiguity set affects the algorithm's performance, although it would be transferred to new parameters $\\omega_{sa}$ and $\\kappa_{sa}$. I do expect that the parameter selection procedure could be discussed more."
                },
                "questions": {
                    "value": "The numerical results and theoretical discussion make sense to me. I have the following questions and suggestions:\n1. The literature review is not comprehensive. A recent paper [1] also studied RMDPs with global optimality, and it would be helpful if the author discussed it. \n2. The official definition of the robust Bellman operator should be added in Section 2.3 for completeness.\n3. While we consider a practical problem lying in the KL-based $(s,a)$-\nrectangular ambiguity set, the only parameter that the agent can choose is $\\beta_{sa}$; however, the other two parameters $\\omega_{sa}$ and $\\kappa_{sa}$ would be settled directly. Could you explain more about the relationship between $\\beta_{sa}$ and the other two parameters $\\omega_{sa}$ and $\\kappa_{sa}$, or how can the agent reach the latter when setting the former? \n\n[1] Wang, Qiuhao and Ho, Chin Pang and Petrik, Marek. \"Policy Gradient in Robust MDPs with Global Convergence Guarantee.\" ICML (2023)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7324/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7324/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7324/Reviewer_qiNv"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7324/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698666823941,
            "cdate": 1698666823941,
            "tmdate": 1699636875484,
            "mdate": 1699636875484,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2bf8HswEol",
                "forum": "8WH6ZlDad6",
                "replyto": "OJ7z2PijLB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7324/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7324/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "We express our gratitude for your valuable suggestions and insightful feedback. It is encouraging to to learn of your appreciation for the clarity of our paper and the effectiveness of our EWoK method. Below, we address the questions and comments.\n\n>\n`Question 1 (Q1):` One reason for not giving a higher score at this point is that it seems to me that all the results in this particular paper are rather intuitive or expected. ... In particular, Theorem 3.5 is a standard result in the analysis of RMDPs, which seems this paper has limited theoretical contributions.\n\n`Response 1 (R1):` To the best of our knowledge, Theorem 3.5 is novel and not straightforward to prove. It establishes the worst kernel can be found iteratively, while each iteration can be estimated via sampling in a model-free setting. We would like to clarify the convergence in Theorem 3.5 is different from the standard convergence analysis of (s,a)-rectangular RMDP. If the reviewer is aware of any existing results akin to Theorem 3.5, we kindly request a reference and will include it to enhance the accuracy of our paper.\n\n>\n`Q2:` Another aspect that seems to be lacking is a discussion on how the radius $\\beta_{sa}$ of the ambiguity set affects the algorithm's performance, although it would be transferred to new parameters $\\omega_{sa}$ and $\\kappa_{sa}$. I do expect that the parameter selection procedure could be discussed more.\n\n`R2:` Thank you for highlighting this point. We would like to clarify that $\\beta_{sa}$ is more of a parameter for modeling purposes. In practice, we do not set it directly. In principle, a higher $\\beta_{sa}$ means a larger uncertainty set, thus a more conservative policy. However, as all non-tabular experiments never have truly rectangular or KL-constrained uncertainty sets, the influence on practical performance might vary.\n\n>\n`Q3:` The literature review is not comprehensive. A recent paper [1] also studied RMDPs with global optimality, and it would be helpful if the author discussed it.\n\n`R3:` Thank you for pointing out this related work. Similar to our work, it also iteratively computes the worst kernel (appearing in its Algorithm 2). However, their method is gradient-based with iteration complexity of $O(\\frac{S^3A}{(1-\\gamma)^6\\epsilon^2})$ (Theorem 4.4 in [1]). In comparison, our method (update rule in our Equation 14) has the iteration complexity of $O(\\log(\\frac{1}{\\epsilon}))$ as stated in our Theorem 3.5. We have included it in the updated version.\n\n>\n`Q4:` The official definition of the robust Bellman operator should be added in Section 2.3 for completeness.\n\n`R4:` Thank you for your suggestion. We have addressed it in the new version.\n\n>\n`Q5:` While we consider a practical problem lying in the KL-based $(s,a)$-rectangular ambiguity set, the only parameter that the agent can choose is $\\beta_{sa}$; however, the other two parameters $\\omega_{sa}$ and $\\kappa_{sa}$ would be settled directly. Could you explain more about the relationship between $\\beta_{sa}$ and the other two parameters $\\omega_{sa}$ and $\\kappa_{sa}$ , or how can the agent reach the latter when setting the former?\n\n`R5:` Thank you for raising the question. We apologize for the unclarity in our presentation. $\\beta_{sa}$ is more of a parameter for modeling purposes, not something chosen by the agent. The connection between $\\beta_{sa}$ and the other parameters $\\omega_{sa}$ and $\\kappa_{sa}$ are described in Eqn.(10) and Proposition 3.3. $\\omega_{sa}$ and $\\kappa_{sa}$ are the solutions of Eqn.(10), though we can't obtain a closed form for them. In our experiments, we consider $\\kappa_{sa}$ as a hyperparameter, and setting it essentially decides a $\\beta_{sa}$ for modeling the problem."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7324/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700049066244,
                "cdate": 1700049066244,
                "tmdate": 1700049066244,
                "mdate": 1700049066244,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6jz8yonAo5",
                "forum": "8WH6ZlDad6",
                "replyto": "OJ7z2PijLB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7324/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7324/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thank you very much for your valuable feedback. As the discussion period is nearing its end, we kindly ask if you have any additional questions or concerns. Should all the raised issues be satisfactorily addressed, we hope you might consider adjusting the score accordingly. We look forward to your input and insights!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7324/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674816372,
                "cdate": 1700674816372,
                "tmdate": 1700674816372,
                "mdate": 1700674816372,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pLY8FaPWLW",
            "forum": "8WH6ZlDad6",
            "replyto": "8WH6ZlDad6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7324/Reviewer_vpVH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7324/Reviewer_vpVH"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to solve the Robust Markov Decision Process (RMDP) problem in a realistic high-dimensional scenario, and introduces a method named EWoK. EWoK assigns a higher probability to the transition where the next state has a lower estimated value such that the agent gets a higher chance to learn from the worse transition. EWoK acts as a layer between the true transition metrics and the agent and directly changes the sampled next state, rather than requiring any specific change on the learning agent. Thus, it is able to work with any non-robust reinforcement learning algorithm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper focuses on a nice topic, dealing with the issue that many RMDP algorithms cannot scale to high-dimensional domains. The idea of EWoK is creative. The method bridges robust and non-robust reinforcement learning algorithms, by changing the transition metrics to let it focus more on the robust case and learning the policy with any non-robust algorithm. Therefore, EWoK reserves the ability to scale to high-dimensional inputs by learning a policy using existing reinforcement learning algorithms. \n\n- The experiment section has enough runs (40 and 10 seeds in different environments) to provide relatively reliable average and confidence intervals.\n\n- The experiment section provides ablation studies for two introduced parameters ($\\kappa$ and $N$)."
                },
                "weaknesses": {
                    "value": "However, the method may need further improvement. \n\n- I am not convinced yet that EWoK can be applied in a realistic domain as claimed. As the paper indicates in the conclusion section, EWoK assumes the environment is able to sample from the same state and action pair multiple times. This requirement is easy to achieve when using simulators or when there exists a perfect model, but is unrealistic in real environments. In the real world, it is almost impossible to reset the environment to the previous state and apply the same action multiple times. Given that the paper defines EWoK as an online method for realistic domains, I think this assumption contradicts the scenario which EWoK is supposed to work with. It might be more accurate if the paper reduces the scope to high-dimensional domains.\n\n- At the end of the experiments section, the paper mentions a larger number of next-state samples does not affect the wall clock time a lot. It is nice to notice and discuss the running time of a method, but  I would like to point out that this happens in simulators because simulators react fast. In real-world scenarios, the environment could be much slower for sampling one next state. It would be nice to also check a case in robotics, or some other environments taking relatively long time to respond.\n\n- It might be worth checking a more difficult experiment setting, such as the one single trajectory case. \n\nSome related works:\n\nZhou, Zhengqing, et al. \"Finite-sample regret bound for distributionally robust offline tabular reinforcement learning.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2021.\n\nYang, Wenhao, Liangyu Zhang, and Zhihua Zhang. \"Toward theoretical understandings of robust Markov decision processes: Sample complexity and asymptotics.\" The Annals of Statistics 50.6 (2022): 3223-3248.\n\nYang, Wenhao, et al. \"Robust Markov Decision Processes without Model Estimation.\" arXiv preprint arXiv:2302.01248 (2023)."
                },
                "questions": {
                    "value": "- Could the authors provide a learning curve to show how long the method takes to converge? It would be better if the learning curve plot could also include your baselines."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7324/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698774078661,
            "cdate": 1698774078661,
            "tmdate": 1699636875354,
            "mdate": 1699636875354,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2m1sAVPJ8i",
                "forum": "8WH6ZlDad6",
                "replyto": "pLY8FaPWLW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7324/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7324/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' response"
                    },
                    "comment": {
                        "value": "We extend our gratitude for your insightful and constructive feedback. We are happy to see your comment that the idea of EWoK is creative. Below, we address the questions.\n\n>\n`Question 1 (Q1):` I am not convinced yet that EWoK can be applied in a realistic domain as claimed. ... It might be more accurate if the paper reduces the scope to high-dimensional domains.\n\n`Response 1 (R1):` Thank you for the suggestion. While EWoK shows potential for integration with learned models to get rid of the assumption, we concur that it is more accurate to reduce the current scope to high-dimensional domains. We have revised the paper to embody this change.\n\n>\n`Q2:` At the end of the experiments section, the paper mentions a larger number of next-state samples does not affect the wall clock time a lot. ... It would be nice to also check a case in robotics, or some other environments taking relatively long time to respond.\n\n`R2:` Thank you for highlighting this point. We agree that the negligible overhead is due to fast simulation, which may not be available in real-world scenarios. We have revised our paper to reduce the confusion. In real-world cases where sampling next states is time-consuming, it could increase simulation time but we believe that this increase won't be prohibitively expensive in most realistic cases.\n\n>\n`Q3:` It might be worth checking a more difficult experiment setting, such as the one single trajectory case.\n\n`R3:` Thank you for the suggestion. Could you provide more details about this specific setting? We would gladly conduct the requested experiments.\n\n>\n`Q4:` Could the authors provide a learning curve to show how long the method takes to converge? It would be better if the learning curve plot could also include your baselines.\n\n`R4:` Sure, we have updated the paper and included the learning curves in the appendix (Fig. 13). Thank you for pointing it out."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7324/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700048789519,
                "cdate": 1700048789519,
                "tmdate": 1700048789519,
                "mdate": 1700048789519,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QeRIVEvnFA",
                "forum": "8WH6ZlDad6",
                "replyto": "pLY8FaPWLW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7324/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7324/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thank you very much for your valuable feedback. As the discussion period is nearing its end, we kindly ask if you have any additional questions or concerns. Should all the raised issues be satisfactorily addressed, we hope you might consider adjusting the score accordingly. We look forward to your input and insights!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7324/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674803575,
                "cdate": 1700674803575,
                "tmdate": 1700674803575,
                "mdate": 1700674803575,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "myWyeEQg8b",
                "forum": "8WH6ZlDad6",
                "replyto": "QeRIVEvnFA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7324/Reviewer_vpVH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7324/Reviewer_vpVH"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "I would like to thank the author for the reply. I will keep my score. \n\nFor Q3, I was thinking of an environment that can only provide a long and consecutive trajectory. This is to test how the result looks like in a more realistic case, or in an extreme case for the proposed algorithm, i.e. there is no resetting on state s_t and N=1. This experiment can act as part of the parameter study, also tell us how the algorithm might perform in reality. But as the scope has already been modified, I do not consider this test for realistic cases as important as before."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7324/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690610165,
                "cdate": 1700690610165,
                "tmdate": 1700690610165,
                "mdate": 1700690610165,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]