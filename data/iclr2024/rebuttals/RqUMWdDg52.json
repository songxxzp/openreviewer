[
    {
        "title": "FireAct: Toward Language Agent Finetuning"
    },
    {
        "review": {
            "id": "bdD9HM6kaM",
            "forum": "RqUMWdDg52",
            "replyto": "RqUMWdDg52",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6842/Reviewer_3cXG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6842/Reviewer_3cXG"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces FireAct, a language agent that involves fine-tuning. The proposed method introduces how to leverage a strong LM with few-shot prompting to generate trajectories for fine-tuning. Experimental results demonstrate that the proposed method can improve the performance of language agents and reduce the gap between open-source LLMs and ChatGPT / GPT-4."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper highlights the importance of fine-tuning to obtain better language agents. Massive experimental results also explore different aspects to improve agents, including scaling effects, robustness, generalization, efficiency and cost."
                },
                "weaknesses": {
                    "value": "1. Generally, I think the main contribution of this paper can be considered as a data augmentation, which aims to utilize strong LLMs to generate trajectories for language models and mix multiple training tasks for fine-tuning. The idea of mixing multiple training tasks is not interesting, and previous works like T5 or instruction tuning also adopted such an experience for tuning.\n2. Although the authors have provided massive experiments to verify different fine-tuning factors in this paper, most of the conclusions are obvious and not inspired. For example like the experiments in sec 5.3, many conclusions are straightforward and it is evident that full-model training is better than LoRA. How to design experiments to prove the innovation of the proposed method (FireAct) is more important, rather than analyzing these factors.\n3. Most of the experiments are only employed on three datasets (HotpotQA, StrategyQA, and MMLU). To verify the effectiveness of agents, more real-world scenarios like interactive environments are required.\n\nOther suggestions:\n1. Too many fine-tuning details and settings are given in the appendix, not the main paper. And many useless experimental results cost too many pages in the whole paper. However, these experiments do not bring any insights. On the contrary, some fine-tuning details are more important for us to know the contribution of this paper."
                },
                "questions": {
                    "value": "1. The papers conduct experiments to validate the robustness to noisy tools. However, previous works (e.g., HuggingGPT, Chameleon) have adopted retrieval-based methods to utilize tools based on ChatGPT or GPT-4 and achieve some performance. So how fine-tuning-based methods compare with retrieval-based methods?\n2. What is the content of Appendix A.2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6842/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698038718638,
            "cdate": 1698038718638,
            "tmdate": 1699636792501,
            "mdate": 1699636792501,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uRTtRtMstG",
                "forum": "RqUMWdDg52",
                "replyto": "bdD9HM6kaM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6842/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6842/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed feedback and constructive criticism. We would like to address your concerns as follows:\n\n**On the Contribution of FiReAct:**\nWhile we acknowledge that the concept of data augmentation and multi-task fine-tuning is not novel in itself, the unique contribution of FiReAct lies in its application to language agent fine-tuning. We are among the first to systematically explore how agents can acquire generalizable skills through carefully designed fine-tuning trajectories. Our findings offer preliminary yet promising insights into the potential of language agent fine-tuning, paving the way for future research in this area.\n\n**Regarding Experiment Design and Conclusions:**\nWe understand your concern about the apparent straightforwardness of some conclusions, such as the comparison between full-model training and LoRA. However, these results are crucial for practitioners in the field, providing concrete, quantitative insights into the performance-efficiency tradeoff in agent fine-tuning. For instance, our findings on the comparative efficiency of LoRA versus full-model training on different hardware setups (detailed in Appendix A.5) are valuable for those implementing these methods in practice.\n\n**On the Choice of Datasets:**\nWe have expanded our experiments to include tasks beyond QA, as detailed in our General Response. These additional tasks encompass a broader range of real-world scenarios, further validating the effectiveness of our approach.\n\n**Concerning the Detailedness of the Appendix:**\nWe appreciate your feedback on the balance between the main paper and the appendix. We invite specific suggestions on which experimental results you find less insightful and which fine-tuning details you believe are crucial but missing. Such feedback will be invaluable in refining our paper.\n\n**Comparison with Retrieval-Based Methods:**\nYour question regarding the comparison between fine-tuning-based and retrieval-based methods is insightful. It's important to note that the concept of an \"agent\" in our context goes beyond mere retrieval; it encompasses reasoning and tool usage in a broader sense. Our approach, therefore, offers a more comprehensive improvement of agent capabilities, as evidenced by our inclusion of tasks like Operating System, Database, and Web Browsing from AgentBench mentioned in General Response.\n\n**Content of Appendix A.2:**\nAppendix A.2 presents a detailed comparison of results across different tasks and methods, focusing on robustness analysis. We have made efforts to enhance the clarity of this section to avoid any ambiguity."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6842/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738987543,
                "cdate": 1700738987543,
                "tmdate": 1700738987543,
                "mdate": 1700738987543,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v7QyE9N3hr",
            "forum": "RqUMWdDg52",
            "replyto": "RqUMWdDg52",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6842/Reviewer_iJYa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6842/Reviewer_iJYa"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes FireAct in the ReAct framework to finetune LMs with agent trajectories generated by GPT-4. FireAct enhances the training process by combining ReAct with COT and Reflexion prompting techniques, resulting in the generation of more diverse and improved training samples. Notably, FireAct does not require a few-shot prompting example during inference. The study presents several intriguing findings and highlights unexplored research questions, including the intricate relationship between the base LM and fine-tuning trajectory data, assessing the robustness of language agents, optimizing their task-solving strategies, and evaluating the effectiveness of strategy selection. In the experimental evaluation, the authors demonstrate the effectiveness and efficiency of the proposed method on various QA tasks, such as HotpotQA, Bamboogle, StrategyQA, and MMLU."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation of this paper to systematically investigate the effect of finetuning language agents is of importance.\n2. The authors have performed fine-tuning on various backbone models, including LLaMA and GPT-3.5, thereby enhancing the soundness of this study.\n3. This paper uncovers valuable insights, such as the increased robustness of fine-tuned agents compared to zero-shot ones, and the ability of LLMs fine-tuned on multi-method training data to implicitly select reasoning methods."
                },
                "weaknesses": {
                    "value": "1. The FiReACT method is not novel today. Several works have proposed fine-tuning Language Model Models (LLMs) on ReACT, COT, or Reflection data. Examples include \"Large Language Models Are Reasoning Teachers\" (Ho et al., 2022) and toolLLaMA (Qin et al., 2023).\n\n2. The authors have not conducted enough experiments on the setting of full fine-tuning. I wonder whether the conclusions of this paper still hold when the method is applied to fully fine-tune an agent."
                },
                "questions": {
                    "value": "1. What is the reason behind the observation that the fine-tuned agents exhibit more robustness compared to the zero-shot ones, and that the LLMs fine-tuned on the multi-method training data can implicitly select reasoning methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6842/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698714185980,
            "cdate": 1698714185980,
            "tmdate": 1699636792391,
            "mdate": 1699636792391,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lDz0Pz0uVU",
                "forum": "RqUMWdDg52",
                "replyto": "v7QyE9N3hr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6842/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6842/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your insightful observations and questions, which allow us to further clarify the contributions of our work.\n\n**On the Novelty of FiReAct:**\nWe acknowledge the existence of concurrent works such as those by Ho et al. (2022) and Qin et al. (2023), which we have duly cited and discussed in our related work section. FiReAct, however, distinguishes itself by exploring the multi-task, multi-method fine-tuning approach for language agents. Our work delves into new dimensions such as generalization to novel tasks, robustness against noise, and efficiency in agent operation, thereby contributing unique insights to the field.\n\n**Experiments on Full Fine-Tuning:**\nOur experimental setup primarily utilized LoRA fine-tuning due to computational constraints. However, we posit that our key findings, particularly regarding the enhanced robustness of fine-tuned agents to tool noise and the efficiency benefits during inference, would likely remain valid in a full fine-tuning scenario. \n\n**Robustness and Implicit Method Selection:**\nThe observed robustness of fine-tuned agents, especially in comparison to zero-shot counterparts, can be attributed to the inclusion of noisy data in the fine-tuning trajectories. This exposure enables the agents to better handle imperfect tool outputs, a scenario less effectively addressed by few-shot prompting, which often relies on idealized examples (refer to Table 3 for details).\n\nThe phenomenon of implicit method selection in multi-method fine-tuned agents is indeed intriguing. Our current hypothesis is that the fine-tuning process, which selectively focuses on successful trajectories, results in a varied distribution of training questions associated with different methods. This could enable the fine-tuned agents to learn associations between question features and the most effective method for addressing them. We believe this presents an exciting avenue for future research to explore in depth."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6842/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737718118,
                "cdate": 1700737718118,
                "tmdate": 1700737718118,
                "mdate": 1700737718118,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WirmLdkegE",
            "forum": "RqUMWdDg52",
            "replyto": "RqUMWdDg52",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6842/Reviewer_6Re8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6842/Reviewer_6Re8"
            ],
            "content": {
                "summary": {
                    "value": "This study delves into the relatively under-researched area of refining language models (LMs) to create linguistic agents. By employing a straightforward, regulated framework that utilizes a Google search API for question answering (QA), this paper conducts a thorough examination across a range of foundational LMs, agent strategies, data used for fine-tuning, and QA challenges. This investigative work uncovers new understanding regarding how the size of the base LM and the fine-tuning dataset influences outcomes, the integration of path data from diverse tasks and agent techniques, and the resilience of these systems to various forms of data disturbances."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Pros:\n1. This paper claims that they study a new direction: Agent fine-tuning.\n2. Comprehensive experiments show the effectiveness of the proposed Agent fine-tuning.\n3. Detailed analyses are given to illustrate Agent fine-tuning."
                },
                "weaknesses": {
                    "value": "Cons:\n1. It is unclear the advantage of FiReAc compared to ReAct considering FiReAc is fine-tuned based on ReAct trajectories. The authors claimed that \u201cFiReAct agents benefit from the diversity of learning support, thus become more robust to external noises, more generalizable to novel tasks, and more flexible to choose various agent methods and task solving strategies adaptive to the problem at hand.\u201d However, intuitively, it is hard to understand the reasons why fine-tuning can make the agents more robust and generalizable. It is suggested that authors could provide more explanations.\n2. It is known that fine-tuning can hurt the generalization ability of LLMs. But the authors claimed that the proposed \u201cagent fine-tuning\u201d method is \u201cmore generalizable to novel tasks\u201d, which is hard to understand.\n3. The method seems to be very trivial. It seems this paper just renames the conventional \u201cfine-tuning\u201d as \u201cagent fine-tuning\u201d.\n4. The technical contribution is limited because It is unclear what is technical challenges of \u201cagent fine-tuning\u201d. It Is suggested the authors provide more explanations.\n5. It is suggested the authors summarize their contributions more clearly. It is very vague to say \u201cTo sum up our contributions, we advocate for the overlooked direction of language agent fine-tuning, and propose novel methodologies, systematic experiment designs, practical insights and guidelines, as well as new research questions for this direction.\""
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6842/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6842/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6842/Reviewer_6Re8"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6842/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839884957,
            "cdate": 1698839884957,
            "tmdate": 1699636792278,
            "mdate": 1699636792278,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rYjQftg7rn",
                "forum": "RqUMWdDg52",
                "replyto": "WirmLdkegE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6842/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6842/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful for your detailed feedback and the opportunity to clarify aspects of our work.\n\n**Advantages of FiReAct over ReAct:**\nYour query about the benefits of FiReAct compared to ReAct is crucial. FiReAct's fine-tuning process, unlike ReAct's few-shot prompting, involves learning from a broader range of trajectories that encompass diverse task-solving strategies. This diversity is key to FiReAct's enhanced robustness and generalizability. Specifically:\n- FiReAct shows improved performance on the test sets of training tasks (refer to Table 2).\n- When applied to novel tasks, FiReAct maintains or enhances performance, as evidenced in Table 3 and our General Response. This is a notable departure from traditional NLP fine-tuning, where the focus is often on specific facts or task formats. FiReAct, by teaching LLMs how to reason and utilize tools like Google search, imparts skills that are transferable across various tasks.\n- FiReAct demonstrates greater resilience to noise in tool outputs. This is because the fine-tuning process includes trajectories with noise, teaching the model to handle imperfect data. In contrast, few-shot prompting often relies on 'ideal' examples, which may not prepare the model for real-world variability (see Table 3).\n\n**On the Alleged Triviality of Agent Fine-Tuning:**\nWe appreciate your concern regarding the perceived simplicity of our approach. However, the novelty of FiReAct lies in:\n- Addressing the gap in research: Prior work has largely overlooked fine-tuning LLMs specifically for agent roles, with most studies either fine-tuning LLMs for non-agent tasks or using prompting for agent tasks.\n- Demonstrating multi-faceted empirical benefits: FiReAct, despite its simplicity, showcases significant benefits in terms of robustness, generalization, efficiency, and cost, which are crucial for practitioners in the field of agent development.\n- Exploring new technical depths: FiReAct pioneers the approach of multi-method, multi-task fine-tuning for agents. It demonstrates that even smaller LLMs can effectively acquire generalizable agent skills through supervised fine-tuning, presenting a viable alternative to more complex and less controllable methods like reinforcement learning.\n\n**Clarification of Contributions:**\nIn response to your suggestion, we have revised our introduction to more clearly articulate our contributions. We now explicitly state how FiReAct advances the field of language agent fine-tuning, detailing the novel methodologies, systematic experiment designs, practical insights, and new research questions that our work introduces."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6842/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736977585,
                "cdate": 1700736977585,
                "tmdate": 1700736977585,
                "mdate": 1700736977585,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ajtMkjlkVa",
            "forum": "RqUMWdDg52",
            "replyto": "RqUMWdDg52",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6842/Reviewer_GhVh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6842/Reviewer_GhVh"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces \"FiReAct,\" a method for fine-tuning language models to function as language agents capable of reasoning and interacting with external environments. It challenges the conventional few-shot prompting approach, suggesting that fine-tuning with agent trajectories is more robust and efficient. The study uses a controlled environment with a Google search API for question answering to demonstrate the benefits of FiReAct. Key contributions include a novel fine-tuning methodology, a set of empirical guidelines for implementing such agents, and the release of code and model checkpoints for future research. The authors argue for fine-tuning over prompting when data and task understanding permit, setting new directions for language agent development."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strengths of the paper, as identified through the experimental results, include the following:\n\n1. FiReAct agents eliminate the need for few-shot prompting examples during inference, which results in cost savings, faster operation, and increased convenience compared to prompting-based agents.\n2. These agents benefit from a diverse learning support, which enhances their robustness against external noise and improves their generalizability to novel tasks.\n3. FiReAct provides flexibility for agents to choose from various methods and strategies for solving tasks, allowing them to adapt to the problem at hand more effectively.\n\nMoreover, the paper opens up new avenues for research by highlighting previously unexplored questions related to the interactions between the base language model (LM) and the fine-tuning data, evaluating the robustness and strategic decision-making of language agents, and the systematic analysis of fine-tuning data design for language agents."
                },
                "weaknesses": {
                    "value": "Task and Tool Limitation: The research is limited to question-answering (QA) tasks and relies on a single tool (Google search) for evaluation. This limitation questions the generalizability of FiReAct to other types of tasks and tools, which is a critical aspect for language agents intended for broader applications. To make the conclusions more convincing, it would be great if you could evaluate on other types of interactive tasks such as ScienceWorld, Mind2Web, InterCode, etc.\n\nMethodology Limitations: The paper focuses on three methods that maintain a single autoregressive trajectory context, which might not be representative of more complex agent behaviors. The ability to handle multiple prompts, roles, and contexts is a significant aspect of agent design that is not addressed in the current research.\n\nMulti-Task Constraints: The multi-task setup is confined to only three QA tasks, and the most advanced language model (LM) fine-tuned is GPT-3.5. The limited scope in both the variety of tasks and the LM capabilities may restrict the insights into the potential of FiReAct for scaling up to more complex multi-task environments."
                },
                "questions": {
                    "value": "1. How do the authors envision the application of FiReAct to tasks beyond QA and the use of tools other than Google search? Can FiReAct work on other types of interactive tasks such as ScienceWorld, Mind2Web, InterCode, etc? Why do you choose to focus on QA only?\n2. Can the authors discuss potential approaches for fine-tuning more advanced agents involving multiple prompts, roles, and contexts?\n3. How might prompting and fine-tuning be optimally combined in a complex agent system?\n4. What are the anticipated difficulties in scaling FiReAct to a large-scale multi-task environment?\n5. How might newer or more powerful models like GPT-4 affect the outcomes, and what would be the implications for the FiReAct methodology?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6842/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698915697051,
            "cdate": 1698915697051,
            "tmdate": 1699636792168,
            "mdate": 1699636792168,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IgRZqdKpTU",
                "forum": "RqUMWdDg52",
                "replyto": "ajtMkjlkVa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6842/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6842/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate your insightful feedback and constructive suggestions. Here is our response to the key points raised:\n\n**Task and Tool Limitation:** \nWe acknowledge your concern regarding the generalizability of FiReAct. In response, we have expanded our experiments to include additional tasks from AgentBench, which extends beyond QA and incorporates a variety of tools. These new experiments are designed to demonstrate FiReAct's adaptability to a broader range of tasks and tools.\n\n**Methodology Limitations:** \nWe understand the importance of handling multiple prompts, roles, and contexts in agent design. Our current focus on methods like CoT, ReAct, and Reflexion, which align with the ReAct autoregressive format, was a strategic choice due to their compatibility with multi-method fine-tuning. However, we recognize this as a limitation and an exciting avenue for future research, as noted in our revised manuscript.\n\n**Multi-Task Constraints:** \nFor further details on our expansion beyond QA tasks, please refer to our General Response. Regarding the use of GPT-3.5, it represents the most advanced publicly available LM for fine-tuning at the time of our study. We are actively exploring the integration of more diverse tasks and advanced LMs in our ongoing research.\n\n**Application of FiReAct Beyond QA and Advanced Agents:** \nAs detailed in our General Response, FiReAct's application extends to various interactive tasks, including those mentioned. Our initial focus on QA was due to its well-defined structure and data availability, making it an ideal starting point for demonstrating FiReAct's capabilities.\n\n**Combining Prompting and Fine-Tuning:** \nAs discussed in Section 8 of our paper, we advocate for a balanced approach. Fine-tuning is preferable for well-defined, frequently used tasks with sufficient data, while prompting offers flexibility and convenience for less defined or zero-shot scenarios.\n\n**Scaling FiReAct to Multi-Task Environments:** \nOur initial experiments do not indicate a performance decrease when scaling FiReAct to more tasks. Future work will investigate the upper limits of task complexity and quantity that FiReAct can effectively manage.\n\n**Impact of Newer Models like GPT-4:** \nOur experiments across various models, including Llama-2 and CodeLlama, demonstrate FiReAct's effectiveness irrespective of model size or family. We anticipate that fine-tuning with GPT-4 would enhance performance, generalizability, and robustness, and we look forward to exploring this in future work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6842/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735422490,
                "cdate": 1700735422490,
                "tmdate": 1700735598420,
                "mdate": 1700735598420,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]