[
    {
        "title": "Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models"
    },
    {
        "review": {
            "id": "CQjouS75C1",
            "forum": "6mLjDwYte5",
            "replyto": "6mLjDwYte5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3245/Reviewer_SYmE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3245/Reviewer_SYmE"
            ],
            "content": {
                "summary": {
                    "value": "This paper demonstrates that large scale instruction tuning (using the FLAN data) of sparse mixture-of-expert (MoE) models before finetuning on downstream task data is crucial for MoE to beat comparable dense models (in terms of inference FLOPs).  Merely performing finetuning with MoE on downstream task data without instruction tuning beforehand underperforms directly finetuning a dense model (without instruction tuning), whereas the addition of the instruction tuning stage to the MoE model causes it to outperform dense models with the equivalent training procedure.  At all model scales, MoE outperforms comparable dense models whenever the instruction tuning phase is present, whereas MoE without instruction tuning underperforms."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originality:** As far as I know the significance of instruction tuning for MoE has not been studied extensively in the manner this paper puts forward.\n\n**Quality:** The claims are plausible and well supported.  The authors conducted comprehensive ablations across model scales, # of tasks for instruction tuning, MoE expert selection method, # of experts, etc.  There is no reason to question the central claim.\n\n**Contribution:** The contribution is to provide high quality data towards the effect of large scale instruction tuning for MoE models in relation to dense models with a comparable number of inference FLOPs.  They demonstrate that instruction tuning may be essential for MoE models to succeed."
                },
                "weaknesses": {
                    "value": "**Weaknesses:**  There is little insight into what is causing the failure of direct finetuning of MoE models on downstream task data.  It could be that MoE models have higher capacity to overfit, however it is unclear if instruction tuning is preventing this or if there are other factors at hand.  More conceptual insight would be nice, however I do not view this as a major weakness."
                },
                "questions": {
                    "value": "*Figure 1 (right):* It is not a big deal but can be a bit confusing that the T5 and Flan-T5 green and blue bars are included for each number of experts as these are independent of the number of experts.\n\n*Table 1:* Why not show Switch and GS performance at the 32G FLOP scale?\n\n*Figure 3:* Why not label the orange and blue curves by model size, at least in the caption?\n\n*Figure 6:* How is expert utilization measured?\n\n**Notes and minor details:**\n\n*Typo:* \u201cbenefits from a richer repertoire of specialized sub-networks .\u201d (extra space before period)\n \n*Figure 2 caption:* \u201cAverage zero performance\u201d --> \u201cAverage zero shot performance?\u201d\n \nAdd period after paragraph title \u201cRouting Strategy\u201d for formatting consistency.\n \n\nTypo near bottom of page 8: \u201cissue may stes\u201d \u2192 \u201cissue may stem\u201d\n\n*Appendix p. 15*\n\u201cWe present a detailed learning efficiency experiment in Figure 7 across number of steps. It shows that MoE starts to outperform Dense counterparts right after 25k steps with instruction tuning.\u201d\n\n* There are no labels for the lines in the figure, thus it\u2019s impossible to tell which is the dense model and which is the MoE model\n\n*Appendix p. 15:* \u201cWe leave the study of scaling decoder-only FLAN-MOE as future works.\u201d --> \u201cWe leave the study of scaling decoder-only FLAN-MOE to future work.\u201d\n\n*Appendix p. 15:*\n\n\u201cbut yield worse performance\u201d \u2192 \u201cbut yields worse performance\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3245/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698294910934,
            "cdate": 1698294910934,
            "tmdate": 1699636272995,
            "mdate": 1699636272995,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rmxhjuvhu6",
                "forum": "6mLjDwYte5",
                "replyto": "CQjouS75C1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3245/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3245/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer SYmE"
                    },
                    "comment": {
                        "value": "Thank you for your helpful comments! We appreciate your recognition of the importance of our research as the pioneering study on MoE Models with instruction tuning, and for appreciating the thoroughness of our experiments and ablation studies. We are glad that these key aspects of our research were well received and appreciated. We address your questions below.\n\n> W1: There is little insight into what is causing the failure of direct finetuning of MoE models on downstream task data. It is unclear if instruction tuning is preventing this or if there are other factors at hand. \n\nThanks for pointing out, we included a discussion regarding the motivation quantitative and qualitative in general response 2. \n\nAlso, as highlighted in [1], MoE models often suffer from greater generalization challenges than their Dense counterparts, despite potentially having equivalent or lower pretraining losses. This is primarily because (1) their significantly larger total parameter count tends to predispose them to overfitting, and (2) the complexity introduced by different routing strategies, auxiliary losses, expert dropout rates, and sensitivities to batch size and precision, which result in a higher number of hyperparameters needing fine-tuning. Given this context, direct task-specific fine-tuning might render MoE models more prone to suboptimal performance compared to dense models. This paper aims to demonstrate that instruction tuning can act as an intermediary phase for pre-trained MoE models. It helps in stabilizing hyperparameters and bridging the gap between pre-training and task-specific fine-tuning, thereby enhancing their generalizability.\n\n[1] Artetxe, Mikel, et al. \"Efficient Large Scale Language Modeling with Mixtures of Experts.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.\n\n> Q1 Table 1: Why not show Switch and GS performance at the 32G FLOP scale?\n\nThis is a very insightful ablation to have! However, training a model with 32G FLOPs could span several months, and considering our current computational resource limitations, we have focused on scaling the most effective ST-MoE-32B models. We acknowledge the importance of this ablation and plan to address it in future work. \n\n> Q2 Figure 3: Why not label the orange and blue curves by model size, at least in the caption?\n\nThanks a lot for the suggestions, we have included the model sizes in the captions in the updated version. \n\n> Q3 How is expert utilization measured?\n\nThank you for emphasizing this important aspect. We have expanded on this point in the revised version of our paper. We calculate the utilization of each expert by dividing the number of tokens dispatched to it by the total capacity available to that expert. This calculation is averaged across all experts, providing a measure of the actual usage of each expert in the model.\n\n> Q4 Notes and minor details:\n\nWe corrected all the typos and added more clarifications of the capations in the updated version."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3245/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683594269,
                "cdate": 1700683594269,
                "tmdate": 1700683594269,
                "mdate": 1700683594269,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7jzGs1ulbt",
            "forum": "6mLjDwYte5",
            "replyto": "6mLjDwYte5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3245/Reviewer_YDYY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3245/Reviewer_YDYY"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the benefits of applying instruction-tuning to MoE models. It presents a series of instruction fine-tuned MoE models, called FLAN-MoE, which have shown superior performance over task-specific fine-tuned MoE and their corresponding dense models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This is a timely study, given that fine-tuning large-scale pre-trained MoE models for specific tasks is quite challenging.\n\n2. The paper provides relatively comprehensive studies of MoE models with instruction tuning, demonstrating that MoE models can benefit from additional instruction tuning.\n\n3. The findings are well-documented, including a range of MoEs sizes and discussions on limitations and failure cases."
                },
                "weaknesses": {
                    "value": "1. While several MoE models have been tested, the conclusion about the necessity of the instruction-tuning stage is not convincingly demonstrated. For instance, the addition of this instruction-tuning stage can introduce additional training and tuning costs, e.g., in comparison to using just task-specific fine-tuning. Is it possible that the performance improvement can also come from this extra training cost?\n\n2. Related to the training cost, the paper claims that its improvement does not stem from increased computational resources or memory requirements. However, this is a bit confusing because instruction fine-tuning in this paper clearly uses a large set of datasets for training, which incurs training costs. Yet no direct report in terms of the training cost is included in the paper. To be more convincing, a detailed report on how the proposed method affects training costs should be included.\n\n3. Some parts of the paper lack clarity. See detailed questions below."
                },
                "questions": {
                    "value": "1. Some statements made by the paper are rather confusing. For example, the paper states, \u201cHowever, we show that conventional, task-specific finetuning MoE models lead to suboptimal performance, often even worse than finetuning dense models with the same computational cost. One of the possible reasons is the discrepancy between general pretraining and task-specific finetuning.\u201d However, regardless of whether the model architecture is dense or sparse, isn't there always a discrepancy between pre-training and task-specific fine-tuning?\n\n2. When the paper says the FLAN-MoE \u201cdoes not come from increased computation resources or memory requirements,\u201d what does it mean? Does it refer to computation and memory requirements during training/inference compared to compute-equivalent dense/MoE models? \n\n3. The paper says, \u201cWe demonstrate that in the absence of instruction tuning, MoE models fall short in performance when compared to dense models on downstream tasks.\u201d However, this seems to be contradictory to some prior studies. For example, https://arxiv.org/pdf/2112.10684.pdf shows that MoE models can outperform compute-equivalent dense models on supervised fine-tuning tasks. \n\n4. What is the difference between FLAN-MoE and MoE in Section 4.1? \n\n5. The paper does not seem to describe the model architecture of FLAN-MoE adequately. The only one mentioned is ST-MoE-32B. It would be interesting to see how different pre-trained MoE models would affect the conclusion."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3245/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698418428891,
            "cdate": 1698418428891,
            "tmdate": 1699636272919,
            "mdate": 1699636272919,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VHhHW7BOm7",
                "forum": "6mLjDwYte5",
                "replyto": "7jzGs1ulbt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3245/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3245/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer YDYY (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your helpful comments! We appreciate your recognition of the importance of our research in fine-tuning large-scale pre-trained MoE models for specific tasks. \n\nWe are also grateful for your appreciation of the thoroughness and detailed documentation in our experiments and ablation studies. We are glad that these key aspects of our research were well received and appreciated. We address your questions below.\n\n> Q1: Clarifying the statement in the paper - \u201cHowever, we show that conventional, task-specific finetuning MoE models lead to suboptimal performance, often even worse than finetuning dense models with the same computational cost. One of the possible reasons is the discrepancy between general pretraining and task-specific finetuning.\u201d However, regardless of whether the model architecture is dense or sparse, isn't there always a discrepancy between pre-training and task-specific fine-tuning?\n\nThanks for raising this important clarification problem. We also updated the clarification in the introduction. Yes, the discrepancy between pre-training and task-specific fine-tuning always exists for both dense and MoE models. \n\nHowever, as pointed out in [1, 5, 2], MoE models may encounter even more generalization challenges even if they have the same or lower pertaining loss as Dense models given (1) the large total number of parameters makes it inclined to overfit; (2) different routing strategies and auxiliary losses, expert dropout rates, sensitivity to batch size and precision introduce more hyperparameters for MoE models to tune. \n\nConsidering this, direct task-specific finetuning may lead MoE models more easily to suboptimal performance compared to dense counterparts. In this paper, we are trying to show that instruction tuning could serve as a transition phase for pre-trained MoE models to fix the hyperparameters and mitigate the additional discrepancy between pre-training and task-specific fine-tuning, and therefore improve its generalizability. \n\n> Q2: Clarifying the statement in the paper - \u201cdoes not come from increased computation resources or memory requirements,\u201d Does it refer to computation and memory requirements during training/inference compared to compute-equivalent dense/MoE models?\n\nThank you for pointing out this essential aspect. For a detailed exploration of training and inference expenses, along with the FLOPs comparisons for MoE and Dense models, kindly refer to the thorough explanation provided in our general response 1.\n\n> Q3: Clarifying the statement in the paper - \u201cWe demonstrate that in the absence of instruction tuning, MoE models fall short in performance when compared to dense models on downstream tasks.\u201d However, [5] shows that MoE models can outperform compute-equivalent dense models on supervised fine-tuning tasks.\n\nThe mentioned work [5] is indeed a very relevant work and we included the discussion in the updated version. We want to kindly point out that task-specific finetuning results in mixed performance in Table 4 of [5]. \n\nSpecifically:\n- Compared to without finetuning,\u2018\u2019fine-tuning of MoE models produces substantial benefits for Storycloze, BoolQ, SST-2, MNLI, and some improvements on OpenBookQA, it results in worse performance for HellaSwag, PIQA, and Winogrande.\u2019\u2019\n- Compared to finetuning dense models, finetuning of MoE models generates mixed performance in general, according to Table 4 of [5].\n\nThe tasks we choose for fine-tuning are following [6] (CondaQA, CxC, PubmedQA, and SearchQA), which could be domain-specific and present extra challenges for MoE models and therefore suboptimal performance. We added more clarifications and the discussion regarding MoE performance challenge in task-specific fine-tuning in the updated version."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3245/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684875165,
                "cdate": 1700684875165,
                "tmdate": 1700684875165,
                "mdate": 1700684875165,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HRpLFvPK9R",
            "forum": "6mLjDwYte5",
            "replyto": "6mLjDwYte5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3245/Reviewer_nXAt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3245/Reviewer_nXAt"
            ],
            "content": {
                "summary": {
                    "value": "MoE model is a sparse model architecture that can be utilized to scale the number of parameters without significantly increasing the computation cost. In this research, the authors conducted experiments comparing dense models with MoE models using instruction tuning. The results indicate that combining sparse MoE models and instruction tuning leads to a significant enhancement in model performance, surpassing dense models across various datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ This paper tries to apply the instruction tuning to the context of MoE models for downstream tasks. The experimental results demonstrate the combination has great potential to improve the performance of large language models.\n\n+ The authors conducted comprehensive experiments on various sparse and dense models to support their claim, and in most cases, the combination of instruction tuning and MoE models shows strong performance over other models."
                },
                "weaknesses": {
                    "value": "+ Lack of clear motivation. The motivation behind the combination of MoE with instruction tuning requires further discussion. While it is acknowledged that instruction tuning and MoE models can outperform dense models or fine-tuning MoE, it will be beneficial to provide some insights into why these approaches were chosen.\n\n+ The presentation of this paper needs some improvements. Some grammar things could be improved in the explanation and discussion of the key component of this paper. \n\n+ The impact of the combination design (instruction tuning and MoE) on training and inference time should have more discussion."
                },
                "questions": {
                    "value": "1. I understand the performance of instruction tuning on MoE models, but can you please provide any analysis or insight about the reasons behind such good performance? Does it help improve the routing strategy, expert specialization, or something else?\n\n2. The author claims that these advancements are attained without necessitating an increase in computational resources or even reducing the resource requirements. I am confused about how it can reduce resource requirements in the training and inference time. Can you discuss more about the details of the process?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3245/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722449640,
            "cdate": 1698722449640,
            "tmdate": 1699636272838,
            "mdate": 1699636272838,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "MNplU4nrUS",
            "forum": "6mLjDwYte5",
            "replyto": "6mLjDwYte5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3245/Reviewer_8EEX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3245/Reviewer_8EEX"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the role of instruction tuning in whether mixture of experts models outperform dense models on language tasks. It turns out that instruction tuned mixture of experts performs better than dense models."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1) This paper does not really present a new algorithm or theory, but is mostly a large collection of experiments showing under what conditions the proposed Flan-MoE model performs well. With that being said, the number and thoroughness of the experiments and ablations is quite impressive. \n2) The authors acknowledge limitations of MoE models and show to mitigate them, i.e. using auxiliary loss to mitigate overfitting"
                },
                "weaknesses": {
                    "value": "I don't see any"
                },
                "questions": {
                    "value": "1) The authors state that MoE can be used to add learnable parameters to LLMs \"without increasing inference cost.\" I think this is somewhat confusing. Increased memory usage is as much of a \"cost\" as increased FLOPs or latency."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3245/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699372125362,
            "cdate": 1699372125362,
            "tmdate": 1699636272777,
            "mdate": 1699636272777,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4T40OAnMXk",
                "forum": "6mLjDwYte5",
                "replyto": "MNplU4nrUS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3245/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3245/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful feedback! We are grateful for your acknowledgment of the significance of our work in empirically enhancing pre-trained MoE models through instruction tuning, backed by extensive experiments. \n\nWe thank your appreciation of our effort to highlight the limitations and failure cases and we are glad that these key aspects of our research were well received and appreciated. We address your questions below.\n\n> W1: The authors state that MoE can be used to add learnable parameters to LLMs \"without increasing inference cost.\" Could the author elaborate on the increased memory usage or increased FLOPs or latency.\n\nThank you for highlighting this important aspect! We provide a detailed discussion in general response 2 w.r.t training and inference cost as well as FLOPs for MoE and Dense models for your reference."
                    },
                    "title": {
                        "value": "Reply to Reviewer 8EEX"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3245/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682678127,
                "cdate": 1700682678127,
                "tmdate": 1700685212054,
                "mdate": 1700685212054,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]