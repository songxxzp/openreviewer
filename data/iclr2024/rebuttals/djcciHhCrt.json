[
    {
        "title": "Misusing Tools in Large Language Models With Visual Adversarial Examples"
    },
    {
        "review": {
            "id": "yRpkmpyPov",
            "forum": "djcciHhCrt",
            "replyto": "djcciHhCrt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9046/Reviewer_cgCA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9046/Reviewer_cgCA"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies prompt injection against large language models (LLMs) through the visual modality, i.e., crafting (universal) adversarial image examples to cause the LLM to invoke tools following real-world syntax. In particular, the authors highlight the consideration of attack stealthiness in terms of both perturbation imperceptibility and response utility. Five variants of attacks with varied attack difficulty are considered."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Prompt injection against LLMs is a promising direction, especially through the relatively new channel of visual modality.\n- The paper is very well written, including sufficient example visualizations and clearly described technical details.\n- Experiments are extensive and insightful, which include human studies and ablation studies of important hyperparameters."
                },
                "weaknesses": {
                    "value": "- The best result is reported among three trials based on the argument that \u201cattackers will always choose the best-performing adversarial image.\u201d However, the reviewer thinks this may not be reasonable because the attacker is not the user and so cannot control how many times they would repeat the attack. The authors should explain why they think it is feasible to stick to this setting. This is important given the fact that significant randomness during adversarial image training is observed.\n\n- The authors motivate the design of separate losses for response utility and tool invocation (i.e., Equation 2) based on the argument \u201cIn real-world conversational systems...we reduce the contribution of the loss term...\u201d First of all, there are comparisons validating the superiority of using such separate losses to the integrated loss (i.e., Equation 1). More specifically, the ablation studies clearly show using a large $\\lambda$, i.e. 1, works better, which conflicts with the argument about \u201creducing the contribution...\u201d.\n\n- It is said that \u201cthe l2 norm is computed with regard to each color channel separately\u201d. However, to the best knowledge of the reviewer, in the literature of adversarial examples, it is indeed computed not separately. Could the authors explain why they chose this unusual setting? \n\n- Considering that the threat model follows the typical prompt injection, it seems unnecessary to highlight its differences from \u201cjailbreaking\u201d. Therefore, the authors are encouraged to tune down some related claims. \n\n- The fact that only three images are tested should be mentioned in the main text rather than only in the appendix."
                },
                "questions": {
                    "value": "See the above weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9046/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697797914696,
            "cdate": 1697797914696,
            "tmdate": 1699637139586,
            "mdate": 1699637139586,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Rbj2hL5L54",
                "forum": "djcciHhCrt",
                "replyto": "yRpkmpyPov",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9046/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9046/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback! Here are our responses to your questions and suggestions. \n\n> The best result is reported among three trials based on the argument that \u201cattackers will always choose the best-performing adversarial image.\u201d However, the reviewer thinks this may not be reasonable because the attacker is not the user and so cannot control how many times they would repeat the attack. The authors should explain why they think it is feasible to stick to this setting. This is important given the fact that significant randomness during adversarial image training is observed.\n\nIt\u2019s 100% true that the attacker has no control over the user, and it\u2019s the exact reason  why the attacker wants the image that is the best performing such that it is expected to affect the most victims.\n\n> The authors motivate the design of separate losses for response utility and tool invocation (i.e., Equation 2) based on the argument \u201cIn real-world conversational systems...we reduce the contribution of the loss term...\u201d First of all, there are comparisons validating the superiority of using such separate losses to the integrated loss (i.e., Equation 1). More specifically, the ablation studies clearly show using a large lambda, i.e. 1, works better, which conflicts with the argument about \u201creducing the contribution...\u201d.\n\nWe will update the text to \u201cattempt to reduce the contribution\u201d as we designed this loss based on intuitions and later experimented with them. During our experiments, we found that lambda_i provides a good tradeoff between imperceptibility and goal achieving (Table 7), while the effect of lambda_r is more ambivalent (Table 6) \u2014 while the response utility drops, the goal achieving likelihood for the unrelated set increases during (lambda_r 1.0 -> 0.1) but further decreases in the range of (lambda_r 0.1 -> 0), while mostly stays the same for the related set. Based on the exact values, we believe an attack that has higher response utility is more favorable to the attacker, therefore in the main experiments, we went with a lambda_r = 1.0. Thanks for pointing this out.\n\n> It is said that \u201cthe l2 norm is computed with regard to each color channel separately\u201d. However, to the best knowledge of the reviewer, in the literature of adversarial examples, it is indeed computed not separately. Could the authors explain why they chose this unusual setting?\n\nIt is actually a historical issue where our initial attempts adopt different normalization coefficients separately for each channel in the preprocessing stage, making us also calculate the l2 norm separately. Given that using both formats can achieve the goal of minimizing the difference between the adversarial image and the original one, we kept the previous way of computing and reported it in the paper for reproducibility.\n\n> Considering that the threat model follows the typical prompt injection, it seems unnecessary to highlight its differences from \u201cjailbreaking\u201d. Therefore, the authors are encouraged to tune down some related claims.\n\nThanks for your suggestion. We have shortened the discussion about the jailbreaking issue in section 2 to make room for other revisions.\n\n\n>  The fact that only three images are tested should be mentioned in the main text rather than only in the appendix.\n\nThanks for pointing this out. We will make this more explicit in the main text. We picked the three images from different sources (we described in the appendix that one the logo of the visual language model we are using, one generated through stable diffusion, and one from online sources), hoping that the present a good coverage over image types. We happily triple the number of images and provide six more examples in Table 9 (Appendix A.9), including two from the Imagenet dataset, and two more each coming from model generations and online sources. We couldn\u2019t conduct a larger experiment with more images due to time constraints but we hope that this provides a good indicator that the attack method would work generally on many images."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9046/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700182366157,
                "cdate": 1700182366157,
                "tmdate": 1700182366157,
                "mdate": 1700182366157,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NlcT9dqVA4",
                "forum": "djcciHhCrt",
                "replyto": "Rbj2hL5L54",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9046/Reviewer_cgCA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9046/Reviewer_cgCA"
                ],
                "content": {
                    "title": {
                        "value": "thanks for your response"
                    },
                    "comment": {
                        "value": "Q1. I still think it is improper to report the best attack performance rather than the average performance (with std.). The authors may describe more details about when the attack happened. As the authors stated, if the attacker can control the image completely, it should happen before the user touches that image.\n\nQ2. I do not quite understand the response, does the author mean \"They try to do that but finally fail?\" If so, I don't think simply tweaking the wording is enough. Since the loss term does not work as expected, it should be not considered at all.\n\nFor the other concerns, the authors have promised to address them in the final version."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9046/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498586416,
                "cdate": 1700498586416,
                "tmdate": 1700498586416,
                "mdate": 1700498586416,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xqJHId1TEQ",
                "forum": "djcciHhCrt",
                "replyto": "NmdP1np2lA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9046/Reviewer_cgCA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9046/Reviewer_cgCA"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the further reply."
                    },
                    "comment": {
                        "value": "My previous concerns are addressed. I still lean toward accepting the paper considering the new threat model and comprehensive explorations. However, I do agree with the reviewers that testing the attack against a real-world VLM system is needed, especially considering that the attack focuses on application-level threats."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9046/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558473021,
                "cdate": 1700558473021,
                "tmdate": 1700558473021,
                "mdate": 1700558473021,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "L67XrJ8Pr2",
            "forum": "djcciHhCrt",
            "replyto": "djcciHhCrt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9046/Reviewer_La6s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9046/Reviewer_La6s"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method to cause malicious tool usage of LLM by using adversarial examples. The idea is to use a loss that includes both the response utility and malicious behavior for training"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper studies a new and timely security issue of LLM.\n2. The proposed method achieves better stealthiness over prior works."
                },
                "weaknesses": {
                    "value": "1. The proposed method seems to be straightforward, which is essentially the way of injecting a backdoor. It is unclear how this method can help generalize the trigger to other prompts. \n\n2. Since the goal is to trigger the misusage of tools, why limiting the adversarial perturbation and enhancing the generalizability are important? The adversary only needs one prompt to trigger the malicious usage of the tools. \n\n3. It is unclear what are the implications of these 5 attack objectives. Does the selection of these attack objectives have an impact on the attack performance? What will happen if more attack objectives are included?\n\n4. The evaluation is unsatisfactory. L_p norm is not provided. Also, it is important to show the chances that the malicious tool is triggered when the adversarial example and prompt pair are not present, or only one of these two is present."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper presents an attack against LLMs that are integrated with third-party tools. However, ethics concerns are not discussed."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9046/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698546222918,
            "cdate": 1698546222918,
            "tmdate": 1699637139439,
            "mdate": 1699637139439,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JyGxOnZNQa",
                "forum": "djcciHhCrt",
                "replyto": "L67XrJ8Pr2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9046/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9046/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> The proposed method seems to be straightforward, which is essentially the way of injecting a backdoor. It is unclear how this method can help generalize the trigger to other prompts.\n\nWe refer the reviewer to the response for Review tv67 above for reasons as to why our contribution matters. To briefly summarize that response, our contribution is in the computer security space \u2013 we show a new threat model for multi-modal LLMs. The fact that an attack is straightforward is a big advantage because attackers always choose the path of least resistance. Furthermore, given that there is 10 years of work on improving visual adversarial example attacks, all of that is now applicable to multi-modal LLMs \u2013 this is a formidable arsenal for attackers targeting LLMs. We view our work as opening the flood-gates of attacks on LLMs. It also serves as strong motivation to start investing in research to defend against these attacks. \n\nTable 4 and 5 establishes that our method generalizes to multiple types of prompts and function invocation syntaxes. \nBeyond that, with regards to the method itself, being able to generalize our attack to various or even arbitrary text prompts provided by the victim user with a decent success rate (on desired tool invocation), while maintaining the response and image stealthiness is a non-trivial task itself and has never been accomplished by any prior work in this domain. Additionally, an attack that is not prohibitively complicated in its nature means it\u2019s more dangerous from a security perspective since it\u2019s more easily deployable and more likely to be used by real world attackers.\n\n> Since the goal is to trigger the misusage of tools, why limiting the adversarial perturbation and enhancing the generalizability are important? The adversary only needs one prompt to trigger the malicious usage of the tools.\n\n\nAttackers ultimately want the attack to be spread wide and long enough to affect as many victims as possible. The image and response stealthiness are critical in this sense because victims will otherwise easily spot the abnormality of the image or the conversation and may even report this to the model provider.\n\n> It is unclear what are the implications of these 5 attack objectives. Does the selection of these attack objectives have an impact on the attack performance? What will happen if more attack objectives are included?\n\nWe chose these five attack objectives to simulate the real-world behavior as realistically as possible by following invocation format from real world systems (see Section 3.1). In fact, the string <function.delete_email which=\"all\"> is the function call format specified in an [older version](https://github.com/microsoft/semantic-kernel/blob/334b22a78460a46b91391c1b41f79e23d55338c2/dotnet/src/Extensions/Planning.SequentialPlanner/skprompt.txt) of Microsoft Semantic Kernel. And the Expedia invocation json is an exact copy of the one generated by ChatGPT (and json is the [standard format](https://platform.openai.com/docs/guides/function-calling) for any plugin calls in ChatGPT). We believe it\u2019s a fair claim that the textual format attack objectives we\u2019ve tested does reflect practical scenarios and demonstrates the potential of our attack if they could be applied on real-world systems. We have revised the paper to clarify.\nAlso, these five attack objectives represent increasing levels of complexity and difficulty to generate the desired attack and serve as examples to understand whether it is still possible and how easy/difficult it is when the target texts to be generated by LLMs are non natural and complicated."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9046/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700182290146,
                "cdate": 1700182290146,
                "tmdate": 1700182290146,
                "mdate": 1700182290146,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wvuVNO6T2b",
                "forum": "djcciHhCrt",
                "replyto": "L67XrJ8Pr2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9046/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9046/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">  The evaluation is unsatisfactory. L_p norm is not provided. Also, it is important to show the chances that the malicious tool is triggered when the adversarial example and prompt pair are not present, or only one of these two is present.\n\nWe additionally provide the L_2 norm and L_inf norm between the adversarial image and its original in Table 4. (Note that Table 5 corresponds to the same adversarial images thus the L_p norms would be the same if provided). Additionally, in appendix Table 7, we provide the L_p norms to understand the quantities when the SSIM score is low; in appendix figure 5, a plot of SSIM values and L_2 norm for experiments in the paper is provided, which shows that SSIM is almost (inverse) proportional to the L_2 norms (as expected).\n\nRegarding the other scenarios of the attack: we are interested in the scenario when the attacker specified tool invocation should appear when the adversarial example is __present__ with an __arbitrary__ prompt provided along with it. When there is no adversarial image provided but only a text prompt, the LLM should respond to the prompt as usual. In the rare case when the user provides no text prompt provided along with the adversarial image, the LLM we tested would respond as if there is a prompt asking \u201cdescribe this image\u201d, and for several of our trained adversarial images, also generate the tool invocation."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9046/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700182297610,
                "cdate": 1700182297610,
                "tmdate": 1700182297610,
                "mdate": 1700182297610,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GN0Sko9J8k",
                "forum": "djcciHhCrt",
                "replyto": "JyGxOnZNQa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9046/Reviewer_La6s"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9046/Reviewer_La6s"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response!"
                    },
                    "comment": {
                        "value": "I appreciate the authors' response. However, the threat model and the setting of the problem are still not very well motivated. The function invocation can be easily addressed by a safeguard after the LLM, if we consider an adaptive defender, which we should do in practice. Besides, in the proposed framework, function invocation can be replaced by any other payloads. It is still not clear to me why enhancing the generalizability is important. Similarly, if we consider an adaptive defender, \"Attackers ultimately want the attack to be spread wide and long enough to affect as many victims as possible\" may not be a valid assumption."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9046/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706177106,
                "cdate": 1700706177106,
                "tmdate": 1700706177106,
                "mdate": 1700706177106,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wddf7RxCXh",
            "forum": "djcciHhCrt",
            "replyto": "djcciHhCrt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9046/Reviewer_NtXf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9046/Reviewer_NtXf"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes using visual adversarial examples to attack vision-integrated LLMs. Particularly, this paper focuses on attacking the LLMs to affect the confidentiality and integrity of users' resources connected to the LLMs. The proposed attack is stealthy (as visual adversarial examples look similar to normal images), and generalizable (the adversarial examples can trigger the targeted generation when paired with different text prompts). And it is shown that the attack can successfully trigger LLMs to output the tools-abusing texts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The big picture of the paper is sound. Indeed, as LLMs are integrated into applications, critical resources may be controlled by the models. Then, attacks on the models can induce broad implications beyond just the misalignment moral values. The threat model and the real-world risk analysis in this paper are quite insightful. \n\n2. The approach is simple and effective.\n\n3. The authors make efforts to collect evaluation datasets as well as comprehensive human evaluation."
                },
                "weaknesses": {
                    "value": "1. **Only a single model LLaMA Adapter is tested.** This makes the scope of the evaluation look somewhat narrow. I suggest the authors also consider other VLMs like Minigpt-4 [1], Instruct-Blip [2], and LLaVA [3]. This can make the evaluation more convincing. \n\n2. **Lack of case studies on real LLM-integrated applications.**  The paper mentioned that LangChain and Guidance facilitate the development of such integrations. But, the paper did not provide a single instance of this to illustrate the practical risks of the proposed attack. In the whole paper, what the attack did was just to induce the generation of something like <function.delete_email which=\"all\"> in a purely textual form, which is essentially nothing different from previous NLP attacks that induced certain targeted generations. In my opinion, the novelty of this paper only comes from the illustration of the \"practical risks\" of such attacks --- because the resources controlled by LLMs can now also be missed to induce broader harms. However the paper did not provide a real example of this. The whole evaluation is still in a purely textual form, judging whether things like <function.delete_email which=\"all\"> are generated... In practice, the LLMs integrated systems may be more complicated than this conceptual form. The paper did not go deep into this. \n\n3. **Inaccurate Literature Review.** There is a factual error in the literature review. Qi et. al. [4] did not show transferability to closed-source models. On the other hand, Carlini et. al. [5] along with Qi et. al. [4] are earlier works showing the usage of visual adversarial examples to hack VLM, which may also need to be noted.\n\n\n\n[1] Zhu, D., Chen, J., Shen, X., Li, X. and Elhoseiny, M., 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592.\n\n[2] Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P. and Hoi, S., 2023. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. arXiv preprint arXiv:2305.06500.\n\n[3] Liu, H., Li, C., Wu, Q. and Lee, Y.J., 2023. Visual instruction tuning. arXiv preprint arXiv:2304.08485.\n\n[4] Qi, X., Huang, K., Panda, A., Wang, M. and Mittal, P., 2023, August. Visual adversarial examples jailbreak aligned large language models. In The Second Workshop on New Frontiers in Adversarial Machine Learning.\n\n[5] Carlini, N., Nasr, M., Choquette-Choo, C.A., Jagielski, M., Gao, I., Awadalla, A., Koh, P.W., Ippolito, D., Lee, K., Tramer, F. and Schmidt, L., 2023. Are aligned neural networks adversarially aligned?. arXiv preprint arXiv:2306.15447."
                },
                "questions": {
                    "value": "Can the attack be directly applied to realistic LLM-integrated applications in the wild? Say, other prompt injection attacks such as [1,2] do show realistic instances. \n\n\n[1] Greshake, K., Abdelnabi, S., Mishra, S., Endres, C., Holz, T. and Fritz, M., 2023. Not what you\u2019ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. arXiv preprint arXiv:2302.12173.\n\n[2] Liu, T., Deng, Z., Meng, G., Li, Y. and Chen, K., 2023. Demystifying RCE Vulnerabilities in LLM-Integrated Apps. arXiv preprint arXiv:2309.02926."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9046/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9046/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9046/Reviewer_NtXf"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9046/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734812946,
            "cdate": 1698734812946,
            "tmdate": 1699637139328,
            "mdate": 1699637139328,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FFMPVCAbQ9",
                "forum": "djcciHhCrt",
                "replyto": "wddf7RxCXh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9046/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9046/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback! Here are our responses to your questions and suggestions. \n\n>Only a single model LLaMA Adapter is tested. This makes the scope of the evaluation look somewhat narrow. I suggest the authors also consider other VLMs like Minigpt-4 [1], Instruct-Blip [2], and LLaVA [3]. This can make the evaluation more convincing.\n\nWe acknowledge this and we are currently working on experiments on other multimodal models. The result will be presented in the updated version soon.\n\n> Lack of case studies on real LLM-integrated applications. The paper mentioned that LangChain and Guidance facilitate the development of such integrations. But, the paper did not provide a single instance of this to illustrate the practical risks of the proposed attack. In the whole paper, what the attack did was just to induce the generation of something like <function.delete_email which=\"all\"> in a purely textual form, which is essentially nothing different from previous NLP attacks that induced certain targeted generations. In my opinion, the novelty of this paper only comes from the illustration of the \"practical risks\" of such attacks --- because the resources controlled by LLMs can now also be missed to induce broader harms. However the paper did not provide a real example of this. The whole evaluation is still in a purely textual form, judging whether things like <function.delete_email which=\"all\"> are generated... In practice, the LLMs integrated systems may be more complicated than this conceptual form. The paper did not go deep into this.\n\nWe tried to simulate the real-world behavior as realistically as possible by following invocation format from real world systems (see Section 3.1). In fact, the string <function.delete_email which=\"all\"> is the function call format specified in an [older version](https://github.com/microsoft/semantic-kernel/blob/334b22a78460a46b91391c1b41f79e23d55338c2/dotnet/src/Extensions/Planning.SequentialPlanner/skprompt.txt) of Microsoft Semantic Kernel. And the Expedia invocation json is an exact copy of the one generated by ChatGPT (and json is the [standard format](https://platform.openai.com/docs/guides/function-calling) for any plugin calls in ChatGPT). We believe it\u2019s a fair claim that the textual format attack objectives we\u2019ve tested does reflect practical scenarios and demonstrates the potential of our attack if they could be applied on real-world systems. We have revised the paper to clarify. \nAlso, compared to previous work\u2019s targets, a crucial difference of our attack goal is that these tool invocation texts are non natural. We believe it is important to understand whether it is still possible and how easy/difficult it is when the target texts are non natural. Our evaluation provides some answers to this question, with various length of the target text as another dimension.\n\n> Inaccurate Literature Review. There is a factual error in the literature review. Qi et. al. [4] did not show transferability to closed-source models. On the other hand, Carlini et. al. [5] along with Qi et. al. [4] are earlier works showing the usage of visual adversarial examples to hack VLM, which may also need to be noted.\n\nThank you for pointing these out!  We have corrected these errors in the revised version."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9046/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700182252557,
                "cdate": 1700182252557,
                "tmdate": 1700182252557,
                "mdate": 1700182252557,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yZ42iACq53",
                "forum": "djcciHhCrt",
                "replyto": "FFMPVCAbQ9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9046/Reviewer_NtXf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9046/Reviewer_NtXf"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thanks for the revision. However, one concern still remains that --- essentially, the whole work is merely to trigger certain targeted text outputs. There are no realistic case studies to show the misuse of tools in practical setups. This makes the contribution of this work unclear, as also pointed out by Reviewer tv67."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9046/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700396012718,
                "cdate": 1700396012718,
                "tmdate": 1700396012718,
                "mdate": 1700396012718,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DhvB4lyPUC",
            "forum": "djcciHhCrt",
            "replyto": "djcciHhCrt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9046/Reviewer_tv67"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9046/Reviewer_tv67"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an attack on multi-modality models that are capable of using tools. The main idea of this paper is to perturb the image side so that the text side can generate malicious instructions that may impact downstream tools. The paper demonstrates that their attack is effective, stealthy, and generalizable."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- In general, this paper is well-structured and easy to follow.\n- I believe the problem this paper addresses is highly significant. It focuses on understanding how to attack systems using LLMs in real-world scenarios, which presents new challenges when viewed from a systemic perspective.\n- The experimental results demonstrate that malicious instructions can be generated by perturbing the input image."
                },
                "weaknesses": {
                    "value": "- This paper assumes that interaction with the tools occurs through an instruction line, followed by normal question answering, as illustrated in Figure 1. Is this setting realistic? What does a real system look like, and how do these VLMs interact with downstream tools like email? Please provide an illustration of why the task in Figure 1 is realistic.\n- This paper lacks technical contributions and depth. The technical contribution of this paper is to generate perturbations on the image side that can prompt the language model to output specific words. However, this paper does not discuss the technical challenges associated with this attack scenario. Instead, they employ a simple gradient-based technique widely adopted in adversarial example attacks. They also do not provide an in-depth analysis of the drawbacks of this technique. For example, is this attack easily bypassed? What is the robustness of this attack?\n- Additionally, the authors claim that the attack is stealthy because it does not alter the semantic meanings of the output answers. They support this claim with the evidence that the answers under attack are 10% less natural compared to the original ones. My question is, why is a 10% difference considered a small one?\n- If the current setting is realistic, I suggest showing the effectiveness of attacking the real-world system."
                },
                "questions": {
                    "value": "To conclude, I think the problem this paper would like to address is attractive. However, it is not clear why the current setting is realistic. Also, the technique contributions and discussion depth hinder its acceptance. I believe these questions are hard to be properly addressed during the rebuttal period."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9046/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699200974070,
            "cdate": 1699200974070,
            "tmdate": 1699637139223,
            "mdate": 1699637139223,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BnGPxRScif",
                "forum": "djcciHhCrt",
                "replyto": "DhvB4lyPUC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9046/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9046/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. We believe there might be a miscommunication  of our motivation and contributions. We attempt to better clarify as follows.\n\n> This paper assumes that interaction with the tools occurs through an instruction line, followed by normal question answering, as illustrated in Figure 1. Is this setting realistic? What does a real system look like, and how do these VLMs interact with downstream tools like email? Please provide an illustration of why the task in Figure 1 is realistic.\n\nCommercial LLMs like ChatGPT and Google Bard support visual images as an input modality by default. Their interaction method assumes an image being uploaded with a prompt alongside, exactly like what we have described in Figure 1. They are also integrating a large number of extensions/plugins which include ones necessary for our attack objective such as email connectors , [google workspace](https://blog.google/products/bard/google-bard-new-features-update-sept-2023/), Expedia, etc. A version of our attack on these models would successfully invoke the plugins/extensions integrated by the victim user. We note that the attacker-desired invocation string can be changed to invoke any other plugin/extension than the 5 attack objectives we have shown in Table 1). **Therefore, our proposed attack scenario is highly realistic even at this moment.**  The trend of more third-party customized versions of GPT ([GPTs](https://openai.com/blog/introducing-gpts)) that will incorporate more powerful tools to LLMs in custom domains will only make our attack more applicable. We have revised the paper to clarify this.\n\n> This paper lacks technical contributions and depth. The technical contribution of this paper is to generate perturbations on the image side that can prompt the language model to output specific words. However, this paper does not discuss the technical challenges associated with this attack scenario. Instead, they employ a simple gradient-based technique widely adopted in adversarial example attacks. They also do not provide an in-depth analysis of the drawbacks of this technique. For example, is this attack easily bypassed? What is the robustness of this attack?\n\nFrom a computer security perspective, we contribute a new threat model \u2014 an attacker can create stealthy images that result in tool misuse. This shows that the past 10 years of attack research on vision adversarial examples can be applied to multi-modal LLMs. This highlights a critical new class of vulnerabilities in LLMs. Furthermore, given that there isn\u2019t an airtight defense for visual adversarial examples, it implies that multimodal LLMs are fundamentally vulnerable to an attack vector with no known good defenses. Furthermore, attackers always prefer the path of least resistance. The simpler an attack technique is, the more likely that attack will actually occur. Thus, we view the simplicity of the attack technique in the paper as an advantage. Finally, from an ML perspective, we observe that a challenge was to balance three competing objectives: (1) getting the LLM to produce a very specific function invocation syntax; (2) Maintaining a natural-semantics conversation with the user; and (3) Keeping the image modifications stealthy."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9046/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700182109054,
                "cdate": 1700182109054,
                "tmdate": 1700182109054,
                "mdate": 1700182109054,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oWowUdB947",
                "forum": "djcciHhCrt",
                "replyto": "DhvB4lyPUC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9046/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9046/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Additionally, the authors claim that the attack is stealthy because it does not alter the semantic meanings of the output answers. They support this claim with the evidence that the answers under attack are 10% less natural compared to the original ones. My question is, why is a 10% difference considered a small one?\n\nDuring our human evaluation, we present the annotators with responses to the questions generated with and without the image being adversarially perturbed. On the image-unrelated subset, human rated 86% of the responses generated with the unperturbed image as natural, while 77% of the responses generated with the perturbed image are annotated as natural. Here, we concluded that in an absolute value, only about 10% sentences on average would make the user feel more unnatural. But what\u2019s probably equally important is that 77% (37% for image-related) of the responses are natural to humans, therefore they wouldn\u2019t realize something unintended has gone wrong. This is why we believe such an attack is stealthy. \n\n> If the current setting is realistic, I suggest showing the effectiveness of attacking the real-world system.\n\nAttacking an existing system is our current goal. We are working on blackbox transferability to be able to attack the well known systems with no publicly-available weights. But again, one of the goals of this work is to show new attack vectors and threats. Even though we were not able to carry our attack on real-world systems yet, the five attack objective we described in Table 1 are all following tool invocation syntax from real world systems (see Sec 3.1) to simulate the behavior as realistically as possible, which demonstrates the potential of our attack if they could be applied on real-world systems."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9046/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700183006892,
                "cdate": 1700183006892,
                "tmdate": 1700183006892,
                "mdate": 1700183006892,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T7AjM25YOv",
                "forum": "djcciHhCrt",
                "replyto": "BnGPxRScif",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9046/Reviewer_tv67"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9046/Reviewer_tv67"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the reply."
                    },
                    "comment": {
                        "value": "1. For the realistic issue, I don't see why the output format, i.e., natural question answer plus an invocation comment, is realistic for commercial VLMs. \n2. For the contribution, I think there are several works that already prove adversarial attacks on the image side can let VLM models output attacker-desired texts (please search for them in google scholar). I don't see a fundamental difference between this paper and these methods, except the desired text in this context is the invocation strings."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9046/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218995749,
                "cdate": 1700218995749,
                "tmdate": 1700218995749,
                "mdate": 1700218995749,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]