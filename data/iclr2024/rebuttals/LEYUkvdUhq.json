[
    {
        "title": "ZipIt! Merging Models from Different Tasks without Training"
    },
    {
        "review": {
            "id": "xpKV3d1X1L",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1179/Reviewer_SEiC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1179/Reviewer_SEiC"
            ],
            "forum": "LEYUkvdUhq",
            "replyto": "LEYUkvdUhq",
            "content": {
                "summary": {
                    "value": "This paper tackles the problem of merging two models trained on different \"tasks\". Their best performing approach is two-fold: 1) identify redundant features within/across models and average those, and 2) only merge some layers. Combined they achieve performance which approaches the ensemble, but their improvements are most notable when the two \"tasks\" come from the same task, such as partitioning CIFAR in two parts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The presentation, including all Figures, is very clear!\n- The idea of merging redundant features is interesting and effective. It seems to me that the authors should consider exploring this as a general way of speeding up inference and not just as a technique for merging models."
                },
                "weaknesses": {
                    "value": "- I would appreciate more motivation for this problem. When would I find myself training from scratch on two different tasks?\n- [Subjective] I suggest the authors are much more clear/transparent about their contributions early, because currently the readers get their hopes up a lot when in reality the best performance occurs either when the \"tasks\" really come from the same task like splitting CIFAR in half. I think there's still a long way to go for training individually on two distinct datasets that are ImageNet level difficulty.\n- Figure 1a/b is not exactly clear with respect to related work, for instance [1] (Sec 3.3), and [2] (Appendix E) do consider merging models trained on different tasks, albeit from the same initialization.\n\n[1] Merging Models with Fisher-Weighted Averaging (https://arxiv.org/abs/2111.09832)\n[2] Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time (https://arxiv.org/abs/2203.05482)"
                },
                "questions": {
                    "value": "- I would be curious when the authors believe this approach is best applied, e.g., when would I find myself training from scratch on two different tasks?\n- Does this approach improve accuracy in more traditional model merging settings? It's pitched for multi-task but is it more general?\n- The \"scaling\" trend in Figure 6b is very interesting! I would be curious to hear if the authors would expect the same trend with a different method to increase FLOPs on the x-axis, such as depth scaling, or joint scaling, or training iterations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1179/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697151024901,
            "cdate": 1697151024901,
            "tmdate": 1699636044485,
            "mdate": 1699636044485,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vPgUSGz0dn",
                "forum": "LEYUkvdUhq",
                "replyto": "xpKV3d1X1L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SEiC"
                    },
                    "comment": {
                        "value": "We thank you for your thoughtful review and suggestions. We have collected our response below.\n\n**W1 / Q1. What is the motivation for this? When would I find myself training from scratch on two different tasks?**  \nThis is a good question. The assumption that we and prior work make in this setting isn\u2019t that you would train these models yourself, but that they would be given to you (e.g., downloaded from the internet), or trained in a federated learning setting where due to, e.g., privacy reasons, not all the data is available at once. In those cases, we would like to be able to merge the models without any additional training, in the former because it may be too expensive, and in the latter because we are not allowed to access the original data.\n\nWe only train these models ourselves to explicitly control the evaluation setting. Note that some of the models in the multi-dataset experiment were not trained by us.\n\n**W2. More transparent claims.**  \nThank you for this comment. We\u2019ve updated the draft to tone down the language in our PDF abstract and Fig 1 caption.\n\n**W3. Figure 1a/b is not exactly clear with respect to related work.**  \nWe appreciate the reviewer for pointing this out and we completely agree. We have updated our draft to specify methods with _different initializations_ in the caption of that figure. \n\n**Q2. Does this improve on same data merging settings?**  \nWe match the performance of baselines on the same data settings, but we focus on the multi-dataset case.\n\n**Q3. I would be curious to hear if the authors would expect the same trend with a different method to increase FLOPs on the x-axis, such as depth scaling, or joint scaling, or training iterations.**  \nWe did not conduct this search, but it is unlikely that we would see the same magnitude of gains as from scaling width. Mode connectivity heavily relies on the width of models, not necessarily the depth or other possible scaling components. Though, ZipIt is less constrained because of Theorem 1, so it is likely that it would still outperform the other methods in those cases."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245134123,
                "cdate": 1700245134123,
                "tmdate": 1700245134123,
                "mdate": 1700245134123,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R7wTynPCVj",
                "forum": "LEYUkvdUhq",
                "replyto": "vPgUSGz0dn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1179/Reviewer_SEiC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1179/Reviewer_SEiC"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609577617,
                "cdate": 1700609577617,
                "tmdate": 1700609577617,
                "mdate": 1700609577617,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZJgJY3PCwk",
            "forum": "LEYUkvdUhq",
            "replyto": "LEYUkvdUhq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1179/Reviewer_GcCE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1179/Reviewer_GcCE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for merging trained models trained on different datasets without additional training based on feature correlations. The proposed method can further adjust the inference cost and accuracy by adjusting the number of layers to be merged."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper addresses the interesting research topic of merging models on different datasets without additional training.\n2. Large-scale experiments such as imagenet1k confirm the effectiveness of the proposed method. This paper conducts experiments in multi-dataset merging, classification and segmentation merging. In particular, this is the first effort, to my knowledge, to merge the different modalities of classification and segmentation.\n3. For reproducibility, the authors mentioned that they plan to publish the source code; the cost of replication experiments would go down significantly if the checkpoints of the trained models were made public since they involve large scale experiments such as cifar100, imagenet1k, and multidataset."
                },
                "weaknesses": {
                    "value": "1. The paper extends feature matching between models to within the model in order to merging models across different datasets. Its contribution should be examined in more detail. In particular, there is no theoretical support as to why matching the feature would lower the loss after merging. I did not understand the connection between Theorem 1 and the fact that joint Acc can be sufficiently large for merging models on different datasets. Is the extension to \"within merge\" not only more flexible in alignment, but also suitable for merging models on different datasets?\n2. The proposed method is inaccurate for meging of imagenet1k and multi dataset unless partial zip is used, while the inference cost increases as the number of models increases with partial zip."
                },
                "questions": {
                    "value": "1. Is git-rebasin activation matching considered equivalent to zipit without wighin merge? I would like to know the difference between git rebasin, permute and zipit.\n 2. Does merge by zipit keep the output strictly invariant like permutation, even when using pseudoinverse matrix?\n 3. What is the same model budget and explain how beta is introduced as an equation?\n 4. Three matching methods were proposed in git-rebasin: activation matching, weight matching, and STE. Which method is the baseline in Table 1 and 2? I doubt the claim that permutation alone makes it difficult to merge models on different datasets. According to [1], using STE, which is a permutation base, and Merge models with cifar10(5+5), it is reported that 90% accuracy is achieved, which is 10% more accurate than zipit. I think the conditions are the same as zipit!20/20 in terms of combining all layers.\n 5. Sec 5.1 says \" `If allow the last stage of the network to remain unzipped (i.e., zip up to 13 layers), our method obtains 83.8%, which is only 3.6% behind an ensemble of model A and model B (which is practically the upper bound for this setting)`\", but how can ensemble be guaranteed to be uppper bound?  In [1], a merging model is reported that can achieve higher accuracy than an ensemble. The upper limit of joint accuracy is the accuracy of the model trained on all data, rather than the accuracy of the ensemble.\n 6. Why is partially zipping more accurate than fully zipping?\n 7. How much gpu is needed for Imagenet-1k (200+200)? I am concerned about the memory and computational cost required to compute the correlation matrix.  \n[1]: https://arxiv.org/abs/2306.05641\n\n\n\n\n===========================================\n\nComments after reading the rebuttal.\n\n===========================================\n\nSince I was unable to set the readers to everyone in the reply, I am writing my comments here.\n\nThank you for your kind reply.\nI did not understand that merging models using loss like STE is outside the scope of this study.\nSince this has been resolved in regards to many of my questions, I would like to raise my score.\n\nAs other reviwers have pointed out, it would be helpful to specify as a limitation in the camera-ready version of the paper that the model must be wide enough to perform."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1179/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1179/Reviewer_GcCE",
                        "ICLR.cc/2024/Conference/Submission1179/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1179/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698508380589,
            "cdate": 1698508380589,
            "tmdate": 1700806227538,
            "mdate": 1700806227538,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jWHDB4ToLb",
                "forum": "LEYUkvdUhq",
                "replyto": "ZJgJY3PCwk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GcCE (1/2)"
                    },
                    "comment": {
                        "value": "We thank you for your review and feedback. We would like to clarify a couple points about our setting and the comparisons to Git-Rebasin:\n\n**Note 1. Our Setting.**  \nTo clarify, we work in the same setting of [2,4,5], where the user would like to merge two models that were trained independently (i.e., different initialization) without full access to the original data and without having to train\u2014with the added stipulation of the models being trained on different tasks. To the best of our knowledge, we are the first to tackle the setting from this angle. Solving this task would allow us to, for instance, merge foundation models for which we do not have the compute to train, or improve federated learning techniques.\n\nIn this setting, the ensemble is the true upper bound, as training is not allowed so the perfect merging would exactly match the output of the original models. Additionally, the extension of working on different tasks / datasets _broadens_ the possible uses of ZipIt! beyond these prior work. Allowing training (like for instance, STE in Git-Rebasin) would increase our performance further, but is out of scope for this work.\n\n**Note 2. Which version of Git-Rebasin?**  \nWe conduct all our Git-Rebasin [2] experiments in their official public code release [6], which for all ResNet models employs _weight-matching (WM)_. Note that [2] find weight-matching actually performs competitively (and better) to their activation-matching version, though our permute baseline performs better than both. We\u2019ve updated our draft to mention that we use the weight-matching version.\n\n**Note 3. The Permute baseline.**  \nWe noticed there may be a misunderstanding about this baseline. The permute method is an ablation of ZipIt!. We use our algorithm, but without merging-within, and utilizes the optimal Hungarian algorithm to find permutations. Thus, it is a strong baseline that already outperforms prior work.\n\nWe have also included our response to each of the reviewer\u2019s concerns below.\n\n**W1. My understanding is that Theorem 1 is positioned to support merging on different datasets + within merging. But this doesn\u2019t make sense to me.**  \nWe clarify that we only position Theorem 1 to motivate why merging **within** models _is better_ than permutation-based methods when redundancy exists within models. We agree that Theorem 1 does not explicitly provide justification for why we would expect strong performance when using within-merging to merge models on different tasks. Instead, we demonstrate this empirically through our extensive experiments (Tables 1/2/3, Figure 6b in the main paper, and more in the Appendix). \n\n**W2. Merged model is inaccurate on imagenet1k and multi-dataset unless partial zip is used.**  \nIt is correct that ZipIt! does not achieve the same performance on ImageNet1k as it does on CIFAR. Yet, we note that _all our baselines including git-rebasin (WM) perform significantly worse_ than ZipIt!, illustrating the task difficulty when **retraining is not allowed (note 1)**. However, we attribute this result to model capacity. It is likely that our ResNet50 is not large enough to contain sufficient redundancy for which ZipIt! (or the baselines / prior work) can make use of. Based on Figure 6b, we expect performance to dramatically improve with scale.\n\nFor low-capacity models, partial zipping is necessary to avoid merging features in later layers that have very low correlation with one another, thus avoiding catastrophic performance when not allowed to train. See our response to Q6 for more details.\n\n**Q1. Is git-rebasin activation matching considered equivalent to zipit without within merge? I would like to know the difference between git rebasin, permute and zipit.**  \nZipIt! within-merging utilizes our _own algorithm_ which achieves near optimal performance (Table 4) whilst being significantly faster and more scalable. The permute method is an ablation of ZipIt!. We use our algorithm, but without merging-within, and utilizes the Hungarian algorithm to find permutations. Git-rebasin with activation matching (AM) is _different_ from both these methods. AM finds permutations based on the pairwise dot-products of neuron activations between models and it appears worse than weight-matching (WM) (Figure 2 of [2]). However, both our permute and ZipIt! significantly outperform WM in every experiment.\n\nNotably, ZipIt! is the only method capable of jointly aligning >2 models into a shared space, as opposed to aligning all models to the space of a chosen anchor model. All prior work (including Git-Rebasin) does the latter, and it is very sensitive to the choice of anchor model."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244962709,
                "cdate": 1700244962709,
                "tmdate": 1700244962709,
                "mdate": 1700244962709,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JEMoVghuwO",
            "forum": "LEYUkvdUhq",
            "replyto": "LEYUkvdUhq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1179/Reviewer_pbds"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1179/Reviewer_pbds"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel method for merging two different models trained from different tasks, maintaining the performance on both tasks. The method resolves the problem of decreasing performance on two disjoint tasks, where prior works fail, \ufeffwhich \ufeffpermutes the model to another one and the average of them. Moreover, this paper proposed partially zipping to get the trade-off between the performance and the computational complexity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposes a novel method named \u201cZipIt!\u201d which can maintain the performance after merging two models trained from two disjoint tasks, which prior works fail. The partially zipping method can further give the option to do a trade-off between the performance and the FLOPs. Finally, a theoretical proof is given to ensure the existence of a transformation matrix."
                },
                "weaknesses": {
                    "value": "The method seems not to be practical for real-world usage, while lacks insights and theoretical analysis of the model properties. The method works fine in the CIFAR dataset, but doesn\u2019t have remarkable advantages in large datasets such as ImageNet, and disjoint tasks. Moreover, the paper lacks comparisons between its method and the model trained on both datasets directly, which limits its usage scenario."
                },
                "questions": {
                    "value": "- I\u2019ve noticed that the \u201cpermute\u201d baseline is stronger than \u201cGit Re-Basin\u201d, which is a little strange. I\u2019m wondering how it happens and what\u2019s the experimental setting?\n- The improvement on CIFAR is remarkable, while in the larger dataset ImageNet, the trade-off between the performance and FLOPs is debatable. Compared to the ensemble, the FLOPs don\u2019t decrease a lot while getting lower accuracy.\n- I\u2019m curious about the comparison between \u201cZipIt!\u201d method and a single model trained on both datasets. Did you think about this?\n- The presentation for \u201cEnsemble\u201d results which uses light grey is a little misleading.\n- Are there the results of Git Re-Basin method for \ufeffMulti-Dataset in Table 3?\n- Did you consider the time cost of computing transformation matrices in your method when comparing the FLOPs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1179/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698653757696,
            "cdate": 1698653757696,
            "tmdate": 1699636044226,
            "mdate": 1699636044226,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e1hoYl1JzE",
                "forum": "LEYUkvdUhq",
                "replyto": "JEMoVghuwO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pbds"
                    },
                    "comment": {
                        "value": "We thank you for your review and feedback. We have responded to your concerns below.\n\n**Clarification: The Permute baseline.**  \nWe noticed there may be a misunderstanding about this baseline. The permute method is an ablation of ZipIt!. We use our algorithm, but without merging-within, and utilize the optimal Hungarian algorithm to find permutations. Thus, it is a strong baseline that already outperforms prior work.\n\n**W1. \u201cThe method seems not to be practical for real-world usage, while lacks insights and theoretical analysis of the model properties\u201d.**  \nOur main goal is to show that it is possible to obtain good performance by merging models from different tasks **without retraining**, which is something no prior work has been able to achieve. Thus, to show this we present merged accuracy in many different settings with many more evaluations in the appendix. While a theoretical analysis of the resulting models might be interesting, it is not the core focus of our work. \n\nHowever, we do include extensive analysis of different aspects of the resulting models in Fig. 6/7/8 and Sec. 6, as well as provide rigorous theoretical motivation for our method in Eq. 4 and Appendix G.\n\n\n**W2. \u201cThe method works fine in the CIFAR dataset, but doesn\u2019t have remarkable advantages in large datasets such as ImageNet, and disjoint tasks.\u201d**  \nWe would like to emphasize that ZipIt! _significantly_ outperforms prior work on both CIFAR-10 and CIFAR-100 and gets close to the _maximum possible_ performance in these settings (i.e., ensemble accuracy). For much harder settings like ImageNet, ZipIt! still outperforms prior work and is able to perform significantly better when using partial unzipping. Note that the \u201cpermute baseline\u201d is a **strong baseline** we introduce that _already_ outperforms prior work.\n\n\n**W3. Moreover, the paper lacks comparisons between its method and the model trained on both datasets directly, which limits its usage scenario.**  \nTo clarify, we work in the same setting of [1,2,3], where the user would like to merge two models that were trained independently (i.e., different initialization) without full access to the original data and without having to train\u2014with the added stipulation of the models being trained on different tasks. To the best of our knowledge, we are the first to tackle the setting from this angle. Solving this task would allow us to, for instance, merge foundation models for which we do not have the compute to train, or improve federated learning techniques.\n\nIn this setting, the ensemble is the true upper bound, as training is not allowed, so the perfect merging would exactly match the output of the original models. Additionally, the extension of working on different tasks / datasets _broadens_ the possible uses of ZipIt! beyond these prior work. Allowing training would increase our performance further, but is out of scope for this work.\n\n\n**Q1. Permute stronger than Git Rebasin?**  \nWe conducted all our Git-Rebasin experiments in their official public code release [4], which for all ResNet models employs _weight-matching_. Note that [1] find weight-matching actually performs competitively (and better) to the activation-matching version. Our permute baseline is simply our ZipIt! algorithm with the matching algorithm replaced by an optimal permutation finding algorithm. Thus, it is a _strong_ baseline and already outperforms prior work.\n\n**Q2. I\u2019m curious about the comparison between \u201cZipIt!\u201d method and a single model trained on both datasets.**  \nSee our response for W3 above. While it might be an interesting comparison, this is out of scope for our setting as the upper bound is ensemble performance.\n\n**Q3. The presentation for \u201cEnsemble\u201d results which uses light grey is a little misleading.**  \nThank you for your comment. We gray out the ensemble because it requires evaluating both models, which uses significantly more (up to double) the parameters / compute compared to the rest of the table entries. Do you have any suggestions on how we can make this more clear?\n\n**Q4. Are there the results of Git Re-Basin method for \ufeffMulti-Dataset in Table 3?**  \nGit Rebasin in its original code-base (all Git-Rebasin experiments are conducted in the official repository [4]) does not support this type of experiment. However, we expect its performance to be much worse than our permute method just like in Tab. 1 and Tab. 2.\n\n**Q5. Did you consider the time cost of computing transformation matrices in your method when comparing the FLOPs?**  \nThe transformation matrices only need to be computed once and are very quick to compute (~1s total on a CPU), so it is not included in the model FLOPs. Benchmark results for the actual transformation calculation can be found in Tab. 4.\n\n\n[1] Git-Rebasin: https://arxiv.org/pdf/2209.04836.pdf  \n[2] OT-Fusion: https://arxiv.org/pdf/1910.05653.pdf  \n[3] REPAIR: https://arxiv.org/pdf/2211.08403.pdf  \n[4] Git-Rebasin Code: https://github.com/samuela/git-re-basin"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244806184,
                "cdate": 1700244806184,
                "tmdate": 1700244806184,
                "mdate": 1700244806184,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IfPlRp6a5U",
            "forum": "LEYUkvdUhq",
            "replyto": "LEYUkvdUhq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1179/Reviewer_3tA9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1179/Reviewer_3tA9"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method to merge two different models sharing the same architecture, possibly with different initialization and for different tasks, without re-training. The key idea is to merge the weights whose outputs are highly correlated, both within and across models. The method also allows weights to be partially merged, adding flexibility to adjust the amount of merging depending on resource budget and accuracy requirements. Experiments show that ZipIt! outperforms the existing baselines in several settings, including merging classification models and merging across tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper studies a new and interesting problem: merging differently initialized models trained on different tasks into a single model without retraining. This relaxes the condition of the model merging problem, recently studied in works such as Model Soups and Git Re-Basin, which assumes the same task over models. This makes the problem more challenging while allowing for broader use cases.\n\n2. The idea of merging weights both within and across models is simple, but achieves reasonably good performance under different conditions in the experiments. It's great that experiments with merging tasks of different modalities (classification, segmentation) are also done and the method still achieves reasonable performances, showing that the method is generalizable to non-classification tasks. In addition, the ability to flexibly adjust the amount of merging is also a strong point.\n\n3. The paper is well written. It was easy to follow. It is also great that the code is already publicly available."
                },
                "weaknesses": {
                    "value": "1. My main concern is the stability of the method. I am not sure if the method works robustly across different settings, because empirical observations are reported in the github issues that a small change (e.g. changing the learning rate and epoch numbers when training the models to merge) can cause the merged model to crash to low accuracy.\n\n2. These are not weaknesses but there are some limitations. The performance drop increase when merging multiple models. Accuracy drop looks large when merging models of different tasks (Table 8). All experiments used CNNs and transformers are not tested."
                },
                "questions": {
                    "value": "1. The meaning of \"ensemble\" in the experiments is unclear to me. I don't think I understood how it works.\n\n2. I would appreciate more experiments to test the stability of the method. It would also be nice to provide an analysis of when the method works and when it does not. Please see the weakness section for more details.\n\n3. It would also be interesting to include additional experiments on more compact architectures such as Mobilenet for a more comprehensive analysis of the behavior of the method. It is great that ZipIt! is already evaluated on models with larger widths (x1.5 in Table 7). I think it might be harder to merge models where the architectures are already compact because there might be less overlap and less redundancy of features.\n\n4. Can BN be removed before applying ZipIt by fusing it into Conv (using that BN is an affine transform when mean and variance are fixed)? I think then the process to reset BN after merge can be avoided."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1179/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698881308962,
            "cdate": 1698881308962,
            "tmdate": 1699636044163,
            "mdate": 1699636044163,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KrtDXRUg37",
                "forum": "LEYUkvdUhq",
                "replyto": "IfPlRp6a5U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3tA9"
                    },
                    "comment": {
                        "value": "We thank you for your comprehensive review and feedback. We have included responses to your questions and concerns below.\n\n**W1 / Q2. Concern about Stability.**  \nMost mode connectivity and model merging work assumes models are well-trained. In the issue referenced, the original models obtained 15% lower accuracy than well-trained models, as they were underfit with too low of a learning rate, and thus themselves underperformed before the merge. This caused all merging algorithms to drop to under 20% in accuracy, so this is not specifically a ZipIt! issue. The merged model\u2019s performance is naturally going to depend on the original models\u2019 performances.\n\n**W2. Performance degradation when merging multiple models. What about transformers?**  \nUnfortunately, that is a limitation of merging multiple models trained on different tasks into the size of one without being able to train\u2013-since each model has some task-specific neurons [1]. However, as shown in Fig. 6b, this could likely be alleviated in practice by increasing the network width. As for transformers, at the time of submission no other model merging work has shown results on transformers with different initializations, but this is a great direction for future work.\n\n**Q1. What does \u201censemble\u201d mean here?**  \nThanks for pointing this out, we\u2019ve clarified this in the baseline section of the updated draft. For joint accuracy, we softmax the logits of each model separately to place their predictions on the same scale, and then concatenate resulting probabilities from each model and take an argmax. For avg task accuracy, we use the correct model for each task (i.e., just average the per-task accuracies). Note that this is upper bound accuracy because with perfect merging (not allowing training), the merged model's outputs would be identical to the originals.\n\n**Q3. Results for smaller models, e.g., MobileNet?**  \nFor smaller models, note that ResNet-20x4 (which we use for our CIFAR experiments) has around the same number of parameters as MobileNetV2, and we show scaling performance (both downward and upward) in Fig. 6b. While it\u2019s true that constraining the model size hurts performance, this depends on task difficulty and is true for all other methods as well.\n\n**Q4. Can BN be fused beforehand, so it doesn\u2019t need to be recomputed?**  \nSadly, while it is possible to fuse the BNs into the models beforehand, [2] shows that simply merging the model\u2019s BNs together would result in \u201cvariance decay\u201d. So some mitigation strategy would be necessary regardless, and we found resetting the batch norm to be the simplest option.\n\n[1] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In ICML, 2019.  \n[2] REPAIR: https://arxiv.org/pdf/2211.08403.pdf"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244594029,
                "cdate": 1700244594029,
                "tmdate": 1700244594029,
                "mdate": 1700244594029,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]