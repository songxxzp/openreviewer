[
    {
        "title": "Towards Interpretable Controllability in Object-Centric Learning"
    },
    {
        "review": {
            "id": "Hyba1WvTg1",
            "forum": "hy84B74XFt",
            "replyto": "hy84B74XFt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5088/Reviewer_jXVc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5088/Reviewer_jXVc"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an equivariant consistency regularization for SlotAttention architectures. The idea is for each augmentation of an image, learn a corresponding augmentation that can be applied directly to the object slots, allowing users to modify object representations directly. \nThe paper is a bit like adding an equivariant consistency loss (e.g. https://paperswithcode.com/paper/unpaired-image-to-image-translation-using) to SlotAttention. Where image augmentations have corresponding transformations on the Slot representations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "### Related Work\n- Decent coverage on learning interpretable latents using VAE/GANs\n\n### Experiments\n- I appreciate including error bars. I wish more papers did this. \n- Generally well-organized experiments ssection"
                },
                "weaknesses": {
                    "value": "### Overall\nOverall the paper is hard to follow, because common terms like equivariances are called different names (e.g. sustainability) and not clearly defined. The experiments are only on variants of CLEVR, with no evaluations on real-world data. Even on CLEVR, the results are not very convincing, and the method requires defining specific image augmentations for each equivariant action \u2014 so it is hard to use this on real-world datasets.\n\n\n### References: \n- Related work ignores the majority of work on object detection + localization. Learning approaches to identifying and localizing objects far predates slot attention \u2014 it\u2019s a core computer vision task. Learning approaches go way back, too \u2014 OverFeat, deformable parts models, RCNN + children, YOLO, SAM, etc.\n\n\n### Method:\nThe authors introduce (as a contribution) new language for agreed-upon terms like equivariance, and the new language doesn\u2019t add anything in my opinion. It is neither intuitive nor well-defined, and only serves to make the work harder to understand.\nFor example:\n1. \u201cIn this work, we introduce sustainability which stands for the concept that object representations should sustain their integrity even after undergoing iterative manipulations.\u201d\nWhat is integrity? It seems to be defined in terms of the \"durability test\" (AKA invertability). But equivariance and invertibility are already in common usage for a while now. \n\n\n### Experiments:\nExperiments are on variants of CLEVR, which is a very simple dataset that was generated in ways that privilege this algorithm. No evaluation on real-world datasets. Other work (e.g. instruct pix2pix https://arxiv.org/pdf/2211.09800.pdf) DOES show zero-shot results on real-world datasets.\nRegardless, the results even on CLEVR are not convincing \u2014 leading to little to no improvement for object detection.\n\n### Misc:\nMany terms feel philosophical, when they could be stated more concretely. E.g. \u201cThen, the model performs spatial binding on img_{ref} to produce slots{_ref}\u201d. Meaning you run the image through the model to get the slot latents?"
                },
                "questions": {
                    "value": "### Comparison to related work:\nFor interpretable latents (e.g. VAE or GANs), the authors note that these require \u201cmanual efforts to identify the features associated with specific properties.\u201d\n    - In this work, too, you have to hand-design the augmentation and regenerate appropriate training data. This is also a manual effort, and arguably harder than a post-hoc approach?\n\n\n### Definition of \"Durability Test\"\nThis is defined in the paper as \u201cThe multi-step test involves a series of instructions to modify an object and another series to restore it to its initial state.\u201d\n1. This is a fine definition, but why not also write out the equations: e.g.  $(g_1 * \u2026 * g_k)^{-1} (g_1 * \u2026 * g_k) x = x$ ?\n\nHowever, the requirement that image augmentations are invertible is a strong one (e.g. viewpoint change is not invertible from the image alone). Why not make the approach more general and focus on measuring equivariance\n\nE.g. you can measure the equivariance of the whole model by augmenting the images $(g_1 * \u2026 * g_k) (image) = (h_1 * \u2026 * h_k) (slots)$ where $g_i$ is an image aug and $h_i$ is the corresponding instruction ref2aug"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5088/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698611920238,
            "cdate": 1698611920238,
            "tmdate": 1699636499783,
            "mdate": 1699636499783,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EGMn9HkJC7",
                "forum": "hy84B74XFt",
                "replyto": "Hyba1WvTg1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jXVc (1/3)"
                    },
                    "comment": {
                        "value": "We appreciate the time and effort the reviewer dedicated to assessing our paper. \nIn our response, we aim to address the reviewer's comments comprehensively, with the hope of resolving the concerns.\n\nBefore addressing specific points, we would like to offer insights into the context of our work within the domain of object-centric learning (OCL). \nThis field, as a subset of compositional understanding, constitutes a significant area within computer vision, aims at achieving human-like generalization by deconstructing visual sensory inputs into symbol-like entities, such as objects.\nTo achieve this, OCL focuses on generating informative object representations, called slots.\nSlightly different from other domains such as object detection and localization, which primarily focus on the final output of deep networks, OCL places its emphasis on finding intermediate representations for objects.\n\nNevertheless, it is true, as the reviewer jXVc mentioned, that \u201cit is hard to use this on real-world datasets\u201d while \u201cother works (e.g. instruct pix2pix) show zero-shot results on real-world datasets.\u201d\nThis is a prevalent problem in recent OCL research, not just ours, considering that several zero-shot and foundation models like SAM demonstrate exceptional performance across various real-world datasets in downstream tasks.\nWe believe that the difference between OCL and the state-of-the-art methods for the real-world datasets can be attributed to the task-agnostic nature of OCL.\nOCL concentrates on examining how to define the objects, how to represent them, and how to understand the object interaction within the visual scene, devoid of any predefined target tasks.\nIn other words, OCL research investigates meaningful patterns defining what we identify as objects, and aims to capture the objects into vector representations only with RGB images, distinct from other artificial tasks influenced by human biases.\nTherefore, in many OCL studies, the primary objective function for training is an RGB reconstruction loss in a self-supervised manner; here, the reconstruction is not even the target task of the OCL research.\n\nWe firmly believe that the approach of OCL, as a stand-alone actively researched topic, holds value within computer vision and artificial neural network research for the following reasons:\n1) OCL explores the vector representations which serves as the foundational elements of all information that we handle in the era of artificial neural network.\n2) OCL explores the objects which are the building blocks of the visual world. Objects are self-contained, separate from one another, and interact actively with others to compose the visual world. OCL tries to interpret these objects and their relations in a vector space using object representations.\n\nGiven these motivations, a lot of works [1-5] explore this domain, and below are some quotes emphasizing the significance of the OCL domain from other research papers.\n- \u201cLearning object-centric representations of complex scenes is a promising step towards enabling efficient abstract reasoning from low-level perceptual features.\u201d (Slot Attention [1])\n- \u201cObject-centric representations are a promising path toward more systematic generalization by providing flexible abstractions upon which compositional world models can be built.\u201d (SAVi [2])\n- \u201cLearning good representations of complex visual scenes is a challenging problem for artificial intelligence that is far from solved.\u201d (IODINE [3])\n\nIn line with many valuable studies in OCL, the contribution of our work is on the first exploration of interpretable (and user-interactive) controllability over object representation \u2014 an aspect that has seen limited investigation within OCL.\nWe sincerely hope that the reviewer can reconsider the view on our work within the OCL domain.\nEchoing the other reviewers\u2019 comments, we politely claim our work is \u201cvery original\u201d (byFg), \u201cnovel\u201d and \u201cencouraging for future research\u201d (dLyp) with the simple approach as Reviewer jXVc also pointed out and Reviewer byFg mentioned as \"the main advantage is its simplicity\".\nWith comments from other reviewers, we respectfully assert that our method is regarded as \"very original\" (byFg), \"novel,\" and \"encouraging for future research\" (dLyp). Additionally, our approach is acknowledged for its simplicity, which Reviewer byFg also commented as \"the main advantage.\u201d\n\n[1] Locatello et al., \"Object-Centric Learning with Slot Attention.\" (NeurIPS 2020)\n\n[2] Kipf et al., \u201cConditional Object-Centric Learning from Video.\u201d (ICLR 2022)\n\n[3] Greff et al., \u201cMulti-Object Representation Learning with Iterative Variational Inference.\u201d (ICML 2019)\n\n[4] Burgess et al., \u201cUnsupervised Scene Decomposition and Representation.\u201d (arXiv 2019)\n\n[5] Singh et al., \u201cNeural Systematic Binder.\u201d (ICLR 2023)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220745238,
                "cdate": 1700220745238,
                "tmdate": 1700220745238,
                "mdate": 1700220745238,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TOb46mkp4c",
                "forum": "hy84B74XFt",
                "replyto": "NQoYiJgZ9V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5088/Reviewer_jXVc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5088/Reviewer_jXVc"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors detailed response, both the clear explanation of how they they see OCL as a fusion of symbolic and connectionist approaches in computer vision,  as well as the technical details and additional experiment provided.\n\nOverall I would say that the proposed method seems to be quite tailored to datasets with a clear notion of \"objects\", and where images can be generated after manipulating properties of the objects in the scene. As other reviewers have noted, this restricts the type of datasets where the method can be used. Other works in OCL (like SlotAttention) can be and are applied on more general datasets -- so this is more about the particular technique than OCL in general.\n\nThis technique feels a bit like [Neural Module Networks](https://openaccess.thecvf.com/content_cvpr_2016/html/Andreas_Neural_Module_Networks_CVPR_2016_paper.html), which learn specific neural operators that correspond to linguistic rules and can be composed. In this case, we are learning specific operators that correspond to transformations of the objects. The trend in NLP (and ML in general) seemed to be that given enough data it is better to let NNs learn the rules, rather than hard-code them. \n\nBut good experiments in real-world settings could convince me otherwise in this context. For an idea such as this, which requires + enforces a lot of structure, I would expect the results to be quite good on datasets that reflect that structure. I'd like to see results on real-world datasets where those assumptions might not be true.\n\nI recognize that other papers in OCL use CLEVR (and variants), and many papers also use MNIST (and variants). As a wise advisor once said -- \"if it doesn't work on MNIST then it doesn't work at all, but it might fail to generalize beyond MNIST.\" I feel CLEVR is similar to MNIST of objects."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685150199,
                "cdate": 1700685150199,
                "tmdate": 1700685150199,
                "mdate": 1700685150199,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DeXzS5E63y",
            "forum": "hy84B74XFt",
            "replyto": "hy84B74XFt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5088/Reviewer_dLyp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5088/Reviewer_dLyp"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces SlotAug, an object-centric learning method that allows for interpretable manipulation of the slots. The model is trained with image-level data augmentation and supports scaling, translating, or color shifting individual objects in the scene. The authors introduce the concept of sustainability, which refers to the ability to preserve the nature of the slots, allowing for multiple iterations of slot manipulations. To achieve sustainability, the authors incorporate two submethods, Auxiliary Identity Manipulation (AIM) and Slot Consistency Loss (SCLoss). In experiments on Tetrominoes, CLEVR6, CLEVRTex6, and PTR, the authors demonstrate the ability to manipulate slots and the effectiveness of their model in achieving sustainability of the slots."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper introduces a novel approach to the important problem of interpretable and controllable object representations. The idea of leveraging image-level augmentations to enable object-level controllability by taking advantage of the independence of the slots has not been done before, as far as I know. I found the paper generally well-written and easy to understand, although I do list some questions and suggestions regarding clarity below. The experiments clearly demonstrate the ability of their model to manipulate the slots and the benefits of AIM and SCLoss for improving sustainability. The results from section 4.3 are also encouraging in showing that this method potentially helps improve the representation quality of the slots themselves."
                },
                "weaknesses": {
                    "value": "- In section 4.1.1, the authors claim that one of the reasons their method works is because of the spatial broadcast decoder independently decoding for each slot. This is supported in the appendix by an experiment on SLATE which uses a decoder where the slots are not completely independent. This seems potentially limiting as several recent works in scaling object-centric learning (OCL) to realistic scenes [2, 3, 4] rely on decoders where each slot may not be decoded independently. This may limit the applicability of this method to more realistic scenes that are supported by those OCL methods.\n- The second claim in section 4.1.1, that the use of ARK is important, does not seem to be supported by any experiments. How well does this method work with vanilla slot attention? Is ARK required for this method to work?\n- I could not find which datasets are used for Table 1 and section 4.3 (Table 3 and Figure 6). The fact that segmentation quality is maintained and representation quality potentially improved is an important result. I would be curious about these results broken down by datasets.\n- In the appendix, the authors mention that the scaling augmentation takes into account the predicted attention maps between the encodings and the slots to handle the translation of objects during scaling. I am a bit confused about this. Does this mean that the augmentation changes as the model gets trained better? Or does this use some other pre-trained Slot Attention encoder?\n- In terms of the presentation, I was initially unsure of the significance of sustainability until I saw the experimental results in section 4.2. For clarity, I would suggest showing a motivating example earlier in the text to explain the necessity of the AIM and SCLoss components."
                },
                "questions": {
                    "value": "- Are the qualitative examples cherry-picked? Are there common failure scenarios the reader should be aware of?\n- I am confused about the use of the SRT decoder in some of the experiments since that method does not have any notion of slots. Was this supposed to be the OSRT decoder [1]?\n\n\n[1] Object Scene Representation Transformer. https://arxiv.org/abs/2206.06922 \n\n[2] Simple Unsupervised Object-Centric Learning for Complex and Naturalistic Videos. https://arxiv.org/abs/2205.14065\n\n[3] Object-Centric Slot Diffusion. https://arxiv.org/abs/2303.10834\n\n[4] SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models. https://arxiv.org/abs/2305.11281"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5088/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5088/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5088/Reviewer_dLyp"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5088/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698692593123,
            "cdate": 1698692593123,
            "tmdate": 1700685335337,
            "mdate": 1700685335337,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "baA7cFuxyR",
                "forum": "hy84B74XFt",
                "replyto": "DeXzS5E63y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dLyp (1/2)"
                    },
                    "comment": {
                        "value": "Appreciate the reviewer's thoughtful review and interest in our paper. \nWe will respond to each of the comments and we hope to have more discussion about our work and the future direction of our research domain. \nSome responses still have pending points, such as ongoing experiments and figure creation, and we will do our best to resolve these aspects during this rebuttal period.\n\n> Constraints on the type of decoder may limit the applicability of this method.\n\nAs the reviewer correctly noted, recent research has been integrating large decoders, allowing for interactions between slots to improve decoding quality. (In this response, we will refer to this approach as a \"mixture decoder.\")\n\nTaking a slightly different viewpoint from the aforementioned research, our study reevaluated the concept of independence decoding, as demonstrated by the spatial broadcast decoder proposed in the original Slot Attention. We believe that independent decoding holds value comparable to the excellent decoding quality achieved by mixture decoders. It's crucial to note that in this research, we empirically and theoretically demonstrated that independent decoding (not only the spatial broadcast decoder but also the transformer-based SRT decoder) divides image-level loss into object-level loss. However, this does not imply that mixture decoding cannot achieve the same; it's just that success is not guaranteed, as illustrated in the SLATE case in the appendix.\n\nFuture research could explore how object-centric learning varies based on the decoder's configuration. Additionally, structuring a decoder with both mixture decoding and slot-wise decoding as two branches holds promise for achieving high-level decoding quality while learning controllability. We look forward to investigating these aspects in upcoming studies.\n\n\n> Is ARK required for this method to work?\n\nIn fact, ARK is not necessary for our method. However, as shown in the SLASH paper, the original Slot Attention (SA) suffers from a bleeding issue, wherein the attention or segmentation masks for objects leak into the background. As we mentioned in the proof in the appendix, the performance of object discovery plays a significant role in object-level controllability. If our proposed training scenario arises where bleeding issues do not occur in the original SA, it can be achieved without the need for ARK.\n\nThe use of ARK is not intended to enhance object discovery performance in a single training session; rather, it is employed to ensure consistent results across multiple experiments. Thanks to the reviewer\u2019s feedback, we acknowledge the necessity of providing more detailed descriptions of these aspects, and also experimental results. We are conducting the experiments with the original Slot attention and will add the results with the descriptions in the revision during the rebuttal period.\n\n> The dataset used for Table 1.\n\nThank the reviewer for finding the lack of description regarding the datasets used in our study. The results are from CLEVR6. \n\nAs you have already figured out, the focus of our study is not to enhance the performance of the object discovery, and the experiment results support our claim that our method does not negatively impact the performance of the existing OCL model. \n\nNevertheless, we also acknowledge the importance of including results from other datasets, such as CLEVRTEX and PTR, which would show the robustness and applicability of our method. We plan to carry out experiments on these datasets and intend to include these additional results in the appendix if the results are ready during the ongoing discussion. Even though we cannot include the experimental results in the revision, we will incorporate them in the following version of our paper.\n\n> Details about the scaling augmentation.\n\nWe appreciate the reviewer for posing this question since this question, we believe, signifies the reviewer's dedication to comprehending our work in detail, and we value the effort invested in understanding our paper thoroughly.\n\nAs the model undergoes training, we obtain more precise calibration results, corresponding to the more accurate predicted attention maps. However, the thing is that we do not use either the augmentation curriculum as the model gets trained better or the pre-trained encoder. Initially, we also considered equipping the model with object discovery and translation abilities before employing scaling augmentation during training. However, we found that a straightforward training curriculum, incorporating all types of augmentation (scaling, translating, and tinting) from the first epoch, proved effective without compromising performance in object discovery and manipulation. To maintain simplicity and clarity in presenting our work, we opted not to include any curriculum learning components."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699934278093,
                "cdate": 1699934278093,
                "tmdate": 1699934278093,
                "mdate": 1699934278093,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x6KFTDNlF4",
                "forum": "hy84B74XFt",
                "replyto": "FtmzWvbwb8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5088/Reviewer_dLyp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5088/Reviewer_dLyp"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to answer my questions. I think this is a good contribution to the object-centric learning community and have decided to increase my score to 8."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685319321,
                "cdate": 1700685319321,
                "tmdate": 1700685319321,
                "mdate": 1700685319321,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "akXNxyYQqD",
            "forum": "hy84B74XFt",
            "replyto": "hy84B74XFt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5088/Reviewer_byFg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5088/Reviewer_byFg"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies slot-based unsupervised image models, e.g., Slot-Attention, and proposes a way to introduce controllability in the slots. This is done via enforcing a form of equivariance of the image -> slot transformation to image augmentations, except that the augmentations applied in the slot space are a learnable mapping from the augmentation instuctions. The results demonstrate that the model successfully manages to control and manipulate slots given the instructions, and gracefully handles the inverse instructions to \"undo\" the given manipulations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper proposes a very original way to manipulate learnable objet slots in slot attention.\n- The main advantage of the method is its simplicity: the augmentations are introduced on the image level, removing the need to implement per-slot manipulation strategies at training, yet the manipulations can be applied to individual slots at inference time leaving the other slots intact.\n- The qualitative results are very impressive, even thought the datasets are quite simple. The findings of the paper are encouraging for the future research on slot controlability."
                },
                "weaknesses": {
                    "value": "The main weakness of the method is the fact requires a pair of (image augmentation, augmentation instruction) to work, rather than only one of them. Iit is easy to generate both the augmentation and its instruction with simple image transformation in a controlled environment, but this is much harder to do in a realistic setup. Some image transformation may not have a clear apriori-known instruction, or vice-versa, some may only have the instruction for the augmentation (e.g., specified as text) without the knowledge of the augmentation.\nEschewing this requirement would largely benefit the method and make it applicable in a more realistic setup."
                },
                "questions": {
                    "value": "No questions"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethic concerns"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5088/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698852794852,
            "cdate": 1698852794852,
            "tmdate": 1699636499616,
            "mdate": 1699636499616,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xhzY0c6toU",
                "forum": "hy84B74XFt",
                "replyto": "akXNxyYQqD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5088/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5088/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer byFg"
                    },
                    "comment": {
                        "value": "Thank the reviewer for both the positive and encouraging comments, and the insightful feedback on the weakness of our work, as well as the potential future direction of this area. We agree with the reviewer's observation regarding the requirement of pairs of (image augmentation, augmentation instruction).\n\nWe recognize that there can be diverse, almost infinite, scenarios for controlling objects and object representations in distinctive domains. Our research, being in its early stages, considered relatively simple instructions and scenarios with clear relationships between instructions and object properties. As suggested by the reviewer, there is a need to address this limitation by advancing our research into more realistic settings. Therefore, future studies should focus on validating controllability in situations that are more practical and usable.\n\nHopefully, recent research, such as [1, 2], demonstrates the potential of incorporating language data with slot representation. Furthermore, dataset papers like [3] offer promising directions in this field. In line with this, we are preparing subsequent research that utilizes caption datasets and visual-language models. We are also curious to explore the feasibility of interpretable controllability over object representation using single images and their corresponding descriptions, even without the need for pairs of (reference image, augmented/edited image) data.\n\nWe appreciate the reviewer's recognition of the potential and contribution of our research despite the mentioned limitation of our study.\n\n[1] Kim et al., Improving Cross-Modal Retrieval with Set of Diverse Embeddings. (CVPR 2023)\n\n[2] Kim et al., Shatter and Gather: Learning Referring Image Segmentation with Text Supervision. (ICCV 2023)\n\n[3] Wang et al., The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World. (arXiv 2023)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5088/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699887396062,
                "cdate": 1699887396062,
                "tmdate": 1699887396062,
                "mdate": 1699887396062,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]