[
    {
        "title": "Interpreting the Inner Mechanisms of Large Language Models in Mathematical Addition"
    },
    {
        "review": {
            "id": "GQN85sAbm5",
            "forum": "VpCqrMMGVm",
            "replyto": "VpCqrMMGVm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3226/Reviewer_h6js"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3226/Reviewer_h6js"
            ],
            "content": {
                "summary": {
                    "value": "The paper identifies attention heads which play a key role for addition and subtraction in transformer-based language models. The authors identify several such heads in three different models and demonstrate that removing them destroys the ability of the model to perform arithmetic."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The analysis is thorough and rigorous. \n2. The paper is clearly written, and the presentation is well-organised and presented.\n3. On top of identifying the \u201ckey heads\u201d being focused on addition, the paper shows that the same heads are also involved in subtraction. While this might be intuitive, considering that one of these tasks is the opposite of the other, it is not obvious that an LLM would discover and utilize this duality. However, it is unclear whether that is because the heads focus only on numbers or because they are utilising the duality of summation and subtraction.\n4. The paper recognises that later heads depend on earlier ones and attempts to analyse these dependencies (although it appears there are none)."
                },
                "weaknesses": {
                    "value": "1. The paper identifies attention heads that take part in the processing of summation but does not look into or explain what each of the \u201ckey heads\u201d actually does and what is the mechanism through which it contributes to summation. Therefore, the paper focuses on _localization_ of the heads that partake in summation, rather than _interpreting_ them.\n\n2. The paper does not look at alternative representations of numbers. For example, in words (\u201ctwo\u201d instead of 2), Roman numerals (II instead of 2), and other languages (\u4e8c or \u0662 instead of 2). The lack of such analysis leaves the question open whether these heads simply attend to numerical tokens or whether they are involved in higher-order reasoning about numbers and arithmetic.\n\n3. Related to the above, the paper seems to focus only on single-digit summation. It is unclear whether the results would translate to the summation of larger numbers (or more than two numbers). This is important as prior works have shown that the ability of LLMs to do arithmetic quickly decreases with the increase of the number of digits. It would be interesting to see if your analysis would be able to provide insights into this phenomenon.\n\n4. I am not sure how to read the attention patterns in Fig. 4. How can the attention be negative? In fact, it does not seem that these heads attend to all numbers. The first head seems to attend to the completion of \u201cor\u201d with \u201canges\u201d and the full stop. Both heads seem to attend only to 3 while solving the task would also require attention to 5. Therefore, it is not clear how these heads participate in performing summation.\n\n5. The paper looks predominantly at attention heads. However, it is well known that a lot of the computation and processing happens in the MLPs. Hence, a full picture of the interoperation of the mechanisms for summation should also include the MLPs"
                },
                "questions": {
                    "value": "1. Does knocking out the heads have effects on other tasks, i.e. are these heads only important for arithmetic or are they polysemantic?\n\n2. In the Introduction, you say _\u201cContrary to expectations that LLMs may involve diverse attention heads across all layers, our findings reveal that only a small percentage (0.5%) of the attention heads significantly impact the model\u2019s performance.\u201d_ However, this is exactly the expectation: attention heads have diverse functions so it is not surprising that only a few of them would be involved in summation.\n\n3. In the Introduction, you say _\u201cRemarkably, only the absence of these heads leads to a notable decline in model performance.\u201d_ But this can\u2019t be true. Surely there are many other weights that, if perturbed, would result in a significant decline in model performance (e.g. the embedding matrix or the final output projection matrix).\n\n4. In Section 4.2, how do you decouple the effect of the individual heads? In the implementation of LLAMA there are no separate $W_O$ for each head but a single one that is applied to the concatenation of all the heads. Therefore, it mixes information across heads. How do you resolve this?\n\n5. In Figure 4 left, how do you know that the effect you see is because of the heads specialising in numbers and not because your test sequences have numbers in them? I\u2019d be curious to see how this plot and the rest of your analysis would look like if applied to sentences which have nothing to do with numbers and arithmetic. Possibly the heads that you have found to be important for arithmetic would be especially unimportant for other tasks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3226/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3226/Reviewer_h6js",
                        "ICLR.cc/2024/Conference/Submission3226/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3226/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697929758622,
            "cdate": 1697929758622,
            "tmdate": 1700653115454,
            "mdate": 1700653115454,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g2v0fTbFQE",
                "forum": "VpCqrMMGVm",
                "replyto": "GQN85sAbm5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h6js [part 1/2]"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for taking the time to review our work. We are grateful that you find our research to be thorough, rigorous, well-organised and presented. Thank you for acknowledging our efforts to the findings of key heads in addition. According to your valuable comments, we provide detailed feedback. \n\n**Q1**: More interpretation for attention heads and MLPs:\n> (a) \"the paper focuses on localization of the heads that partake in summation, rather than interpreting them\"  \n> (b) \"a full picture of the interoperation of the mechanisms for summation should also include the MLPs\"\n\n**Ans for Q1:**  \n**(a)** Thank you for bringing up the insightful comment!\n* We agree that delving into the *\"causal mechanism\"* of addition calculation is a quite promising direction. In this study, we follow the recent research in mechanistic interpretability [1, 2] that adapt the theory of causal mediation analysis to measure the *\"causal effect\"* of component mediators to the model behavior. We would like to highlight the identification of key heads *implicitly* interpret the model behavior (e.g. addition calculation).\n* Before this study, there was a lack of clear interpretation regarding several important questions of addition calculation in LLMs. \n  (i) Are there attention heads consistently involved in addition calculation? \n  (ii) If yes, are they sparsely or densely distributed? \n  (iii) In the case of sparsity, where exactly are these key heads located?  \nTo shed light on these matters, we performed sufficient experiments using different models and data formats, striving to present the observed phenomena in a clear and understandable manner.  \n\nWe hope that our efforts could contribute to making the inner workings of LLMs in addition more understandable to humans.\n\n**(b)** Thank you for the valuable suggestion on MLPs. \n* We added one experiment of path patching to measure the causal effect of each MLP layer. The results indicate that: (i) the early MLP layers 0-13 before the key heads (e.g., 13.11) have a slight effect on the output (approximately $\\pm$0.5%). (ii) The late MLP layers 14-31 after the key heads exhibit a much larger effect (approximately $\\pm$10.0%). These findings may reveal that the key heads are responsible for attending to number tokens, while the following MLP layers process the number tokens.\n* We also noticed that previous works [1, 2] tried to explain MLPs for performing computations and retrieving facts. However, it necessitates thorough and rigorous analysis to make the detailed process in MLPs human-understandable, and interpret the collaboration between attention heads and MLPs. We believe that making the calculation fully understandable to humans is an intriguing direction that warrants sustained research efforts.\n\n\n[1] Locating and Editing Factual Associations in GPT. In NeurIPS 2022.  \n[2] Transformer Feed-Forward Layers Are Key-Value Memories. In EMNLP 2021.\n\n**Q2**: More alternatives of numbers:\n> (a) \"The paper does not look at alternative representations of numbers. For example, in words (\u201ctwo\u201d instead of 2), Roman numerals (II instead of 2), and other languages (\u4e8c or \u0662 instead of 2).\"  \n> (b) \"It is unclear whether the results would translate to the summation of larger numbers (or more than two numbers)\"\n\n**Ans for Q2:**  \nThanks for your constructive suggestion. We conduct the knockout experiments on the following number formats. The table shows one representative sample for each format and the accuracy change after knocking out the key heads in LLaMA2-7B.\n\n| settings | formats | samples | accuracy |\n|-------| ------- | ------- | ------- |\n|    a1     | English  | \"The addition of two and five is \" |   -45%      |\n|    a2   |  Chinese |     \"\u4e24\u548c\u4e94\u76f8\u52a0\u7684\u7ed3\u679c\u662f\"   |    -5%     |\n|    a3     |  Roman |   \"II + V = \"  |   -5%      |\n|     b1    | two-digit  |   \"35 + 42 = \" |    -69%     |\n|     b2   | three-digit  |   \"154 + 243 = \" |     -72%    |\n|     b3  | four-digit  |   \"1524 + 3463 = \" |    -76%     |\n\n**(a)** To investigate the effects of numbers represented in different languages, we conduct the experiments as settings a1-a3. After knocking out the key heads identified based on Arabic numbers, the prediction accuracy for samples containing English numbers still decreases noticeably, while there is almost no decline in accuracy for samples containing Chinese and Roman numbers. We assume this is because the training corpus of LLaMA2 mainly consists of English language texts, which could explain the disparity in performance."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3226/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149924180,
                "cdate": 1700149924180,
                "tmdate": 1700149924180,
                "mdate": 1700149924180,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XmiOLrJmzA",
                "forum": "VpCqrMMGVm",
                "replyto": "GQN85sAbm5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Welcome for more discussions (#h6js)"
                    },
                    "comment": {
                        "value": "Thanks for your valuable time in reviewing and constructive comments, according to which we have tried our best to answer the questions and carefully revise the paper. Here is a **summary of our response** for your convenience:\n\n- (1) **Interpretation issues**: Following your constructive comments, we have provided more studies to interpret the behavior of LLMs w.r.t. key heads and MLPs. We also conducted experiments on MLPs to provide a full picture of the inner workings of LLMs in addition. \n\n- (2) **Experiment issues**: Following your valuable suggestions, we have conducted experiments on more number representations, including different languages and multi-digit numbers. The results of scaling to larger numbers provide an insightful explanation for \"the ability of LLMs to do arithmetic quickly decreases with the increase of the number of digits\". Moreover, we also added experiments to evaluate the polysemanticity of key heads, and delve deeper into the effects of numbers and arithmetic.\n\n- (3) **Writing issues**: Following your kind suggestions,  we have clarified the potentially confusing issues, including the visualization case in Fig. 4, the expression in Introduction, and implementation details about $W_O$. We have also revised these issues, highlighted in blue, in our revision.\n\nWe humbly hope our repsonse has addressed your concerns. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work.\n\n\nBest regards\n\nAuthors of #3226"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3226/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700445692908,
                "cdate": 1700445692908,
                "tmdate": 1700445692908,
                "mdate": 1700445692908,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k6R891reOi",
                "forum": "VpCqrMMGVm",
                "replyto": "GQN85sAbm5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responsing and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #h6js,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responsing and paper revision is closing, would you mind checking our response ([a brief summary](https://openreview.net/forum?id=VpCqrMMGVm&noteId=XmiOLrJmzA), and [details](https://openreview.net/forum?id=VpCqrMMGVm&noteId=g2v0fTbFQE)) and confirm whether you have any further questions? We are very glad to provide answers and revision to your further questions.\n\nBest regards and thanks,\n\nAuthors of #3226"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3226/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527043314,
                "cdate": 1700527043314,
                "tmdate": 1700527043314,
                "mdate": 1700527043314,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "30t6T6wBcu",
                "forum": "VpCqrMMGVm",
                "replyto": "k6R891reOi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3226/Reviewer_h6js"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3226/Reviewer_h6js"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response. \n\n> We would like to highlight the identification of key heads _implicitly_ interpret the model behavior (e.g. addition calculation).\n\nConsidering that _interpretability_ already is a vague term that you have not formally defined, I am concerned that _implicit interpretability_ has even less practical or scientific value.\n\n> Before this study, there was a lack of clear interpretation regarding several important questions of addition calculation in LLMs. (i) Are there attention heads consistently involved in addition calculation? (ii) If yes, are they sparsely or densely distributed? (iii) In the case of sparsity, where exactly are these key heads located?\nTo shed light on these matters, we performed sufficient experiments using different models and data formats, striving to present the observed phenomena in a clear and understandable manner.\n\nI agree that these questions may not have been studied before. But it is not clear to me why are these questions interesting, important, or relevant. Why is addition so important? (Much more than the myriad of other operations and behaviors that one could study?) What value does it bring knowing where these heads are located?   \n\n> We added one experiment of path patching to measure the causal effect of each MLP layer. The results indicate that: (i) the early MLP layers 0-13 before the key heads (e.g., 13.11) have a slight effect on the output (approximately \n0.5%). (ii) The late MLP layers 14-31 after the key heads exhibit a much larger effect (approximately\n10.0%). These findings may reveal that the key heads are responsible for attending to number tokens, while the following MLP layers process the number tokens.\n\nYour response seem to confirm my concerns. The MLPs are likely involved in the computation and you cannot ignore them. Moreover, the Path Patching procedure you describe maintains the connections through the MLPs. Therefore, how can we disambiguate which components are \"responsible\" when they might be _jointly necessary conditions_ to exhibit the behavior you study.\n\n> We also noticed that previous works [1, 2] tried to explain MLPs for performing computations and retrieving facts. However, it necessitates thorough and rigorous analysis to make the detailed process in MLPs human-understandable, and interpret the collaboration between attention heads and MLPs. We believe that making the calculation fully understandable to humans is an intriguing direction that warrants sustained research efforts.\n\nYour abstract claims that you _\"take the first attempt to reveal a specific mechanism relating to how LLMs implement the reasoning task of a mathematical addition, e.g., scenarios involving simple one-digit integer addition\"_. That expects that you would have included this _\"thorough and rigorous analysis\"_...\n\n> Thanks for your constructive suggestion. We conduct the knockout experiments on the following number formats. The table shows one representative sample for each format and the accuracy change after knocking out the key heads in LLaMA2-7B. \n\nThank you for running these additional experiments! They indeed confirm my concerns: If you managed to locate all the key heads involved in summation, then knocking them out would have also resulted in significantly lower accuracy for the Chinese and Roman cases. The fact that they see only 5% drop in performance shows that there are summation-relevant heads that your analysis has failed to identify.\n\n> Q3: More illustration\n\nThe second example in Fig4 still doesn't exhibit the behavior you claim: the space after \"equal to\" should attend to \"1\" and \"3\", not to \"2\". Similarly, Fig 9.2 doesn\u2019t attend to \"7\". From your response it also seems that these examples might have been cherry-picked to represent the behavior you want to demonstrate.\n\nOverall it feels like this paper applies existing mechanistic interpretability techniques to the problem of summing numbers. In light of the outstanding concerns above and (my subjective view) that this work does not contribute new knowledge and sufficient value to the community, I will lower my score."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3226/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653053062,
                "cdate": 1700653053062,
                "tmdate": 1700653053062,
                "mdate": 1700653053062,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1bdsUgaWIC",
            "forum": "VpCqrMMGVm",
            "replyto": "VpCqrMMGVm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3226/Reviewer_7p7z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3226/Reviewer_7p7z"
            ],
            "content": {
                "summary": {
                    "value": "In this study, the authors aim to delve into the underlying mechanisms of Large Language Models (LLMs) by analyzing attention heads at various layers in tasks that require the addition of two integers. Specifically, they focus on the LLAMA2-7B, Qwen-7B, and ChatGLM2-6B language models. Their findings reveal that a limited number of attention heads significantly influence the model's output, and these conclusions are drawn from a range of experiments. Furthermore, the authors show some preliminary results indicating that these same attention heads play a significant role in the performance of subtraction tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Authors are tackling an important problem by aiming to understand the inner workings of LLMs. With the increased pace of advancements happening in the field, it is imperative to gain this understanding. \n\nAuthors tackle the problem in a clear manner, by coming up with a clean task (involving addition of 2 integers) and testing their hypothesis systematically. \n\nTheir findings indicate that a limited number of attention heads suffice for achieving strong performance across a range of addition tasks. Importantly, the methodology they introduce can prove valuable for conducting sensitivity analyses in other areas of interest and even facilitate model sparsification.\n\nThey validate their hypothesis on several LLMs and a few addition tasks. Additionally, their preliminary investigations reveal that the attention heads vital for addition tasks also exert a substantial influence on subtraction."
                },
                "weaknesses": {
                    "value": "While the authors have indeed posed a clear problem and approached it systematically, I find the setup to be somewhat restrictive.\n\n- Although the authors make a great effort to tackle the task of addition, their focus remains solely on the addition of two integers. It would be intriguing to see whether their findings extend to addition of multiple integers and rational numbers, as well as their applicability to problems involving multiple addition operations.\n\n- The robustness of this study could be significantly enhanced if the authors were to conduct analogous experiments on subtraction, multiplication, and division. Such investigations would shed light on whether a select group of attention heads can consistently influence performance across all four mathematical operations."
                },
                "questions": {
                    "value": "Please refer to weakness section. It would be great if authors have any additional insights regarding the points in weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3226/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3226/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3226/Reviewer_7p7z"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3226/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698649552223,
            "cdate": 1698649552223,
            "tmdate": 1700596843779,
            "mdate": 1700596843779,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Bt8RQxcX6u",
                "forum": "VpCqrMMGVm",
                "replyto": "1bdsUgaWIC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7p7z [part 1/2]"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for taking the time to review our work. We appreciate that you find our work tackles an important problem in a clear and systematical manner, and is transferable to facilitates other areas. Thank you for acknowledging our efforts to the significant findings. According to your valuable comments, we provide detailed feedback. \n\n**Q1**: More experiments on addition task:\n> \"their focus remains solely on the addition of two integers ... extend to addition of multiple integers and rational numbers, as well as their applicability to problems involving multiple addition operations.\"\n\n**Ans for Q1:**  \nThank you for the constructive suggestion. \n* We agree that it's necessary to take experiments on more formats of numbers besides one-digit integer numbers. The reason why we perform on one-digit addition in the primary experiments is that the adopted three LLMs (e.g., LLaMA2-7B) tokenize each digit individually (e.g., '42' is tokenized to '4' and '2'). We follow the one-digit nature of LLMs for simplicity in generating large-scale data.  \n\n* In response to the suggestion, we conduct the experiments of path patching on the two-digit sentence templates: \"{A1}{A2} + {B1}{B2} = \", and measure the causal effects on the averaged logit of both {C1} and {C2}. We find that the distribution of key heads remains analogous to the results on \"{A} + {B} = \". This phenomenon reveals that the key heads responsible for attending to the numbers are also involved in two-digit addition. We hypothesize this is because one-digit addition serves as the fundamental computation *\"unit\"* for multi-digit addition.  \n\n* To investigate the potential of extending the observed effects of one-digit addition to more addition formats, we conduct the knockout experiments on multi-digit integers (a1-a3), rational numbers (b1), and multiple addition operations (c1). The table shows one representative sample for each format and the accuracy change after knocking out the key heads in LLaMA2-7B.\n\n| settings | formats | samples | accuracy |\n|-------| ------- | ------- | ------- |\n|     a1    | two-digit  |   \"35 + 42 = \" |    -69%     |\n|     a2   | three-digit  |   \"154 + 243 = \" |     -72%    |\n|     a3  | four-digit  |   \"1524 + 3463 = \" |    -76%     |\n|     b1   | rational  |   \"2.4 + 7.2 = \" |     -70%    |\n|     c1  | multi-add  |   \"42 + 21 + 15 = \" |    -78%     |\n\n* Upon observation, it is evident that in settings a1-a3, the performance of samples containing multi-digit numbers significantly declines after knocking out the identified key heads based on one-digit addition. The decline becomes more pronounced when scaling up to larger numbers. This suggests that the perturbation effects on one-digit addition may accumulate and have a greater impact when applied to larger numbers.  \nFurthermore, in settings b1 and c1, the decline in performance provides additional support for the hypothesis that the one-digit addition *\"unit\"* could lay the foundation for more complex addition operations."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3226/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149756336,
                "cdate": 1700149756336,
                "tmdate": 1700149756336,
                "mdate": 1700149756336,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ppKUir1vrs",
                "forum": "VpCqrMMGVm",
                "replyto": "1bdsUgaWIC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Welcome for more discussions (#7p7z)"
                    },
                    "comment": {
                        "value": "Thanks for your valuable time in reviewing and constructive comments, according to which we have tried our best to answer the questions and carefully revise the paper. Here is a **summary of our response** for your convenience:\n\n- (1) **Addition task issues**: Following your constructive comments, we have conducted experiments on more number samples, including \"multiple integers\", \"rational numbers\", and \"multiple addition operations\". The results reveal that the potential of extending the scope of this work to multi-digit and non-trivial scenarios.\n- (2) **More math tasks**: Following your valuable suggestions, we have conducted experiments on more tasks of subtraction, multiplication and division. We also summarize the insightful phenomena of the shared key heads across the opposite tasks of \"addition-subtraction\" and \"multiplication-division\".\n\nWe humbly hope our repsonse has addressed your concerns. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work.\n\n\nBest regards\n\nAuthors of #3226"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3226/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700445676579,
                "cdate": 1700445676579,
                "tmdate": 1700445676579,
                "mdate": 1700445676579,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VIBRRjxEUc",
                "forum": "VpCqrMMGVm",
                "replyto": "1bdsUgaWIC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responsing and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #7p7z,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responsing and paper revision is closing, would you mind checking our response ([a brief summary](https://openreview.net/forum?id=VpCqrMMGVm&noteId=ppKUir1vrs), and [details](https://openreview.net/forum?id=VpCqrMMGVm&noteId=Bt8RQxcX6u)) and confirm whether you have any further questions? We are very glad to provide answers and revision to your further questions.\n\nBest regards and thanks,\n\nAuthors of #3226"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3226/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527031328,
                "cdate": 1700527031328,
                "tmdate": 1700527031328,
                "mdate": 1700527031328,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QFA17EFIiE",
                "forum": "VpCqrMMGVm",
                "replyto": "1bdsUgaWIC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3226/Reviewer_7p7z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3226/Reviewer_7p7z"
                ],
                "content": {
                    "title": {
                        "value": "Re: authors response"
                    },
                    "comment": {
                        "value": "I thank the authors for their time and clarifying the points I raised. The supplementary experiments and insights provided by the authors can be of a significant value of the paper. I am revising my score from 5 to 6."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3226/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596817980,
                "cdate": 1700596817980,
                "tmdate": 1700596873721,
                "mdate": 1700596873721,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WHjm3zElaq",
            "forum": "VpCqrMMGVm",
            "replyto": "VpCqrMMGVm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3226/Reviewer_K8c8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3226/Reviewer_K8c8"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates how three different language models (LLMs) perform on simple one-digit addition problems. The researchers generated 10,000 sample addition questions across 20 different formats (such as \"42 plus 34 equals ___\") to analyze. Through this analysis, they identified the most important attention heads involved in the addition calculations for each model. To confirm the importance of these heads, the researchers ablated them and used counterfactual examples, which showed a clear impact on loss when these heads were removed. Interestingly, only a very small number of attention heads were consistently involved in the addition across all the different question formats. Further examination showed these heads specifically focus on the numerical tokens in the input strings. The researchers replicated some of these findings with one-digit subtraction as well. The paper clearly maps out how a few key attention heads enable simple addition across different state-of-the-art LLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n\n- The language of the paper is concise and clear.\n- The breadth and depth of the paper is excellent - specifically the use of 3 LLMs\n(LLaMA2-7B, Qwen-7B and chatGLM2-6B), 20 question formats and 10K sample\nquestions.\n- The rigorous nature of the paper is excellent - the claims re addition are confirmed via\ndetailed experimentation.\n- The most significant finding is that a small number of attention heads are consistently\nused by each model to perform one-digit addition across the various question formats."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n\n- The paper (seems to) limit itself to one-digit addition and subtraction - reducing its scope\nto a subset of addition and subtraction. The abstract should explicitly say that the scope\nis one-digit integer addition.\n-  The paper (seems to) limit itself to simple one-digit addition and subtraction (without\n\u201ccarry over one\u201d or \u201cborrow one\u201d examples - reducing its scope to a subset of addition\nand subtraction. The abstract should explicitly say that the scope is simple one-digit\ninteger addition.\n-  The paper does not explain how the attention heads (&/or MLP layer) actually perform\nthe addition calculation. This explanation is left for future work.\n-  The paper touches on subtraction, showing similarities, but a detailed analysis is left for\nfuture work.\n-  A discussion of the differences in how each of the LLMs implement one-digit addition\nwould have been interesting e.g. do all the models use roughly the same number of attention heads to implement addition? If no differences were found, then this would be\nan interesting finding in itself.\n-  The small scope of this paper limits the reusability of this work."
                },
                "questions": {
                    "value": "Questions:\n\n- The addition examples seem to be \u201csimple\u201d one-digit integer addition with a one\ncharacter answer. There appear to be no \u201ccarry over one\u201d examples in the test questions\ne.g \u201c5 plus 7 is equal to 1_\u201d. If this is so, it reduces the findings scope to some\nsubclasses of addition.\n\n- The subtraction examples all seem to be \u201csimple\u201d one-digit integer subtraction with a one\ncharacter answer. There appear to be no \u201cborrow one\u201d examples in the test questions\ne.g \u201c112 minus 5 is equal to 10_\u201d. If this is so, it reduces the findings scope to some\nsubclasses of subtraction.\n\n- The calculation of the subtraction question \u201c{A} - {B} =\u201d likely has two distinct calculation\nalgorithms: one for when A > B and one for when A < B. Do the authors think that this\nexplains the 52% performance drop when the addition attention heads are ablated?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3226/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3226/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3226/Reviewer_K8c8"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3226/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822466294,
            "cdate": 1698822466294,
            "tmdate": 1699636270739,
            "mdate": 1699636270739,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LyKFOSwBNg",
                "forum": "VpCqrMMGVm",
                "replyto": "WHjm3zElaq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer K8c8 [part 1/2]"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for taking the time to review our work. We are grateful that you find our research to possess excellent breadth, depth, and rigorous nature, while also being presented in a concise and clear manner. Thank you for acknowledging our efforts to the significant findings. According to your valuable comments, we provide detailed feedback. \n\n**Q1**: The scope of this work:\n> \"i) one-digit addition and subtraction - reducing its scope to a subset of addition and subtraction. The abstract should explicitly say that the scope is one-digit integer addition.\"  \n> \"ii) simple one-digit addition and subtraction (without \u201ccarry over one\u201d or \u201cborrow one\u201d examples - reducing its scope to a subset of addition and subtraction. The abstract should explicitly say that the scope is simple one-digit integer addition.\"  \n> \"iii) The small scope of this paper limits the reusability of this work.\"  \n\n**Ans for Q1:**  \n**i)** Thanks for your constructive suggestion. Accordingly, we have conducted more experiments and added the results/explanations to our revision.\n\n* We have revised the scope in the Abstract to \"simple one-digit integer addition\" in the revision. We would like to note that the reason why we experiment on one-digit addition in the primary scope is that the adopted three LLMs (e.g., LLaMA2-7B) tokenize each digit individually (e.g., '42' is tokenized to '4' and '2'). We follow the one-digit nature of LLMs for simplicity in generating large-scale data.\n\n* In response to your kind suggestion, we conduct the experiments of path patching on the two-digit sentence templates: \"{A1}{A2} + {B1}{B2} = \", and measure the causal effects on the averaged logit of both {C1} and {C2}. We find that the distribution of key heads remains analogous to the results on \"{A} + {B} = \", albeit with different magnitude of the effect. This phenomenon reveals that the key heads responsible for attending to the numbers are also involved in two-digit addition. We assume this is because one-digit addition serves as the fundamental computation *\"unit\"* for multi-digit addition, demonstrating the potential of extending the observed effects of one-digit addition to multi-digit addition.\n\n**ii)** Thank you for the insightful comment. Following the knockout experiments in Section 4.3, we first generate 200 samples that need to carry over one or borrow one (e.g., \"17 + 9 = \" and \"17 - 9 = \"). Then we knock out the identified key heads in addition, resulting in a wrong prediction on over 150 samples. We assume this is because the key heads attend to the addends \"7\" and \"9\", regardless of whether it needs to carry over one or borrow one.\n\n**iii)** We agree that a larger scope is important for the reusability. The above studies suggest the potential of extending the scope of this work to multi-digit and non-trivial scenarios. More analyses and discussions will be updated in the revision.\n\nWe believe these novel observations would benefit a lot to the quality of our paper. Thanks again for your constructive suggstions.\n\n**Q2**: More explanation:\n> \"explain how the attention heads (&/or MLP layer) actually perform the addition calculation\"  \n\n**Ans for Q2:**  \nThank you for bringing up the insightful comment!\n* We agree that delving into the *\"causal mechanism\"* of addition calculation is a quite promising direction. In this work, our primary contribution lies in analyzing the *\"causal effect\"* of component mediators to the model behavior. We also noticed that recent research [1, 2] tried to explain MLPs responsible for performing computations and retrieving facts. However, it necessitates thorough and rigorous analysis to make the detailed process in MLPs human-understandable, and interpret the collaboration between attention heads and MLPs. We believe that making the calculation fully understandable to humans is an intriguing direction that warrants sustained research efforts.\n* In response to the comment, we added one experiment of path patching to measure the causal effect of each MLP layer. The results indicate that: (i) the early MLP layers 0-13 before the key heads (e.g., 13.11) have a slight effect on the output (approximately $\\pm$0.5%). (ii) The late MLP layers 14-31 after the key heads exhibit a much larger effect (approximately $\\pm$10.0%). These findings may reveal that the key heads are responsible for attending to number tokens, while the following MLP layers process the number tokens.\n\n\n[1] Locating and Editing Factual Associations in GPT. In NeurIPS 2022.  \n[2] Transformer Feed-Forward Layers Are Key-Value Memories. In EMNLP 2021."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3226/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149556636,
                "cdate": 1700149556636,
                "tmdate": 1700149556636,
                "mdate": 1700149556636,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1ecAeVExTs",
                "forum": "VpCqrMMGVm",
                "replyto": "WHjm3zElaq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Welcome for more discussions (#K8c8)"
                    },
                    "comment": {
                        "value": "Thanks for your valuable time in reviewing and constructive comments, according to which we have tried our best to answer the questions and carefully revise the paper. Here is a **summary of our response** for your convenience:\n\n- (1) **Work scope issues**: Following your constructive comments, we have conducted experiments on more examples, including \"multi-digit\", \"carry over one\", and \"borrow one\". The results reveal that the potential of extending the scope of this work to multi-digit and non-trivial scenarios.\n- (2) **More discussions**: Following your valuable suggestions, we have provided more discussions about the interpretation of attention heads and MLPs, the analysis on the symmetry between addition and subtraction, the comparison of key heads between different LLMs, and the reason why the performance in subtraction decreases. \n\nWe humbly hope our repsonse has addressed your concerns. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work.\n\n\nBest regards\n\nAuthors of #3226"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3226/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700445655744,
                "cdate": 1700445655744,
                "tmdate": 1700445655744,
                "mdate": 1700445655744,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BMjq31SvnF",
                "forum": "VpCqrMMGVm",
                "replyto": "WHjm3zElaq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responsing and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #K8c8,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responsing and paper revision is closing, would you mind checking our response ([a brief summary](https://openreview.net/forum?id=VpCqrMMGVm&noteId=1ecAeVExTs), and [details](https://openreview.net/forum?id=VpCqrMMGVm&noteId=LyKFOSwBNg)) and confirm whether you have any further questions? We are very glad to provide answers and revision to your further questions.\n\nBest regards and thanks,\n\nAuthors of #3226"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3226/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527016444,
                "cdate": 1700527016444,
                "tmdate": 1700527016444,
                "mdate": 1700527016444,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XOF0lpjtF3",
                "forum": "VpCqrMMGVm",
                "replyto": "IxIZYkAr4A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3226/Reviewer_K8c8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3226/Reviewer_K8c8"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to respond to my questions and comments. I find the responses helpful and i think the papers quality can improve with further experiments and results. I will maintain my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3226/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557972257,
                "cdate": 1700557972257,
                "tmdate": 1700557972257,
                "mdate": 1700557972257,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ax1aEHFP6r",
            "forum": "VpCqrMMGVm",
            "replyto": "VpCqrMMGVm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3226/Reviewer_jEqH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3226/Reviewer_jEqH"
            ],
            "content": {
                "summary": {
                    "value": "Three workings of models, LLaMA2-7B, Qwen-7B, and  chatGLM2-6B, are interpreted using the path patching method (initially introduced in [1], which is an interoperability method rooted in causal intervention) on tasks involving mathematical addition and subtraction. The authors create various datasets for this purpose. They find that only a small number of attention heads are responsible for reasoning.\n\nThis represents a good effort to interpret large language models using path patching and mean ablation and it is the first paper where mathematical addition is interpreted in this way.\n\n[1] https://openreview.net/pdf?id=NpsVSN6o4ul"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- a timely topic is treated, how models that are used in practice perform mathematical addition and subtraction\n- a large number of figures that show how attention heads are activated on concrete examples help to make the paper readable"
                },
                "weaknesses": {
                    "value": "- The authors didn't include, as related work, some publications that also deal with mathematical reasoning, such as [1]\n- studying only mathematical addition and subtraction seems restrictive. I do note that the authors state at the end however: \"_A more thorough study on the subtraction task as well as the validation on more computation tasks (e.g., multiplication and division, etc.) is left for future work._\"\n\n[1] https://arxiv.org/pdf/2305.08809.pdf"
                },
                "questions": {
                    "value": "-Since addition and subtraction are opposite mathematical operations, is there some kind of similar symmetry observable on the level of attention heads?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "(not applicable)"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3226/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3226/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3226/Reviewer_jEqH"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3226/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835822297,
            "cdate": 1698835822297,
            "tmdate": 1699636270657,
            "mdate": 1699636270657,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PhYFDwLBxp",
                "forum": "VpCqrMMGVm",
                "replyto": "ax1aEHFP6r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jEqH"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for taking the time to review our work. We appreciate that you find our work represents a good effort in **a timely topic** and is easy to follow with concrete examples. Thank you for acknowledging our research as the first paper with significant findings of interpreting mathematical addition in LLMs. According to your valuable comments, we provide detailed feedback. \n\n**Q1**: Related work:\n> \"some publications that also deal with mathematical reasoning, such as [1].\"  \n\n**Ans for Q1:**  \nThanks for bringing attention to this outstanding work [1], we have added a discussion in the Background section. The studies in [1] scale the methods from causal abstraction to understand how Alpaca (7B) follows a particular instruction: \"Please say yes only if it costs between [X] and [Y] dollars, otherwise no.\", where it needs to compare the input value with the lower bound [X] and the upper bound [Y]. Different from the task of *number comparison*, we focus on the *number addition* behavior of LLMs and attempt to make their inner workings more understandable to humans.\n\n[1] Interpretability at Scale: Identifying Causal Mechanisms in Alpaca. In ArXiv preprint 2023.\n\n**Q2**: Extend to more tasks:\n> \"studying only mathematical addition and subtraction seems restrictive.\"  \n\n**Ans for Q2:**  \nThanks for your constructive suggestion. We conduct experiments on four mathematical tasks using the following representative templates.  \n\n| tasks | template1 | template2 | template3 |\n| ------- | ------- | ------- |  ------- | \n|    addition   | \"{A} + {B} = \" |    \"The sum of {A} and {B} is \"   | \"Question: What is {A} plus {B}? Answer: \"|\n|    subtraction   | \"{A} - {B} = \"|    \"The difference between {A} and {B} is \"   | \"Question: What is {A} minus {B}? Answer: \" |\n|    multiplication   | \"{A} * {B} = \" |    \"The product of {A} and {B} is \"   | \"Question: What is {A} times {B}? Answer: \"|\n|    division   | \"{A} / {B} = \"|    \"The ratio between {A} and {B} is \"   | \"Question: What is {A} over {B}? Answer: \" |\n\n- Our experimental results show that: \n  - The sparsity of key heads remains consistent across all four tasks (less than 1.0% of all heads); \n  - The key heads mainly distribute in the middle layers.  \nThe phenomena are analogous to the primary findings on the addition task (Section 4.1), demonstrating the potential of extending the observed effects of the addition task to other mathematical tasks. \n* We compare the location of key heads across four mathematical tasks. An interesting finding is that the key heads used in \"subtraction\" and \"addition\" tasks overlapped significantly, as did the key heads used in \"multiplication\" and \"division\" tasks. Moreover, the four tasks share the heads (e.g., 13.11 and 12.22) that deliver the most significant effects, while they have task-specific heads that only emerge in its own task. These findings suggest that LLMs exhibit behavior aligned with human thinking to some extent, since \"subtraction-addition\" and \"multiplication-division\" are opposite mathematical operations.\n\nThe above results and discussions have been incorporated into the revision (Apendix B).\n\n**Q3**: Analysis of addition and subtraction tasks:\n> \"is there some kind of similar symmetry observable on the level of attention heads?\"  \n\n**Ans for Q3:**  \nThanks a lot for bringing up this insightful question!\n* In the above experiments for **Q2**, we find that the identified key heads in the addition task are almost the same to those in the subtraction task, albeit with different magnitude of the effect. This phenomenon may reveal the symmetry of key head *\"location\"* in addition and subtraction. \n* Moreover, as we show the attention pattern of key heads, we observe that these heads particularly attend to the number tokens regardless of whether they are given addition or subtraction sentences. This phenomenon may reveal the symmetry of key head *\"behavior\"* in addition and subtraction."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3226/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149393268,
                "cdate": 1700149393268,
                "tmdate": 1700149393268,
                "mdate": 1700149393268,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cxle2Siqb5",
                "forum": "VpCqrMMGVm",
                "replyto": "ax1aEHFP6r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Welcome for more discussions (#jEqH)"
                    },
                    "comment": {
                        "value": "Thanks for your valuable time in reviewing and constructive comments, according to which we have tried our best to answer the questions and carefully revise the paper. Here is a **summary of our response** for your convenience:\n\n- (1) **Related work issues**: Following your constructive comments, we have discussed related works in mathematical reasoning (Alpaca) to highlight our novelty. And we also add these discussions into our revision to enhance our work.\n- (2) **Experiment task issues**: Following your valuable suggestions, we have conducted experiments on more tasks of subtraction, multiplication and division. We also summarize two aspects of the similar symmetry in addition and subtraction.\n\nWe humbly hope our repsonse has addressed your concerns. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work.\n\n\nBest regards\n\nAuthors of #3226"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3226/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700445640953,
                "cdate": 1700445640953,
                "tmdate": 1700445640953,
                "mdate": 1700445640953,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g5BMQGXOmB",
                "forum": "VpCqrMMGVm",
                "replyto": "ax1aEHFP6r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3226/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Window for responsing and draft updating is closing"
                    },
                    "comment": {
                        "value": "Dear Reviewer #jEqH,\n\nThanks very much for your time and valuable comments. We understand you're busy. But as the window for responsing and paper revision is closing, would you mind checking our response ([a brief summary](https://openreview.net/forum?id=VpCqrMMGVm&noteId=cxle2Siqb5), and [details](https://openreview.net/forum?id=VpCqrMMGVm&noteId=PhYFDwLBxp)) and confirm whether you have any further questions? We are very glad to provide answers and revision to your further questions.\n\nBest regards and thanks,\n\nAuthors of #3226"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3226/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527006598,
                "cdate": 1700527006598,
                "tmdate": 1700527006598,
                "mdate": 1700527006598,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D4qoR586xH",
                "forum": "VpCqrMMGVm",
                "replyto": "g5BMQGXOmB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3226/Reviewer_jEqH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3226/Reviewer_jEqH"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer answer"
                    },
                    "comment": {
                        "value": "I thank the authors for their efforts in revising the paper.\n\nIt reads much better now. (But please be careful, there are still some typos in some sections, e.g. \"GPT2-samll\" in the background sections.) I will maintain my score."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3226/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733664206,
                "cdate": 1700733664206,
                "tmdate": 1700733664206,
                "mdate": 1700733664206,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]