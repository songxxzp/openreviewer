[
    {
        "title": "Elucidating the Solution Space of Extended Reverse-Time SDE for Diffusion Models"
    },
    {
        "review": {
            "id": "7d1ehHOd2A",
            "forum": "XasWgF5WsZ",
            "replyto": "XasWgF5WsZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission613/Reviewer_bLLA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission613/Reviewer_bLLA"
            ],
            "content": {
                "summary": {
                    "value": "To speed the sampling in diffusion models, this paper proposes an extended reverse-time SDE (ER SDE). Contrary to the usual order of treatment, they unify prior explorations into ODEs and SDEs, wherein avoiding the lower diversity caused by neural ODEs. Moreover, some mathematical insights is presented to elucidate the fast reason of ODE solvers compared to SDE solvers. Importantly, the experiments show remarkably performance on all stochastic samplers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method is novel and interesting to speed the sampling in diffusion models, which also avoid the lower diversity caused by ODE solvers. In my humble opinion, the mathematical analysis is rigorous and can support the improvements on the experiment results. Furthermore, the SOTA performance on stochastic samplers contributes to diffusion models to practice applications."
                },
                "weaknesses": {
                    "value": "1.Though it is know that the diversity of generated images will be increased while using SDE solvers, it is better to use some metrics to demonstrate it, such as Inception Score and Precision. I guess it will further demonstrate the superior of ER SDE in community.\n\n2.The $\\phi (x)$ is a hyper-parameter, can it be adaptive implement on various diffusion models? since it is just set it manually in this paper."
                },
                "questions": {
                    "value": "The same as Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission613/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission613/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission613/Reviewer_bLLA"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission613/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698544417170,
            "cdate": 1698544417170,
            "tmdate": 1699635989085,
            "mdate": 1699635989085,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OSBh0zO640",
                "forum": "XasWgF5WsZ",
                "replyto": "7d1ehHOd2A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to your valuable comments"
                    },
                    "comment": {
                        "value": "We extend our sincere appreciation to Reviewer bLLA for the thoughtful consideration of our manuscript and the valuable insights provided. We address each query in detail as outlined below. \n## **Q1\uff1aUse some metrics to demonstrate diversity, such as Inception Score and Precision**\nThanks for your constructive suggestion! In order to demonstrate diversity, Fig.6 - Fig.10 in Appendix D compare the sampling results between stochastic samplers\uff08ER-SDE-Solvers\uff09 and deterministic samplers on various datasets when fixing the random seed. As stochastic samplers introduce stochastic noise at each step of the sampling process, the generated images exhibit greater diversity, which becomes more pronounced in higher-resolution images. \\\nIn addition to the FID metric already presented in our paper, we appreciate your suggestion and have included additional metrics to further showcase the superiority of ER-SDE-Solvers. Specifically, we have chosen four metrics: **Inception Score, Precision, Recall, and sFID**. The relevant results are provided in Appendix E. **Experimental findings consistently indicate that ER-SDE-Solvers perform optimally across all metrics, highlighting superior diversity and fidelity across all stochastic samplers.**\n## **Q2\uff1aCould $\\phi(x)$ be adaptive rather than manual?**\nThank you for this constructive suggestion! Currently, there lacks a comprehensive theoretical explanation regarding the selection of the noise schedules. Previous studies [1][2][3] have predominantly relied on manually designed noise schedules. [4] argued that the implicit choice for noise schedules enjoys no special properties and the optimal amount of stochasticity should be determined empirically.\\\n**Compared to entirely empirical choices, we have taken a significant stride in the study of noise schedule principles.** Our criterion for selecting the noise schedules (noise scale functions) is based on the analysis of the First-order Euler Integral (FEI) coefficient, coupled with relevant experimental observations, rendering it practical (see Appendix A.8). The customizable $\\phi(x)$ provides practitioners with a broad spectrum of choices, allowing them to make diverse selections based on specific application scenarios. This flexibility enables a fine balance between the efficiency of sampling and the diversity of generated images.\\\nDifferent noise scale functions $\\phi(x)$ correspond to different solutions, collectively forming the solution space of the ER SDE. **This aligns with the focus of this study, which is to elucidate the solution space of the ER SDE**. Therefore, this paper only explores some of the noise scale functions for the reverse process, providing examples of excellent performance (such as ER SDE 4 and ER SDE 5). Of course, whether an optimal and adaptive $\\phi(x)$ exists is worth further investigation.\n\nWe sincerely hope that our responses align with your expectations, and we would be extremely grateful if these clarifications contribute positively to the assessment of our manuscript. \n\n[1]Denoising diffusion probabilistic models, Ho, J. et al.\\\n[2]Improved techniques for training score-based generative models, Song, Y. and Ermon, S.\\\n[3] gDDIM: Generalized denoising diffusion implicit models, Zhang et al.\\\n[4] Elucidating the design space of diffusion-based generative models, Karras et al."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699947313191,
                "cdate": 1699947313191,
                "tmdate": 1699947313191,
                "mdate": 1699947313191,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ak31KSE0Ih",
            "forum": "XasWgF5WsZ",
            "replyto": "XasWgF5WsZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission613/Reviewer_f7G3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission613/Reviewer_f7G3"
            ],
            "content": {
                "summary": {
                    "value": "This paper formulates the sampling process as extended reverse-time SDE (ER SDE) for both VP and VE SDE, unifying previous diffusion ODEs and SDEs. Based on ER SDE and its semi-linear structure, the authors derive an analytical solution of ER SDE and then devise training-free SDE samplers. The authors then test their proposed methods on several image-generation experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is overall well-written and easy to follow.\n2. This paper tested several hand-crafted noise schedules $\\phi(x)$ and the numerical experiments show improvements compared to other baseline solvers."
                },
                "weaknesses": {
                    "value": "1. The idea of extended reverse-time SDE with noise schedules uncorrelated with $g(t)$ has appeared in several papers, e.g. [1][2]. I think it would be beneficial to discuss them to motivate this paper better.\n2. Proposition 3 and Proposition 5 in this paper claim it achieves arbitrary order approximations for SDEs, which is incorrect. The main fault is while deriving approximation error for SDEs, Ito-Taylor expansion rather than Taylor expansion should be employed. The authors may refer to [2][3] for more details.\n3. The idea of utilizing the semi-linear structure in reverse diffusion process and analytical solutions has been well-established for ODEs, e.g. [4], and for SDEs, e.g. [2][3][5], which may lower the novelty of this paper.\n4. The authors tested on different hand-crafted noise schedules $\\phi(x)$. It would be better for authors to do some comprehensive experiments and design theory-motivated noise schedule principles.\n\n\n\n\n[1] Elucidating the design space of diffusion-based generative models, Karras et al.\n\n[2] SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models, Xue et al.\n\n[3] SEEDS: Exponential SDE Solvers for Fast High-Quality Sampling from Diffusion Models, Gonzalez et al.\n\n[4] DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps, Lu et al.\n\n[5] DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models, Lu et al."
                },
                "questions": {
                    "value": "I have no other questions, see the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission613/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission613/Reviewer_f7G3",
                        "ICLR.cc/2024/Conference/Submission613/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission613/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698635139204,
            "cdate": 1698635139204,
            "tmdate": 1700583071681,
            "mdate": 1700583071681,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kvPqLVv0bT",
                "forum": "XasWgF5WsZ",
                "replyto": "ak31KSE0Ih",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to your valuable comments"
                    },
                    "comment": {
                        "value": "We sincerely appreciate you for the thoughtful consideration and valuable insights.We address each query in detail as outlined below.\n# Q1:Discuss SDE with uncorrelated g(t) in [1,2]\nThanks for highlighting the relevance of SDE with uncorrelated g(t) in [1,2].Our paper differs in the following aspects:\n1. Though [2] used $\\tau(t)g(t)$ when modeling the reverse process, they took $\\tau(t)=\\tau$to a constant function for computational convenience in experiments(Sec.6).This implies **[2] essentially introduced a parameter to control the extent of noise.** However, the form of the noise scale is associated with g(t). **In contrast, ER SDE proposes an entirely new noise scale h(t) for the reverse process**, suggesting h(t) may not necessarily correlate with g(t). \n2. Due to the challenges in solving ER SDE,**[1,2] didn't offer specific implementable solvers. To our knowledge, our paper is the first to offer exact analytical solution for ER SDE.**\n3. While [1,2] generalized SDE and ODE,as noted by Reviewer yrnV,our paper tackles the ratio of how the solver should work as an SDE or ODE solver.\n\nIn the revised version,Sec.3.1 provides discussion with[1,2],significantly enhance the motivation for our paper.\n# Q2:Ito-Taylor expansion rather than Taylor expansion\nThanks for pointing it out. It is appropriate to use Ito-Taylor expansion when deriving the approximation error for SDE.The key difference between Ito-Taylor and Taylor expansion is the infinitesimal operators. However, we\u2019ve dropped the high-order terms,aligning with common practice in[2,3].\n\nWe sincerely apologize for the oversight. It is noteworthy the approximated solutions derived from Ito-Taylor and Taylor expansion are identical after dropping residual terms.As a result, our conclusions remain valid, supported by extensive experiments.Corrections have been made in Appendix A.3&A.5.We deeply appreciate your feedback again.\n# Q3:Utilizing the existing semi-linear structure lowers the novelty\nThough the utilization of the semi-linear structure in analytical solutions has been established for ODE[4] and SDE[2,3,5],distinctions are as follows:\n1. Equations being solved are different.[4] is based on ODE and[2,5] are based on the original SDE, while we use the semi-linear structure to solve ER SDE. Compared to the former, solving ER SDE is more challenging[2].Although [3] and our paper are contemporaneous, we are open to comparisons. SDE in [3] is $dx_t=[A(t)x_t+b(t)\\nabla_x\\log p_t(x_t)]dt+g(t)dw_t$, while ER SDE is $dx_t=[f(t)x_t-\\frac{g^2(t)+h^2(t)}{2}\\nabla_{x}\\log p_t(x_t)]dt+h(t)dw_t$.We can see these two SDEs are distinct.\n2. Using the semi-linear structure to solve differential equations is common, dating back to [6].However,**we aim at elucidating more comprehensive general solutions for SDEs.** Based on it,we find the superior performance of ODE solvers for fast sampling over SDE solvers, and the equal performance of VP solvers with VE solvers.\n\nWe hope our response clearly articulates the unique contributions of our paper,addressing your concerns effectively.\n# Q4:Design theory-motivated noise schedule principles\nThanks for constructive suggestion!Now, there lacks theoretical explanations for selecting noise schedules.Previous studies[7,8,9] have mainly relied on manually designed noise schedules.[1] argued the choice for noise schedules enjoys no special properties and the optimal amount of stochasticity should be determined empirically.\\\n**Compared to purely empirical choices,we've taken a significant stride in the study of noise schedule principles.** Our criterion for selecting noise schedules is based on FEI coefficient and experimental observations, which is practical.Customizable $\\phi(x)$ offers practitioners diverse options,enabling a balance between sampling efficiency and image diversity tailored to specific application scenarios.\\\nDifferent noise schedules $\\phi(x)$ correspond to different solutions, collectively forming the solution space of the ER SDE,**which aligns with the focus of our paper: elucidating the solution space of ER SDE.** Therefore,we only explore some of $\\phi(x)$ for the reverse process (ER SDE 4&5).The optimal $\\phi(x)$ is worth further investigation.\\\nWe hope our responses meet your expectations,and we'd be grateful if these clarifications positively contribute to assessing our paper.\n\n[1]Elucidating the design space of diffusion-based generative models.\\\n[2]SA-Solver:Stochastic Adams Solver for Fast Sampling of Diffusion Models.\\\n[3]SEEDS:Exponential SDE Solvers for Fast High-Quality Sampling from Diffusion.\\\n[4]DPM-Solver:A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps.\\\n[5]DPM-Solver++:Fast Solver for Guided Sampling of Diffusion Probabilistic Models.\\\n[6]Numerical Solution of Stochastic Differential Equations.\\\n[7]Denoising diffusion probabilistic models.\\\n[8]Improved techniques for training score-based generative models.\\\n[9]gDDIM:Generalized denoising diffusion implicit models."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699805743492,
                "cdate": 1699805743492,
                "tmdate": 1699805743492,
                "mdate": 1699805743492,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lvrMGHLy95",
                "forum": "XasWgF5WsZ",
                "replyto": "kvPqLVv0bT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission613/Reviewer_f7G3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission613/Reviewer_f7G3"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response. The authors' response addressed most of my issues. However, I still have questions about the convergence order of the proposed algorithm. In the revised version of the paper, Proposition 3 and Proposition 5 claim that the proposed scheme achieves arbitrary-order approximations for the SDE, which I highly disagree with. In the proof of Proposition 3 and 5, which is Appendix A.3 and A.5, the omitted term $R_k$ is of order $h^{\\frac{3}{2}}$ for which the proposed scheme has a global error of at least $O(h)$. I will consider raising my score if the authors could revise it."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576441112,
                "cdate": 1700576441112,
                "tmdate": 1700576441112,
                "mdate": 1700576441112,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7ploW47OLv",
                "forum": "XasWgF5WsZ",
                "replyto": "8vr8ix1s2U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission613/Reviewer_f7G3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission613/Reviewer_f7G3"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. I will raise my score to 5."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583158731,
                "cdate": 1700583158731,
                "tmdate": 1700583158731,
                "mdate": 1700583158731,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0EW580Jr0c",
            "forum": "XasWgF5WsZ",
            "replyto": "XasWgF5WsZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission613/Reviewer_RiM8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission613/Reviewer_RiM8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Extended Reverse-time SDE (ER-SDE) to model the sampling process of diffusion models, which can unify ODE-based and SDE-based sampling. Based on the approximation of the exact solution of ER-SDE, the authors propose ER-SDE-solver, a fast stochastic sampler for diffusion models. The experimental results on various datasets show that ER-SDE-solver achieves great sample quality within 20-50 NFE."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method is clear and complete. \n2. RE-SDE-solver consistently outperforms other stochastic samplers within 50 NFE on different models and various datasets. \n3. Extensive ablation studies are provided to understand the design component of ER-SDE-solver."
                },
                "weaknesses": {
                    "value": "1. The discretization error discussed in Section 4.1 needs further clarification. If I'm not mistaken, the discretization error is defined to be the remainder of the k-th order Taylor approximation. I do not see how the remainder is related to FEI and how you can control FEI to reduce the error. \n2. ER-SDE-solver needs to tune the hyperparameter for N-point numerical integration and design the noise scale function by monitoring the FID metric, which could be tricky to find the right balance for different models in practice. \n3. Section 4.2 discusses the results of ER SDE 4, ER SDE 5, and ODE. However, it is difficult to see the difference from Figure 3 (b). \n4. What is the order of the algorithm used to report Figure 3? The basic detail about the experiments in Figure 3 is missing in Section 4.2.\n5. The claim of outperforming all other stochastic samplers is not well supported. For example, EDM sampler [1] can achieve FID 1.55 on ImageNet64 but the best FID reported in this paper is 2.24. \n6. Typo: in the last second paragraph of introduction, \"... theoretically establish that the VP SDE solves yield image quality .. \" -> \"theoretically establish that the VP SDE solvers yield image ...\"\n\n[1]: Karras, Tero, Miika Aittala, Timo Aila, and Samuli Laine. \"Elucidating the design space of diffusion-based generative models.\"\u00a0_Advances in Neural Information Processing Systems_\u00a035 (2022): 26565-26577."
                },
                "questions": {
                    "value": "1. Regarding Table 2, can ER-SDE-solver further improve sample quality by increasing NFE? For example, in Figure 4(c) of EDM paper [1], EDM-solver can achieve FID 1.55 with more NFE."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission613/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission613/Reviewer_RiM8",
                        "ICLR.cc/2024/Conference/Submission613/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission613/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823497303,
            "cdate": 1698823497303,
            "tmdate": 1700720036757,
            "mdate": 1700720036757,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RKMZ2G7ulV",
                "forum": "XasWgF5WsZ",
                "replyto": "0EW580Jr0c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to your valuable comments[1/2]"
                    },
                    "comment": {
                        "value": "We extend our sincere appreciation to Reviewer RiM8 for the thoughtful consideration of our manuscript and the valuable insights provided. We address each query in detail as outlined below.\n## **Q1\uff1aThe discretization error discussed in Sec.4.1 needs further clarification**\nWe sincerely apologize for confusion! Sec.3 demonstrates that the exact solution of the ER SDE comprises three components: a linear function of the data variables, a non-linear function parameterized by neural networks and a noise term. The linear and noise terms can be precisely computed, while discretization errors are present in the non-linear term. Due to the decreasing error as the order increases (see Table1), the first-order error predominantly influences the overall error. Therefore, we exemplify the case with order k = 1 for error analysis.\\\nIt can be observed from Eq.(16) and Eq. (17) that the error arises from the second term$\\Big{[}1-\\frac{\\phi(\\sigma_{t_{i}})}{\\phi(\\sigma_{t_{i-1}})}\\Big{]} x_\\theta(\\tilde{x} _ {\\sigma_{t_{i-1}}},\\sigma_{t_{i-1}})$, as $x_\\theta(\\tilde{x} _ {\\sigma_{t_{i-1}}}, \\sigma_{t_{i-1}})$ is a neural network trained in the forward process to estimate data, whose precision is fixed. To reduce discretization error, it is necessary to minimize the coefficient $1 - \\frac{\\phi(x_t)}{\\phi(x_s)}$, referring as the First-order Euler Integral (FEI) coefficient. **Therefore, when using the same model weights, a lower FEI coefficient leads to smaller discretization error. In turn, the images generated in the reverse process more similar to real images in the forward process, resulting in higher image quality and a smaller FID**. We hope our explanation addresses your concerns.\n## **Q2\uff1aIt is tricky to find N-point and noise scale function for different models in practice**\nThank you for your thorough review! We conducted ablation experiments in Appendix C.3 to investigate the choice of the numerical integration points \\(N\\). To strike a balance between efficiency and performance, we fixed \\(N = 100\\) for all experiments in our paper. Although the ablation experiments were conducted on CIFAR-10 dataset using the pretrained EDM, our experimental results on other datasets such as FFHQ, ImageNet, and LUSN, using Guided-diffusion as the pretrained model, indicate that the chosen integration point \\(N=100\\) still performs well. **Therefore, practitioners do not need to excessively adjust the number of numerical integration points in practice**.\\\nRegarding the noise scale function, our experiments were conducted using the pretrained EDM on CIFAR-10 dataset (See Fig.3). Extensive experimental results indicate that our designed ER SDE 5 can generalize to all datasets and pretrained diffusion models. However, we still provide practitioners with the opportunity to customize the noise scale function according to specific application scenarios. **Compared to entirely empirical choices [1][2][3], we have taken a significant stride in the study of noise schedule principles**.\nDifferent noise scale functions $\\phi(x)$ correspond to different solutions, collectively forming the solution space of the ER SDE. **This aligns with the focus of our study, which is to elucidate the solution space of the ER SDE**. Therefore, we only explores some of the noise scale functions for the reverse process, providing examples of excellent performance (such as ER SDE 4 and ER SDE 5). Of course, whether an optimal and adaptive $\\phi(x)$ exists is worth further investigation.\n## **Q3\uff1aIt is difficult to see the difference from Fig. 3 (b)**\nWe apologize for the lack of clarity in the figures. We have provided enlarged details for Fig. 3(a), Fig. 3(b), Fig. 5(a) and Fig. 5(b) in the revised version. As observed, the ODE solver exhibits minimal discretization error. ER SDE 4 exhibits discretization error that closely adheres to the behavior of the ODE. ER SDE 5 demonstrates elevated error in the initial 100 steps and gradually converges to the ODE\u2019s error profile. Both ER SDE 4 and ER SDE 5 exhibit comparable efficiency to the optimal ODE solver. Image quality deteriorates for ill-suited noise scale functions (like ER SDE 2).\n\n\nWe sincerely hope that our responses align with your expectations, and we would be extremely grateful if these clarifications contribute positively to the assessment of our manuscript. \n\n[1]Denoising diffusion probabilistic models, Ho, J. et al.\\\n[2]Improved techniques for training score-based generative models, Song, Y. and Ermon, S.\\\n[3] gDDIM: Generalized denoising diffusion implicit models, Zhang et al."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699966357753,
                "cdate": 1699966357753,
                "tmdate": 1699966357753,
                "mdate": 1699966357753,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qu2MGzNmyv",
                "forum": "XasWgF5WsZ",
                "replyto": "0EW580Jr0c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to your valuable comments[2/2]"
                    },
                    "comment": {
                        "value": "We extend our sincere appreciation to Reviewer RiM8 for the thoughtful consideration of our manuscript and the valuable insights provided. We address each query in detail as outlined below. \n## **Q4\uff1aWhat is the order of the algorithm used to report Fig.3?**\nWe sincerely apologize for confusion. In the experiments depicted in Fig. 3, we utilize the first-order ER SDE Solvers. The relevant experimental details have been supplemented in the legend of Fig. 3. Thank you for pointing it out!\n## **Q5\uff1aThe claim of outperforming all other stochastic samplers is not well supported\uff1fCan ER-SDE-solver further improve sample quality by increasing NFE?**\nThanks for your question! The purpose of our paper is to accelerate sampling for diffusion models, so we limit our experiments to a finite number of steps (<=50 NFE), which is consistent with many works in the field [4][5][6]. For a fair comparison, FID metrics should be compared under the condition of fixed NFE. When NFE=50, ER SDE 5 achieves 2.24 FID on ImageNet64, while EDM-Stochastic only achieves 2.49. Therefore, **we claim that ER-SDE-Solvers achieve state-of-the-art performance across all stochastic samplers in the context of rapid sampling**.\\\nIn fact, ER-SDE-Solver can enhance sample quality by increasing NFE. Experimental findings indicate that judiciously introducing additional stochastic noise can effectively rectify errors in the early sampling steps, thereby generating high-quality samples in prolonged sampling scenarios. However, blindly increasing NFE for better FID might not be practical in real-world applications. Therefore, **our paper focuses on rapid sampling rather than achieving absolute high-quality samples**. Leveraging ER-SDE-Solvers for high-quality sampling can be considered for future work.\n## **Q6\uff1aSpelling correction**\nWe sincerely apologize for the spelling error. We have rectified it in the revised version. Thank you for pointing it out!\n\nWe sincerely hope that our responses align with your expectations, and we would be extremely grateful if these clarifications contribute positively to the assessment of our manuscript. \n\n[4] Fast sampling of diffusion models with exponential integrator, Zhang et al.\\\n[5] DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps, Lu et al.\\\n[6] DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models, Lu et al."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699966654820,
                "cdate": 1699966654820,
                "tmdate": 1699966654820,
                "mdate": 1699966654820,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qQy4TxgDm6",
                "forum": "XasWgF5WsZ",
                "replyto": "RKMZ2G7ulV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission613/Reviewer_RiM8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission613/Reviewer_RiM8"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for adding details to the figures. \n\nRegarding the discretization error, it seems you are discussing a different error than the one in my review. Can you precisely define what you mean by discretization error? I think what really matters here is the local error $\\tilde{\\boldsymbol{x}}_t - \\boldsymbol{x}_t$, which will accumulate over time. I still don't see how FEI can control the error. \n\nThanks for your elaboration on N-point integration and noise scale function. Assume your claim about the relation between FEI and the discretization error holds. Why does ER-SDE 5 have better results than ER-SDE 4 on CIFAR10 and ImageNet64? \n\nAnother point that confuses the reviewer is that the authors claim that ODE is the optimal solution of ER SDE as it minimizes the FEI, as shown in figure (b). Then why not just do ODE instead of ER SDE? The experimental results also show that the proposed ER SDE solvers do not consistently outperform ODE sovler."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584207928,
                "cdate": 1700584207928,
                "tmdate": 1700584207928,
                "mdate": 1700584207928,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "my82Igxzy7",
                "forum": "XasWgF5WsZ",
                "replyto": "qQy4TxgDm6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission613/Reviewer_RiM8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission613/Reviewer_RiM8"
                ],
                "content": {
                    "comment": {
                        "value": "BTW, what do you mean by Eq (9) is linear in the narrow sense? $\\boldsymbol{x}_{\\theta}$ is nonlinear so Eq(9) is a nonlinear SDE, right?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586735718,
                "cdate": 1700586735718,
                "tmdate": 1700586735718,
                "mdate": 1700586735718,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ag6XBJjBqK",
                "forum": "XasWgF5WsZ",
                "replyto": "0EW580Jr0c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to your valuable comments"
                    },
                    "comment": {
                        "value": "We extend our sincere appreciation for your valuable insights. We address each query in detail as outlined below. \n## **Q1\uff1aPrecisely define discretization error**\n**The discretization error as defined by us does not refer to the local error $\\tilde{x}_t-x_t$, but pertains to the error introduced when discretizing the nonlinear integral term**  $\\int_{\\sigma_t}^{\\sigma_s}\\frac{\\phi^{(1)}(\\sigma)}{\\phi^2(\\sigma)}x_\\theta(x_\\sigma,\\sigma)d \\sigma$. The discretization error we define directly influences the local error, as Section 3 demonstrates that the exact solution of ER SDE consists of three components: a linear function of the data variables(1st term), a non-linear function parameterized by neural networks(2nd term), and a noise term(3rd term), i.e., \n$$\nx_t=\\frac{\\phi(\\sigma_t)}{\\phi(\\sigma_s)}x_s+\\phi(\\sigma_t) \\int_{\\sigma_t}^{\\sigma_s}\\frac{\\phi^{(1)}(\\sigma)}{\\phi^2(\\sigma)}x_\\theta(x_\\sigma, \\sigma)d \\sigma+\\sqrt{\\sigma_t^2-\\sigma_s^2[\\frac{\\phi(\\sigma_t)}{\\phi(\\sigma_s)}]^2 }z_s\n$$\n**The linear and noise terms can be precisely computed, while local errors are present in the non-linear term**. Due to the decreasing error as the order increases (Table1), the first-order error predominantly influences the overall error. Therefore, we exemplify the case with order k = 1 for error analysis.\nIt can be observed that the discretization error arises from the second term is a neural network trained in the forward process to estimate data, whose precision is fixed. To reduce discretization error, it is necessary to minimize the coefficient $1 - \\frac{\\phi(x_t)}{\\phi(x_s)}$, referring as the First-order Euler Integral (FEI) coefficient. **Therefore, when using the same model weights, a lower FEI coefficient leads to smaller discretization error**. We hope our explanation addresses your concerns.\n## **Q2\uff1aWhy does ER-SDE 5 have better results than ER-SDE 4 on CIFAR10 and ImageNet64?**\nThanks for your question! In fact, **the sampling performance of ER-SDE 5 is inferior to that of ER-SDE 4 when NFE is relatively small**. Table 4 and Table 5 in Appendix D present the image generation quality of different noise scale functions on ImageNet 64 \u00d7 64 and CIFAR-10. We cite here to demonstrate clearly:\n\nTable 4: Sample quality measured by FID$\\downarrow$ on ImageNet $64\\times64$ for different noise scale functions with VE ER-SDE-Solver-3.\n|Noise scale functions\\NFE|10|15|20|25|30|40|50|\n|----------------------|----|----|----|----|----|----|----|\n|ER SDE 4|**10.79**|**4.99**|**3.45**|2.96|2.71|2.43|2.38|\n|ER SDE 5|11.46|5.08|**3.45**|**2.92**|**2.58**|**2.37**|**2.24**|\n\nTable 5: Sample quality measured by FID$\\downarrow$ on CIFAR-10 for different noise scale functions with VE ER-SDE-Solver-3.\n\n|Noise scale functions\\NFE|10|15|20|25|30|40|50|\n|----------------------|----|----|----|----|----|----|----|\n|ER SDE 4|**9.28**|**4.37**|**3.01**|**2.52**|**2.25**|**2.07**|**1.97**|\n|ER SDE 5|9.86|4.57|3.09|2.54|2.29|2.10|**1.97**|\n\nDue to the discretization errors of ER-SDE 4 being sufficiently close to the minimal error (see Fig. 3), it exhibits outstanding FID scores on CIFAR-10 and ImageNet 64\u00d764 (NFE\u226420). This result once again confirms the validity of the relationship between FEI and discretization error.\nIn the experiments on ImageNet 64\u00d764, despite the initial significant discretization error in ER-SDE 5, its later-generated image quality (NFE\u226520) is comparable to ER-SDE 4. This is attributed to the fact that ER SDE involves implicit Langevin diffusion, which can effectively correct any errors in the early sampling steps [1]. Consequently, the early errors in ER-SDE 5 are rectified in the later stages.\n## **Q3\uff1aWhy not just do ODE instead of ER SDE?**\n\nThanks for your insightful question! As highlighted in Sec.1, **SDE-based stochastic samplers exhibit an enhanced capability to generate diverse images by injecting additional noise into the data state at each sampling step**. This conclusion is further supported by Fig.6-Fig.10 in Appendix D, where stochastic samplers demonstrate a greater variability in generated images when the random seed is fixed. This advantageous feature becomes particularly prominent in the context of high-resolution image generation, effectively meeting the demands of industries such as art creation and game development.\\\nIn addition, **stochastic samplers with classifier guidance exhibit superior performance in rapid sampling compared to deterministic samplers(see Table 3)**. \nThis is attributed to the customized noise injected into the sampling process, which mitigates the inaccuracies in data estimation introduced by classifier gradient guidance.\n\nWe sincerely hope that our responses align with your expectations, and we would be extremely grateful if these clarifications contribute positively to the assessment of our manuscript. Thank you for your time and consideration.\\\n[1] Elucidating the design space of diffusion-based generative models, Karras et al."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621222571,
                "cdate": 1700621222571,
                "tmdate": 1700622366445,
                "mdate": 1700622366445,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zMptVPn8pO",
                "forum": "XasWgF5WsZ",
                "replyto": "0EW580Jr0c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to your valuable comments"
                    },
                    "comment": {
                        "value": "We extend our sincere appreciation for your valuable insights. We address each query in detail as outlined below. \n\n## **Q4\uff1aWhy Eq.(9) is linear in the narrow sense?**\n\nThanks for your question! Although $x_\\theta$ is a nonlinear neural network, it is pre-trained during the forward process. **Eq.(9) represents a stochastic differential equation with the data state $x_t$ in the reverse process as the variable**. According to the discussion in Section 4.2 of [2] regarding Eq.(2.3), we refer to Eq.(9) as linear in the narrow sense.\n\nWe sincerely hope that our responses align with your expectations, and we would be extremely grateful if these clarifications contribute positively to the assessment of our manuscript. Thank you for your time and consideration.\n\n[2] Numerical Solution of Stochastic Differential Equations, Peter E Kloeden and Eckhard Platen."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621406889,
                "cdate": 1700621406889,
                "tmdate": 1700622481274,
                "mdate": 1700622481274,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d5kMz6Uu4s",
                "forum": "XasWgF5WsZ",
                "replyto": "zMptVPn8pO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission613/Reviewer_RiM8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission613/Reviewer_RiM8"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response but I disagree. The linear SDE in the narrow sense defined in Section 4.2 of [2] is \n$$\\mathrm{d}X_t = \\left(a_1(t)X_t+a_2(t)\\right)\\mathrm{d}t + b_2(t)\\mathrm{d}W_t. $$\nHowever, the equation (9) is \n$$\\mathrm{d}X_\\sigma = a_1(\\sigma)\\left(X_\\sigma - X_{\\theta}(X_\\sigma)\\right)\\mathrm{d}\\sigma+b_2(\\sigma)\\mathrm{d}W_\\sigma, $$\nwhich is clearly nonlinear as the function $X_{\\theta}(X_\\sigma)$ is a nonlinear function of $X_{\\sigma}$. The nonlinearity of $X_{\\theta}$ has nothing to do with whether it is pre-trained. \n\n[2] Numerical Solution of Stochastic Differential Equations, Peter E Kloeden and Eckhard Platen."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638967122,
                "cdate": 1700638967122,
                "tmdate": 1700638967122,
                "mdate": 1700638967122,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1V4TJjFJE7",
                "forum": "XasWgF5WsZ",
                "replyto": "0EW580Jr0c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission613/Reviewer_RiM8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission613/Reviewer_RiM8"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed reply. \n\nR1: Could you provide a precise mathematical definition of the discretization error discussed in the paper and clearly show how FEI controls the discretization error? The current response seems to repeat what you wrote in the paper. \n\nR3: Thanks for referring to the Fig.6-Fig.10. However, these are just some generated samples. A more quantitative comparison of sample diversity is needed if the focus is to enhance diversity. For example, report Recall on ImageNet256 and compare with existing methods like EDM, DPM solvers. \nRight now, this is not strongly supported by the experiments. For example, in Table 13, ER-SDE-Solver has worse Recall than the deterministic DPM-solver, which is an important metric for measuring sample diversity."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640094122,
                "cdate": 1700640094122,
                "tmdate": 1700640141268,
                "mdate": 1700640141268,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JPE4tjU3mQ",
                "forum": "XasWgF5WsZ",
                "replyto": "0EW580Jr0c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to your valuable comments"
                    },
                    "comment": {
                        "value": "We extend our sincere appreciation for your valuable insights. We address your query in detail as outlined below. \n## **Q: The diversity of generated images may not be strongly supported by the experiments?**\nThanks for your insightful question! We suppose that the **Recall metric may not adequately reflect the diversity of generated images when the Number of Function Evaluations (NFE) is small**. For instance, when employing DDIM ($\\eta=1$)(Stochastic Sampler), the Recall is lower than that of DDIM (Deterministic Sampler), and there exists a significant gap between them. However, when NFE<=20, the Recall of the ER-SDE-Solver is slightly lower than that of the DPM-Solver by 0.01-0.02. **This demonstrates that our approach indeed enhances the diversity of generated images while ensuring efficient sampling performance**. This may be attributed to the lower quality of images when NFE is low, which could potentially impact the Recall metric.\n\n\nAdditionally, the pre-trained model employed in Table 13 is Guided-diffusion[1]. According to the results reported in [1], when NFE is around 250, Guided-diffusion achieves its best Recall metric at only 0.65 (Table 5 in [1]). In contrast, the ER-SDE-Solver already reaches a Recall of 0.66 at NFE=30. **Due to limitations in the model's fitting capacity, even with further increases in NFE, it is challenging to achieve significant improvements in Recall**.\n\nWe sincerely hope that our responses align with your expectations, and we would be extremely grateful if these clarifications contribute positively to the assessment of our manuscript. Thank you for your time and consideration.\n\n[1] Diffusion models beat gans on image synthesis, Dhariwal P, Nichol A."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662854345,
                "cdate": 1700662854345,
                "tmdate": 1700662974029,
                "mdate": 1700662974029,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tZxZJ77fgC",
                "forum": "XasWgF5WsZ",
                "replyto": "VupDpzRTKu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission613/Reviewer_RiM8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission613/Reviewer_RiM8"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for providing the detailed derivation. However, the third equality is incorrect because $x_0$ is a function of $\\sigma$. You cannot treat it as a constant to compute the integral. \n$$\\phi(\\sigma_t)\\int_{\\phi_t}^{\\phi_s} \\frac{\\phi^{(1)}(\\sigma)}{\\phi^2(\\sigma)}x_0(\\sigma)\\mathrm{d}\\sigma \\neq \\left. \\phi(\\sigma_t)\\left(- \\frac{1}{\\phi(\\sigma)}\\right)\\right|_{\\sigma_t}^{\\sigma_s}x_0(\\sigma?). $$"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679498703,
                "cdate": 1700679498703,
                "tmdate": 1700679498703,
                "mdate": 1700679498703,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QsXzd1uGZ7",
                "forum": "XasWgF5WsZ",
                "replyto": "JPE4tjU3mQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission613/Reviewer_RiM8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission613/Reviewer_RiM8"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I agree that the Recall metric is not the most reliable way to measure diversity. It would be beneficial if the paper provided additional quantitative methods to measure sample diversity and demonstrate the superiority of the ER SDE solver. \nHowever, this is not presented in the paper so far. The main concern is that the paper lacks significant evidence to support the enhanced diversity of the ER SDE solver, both theoretically and empirically. The Recall metric is the only metric provided to measure diversity, and while FID can measure diversity to some extent, it mainly focuses on sample quality. The deterministic sample DPM-solver outperforms the ER-SDE-solver over this metric, raising doubts about whether the ER-SDE solver actually provides much better sample diversity."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680319063,
                "cdate": 1700680319063,
                "tmdate": 1700680319063,
                "mdate": 1700680319063,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jjCTJFWDJO",
                "forum": "XasWgF5WsZ",
                "replyto": "0EW580Jr0c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission613/Reviewer_RiM8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission613/Reviewer_RiM8"
                ],
                "content": {
                    "comment": {
                        "value": "## Concern 1:\nI'm not sure how you apply the Taylor expansion here. But I feel the second equation should be \n$$x_t = \\frac{\\phi(\\sigma_t)}{\\phi(\\sigma_s)}+\\phi(\\sigma_t) \\frac{\\phi^{(1)}(\\sigma_s)}{\\phi^2(\\sigma_s)}x_0(x_{\\sigma_s}, \\sigma_s)\\int_{\\sigma_t}^{\\sigma_s}\\mathrm{d}\\sigma + ...$$\nCan you provide intermediate steps to derive that? \n\nPutting this derivation aside, suppose your derivation is correct. If I'm not mistaken, you define the $x_0$ to be the data state (some form of score function) of the actual training data distribution.\nThe error you defined is the approximation error that arises from the neural network rather than the discretization error. The discretization error is from discretizing a continuous function, which has nothing to do with the accuracy of the neural network. Controlling the approximation error is not a meaningful goal for the sampling problem of the pre-trained diffusion model. The goal here is sampling from the distribution defined by the trained diffusion model rather than sampling from the training data distribution. \n\n## Concern 2\nThank you for explaining your motivation in detail. However, I find the paper confusing now, as I'm not sure about its key message. Here are my observations:\n \n1. In section 4.1, the paper claims that the ODE sampler is superior to the stochastic sampler for rapid sampling based on its error analysis.\n \n2. In contrast, the authors propose a specialized stochastic sampler, ER-SDE-solver, for rapid sampling, which seems contradictory to the previous claim. The motivation behind this proposal is not clear.\n \n3. On page 2, the last sentence of the first paragraph suggests that stochastic samplers are more promising for larger sampling steps. However, the paper focuses on rapid sampling that uses small steps. During the rebuttal, the authors stated that their focus is on rapid sampling rather than achieving the highest quality samples. However, they also claim that the ODE sampler is better than the stochastic sampler in the rapid-sampling region. This raises my question of why not just use an ODE sampler instead of proposing a new stochastic sampler.\n \n4. In response to this question, the authors claim that SDE-based stochastic samplers have the ability to generate diverse images by injecting additional noise into the data state at each sampling step. However, this claim is not well-supported by either empirical or theoretical results. Also, the proposed stochastic solver doesn't even outperform the deterministic DPM-solver on the only metric report in the paper for measuring diversity. \n\nI think this paper might need major revisions to address these concerns. I'll change the rating to 3."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719946394,
                "cdate": 1700719946394,
                "tmdate": 1700720327624,
                "mdate": 1700720327624,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nhKs6I0tha",
            "forum": "XasWgF5WsZ",
            "replyto": "XasWgF5WsZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission613/Reviewer_yrnV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission613/Reviewer_yrnV"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a generalized SDE framework called extended reverse-time SDE (ER-SDE) and the solver (ER-SDE-Solver) involved with this generalized SDE formulation. And this paper provides the formulation of the sampling process using the ER-SDE, providing the exact (integral) solution and approximate (linear, or higher-order) solution in both VP/VE cases, which can be generalized to all widely used SDEs. And this paper provides insights on the reasons on why ODE solvers show superior performance in terms of fast sampling. Finally, they validate the image generation performance with ImageNet64 dataset and CIFAR-10 datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Even though existing works generalized the SDE and its equivalent ODE (i.e., yielding the same solution of the Fokker-Planck equation), this paper dealt with the ratio with \"how the solver should work as an SDE solver or ODE solver\" with dynamically varying rate with respect to time (=sigma, SNR). And by some designing of this new time-dependent variable, this paper showed that some of the new SDE design choices (such as ER-SDE-5) shows superior performance compared to existing methods.\n* The writing is concrete, and the additional experiments in the appendix resolved some of my questions (large-scale image datasets, or some ER-SDE ablations.)"
                },
                "weaknesses": {
                    "value": "* The design of phi(x) is one of the keys of this paper that distinguishes this to other existing works, but this is not interpreted enough.\n* The necessity of the FEI coefficient is vague. Making the FEI coefficient as small as possible is equivalent to directly removing all noise, i.e., h(t)=0. And the trivial question arises; Why not just directly use the ODE solver and the Taylor-approximation-based higher-order sampler?"
                },
                "questions": {
                    "value": "* I am not fully understanding the motivation part; why does the low FEI coefficient lead to high sampling performance in low-NFE regime?\n* The Figure 3 shows that the FID score is always the best when we use the ODE solver. Then, what is the advantage of the stochastic solver compared to the deterministic solver? And to the best of our knowledge, the FID score is lower (=better) with the stochastic sampler in the high-NFE regime. Even though the dynamically varying phi(x) looks sound, there is not enough evidence of the design of phi(x). Specifially, it will be better if there is some reasoning with the superior performance of ER-SDE-5, compared to other designs.\n\n* What is the phi(x) of ER-SDEs used for experiments in ER-SDE-Solvers of ImageNet64?\n* Could you compare your method to other sampling methods, such as PNDM and DEIS?\n* What does the 'step' in Figure 3 stand for, in both FEI coefficients and FID scores cases? It seems that the steps stand for the sampling step within the whole 200 steps of the reverse process, and the number of function evaluations (NFE) for FID scores.\n\n* In my opinion, some of the large-scale results in the appendix better explain the benefits of using this ER-SDE-Solver than small-scale results. I recommend aligning some of these results to the main material."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission613/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission613/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission613/Reviewer_yrnV"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission613/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836230598,
            "cdate": 1698836230598,
            "tmdate": 1699635988822,
            "mdate": 1699635988822,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NAF0P5sMpy",
                "forum": "XasWgF5WsZ",
                "replyto": "nhKs6I0tha",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to your valuable comments"
                    },
                    "comment": {
                        "value": "We extend our sincere appreciation to Reviewer yrnV for the thoughtful consideration of our manuscript and the valuable insights provided. We address each query in detail as outlined below. \n## **Q1\uff1aWhy does the low FEI coefficient lead to high sampling performance in low-NFE regime?**\nThank you for your question! In our paper, Sec.3 demonstrates that the exact solution of the ER SDE comprises three components: a linear function of the data variables, a non-linear function parameterized by neural networks and a noise term. The linear and noise terms can be precisely computed, while discretization errors are present in the non-linear term. Due to the decreasing error as the order increases (see Table1), the first-order error predominantly influences the overall error. Therefore, we exemplify the case with order k = 1 for error analysis.\nIt can be observed from Eq.(16) and Eq. (17) that the error arises from the second term \n$\\Big{[}1-\\frac{\\phi(\\sigma_{t_{i}})}{\\phi(\\sigma_{t_{i-1}})}\\Big{]} x_\\theta(\\tilde{x} _ {\\sigma_{t_{i-1}}},\\sigma_{t_{i-1}})$, as $x_\\theta(\\tilde{x} _ {\\sigma_{t_{i-1}}}, \\sigma_{t_{i-1}})$ is a neural network trained in the forward process to estimate data, whose precision is fixed. To reduce discretization error, it is necessary to minimize the coefficient $1 - \\frac{\\phi(x_t)}{\\phi(x_s)}$, referring as the First-order Euler Integral (FEI) coefficient. **Therefore, when using the same model weights, a lower FEI coefficient leads to smaller discretization error. In turn, the images generated in the reverse process more similar to real images in the forward process, resulting in higher image quality and a smaller FID.**\\\nIn summary, our ER-SDE-Solvers directly utilize raw information without retraining, offering broad applicability and high flexibility. We hope our explanation addresses your concerns.\n## **Q2\uff1aWhat is the advantage of the stochastic solver compared to the deterministic solver?**\nThanks for your insightful question! \u00a0 \n1. **Stochastic samplers can produce higher-quality images with a minimal increase in NFE**(refer to Table 2, Table 9). \n2. **stochastic samplers with classifier guidance exhibit superior performance in rapid sampling compared to deterministic samplers.** We appreciate your suggestion and have incorporated this result into the main material to further substantiate the benefits of ER SDE 5. \n3. SDE-based stochastic samplers exhibit an enhanced capability to generate variable images by injecting additional noise into the data state at each sampling step. This conclusion is further supported by Fig.6-Fig.10 in Appendix D, where stochastic samplers demonstrate a greater variability in generated images when the random seed is fixed.\n## **Q3\uff1aWhat is $\\phi(x)$ used for experiments in ER-SDE-Solvers on ImageNet64?**\n$\\phi(x)$ used for experiments in ER-SDE-Solvers on ImageNet64 is $\u03c6(x) = x(e^{x^{0.3}} + 10)$\uff08ER SDE 5\uff09. As outlined in Sec.4.2, we select ER SDE 5 as the noise scale function by default if there is no special indication. We hope our clarification addresses your concerns.\n## **Q4\uff1aCompare ER-SDE-Solvers to PNDM and DEIS**\nThank you for your suggestions! One of the contributions of our paper is to yield mathematical insights elucidating the superior performance of ODE solvers over SDE solvers in terms of fast sampling. Building upon it, we design a stochastic sampler with rapid sampling capabilities comparable to ODEs. To demonstrate the effectiveness of our sampler, we select only a subset of representative ODE-based samplers for comparison due to space constraints.\\\nPNDM and DEIS are also ODE-based samplers with superior performance. Following your suggestion, we compare our method with them in Table 10. It can be observed that ODE-based samplers demonstrate superiority in rapid sampling, which further validates our standpoint.\n## **Q5\uff1aWhat does the 'step' in Figure 3 stand for, in both FEI coefficients and FID scores cases?**\nYour opinion is valid. The steps stand for the sampling step within the whole 200 steps of the reverse process, and the number of function evaluations (NFE) for FID scores. Thank you for your thorough review!\n## **Q6\uff1aAlign some of the large-scale results to the main material**\nAppreciate your suggestion! Stochastic samplers guided by classifiers demonstrate superior performance in rapid sampling of high-resolution images compared to deterministic samplers. We have incorporated this result into the main material (Table 3) to further substantiate the benefits of ER SDE 5. \n\n\nWe sincerely hope that our responses align with your expectations, and we would be extremely grateful if these clarifications contribute positively to the assessment of our manuscript."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699885760832,
                "cdate": 1699885760832,
                "tmdate": 1700707496936,
                "mdate": 1700707496936,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mLvQuV6yse",
                "forum": "XasWgF5WsZ",
                "replyto": "nhKs6I0tha",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely looking forward to the further discussions"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe sincerely hope that our recent response and revisions have successfully addressed your concerns. Your feedback is invaluable to us, and if you find that our efforts have met your expectations, we would be immensely grateful if you could reconsider and possibly elevate the score for our work.\n\nIf you have any additional questions or suggestions, we would be happy to have further discussions.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634853854,
                "cdate": 1700634853854,
                "tmdate": 1700637855603,
                "mdate": 1700637855603,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]