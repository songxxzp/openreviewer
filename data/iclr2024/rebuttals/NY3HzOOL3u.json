[
    {
        "title": "Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks"
    },
    {
        "review": {
            "id": "cgRCpLz0Nz",
            "forum": "NY3HzOOL3u",
            "replyto": "NY3HzOOL3u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6973/Reviewer_drhA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6973/Reviewer_drhA"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of skill learning and planning for solving long-horizon tasks. The proposed method first learns basic skills via RL with intrinsic rewards, and leverages LLM to construct a skill dependency graph, which is then used to search the skill sequence for completing the task. Experiments are performed on Minecraft tasks to validate the idea."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed pipeline that leverages skill reusing for solving long-horizon tasks is straightforward and reasonable;  \n\nThe method achieves good results from the evaluation perspective;  \n\nThe paper is well-organized and easily read."
                },
                "weaknesses": {
                    "value": "The method relies on pre-defined basic skills, which need human understanding of the task decomposition;  \n\nLeveraging pre-trained LLM for generating skill relationships seems not very reliable, the qualities of the skill dependency graph highly depend on the quality of the prompts coming from the user definition. \n\nOnly a few baselines are compared in the experiments."
                },
                "questions": {
                    "value": "How to ensure the completeness of the pre-defined skill set? \n\nHow many human interventions are needed to generate skill dependencies during preprocessing? And how to ensure the LLM-generated skill graph is correct? \n\nSince this work also needs expert knowledge while providing the prompts for invoking LLM, it would be interesting to see how the proposed method compares to skill planning baselines that utilize symbolic reasoning for planning [1][2][3].  \n\nSome relevant references, please consider discussing and citing:  \n[1] LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon Manipulation, RA-L 2023; \n[2] Symbolic Plans as High-Level Instructions for Reinforcement Learning, ICAPS 2020; \n[3] PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement Learning for Robust Decision-Making, IJCAI 2018."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6973/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6973/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6973/Reviewer_drhA"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6973/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698007066564,
            "cdate": 1698007066564,
            "tmdate": 1699636815322,
            "mdate": 1699636815322,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "Sw245cwmPf",
            "forum": "NY3HzOOL3u",
            "replyto": "NY3HzOOL3u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6973/Reviewer_Dk5Z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6973/Reviewer_Dk5Z"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to accomplish long-horizon tasks in the Minecraft environment. Instead of trying to learn the task directly with RL algorithms, this paper decomposes the long-horizon tasks into atomic subtasks. It then uses a planning module to execute the atomic subtasks sequentially.\n\nThis paper indicates that learning the subtasks with vanilla RL could be hard and sample inefficient. To overcome this problem, they use a semi-rule-based Finding-skill to locate the agent close to the target objects/mobs, which makes subtasks much easier to accomplish. Additionally, the paper uses intrinsic rewards generated from a pretrained MineCLIP model.\n\nAfter learning the atomic subtasks, the proposed model uses a depth-first search on the subtask dependency graph generated by LLMs to determine the sequence of sub-tasks to be executed.\n\nExperiments demonstrate improved success rates on 40 Minecraft tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The main merit of this paper is the decomposition of long-horizon tasks into atomic subtasks, which are much easier and more sample-efficient to learn compared to the original task. Although the idea of task decomposition is not novel in the Minecraft environment, this paper uses a novel way to perform the decomposition via its \"Finding-skill\"."
                },
                "weaknesses": {
                    "value": "My main concern with the proposed Plan4MC model is its generalizability to other environments. The model heavily relies on the Finding-skill that navigates the agent to initial positions that make it relatively easy to accomplish atomic subtasks. Specifically, the Finding-skill separates the Minecraft terrain into grids, and uses a learned policy to navigate the agent to different grids. Within each grid, the agent uses lidar information to move itself close to the target mob. The lidar information is not feasible for humans when playing Minecraft, and is not used in many other related works. Although the authors claim that they can easily replace the lidar with a CV model, this contradicts a motivation of the paper that claims Minecraft data is hard to acquire, since the CV model would need to train on a large amount of Minecraft data.\n\nEmpirically, although ablation studies show that the Finding-skill helps improve the overall success rate of Plan4MC, it would be nice for the authors to compare it with other models such as Dreamer V3 and STEVE-1 [1], which have demonstrated very good performance on most tasks reported in the paper including cut tree and mine stone.\n\nAnother main concern is that, although the paper argues that Minecraft data is hard to acquire, it would be nice to develop sample-efficient RL approaches, the proposed method also requires extensive Minecraft data. Specifically, when training the atomic subtasks, intrinsic rewards from MineCLIP, a CLIP model trained with Minecraft videos paired with captions, are used. Therefore, Plan4MC actually \"uses\" lots of Minecraft demonstration videos under the hood.\n\nThe proposed model uses LLMs to acquire task-related knowledge. However, it is unclear from the paper its relation between models such as Voyager, which uses LLMs to extract knowledge in Minecraft for lifelong learning.\n\n\n[1] Lifshitz, Shalev, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila McIlraith. \"STEVE-1: A Generative Model for Text-to-Behavior in Minecraft.\" arXiv preprint arXiv:2306.00937 (2023).\n\n[2] Wang, Guanzhi, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. \"Voyager: An open-ended embodied agent with large language models.\" arXiv preprint arXiv:2305.16291 (2023)."
                },
                "questions": {
                    "value": "Is it possible that the Finding-skill can be made more general for other domains?\n\nHow does the proposed atomic subtask model compare to other methods such as STEVE-1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6973/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698573623680,
            "cdate": 1698573623680,
            "tmdate": 1699636815203,
            "mdate": 1699636815203,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "VU3UM7xYTu",
            "forum": "NY3HzOOL3u",
            "replyto": "NY3HzOOL3u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6973/Reviewer_ALHp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6973/Reviewer_ALHp"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on creating a generalized agent capable of addressing multiple long-horizon tasks in open-world settings, specifically in the Minecraft game environment. \n\nTo do so, the authors employ a hierarchical decision-making approach which integrates two main schemes: (1) the LLM-based high-level skill sequence planning scheme that exploits the prior knowledge of LLM and a skill dependency graph, and (2) the fine-grained basic skill learning scheme including Finding-skill that provides a common navigation technique in the Minecraft\u2019s open-world settings. \n\nThese schemes are implemented as a single framework, Plan4MC, and tested across 40 different Minecraft tasks. These skill planning and learning schemes are designed specifically for the long-horizon open-world Minecraft tasks and they can be seen as the main contributions of the paper. This paper also includes the performance evaluation and ablations of the schemes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Within the Minecraft game context, the Plan4MC scheme of learning fine-grained basic skills including the general Find-skill seems promising for improved open-world exploration. \n\nRegarding the performance, Plan4MC outperforms the MineAgent (in MineDojo) baseline and the Plan4MC without Finding-skill approach for learning basic skills. It also provides robust performance over the interactive LLM-based skill planning."
                },
                "weaknesses": {
                    "value": "The paper primarily demonstrates the Plan4MC capabilities within the Minecraft environment, offering limited insight into the broader applicability of the two key schemes: LLM-based skill planning with a skill graph and the general Finding-Skill exploration. While Minecraft indeed offers a diverse range of long-horizon tasks in open-world settings, Plan4MC does not sufficiently establish that the two schemes can be generalized beyond this specific context. \n\nThe two schemes appear tailored for the Minecraft environment settings (see the questions); thus, it would enhance the paper's contributions if it included experiments and analyses that demonstrate the broader applicability of these proposed schemes beyond the Minecraft context. If conducting experiments outside of Minecraft is considered too labor-intensive, a more in-depth discussion on potential generalization, beyond what's presented in Appendix I, would be beneficial."
                },
                "questions": {
                    "value": "When formulating and implementing the Finding-skill to provide a good initialization of other skills, it is assumed that target items are uniformly distributed across the world surface. Is this assumption valid in general open-world environments? \nIf the distribution of target items is influenced by the surrounding environment (i.e., not uniformly distributed), wouldn't the proposed RL (based on a 10x10 grid and state counting exploration) be inefficient? It seems that leveraging intrinsic reward techniques [1, 2, 3] might be more effective for efficient exploration. \nCould the author clarify the contribution and generalization of the Find-skill not just for the Minecraft environment but for general open-world tasks?\n\n[1] Burda, Yuri, et al. \"Exploration by random network distillation.\" arXiv preprint arXiv:1810.12894 (2018). \n\n[2] Badia, Adri\u00e0 Puigdom\u00e8nech, et al. \"Never give up: Learning directed exploration strategies.\" arXiv preprint arXiv:2002.06038 (2020).\n\n[3] Seo, Younggyo, et al. \"State entropy maximization with random encoders for efficient exploration.\" International Conference on Machine Learning. PMLR, 2021.\n\nThe paper leverages ChatGPT\u2019s prior knowledge to construct a skill graph tailored for Minecraft tasks. In scenarios where the target environment is not familiar to the LLM training data (unlike Minecraft), how can this LLM-based skill plannings scheme be adopted? \nCould the authors clarify the originality and superiority of Plan4MC\u2019s LLM-based skill graph generation, compared to similar techniques such as [4, 5, 6] where LLM is leveraged for high-level skill planning. \n\n[4] Ahn, Michael, et al. \"Do as i can, not as i say: Grounding language in robotic affordances.\" arXiv preprint arXiv:2204.01691 (2022).\n\n[5] Huang, Wenlong, et al. \"Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.\" International Conference on Machine Learning. PMLR, 2022. \n\n[6] Song, Chan Hee, et al. \"Llm-planner: Few-shot grounded planning for embodied agents with large language models.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\nMinor typo: on Page 23, in Table 9. stone sward, stone axe, iron trap door, skill icon are in wrong images."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6973/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6973/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6973/Reviewer_ALHp"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6973/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754979989,
            "cdate": 1698754979989,
            "tmdate": 1699636815065,
            "mdate": 1699636815065,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "Yb6rzgKW8v",
            "forum": "NY3HzOOL3u",
            "replyto": "NY3HzOOL3u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6973/Reviewer_V7Pc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6973/Reviewer_V7Pc"
            ],
            "content": {
                "summary": {
                    "value": "The authors use hierarchical policies to solve tasks in the Minecraft game. Minecraft is a huge game and therefore has large exploration bottlenecks. To tackle this, they break tasks down into subtasks using ChatGPT and then construct a DAG representing the graph over skills. When presented with a task, they search over the DAG using DFS to construct a plan. As the agent\u2019s state changes, they research the DAG to construct a new plan and new next goal.\n\nThey pretrain a \u201cfinding\u201d policy trained with state-count based exploration for efficient navigation. The finding policy is a combination of a high level policy that emits a goal for a fixed, pretrained low level policy. They then train a policy to do manipulation and crafting tasks, which can use the finding policy to locate relevant blocks or mobs.\n\nThey find their method outperforms other methods at similar levels of sample efficiency. They find they can even construct iron tools."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Using LLMs to construct a DAG is interesting and original\n2. Their method seems fairly sample efficient, and it seems promising to extract domain specific knowledge from LLMs for the purpose of planning."
                },
                "weaknesses": {
                    "value": "1. The method seems very cobbled together. It\u2019s hard to tell what would generalize to other environments as there is so much environment-specific information used. For example: choices like using DQN to train a low level policy but then PPO to train the high level policy is odd and unmotivated. \n2. Paper organization and writing seems very poor to me. Only after 1/2 to 2/3 through the paper did I understand (though I\u2019m still unclear on many details) most of the technical details. I think the authors could spend a lot of time revamping their abstract, introduction, method diagram, etc. to be more clear.\n3. I found the language used in the abstract and the introduction hard to follow. The authors go into some detail, but not enough to fully understand, but too much such that I ended up confused at the end of those sections. I would recommend either (a) keeping the method more high level in these sections or (b) describing the method in enough detail so that one isn\u2019t left confused. \n4. In abstract, unclear what \u201cpropose a Finding-skill to improve the sample efficiency for training all the skills\u201d means. It\u2019s still unclear when mentioned in the Introduction.\n5. There were many mentions of \u201cskill search algorithm\u201d without just saying it's DFS\n6. There is a long history of hierarchical RL. I recommend the authors include discussion of related work.\n7. It\u2019s unclear to me that you could easily pre-generate a DAG in more real settings. It would be nice to have some discussion around this."
                },
                "questions": {
                    "value": "- It was unclear to me in section 3.1 what the goals of the high level finding policy are. What do you mean you decompose each 10x10 area into grids?\n- Section 3.2. What is the goal distribution?\n- Did the LLM skill graph do anything you couldn\u2019t have done just using structured knowledge of the skill tree in minecraft?\n- For the Interactive LLM planner and other ablations, do you include the finding skill?\n- What FPS does the game operate at?\n- Do you use automated craft actions or use the game interface?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6973/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812781229,
            "cdate": 1698812781229,
            "tmdate": 1699636814929,
            "mdate": 1699636814929,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]