[
    {
        "title": "Detection-Oriented Image-Text Pretraining for Open-Vocabulary Detection"
    },
    {
        "review": {
            "id": "FEXRgDsRQk",
            "forum": "wHlYlC2TCM",
            "replyto": "wHlYlC2TCM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission82/Reviewer_aquR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission82/Reviewer_aquR"
            ],
            "content": {
                "summary": {
                    "value": "This paper targets detection-oriented image-text pertaining to the open-vocabulary detection task. The authors employ a detector architecture to facilitate contrastive learning from noisy, large-scale image-text pairs. Additionally, they introduce the Shifted-Window Learning technique to enhance detection performance and generate more robust, translation-invariant representations using pretrained vision transformers. Extensive experiments demonstrate the effectiveness of these methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to follow.\n- The motivation of the method is clear and the proposed strategy is straightforward yet effective."
                },
                "weaknesses": {
                    "value": "My primary concern pertains to the novelty of the proposed method.  Firstly, the detection-oriented image-text pertaining has been explored extensively since RegionCLIP[1]. Subsequently, there have been endeavors to employ randomly selected proposals for augmentation [2] and align multiple regions [3]. Given the foundation laid by the aforementioned works, the contrastive pretraining methodology introduced in this article, employing a detector architecture, may appear to lack a sufficient level of novelty.\nSecondly,  I think the shifted-window learning technique is not enough to support the second technical contribution. Overall, I agree with the practical values of this paper, but more scientific values from a paper in ICLR are expected.\n\n[1] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, and Jianfeng Gao. Regionclip: Region-based languageimage pretraining. In CVPR, 2022.\n\n\n[2] Wei, Fangyun, et al. \"Aligning pretraining for detection via object-level contrastive learning.\" Advances in Neural Information Processing Systems 34 (2021): 22682-22694.\n\n\n[3] Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and Chen Change Loy. Aligning bag of regions for open-vocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15254\u201315264, 2023a."
                },
                "questions": {
                    "value": "Why didn\u2019t the author compare the performance of Grounding-DINO? It has already had greater influence and zero-shot performance in the open-vocabulary detection field."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission82/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698647054731,
            "cdate": 1698647054731,
            "tmdate": 1699635932971,
            "mdate": 1699635932971,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "W6Z427OuKh",
            "forum": "wHlYlC2TCM",
            "replyto": "wHlYlC2TCM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission82/Reviewer_tAz1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission82/Reviewer_tAz1"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a detection oriented image-text pre-training to bridge the gap between image-level pre-training and open-vocabulary object detection.\nIt utilises a simple extension of the contrastive learning method to learn region-level semantic cues.\nIt also proposes a shifted-window learning approach to improve window attention to make the backbone representation more robust. \nFurther experiments show good performance on open-vocabulary object detection task on COCO and LVIS."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper is easy to follow.\n+ The method demonstrates good performance on rare categories."
                },
                "weaknesses": {
                    "value": "- The proposed contrastive pre-training method is not novel. Incorporate region information in contrastive learning has been explored in many works [1][2][3].\n- Using a private data on pre-training is not fair and not convincing. \n- The baseline method is already better than previous works. It makes the effectiveness of proposed components questionable.\n- Figure 2 is misleading. The similarity map using the backbone features in the middle should be comparing with CLIP based method, not your own.\n- In Table 6 (c), stride 4 clearly performs better. The paper doesn't explain why it chooses a suboptimal one.\n- Since the method also involves external box annotations, it should compare with previous methods on pure zero-shot OVD benchmark.\n\n[1] RegionCLIP: Region-based Language-Image Pretraining. CVPR 2022\n\n[2] Aligning Pretraining for Detection via Object-Level Contrastive Learning. NIPS 2021\n\n[3] Point-Level Region Contrast for Object Detection Pre-Training. CVPR 2022"
                },
                "questions": {
                    "value": "Consider to re-run experiments using public dataset such as CC3M and LAION."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper uses a private dataset to pre-train the model. It is unclear that whether the private data has overlaps with testing benchmarks."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission82/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824855724,
            "cdate": 1698824855724,
            "tmdate": 1699635932881,
            "mdate": 1699635932881,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "Q6RO939rgb",
            "forum": "wHlYlC2TCM",
            "replyto": "wHlYlC2TCM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission82/Reviewer_Gqbm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission82/Reviewer_Gqbm"
            ],
            "content": {
                "summary": {
                    "value": "This submission proposes DITO, a visual-linguistic (VL) pretraining method well \naligned for downstream detection tasks.\nCLIP uses global embeddings of images for VL contrastive learning, and thus, it may not be the best choice for localized representation learning and downstream detection.\nHere, DITO conducts VL pretraining with a detector-like architecture equipped with\na feature pyramid and detection heads. \nDuring pretraining, the detector heads are trained with ROI features extracted from random regions\ndue to the lack of bounding box annotations.\nExperiments in LVIS OVD show that DITO outperforms previous OVD methods with a margin."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- VL pretraining for detection seems a valuable direction of research and\nto my knowledge, this work is one of the first attempts along this direction.\n\n- The overall frame is kept simple without using extra annotation or region proposals, \nwhich may inspire the following researches for extensions or applications for new tasks.\n\n- Clear SOTA performance in the large-scale LVIS OVD benchmark."
                },
                "weaknesses": {
                    "value": "- Justification of pretraining with random boxes: I am concerned that the random boxes may produce noisy supervisory signals.\nFor example, multi-object captions like \"a bird and a dog\"  may be assigned both for bird regions and dog regions.\nI think that some discussions around here are needed.\nDid the Authors test other choices here, for example, using regular grids?\n\n\n- Basic comparisons with CLIP: DITO looks like a VL pretraining method, in essence, rather than an OVD method,\nbecause a significant part of the proposed modifications are in the pretraining phase.\nIt would be natural to compare it with CLIP in the tasks that it was designed for, \nfor example, text-sentence matching or zero-shot classification.\nOr is it impossible due to architectural changes?"
                },
                "questions": {
                    "value": "Please see Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission82/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission82/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission82/Reviewer_Gqbm"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission82/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830179096,
            "cdate": 1698830179096,
            "tmdate": 1699635932783,
            "mdate": 1699635932783,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "bxjnijrmoD",
            "forum": "wHlYlC2TCM",
            "replyto": "wHlYlC2TCM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission82/Reviewer_rKe8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission82/Reviewer_rKe8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an open vocabulary object detection framework for detection pre-training. \n\n* Unlike previous work using image-level pre-trained VLMs, Dito is based on proposal-level semantic alignment during pre-training. \n\n* Dito consists of three stages. The first stage is image-level alignment based on the Align datasets. The second stage is detection pre-training(or region-semantic pre-training) on this image-text dataset.  The final stage is detection fine-tuning on the base detection dataset. \n\n* The proposed method achieves state-of-the-art performance on the COCO & LVIS dataset benchmark."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall, the performance of the proposed framework is very good, and the motivation is reasonable. \n\n* I also believe that aligning semantics at the region-level is very important and meaningful. Now many methods are exploring how to achieve region-level pre-training. The core point of open-vocabulary lies in region-language alignment, thereby generalizing to novel classes. \n\n* Due to the framework being based on detection pre-training, it eliminates the need for distillation or pseudo-labeling, which is a strength of this work.\n\n* The comparison experiments are very sufficient, and the proposed method achieves state-of-the-art results on various benchmarks."
                },
                "weaknesses": {
                    "value": "I have several points that I find confusing in this article: \n\n* 1. The entire framework relies on two backbones during inference. The article mentions (there is a tendency to lose the pretrained image-text knowledge, Inspired by previous work (Kim et al., 2023a), we use a separate frozen ViT backbone as an open-vocabulary region classi\ufb01er at inference time to compute the VLM score z). This indicates that the entire framework does not maintain semantic information during detection pre-training, which seems strange to me and makes the framework appear bulky. If the image-text semantics could be preserved during detection pre-training, this would make the work more reasonable. \n\n* 2. This method uses the aligned dataset, while previous methods follow the clip pretrain setting, making it difficult to compare this method with previous methods. It gives me the feeling that the good results are due to the large amount of data pre-training, rather than Detection-Oriented Pretraining.\n\n* 3. The design of shifted-window learning is strange. It is more of a general network design rather than a solution for open vocabulary detection. Furthermore, shifted-window and open vocabulary seem disconnected."
                },
                "questions": {
                    "value": "* In Table 1, the performance of DITO(ViT-L/16) : mask APr : 40.4, mask AP: 37.7, why the overall mask AP is lower than mask APr, it is really strange here, normally the mask AP(F & C) will be higher than mask APr.\n\n* Overall, the motivation of this paper is reasonable, but it could be more concise. Additionally, the use of a large amount of other data makes it difficult to compare with other works (of course, open vocabulary does require a large amount of image-text data for training). However, I still believe that the experimental setting of this framework is not very clear."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission82/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699104377212,
            "cdate": 1699104377212,
            "tmdate": 1699635932676,
            "mdate": 1699635932676,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]