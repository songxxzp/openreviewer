[
    {
        "title": "Duality of Information Flow: Insights in Graphical Models and Neural Networks"
    },
    {
        "review": {
            "id": "O19ynR8856",
            "forum": "3husFxdHI1",
            "replyto": "3husFxdHI1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7497/Reviewer_Ac7q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7497/Reviewer_Ac7q"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the relationship between the graphical models and the neural networks. The authors first indicate the equivalence between the belief propagation and the back propagation. Then the duality between the Bayesian neural networks and the graphical models are specified via the relationship between Langevin and the Fokker-Planck dynamics. Based on these observations, the authors propose a new training method, whose efficacy is demonstrated by the numerical results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper provides both theoretical analysis and the empirical verification for the relationship between the Bayesian neural networks and the Markov random fields. The results of this paper are interesting for the deep learning community, and they can serve the basis for understanding the training process of neural networks."
                },
                "weaknesses": {
                    "value": "1. The theoretical results of this paper are built on the basis of Gaussian linear model. This can be strict in many realistic problems. It would be helpful to discuss the extension of this model. For example, is it possible to extend the results in this paper to the mixture of Gaussian.\n\n2. The writing of the theoretical results can be improved. The authors define many things in the Theorems 1 and 2. It will be more friendly to readers to define these quantities before the statement of the theorems.  In addition, the fontsize of the math characters should be consistent.  For example, in (3) of Theorem 1, the fontsize of characters changes after $=$."
                },
                "questions": {
                    "value": "The questions are listed in the Weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7497/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698497844115,
            "cdate": 1698497844115,
            "tmdate": 1699636905033,
            "mdate": 1699636905033,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5ZTpEr3h7m",
                "forum": "3husFxdHI1",
                "replyto": "O19ynR8856",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7497/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7497/Authors"
                ],
                "content": {
                    "title": {
                        "value": "reformatting theorems for reader-friendliness & on the limitations of Gaussian linear model"
                    },
                    "comment": {
                        "value": "Thank you for your valuable reivew!\n\n**Concerning reformatting theorems for reader-friendliness:** In our revised manuscript, we have repositioned the key quantities and concepts to precede the statements of the theorems, along with concise explanations for each. This reorganization aims to enhance clarity and comprehension for our readers. Additionally, we have included brief, intuitive explanations of the central results within each theorem. The revised sections now reads as follows:\n\n*Theorem 1: Equivalence between Backpropagation and Belief Propagation in a Bayesian Neural Network.*\n\n(1) *Gradient of Loss with Respect to Parameters:* In its most general form, the gradient of loss with respect to the post-activation filtering distribution parameters $\\boldsymbol{\\theta}\\_{l}$ and the variational weights distribution parameters $\\boldsymbol{\\theta}\\_{\\mathbf{W}\\_{l}}$ can be expressed as:\n\n$\\nabla_{\\boldsymbol{\\theta}\\_{l}} J=\\dots$, $\\nabla_{\\boldsymbol{\\theta}\\_{\\mathbf{W}\\_{l}}} J =\\dots$.\n\n(2) *Error Gradient for Canonical Parameters:* When filtering distribution is in the exponential family, error gradient for the canonical parameters is the difference in the mean parameters between smoothing and filtering distributions. Backpropagation mirrors backward belief propagation:\n\n$\\nabla_{\\boldsymbol{\\theta}\\_{l}} J=\\dots$, $\\nabla_{\\boldsymbol{\\theta}\\_{\\mathbf{W}\\_{l}}} J =\\dots$.\n\n(3) *Error Gradient for Post-Activation Mean and Variance:* In a Gaussian linear Bayesian neural network, the error gradient for post-activation mean and variance is related to the difference between smoothing and filtering mean and variance in the corresponding Gaussian linear dynamics. Gradient backpropagation parallels belief backward propagation induced by smoothing gain $G_l$ and $G_{\\mathbf{W}_l}$:\n\n$\\nabla_{\\hat{\\mathbf{x}}\\_{l}}J = \\dots$, $\\nabla_{P\\_{l}}J = \\dots$, $\\nabla_{\\hat{\\mathbf{W}}\\_{l}}J = \\dots$, $\\nabla_{\\boldsymbol\\sigma_{\\mathbf{W}_l}^2}J =\\dots$.\n\nThe error gradient for the canonical parameters of post-activation filter distributions in a Gaussian linear Bayesian neural network is the difference in the first and second moments between smoothing and filtering distributions in the corresponding Gaussian linear dynamics. Gradient backpropagation again parallels belief backward propagation induced by smoothing gain:\n\n$\\nabla_{\\boldsymbol{\\eta}\\_{l}}J =\\dots$, $\\nabla_{\\boldsymbol{\\Lambda}\\_{l}}J =\\dots$\n\n*Theorem 2: Relationship between the Langevin and the Fokker-Planck Dynamics of a Bayesian Neural Network.*\n\n(1) For post-activations distributed as per an exponential family, the error gradient relative to post-activation equals the potential energy gradient, resulting from differences in canonical parameters between filtering and smoothing distributions:\n\n$\\nabla_{\\mathbf{x}\\_{l}}\\log  p(\\mathbf{y}|\\mathbf{x}\\_{l},\\mathbf{x}\\_{0}=\\mathbf{x}) =\\dots$.\n\nThis difference in canonical parameters defines the orthogonal projection from the Jacobian of the sufficient statistics to the error gradient relative to post-activation. Here, $\\bullet^+$ is the pseudo-inverse.\n\n$\\boldsymbol{\\theta}\\_{l}-\\boldsymbol{\\theta\\_{l|L}} =\\dots$\n\n(2) In Gaussian-linear Bayesian neural networks, drift and diffusion processes, defined by the error gradient and Hessian, guide post-activations toward the variational posterior of Gaussian linear dynamics as follows:\n\n$\\nabla_{\\mathbf{x}\\_{l}}\\log p(\\mathbf{y}|\\mathbf{x}\\_{l},\\mathbf{x}\\_{0}=\\mathbf{x})=\\dots$,\n$\\nabla_{\\mathbf{x}\\_{l}\\mathbf{x}\\_{l}^{\\top}}\\log p(\\mathbf{y}|\\mathbf{x}\\_{l},\\mathbf{x}\\_{0})=\\dots$,\n$\\nabla_{\\hat{\\mathbf{x}}\\_{l}}\\log p(\\mathbf{y}|\\mathbf{x}\\_{0})=\\dots$,\n$\\nabla_{P_{l}}\\log p(\\mathbf{y}|\\mathbf{x}\\_{0}=\\mathbf{x}) =\\dots$.\n\n(3) When the weight variance vanishes, making the state transition deterministic with  $p(\\mathbf{x}\\_{l+1}|\\mathbf{x}\\_{l})=\\delta(\\mathbf{x}\\_{l+1}-f(\\mathbf{x}\\_{l}))$, Bayesian back propagation degenerates into non-Bayesian back propagation: \n\n$\\nabla_{\\mathbf{x}\\_{l}}\\log p(\\mathbf{y}|\\mathbf{x}\\_{l})=$\n\n**Concerning the limitations of Gaussian linear model:** Thank you for your valuable input. Our research establishes a fundamental framework, accommodating a diverse array of weight variational posteriors and inference methods. While our algorithm is rooted in Gaussian linear models, it also aligns with the approximate linearity of neural networks in overparameterized regimes, as detailed by Jacot et al. (2018) in their study on neural tangent kernels. This alignment is substantiated by our empirical findings, showcasing effective performance across practical scenarios. We appreciate your recommendation to expand our model to include mixtures of Gaussians, recognizing the potential to handle more intricate data distributions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700350724688,
                "cdate": 1700350724688,
                "tmdate": 1700350724688,
                "mdate": 1700350724688,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xxZ2NBpsSd",
                "forum": "3husFxdHI1",
                "replyto": "5ZTpEr3h7m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7497/Reviewer_Ac7q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7497/Reviewer_Ac7q"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the detailed response! The response addressed my concerns. It is encouraged to include the discussion about the extension beyond the Gaussian linear case in the main paper. I will maintain my scores."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714919165,
                "cdate": 1700714919165,
                "tmdate": 1700714919165,
                "mdate": 1700714919165,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xkjZKd5h9e",
            "forum": "3husFxdHI1",
            "replyto": "3husFxdHI1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7497/Reviewer_2p2Z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7497/Reviewer_2p2Z"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the parallels between probabilistic graphical models and Bayesian neural networks, specifically the main results highlights the equivalence between message passing in probabilistic graphical models and belief propagation in neural networks. Through empirical assessments conducted across diverse scenarios, the paper substantiates this convergence, highlighting their equivalence"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "One of the primary strengths of the paper lies in its problem statement which is exploratory in nature. Specifically, the paper seeks to establish an equivalence between Bayesian neural networks and Markov random fields."
                },
                "weaknesses": {
                    "value": "- While the exploratory nature of the problem is acknowledged, the paper could benefit from a stronger motivation. It is essential to explain why the question of equivalence between Bayesian neural networks and Markov random fields is important. Clarifying the practical implications and real-world significance of this equivalence would strengthen the paper's rationale.\n- A brief note on potential future directions would enhance the appeal of the paper."
                },
                "questions": {
                    "value": "- The paper asserts an equivalence between Bayesian neural networks and probabilistic graphical models, suggesting the potential for \"enhanced models.\" However, the term \"enhanced models\" remains vague and requires precise definition. Providing insight into how this equivalence can be leveraged to create more effective or advanced models would enhance the paper's clarity.\n\n-  The equations in the paper are dense and lack organization, making the paper challenging to comprehend. To improve readability, the authors should offer intuitive explanations for their main results and their consequences. Additionally, providing a brief sketch of the proofs, especially in Theorems 1 and 2, which establish the equivalence between back-propagation and belief propagation, would be beneficial. Clearly defining terms like \"tensor particles,\" \"stochastic tensor flow,\" \"variational message passing,\" \"sensitivity of probability distribution,\" \"tensor distribution evolution,\" and \"mean field Gaussian variational posterior\" is crucial to ensure clarity and readability.\n\n- The plots in Figure 1 are too small to discern the axes, scaling, and legend effectively. The authors should consider enlarging the plots or providing clearer visual aids to improve the readability of the figures.\n\n-  Figure 2's main point is unclear, and it is not evident why the plots emphasize the diagonal. The authors should provide a more detailed explanation of the purpose and focus of Figure 2 to enhance the reader's understanding.\n\n- The paper's GitHub link for the code is not anonymized, compromising the authors' anonymity and violating the double-blind review protocol. The authors should address this issue promptly to maintain the integrity of the double-blind review process."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7497/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7497/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7497/Reviewer_2p2Z"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7497/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829429861,
            "cdate": 1698829429861,
            "tmdate": 1699636904900,
            "mdate": 1699636904900,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lAw0dQgyxe",
                "forum": "3husFxdHI1",
                "replyto": "xkjZKd5h9e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7497/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7497/Authors"
                ],
                "content": {
                    "title": {
                        "value": "balancing contents within the 9-page limit, restatement of motivation, significance, and potential future directions"
                    },
                    "comment": {
                        "value": "Thank you for your valuable reivew!\n\n**Concerning authors' anonymity and violation of the double-blind review protocol:** Thank you for your concern. We certify that there is no URL (e.g., GitHub page) that could be used to find our identity. We are perhaps not who you guessed we are. \n\n**Concerning providing a brief sketch of the proofs:** Thank you for your suggestion regarding the presentation of our proofs. We did. The complete proofs involve complex concepts, such as convex conjugacy and matrix derivatives, so they are detailed in the supplementary material. We have included brief, intuitive explanations of the central results within each theorem in the revised manuscript.\n\n**Concerning clearly defining terms:**\n\n- \"tensor *particles*,\" \"tensor distribution evolution,\" pp 6 and chapters on Fokker-Planck equations and Stochastic Processes in Gardiner et al., (1985)  \n- \"*sensitivity* of probability distribution\", Eq. 14 (pp 291) in Stork et al., (2000)\n- \"variational message passing\" Section 3.3, Definition of the Variational Message Passing algorithm in Winn et al., (2005) \n- \"mean field Gaussian variational posterior\", Examples 5.2 & 5.3, Chapter 5, Wainwright & Jordan, (2008)\n\n**Concerning motivation, significance, and implications:**\n\n*Motivation*\n\nOur primary motivation is to advance the theoretical understanding of deep learning, focusing on the general assumptions necessary for learning to generalize and converge. Our approach is inspired by the \"society of mind\" concept, which proposes that intelligence emerges from the interactions of simple agents, similar to various networks processing information and optimizing operations. Our research aligns with a lineage that views deep learning through the lens of probability theory, encompassing Bayesian neural networks, Boltzmann machines, variational and Monte Carlo methods, deep generative models, and neural network Gaussian processes.\n\n*Significance*\n\nOur research, highlighting the parallels between graphical models and neural networks, is significantly practical and theoretical. It enables learning and inference within graphical models using auto-differentiation in mainstream deep learning APIs like TensorFlow, PyTorch, and JAX. Our code demonstrates the creation of symbolic layers for handling probability distributions and error gradients, seamlessly integrating with standard non-Bayesian layers. This advancement broadens the spectrum of deep neural network architectures and learning algorithms, incorporating a diverse range of graphical models and variational inference methods.\n\nTheoretically, our work lays down a mathematical foundation to discuss the probability of choosing an effective non-Bayesian neural network from a Bayesian neural network ensemble based on a specific training dataset, in line with statistical learning theory. Additionally, our introduction of the Gaussian linear neural Gaussian Markov random field model establishes parameters for global smoothness and local continuity in high-dimensional Gaussian distributions, aiding in the calculation of stochastic gradient descent convergence rates. This is particularly relevant given the tendency of neural networks to maintain approximate linearity in overparameterized settings.\n\n*Future Directions:*\n\n- Exploring normalization-free neural networks for SOTA performance in computer vision and NLP tasks.\n- Developing Bayesian language models, including large hierarchical hidden Markov models, to compete with transformer architectures.\n- Investigating graphical models that mirror transformer functionality and exploring improvements for better performance with fewer parameters.\n- Utilizing Fisher divergence within the Bayesian neural network framework to advance deep generative models, possibly offering alternatives to sliced score matching.\n- Establishing generalization error bounds using concentration inequalities and demonstrating these theoretical results with SOTA architectures.\n- Examining the convergence rate of learning SOTA neural networks using neural Gaussian linear Markov random field approximations.\n- Evolving neural network architectures from data, exploring structural features of complex systems, and evolving neural architectures from scratch.\n- Exploring the equivalence between intelligence and randomness, establishing a bidirectional relationship between data and synaptic weight distributions, and investigating the functional equivalence of synaptic weights.\n\n**Figure 1 are too small:** We have enlarged the panels as much as we can to not exceed the 9-page limit.\n\n**Figure 2's main point:** Main point is that Markov random field is a generative model. We added detailed explanations in the supplementary material in the revised manuscript."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700350699338,
                "cdate": 1700350699338,
                "tmdate": 1700352937822,
                "mdate": 1700352937822,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "p07aw0Dy9r",
            "forum": "3husFxdHI1",
            "replyto": "3husFxdHI1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7497/Reviewer_kosy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7497/Reviewer_kosy"
            ],
            "content": {
                "summary": {
                    "value": "This paper identifies and establishes connections between Bayesian neural networks and probabilistic graphical models (Markov random fields). In particular, they establish equivalence between backpropagation and belief propagation in Theorem 1. Their Theorem 2 presents a relationship between Langevin and the Fokker-Planck dynamics. They further leverage these connections to develop a belief propagation-based algorithm to train Bayesian neural networks and show its efficacy through numerical experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The connection between backpropagation and belief propagation is an interesting result. It has the potential to pave the way for the adaptation of various algorithms from the field of probabilistic graphical models to Bayesian neural networks. The authors have also substantiated their theory through a series of numerical experiments, demonstrating that their proposed algorithm consistently surpasses other baseline methods in terms of performance."
                },
                "weaknesses": {
                    "value": "The paper is quite heavy on notations and a little difficult to follow at times. Some of the notations are inconsistent and create confusion while reading. Few examples (there could be more):\n1. I believe it would be easier to understand the equations if the dimensions of the parameters were stated clearly. \n2. $f_l$ seems to take arguments in different orders (check eq. for $x_l$ before (4) and then in the fifth line after (5)).\n3. Is there a purpose for italicizing one of the $f_{x_{l-1}}$ in (6)?\n4. In Theorem 1 (2), $T(x_l)$ is written in both regular and bold fonts. In general, there does not seem to be a consistent interpretation of boldface parameters.\n\nCheck the following statement (and other similar statements): \"This research emphasizes the convergence between probabilistic graphical models and neural networks, revealing their intrinsic parallels.\" \nIt seems that the authors are claiming that probabilistic graphical models and neural networks are the same. I understand the parallels but as a whole the statement seems to be too strong. It would be better to qualify such string statements by formal evidence.  \n\nPlease also see the questions section."
                },
                "questions": {
                    "value": "1. The term tensor \"particles\" has been used repeatedly throughout the paper without a formal definition or reference. Could you please explain what exactly is a tensor particle?\n2. In the context of probabilistic graphical models, belief propagation comes with no convergence guarantees for graphs with cycles. Does such consideration occur in the proposed method? In general, would the proposed algorithm always converge?\n3. Could you explain the following comment under Theorem 1 (a formal derivation or a pointer to a reference would help)? \n\"The exponential family assumption in (2) generally holds as long as the joint probability density of the random variables is strictly positive, as per the Hammersley-Clifford theorem (Hammersley & Clifford, 1971).\"\n4. I am trying to place this work in the context of the existing work relating belief propagation with neural networks. Has there been any work in this regard or are the authors presenting a completely novel observation? \n\nI am open to changing my score based on the answers from the authors."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7497/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7497/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7497/Reviewer_kosy"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7497/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699509792659,
            "cdate": 1699509792659,
            "tmdate": 1699636904768,
            "mdate": 1699636904768,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xNWnDjj2QF",
                "forum": "3husFxdHI1",
                "replyto": "p07aw0Dy9r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7497/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7497/Authors"
                ],
                "content": {
                    "title": {
                        "value": "authors presenting a completely novel observation & revisions for user-friendliness"
                    },
                    "comment": {
                        "value": "Thank you for your valuable reivew! \n\n**Regarding dimensions in our equations**. In our notation, operators are applied from the left, meaning, \n$\\mathbf{W}\\_{l}\\cdot \\mathbf{x}\\_{l-1}=\\left( \\sum_{j_{l-1}}\\textnormal{W}\\_{i_l,j_{l-1}}\\textnormal{x}\\_{j_{l-1}} \\right)\\_{i_l}$ \ncalculates pre-activation elements at multi-index $i_l$ as weighted sums of post-activation elements at $j\\_{l-1}$. For ease of understanding, multi-indexes can be viewed as standard integer indexes, with synaptic weights as matrices and post-activation tensors as vectors. This clarification is in the revised paper's first paragraph, Section 2, Notation.\n\n**Regarding notation consistency**: We use `\\mathbf` for synaptic weights, post-activations, inputs, and outputs; `\\boldsymbol` for weight and post-activation distribution parameters; and standard symbols for other mathematical elements. This notation has been thoroughly checked for consistency in the revised paper.\n\n**Responding to the overstatement concern**: We aim to highlight similarities between probabilistic graphical models and neural networks, not asserting their identity but their ability to inform and enhance each other's methodologies. We have amended the statement to better reflect this nuanced perspective in the revised paper.\n\n**On the term \"tensor particles\"**: It describes a set comprising inputs, post-activations, synaptic weights, and labels in a Bayesian neural network, drawing from Monte Carlo methods in statistical physics, where a \"particle\" represents a possible state of the system under study, similar to how physical particles behave and interact (Gardiner et al., 1985). This explanation is added at the end of Section 1 Introduction's 2nd paragraph.\n\n**Addressing convergence in belief propagation**: While it's true that the exact conditions for loopy belief propagation's convergence are not fully established, our theoretical contributions\u2014specifically the equivalence between backpropagation and belief propagation in Bayesian neural networks, and the analysis of Langevin and Fokker-Planck dynamics\u2014are foundational. They enable the use of other approximate methods with known convergence guarantees, such as variational and Monte Carlo methods. Our approach, centered on Gaussian belief propagation, incorporates well-understood convergence conditions that are easier to analyze. Practically, our algorithm shows faster convergence compared to stochastic gradient descent for non-Bayesian networks and Monte Carlo methods for Bayesian neural networks, as demonstrated in Figure 1 and detailed in Section 4.1.\n\n**Clarifying Theorem 1's exponential family assumption**: The Hammersley\u2013Clifford theorem states that a strictly positive probability measure Markov with respect to a graph G equates to a Gibbs random field. Our theorem's Bayesian neural network computational graph forms an undirected graphical model, satisfying this and ensuring a Gibbs measure. The proof of the Hammersley-Clifford Theorem is technical; a machine learning-aligned version is under Theorem 7.12 in Grimmett's \"Probability on Graphs.\" It is based on establishing the equivalence between global Markov property, local Markov property, and pairwise Markov property. To prove factorization from pairwise Markov property, they show that that the potential on any non-complete subgraph $a$ is 0. This is translated into $\\phi_a(x)=\\sum_{b \\subseteq a} (-1)^{|a\\backslash b|} \\log f_b\\left(x_b, x_{b^c}^\\star\\right)=0$. The proof of this, in turn, is based on Mobius Inversion Lemma.\n\n**Regarding novel contributions**: In this paper, we presented a **completely novel observation**, specifically: In the context of existing work on belief propagation and neural networks (cf. Section 5 Related Works), our paper introduces a unique angle by demonstrating the equivalence of Bayesian neural networks (BNNs) with Markov random fields, connecting backpropagation with belief propagation, and relating probabilistic perspective of tensor flow at the microscopic level and the deterministic perspective of tensor distribution evolution at the macroscopic level. Previous research primarily focused on developing learning algorithms through MCMC and variational inference, applying BNNs in NLP and computer vision, and exploring Bayesian learning biases. Extending beyond the scope of our surveyed works, if parallels existed in areas like neural tangent kernels, neural network Gaussian processes, or neural ordinary differential equations, they would likely entail interpreting neural networks through the lens of established mathematical/probability theories. However, our extensive review of the recent literature revealed no such works aligning closely with our specific methodologies and theoretical insights in these domains."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7497/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700350745672,
                "cdate": 1700350745672,
                "tmdate": 1700396955434,
                "mdate": 1700396955434,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]