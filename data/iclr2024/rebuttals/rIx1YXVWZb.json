[
    {
        "title": "Understanding Addition in Transformers"
    },
    {
        "review": {
            "id": "O0mjSDChA0",
            "forum": "rIx1YXVWZb",
            "replyto": "rIx1YXVWZb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7065/Reviewer_ExS5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7065/Reviewer_ExS5"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the interpretability of Transformers. The authors focus on the 5-digit number addition task and analyze how a one-layer Transformer model finishes this task. To understand the model clearly, they propose a mathematical framework for integer addition, consisting of five tasks: Base Add, Make Carry 1, Make Sum 9, Use Carry 1, and Use Sum 9. The first three tasks can be independently executed for each digit pair, representing the sum of two digits modulo 10, checking for carry, and determining if the addition results in 9, respectively. The last two tasks chain operations across digits, respectively denoting adding the previous column's carry to the sum of the current digit pairs, and propagating a carry when Make Sum 9 and Use Carry 1 are true. The authors then analyze the one-layer Transformer model under this framework during the training phase and testing phase. More precisely, during the training phase, the authors investigate the training loss for Base Add (BA), Use Carry 1 (UC1), and Use Sum 9 (US9) three tasks. According to the experimental results,   US9 is the most complicated, especially in the case where more than one column carry occurs (e.g. 445+555=1000) and BA, UC1 two tasks are highly correlated. During the testing phase, the authors use ablation experiments to evaluate each attention head and conclude that for different digit pairs, the model uses slightly different algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The paper advances in the direction of opening the black box of Transformers, which is a very important topic as Transformers are being applied in an increasing number of domains. \n\n- The authors decompose integer addition into several subtasks and investigate the loss of each task during the training. This might provide some inspiration for future improvements in deep learning for math.\n\n- The paper is well-organized. The basic idea is clean and easy to follow."
                },
                "weaknesses": {
                    "value": "- One experimental flaw is that the test accuracy of the model is not provided. In addition, it is also worthwhile to explore using the trained model directly for the addition of integers with more digits.\n\n- In the integer addition task, digit 0 should be treated as a special case, since intuitively, when humans perform integer calculations, the more zeros there are, the easier the calculation becomes. In other words, the digit 0 requires special attention. So it might be interesting to include Make Sum 0 and Use Sum 0 in the mathematical framework."
                },
                "questions": {
                    "value": "(1) What's the performance of the trained model on the test data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7065/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7065/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7065/Reviewer_ExS5"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7065/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698637712886,
            "cdate": 1698637712886,
            "tmdate": 1699636831615,
            "mdate": 1699636831615,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VlDohj9m90",
                "forum": "rIx1YXVWZb",
                "replyto": "O0mjSDChA0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7065/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer ExS5's review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments and feedback. They helped improve the paper.\n\nWe now address the weaknesses and questions raised point by point:\n\n__Weakness: One experimental flaw is that the test accuracy of the model is not provided. In addition, it is also worthwhile to explore using the trained model directly for the addition of integers with more digits.__\n\nThe new Appendix 17 provides this detail, including loss for 5, 10 and 15 digit cases.\n\n__Weakness: In the integer addition task, digit 0 should be treated as a special case, since intuitively, when humans perform integer calculations, the more zeros there are, the easier the calculation becomes. In other words, the digit 0 requires special attention. So it might be interesting to include Make Sum 0 and Use Sum 0 in the mathematical framework.__\n\nThe Make Sum 0 ideas was indeed considered by the authors. Make Sum 0 is a useful efficiency gain for humans doing a specific sub-class of addition questions. Our intuition is that the model is calculating bigrams of tokens without any real understanding of numbers - it treats the digits as string tokens \"3\" + \"4\" = \"7\". The model can efficiently map all 100 pairings without needing to or benefiting from treating \"0\" as a special case. Our intuition is that if the model treated \"0\" as a special case, this would increase the algorithm complexity without increasing its accuracy and so the model would not benefit from adding this rule.\n\n__Q1. What's the performance of the trained model on the test data?__\n\nThe overall response and the new Appendix 17 provide this detail."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559594930,
                "cdate": 1700559594930,
                "tmdate": 1700559594930,
                "mdate": 1700559594930,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YrX4SulX2v",
                "forum": "rIx1YXVWZb",
                "replyto": "VlDohj9m90",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7065/Reviewer_ExS5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7065/Reviewer_ExS5"
                ],
                "content": {
                    "comment": {
                        "value": "The reviewer thanks the author for their response. The new appendix strengthens the paper a lot in my opinion."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667888441,
                "cdate": 1700667888441,
                "tmdate": 1700667888441,
                "mdate": 1700667888441,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lExhXVmZCt",
            "forum": "rIx1YXVWZb",
            "replyto": "rIx1YXVWZb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7065/Reviewer_sWU9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7065/Reviewer_sWU9"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into the intricacies of a one-layer Transformer model trained for integer addition, emphasizing the importance of understanding machine learning models for safety and ethical considerations. The study uncovers that the model breaks down the addition task into parallel, digit-specific streams, using different algorithms for various digit positions. Interestingly, the model starts its calculations later but completes them swiftly. A unique use case with a high loss is pinpointed and elaborated upon."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This is a technically solid, moderate to high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
                },
                "weaknesses": {
                    "value": "1. The adaptability of the methodology in this paper is limited, as it only applies to a one-layer Transformer model. Perhaps further analysis on two-layers or even more complex models would be beneficial. Moreover, the study solely focuses on integer addition, making it challenging to extend to other operations like subtraction or multiplication.\n2. The writing of this paper is not comprehensive. For instance, the descriptions for Figure 4 and 8 are difficult to comprehend.\n3. The experiments conducted are not exhaustive. In the \"Prediction Analysis\" section, the authors failed to provide a specific metric and results compared to baselines."
                },
                "questions": {
                    "value": "Please see the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7065/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7065/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7065/Reviewer_sWU9"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7065/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824766950,
            "cdate": 1698824766950,
            "tmdate": 1699636831502,
            "mdate": 1699636831502,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "p0LLgOCp8V",
                "forum": "rIx1YXVWZb",
                "replyto": "lExhXVmZCt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7065/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer sWU9's review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments and feedback. They helped improve the paper.\n\nWe now address the weaknesses raised point by point:\n\n__Weakness: The adaptability of the methodology in this paper is limited, as it only applies to a one-layer Transformer model. Perhaps further analysis on two-layers or even more complex models would be beneficial. Moreover, the study solely focuses on integer addition, making it challenging to extend to other operations like subtraction or multiplication.__\n\nFeedback includes that analysis should be extended to two-layer or more complex models. We agree with this direction but see this work as outside the scope of this paper. When this paper was written we did not understand the more-complex 2-layer addition algorithm well. (Our analysis of this model is now well advanced. The 2-layer model utilizes the 1-layer algorithm but also includes an additional algorithm covering the \u201ccascading US9\u201d high-loss edge case. This will be the subject of a future paper.). We see this paper is a solid first step on a long journey explaining ever more complex algorithms (e.g. multiplication) partially by re-using well-understood foundational components (e.g. addition).\n\nThe concern is raised that the paper\u2019s methodology is not reusable and the applicability of the paper is limited. We respectfully disagree. Table 5 in the new Appendix 17 \u201cThe loss function and loss measures\u201d shows one instance of this approach, showing the output from an automated test framework. \n\nThe concern is raised that the methodology only applies to one-layer transformer model. While, it is very likely that 1-layer addition, 2-layer addition, subtraction, multiplication implement substantially different algorithms, this paper\u2019s methodology provides a way to gain many insights into any algorithm, before the algorithm\u2019s implementation detail is understood. While these insights do not explain the algorithm\u2019s implementation directly, from experience they prove to be very useful in constraining the feasible algorithm space, leading to further researcher insights. \n\n__Weakness: The writing of this paper is not comprehensive. For instance, the descriptions for Figure 4 and 8 are difficult to comprehend.__\n\nThe concern was raised that the paper was not comprehensive. In this paper we:\n- Provide an alternative mathematical framework for addition suited to neural networks\n- Show not only that the model can do addition and is low loss, but explains the edge-case causing the loss in terms of the mathematical framework\n- Propose an explanation for the model\u2019s implementation of the addition algorithm in terms of the mathematical framework\n- Validate the proposed algorithm with experimental results. Details of this validation are in the new \u201cLoss function and loss measures\u201d appendix.\n\nA concern was raised that Figure 4 is difficult to understand. The addition algorithm that the model learnt is more complex than the traditional human process for addition. Understanding the model\u2019s approach requires us to ignore our existing process, disassemble the addition process into its foundational components and then reassemble an alternative process. Figure 4, supported in the paper, documents this alternative approach. It is non-trivial to understand but led to insights key to understanding the model\u2019s algorithm. We believe Figure 4 is useful and instructive.\n\nA concern was raised that Figure 8 is difficult to understand. The addition algorithm that the model learnt is complex - more complex than the traditional human approach to addition. Some complexity in description is unavoidable. Figure 1 introduces the algorithm concepts and shows a simplified representation of the algorithm. Figure 8 fleshes out the algorithm\u2019s detail. The two figures work together, supported in the paper, to build out the concepts. We believe these figures are very good representations of the algorithm, and that further simplification in the diagram would require omitting pertinent information. To aid in comprehension, a new Appendix 16 \u201cModel Algorithm as Pseudocode\u201d reproduces the algorithm in Figure 8 as pseudocode.\n\n__Weakness: The experiments conducted are not exhaustive. In the \"Prediction Analysis\" section, the authors failed to provide a specific metric and results compared to baselines__\n\nThe overall rebuttal and the new Appendix 17 \u201cLoss function and loss measures\u201d provide this detail."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559239926,
                "cdate": 1700559239926,
                "tmdate": 1700679643998,
                "mdate": 1700679643998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u4sZ84lzo2",
                "forum": "rIx1YXVWZb",
                "replyto": "lExhXVmZCt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7065/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request for feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer sWU9\n\nThanks a lot for your time in reviewing and insightful comments, which we used to carefully revise the paper to answer your questions. We sincerely understand you\u2019re busy. But since the discussion due is approaching, would you mind checking the response and revision to confirm whether you have any further questions?\n\nWe are looking forward to your reply and are happy to answer your further questions.\n\nBest regards"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679705160,
                "cdate": 1700679705160,
                "tmdate": 1700679705160,
                "mdate": 1700679705160,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LsiisUjwEp",
            "forum": "rIx1YXVWZb",
            "replyto": "rIx1YXVWZb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7065/Reviewer_f5y1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7065/Reviewer_f5y1"
            ],
            "content": {
                "summary": {
                    "value": "Very interesting paper that focuses on explaining the \"inner workings\" of the foundational model of Transformer. While the use-case demonstrated (integer addition with a single layer transformer) is simplistic, the idea is novel and the visualisations are meaningful and make sense for better trust and confidence in how a transformer model works for the AI community."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Transformer model focus - no doubt an important model in the current AI landscape. Solid mathematical explainations and interpretation of the model working, the attention visualisations shown are very interesting and the model training loss curve which shows how a transformer trains individual digits semi-independently was promising to see."
                },
                "weaknesses": {
                    "value": "No major weakness other than the paper applying the framework of explainability to a simple problem (integer addition). Though, this is well the strength as well of the paper as it makes the model easier to interpret and understand."
                },
                "questions": {
                    "value": "Solid theoretical framework in the paper, good interpretation and visualisations - no further questions from this reviewer. The paper is very well written, easy to understand and the method is clear."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7065/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698965626523,
            "cdate": 1698965626523,
            "tmdate": 1699636831380,
            "mdate": 1699636831380,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2mwg6VdHKX",
                "forum": "rIx1YXVWZb",
                "replyto": "LsiisUjwEp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7065/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer f5y1's review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments and feedback."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558994285,
                "cdate": 1700558994285,
                "tmdate": 1700558994285,
                "mdate": 1700558994285,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3wXj2eYVyz",
            "forum": "rIx1YXVWZb",
            "replyto": "rIx1YXVWZb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7065/Reviewer_wEos"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7065/Reviewer_wEos"
            ],
            "content": {
                "summary": {
                    "value": "This paper attempts to reveal the internal working mechanism of Transformers for integer addition tasks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "\u2022 Study an important problem of transformer models' application in numerical computation tasks."
                },
                "weaknesses": {
                    "value": "\u2022 The analysis framework and analysis lacks mathematical rigidity. \n\t\u2022 The conclusion is not well established based on rigorous mathematical framework. \n\t\u2022 The paper does not fully utilize/choose the most relevant aspect of transformers for addition tasks."
                },
                "questions": {
                    "value": "1. Page 2, a latex error: \"d_e-dimensional embeddings\"\n\t2. Page 3, section 3, paragraph 3, \"detailing identified circuits\", is this a typo? Or which \"identified\" circuits? What is the \"identified\" process?\n\t3. Page  3, section 3, paragraph 4, \"techniques in works like symbolically \u2026 \", a grammar error?\n\t4. Page 3, section 3, paragraph 7, \"Surveys like  overview techniques\u2026\", missing citations after \"Surveys like\"?\n\t5.  Page 4, section 4, paragraph 3, \"Fig. 2 shows \u2026 semi-indendently\u2026\", what is the loss per digit is being plot in Figure 2? \n\t6. Page 4, section 4, paragraph 4, \"Transformer models always process text from left to right\u2026\", this is not true. It is just an artifact of GPT-style attention masking. For example, we can do config the attention mask to enable full order attention over the two addends and generate the outputs in all kinds of order, e.g. from the tens digit to higher value digits, from the middle digit to two ends, and so on. We can also do non-autogression generation, e.g. incremental masking output generation. \n\t7. Page 4, figure 3 caption \"..After the question is fully revealed (at layer 11)..\", by \"layer 11\" do you mean the 11th row? To avoid ambiguity, it is better to number the attention matrix and refer to them the row or column number across the paper.  Also what are the sub-figures of 0.0, 0.1, 0.2? Different heads? What are the labels? \n\t8.   Page 4-5, section 5, please clarify whether the  \"mathematical framework\" is  for characterizing (grouping) addition data instances-digits only? Or is there a link to the loss? If so, please formulate the framework and what kind of mathematical hypotheses this framework can verify formally in mathematical terms?  Also please detail the loss on each digits formally. Also please detail the statistics of your training and valid datasets in terms of your classification of digits in your framework. \n\t9. Page 4-6, please detail how the loss is being average. Are they per digits or per digit average?\n\t10. Page 6, please introduce or define or describe phase 1, 2, 3 formally?  \n\t11. Page 7, section 7, \"During model prediction we overrode \u2026 the model memory (residual stream)\u2026\", please detail the approach formally? Are your conclusions/assertions based on checking the attention scores?  Please discuss explicitly with formal treatment. Otherwise, the plain English language analysis in Section 7 is difficult to follow and justify. Also formally define \"independent of every other digit\", \"most impact on loss\" and define it based on measure statistics during model inference time."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7065/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699353481880,
            "cdate": 1699353481880,
            "tmdate": 1699636831229,
            "mdate": 1699636831229,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GBaiRyc7jW",
                "forum": "rIx1YXVWZb",
                "replyto": "3wXj2eYVyz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7065/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer wEos's review - Weaknesses [1/2]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and feedback. \n\nOur paper presents an in-depth analysis of a one-layer Transformer model trained for n-digit integer addition, showing the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. A rare use case with high loss is identified and explained. The model\u2019s algorithm is explained, with findings validated through mathematical modeling and rigorous testing.\n\nWe now address the weaknesses and questions raised point by point:\n\n__Weakness: The analysis framework and analysis lacks mathematical rigidity.__\n\nA concern was raised that the conclusion is not well established based on rigorous mathematical framework. Please refer to the overall rebuttal and the new Appendix 17 that provides detail on the experimental results that underpin the paper\u2019s claims. \n\n__Weakness: The conclusion is not well established based on rigorous mathematical framework__\n\nA concern was raised that the analysis framework and analysis lacks mathematical rigidity. Please refer to the overall rebuttal and the new Appendix 17.\n\n__Weakness: The paper does not fully utilize/choose the most relevant aspect of transformers for addition tasks.__\n\nA concern is raised that the paper does not fully utilize/choose the most relevant aspect of transformers for addition tasks. We\u2019re not clear on what this refers to. Assuming this refers to using a 1-layer rather than a 2-layer model, when this paper was written we did not understand the more-complex 2-layer addition algorithm well. Our analysis of the 2-layer addition model is now well advanced, and leverages some of the methodologies and findings of this paper. We see this paper is a solid first step on a long journey explaining ever more complex algorithms and transformers.\n\nsee below Response to reviewer wEos's review - Questions [2/2] for responses to questions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559836459,
                "cdate": 1700559836459,
                "tmdate": 1700560409387,
                "mdate": 1700560409387,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KHt6Nojm8O",
                "forum": "rIx1YXVWZb",
                "replyto": "3wXj2eYVyz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7065/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer wEos's review - Questions [2/2]"
                    },
                    "comment": {
                        "value": "__Q1, Q2, Q3, Q4. Various small issues__\n\nThank you for pointing out these issues. They have been resolved in the paper.\n\n__Q5. Page 4, section 4, paragraph 3, \"Fig. 2 shows \u2026 semi-independently\u2026\", what is the loss per digit is being plot in Figure 2?__\n\nThe per-digit loss is detailed in the new Appendix 17.\n\n__Q6. Page 4, section 4, paragraph 4, \"Transformer models always process text from left to right\u2026\"__\n\nA concern was raised about the statement \u201cTransformer models always process text from left to right\u201d. This has been fixed in the paper to read: This autoregressive transformer model processes text from left to right.\n\n__Q7. Page 4, figure 3 caption \"..After the question is fully revealed (at layer 11)..\", by \"layer 11\" do you mean the 11th row?__\n\nA concern was raised about the inconsistent use of the term \u201clayer\u201d in explanations. We have fixed the paper to consistently use the terms: \u201clayer\u201d only for number of layers in the Transformer and \u201crow\u201d for each (vertical) step in the attention pattern.\n\nThe meaning of the labels in the attention pattern figure was queried. The Figure image has been improved in the paper to clarify the meaning of the image labels as Layer and Head. \n\n__Q8. Page 4-5, section 5, please clarify whether the \"mathematical framework\" is for characterizing (grouping) addition data instances-digits only? Or is there a link to the loss?__\n\nThe concern about the independence (or otherwise) of the mathematical framework and the loss calculations is covered by the \u201cWeaknesses\u201d section above and the new Appendix 17. The \u201cMathematical Framework\u201d section now concludes with: *We use this mathematical framework solely for analysis to gain insights. The model training and all loss calculations are completely independent of this mathematical framework.*\n\n__Q9. Page 4-6, please detail how the loss is being average. Are they per digits or per digit average?__\n\nThere is a loss function used with per-digit graphs, and a separate \"mean\" loss function used with all-digit graphs. Both loss functions are defined in the overall feedback and in the new Appendix 17.\n\n__Q10. Page 6, please introduce or define or describe phase 1, 2, 3 formally?__\n\nThe suggestion is made to introduce or define the phases 1, 2 & 3 formally. We are hesitant to make strong claims about these phases at this time. Experimentally, and as per the literature, they seem to correspond to \u201cmemorization\u201d, \u201calgorithm refinement\u201d and \u201cclean-up\u201d phases. The \u201cTraining Analysis\u201d figure description now includes: The 3 phases seem to correspond to \u201cmemorization\u201d, \u201calgorithm discovery\u201d and \u201cclean-up\u201d.\n\n__Q11.Page 7, section 7, \"During model prediction we overrode \u2026 the model memory (residual stream)\u2026\", please detail the approach formally__\n\nA request for a formal treatment of the conclusion/assertions including how the model prediction was overridden was requested is covered in Appendix 17.\n\nA request was made for a formal definition of the independent calculation of each digit. The new Appendix 16 \u201cModel Algorithm as Pseudocode\u201d shows how the algorithm calculates each digit independently."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560242694,
                "cdate": 1700560242694,
                "tmdate": 1700560354439,
                "mdate": 1700560354439,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JMYEkdTcI1",
                "forum": "rIx1YXVWZb",
                "replyto": "3wXj2eYVyz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7065/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request for feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer wEos\n\nThanks a lot for your time in reviewing and insightful comments, which we used to carefully revise the paper to answer your questions. We sincerely understand you\u2019re busy. But since the discussion due is approaching, would you mind checking the response and revision to confirm where you have any further questions?\n\nWe are looking forward to your reply and are happy to answer your further questions.\n\nBest regards"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679617450,
                "cdate": 1700679617450,
                "tmdate": 1700679617450,
                "mdate": 1700679617450,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sRwkSQ93wD",
                "forum": "rIx1YXVWZb",
                "replyto": "JMYEkdTcI1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7065/Reviewer_wEos"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7065/Reviewer_wEos"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for your efforts to address my questions. For addition or for handling digits using transformer, besides 1 layer or multi-layer design choices, there are more options in how the sequencs of digits are being embedded,  prococessed and generated. The conclusion in the paper on transformer in handling addition jumps to the conclusion too quick without exploring other possibilities. It might be misleading. So I would keep my original rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709936515,
                "cdate": 1700709936515,
                "tmdate": 1700709936515,
                "mdate": 1700709936515,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]