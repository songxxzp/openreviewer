[
    {
        "title": "A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks"
    },
    {
        "review": {
            "id": "r1i31ADlgH",
            "forum": "LnLySuf1vp",
            "replyto": "LnLySuf1vp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2917/Reviewer_bEwP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2917/Reviewer_bEwP"
            ],
            "content": {
                "summary": {
                    "value": "This work presents SpikeGCL which targets on optimizing GCL with SNN. They provide detail experiments to demonstrate the efficiency of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tConnect SNN and GNN is a very important topic, since GNN is closer to the neuron system and SNN is closer to the neuron dynamic.\n2.\tIt is interesting to combine feature dimension and temporal axis."
                },
                "weaknesses": {
                    "value": "1.\tThe detailed background does not provide in background section, and there are too many existing work descriptions in the method sections.\n2.\tSome design choices do not present."
                },
                "questions": {
                    "value": "1.\tIn Sec3, does the problem formulation special for this work, or it is general for all GCL application? Please make it clearly. \n2.\tI think author should formulate the GCL problem in background (instead of giving a brief introduction). Also, it is better to highlight which part is optimized by the proposed methods.\n3.\tWhy T encoders? Usually, neurons adopt the same weight among T time-steps, this design may increase the model size. Also, it is not clear how computation complex relates to time step size, since the author claim that they partition the feature dimension into T blocks. In my opinion, the computation complexity would not change when modifying T, which is not consist to Fig4.\n4.\tFig3(b) \u2018y1->y2\u2019. Also, it is not clear how the entire backpropagation work. whether all y1\u2026yn can directly receive gradient from the loss? Sec 4.4 introduces too much previous studies, it is better to clarify the gradient diagram in revision, i.e. for different blocks, where the gradient come from\n5.\tHow SPIKEGCL can reduce parameter size? Usually, SNN can reduce the activation size but keep parameter size unchanged. Author should provide a diagram of how to compute the parameter size."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2917/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2917/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2917/Reviewer_bEwP"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2917/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698654736314,
            "cdate": 1698654736314,
            "tmdate": 1699636235332,
            "mdate": 1699636235332,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0NfyWGiaIv",
                "forum": "LnLySuf1vp",
                "replyto": "r1i31ADlgH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2917/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2917/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bEwP (part 1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for detailed comments and suggestive feedback. We want to clarify some misunderstandings that caused some of your concerns.\n\n\n\n>  **W1, Q1, and Q2:** (1) Background section; (2) In Sec3, does the problem formulation special for this work, or it is general for all GCL application? Please make it clearly. (3) Author should formulate the GCL problem in background (instead of giving a brief introduction). Also, it is better to highlight which part is optimized by the proposed methods.\n\nWe respectfully disagree with the point on our presentation. The current manuscript is carefully polished, as also recognized by Reviewers n3GK, jihE, and HWYD with high scores 3, 4, and 3 on the **Presentation** part. We would like to address your main concerns w.r.t. the background part below:\n\n+ The problem addressed in this work is specifically formulated for binarized graph contrastive learning, with a slight modification from the general formulation of GCL. In this work, the output representations of GCL are required to be binarized.\n\n+ Given the prevalence of GCL in current research, we don't think it is necessary to put a large body of context to provide a detailed formulation of the GCL problem in the paper. We believe that providing a brief introduction to GCL is sufficient to establish the context for our work on binarized graph contrastive learning, as the readers in this field are likely to be familiar with the fundamental concepts and principles of GCL.\n+ In SpikeGCL, our primary focus is on optimizing the encoder part of GCL using SNNs to achieve the binarized node representation.\n\n\n\n> **Q3:** Why T encoders?\n\nWe believe there are some misunderstanding on our work. We want to clarify some misunderstandings that caused some of your concerns.\n\n+ **Reason to use T encoders.** The use of $T$ encoders are well-motivated in response to the increasing scale of graph data. As explained in Sec. 4.1, previous approaches for handling sequential inputs from non-temporal graphs often involve repeating the graph $T$ times with augmentation strategies.  However, this increases the computational complexity of the encoder, resulting in $T$ times the forward pass, with each step having a d-dimensional input. To address this issue, SpikeGCL utilizes $T$ encoders applied to T groups of inputs, where each group has a $\\frac{d}{T}$-dimensional input. In other words, the computational complexity is reduced to approximately $\\frac{1}{T}$ compared to previous methods that repeat the GNN T times. This reduction significantly decreases the computational and memory overheads associated with applying SNNs on non-temporal graphs.\n\n+ **Complexity of $T$ encoders in SpikeGCL.** The use of $T$ encoders in SpikeGCL **does not** introduce additional overheads because the learnable parameters of the T encoders, **except for the first layer**, are shared across layers. Moreover, the first layer of each encoder handles only $\\frac{d}{T}$ features, allowing the complexity of SpikeGCL to remain certainly low."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107037761,
                "cdate": 1700107037761,
                "tmdate": 1700107037761,
                "mdate": 1700107037761,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DO6rQLUi0e",
                "forum": "LnLySuf1vp",
                "replyto": "r1i31ADlgH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2917/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2917/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bEwP (part 2/2)"
                    },
                    "comment": {
                        "value": "> Q4: Typo in Fig.3(b).\n\nThank you for pointing it out, corrected in the revision.\n\n\n\n> Q5: It is not clear how the entire backpropagation work. whether all y1\u2026yn can directly receive gradient from the loss? Sec 4.4 introduces too much previous studies, it is better to clarify the gradient diagram in revision, i.e. for different blocks, where the gradient come from.\n\nWe apologize for any confusion caused. The gradient of each block is derived from the contrastive loss computed on the output spikes at each time step. In other words, **we calculate the contrastive loss for the outputs of each time step and perform backward propagation for each individual block.** To prevent the gradients from flowing through the blocks, we utilize stop-gradient. For a clearer illustration, we kindly refer you to Algorithm 2 in Appendix C.\n\n\n\n> Q6: How SPIKEGCL can reduce parameter size? Usually, SNN can reduce the activation size but keep parameter size unchanged. Author should provide a diagram of how to compute the parameter size.\n\n+ **Parameter-shared Encoders**. As discussed in Appendix D.1, the learnable parameters of the $T$ encoders are shared across layers **except the first layer** to meet the diverse input dimensions of group of features.  The matrix dimension of learnable weight of the $T$ encoders are: layer 1 ($T \\times \\frac{d_1}{T}\\sim d_1$), layer 2 ($d_2$), ..., layer $L$ ($d_L$), which is equivalent to a simple GNN with $d_1$, $d_2$,..., $d_L$ dimensions.\n\n+ **Many-to-one Decoder**. We introduce a simple single-layer MLP as our decoder rather than other complicated architectures. The decoder is peformed on the output spikes of $T$ encoders.\n+ **Small Embedding Size**: As SpikeGCL can accumulate the output spikes across $T$ time steps, it is sufficient to use a small embedding size (e.g., 16) to achieve a decent performance.\n\nIn conclusion, the learnable parameters in SpikeGCL consist of a single GCN and a single MLP, both with small embedding sizes. Compared to other SNNs and GCLs that utilize complex encoders/decoders and large embedding sizes (e.g., 1024), SpikeGCL demonstrates much higher parameter efficiency.\n\n\n\nIf there are any remaining concerns about the presentations of our work, please let us know. We are more than happy to provide additional information to help clarify the situation."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107078623,
                "cdate": 1700107078623,
                "tmdate": 1700107078623,
                "mdate": 1700107078623,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yr3OEzkjmc",
                "forum": "LnLySuf1vp",
                "replyto": "DO6rQLUi0e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2917/Reviewer_bEwP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2917/Reviewer_bEwP"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. Based on authors' reply, I have a few more questions:\n\n1. Based on theorem 1, does it mean there is some redundancy in the node feature? If yes, why not directly apply compression technology on the node features, since the proposed method also has accuracy loss. Furthermore, is it necessary to identify which features should be grouped together? \n\n2. Based on the explanation, I think the T encoders use identical weights, if yes please clarify this in the document. \n\n3. (minor  & not necessary in revision) Based on Eq. 31, I cannot understand why the proposed method can achieve such energy saving, I think the encoding part in summation should be d/T? It would be clearer if the author could provide a table that compares the #computation and memory consumption between the proposed method and previous studies (in formula). Also, it would be better to provide the computation portion of the encoder in GNN."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662457374,
                "cdate": 1700662457374,
                "tmdate": 1700662457374,
                "mdate": 1700662457374,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kykct4jyUE",
                "forum": "LnLySuf1vp",
                "replyto": "r1i31ADlgH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2917/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2917/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for the follow-up discussion and all your suggestions. Please find our responses to your latest comments below.\n\n> Based on theorem 1, does it mean there is some redundancy in the node feature? \n\nNo, the core insights of Theorem 1 aim to reveal that SpikeGCL can approximate the performance of its full-precision counterpart (GNNs) with binary spikes in $T$ time steps. We draw this conclusion by demonstrating that **the SNN neuron, such as IF and LIF, in SpikeGCL serves as an unbiased estimator of the ReLU activation function of GNNs over time.**\n\n> Furthermore, is it necessary to identify which features should be grouped together?\n\nThat's a good point. Honestly, we were considering employing clustering methods, such as KMeans, to group the input features. This way, each encoder can learn from a corresponding group with similar features. However, the challenge here is **how to define a new augmentation method to generate negative samples, as the clustering method is permutation-insensitive, and simple random shuffling on features does not work in this case.** Additionally, another potential issue is that grouping similar features together may pose a risk to the learning of downstream encoders, as it may lead to the oversmoothing issue. We leave them as future work.\n\n> Based on the explanation, I think the T encoders use identical weights, if yes please clarify this in the document.\n\nNot exactly. The $T$ encoders use identical weights, **except for the first layer**, to accommodate the diverse input dimensions of different groups of features. This has been clarified in Paragraph **Encoder**, Section 4.3 (page 6)."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739893481,
                "cdate": 1700739893481,
                "tmdate": 1700739942648,
                "mdate": 1700739942648,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JrwJWVhgrU",
                "forum": "LnLySuf1vp",
                "replyto": "r1i31ADlgH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2917/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2917/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (2/2)"
                    },
                    "comment": {
                        "value": "> Based on Eq. 31, I cannot understand why the proposed method can achieve such energy saving, I think the encoding part in summation should be d/T? It would be clearer if the author could provide a table that compares the #computation and memory consumption between the proposed method and previous studies (in formula).\n\nThank you for your question. Firstly, it's important to note that **Eq. 31 primarily focuses on computing the energy in SNN-based methods**. Therefore, in this response, we specifically discussion the energy efficiency of SpikeGCL in comparison to other graph SNNs.\n\n+ The enhanced efficiency of SpikeGCL primarily stems from the improved encoding process.  As mentioned in the paper, prior works typically repeat the graph $T$ times to generate the sequential inputs for SNNs. This leads to high energy consumption $E_\\text{encoding}$ particularly on large-scale graph datasets. In SpikeGCL, we address this issue by partitioning features into $T$ groups.\n+ Following your suggestion, we provide a summary of the energy consumption of various graph SNNs in formula and conduct an empirical comparison of encoding and spiking energy consumption. The evaluation is performed with $T=30$ on the Computers dataset.The results are presented below.\n|   |     |     |     |     |     |\n|---|:---:|:---:|:---:|:---:|:---:|\n|Models     |Energy Consumption|$E_\\text{encoding}$(mJ)|$E_\\text{spiking}$(mJ)|$E$(mJ)||\n|SpikeNet   | $E_{MAC} \\times \\sum_{t=1}^{T}  NDs^t + E_{SOP} \\times \\sum_{t=1}^{T} S_{Sage_1}^t + S_{Sage_2}^t + S_{FC}^t$  | 0.403 | 0.031 | 0.434 ||\n|SpikingGCN | $E_{MAC} \\times \\sum_{t=1}^{T} NDs^t + E_{SOP} \\times \\sum_{t=1}^{T} S_{SGC}^t$ | 0.403 | 0.006 | 0.409 ||\n|GC-SNN     | $E_{MAC} \\times \\sum_{t=1}^{T} Nds^t + E_{SOP} \\times \\sum_{t=1}^{T} S_{GCN_1}^t + S_{GCN_2}^t + S_{SFTN_1}^t + S_{SFTN_2}^t$ | 0.403 | 0.062 | 0.465 ||\n|GA-SNN     | $E_{MAC} \\times \\sum_{t=1}^{T} Nds^t + E_{SOP} \\times \\sum_{t=1}^{T} S_{GAT_1}^t + S_{GAT_2}^t + S_{SFTN_1}^t + S_{SFTN_2}^t$ | 0.403 | 0.055 | 0.458 ||\n|SpikeGCL   | $E_{MAC} \\times Nds + E_{SOP} \\times \\sum_{t=1}^{T} S_{GCN}^t$ | 0.013 | 0.039 | 0.052 ||\n\n+ Here, $s^t$ represents the firing rate of input node features after rate-based encoding at each time step $t$, $S_{*}^t$ denotes the number of output spikes for each layers in the network at time step $t$. \n+ As can be observed, SpikeGCL significantly reduces $E_\\text{encoding}$ and therefore the overall energy consumption is saved.\n+ As $T$ increases and the graph scales, the superiority of SpikeGCL over other graph SNNs would become more pronounced.\n\n---\n\nRegardless, we are deeply grateful for all your previous feedback and advice, which have been invaluable to us."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739967846,
                "cdate": 1700739967846,
                "tmdate": 1700740531601,
                "mdate": 1700740531601,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BzSZ4ISiU8",
            "forum": "LnLySuf1vp",
            "replyto": "LnLySuf1vp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2917/Reviewer_HWYD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2917/Reviewer_HWYD"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents SpikeGCL, a GCL framework built upon SNNs to learn 1-bit binarized graph representations and enable fast inference. The authors shows that Spike GCL achieves high efficiency and reduces memory consumption, and is also theoretically guaranteed with powerful capabilities to learn representations. Extensive experimental results verified that spikeGCL achieves comparable or superior performance to full-precision competitors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper presents a new framework for learning on graph data, the spikeGCL. The paper is well written with clear introduction of the model and the learning algorithm to prevent the vanishing gradient problem, and presents both theoretical guarantees and extensive numerical results to demonstrate the capabilities of the model."
                },
                "weaknesses": {
                    "value": "The paper focuses on the learning algorithm and performance of SpikeGCL, I think it would be interesting to further explore the properties of the 1-bit node representations themselves and compare them with other baseline models, to better understand why the learned graph representations are superior to other binary GNNs."
                },
                "questions": {
                    "value": "How much does the result rely on detailed implementation of the SNN (such as reset to 0/reset by subtraction/IF or LIF)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2917/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2917/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2917/Reviewer_HWYD"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2917/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698718758730,
            "cdate": 1698718758730,
            "tmdate": 1699636235255,
            "mdate": 1699636235255,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6RCSktrZDl",
                "forum": "LnLySuf1vp",
                "replyto": "BzSZ4ISiU8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2917/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2917/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HWYD"
                    },
                    "comment": {
                        "value": "We thank the reviewers for reading our paper and providing detailed review on our submission. We respond to the reviewers\u2019 major concerns and questions below.\n\n> Q1: How much does the result rely on detailed implementation of the SNN (such as reset to 0/reset by subtraction/IF or LIF)?\n\nThank you for your feedback. We appreciate your constructive suggestion. In the submitted version, we have included the ablation results of different spiking neurons (IF, LIF, and PLIF) in Table 7, Appendix F. Additionally, following your suggestion, we have now added the ablation results on the reset mechanism for four datasets. The results are presented below:\n\n|                               | Computers                         | Photo                             | CS                                | Physics                           |\n| ----------------------------- | --------------------------------- | --------------------------------- | --------------------------------- | --------------------------------- |\n| zero-reset + IF               | 88.0\u00b10.2                   | 92.7\u00b10.3                   | 91.2\u00b10.1                   | 95.0\u00b10.2                   |\n| subtract-reset + IF           | 88.1\u00b10.1                   | 92.9\u00b10.2                   | 91.2\u00b10.1                   | 95.0\u00b10.1                   |\n| zero-reset + LIF              | 88.5\u00b10.1                   | 92.9\u00b10.3                   | 92.0\u00b10.2                   | 95.2\u00b10.1                   |\n| subtract-reset + LIF          | 88.6\u00b10.1                   | 92.9\u00b10.1                   | 92.1\u00b10.1                   | **95.3\u00b10.2**              |\n| zero-reset + PLIF             | **89.0\u00b10.3**               | 92.5\u00b10.3                   | 92.1\u00b10.1                   | 95.2\u00b10.1                   |\n| ours (subtract-reset + PLIF) | 88.9\u00b10.3                   | **93.0\u00b10.1**               | **92.8\u00b10.1**               | 95.2\u00b10.6                   |\n\nBased on our findings, the performance does not necessarily depend on the specific implementation details of the SNN architectures. The performance gap between the best architecture and the worst is not significant. Even a simple IF neuron or a reset to 0 mechanism can achieve satisfactory performance.\n\n\n\nWe would be grateful if you tell us whether the response answers your questipns of SpikeGCL, if not, what we are lacking, so we can provide better clarification. Thank you for your time."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700106997662,
                "cdate": 1700106997662,
                "tmdate": 1700452541133,
                "mdate": 1700452541133,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OBngzI0iUZ",
            "forum": "LnLySuf1vp",
            "replyto": "LnLySuf1vp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2917/Reviewer_jihE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2917/Reviewer_jihE"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the challenge of learning full-precision representations in graph neural networks,\nwhich can be computationally and resource-intensive. The authors propose a new approach that\ncombines graph contrastive learning with spiking neural networks to improve efficiency and accuracy.\nThe proposed framework, SPIKEGCL, learns binarized 1-bit representations for graphs and provides\ntheoretical guarantees to demonstrate its comparable expressiveness with full-precision counterparts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation is clear. The paper combines graph contrastive learning with spiking neural\nnetworks to improve efficiency and accuracy.\n2. The proposed method is tested on several benchmarks.\n3. The paper is well-written and provides a promising direction for graph contrastive learning with\nspiking neural networks."
                },
                "weaknesses": {
                    "value": "1. The author divided the original graph in time in the feature dimension and obtained T graph\nstructures with the same structure and reduced the node feature dimension to N/T. Compared with\ncopying T copies, it saves storage resources. However, the author did not explain the reason for\nthis approach. For example, from my personal understanding, the author's approach can be\nunderstood as for a 1xd feature vector, there is a temporal relationship between the 0th value and\nthe N/T-th value, which we can\u2019t understand.\n2. The author used the SNN method to compress the original representation. One problem is that\nSNN considers the accumulation in time and does not take into account the distribution\ncharacteristics in time. What I mean is, if the characteristics of time T-1 and time T-2 are\nexchanged. It seems that the value of time T will not be affected, but they will become two\ncompletely different vectors. In this way, will there be a many-to-one situation during the\ncompression process?\n3. I think the article lacks some quantitative analysis, such as what is the connection between the\ncompressed binary vector and the original vector, what is the distribution of the conventional"
                },
                "questions": {
                    "value": "Please see the weakness section for detailed questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2917/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823846893,
            "cdate": 1698823846893,
            "tmdate": 1699636235164,
            "mdate": 1699636235164,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L0uByxjgDD",
                "forum": "LnLySuf1vp",
                "replyto": "OBngzI0iUZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2917/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2917/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jihE"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for detailed comments and suggestive feedback. We want to clarify some misunderstandings that caused some of your concerns.\n\n\n\n> Q1: The feasibility of feature partitioning for SNNs.\n\nThank you for your insightful question. While SNNs require sequential inputs, they do not necessarily require a strict temporal relationship between individual inputs like RNNs. In SNNs, the main operation is to accumulate inputs to generate spikes through an integrate-and-fire process. The occurrence of spikes carries information about the input data, where the number of spikes fired in the output of encoders reflects the importance of input feature groups. \n\nIn SpikeGCL, we leverage the **spikes in all time steps** by concatenating them to obtain node representations. As a result, the precise timing or order of spikes is not always crucial as the relative timing and occurrence of spikes can still capture essential information.\n\nIn the literature, there have been successful attempts, such as SpikingGCL and GC-SNN, where SNNs are employed on non-temporal graphs by augmenting them to form the sequential inputs.\n\n\n\n> Q2: If the characteristics of time T-1 and time T-2 are exchanged. It seems that the value of time T will not be affected, but they will become two completely different vectors. In this way, will there be a many-to-one situation during the compression process?\n\nThere seems to be a misunderstanding regarding the mechanism of SpikeGCL. While a strict temporal relationship between inputs is not necessary, the input order does matter for SpikeGCL because the SNN neuron determines whether to fire-and-reset based on the input at the current time step. Therefore, if the characteristics of time T-1 and time T-2 are exchanged, it will affect the value at time T.\n\nTechnically, SNNs are a many-to-many architecture, where each time step is associated with a fired output spike. Hence, there won't be a many-to-one situation during the compression process.\n\n\n\n> Q3: Quantitative analysis of the connection between the compressed binary vector and the original vector.\n\nThank you for the insightful suggestion. We have included the experiments regarding the similarity between the compressed binary vector and the original input vector in **Appendix F, page 23 of the revised manuscript.** The results show that each group of features exhibits a strong correlation with the corresponding output spikes while demonstrating minimal correlation with spikes in other time steps. This suggests that the learned binary representations are disentangled from each other, thereby providing improved expressiveness in representing the input features.\n\n\n\nIf any concern still remains that might prohibit a positive recommendation of this work, we would appreciate if you could let us know now."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700106974823,
                "cdate": 1700106974823,
                "tmdate": 1700106974823,
                "mdate": 1700106974823,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Fo5JXcyc1I",
            "forum": "LnLySuf1vp",
            "replyto": "LnLySuf1vp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2917/Reviewer_n3GK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2917/Reviewer_n3GK"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel graph contrastive learning (GCL) framework called SPIKEGCL, which leverages sparse and binary characteristics to learn more biologically plausible and compact representations. The proposed framework outperforms many state-of-the-art supervised and self-supervised methods across several graph benchmarks, achieving nearly 32x representation storage compression. The paper also provides experimental evaluations and theoretical guarantees to demonstrate the effectiveness and expressiveness of SPIKEGCL."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThis paper propose a novel GCL framework called SPIKEGCL that leverages sparse and binary characteristics to learn more biologically plausible and compact representations. \n2.\tThis paper provides theoretical guarantees to demonstrate the expressiveness of SPIKEGCL.\n3.\tSpikeGCL nearly 32x representation storage compression and outperforming many state-of-the-art supervised and self-supervised methods across several graph benchmarks. \n4.\tExtensive experimental evaluations to demonstrate the effectiveness of the proposed framework."
                },
                "weaknesses": {
                    "value": "1.\tIn Section 4.1, to reduce the complexity of SNNs by sampling from each node, the authors uniformly partition the node features into T groups, which is unreasonable. Features of different dimensions may represent different meanings, and operations after grouping these features may lead to inconsistencies in the feature space between different groups. On the contrary, the traditional mask method retains most features by randomly masking some features, ensuring the consistency of feature distribution. Therefore, in this section, the author can consider using random masks to reduce computational complexity while ensuring the consistency of data distribution.\n2.\tIn table 2, the authors compare the parameter size and energy consumption between proposed method with traditional unsupervised/self-supervised mehtods. Howvere, from table 1, the performance of spikeGCL is worse than the spike-based mehtods in most cases, there\u2019s no evidence that SpikeGCL is better than other mehtods. The authors should add the comparision between SpikeGCL with spike-based mehtods in Table 2.\n3.\tAn intuitive question: Contrastive learning usually generates rich features from multiple perspectives to represent the target. However, spike-based methods usually lose a large amount of data, that is, a large number of learnable features are lost. Why can SpikeGCL still achieve similar results compared with traditional contrastive learning methods?\n4.\tGenerally speaking, combining Spiking and GCL is a good idea, but the novelty is not enougt. Compared with traditional methods, SpikeGCL only groups features and then uses the traditional GCL method for learning, which does not present the special nature of contrastive learning in the scenario where spike and graph are combined."
                },
                "questions": {
                    "value": "check the comments above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2917/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825157801,
            "cdate": 1698825157801,
            "tmdate": 1699636235035,
            "mdate": 1699636235035,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HivHtsdzHg",
                "forum": "LnLySuf1vp",
                "replyto": "Fo5JXcyc1I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2917/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2917/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer n3GK (part 1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewers for reading our paper and providing detailed review on our submission. We respond to the reviewers\u2019 concerns and questions one by one.\n\n>  Q1: (1) The authors uniformly partition the node features into T groups, which is unreasonable. (2) the author can consider using random masks to reduce computational complexity while ensuring the consistency of data distribution.\n\nWe believe there are some misunderstandings on our work. To clarify the situation, here we take a simple MLP (without bias terms) as the encoder. Given an input d-dimensional vector $X$, the output $y$ is defined as: $y= X W$, where $W \\in \\mathbb{R}^{d \\times f}$ and $f$ is the output dimension. Typically, it is equivalent to:\n$$\n\\begin{align}\ny = y1 || y2 || ...||yT, \\quad\ny_t= X_t W_t\n\\end{align}\n$$\n\nWhere $X_t$ is the $t$-th group of features with dimension $\\frac{d}{T}$ and $W_t \\in \\mathbb{R}^{\\frac{d}{T} \\times f}$ is the learnable weight matrix of the $t$-th encoder. Each group of features is independently learned and this does not lead to any inconsistence in the feature space across different groups. \n\nWe appreciate the reviewer\u2018s suggestion. While using random masks can be a valid approach to create a sequential input for SNNs, **it is not beneficial for reducing computational complexity.** In fact, it is worth noting that SpikingGCN has employed this technique to generate $T$ graphs, each with $d$-dimensional feature inputs. However, as discussed in Sec. 4.1, this method remains a significant bottleneck for SNNs when scaling to large graphs.\n\n\n\n>  Q2: From table 1, the performance of spikeGCL is worse than the spike-based mehtods in most cases, there\u2019s no evidence that SpikeGCL is better than other mehtods. The authors should add the comparision between SpikeGCL with spike-based mehtods in Table 2.\n\nFirst of all, we would like to point out that the comparision between SpikeGCL with spike-based mehtods has been presented in **Table 5 of Appendix F.** In Table 5,  SpikeGCL demonstrates an average energy consumption that is approximately 50x lower, while using only 1/3 of the parameters compared to other spike-based methods. \n\nSecondly,  it is important to note that SpikeGCL is an **unsupervised (U) and binarized (B)** method, which is expected to underperform **supervised and full-precision** spike-based baselines. We understand that there is a tendency in the deep learning community to place a strong emphasis on achieving state-of-the-art performance on benchmark datasets, often measured in terms of specific performance metrics. In our work, SpikeGCL, which is an energy and memory-efficient method, demonstrates only a minor performance decrease (less than 3%) and even surpasses most supervised and full-precision spike-based baselines in 4 out of 9 datasets, as indicated in Table 1 and Table 4.\n\nWhile outperforming state-of-the-art methods can be an important and exciting goal for many researchers, it is also important to consider the broader context such as efficiency on the wider deep learning community. **SpikeGCL is not proposed as a new state-of-the-art SNN method. It is designed to achieve satisfied performance in low-power and resource-constrained settings, addressing the challenges posed by the increasing scale of real-world graph data.**"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700106827004,
                "cdate": 1700106827004,
                "tmdate": 1700106923478,
                "mdate": 1700106923478,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YNRAEYxG8c",
                "forum": "LnLySuf1vp",
                "replyto": "Fo5JXcyc1I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2917/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2917/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer n3GK (part 2/2)"
                    },
                    "comment": {
                        "value": ">  Q3: Spike-based methods usually lose a large amount of data, that is, a large number of learnable features are lost. Why can SpikeGCL still achieve similar results compared with traditional contrastive learning methods? Why can SpikeGCL still achieve similar results compared with traditional contrastive learning methods?\n\nWe acknowledge that spike-based methods (SNNs) typically involve a loss of data compared to traditional rate-based methods employed in artificial neural networks (ANNs). This discrepancy arises from the fact that SNNs represent information using discrete spikes or events rather than continuous firing rates. However, it is worth noting that there have been studies[1,2] demonstrating that SNNs can approximate a ReLU network when given reasonably large time steps $T$.\n\nIn our work, we also provide theoretical guarantees regarding the expressiveness of SpikeGCL. Specifically, we show that SpikeGCL has the capability to (almost) approximate a comparable GNN when utilizing a large time step $T$. As a result, SpikeGCL has the potential to achieve comparable results to traditional GCL methods. Please kindly refer to Sec. 5 and Appendix A for further detailes.\n\n[1] Rueckauer B, Lungu I-A, Hu Y, Pfeiffer M and Liu S-C (2017) Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification. Front. Neurosci. 11:682.\n\n[2] Rueckauer, B., Lungu, I. A., Hu, Y., & Pfeiffer, M. (2016). Theory and tools for the conversion of analog to spiking convolutional neural networks. arXiv preprint arXiv:1612.04052.\n\n\n\n> Q4: Novelty of SpikeGCL.\n\nWe respectfully disagree with the reviewer that the novelty of our paper is limited. Here we would like to reiterate the three research contributions of our work:\n\n- **[Novelty 1]** The first-ever binary GCL method leveraging the sparse and energy-efficient characteristics of SNNs.\n- **[Novelty 2]** The theoretical analysis about why SpikeGCL can approximiate the performance of full-precision GCLs.\n- **[Novelty 3]** The block-wise training scheme on SNNs prevent them from suffering from the notorious problem of vanishing gradients with large time steps $T$.\n\nParticularly, **Novelty 2 and 3 are built upon the special nature of SNNs and are proposed to solve the unique challenges in the context of GCL.** Our work digs deeper into the SNNs in GCL and offers more insights that would potentially benefit the whole community, which could motive more interesting research in this area. We believe this is an interesting and insightful work and hope you could reevaluate the contributions of our work. If there are any remaining concerns about the novelty of our work, please let us know. We are more than happy to provide additional information to help clarify the situation. \n\n\n\nThank you again for taking the time to review our paper. We hope our responses could clarify your concerns, and hope you will consider increasing your score. If we have left any notable points of concern unaddressed, please do share and we will attend to these points."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2917/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700106877129,
                "cdate": 1700106877129,
                "tmdate": 1700106936217,
                "mdate": 1700106936217,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]