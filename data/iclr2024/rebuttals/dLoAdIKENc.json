[
    {
        "title": "Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks"
    },
    {
        "review": {
            "id": "QvOnxAP7cl",
            "forum": "dLoAdIKENc",
            "replyto": "dLoAdIKENc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6287/Reviewer_APfc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6287/Reviewer_APfc"
            ],
            "content": {
                "summary": {
                    "value": "The paper analyzes the robustness of two common AI-generated image detection approaches: watermarking and classification. For watermarking, it points out that diffusion purification can effectively remove low-perturbation budget watermarks but fails to work on high-perturbation budget ones. For the latter one, the paper proposes using a model-substitution adversarial attack to remove the watermarks. It also proposes a spoofing attack against watermarking by blending a watermarked noise image with the non-watermarked target image. Finally, it demonstrates a trade-off between the robustness and reliability of classification-based deepfake detectors."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper lacks a clearly identifiable strength."
                },
                "weaknesses": {
                    "value": "The paper is poorly written and not well-organized. For example, acronyms were defined far after their first usage. Figures are hard to follow and understand.\n \nThe contribution of the paper is unclear. It is common sense pointed out by several previous research that the stronger the perturbations, the more difficult for purification. Using random noise or adversarial perturbations to compromise the machine-learning-based forensics models (watermarking and deepfake detection) has been studied in the past, which is also mentioned in the related work section.\n \nThe experiment settings are insufficient to demonstrate the claimed problems. Besides DiffPure (Nie et al., 2022), there are other diffusion-based approaches, such as DDNM (Wang et al., 2022), and non-diffusion-based ones, which are not investigated.\n \nThe robustness of the proposed attack methods, which is an important property, was not evaluated. DiffPure (Nie et al., 2022), JPEG compression, or Gaussian blur can be used to mitigate such attacks, although it may slightly degrade the clean accuracy.\n\nThe paper should include a paragraph of ethics statement."
                },
                "questions": {
                    "value": "Please refer to the comments in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6287/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6287/Reviewer_APfc",
                        "ICLR.cc/2024/Conference/Submission6287/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6287/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698506718341,
            "cdate": 1698506718341,
            "tmdate": 1700725728348,
            "mdate": 1700725728348,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Yq6baAgCEW",
                "forum": "dLoAdIKENc",
                "replyto": "QvOnxAP7cl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6287/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your insightful comments. We hope the following responses can help address your concerns.\n\n> **Concern 1:** The paper is poorly written and not well-organized.\n\nWe have updated several parts in the new version of the draft, in order to increase clarification and organization.\n- We added reference to the definition of terms that are formally defined in the paper after their usage, including $e_0, e_1$ in the introduction, and $R, F, R^t, F^t$ in Figure 1.\n- The Y-axis label of Figure 2 had a typo which is fixed now. New labels were added to Figure 1 to enhance readability.\n- New references were added to Section 3.1, to help the readers get familiar with the diffusion model terms that are being used in the section.\n- More information about the importance of Figure 2 was added to Section 3.1.\n- More explanation on the reason for the usage of denoising diffusion models in our theory was added to Section 3.1 and Appendix B. It is explained that other denoising techniques could also be used instead of denoising diffusion models, and references to related work were added.\n- A more detailed description of the spoofing attack, together with a pseudocode has been added to Appendix A.4.\n- In Section 4, formal definitions for some of the terms that are used in Theorem 2 were added to the main text.\n\n> **Concern 2:** The contribution of the paper is unclear. \n\nOur paper includes several noteworthy contributions. \n\n- Our first and perhaps most important contribution is the theoretical impossibility of designing reliable and imperceptible watermarks  (i.e., Theorem 1 in the paper). Lots of previous studies use image watermarks for different applications, such as detecting AI-made images, protecting copyrighted images, and establishing image ownership. In all but a few cases, these watermarking techniques utilize imperceptible perturbations. Our theoretical findings show the fundamental unreliability of such techniques and suggest a paradigm shift toward watermarks with higher perturbation budgets within the research community.\n- Our second contribution is the development of a novel model substitution black-box adversarial attack that breaks existing high perturbation budget watermarking techniques. This approach allows for attacking watermarking methods without requiring online query access to the models. To the best of our knowledge, the watermarking technique TreeRing has previously withstood black-box attacks in the literature, establishing itself as a reliable method. However, our attack successfully breaks this technique with an $\\ell_\\infty$ perturbation as small as 2/255.\n- Our third contribution is developing a spoofing attack against watermarking by adding a watermarked noise image to non-watermarked images to deceive the detector into flagging them as watermarked.\n- Our fourth and final contribution is to characterize a fundamental trade-off between the robustness and reliability of deepfake detectors and substantiate this concept through experiments.\n\n\n> **Concern 3:** Besides DiffPure (Nie et al., 2022), there are other diffusion-based approaches, such as DDNM (Wang et al., 2022), and non-diffusion-based ones, which are not investigated.\n\nWe appreciate the reviewer's observation regarding the potential use of alternative denoising techniques to eliminate the introduced Gaussian noise from images. We have added this explanation to the new draft of the paper (in Section 3.1 and Appendix B), while citing some related work.\n\nOur theoretical findings establish an upper bound on the total variation of the distributions of watermarked and non-watermarked images by adding Gaussian noise to images. It is important to note that we do not rely on the denoising technique to calculate these bounds; rather, we employ them solely to generate high-quality output images. Consequently, any denoising technique, aside from DiffPure, can be employed to achieve the same theoretical bounds.\n\nIt is true that a stronger denoising technique permits the use of a higher magnitude of Gaussian noise, resulting in a more substantial lower bound on the error according to our theory. Nonetheless, it's worth emphasizing that DiffPure has already proven its capability to empirically break existing imperceptible watermarks, showing the effectiveness of our proposed methodology."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575778520,
                "cdate": 1700575778520,
                "tmdate": 1700575778520,
                "mdate": 1700575778520,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2C6RNTw0At",
                "forum": "dLoAdIKENc",
                "replyto": "K4RoRtvzGR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6287/Reviewer_APfc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6287/Reviewer_APfc"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their responsiveness in addressing my inquiries and for revising the paper. \n\n> Our first and perhaps most important contribution is the theoretical impossibility of designing reliable and imperceptible watermarks (i.e., Theorem 1 in the paper). Lots of previous studies use image watermarks for different applications, such as detecting AI-made images, protecting copyrighted images, and establishing image ownership. In all but a few cases, these watermarking techniques utilize imperceptible perturbations. Our theoretical findings show the fundamental unreliability of such techniques and suggest a paradigm shift toward watermarks with higher perturbation budgets within the research community.\n\nThe above statement, which is the most important contribution of the paper, is common knowledge in the watermarking community [A]. It is also common knowledge in the adversarial machine learning community that there is a trade-off between the attack ability and the imperceptibility of the perturbations. Recent work also demonstrated that invisible image watermarks are removable [B].\n\n\n> Our second contribution is the development of a novel model substitution black-box adversarial attack that breaks existing high perturbation budget watermarking techniques.\n\nThe idea of using model substitution for black-box adversarial attacks is also not new [C].\n\n\nWhile I appreciate the effort put into the manuscript, because of the above reasons, regrettably, I must maintain my score. I wish the authors every success in their subsequent submission with their further improvements.\n\n*References:*\n\n[A] Agarwal, Namita, Amit Kumar Singh, and Pradeep Kumar Singh. \"Survey of robust and imperceptible watermarking.\" Multimedia Tools and Applications 78 (2019): 8603-8633.\n\n[B] Zhao, Xuandong, Kexun Zhang, Zihao Su, Saastha Vasan, Ilya Grishchenko, Christopher Kruegel, Giovanni Vigna, Yu-Xiang Wang, and Lei Li. \u201cInvisible Image Watermarks Are Provably Removable Using Generative AI.\u201d arXiv, August 6, 2023.\n\n[C] Papernot, Nicolas, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami. \"Practical black-box attacks against machine learning.\" Asia conference on computer and communications security, pp. 506-519. 2017."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629074848,
                "cdate": 1700629074848,
                "tmdate": 1700629074848,
                "mdate": 1700629074848,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "087GldQIe4",
                "forum": "dLoAdIKENc",
                "replyto": "6OxSnvxX2M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6287/Reviewer_APfc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6287/Reviewer_APfc"
                ],
                "content": {
                    "comment": {
                        "value": "I extend my gratitude once more to the authors for their prompt responses.\n\nUpon thorough reflection, I made a slight adjustment to the score. Despite this, achieving a positive rating remains challenging due to the high standards set by ICLR. Nevertheless, I wish the author continued success."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725739076,
                "cdate": 1700725739076,
                "tmdate": 1700725739076,
                "mdate": 1700725739076,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PhVze7B7Hf",
            "forum": "dLoAdIKENc",
            "replyto": "dLoAdIKENc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6287/Reviewer_1Mes"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6287/Reviewer_1Mes"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes the robustness of various AI image detectors, including watermarking and classifier-based deepfake detectors, wherein watermarking is considered a promising method for identifying AI-generated images. \n\nThe paper also evaluates the trade-off between evasion error rate and spoofing error rate in watermarking methods, introducing subtle image perturbations. Besides, it is demonstrated that a diffusion purification attack that amplifies the error rates of low perturbation budget watermarking methods, thereby revealing the fundamental limits of the robustness of image watermarking methods. For large-perturbation watermarking methods, the diffusion purification attack is ineffective. Therefore, the authors propose a model substitution adversarial attack to successfully remove watermarks.\n\nOverall, this paper makes significant contributions to the robustness of AI image detection methods, supported by detailed theoretical proofs of the viewpoints presented. Some of the theoretical analyses and attack methods in the article are relatively complex and may require readers to have a certain level of background knowledge to fully understand."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) Originality: The paper challenges the limitations of existing watermarking techniques by proposing new attack methods, driving progress in the field.\n\n2) Quality: The quality of the paper is very high, with in-depth and rigorous theoretical analysis, reasonable experimental design, and results that fully validate the theoretical analysis.\n\n3) Clarity: The structure of the paper is clear, the logic is tight, and the discussion is detailed and easy to understand. However, some of the complex theoretical analyses and attack methods may require readers to have a certain level of background knowledge.\nImportance: By revealing the vulnerabilities of existing watermarking techniques, the paper lays the groundwork for further research and development in the field. Additionally, by proposing new attack methods, the paper challenges the limitations of existing watermarking technologies and promotes progress in the field."
                },
                "weaknesses": {
                    "value": "1) Complexity of Theoretical Analysis: \nWhile the paper provides an in-depth theoretical analysis, the complexity of these analyses might pose challenges for some readers, especially those who are not familiar with diffusion models and the theoretical underpinnings of adversarial attacks. Certain sections of the paper may appear somewhat opaque to these readers. It would be beneficial if the authors could simplify the explanations or provide additional resources to aid understanding.\n\n\n2) Complexity of Theoretical Analysis: \nWhile the paper provides an in-depth theoretical analysis, the complexity of these analyses might pose challenges for some readers, especially those who are not familiar with diffusion models and the theoretical underpinnings of adversarial attacks. Certain sections of the paper may appear somewhat opaque to these readers. It would be beneficial if the authors could simplify the explanations or provide additional resources to aid understanding."
                },
                "questions": {
                    "value": "1) Problem Description: The experiments conducted in the article primarily utilize the ImageNet dataset, potentially limiting the generalizability of the results. \nRecommendation: In future work, conduct experiments using multiple datasets from various fields and sources to validate the effectiveness and stability of the method.\n\n2) Problem Description: The theoretical analysis presented in the article is quite complex, which may be challenging for all readers to comprehend. \nRecommendation: Provide more intuitive explanations and examples, and simplify some of the theoretical derivations to make them more accessible and easier to understand."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6287/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6287/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6287/Reviewer_1Mes"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6287/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698652403976,
            "cdate": 1698652403976,
            "tmdate": 1699636689783,
            "mdate": 1699636689783,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RPos1eaJh5",
                "forum": "dLoAdIKENc",
                "replyto": "PhVze7B7Hf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6287/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your helpful comments. We hope the following responses can help address your concerns.\n\n>  **Weakness 1:** It would be beneficial if the authors could simplify the explanations or provide additional resources to aid understanding.\n\nIn the new version of our draft, we have added a reference to [1] (in Section 3.1) which provides comprehensive definitions for diffusion models and the terms that are used in our paper. Most of the terms that we use in our work such as $x_0$, $x_t$, $\\bar{\\alpha}_t$ are consistent with this survey paper.\n\n>  **Question 2:** Provide more intuitive explanations and examples, and simplify some of the theoretical derivations to make them more accessible and easier to understand.\n\nTo enhance the clarity of the theoretical parts of the paper, we have added more explanations to Sections 3 and 4, and the appendix. We added the formal definition of the terms used in Theorem 2 to the main text of Section 4. Additionally, we clarified the reason for the usage of denoising diffusion models in our theory, and how it can be replaced with other denoising techniques (Section 3.1, and Appendix B).\n\n> **Question 1:** In future work, conduct experiments using multiple datasets from various fields and sources to validate the effectiveness and stability of the method.\n\nWe agree with the reviewer\u2019s comment that including more datasets could support the generalizability of our work. Adding new datasets can be time-consuming, especially since some of the watermarking methods require training of their models on the target dataset. Nonetheless, it will be one of our priorities to include more datasets such as COCO and CIFAR in our work, hopefully for the final draft of the paper.\n\n\n\n [1] Understanding Diffusion Models: A Unified Perspective, Calvin Luo 2022"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575747923,
                "cdate": 1700575747923,
                "tmdate": 1700575747923,
                "mdate": 1700575747923,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eR7KISznZC",
            "forum": "dLoAdIKENc",
            "replyto": "dLoAdIKENc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6287/Reviewer_h5mn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6287/Reviewer_h5mn"
            ],
            "content": {
                "summary": {
                    "value": "This work investigates the resilience of AI-image detection methods, focusing on watermarking and classifier-based deepfake detectors. The authors highlight the crucial need to distinguish between authentic and AI-generated content due to the rising threat of fake materials being used as genuine ones. They reveal a fundamental trade-off in the effectiveness of watermarking techniques, showcasing the limitations of low-perturbation and high-perturbation watermarking methods. Specifically, they propose diffusion purification as a certified attack against low-perturbation watermarks and a model substitution adversarial attack against high-perturbation watermarks. Additionally, the paper emphasizes the vulnerability of watermarking methods to spoofing attacks, which can lead to the misidentification of authentic images as watermarked ones. Finally, the authors extend their analysis to classifier-based deepfake detectors, demonstrating a trade-off between reliability and robustness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strengths of this work are as follows:\n1. Comprehensive analysis: The paper provides a comprehensive analysis of the robustness of AI-image detection methods, focusing on both watermarking and classifier-based deepfake detectors. This thorough investigation helps in understanding the limitations and vulnerabilities of these methods.\n2. Practical attacks: The paper introduces practical attacks, such as diffusion purification and model substitution adversarial attacks, to illustrate the vulnerabilities of different watermarking methods. \n3. Clarity in trade-offs: The paper effectively highlights the trade-offs between various aspects of AI-image detection methods, such as the trade-off between evasion error rate and spoofing error rate in the case of watermarking methods. This clarity helps in understanding the challenges associated with designing robust AI-image detection systems.\n4. Sound theoretical study and guidelines for designing robust watermarks: The paper offers insights into the attributes that a robust watermark should possess, including significant perturbation, resistance to naive classification, and resilience to noise from other watermarked images. These guidelines can serve as a valuable reference for researchers and developers working on improving the security and reliability of AI-image detection methods."
                },
                "weaknesses": {
                    "value": "The experimental results are missing a key element, specifically the PSNR, SSIM, or other image quality metrics comparing the diffusion-purified or adversarially attacked images with the original images. This result is crucial as adversaries aim to eliminate watermarks while preserving high-quality images simultaneously."
                },
                "questions": {
                    "value": "Please refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6287/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699173072699,
            "cdate": 1699173072699,
            "tmdate": 1699636689620,
            "mdate": 1699636689620,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OjGqObyMfF",
                "forum": "dLoAdIKENc",
                "replyto": "eR7KISznZC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6287/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your positive feedback. We hope the following response can help address your concern.\n\n> **Weakness 1:** The experimental results are missing a key element, specifically the PSNR, SSIM, or other image quality metric\n\nWe appreciate your valuable suggestion, which was an aspect we overlooked in our experiments. Below, you'll find the PSNR and SSIM values for images subjected to both DiffPure and adversarial attacks. In the case of the DiffPure attack, the output images with t=0.2 exhibit reasonable quality, while the attack significantly lowers the AUROC for imperceptible watermarking techniques, as shown in Figure 3 of the paper. As for the adversarial attack, the epsilon values employed in the paper align with standard perturbation budgets in the adversarial robustness literature. As indicated in the tables, these values result in images with reasonable quality. We have added these tables to the appendix of our paper.\n\n[mean PSNR of images attacked with DiffPure w.r.t t]\n| Method      | t=0.1 | t=0.2 | t=0.3 |\n| ----------- | ---- | ---- | ---- | \n| WatermarkDM | 30.33 | 26.41 | 23.87|\n| MBRS        | 29.96 | 26.23 | 23.76 |\n| RivaGAN     | 29.77 | 26.10| 23.61|\n| DwtDct      | 29.64| 26.03  | 23.70 |\n| DwtDctSVD   | 29.69 | 26.08| 23.60|\n| TreeRing    | 32.45 | 28.27 | 25.49 |\n| StegaStamp  | 30.35 | 26.52| 24.08|\n\n[mean SSIM of images attacked with DiffPure w.r.t t]\n| Method      | t=0.1 | t=0.2 | t=0.3 |\n| ----------- | ---- | ---- | ---- | \n| WatermarkDM | 0.86 | 0.75| \t0.66 |\n| MBRS        | 0.83 | 0.73| 0.64|\n| RivaGAN     | 0.83| 0.72 |0.63|\n| DwtDct      | 0.83| 0.72 | 0.63|\n| DwtDctSVD   | 0.83|0.72|0.63|\n| TreeRing    | 0.92 | 0.86| 0.81|\n| StegaStamp  | 0.84|0.73|0.64|\n\n[mean PSNR of adversarially attacked images w.r.t epsilon]\n| Method      | eps=4 | eps=8 | eps=12 |\n| ----------- | ---- | ---- | ---- | \n| TreeRing    | 36.60 | 32.41 | 30.18|\n| StegaStamp  | 36.11  |31.20|28.20|\n\n[mean SSIM of adversarially attacked images w.r.t epsilon]\n| Method      | eps=4 | eps=8 | eps=12 |\n| ----------- | ---- | ---- | ---- | \n| TreeRing    | 0.96 | 0.90 |0.86|\n| StegaStamp  | 0.95 |0.86|0.75|"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575711417,
                "cdate": 1700575711417,
                "tmdate": 1700575711417,
                "mdate": 1700575711417,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "577wNrUarr",
                "forum": "dLoAdIKENc",
                "replyto": "OjGqObyMfF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6287/Reviewer_h5mn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6287/Reviewer_h5mn"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thanks for your efforts in addressing my concerns. Based on the responses and evaluation of the work, I would like to maintain my rating."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666003571,
                "cdate": 1700666003571,
                "tmdate": 1700666003571,
                "mdate": 1700666003571,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XgiPAH8B6N",
            "forum": "dLoAdIKENc",
            "replyto": "dLoAdIKENc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6287/Reviewer_pjit"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6287/Reviewer_pjit"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce a diffusion purification attack to break AI-image detection methods using watermarking with a low perturbation budget. For high perturbation image watermarking, they develop a model substitution adversarial attack. Besides, they successfully implemented a spoofing attack by adding a watermarked noise image with non-watermarked ones. Furthermore, they use comprehensive experiments to substantiate the trade-off between robustness and reliability of deepfake detectors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "A novel watermarking erasing method is proposed to break high perturbation budget watermarking like Tree Ring or StegaStamp.\nThis paper is well-written and easy to understand. The presentation and organization of the paper are good."
                },
                "weaknesses": {
                    "value": "1.\tThe crux of this paper is theorem 1, which gives a lower bound for the sum of evasion and spoofing errors with regard to the Wasserstein distance between diffusion purification processed images. However, the empirical studies are not sufficient.\n2.\tThe empirical studies for theorem 2 are unpractical. It is almost impossible to add noise to the interior feature maps directly in practical cases."
                },
                "questions": {
                    "value": "1. In Fig. 6, authors adopt four low perturbation budget watermarking, including DWTDCT, DWTDCTSVD, RivaGAN, WatermarkDM, to validate theorem 1. More watermarking methods are required to be considered, like reference [1]. Many cutting-edge watermarking methods are absent in Fig. 6.\n2. The experimental setup of section 4 is impractical. I suggest authors add noise to spatial images directly instead of interior feature maps. \n3. The title of the paper is \u201cAI-image detectors\u201d. The authors only consider fake facial images in section 4. AI-generated facial detectors are only a small part of AI-image detectors. Therefore, more AI-image detectors for arbitrary contexts require consideration.\n\n[1] MBRS : Enhancing Robustness of DNN-based Watermarking by Mini-Batch of Real and Simulated JPEG Compression"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6287/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699342877342,
            "cdate": 1699342877342,
            "tmdate": 1699636689487,
            "mdate": 1699636689487,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AOUra1sMEx",
                "forum": "dLoAdIKENc",
                "replyto": "XgiPAH8B6N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6287/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6287/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your valuable comments. We hope the following responses can help address your concerns.\n\n> **Question 1:** More watermarking methods are required to be considered.\n\nWe are thankful for the reviewer's suggestion. We have included results for the MBRS method in our paper, as recommended, and we are working on adding more methods such as StableSignature [1] and HiDDeN [2] to the final draft. We would like to highlight that our DiffPure attack is not just an empirical attack; it comes with a theoretical guarantee (Theorem 1) establishing a clear trade-off between type I and type II errors for any imperceptible watermarking methods. This, in our view, reduces the urgency for an exhaustive evaluation of every imperceptible watermarking technique that currently exists. \n\n> **Question 2:**  The experimental setup of section 4 is impractical. I suggest authors add noise to spatial images directly instead of interior feature maps.\n\nThank you for the comment. In Appendix F (Figure 19), we add new experiments (in purple text) where noise is added to the images. This result shows that adding small noises to the image space can cause high $\\ell_2$ perturbations in the latent space. Also, note that the tradeoff we present in Figure 8 (Theorem 2) will be more significant as AI images become more realistic and the Wasserstein distance between real and fake distributions decreases. This means that it will become easier to affect the performance of image detectors by adding lower levels of noise to the images as generative image models evolve.\n\n> **Question 3:** More AI-image detectors for arbitrary contexts require consideration.\n\nOur theory in Section 4 is general and holds for any real and fake distributions. We consider an application of deepfakes and perform our experiments on facial images since they are popular in the research community. We take your feedback positively and are working on adding more experiments on a different dataset. We will update the manuscript if we finish our experiments before the discussion period ends. Otherwise, we will add these results to the final version of the paper. \n\n\n[1] The Stable Signature: Rooting Watermarks in Latent Diffusion Models, Fernandez et. al., 2023\n\n[2] HiDDeN: Hiding Data With Deep Networks, Zhu et. al., 2018"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6287/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575693517,
                "cdate": 1700575693517,
                "tmdate": 1700575693517,
                "mdate": 1700575693517,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]