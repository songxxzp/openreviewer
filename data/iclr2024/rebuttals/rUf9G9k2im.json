[
    {
        "title": "Image Inpainting via Iteratively Decoupled Probabilistic Modeling"
    },
    {
        "review": {
            "id": "lkg4WjBKkT",
            "forum": "rUf9G9k2im",
            "replyto": "rUf9G9k2im",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2546/Reviewer_rxeq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2546/Reviewer_rxeq"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a novel method to tackle high-resolution image inpainting problems using an iteratively decoupled probabilistic modeling approach. In particular, given a masked image, the proposed method iteratively inpaints masked pixels by a GAN model, as well as the variance of the generated pixel. By keeping the low variance pixels the masked area is reduced. Then this step is repeated until the entire masked area is filled. \n\nThe authors did comprehensive evaluations with other methods and showed that the proposed method achieves the best performance and is more computationally efficient."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The originality of the proposed method mainly comes from the decoupling of mean and variance estimation for each pixel. In particular, at each iteration, the variance gives a metric to select the high-quality pixels as the input of the next iteration so that GAN artifacts are reduced. Also, since at each iteration, the entire image is generated, the whole process is scalable to high-resolution and large-hole inpainting problems. The experiments show strong numerical evidence of the performance of the method. Overall the paper is well-written and solid."
                },
                "weaknesses": {
                    "value": "Overall the paper is well-written and sound. I have some specific questions regarding the math expressions (see the sections below)."
                },
                "questions": {
                    "value": "1. In Eq. 1, does $\\mathcal(N)$ mean the density function of the Gaussian distribution?\n2. In Eq. 1, what's the relationship between the $y$ being integrated and the $y$ in the integral bounds? Are they the same or different?\n3. In Eq. 1, where does $y_i$ come from during training? Is it the ground-truth unmasked image or the GAN-generated one? If it is the latter, it also depends on $\\theta$?\n4. Above Eq. 9, there seems to be a typo: 'filed' -> 'field'"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2546/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698613764245,
            "cdate": 1698613764245,
            "tmdate": 1699636191261,
            "mdate": 1699636191261,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4mUSdC426I",
                "forum": "rUf9G9k2im",
                "replyto": "lkg4WjBKkT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2546/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2546/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rxeq"
                    },
                    "comment": {
                        "value": "Thanks for your comments. We have carefully revised our paper according to your suggestions.\n\n1. **In Eq. 1, does $\\mathcal N$ mean the density function of the Gaussian distribution?**\n\n    Correct, $\\mathcal N(\\cdot, \\cdot)$ is the density function of Gaussian.\n\n2. **In Eq. 1, what's the relationship between the $y$ being integrated and the $\\boldsymbol y$ in the integral bounds? Are they the same or different?**\n\n    They are different. The $\\boldsymbol y$ in the integral bounds refers to pixel values of the ground-truth unmasked image. Meanwhile, the $y$ being integrated follows a Gaussian distribution with the estimated mean and variance. We appreciate your highlighting this distinction. For improved clarity, we have represented the integrated variable as $z$ in the revised manuscript.\n\n3. **In Eq. 1, where does $\\boldsymbol y^i$ come from during training? Is it the ground-truth unmasked image or the GAN-generated one? If it is the latter, it also depends on $\\boldsymbol \\theta$?**\n\n    The $\\boldsymbol y^i$ denotes the $i$-th pixel of the ground-truth unmasked image. We have explicitly clarified this in the revised manuscript.\n    \n4. **Above Eq. 9, there seems to be a typo: 'filed' -> 'field'.**\n\n    Thanks for pointing this out. We have fixed this typo."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2546/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700298632610,
                "cdate": 1700298632610,
                "tmdate": 1700298632610,
                "mdate": 1700298632610,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AoS0KNzf86",
                "forum": "rUf9G9k2im",
                "replyto": "lkg4WjBKkT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2546/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2546/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rxeq"
                    },
                    "comment": {
                        "value": "Dear Reviewer rxeq,\n\nThank you for your detailed review and the valuable feedback. We have carefully provided clarifications and experiments to your previous comments. We appreciate your thorough evaluation of our work and look forward to hearing from you and addressing any further questions or concerns you may have.\n\nThank you for your continued engagement and support."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2546/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719690255,
                "cdate": 1700719690255,
                "tmdate": 1700719690255,
                "mdate": 1700719690255,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QzvocTUM3w",
            "forum": "rUf9G9k2im",
            "replyto": "rUf9G9k2im",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2546/Reviewer_N7bD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2546/Reviewer_N7bD"
            ],
            "content": {
                "summary": {
                    "value": "Authors present an iterative model for image in-painting, aiming at\nfilling large-holes with realistic looking areas. The model uses a GAN\nand a Gaussian model to estimate an uncertainty term, which as far as\nI can understand aims to find a covariance matrix that will minimize\nnegative log-likelihood of real intensities with respect to a Gaussian\nmodel that uses the GAN output as the mean. Thus, this model uses the\ncovariance to account for the synthesis error. Pixels with low\nvariance are added to the current prediction and the process is\nrepeated until the entire empty region is filled. Experiments with\nlarge-scale data sets are given."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. A very simple method that gives really good results.\n2. Authors experimented with large-scale data sets.\n3. Results are very impressive. The presented comparisons with\n   state-of-the-art methods shows that the proposed iterative scheme\n   yields better in-painting than all the rest. Furthermore, it does\n   it in a fraction of the time compared to the most recent methods. I\n   believe these results are extremely promising, and if they can be\n   reproduced they may have a substantial impact."
                },
                "weaknesses": {
                    "value": "1. The training strategy is not well explained. Authors should clarify\n   whether and how the iterations are taken into account during the\n   training.\n2. There are some suspiciously accurate in-painting results in the\n   appendix - particularly in Fig. M7. Can authors explain how the\n   proposed model can be so accurate? I can understand being\n   realistic, however, the third column shows that the generation is\n   pretty much the same. Similar in-apinting results are given in\n   Figure M8."
                },
                "questions": {
                    "value": "1. The in-painting results are suspiciously good in some\n   cases. Can authors explain how their model can be so accurate - not\n   just realistic - in M7 and M8? May there be a problem with training\n   and test data leakage? \n\nI am happy to increase my score, however, first I would like to wait authors' response to this question."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2546/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2546/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2546/Reviewer_N7bD"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2546/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764707339,
            "cdate": 1698764707339,
            "tmdate": 1700660781829,
            "mdate": 1700660781829,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fVrPmtqhyn",
                "forum": "rUf9G9k2im",
                "replyto": "QzvocTUM3w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2546/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2546/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer N7bD"
                    },
                    "comment": {
                        "value": "1. **The training strategy is not well explained. Authors should clarify whether and how the iterations are taken into account during the training.**\n\n    Thank you for your valuable advice. During training, our model undergoes the full pipeline with all iterations (we iterate twice for training efficiency, as stated in Sec.4.2), and the updating scheme between iterations is detailed in Sec.3.1.2. However, for testing, we may iterate for different numbers. As shown in Table 2, using more iterations results in higher FID performance. We have revised our manuscript to provide clearer illustrations of this point.\n\n2. **There are some suspiciously accurate in-painting results in the appendix - particularly in Fig.N7 (Fig.M7 in the original paper corresponds to Fig.N7 in the revised version). Can authors explain how the proposed model can be so accurate? I can understand being realistic, however, the third column shows that the generation is pretty much the same. Similar inpainting results are given in Fig.N8. May there be a problem with training and test data leakage?**\n\n    There is no problem with training and test data leakage. Following established methodologies like LaMa [a] and MAT[b], we utilize the official training (24183 images) and validation (2993 images) sets of CelebA-HQ. We have carefully checked that individuals present in the validation set do not overlap with the training data. \n\n    Our model performs well when the input image contains sufficient visible pixel information, enabling high-quality generation while maintaining coherence with the existing content. This success originates from our model's pixel spreading mechanism, which initiates from visible pixels and progressively diffuses valuable information throughout the image.  This approach is particularly effective for facial images characterized by strong inherent priors, such as symmetry and structural attributes. In the third example in Figure N.7, where the right half of the face is visible, our model reconstructs a comparatively realistic and consistent outcome using visible features like eyes and beard, albeit with potential discrepancies in details like ears and nose. However, when the entire face is masked, as in the first example in Figure N.7, our generated facial image notably diverges from ground truth. This phenomenon also appears in natural scenes; for instance, in the third example in Figure N.4, our method successfully restores the airplane structure due to the visibility of the frontal part, while other methods fail to inpaint such regions.\n\n    We will release the training code and models. All results are reproducible.\n    \n    [a] Suvorov, Roman, et al. \"Resolution-robust large mask inpainting with fourier convolutions.\" WACV. 2022.  \n    [b] Li, Wenbo, et al. \"Mat: Mask-aware transformer for large hole image inpainting.\" CVPR. 2022."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2546/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700298596957,
                "cdate": 1700298596957,
                "tmdate": 1700298596957,
                "mdate": 1700298596957,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EhntcPJLTg",
                "forum": "rUf9G9k2im",
                "replyto": "fVrPmtqhyn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2546/Reviewer_N7bD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2546/Reviewer_N7bD"
                ],
                "content": {
                    "title": {
                        "value": "thanks"
                    },
                    "comment": {
                        "value": "Thanks for the clarifications. The quality of the results is quite impressive."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2546/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660749226,
                "cdate": 1700660749226,
                "tmdate": 1700660749226,
                "mdate": 1700660749226,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2nS5SZO3aC",
            "forum": "rUf9G9k2im",
            "replyto": "rUf9G9k2im",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2546/Reviewer_EPEv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2546/Reviewer_EPEv"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose the pixel spread model (PSM) to gradually fill the masked regions of an image. Specifically, the model learns the mean by GAN loss and the variance by negative log likelihood (NLL). In each iteration, the model first makes the prediction, then picks the most valuable pixels to fill the region and leave the others masked. Experiments show that the method works well."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The idea is novel to gradually fill the regions through the computed mean and variance.\n- The method is efficient by utilizing the optimized computational resource of GAN model and the thoughts of autoregressive model."
                },
                "weaknesses": {
                    "value": "- Some parts of the paper is not very clear. For example,\n    - What's the meaning of $y^i$ in equation (1)? Is it the groundtruth?\n    - More explanation is necessary for equation (5). What happens when $m_t=1$ and $m_0=0$ or vise versa?\n    - Is the uncertainty map computed only in the mask region or the entire image?\n    - Why $\\sigma_t$ is in the range $[0,1]$?\n\n- Comparison to Controlnet is necessary.\n- It seems that sometimes the model is hard to grasp some structure of the image. For example, in the last row of Fig.4, the result of Model A cannot reconstruct the structure of stairs."
                },
                "questions": {
                    "value": "Please see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2546/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770785740,
            "cdate": 1698770785740,
            "tmdate": 1699636191098,
            "mdate": 1699636191098,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dZDhJ8sLwC",
                "forum": "rUf9G9k2im",
                "replyto": "2nS5SZO3aC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2546/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2546/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EPEv"
                    },
                    "comment": {
                        "value": "1. **What's the meaning of in Eq.1? Is it the ground truth?**\n\n    The $y^i$ represents the $i$-th pixel of the ground-truth image. We have revised the paper to improve the clarity of this illustration.\n\n2. **More explanation is necessary for Eq.5. What happens when $\\boldsymbol m_t=1$ and $\\boldsymbol m_0=0$ or vice versa?**\n\n    In the mask $\\boldsymbol m$, we denote known regions with values of 1 and missing regions as 0. In Eq.5, we assign pixel values in the missing regions to 0 while preserving the original pixel values in the initially known area. Subsequently, we progressively fill in the missing regions using sampled results based on newly estimated mean and variance values. It's important to note that the previously inpainted content in the missing regions is updated at each iteration to maintain consistency with the newly inpainted pixels. \n\n    In cases when $\\boldsymbol m_0=0$, the initially visible pixels are absent. We consider this as a generation task. However, in our inpainting mask, such a phenomenon does not occur since we do not set $\\boldsymbol m_0=0$. Furthermore, we only set $\\boldsymbol m_t=1$ during the final iteration to ensure training convergence. When $\\boldsymbol m_0=1$, all pixels are regarded as valid. So we will not update anymore.\n    \n3. **Is the uncertainty map computed only in the mask region or the entire image?**\n\n    The uncertainty map is computed for the entire image. As discussed in the second step (i.e., 'Pick'), the uncertainty score of each initially known pixel is set to 0. Additionally, for the pixels that have been inpainted up to the current time step, their uncertainty scores  are computed by using the $\\boldsymbol \\sigma_t$ map. For the pixels located in regions that are still missing, their values are set as 1."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2546/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700298463187,
                "cdate": 1700298463187,
                "tmdate": 1700298463187,
                "mdate": 1700298463187,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "na76VZugUX",
                "forum": "rUf9G9k2im",
                "replyto": "2nS5SZO3aC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2546/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2546/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EPEv (Part 2)"
                    },
                    "comment": {
                        "value": "4. **Why is $\\boldsymbol \\sigma_t$ in the range [0,1]?**\n\n    We do not constrain the range of $\\boldsymbol \\sigma_t$ to [0,1]. Instead, we normalize $\\boldsymbol \\sigma_t$ to compute the uncertainty map $\\boldsymbol u_t$, which ranges from [0,1]. In the last sampling step, we use the original $\\boldsymbol \\sigma_t$ values. The $\\boldsymbol \\sigma_t$ maps in Figure 2 and Figure 3 are normalized only for better visualization. We have clarified this point in the revised paper.\n\n5. **Comparison to Controlnet is necessary.**\n\n    Thank you for the constructive comments. In addition to ControlNet [a], we also include MI-GAN \\[b\\] (ICCV 2023) in our comparisons. Although ControlNet is not trained on the Places2 dataset, it undergoes training on a more extensive LAION dataset. Results from our proposed PSM show significant improvements across all metrics, highlighting the effectiveness of our method. We observe that ControlNet might produce undesired results due to its tendency to generate new objects that might not align harmoniously with the existing content. We have added the comparisons to Sec.I of the supplementary material due to the page limit.\n    \n    #### 512x512 Places2 under the small mask ratio setting\n    | | | | | | |\n    |---|---|---|---|---|---|\n    | Method | FID$\\downarrow$ | P-IDS$\\uparrow$ | U-IDS$\\uparrow$ | LPIPS$\\downarrow$ | PSNR$\\uparrow$ |\n    | **PSM (ours)** | **0.72** | **30.95** | **43.91** | **0.084** | **25.51** |\n    | MIGAN | 1.40 | 18.43 | 39.35 | 0.103 | 24.38 | \n    | ControlNet | 1.86 | 12.63 | 35.71 | 0.117 | 24.68 |\n    | | | | | | |\n    \n    #### 512x512 Places2 under the large mask ratio setting\n    | | | | | | |\n    |---|---|---|---|---|---|\n    | Method | FID$\\downarrow$ | P-IDS$\\uparrow$ | U-IDS$\\uparrow$ | LPIPS$\\downarrow$ | PSNR$\\uparrow$ |    \n    |**PSM (ours)**| **1.68** | **25.33** | **39.30** | **0.161** | **20.89** |\n    |MIGAN| 3.81|13.50|32.42|0.195|20.00|\n    |ControlNet|5.55|6.60|25.65|0.219|19.73|\n    | | | | | | |\n\n\n    [a] Zhang, Lvmin, Anyi Rao, and Maneesh Agrawala. \"Adding conditional control to text-to-image diffusion models.\" ICCV. 2023.  \n    [b] Sargsyan, Andranik, et al. \"MI-GAN: A Simple Baseline for Image Inpainting on Mobile Devices.\" ICCV. 2023.   \n    \n6. **It seems that sometimes the model is hard to grasp some structure of the image. For example, in the last row of Fig.4, the result of Model A cannot reconstruct the structure of stairs.**\n\n    We agree that our model sometimes hard to grasp some structures during generation. This is attributed to our pixel spread model, which functions by spreading informative pixels from initially visible regions across the entire image. In the last example depicted in Figure 4, despite some remaining stairs, the spreading process appears to be primarily influenced by the window representation due to its frequent appearance in the input. Consequently, our model effectively reconstructs the windows. However, even in the absence of stairs, our output remains reasonable.\n    \n    Indeed, it is very challenging to consistently generate all subtle structures with high fidelity, as the content of the free-form masked regions is typically complex and ambiguous. This challenging factor also deteriorates diffusion models during generation although they have shown great generation abilities, as shown in Figure 6. We will concentrate on improving the generation quality in our future work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2546/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700298511846,
                "cdate": 1700298511846,
                "tmdate": 1700298511846,
                "mdate": 1700298511846,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rd7jJymdf5",
                "forum": "rUf9G9k2im",
                "replyto": "2nS5SZO3aC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2546/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2546/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EPEv"
                    },
                    "comment": {
                        "value": "Dear Reviewer EPEv,\n\nThank you for your detailed review and the valuable feedback. We have carefully provided clarifications and experiments to your previous comments.  We appreciate your thorough evaluation of our work and look forward to hearing from you and addressing any further questions or concerns you may have.\n\nThank you for your continued engagement and support."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2546/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719651499,
                "cdate": 1700719651499,
                "tmdate": 1700719651499,
                "mdate": 1700719651499,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Wqq7ekxT2E",
            "forum": "rUf9G9k2im",
            "replyto": "rUf9G9k2im",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2546/Reviewer_HZRy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2546/Reviewer_HZRy"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a pixel spread model for image inpainting, which iteratively employs decoupled probabilistic modeling. Specifically, this work combines the optimization efficiency of GANs with the prediction tractability of probabilistic models and selectively spreads informative pixels throughout the image in a few iterations, improving the completion quality and efficiency. Experimental results show the proposed method achieves a state-of-the-art performance on several benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed pixel spread model makes use of the merits of GANs\u2019 efficient optimization and the tractability of probabilistic models.\n\nGood performance is achieved on several benchmark datasets."
                },
                "weaknesses": {
                    "value": "There are several unclear statements listed as follows."
                },
                "questions": {
                    "value": "1. Please explain in detail how the uncertainty is obtained in each iteration.\n2. The third paragraph in Section 3.1.1 is inconsistent with Figure 2. The input image x_t at time t should be x_{t-1} at time t-1, right?\n3. Please discuss in detail the effect of the learnable function F in Eq. (6).\n4. The authors state several times that the proposed method improve the efficiency. Please demonstrate in detail, e.g., the comparison of inference time.\n5. What is the mask size in Table 4?\n6. In the experiments, please add the comparisons with more recent methods, e.g.,\n[a] Chu et al., Rethinking Fast Fourier Convolution in Image Inpainting, ICCV 2023.\n[b] Sargsyan et al., MI-GAN: A Simple Baseline for Image Inpainting on Mobile Devices, ICCV 2023.\n[c] Ko et al., Continuously Masked Transformer for Image Inpainting, ICCV 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2546/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698897634551,
            "cdate": 1698897634551,
            "tmdate": 1699636191032,
            "mdate": 1699636191032,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TMrjorVeN2",
                "forum": "rUf9G9k2im",
                "replyto": "Wqq7ekxT2E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2546/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2546/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HZRy"
                    },
                    "comment": {
                        "value": "1. **Please explain in detail how the uncertainty is obtained in each iteration.**\n\n    We provide the details of obtaining the uncertainty in each iteration. Taking the $t$-th iteration as an example, we forward the masked image $\\boldsymbol x_{t-1}$, binary mask $\\boldsymbol m_{t-1}$ and uncertainty map $\\boldsymbol u_{t-1}$ to the network to obtain the estimated mean $\\boldsymbol \\mu_t$ and variance $\\boldsymbol \\sigma_t^2$. To obtain a preliminary uncertainty map $\\tilde {\\boldsymbol u}_t$ in \\[0,1\\], we normalize the standard deviation map $\\boldsymbol \\sigma_t$ by subtracting its min value and dividing by absolute max-min value (the smaller $\\boldsymbol \\sigma_t$, the lower uncertainty). Note that the final sampling (Step 3 in Sec.3.1.2) is performed using the original $\\boldsymbol \\sigma_t$ without normalization.  \n    \n    We then sort the uncertainty scores only for unknown pixels based on $\\boldsymbol m_{t-1}$. According to the pre-defined mask schedule, we figure out the pixels that will be newly added in this iteration. Based on the preliminary uncertainty map $\\tilde {\\boldsymbol u}_t$, by marking pixel locations that are still missing as 1 and the initially known pixel locations (referring to $\\boldsymbol m_0$) as 0, while keeping the $\\tilde {\\boldsymbol u}_t$ values of inpainted pixels unchanged, we obtain the final uncertainty map $\\boldsymbol u_t$.\n\n2. **The third paragraph in Section 3.1.1 is inconsistent with Figure 2. The input image $\\boldsymbol x_t$ at time t should be $\\boldsymbol  x_{t-1}$ at time $t-1$, right?**\n\n    Thanks for pointing this out. We have fixed this typo. It should be $\\boldsymbol x_{t-1}$.\n\n3. **Please discuss in detail the effect of the learnable function $\\mathcal F$ in Eq.6.**\n\n    The learnable function $\\mathcal F$ introduces the proposed uncertainty-guided attention mechanism. In traditional attention based solely on feature similarity, all pixels have an equal chance to exchange information. However, in image inpainting, where missing pixels are initialized with specified identical values, conventional attention mechanisms are not effective in conveying valuable information from visible regions to missing regions. This phenomenon often yields unsatisfied pixels, resulting in blurry content and undesirable artifacts.\n\n    By incorporating the uncertainty map as an input, the function $\\mathcal F$ discerns between valid and invalid pixels. This enables our network to prioritize attention on valid pixels when filling in the holes. Removal of this design (i.e., 'model F' in Table 1) results in a performance drop compared to our full 'model A'. Additionally, the structures are sharper and clearer in the inpainted results via our uncertainty-guided attention in Figure 4."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2546/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700298302347,
                "cdate": 1700298302347,
                "tmdate": 1700298302347,
                "mdate": 1700298302347,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4ZgyGwoRhc",
                "forum": "rUf9G9k2im",
                "replyto": "Wqq7ekxT2E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2546/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2546/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HZRy (Part 2)"
                    },
                    "comment": {
                        "value": "4. **The authors state several times that the proposed method improves efficiency. Please demonstrate in detail, e.g., the comparison of inference time.**\n\n    We provide efficiency comparisons in Table F.5, where our model takes less time for inference. We also compare inference speed tested on the A100 GPU and inpainting quality in the table below where our model achieves favorable performance.\n\n    | | | | | | |\n    |---|:---:|:---:|:---:|:---:|:---:|\n    |Method |**Ours**|Stable Diffusion|LDM|ControlNet|MAT|\n    |FID$\\downarrow$ | **1.68** | 2.11 | 2.76 | 5.55 | 2.90 |\n    |Speed$\\downarrow$ (512\u00d7512) |**0.25s**|3.6s|2.7s|3.1s|0.26s|\n    | | | | | | |\n\n5. **What is the mask size in Table 4?**\n\n    The mask size is 512x512, consistent with the image sizes of Places2 and CelebA-HQ as indicated in Table 4. The small and large mask settings refer to different masking ratios. The mask statistics have been provided in Sec.B of Mat's [e] supplementary file. The small and large masks are established with an average masking ratio of approximately 20% and 50%, respectively.\n\n6. **In the experiments, please add the comparisons with more recent methods, e.g., [a] Chu et al., Rethinking Fast Fourier Convolution in Image Inpainting, ICCV 2023. [b] Sargsyan et al., MI-GAN: A Simple Baseline for Image Inpainting on Mobile Devices, ICCV 2023. [c] Ko et al., Continuously Masked Transformer for Image Inpainting, ICCV 2023.**\n\n    Thanks for the valuable comments. We have cited all the referred works. In practice, we find out that only MI-GAN [b] provides implementation code and models, so we compare our method to MI-GAN and ControlNet \\[d\\] (ICCV 2023 Best Paper Award) below. Note that ControlNet is trained on a more extensive LAION dataset rather than the Places2 dataset. The results obtained from our proposed PSM exhibit significant improvements across all metrics. We observe that ControlNet might produce undesired results due to its tendency to generate new objects that might not align harmoniously with the existing content. A detailed comparison is shown in Sec.I due to page limit.\n    \n    #### 512x512 Places2 under the small mask ratio setting\n    | | | | | | |\n    |---|---|---|---|---|---|\n    | Method | FID$\\downarrow$ | P-IDS$\\uparrow$ | U-IDS$\\uparrow$ | LPIPS$\\downarrow$ | PSNR$\\uparrow$ |\n    | **PSM (ours)** | **0.72** | **30.95** | **43.91** | **0.084** | **25.51** |\n    | MIGAN | 1.40 | 18.43 | 39.35 | 0.103 | 24.38 | \n    | ControlNet | 1.86 | 12.63 | 35.71 | 0.117 | 24.68 |\n    | | | | | | |\n    \n    #### 512x512 Places2 under the large mask ratio setting\n    | | | | | | |\n    |---|---|---|---|---|---|\n    | Method | FID$\\downarrow$ | P-IDS$\\uparrow$ | U-IDS$\\uparrow$ | LPIPS$\\downarrow$ | PSNR$\\uparrow$ |    \t   \n    |**PSM (ours)**| **1.68** | **25.33** | **39.30** | **0.161** | **20.89** |\n    |MIGAN| 3.81|13.50|32.42|0.195|20.00|\n    |ControlNet|5.55|6.60|25.65|0.219|19.73|\n    | | | | | | |\n\n    [a] Chu, Tianyi, et al. \"Rethinking Fast Fourier Convolution in Image Inpainting.\" ICCV. 2023.     \n    [b] Sargsyan, Andranik, et al. \"MI-GAN: A Simple Baseline for Image Inpainting on Mobile Devices.\" ICCV. 2023.      \n    [c] Ko, Keunsoo, and Chang-Su Kim. \"Continuously Masked Transformer for Image Inpainting.\" ICCV. 2023.     \n    [d] Zhang, Lvmin, Anyi Rao, and Maneesh Agrawala. \"Adding conditional control to text-to-image diffusion models.\" ICCV. 2023.\n    [e] Li, Wenbo, et al. \"Mat: Mask-aware transformer for large hole image inpainting.\" CVPR. 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2546/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700298365074,
                "cdate": 1700298365074,
                "tmdate": 1700298365074,
                "mdate": 1700298365074,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]