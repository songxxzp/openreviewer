[
    {
        "title": "The Power of the Senses: Generalizable Manipulation from Vision and Touch through Masked Multimodal Learning"
    },
    {
        "review": {
            "id": "PjBMiUpv0A",
            "forum": "FMsmo01TaI",
            "replyto": "FMsmo01TaI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6575/Reviewer_HDvH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6575/Reviewer_HDvH"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduced a technique that merges multimodal self-supervised representation learning with robot policy learning through the PPO algorithm. This approach showcases that by utilizing masked autoencoding for both policy and visual-tactile representations, one can enhance sample efficiency and achieve superior generalization capabilities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The content is articulately written, making it easily comprehensible.\n2. The appendix offers in-depth details about both the model and the environment, ensuring reproducibility of the work.\n3. The model innovatively incorporates multimodal representation features to address robot learning tasks, marking a significant stride in this research direction."
                },
                "weaknesses": {
                    "value": "1. A prevalent challenge in contemporary research within this domain is tactile simulation. The tactile simulation employed in this study lacks the desired accuracy. Moreover, the introduced method hasn't been empirically tested or combined with tactile sensors in real-world experiments, which is a missing link for comprehensive validation.\n2. The ViT model, as utilized, is computationally intensive and has a propensity to overfit to specific problems. The potential benefits of ViT's self-supervised pre-training techniques, such as MAE, ought to be realized across a diverse range of datasets or environments. Given the constraints observed in the study\u2014limited tasks, a restricted set of objects, and minimal variations in background textures\u2014it's challenging to fully endorse the promise of the proposed approach.\n3. As discerned from Figure 6, there's an inconsistency in the results between the baseline and the proposed method across various tasks. This discrepancy muddles the clarity needed to conclusively determine the efficacy of the proposed method. It would be advisable to extend the scope by implementing over 10 distinct tasks and undertaking comprehensive statistical analyses to more convincingly demonstrate the method's effectiveness."
                },
                "questions": {
                    "value": "Please see the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6575/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6575/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6575/Reviewer_HDvH"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6575/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698623369451,
            "cdate": 1698623369451,
            "tmdate": 1699636746417,
            "mdate": 1699636746417,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mL8Sw4ESvJ",
                "forum": "FMsmo01TaI",
                "replyto": "PjBMiUpv0A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6575/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "**#1: The tactile simulation employed in this study lacks the desired accuracy. Moreover, the introduced method hasn't been empirically tested or combined with tactile sensors in real-world experiments.**\n\nMuJoCo\u2019s contact force optimization is equivalent to a penalty-based approach, which is essentially the same used to compute force maps in [A] and [B], with those works showing zero-shot transfer to the real-world for the specific tasks considered. Additionally, there are other remarkable examples of sim-to-real transfer using MuJoCo [C], [D]. Based on such evidence, we would like to highlight that MuJoCo\u2019s touch grid is currently the most realistic general-purpose simulator of tactile maps, especially given the wide range of possibilities for proper system identification. We expect this very important clarification to change the reviewer\u2019s evaluation of the contribution.\n\nWhile the main focus of our work was on studying how to extract stronger representations when visual and tactile data are available, we also showed some promising direction (e.g., improvement of vision-only policies with tactile-enabled representation training) for real-world deployment, which we leave as future work.\n\n**#2: The ViT model, as utilized, is computationally intensive and has a propensity to overfit to specific problems. The potential benefits of ViT's self-supervised pre-training techniques, such as MAE, ought to be realized across a diverse range of datasets or environments.**\n\nThanks for suggesting this comparison! We indeed compared to MVP [E], which is basically relying on ViT representations pretrained across datasets, and on an additional baseline using CLIP [F] visual encoder. As shown in Figure 10b of the updated appendix, both M3L and its vision-only MAE variation outperform MVP and CLIP representations on the in-domain learning task (tactile insertion). We believe this is due to the fact that we let the representations further adapt during task learning. Note that training such MVP and CLIP baselines was even more computationally intensive than training our all-in-one approach, due to the inference of the much larger MVP (22M) and CLIP (88M) encoders, compared to the small ViT used in M3L (5M).\n\nWe omit generalization results on the new baselines due to the low success rates on the in-domain task.\n\n**#3: There's an inconsistency in the results between the baseline and the proposed method across various tasks. It would be advisable to extend the scope by implementing over 10 distinct tasks and undertaking comprehensive statistical analyses to more convincingly demonstrate the method's effectiveness.**\n\nOur results show that 1) vision+touch policies perform best when investigating zero-shot generalization, 2) training representations with vision+touch also improve vision policies, 3) representation learning approaches outperform end-to-end approaches when learning in-domain. 1) and 2) are shown in Figure 5 in the paper, while 3) is shown in Figure 6 in the paper. These results are consistent across all tasks, so we disagree with the reviewer comment. \n\nAs the reviewer pointed out in #1, the use of tactile simulation is still at an early stage. While we agree that evaluation on a large variety of tasks would be meaningful, note that our three environments are the first available implementations of RL environments with availability of both visual and high-resolution tactile sensing on a general-purpose like MuJoCo. This required carefully designing and adapting each of the tasks for this work, as opposed to other fields in RL that have the possibility of benchmarking on existing environments. To give an idea of the process involved, to increase the number of contact points on the Shadow Hand, we needed to split each collision mesh in a much higher number of meshes making sure that each of these was still convex (as required by MuJoCo), which is a non-trivial process for each robot embodiment. Ours is an effort in this direction, and open-sourcing such environments will hopefully give the opportunity to fellow researchers to further extend the benchmarking opportunities in the area. As the reviewer pointed out, however, we purposely focused on tasks that are very different to cover a wider spectrum of applications. We are currently working on further extending the number of tasks."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700281258654,
                "cdate": 1700281258654,
                "tmdate": 1700281258654,
                "mdate": 1700281258654,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xxJMl50kvX",
                "forum": "FMsmo01TaI",
                "replyto": "PjBMiUpv0A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6575/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional tasks"
                    },
                    "comment": {
                        "value": "**Additional tasks**\n\nWe performed additional experiments on two new in-hand rotation tasks, namely, egg and pen rotation tasks. \n\nThese tasks confirm that M3L outperforms the baselines in terms of generalization, particularly by a large margin when altering the pen color in the corresponding rotation task. Please find the corresponding discussion in Appendix I of the paper. We hope that this clarifies your related concern. We look forward to hearing your feedback before the end of the rebuttal period."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642104374,
                "cdate": 1700642104374,
                "tmdate": 1700642145666,
                "mdate": 1700642145666,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vtpWVzuaUU",
            "forum": "FMsmo01TaI",
            "replyto": "FMsmo01TaI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6575/Reviewer_pVq9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6575/Reviewer_pVq9"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an approach for combining visual and tactile data in training robot manipulation policies in simulation. The main contribution of the paper is a representation learning algorithm (M3L) based on masked auto-encoding that can ingest both image observations and tactile observations (taxels). The policy learning with the learned representations can be performed with existing reinforcement learning algorithms like PPO. Experiments on three robot simulation tasks show that the proposed approach is effective in distilling multimodal touch+vision representations for policy learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper presents a neat idea of combining touch and vision through an existing paradigm of masked autoencoding, and showing that this is helpful for simulated robot manipulation tasks. The overall idea is simple and can be used widely with minimal modifications. \n\n- The simulation results are interesting in terms of the same algorithm being able to learn policies for very different tasks when trained separately on each simulation environment. It is good to note that these tasks span fine-grained manipulation, articulated object opening, and dexterous manipulation. \n\n- The overall paper is well-written and insights are communicated clearly. Sufficient details are provided in the Appendix, and the video results are helpful in understanding the tasks."
                },
                "weaknesses": {
                    "value": "1. The paper ignores several different related works, both in terms of tactile sensors used for robotics, and machine learning approaches that use touch as a modality for manipulation tasks. Here are some missing papers that should be discussed:\n\n         a) robot learning papers with tactile data [1,2] these papers show real-robot dexterous manipulation tasks with either tactile data along or with both vision and tactile data (followed by RL with sim2real transfer)\n         b) versatile tactile sensors for real-world manipulation [3] \n\n2. The simulation results are interesting in terms of the same algorithm being able to learn policies for very different tasks (as mentioned in the strengths) but it is unclear to me why experiment with only three tasks? Most reinforcement learning in simulation papers show results on a large number of environments, and this is important to understand the capabilities of the proposed approach, without over-indexing on a particular environment. \n\n3. There are no external baselines compared against. All the comparisons are with variants of the proposed M3L algorithm. The related works in representation learning that use vision-only observations, including papers that do pre-training for representation learning are relevant for empirical comparisons (for example [4,5])\n\n4. The introduction mentions \"M3L demonstrates better zero-shot generalization to unseen objects and variations of\nthe task scene\" however the experiments show only very weak generalization to minor changes in the object (in particular mass, damping, friction variations). Hence the claim is not really substantiated in my understanding. It is important to show generalization to actually different objects (for example with geometric variations like shape and size, in addition to the current variations). Also, the results should evaluate this for a number of different objects and not just a single new object, in order for the conclusions to be statistically significant.  \n\n\n[1] Qi, Haozhi, Brent Yi, Sudharshan Suresh, Mike Lambeta, Yi Ma, Roberto Calandra, and Jitendra Malik. \"General in-hand object rotation with vision and touch.\" arXiv preprint arXiv:2309.09979 (2023).\n\n[2] Yin, Zhao-Heng, Binghao Huang, Yuzhe Qin, Qifeng Chen, and Xiaolong Wang. \"Rotating without Seeing: Towards In-hand Dexterity through Touch.\" arXiv preprint arXiv:2303.10880 (2023).\n\n[3] Bhirangi, Raunaq, Tess Hellebrekers, Carmel Majidi, and Abhinav Gupta. \"ReSkin: versatile, replaceable, lasting tactile skins.\" arXiv preprint arXiv:2111.00071 (2021).\n\n[4] Nair, Suraj, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. \"R3m: A universal visual representation for robot manipulation.\" arXiv preprint arXiv:2203.12601 (2022).\n\n[5] Radosavovic, Ilija, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. \"Real-world robot learning with masked visual pre-training.\" In Conference on Robot Learning, pp. 416-426. PMLR, 2023."
                },
                "questions": {
                    "value": "Refer to the list of weakness for more details. \n\n1. Is it possible to provide a more detailed treatment of related works? In particular, robot learning papers with tactile data [1,2] and versatile tactile sensors for real-world manipulation [3] are relevant, and missing.\n\n2. Is it possible to have more than three simulation tasks? Most reinforcement learning in simulation papers show results on a large number of environments, and this is important to understand the capabilities of the proposed approach, without over-indexing on a few particular environments.\n\n3. Is it possible to compare with external baselines? The related works in representation learning that use vision-only observations, including papers that do pre-training for representation learning are relevant for empirical comparisons (for example [4,5])\n\n4. Is it possible to evaluate generalization with different objects (shapes, sizes, visual appearances) ? Since generalization is a main claim of the paper, this seems particularly important. \n\n5. Real-World tactile data is usually very noisy, but simulation data is clean. Is there anything that will need to be changed in the approach for bridging this gap or making the representations more robust to noise, when real-world deployment is attempted? In general, the results in the paper are not convincing for justifying the last paragraph of page 9, because there is very little generalization demonstrated in simulation itself."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6575/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793273167,
            "cdate": 1698793273167,
            "tmdate": 1699636746291,
            "mdate": 1699636746291,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TL9kaMutjj",
                "forum": "FMsmo01TaI",
                "replyto": "vtpWVzuaUU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6575/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "**#1: The paper ignores several different related works [A], [B], [C], both in terms of tactile sensors used for robotics, and machine learning approaches that use touch as a modality for manipulation tasks.**\n\nThank you for pointing out relevant works. We have included these works in our updated related work section and also compared our method with [A] in Figure 10, Appendix. However, please note that [A] is a contemporaneous work according to ICLR guidelines, having been arxived only 10 days prior to the submission of this paper.\n\n**#2: Is it possible to have more than three simulation tasks?**\n\nWe agree that evaluation on a large variety of tasks would be meaningful and thus, we are currently working on further extending the number of tasks. \n\nWe want to emphasize that our three environments are the **first** RL environments both with visual and **high-resolution tactile** sensing on a general-purpose physics engine like MuJoCo. This required carefully designing and adapting each of the tasks for this work, as opposed to other fields in RL that have the possibility of benchmarking on existing environments. To give an idea of the process involved, to increase the number of contact points on the Shadow Hand, we needed to split each collision mesh in a much higher number of meshes making sure that each of these was still convex (as required by MuJoCo), which is a non-trivial process for each robot embodiment. Ours is an effort in this direction, and open-sourcing such environments will hopefully give the opportunity to fellow researchers to further extend the benchmarking opportunities in the area. As the reviewer pointed out, however, we purposely focused on tasks that are very different amon each other to cover a wider spectrum of applications.\n\n**#3: There are no external baselines compared against. All the comparisons are with variants of the proposed M3L algorithm.**\n\nAs suggested by the reviewer, we have included comparisons to other approaches in Figure 10 of the updated version: contact location-based tactile representations [A] and vision-only pre-trained representations (MVP [D], CLIP [E]). \n\nWhile [A] is contemporaneous work, we found it interesting to compare with a similar transformer architecture, trained in a single stage from vision and touch. In particular, we also investigate the tactile representation used in [A], that is, discretized contact locations. In contrast, our approach uses intensity and directions of spatially distributed force vectors (force maps). The results are shown in Figure 10a of the updated appendix, with the discretized contact location approach outperformed by both our M3L and end-to-end baselines even for the in-domain learning task (tactile insertion).\n\nWe also added comparisons to baselines based on vision-only representations extracted through MVP [D] and CLIP [E] visual encoders, kept frozen during PPO training. Note that both encoders are much larger than the one used in M3L (5M for M3L, 22M for MVP, and 88M for CLIP), leading to considerably slower training. The results are shown in Figure 10b of the updated appendix, with both M3L and its vision-only MAE variation outperforming MVP and CLIP representation on the in-domain learning task (tactile insertion).\n\nWe omit generalization results on the new baselines due to the low success rates on the in-domain task.\n\n**#4: The experiments show only very weak generalization to minor changes in the object (in particular mass, damping, friction variations). It is important to show generalization to actually different objects.**\n\nIn the tactile insertion task, we indeed test the generalization capability to **different objects**, as illustrated in Figure 4 and Figure 8.\n\nMoreover, we would argue that other perturbations in the objects are not minor, i.e., 10x friction and damping, 2x weight. In addition, we test on different object poses in Door Opening and camera poses in In-Hand Rotation.\n\n**#5: Real-World tactile data is usually very noisy, but simulation data is clean. Is there anything that will need to be changed in the approach for bridging this gap or making the representations more robust to noise, when real-world deployment is attempted?**\n\nThanks for the great suggestion! We added a noise robustness analysis in Figure 12 of the updated appendix, where we add zero-mean Gaussian noise to both visual and tactile data at inference time. The figure shows the performance of M3L vs the end-to-end for increasing noise variance, highlighting how M3L success rates do not degrade up to a reasonable degree of disturbances and also consistently outperforms the end-to-end approach also in terms of robustness.\n\nAlso, note that force maps have already been shown to be transferable to the real-world, either by estimating them [F] or by mapping them to optical flow [G]."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700281110637,
                "cdate": 1700281110637,
                "tmdate": 1700281110637,
                "mdate": 1700281110637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gzCE5OeDK2",
                "forum": "FMsmo01TaI",
                "replyto": "vtpWVzuaUU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6575/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional tasks"
                    },
                    "comment": {
                        "value": "**Additional tasks**\n\nWe performed additional experiments on two new in-hand rotation tasks, namely, egg and pen rotation tasks. \n\nThese tasks confirm that M3L outperforms the baselines in terms of generalization, particularly by a large margin when altering the pen color in the corresponding rotation task. Please find the corresponding discussion in Appendix I of the paper. We hope that this clarifies your related concern. We look forward to hearing your feedback before the end of the rebuttal period."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642048949,
                "cdate": 1700642048949,
                "tmdate": 1700642154046,
                "mdate": 1700642154046,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eUhpArik53",
                "forum": "FMsmo01TaI",
                "replyto": "TL9kaMutjj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6575/Reviewer_pVq9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6575/Reviewer_pVq9"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "Dear authors, thank you for your rebuttal responses and update to the paper. The additional comparisons with vision-only pre-training are helpful however several of my other concerns have not been adequately addressed. In particular, I am not convinced regarding the generalization to different objects, and real world applicability. Similar concerns about real world deployment have also been raised by reviewer bnCo which I agree with. Hence, I am unable to recommend accepting the paper."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714712664,
                "cdate": 1700714712664,
                "tmdate": 1700714712664,
                "mdate": 1700714712664,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WiODDXiM7U",
            "forum": "FMsmo01TaI",
            "replyto": "FMsmo01TaI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6575/Reviewer_bnCo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6575/Reviewer_bnCo"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes to learn visuo-tactile representation for manipulation with mask autoencoding. By utilizing MAE objective and structure, the learned joint representation improves the performance, sample efficiency and generalization ability of the policy. The method is evaluated in simulation for tactile insertion, door opening and in-hand cuda rotation tasks. Results also show the joint representation improves the vision-only policy performance in the test time."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Utilizing MAE architecture for modeling multimodal information for manipulation is intuitive and easy to understand. MAE objective for modeling joint representations is also a natural choice.\n2. This work provides enough technical details, hyper-parameters, and visualization for reproducing the results.\n3. This provides an interesting result that with multimodal self-supervised learning, the performance of vision-only policy improved during the test."
                },
                "weaknesses": {
                    "value": "1. Manipulation with tactile sensor and visual input has been evaluated in the real world for different tasks[1]. To actually validate the performance of the proposed method, the real world evaluation is required. Especially for noisy tactile sensory information. \n2. To validate the effectiveness of MAE architecture and self-supervised objective in this application, additional baselines (not limited to the baselines i mentioned) need to be compared. Qi, et al utilize a visuotactile transformer to model sequential observations. Hansen, et al show promising result in applying self-supervised learning in visual RL. \n3. Missing tactile-only baseline, to fully address the essence of the multimodal representation.\n4. Additional arm (or hand) proprioception information should be included in this kind of multi-modal manipulation pipeline, since it\u2019s much easier to get (in sim and real) and provide rich information for manipulation tasks.\n5. Out of three tasks, the proposed framework only outperforms vision-only baselines in In-hand rotation tasks. It seems tactile insertion and door-opening tasks are not quite tasks to evaluate the additional usage of tactile information.\n6. For generalization experiments, randomization on on physical parameters might not be enough. Adding different level of noise to the visual observation and tactile reading and test the generalization on that end could further validate the effectiveness of the proposed self-supervised pipeline.\n\n\nReference: \n\n[1] Qi, et al. General In-Hand Object Rotation with Vision and Touch\n\n[2] Hansen, et al. Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation"
                },
                "questions": {
                    "value": "1. It would be great for authors to address the things mentioned in the weakness section\n2. The result in FIgure 10, shows the learned MAE could reconstruct almost the full observation with quite limited observation. I\u2019m wondering whether the learned model can show the same level of performance if there are noise in visual observation or tactile reading. Otherwise it looks lille the model is overfitting to the training distribution."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6575/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698857541687,
            "cdate": 1698857541687,
            "tmdate": 1699636746144,
            "mdate": 1699636746144,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F3VgdwXBAN",
                "forum": "FMsmo01TaI",
                "replyto": "WiODDXiM7U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6575/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6575/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "**#1: Manipulation with tactile sensor and visual input has been evaluated in the real world for different tasks [A]. To actually validate the performance of the proposed method, the real world evaluation is required. Especially for noisy tactile sensory information.**\n\nWe would like to highlight that [A] is a contemporaneous work according to ICLR guidelines, having been arxived only 10 days prior to the submission of this paper. We are however happy to cite and compare to [A] (see also point #2). \n\n[A] trained an end-to-end baseline (in two privileged learning stages) and achieved sim-to-real transfer through large-scale GPU parallelization and domain randomization. The focus of our work was instead on studying how to extract stronger representations when visual and tactile data are available. We also showed some promising direction (e.g., improvement of vision-only policies with tactile-enabled representation training) for real-world deployment, which we leave as future work.\n\nRegarding the robustness to noise, please see our response to #7.\n\n**#2: Additional baselines need to be compared.**\n\nThank you for your suggestion. We have compared our method with additional baselines: contact location-based tactile representations [A] and vision-only pre-trained representations (MVP [B], CLIP [C]).\n\nFigure 10a of the updated appendix shows that both our M3L and end-to-end baselines outperform the contemporaneous work that uses discretized contact location approach [A] for the in-domain learning task (tactile insertion), confirming the advantages of using high-resolution force maps as opposed to low dimensional contact quantities.\n\nThe pre-trained representations, MVP [B] and CLIP [C], also show slower training curves in Figure 10b. Note that the encoders used in MVP (22M parameters) and CLIP (88M parameters) are much larger than our M3L (5M parameters).\n\nWe omit generalization results on the new baselines due to the low success rates on the in-domain task.\n\n**#3: Missing tactile-only baseline.**\n\nThis is a great point! We have run additional experiments with the tactile-only baseline. Interestingly, although the Door Opening task can be fully solved without vision, M3L (vision + tactile) achieves much faster and better learning results than the tactile-only baseline. The benefit of M3L could come from (1) our multimodal representation learning and (2) the use of visual observations. We included this result in Figure 11 of the updated paper.\n\nNote that the insertion and hand tasks are misspecified when vision is missing, because of missing target and incomplete state information. We ran such a baseline on the insertion task to confirm such a claim (see Figure 11).\n\n**#4: Adding proprioception information.**\n\nWe agree that proprioception (and any other sensing modalities) can certainly help robotic manipulation. However, this study aimed to isolate the contributions of vision and touch for representation learning, which is why we focus on these two modalities. We leave integration of proprioception and other modalities into our pipeline as future work.\n\n**#5: Out of three tasks, the proposed framework only outperforms vision-only baselines in In-hand rotation tasks.**\n\nOur results show that 1) vision+touch policies perform best when investigating zero-shot generalization, 2) training representations with vision+touch also improve vision policies, 3) representation learning approaches outperform end-to-end approaches when learning in-domain. 1) and 2) are shown in Figure 5 in the paper, while 3) is shown in Figure 6 in the paper. These results are consistent across all tasks, so we disagree with the reviewer comment. Please let us know if you have any other concerns regarding this.\n\n**#6: For generalization experiments, randomization on physical parameters might not be enough.**\n\nWe do not only have randomization on physical parameters, but also variation of object pose (door) and camera pose (hand) in the generalization experiments. On all such randomizations, M3L shows the smallest performance degradation compared to baselines.\n\n**#7: Can the learned model show the same level of performance if there is noise in visual observation or tactile reading?**\n\nThanks for the great suggestion! We added a noise robustness analysis in Figure 12 of the update appendix, where we add zero-mean Gaussian noise to both visual and tactile data at inference time. The figure shows the performance of M3L vs the end-to-end for increasing noise variance, highlighting how M3L success rates do not degrade up to a reasonable degree of disturbances and also how M3L consistently outperforms the end-to-end approach in terms of robustness."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700280927970,
                "cdate": 1700280927970,
                "tmdate": 1700280989611,
                "mdate": 1700280989611,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xR4ozpUqm5",
                "forum": "FMsmo01TaI",
                "replyto": "LF4JUD0yza",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6575/Reviewer_bnCo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6575/Reviewer_bnCo"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I appreciate authors for addressing many of my concern regarding this work, including additional baseline, visualization. \n\nHowever, for a work regarding tactile sensing, It's necessary to show results in the real world (with proprioception). for the following reasons:\n1. For most of robotics task, (if not all), proprioception of the robot is accessible and much easier to get compared to tactile and visual information. If proposed method only use vision or tactile information, it's possible the performance gain is from recovering the information loss for not using proprioception information. Overall, it's hard to evaluate the contribution of the work without proprioception in robotics work. \n2. Tactile sensing is known to be noisy and hard to transfer to the real world. Since this work is addressing the usage of tactile sensing for manipulation, it's necessary to show it in the real world. Simulation-only results can hardly show whether it's actually working or not.\n\nIn this case, I'll keep my original evaluation."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6575/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543687936,
                "cdate": 1700543687936,
                "tmdate": 1700543687936,
                "mdate": 1700543687936,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]