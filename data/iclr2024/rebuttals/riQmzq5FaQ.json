[
    {
        "title": "Reinforcement Learning with Elastic Time Steps"
    },
    {
        "review": {
            "id": "pOlhGTyhm8",
            "forum": "riQmzq5FaQ",
            "replyto": "riQmzq5FaQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2750/Reviewer_FcT3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2750/Reviewer_FcT3"
            ],
            "content": {
                "summary": {
                    "value": "The main contribution of this paper is the idea that, in RL, a policy can be made to specify both a control action to apply *and* the length of time an actuator should apply that action. The paper integrates this idea within an existing, popular algorithm for model-free RL (the SAC algorithm), and presents comparative results in a small example problem."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "As far as I know this precise idea is novel, and it is certainly intuitive. Results and other details aside, I think the community should investigate this direction more deeply and this paper provides a nice starting point for that effort."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n- The literature review is quite thin, and as a circumspect reader I do wonder how novel this idea really is, given how little literature is referenced. For example, a quick google scholar search reveals the following papers that seem to be very closely related: [1, 2]. I would also add that variable rate decision making is widely studied in the control theory literature. A key phrase to find this literature is \u201cadaptive time step.\u201d\n- The paragraph immediately above section 3.1 indicates that there are a lot of loose ends that are not being discussed in detail, and which may strongly affect results. The imprecision of this discussion (e.g., what is a \u201cpartial MPC\u201d and what role does the PID serve if you already use MPC?) suggests that the work may be somewhat immature.\n- The reward structure discussed section 3.1 is *not* what one would properly call a \u201cmulti-objective optimization problem.\u201d A distinguishing trait of such problems is the concept of \u201cPareto optimality\u201d which encodes all of the tradeoffs among optimal performance with respect to each separate objective. By assuming a fixed weighting, this paper effectively reduces the problem to a standard optimization problem (and picks a single point on the Pareto frontier). I recommend consulting [3] for further details.\n- Relatedly, the construction in Definition 1 is not as clear as it could be. For instance: are the R terms intended to be functions of state (and action)? If so, why does it make sense to only accrue reward at the times when actions are changed? Doesn\u2019t that lead to some obvious opportunities for reward hacking? For example, could an agent decide to plow straight through some region of low reward for a bunch of (unactuated) time steps? Also, can R_t and R_\\epsilon be evaluated at every time, or only at the end of an episode? Evidently, at every time t, but then I am lost as to why the agent is incentivized to minimize n, the length (in steps) of an elastic time step. I am lost.\n- The details of the method are really not very clearly explained. For example, throughout the discussion of section 3 it appears that the there is some notion of an agent physically moving and the policy gets to access a measure of distance somewhere. This is unclear: everything up until this point (and in general) is framed around general MDPs, which have nothing to do with physical embodiment. How general-purpose is the proposed approach?\n- Relatedly, the test environment is not very clearly explained, or at the very least, suggests a very basic question: wouldn\u2019t it make more sense for the policy to output a force, rather than a target position? This would remove the need for lower-level tracking control (MPC, PID) and also mitigate the \u201cmeasure of distance\u201d question above, I believe. \n- I do not follow the \u201csix dimensions of the state in the environment\u201d - in fact, I count 9: 2 each for agent/obstacle/goal position, 2 for agent velocity, and 1 for duration. What am I missing? In the same paragraph, the discussion of semi-Markov processes and recurrence is rather opaque. Use of words like \u201cmight\u201d and \u201ccould\u201d lead me to wonder how clearly this point is understood. I suggest clarifying the language here.\n- There are no discernible error bards in the plots, and the shaded areas appear to be traces of other plotted data - this needs to be explained precisely, and plots should show some measure of error in order to be interpreted statistically.\n- More importantly, even: there is little to no interpretation of the behavior of the proposed policies. Results here indicate some differences in aggregate behavior (although the interpretation to that effect should really require some error bars as above), but it would really help to understand what is going on if the authors expanded upon Fig. 7 to illustrate what was going on in the environments in these situations and why it made sense to change the control rate as shown.\n\nOther nitpicks:\n- It seems like the main motivation here is one of saving computational resources. Obviously, most control systems are pretty lightweight and so I imagine these savings really come in from the perception side, e.g., if you no longer have to process big images at high frame rate. Experimental results to illustrate these savings more directly than the abstraction of \u201cnumber of repeated actions\u201d would be highly motivating.\n- There are quite a few typos and other small syntax issues.\n- The vertical axis labels are wrong in Fig. 5.\n- Figures 5 and 6 could be more clear about indicating that the right hand sides are insets of the left. Also, why were the methods run for so long - it seems they all converged quite a bit earlier and then for some reason PPO destabilized. Something seems off here.\n- Why does Fig. 7 say \u201cepochs\u201d instead of \u201cconfigurations?\u201d\n\n\n[1] Chen, Y., Wu, H., Liang, Y., & Lai, G. (2021, July). VarLenMARL: A framework of variable-length time-step multi-agent reinforcement learning for cooperative charging in sensor networks. In 2021 18th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON) (pp. 1-9). IEEE.\n\n[2] Sharma, Sahil, Aravind S. Lakshminarayanan, and Balaraman Ravindran. \"Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning.\" International Conference on Learning Representations. 2016.\n\n[3] Deb, Kalyanmoy, and Kalyanmoy Deb. \"Multi-objective optimization.\" Search methodologies: Introductory tutorials in optimization and decision support techniques. Boston, MA: Springer US, 2013. 403-449."
                },
                "questions": {
                    "value": "Please see my comments above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2750/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697840366365,
            "cdate": 1697840366365,
            "tmdate": 1699636217885,
            "mdate": 1699636217885,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cKQdzaaHVi",
                "forum": "riQmzq5FaQ",
                "replyto": "pOlhGTyhm8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2750/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2750/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for acknowledging the merits of our paper. We share your conviction\nthat the elastic time step method holds significant potential, particularly in\nscenarios with constrained computational resources in real-world applications. We\nare exploring further developments in this direction.\n\nAt the same time, we extend our thanks for bringing to light the deficiencies in\nour paper. We are committed to addressing each of them.\n\n- In response to Q1, Thanks to the two references. We spent some time reading\n  them and agreed that our literature review section does need further\n  refinement. The core of these two articles is standby action or repeated\n  action to achieve changes in control rate, and they are all based on fixed\n  frequency without exception. They do not directly pursue the goal of\n  minimizing the number of control actions. We will include\n  these two articles in our literature review and clarify our differences.\n\n- In response to Q2, as described in Section 3.1, our model only provides the\n  execution time of each action, which is only one part of the control of an\n  actual robot. We cite MPC as just one of many possible control algorithms. We\n  agree that the description can cause confusion, and we will modify the\n  text to \"an appropriate control algorithm.\"\n\n- In response to Q3, it is a pertinent suggestion. Our core idea is to free as\n  many computing resources as possible, which can then be used for perception,\n  security, etc. Our description here can be improved according to the Definition\n  1, we scalarize this multi-objective problem into a single value through\n  weighting. We will remove the description of \"multiple-objective\" to ensure\n  there is no ambiguity.\n\n- In response to Q4, yes, we expect R to be a function of state (and action).\n  The reward R_t for the task can be arbitrarily defined, and it does not\n  necessarily require an action change to obtain the reward value. We will\n  improve our description of Definition 1 to underline that R is used to\n  evaluate each step individually.\n\n- In response to Q5, Due to space limitations, we have put the measurements\n  of the agent and environment in Appendix B. We have only\n  done experiments in this simple kinematic environment so far. The conversion\n  formulas are\uff1a\n\nD_{aim} = 1/2 \\cdot (V_{aim}+V_{current}) \\cdot T;\nV_{aim} = V_{current} + AT;\nF_{aim} = mA;\nF_{true} = F_{aim} \u2013 f_{friction};\nf_{friction}=\\mu mg, if F_{aim} > f_{friction}, else: f_{friction} = F_{aim}.\n\nD_{aim} is the distance that the agent needs to move as computed by the policy,\nT is the time to complete the movement as generated by the policy, m is the mass\nof the agent, \\mu is the friction coefficient, and g is the gravity acceleration.\nThrough the movement distance and time generated by the strategy, we can know\nthe target speed required to complete the strategy and then calculate the\nacceleration. Knowing the acceleration, we can find the force. However, due to\nfriction, the actual speed generated will be inconsistent with the target speed,\nand the movement of the agent will be affected by Newtonian kinematics. Our\nmethod aims at minimizing the use of computing resources by minimizing the\nnumber of steps and time required to complete the task.\n\n- In response to Q6, it is a pertinent suggestion. Setting the output to force\n  is indeed a better choice. We will add additional experiments to the paper.\n\n- In response to Q7, yes, you are right. We initially thought that time and past\n  action values should not belong to the state. But we agree they should also be\n  correctly defined as part of the state. We will restate the relevant\n  definitions.\n\n- In response to Q8, based on your feedback, we will conduct multiple\n  experiments (for example, six sets) and use this data to redraw Figures 5 and\n  6.\n\n- In response to Q9, Figure 7 records the time-related generation strategy of\n  the SEAC model in this environment. We do not have additional drawings because\n  SAC and PPO are both fixed times (such as 5 Hz mentioned in the article).\n  Figure 7 needs to be examined together with Figure 8: the SEAC model strategy\n  shows the use of fewer time steps in Figure 8. In Figure 6, SEAC also has a\n  shorter completion time, showing an advantage of SEAC in terms of total energy\n  consumption. We will add some additional explanation diagrams based on Figure\n  7 to explain the working of SEAC, SAC, and PPO strategies to make our work\n  more understandable and readable.\n\n- In response to Q10, thank you for recognizing our core concept of reducing the use of\n  computing resources. Based on your feedback, we will focus more on this aspect\n  in the text if modifications are needed. Unfortunately, we have not\n  yet completed the hardware experiment.\n\n- In response to Q11, thank you for pointing out grammar issues and typos; we will\n  try to improve the text as much as possible.\n\n- In response to Q12, thank you for pointing this out. We will fix the axis\n  of Figure 5."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626264123,
                "cdate": 1700626264123,
                "tmdate": 1700626264123,
                "mdate": 1700626264123,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QPeh8Edq9u",
                "forum": "riQmzq5FaQ",
                "replyto": "pOlhGTyhm8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2750/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2750/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "- In response to Q13, because the environment and tasks we set are relatively\n  simple, these algorithms have converged around 3 million steps. However, we\n  noticed that PPO would be unstable in this final period. We are not fully able to\n  explain the behavior of the neural network, and we are reporting it as is.\n\n- In response to Q14, thanks for the feedback. It should not be \"epoch\" but\n  \"episode.\" We will fix it."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626290719,
                "cdate": 1700626290719,
                "tmdate": 1700626290719,
                "mdate": 1700626290719,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3Xw5n5sLI5",
                "forum": "riQmzq5FaQ",
                "replyto": "QPeh8Edq9u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2750/Reviewer_FcT3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2750/Reviewer_FcT3"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response to my comments."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632182859,
                "cdate": 1700632182859,
                "tmdate": 1700632182859,
                "mdate": 1700632182859,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GBWY4BJU1r",
            "forum": "riQmzq5FaQ",
            "replyto": "riQmzq5FaQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2750/Reviewer_DZcS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2750/Reviewer_DZcS"
            ],
            "content": {
                "summary": {
                    "value": "This work presents relaxes the fixed frequency assumption of MDP typically studied in RL and proposes RL with elastic time steps.  Also a Soft Elastic Actor-Critic algorithm is derived with theoretical and practical benefits."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The work is concisely summarized.\n2. The use of elastic time is important in the tasks such as robotics etc."
                },
                "weaknesses": {
                    "value": "1. There are many existing studies with varying time (e.g. option framework, action repetitions\u2026)\nAuthors introduce some notions of options and semi-MDP in appendix, but without clear definitions of each notation, which makes it harder to see the clear connections to the main work and the option framework.  (It was not clear how the authors validated Bellman-like equations for elastic time case)  Assuming the algorithm is properly derived from the option framework, it is necessary to compare to the existing work based on the framework.  (Or at least it should show significant practical results compared to the existing work; it seems the experiments are not for sufficiently complex tasks.)\n2. Existing environments such as OpenAI Gym can be easily adjusted to include time as information for states; I am not sure what the authors mean by \u201c...additional input and output information that is not available within existing RL environments\u2026\u201d\n(Note that simulators anyway need to run with small time interval to maintain accuracy, and action durations can be just a repetition of that.)\n4.  Figure 5 is a bit hard to parse: why time in seconds are negative?  I could guess this but it is better to make them crystal clear.\n5.  It would be better to show baseline with 100Hz (fixed) case, not 5.0 Hz since the elastic one uses 1 to 100 Hz.\n6.  Figure 7 is also hard to interpret; why are there only 2 time steps\u2026?  2 steps are enough to complete tasks\u2026?\n7.  Finally, it was not clear why the authors specifically used the reward defined in Definition 1."
                },
                "questions": {
                    "value": "1.  Figure 4 right seems too sparse; what does it try to imply?\n2.  What is the action space A?  Is it the Cartesian product of \u201caction\u201d and \u201ctime\u201d?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2750/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2750/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2750/Reviewer_DZcS"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2750/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698475622052,
            "cdate": 1698475622052,
            "tmdate": 1699636217797,
            "mdate": 1699636217797,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gfIED5vq8T",
                "forum": "riQmzq5FaQ",
                "replyto": "GBWY4BJU1r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2750/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2750/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your appreciation of our work. Our core idea is to reduce the\ncomputational resources the agent requires by minimizing the number of steps\n(each of which requires to generate an action) rather than simply generating the\nexecution time of the action variable. For example, in simulation, there may be\nno difference between executing an action of 1 second 10 times or executing an\naction once for 10 seconds. However, their computational cost differs greatly in\na physical, embedded environment. A computer engineering analysis would be\ncomparing polling a resource as opposed to using an interrupt signal: in the former\ncase, the computer constantly checks if a resource is available, while in the latter\nthe computer is notified when the resource is available and can access it only\nwhen necessary.\n\n- In response to Q1, our work differs from action repetition or the options\n  framework (HRL for extended actions). These methods do not reduce the amount\n  of steps and thus do not reduce computational resources. For example, the\n  agent with SEAC policy can reduce the frequency at which it samples the\n  environment. However, we will add relevant references to\n  improve our literature review so that readers can more clearly understand the\n  differences with our work.\n\n  As for the issue in Appendix A, we will modify the relevant description to\n  clarify it. Although we do not have complex experimental results, we are\n  working in this direction. Our core idea can save computing resources compared\n  with methods such as action repetition.\n\n- In response to Q2, additional input refers to the historical value of the\n  agent, the duration of the previous step, and the action value of the last\n  step. The other output refers to the duration of the action to be performed.\n  We have a maximum frequency limit to ensure accurate execution of the\n  action, which is 100Hz. The above information can be found in Sections 3 and 5\n  of the article.\n\n- In response to Q3, thanks for pointing it out. We will fix Figure 5.\n\n- In response to Q4, it is a good proposal. We can do more groups, such as 1 Hz,\n  10 Hz, 60 Hz, 100 Hz, etc. We will use your feedback to improve our\n  experiments.\n\n- In response to Q5, our environmental tasks are relatively simple. Please refer\n  to Appendix B for specific physical information. Figure 7 needs to be viewed\n  together with Figure 8: to complete a task, the average number of steps\n  required by SEAC is 3, and the average number of steps needed by SAC is more\n  than 10. This is a reflection of our success in reducing computational resources\n  and time. In addition, we will add some model explanation diagrams based on\n  Figure 7.\n\n- In response to Q6, Figure 4 is a situation where we simplify an autonomous\n  driving problem into a simple Newtonian mechanics environment. Figure 4 just\n  shows the environment that we use to verify our algorithms. Please refer to\n  Appendix B for specific physical information. We will adjust Figure 4 so it\n  does not look so empty.\n\n- In response to Q7, the action set is (time for next step, move distance on X,\n  move distance on Y). Yes, it is the Cartesian product of \"time\" and \"action\".\n  The specific information can be viewed in Definition 10 of Appendix B."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626353329,
                "cdate": 1700626353329,
                "tmdate": 1700626353329,
                "mdate": 1700626353329,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uM4qwXiFfp",
                "forum": "riQmzq5FaQ",
                "replyto": "gfIED5vq8T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2750/Reviewer_DZcS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2750/Reviewer_DZcS"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Thank you for your responses;\nI still have concerns that the existing work could be easily used to reduce the amount of steps if the reward/state are properly designed.\nIf the focus of this work is really about reducing the required steps, I would like to see more focused arguments around it.\nAlthough I see some potential benefits of this work in the future, I keep my score for now.\nBut thank you for your clarification; it makes me feel that the work should become more solid in the future."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648389652,
                "cdate": 1700648389652,
                "tmdate": 1700648389652,
                "mdate": 1700648389652,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UGJiwWZVEU",
            "forum": "riQmzq5FaQ",
            "replyto": "riQmzq5FaQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2750/Reviewer_XuXs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2750/Reviewer_XuXs"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a reactive reinforcement learning policy, which breaks the fixed time step assumption commonly adopted in RL and determines the next action and the duration of the next time step as input to the controller, thus integrating the temporal aspect into the learning process. The authors test their approach in a simulation of a simple word with Newtonian kinematics, showing its effectiveness in leading to higher efficiency in terms of speed and energy consumption."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The contribution is clearly stated and it is relevant to the development of real-world efficient and effective RL-based control systems. The paper structure is well organized and clear. Figures and schemes are helpful and explanatory. Limitations of the proposed approach (which components would be necessary for a real-world implementation) are clearly stated."
                },
                "weaknesses": {
                    "value": "The contribution is relevant but it is limited compared to the existing state of the art. Since the contribution is mainly aimed to applying RL control outside of simulation, a proof of concept of the functioning of the proposed algorithm on a real-world application (rather than only in a simulation environment) would be important, in my view. \nAlthough the paper\u2019s quality of presentation is generally fair, I found the comparison with the related works poor and lacking of an insightful discussion about existing time-sensitive RL tasks, which are only quickly listed at the end of section 2. Expanding such a paragraph could make the relevance and applicability of the paper\u2019s contribution clearer.\nThe presentation of the results could also be improved (see specific comments on the next section)."
                },
                "questions": {
                    "value": "-\tFig 1: I don\u2019t find Fig 1 completely effective, based on the description within the Introduction. Since one of the contributions of the Elastic Time Step RL is that of enabling the policy to output the time step duration, together with the action, this could be somehow explicitly indicated in the Figure. Also, even though I understand the intention of splitting the \u201clearning\u201d and \u201cexecution\u201d part of a RL implementation, I find the brain-like icon confusing when used to indicate the \u201cexecution\u201d rather than the \u201clearning\u201d component of the system.\n-\tI would be curious to know from which specific practical application (robotics, autonomous driving?) comes the authors\u2019 inspiration for the paper.\n-\tPage 4, sentence preceding Definition 1: \u201cThe aggregate reward for task completion is represented by r\u201d. Did you mean \u201cR\u201d (capital letter)? \n-\tThe paragraph after Definition 1 (\u201cWe validate our reward strategy\u2026\u201d) could be rephrased to highlight SEAC differences compared to SAC.\n-\tWhat do you mean when you say \u201c\u2026giving a high probability that the agent can discover the optimal solution to complete the task\u201d?  Maybe this sentence can be rephrased to make the exploration strategy clearer.\n-\tIn general, from the sentence starting \u201cwe assume the agent\u2026\u201d to the sentence ending with \u201c\u2026Bellman equation\u201d, I find the flow of the text, which can be read while referring to the scheme on Fig 3, a little hard to follow, in the sense that it jumps from one block to another one (of the Fig.3) without a precise order. Incorporating more references to the visual scheme and aligning the text with the functional flow of the figure 3 (rather than simply listing the meaning of the symbols) could help the readability. \n-\tYou mention that one major contribution of the SEAC is to include the execution time of each action to the output, but this term is not explicitly indicated on Fig.3, together with the At.\n-\tThe meaning of the double arrows in Fig.3 is not very clear to me. Maybe an explanation could be included either on the caption or on the main text.\n-\tThe impact value of the execution is defined, based on the chosen environment, as the target movement distance. Do you have in mind some examples of different implementations for different problems?\n-\tIn the end of paragraph 3.1, when you say \u201cthe controller will compute a range of control-related parameters\u201d, is this represented by Mt?\n-\tIn the end of paragraph 3.1, when you say \u201cour objective is for the agent to learn the optimal execution time\u201d, is the execution time equivalent to the action time, and therefore represented by Tt?\n-\tTypo: \u201cbut but\u201d in the sentence starting with \u201cit is worth noting\u2026\u201d in paragraph 3.2\n-\tWhat is the meaning of \u201cp\u201d in eq. (2)?\n-\tSince the SAEC loss functions are (if I understand well) equal to those of SAC, rather than simply reporting the definitions, I would suggest to reorganize Section 4 to better explain how your formulation of the reward function is included in the update steps of the RL algorithm.\n-\tSection 5: When you refer to the \u201cthree RL algorithms\u201d, do you mean SEAC, SAC and PPO? In this case, you should first say that you are comparing SEAC results with SAC and PPO in the text, otherwise it is not clear to the reader.\n-\tWhat are you representing differently on the left and right side of Fig. 5 and 6? Is it the right side simply a y-axis zoom-in of the left side? You should specify it on the figures' captions. What is the legend for the lighter colored plots?\n-\tI think that Fig. 7, as it is, is not very informative. It shows that SEAC dynamically changes the control rate, but it doesn\u2019t allow to evaluate whether it does it in a meaningful way. Showing the scenario and/or information about the corresponding actions would make the concept clearer.\n-\tI feel Fig.8 would be more readable by inverting x and y axes (evaluation metric on the y-axis). Furthermore, you mention the overall reward both in the section and in the figure caption, but is the overall reward shown somewhere?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2750/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2750/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2750/Reviewer_XuXs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2750/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698673454455,
            "cdate": 1698673454455,
            "tmdate": 1699636217727,
            "mdate": 1699636217727,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GF68fmSyks",
                "forum": "riQmzq5FaQ",
                "replyto": "UGJiwWZVEU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2750/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2750/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments. We are dedicated to enriching our literature\nreview to give readers a comprehensive understanding of how our algorithms are\napplied in real-life scenarios. \n\n- In response to Q1, it is a pertinent suggestion. SEAC itself does not contain\n  an execution unit. It only provides the action value required for execution\n  and the time of the action. We can optimize it by replacing the brain icon with\n  a clock and changing the description of the arrow to \"Execute an action with a\n  fixed amount of time.\"\n\n- In response to Q2, onboard computing can be severely constrained in many\n  robots. It is worth mentioning that the core of article [1] is the opposite of\n  ours: [1]1 aims to control as fast as possible, while we strive to control as\n  little as possible.\n\n- In response to Q3, yes, thanks for your correction. We will fix it.\n\n- In response to Q4, it is a pertinent suggestion. The loss function of SEAC is\n  consistent with that of SAC. As you proposed next, we will modify the\n  expression here so that people can clearly understand that we only implement\n  Definition 1 through SAC. The same idea can be achieved with other types of\n  reinforcement learning algorithms. It is worth noting that we didn't create a\n  new reinforcement learning algorithm.\n\n- In response to Q5, we will rewrite it as \"make the agent learn the best time and\n  corresponding action policy\".\n\n- In response to Q6, Q7, and Q8, they are pertinent suggestions. We can modify\n  Figure 3 and its caption describe more clearly how some low-level\n  parameters, such as time t, etc., participate in the training process.\n\n- In response to Q9, yes, such as the classic robot arm problem: one assumes\n  that the output of the strategy set is a force set. Suppose you want to\n  quickly grab a fragile object, such as an egg, without considering other\n  interferences, such as the material of the robot arm. In that case, the grip\n  strength of the robot arm and direction are essential, and the duration of the\n  force is even more critical. Any application of reinforcement learning\n  algorithms in real-world environments of continuous control type can adopt our\n  elastic time step idea.\n\n- In response to Q10, no, M_t refers to the movement value of the previous step.\n  The physics-related properties of the vehicle itself do not, and should not,\n  participate in network training. Please refer to Appendix B. The series of\n  related control parameters here refer to speed and acceleration.\n  As we mentioned in another reply, we will modify Figure 3 and the\n  corresponding marks to make our work more readable and understandable.\n\n- In response to Q11, yes, the execution time equals the action time.\n\n- In response to Q12, sorry, we will fix it.\n\n- In response to Q13, p is the last step. We will improve the expression of\n  Definition 2.\n\n- In response to Q14, it is a good suggestion. We can rephrase Section 4 to show\n  how some low-level parameters work with the model training based on a modified\n  Figure 3.\n\n- In response to Q15, it is a good suggestion. We will modify the description to\n  explain what kind of algorithms we are comparing.\n\n- In response to Q16, our original intention with Figure 7 was to inform readers\n  that SEAC takes fewer time steps to achieve the set goals. The agent will\n  first take a long step to achieve the goal and then fine-tune it. We will add\n  some model explanation diagrams based on Figure 7 to explain the workings of\n  SEAC, SAC, and PPO strategies to make our work more understandable and\n  readable.\n\n- In response to Q17, it is a good suggestion. We can change it to a rain cloud\n  image. Figure 5 is realistic about the overall reward. As defined in 1, the\n  total reward includes the number of steps and time used.\n\n[1] Bouteiller, Yann, et al. \"Reinforcement learning with random delays.\" International conference on learning representations. 2020.\n\n[2] Bregu, Endri, et al. \"Reactive control of autonomous drones.\" Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services. 2016."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626408943,
                "cdate": 1700626408943,
                "tmdate": 1700626408943,
                "mdate": 1700626408943,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BGMjBm8M96",
            "forum": "riQmzq5FaQ",
            "replyto": "riQmzq5FaQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2750/Reviewer_VwVw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2750/Reviewer_VwVw"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends the classical RL setting, where there is no concept of the action execution time, to RL with elastic time steps. The authors propose SEAC to output the next action as well as the duration of the next time step."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The proposed problem is interesting. The figures are vivid, and the paper is easy to follow."
                },
                "weaknesses": {
                    "value": "The contribution and novelty is vague. As for the traditional RL, the control frequency is only an abstract definition. I think the proposed framework can be seen as a special instance of the traditional RL framework given a reformulated action space / state space / reward function. The algorithm also seems quite like SAC with new state / actions. Also, what is the relationship between the proposed algorithm with HRL methods?"
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2750/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698679131509,
            "cdate": 1698679131509,
            "tmdate": 1699636217647,
            "mdate": 1699636217647,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "heHHlAoI94",
                "forum": "riQmzq5FaQ",
                "replyto": "BGMjBm8M96",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2750/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2750/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments. The framework we propose should be a flexible time-step \nalgorithm that is versatile and minimizes energy loss and time. SEAC is\nimplemented on top of SAC, but this idea can be applied to other off-policy\nalgorithms. Please imagine you do not need high-frequency computing of\nenvironmental perception-related components, such as images, point clouds, and\nadditional information: the freed computational resources can be used to make\nrobots cheaper or more reactive to the environment.\n\n\nIn response to your question. HRL, as we understand it, aims to solve the sparse\nreward problem. When the environmental rewards are too light, the agent may be\nunable to obtain samples with positive rewards for a long time, which brings\ndifficulties in learning value functions and strategies. Therefore, it has a\nhigh-level reward policy and at least one low-level reward policy. In our case,\nwe have only one policy. What\u2019s more, the focus of HRL is not to reduce the\namount of calculation, nor does it help reduce the time. One can use HRL to\nlearn the duration of time steps to plan a large goal and multiple subgoals\nand set different rewards, but this will require more complex models to be\ntrained, increasing the computational burden on the agent.\n\nIn short, our algorithm is not an HRL algorithm. Its ideas could be implemented\nin an HRL framework, but our goals are not the same.\n\nWe will add HRL and repeat related references to improve our literature review\nso that readers can more clearly understand the differences between our and\noption algorithms."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626471724,
                "cdate": 1700626471724,
                "tmdate": 1700626471724,
                "mdate": 1700626471724,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]