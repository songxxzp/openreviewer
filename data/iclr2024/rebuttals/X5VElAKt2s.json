[
    {
        "title": "LoRA ensembles for large language model fine-tuning"
    },
    {
        "review": {
            "id": "KIsX1SVvyF",
            "forum": "X5VElAKt2s",
            "replyto": "X5VElAKt2s",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3048/Reviewer_QbwE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3048/Reviewer_QbwE"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces LoRA ensembling to improve LLMs' performance and uncertainty calibration. The core idea is to train multiple LoRA adapters and ensemble them to get more accurate and calibrated predictions, as typically shown in deep ensembling literature. Experiments results verify that LoRA ensembling improves over baseline approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method is built on top of well-known results that model ensembles can lead to more accurate and calibrated predictions.\n- LoRA ensemble alleviates the need to finetune and update the entire model which is computationally prohibitive.\n- Experiment results show that LoRA ensemble does lead to more accurate results as well as reduced calibration error."
                },
                "weaknesses": {
                    "value": "- It is rather straightforward to consider LoRA finetuning for ensembling. The technical novelty of the proposed method is a bit limited.\n- There are several relevant and stronger baselines not considered in the experiments, including calibration for in-context learning [1], and self-consistency [2], both of which shows decent improvements on prediction accuracy.\n- The current set of experiments considered is limited to multiple choice questions (predicting only a single token). While the method is indeed compatible with generative tasks, there are no such tasks considered in the experiments. The results would be more convincing with generative tasks since LLMs are commonly used for complex tasks.\n- LoRA ensemble requires finetuning datasets. However, LLMs are commonly used in zero or few-shot ways in many scenarios. How does the method perform if only a few finetuning data is available?\n\n[1] Calibrate Before Use: Improving Few-Shot Performance of Language Models. Zhao et al. 2021.\n\n[2] Self-Consistency Improves Chain of Thought Reasoning in Language Models. Wang et al. 2022."
                },
                "questions": {
                    "value": "- The presentation of experiment results are scattered. It would be easier to compare different methods if they are all listed in the same table/figure. Currently, comparisons to different baselines are in separate tables/figures, making it a bit hard to read.\n- In Figure 3, 4, 5, and 6, why ECE increases as number of epoch increases? In this case, isn't the method making calibration worse by LoRA finetuning?\n- In many cases, finetuning on specific dataset can compromise the model's performance on other tasks. Does LoRA ensemble suffer the same problem?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3048/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816621679,
            "cdate": 1698816621679,
            "tmdate": 1699636250194,
            "mdate": 1699636250194,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D3zStX8tJv",
                "forum": "X5VElAKt2s",
                "replyto": "KIsX1SVvyF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3048/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3048/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "## Novelty of the method\n\n Please refer to the general response.\n\n## Comparision with self-consistency baseline\n\nSelf-Consistency [1] shows improvement upon chain-of-thought, however, as a few-shot-learning-based approach, it is outperformed by LoRA ensemble, which utilizes model fine-tuning, we provide a comparison of accuracy based on the numbers from Table. 3 in [1]. \n| Method                           | Commonsense QA | ARC-C | ARC-E |\n|----------------------------------|----------------|-------|-------|\n| LoRA Ensemble with Llama-13B     | 83             | 69    | 86    |\n| Self-Consistency with LaMDA-137B | 63.1           | 59.8  | 79.3  |\n\nwhere LoRA ensembles outperform Self-Consistency with a backbone model ten times smaller.\n\n## Multiple choice QA tasks\n\nThe main reason we choose to stick to multiple-choice QA is that the metric for measuring calibration and uncertainty quantification is well-defined. On open-ended generative tasks, how to measure output's calibration and uncertainty remains an open question. For example, [3] shows that there are multiple ways to quantify the uncertainty in open-ended QA problems while each metric could have different behaviors. We consider combining these advanced metrics with LoRA ensemble as a future research problem.\n\n\n## Scenarios with limited fine-tuning data\n\nWhen there is only a limited amount of fine-tuning data available, the fine-tuned model would demonstrate severe overconfidence, as we can see from the experiment results from MMLU social sciences and STEM experiments, where there are only 397 and 411 training samples available: The single model (LoRA M=1) shows high ECE and NLL while LoRA ensembling significantly resolves the overconfidence. Our results show that the performance can be further improved by combining regularization with LoRA ensemble.\n\n## Presentation of the results\n\nThanks for the suggestion, we plan to add a table that summarizes the results from all different methods and baselines in the appendix for easier comparison.\n\n## Increasement calibration error\nNote that the training objective for LoRA fine-tuning is cross-entropy loss, therefore the fine-tuned model would tend to keep reducing the training loss by making the predictive distribution sharper, this is fine on the training set, as the model can reach %100 accuracy, however on the test set, the model would suffer from *increasing* calibration error (ECE) and negative log-likelihood (NLL) as the model is making overconfident and wrong predictions. Our results show that this can be alleviated by using LoRA ensemble, while weight decay regularization can further prevent the ECE from growing up.\n\n\n## Performance degradation on other tasks\nThe top row in Figure. 5 shows the performance on MMLU from the model fine-tuned on commonsense QA. We can see that, while both the single LoRA and LoRA ensembles show decreased accuracy as fine-tuning proceeds, LoRA ensembles still show improvement of performance upon the single fine-tuned model, we believe this implies that LoRA ensembles suffer less from performance degradation.\n\n---------------\n[1] Calibrate Before Use: Improving Few-Shot Performance of Language Models. Zhao et al. 2021.\n\n[2] Self-Consistency Improves Chain of Thought Reasoning in Language Models. Wang et al. 2022.\n\n[3] Kuhn, Lorenz, Yarin Gal, and Sebastian Farquhar. \"Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation.\" arXiv preprint arXiv:2302.09664 (2023)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3048/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700109054936,
                "cdate": 1700109054936,
                "tmdate": 1700109054936,
                "mdate": 1700109054936,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uPr4jfPZF1",
                "forum": "X5VElAKt2s",
                "replyto": "D3zStX8tJv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3048/Reviewer_QbwE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3048/Reviewer_QbwE"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you to the authors for the response. \n\nFor comparison to other baselines like self-consistency, a fair comparison would be self-consistency or calibration on a LoRA \nfine-tuned model.\n\nOverall, I still have concern on the novelty of the method and the need of finetuning data for the method to work. I would remain my score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3048/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504606246,
                "cdate": 1700504606246,
                "tmdate": 1700504606246,
                "mdate": 1700504606246,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wXDV7zfWvS",
            "forum": "X5VElAKt2s",
            "replyto": "X5VElAKt2s",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3048/Reviewer_o2gF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3048/Reviewer_o2gF"
            ],
            "content": {
                "summary": {
                    "value": "This conference paper discusses issues of Large Language Models (LLMs) in overconfidence and uncertainty in their predictions. To mitigate these issues, the paper proposes a new approach called LoRA ensembles, which leverages low-rank adapters and random initialization to create diverse model components. This method addresses the limitations of traditional ensemble approaches, such as excessive storage requirements and a lack of diversity in fine-tuned LLMs. The authors demonstrate the effectiveness of LoRA ensembles in improving accuracy and calibration for various reasoning tasks compared to alternative fine-tuning methods and introduce the concept of regularized LoRA ensembles to further enhance performance and address potential correlations between components. This research enables the scaling of ensemble methods to LLMs with billions of parameters, offering a promising solution for enhancing the reliability of LLM predictions in safety-critical applications like medical diagnosis, finance, and decision-making processes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) The paper is well-written and easy to understand.\n\n(2) This paper analyses potentials of LoRA ensemble with various techniques, such as regulizers, Dropout, weight decay. \n\n(3) The ablation study of LoRA with randomness is interesting."
                },
                "weaknesses": {
                    "value": "(1) One major concern is the idea is very naive and straightforward. The performance improvement of deep ensemble is already well-known  to the community, and it is in no way surprising that we can combine LoRA with ensemble to improve the performance, uncertainty, etc.\n\n(2) Another concern is that no computational costs is reported in this paper. I understand the inference costs of LoRA ensemble is much lower than traditional finetuning ensemble, but it is good to demonstrate this. \n\n(3) The title in the paper doesn't exactly match the title in the openreview system.\n\n(4) Figure 7 is not easy to understand. It is better to put all variants in one figure or use tables to compare the performance of various combination of methods."
                },
                "questions": {
                    "value": "Please see the above weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3048/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698960535746,
            "cdate": 1698960535746,
            "tmdate": 1699636250114,
            "mdate": 1699636250114,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EmIUoSSYN9",
                "forum": "X5VElAKt2s",
                "replyto": "wXDV7zfWvS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3048/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3048/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "## Novelty of the method\n\nPlease refer to the general response.\n\n## Computational cost\n\nWe provide a summarization of loading time and memory cost below under Llama-13b and LoRA of rank 8. Note that the memory requirements for full-model ensembling are beyond the capacity of an 80GB A100 GPU, therefore at test time, one may need to load and unload the models for ensembling, causing extra overhead.\n\n|              | Full model ensemble (M=5) | LoRA ensemble (M=5) |\n|--------------|---------------------------|---------------------|\n| Loading time | ~30s                      | ~6.5s               |\n| Memory usage | ~129GB                     | ~26GB               |\n\n## Discrepancy in title\nWe apologize for the mistake, we will fix the title in later revisions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3048/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700109143129,
                "cdate": 1700109143129,
                "tmdate": 1700109143129,
                "mdate": 1700109143129,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KB2XHzErfI",
            "forum": "X5VElAKt2s",
            "replyto": "X5VElAKt2s",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3048/Reviewer_zRFn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3048/Reviewer_zRFn"
            ],
            "content": {
                "summary": {
                    "value": "The paper argued ensembling LLMs is computationally challenging due to the sheer size of such models. In light of this, the authors proposed a solution to create an ensemble of models with Low-Rank Adapters (LoRA), referred to as LoRA ensembles. These ensembles are much smaller in terms of parameters and can be efficiently constructed on top of underlying pre-trained models. The results demonstrate that LoRA ensembles, when applied independently or in combination with other regularization techniques, offer improved predictive accuracy and achieve better uncertainty quantification."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The introduction of LoRA for ensembling is a unique approach, particularly for large models like LLMs. This could be a  useful exploration of how to ensemble such massive models.\n- LoRA ensembles, whether used independently or in conjunction with other techniques, demonstrate enhancements in both prediction accuracy and the quantification of uncertainty.\n- The observation that regularization may benefit calibration over just the improvement from ensembling can hold practical value."
                },
                "weaknesses": {
                    "value": "- The paper lacks a comprehensive survey of existing ensemble methods, and it does not adequately discuss or compare with related works such as [1,2,3,4,6] in the literature.\n- The focus of the paper is only on prediction ensembles, which neglects the important weight ensemble methods [1,3,4,6]. The paper argues that maintaining an ensemble of, for instance, 5 LLMs in memory can be challenging in certain scenarios. However, it's worth noting that weight ensembles require the maintenance of just one model. Recent papers adopt online ensemble methods [3] that continuously average weight parameters.\n- The concept of ensembling adapters in LLMs has been previously explored in [5], yet this prior work is neither discussed nor compared in the paper.\n- The method's evaluation is restricted to small datasets, and its scalability remains unverified. Furthermore, the absence of actual ensemble baselines is notable. For example, [1,3] employ ensemble techniques while training the model only once, which is highly relevant to the task addressed in this paper.\n\nA minor issue is the presence of a discrepancy between the title displayed on OpenReview and the actual title in the paper.\n\n[1] Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity. ICLR 2021\n\n[2] Training Independent Subnetworks for Robust Prediction. ICLR 2020\n\n[3] SWAD: Domain Generalization by Seeking Flat Minima. NeurIPS 2021\n\n[4] DNA: Domain generalization with diversified neural averaging. ICML 2022\n\n[5] AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models. EACL 2023\n\n[6] Averaging Weights Leads to Wider Optima and Better Generalization. UAI 2018"
                },
                "questions": {
                    "value": "Please refer to the weaknesses mentioned above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3048/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3048/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3048/Reviewer_zRFn"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3048/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698988087685,
            "cdate": 1698988087685,
            "tmdate": 1699636250030,
            "mdate": 1699636250030,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E3PZFfn5I5",
                "forum": "X5VElAKt2s",
                "replyto": "KB2XHzErfI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3048/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3048/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "## Weight averaging v.s. (output space) ensembling.\n\nThanks for pointing out the related literature on weight averaging. We will add discussion to weight space averaging methods in later revisions.  It is important to clarify, though, that we use the term \"ensemble\" to represent output space ensembling following classic literature ([2]), we are unaware of the potential confusion with weight space averaging during writing, which we will further clarify in later revisions.\n\nWe opted not to include weight averaging methods as a baseline for several reasons. Firstly, in studies focusing on ensembling to enhance neural networks' calibration and uncertainty quantification, such as [4, 5, 6], weight averaging methods like SWA [7] are rarely used as a baseline. The reason is that weight space averaging aims at solving problems fundamentally different from output space ensembling: Weight space averaging methods target acquiring models with **better generalization**, which is usually measured by accuracy. In contrast, output space ensembling methods, including Bayesian neural networks, MC dropout, and deep ensembles, aim to address issues of overconfidence and poor calibration, evaluated using metrics like NLL and ECE. Notably, weight-averaging methods are **not** recognized for resolving calibration and overconfidence issues, with calibration error and out-of-distribution detection performance seldom serving as evaluation metrics in weight-averaging studies.\n\nMoreover, an examination of Tables 3, 4, and 5 in the appendix of [1] reveals that SWA's calibration error is consistently surpassed by output space ensembling methods. To further investigate the impact of weight averaging on LLM fine-tuning, we applied SWA to LoRA by averaging weights across five iterations. Our results, detailed below, indicate that while SWA does enhance accuracy, it fails to improve NLL and ECE. This suggests that SWA is not effective in addressing the overconfidence and calibration problems central to our study.\"\n\n\n| Method              | Accuracy | Negative log-likelihood (NLL) | Expected calibration error (ECE)|\n|---------------------|----------|-------------------------|----------------------------|\n| LoRA (M=1)          | 0.808    | 0.948                   | 0.153                      |\n| LoRA (M=1, SWA)     | 0.813    | 1.18                    | 0.156                      |\n| LoRA Ensemble (M=5) | **0.832**    | **0.581**                   | **0.064**                      |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3048/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700109235983,
                "cdate": 1700109235983,
                "tmdate": 1700109308521,
                "mdate": 1700109308521,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CT9WTOovMq",
                "forum": "X5VElAKt2s",
                "replyto": "KB2XHzErfI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3048/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3048/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response cont."
                    },
                    "comment": {
                        "value": "## Scalability of the method.\n\nIt is unclear to us what type of \"scalability\" the reviewer refers to. The biggest dataset we used is commonsense QA, which has 9.8K training samples. With LoRA ensemble, we are also able to perform large-scale ensembling experiments using 15 ensemble components (Fig. 6), which is not possible for full model fine-tuning. We also did not see any obstacles in applying LoRA ensembles to larger datasets or models without trouble. Essentially, if LoRA can be applied for fine-tuning, LoRA ensemble can be applied to boost the performance. We are happy to incorporate more experiments if the reviewer could elaborate more on scalability.\n\n##  Baseline on ensemble method.\n\nIn the \"Ensembling of Neural Networks\" and \"Ensembling in LLMs\" paragraphs in the related work section, we discussed works on ensembling methods. However, these methods only consider training ensembles from scratches and it is unclear how one can apply these methods for LLM fine-tuning: Adapting these methods to LLM fine-tuning is beyond the scope of our paper. We are also not aware of any works that perform (output space) ensembling for LLM fine-tuning, we are happy to include these methods as baselines if the reviewer could kindly point out references to these works.\n\n## Relationship with AdapterSoup\n\nWe apologize for missing the reference. However, our method differs from AdapterSoup in the following aspects: 1. AdapterSoup focuses on improving out-of-domain generalization while LoRA ensembles focus on improving overconfidence and calibration; 2. AdapterSoup trains multiple adapters on different domains starting from the same random seed while LoRA ensemble trains multiple adapters on a single task using different random seeds;  3. AdapterSoup uses the adapter proposed by [8] while LoRA ensembles use low-rank adapters and it is unclear (from the paper) whether AdapterSoup can alleviate overconfidence and bad calibration.\n\n -----------\n\n[1] Maddox, Wesley J., et al. \"A simple baseline for bayesian uncertainty in deep learning.\" Advances in neural information processing systems 32 (2019).\n\n[2] Sollich, Peter, and Anders Krogh. \"Learning with ensembles: How overfitting can be useful.\" Advances in neural information processing systems 8 (1995).\n\n[3] AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models. EACL 2023\n\n[4] Wen, Yeming, Dustin Tran, and Jimmy Ba. \"Batchensemble: an alternative approach to efficient ensemble and lifelong learning.\" arXiv preprint arXiv:2002.06715 (2020).\n\n[5] Izmailov, Pavel, et al. \"What are Bayesian neural network posteriors really like?.\" International conference on machine learning. PMLR, 2021.\n\n[6] Florian Wenzel, Jasper Snoek, Dustin Tran, and Rodolphe Jenatton. Hyperparameter ensembles for robustness and uncertainty quantification. Advances in Neural Information Processing Systems, 33:6514\u20136527, 2020b.\n\n[7] Averaging Weights Leads to Wider Optima and Better Generalization. UAI 2018\n\n[8] Bapna, Ankur, Naveen Arivazhagan, and Orhan Firat. \"Simple, scalable adaptation for neural machine translation.\" arXiv preprint arXiv:1909.08478 (2019)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3048/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700109267374,
                "cdate": 1700109267374,
                "tmdate": 1700109335114,
                "mdate": 1700109335114,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]