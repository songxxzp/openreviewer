[
    {
        "title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing"
    },
    {
        "review": {
            "id": "QHjVJbOXOx",
            "forum": "Sx038qxjek",
            "replyto": "Sx038qxjek",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4343/Reviewer_hutJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4343/Reviewer_hutJ"
            ],
            "content": {
                "summary": {
                    "value": "The papers proposes CRITIC, a framework for composing programs involving LMs self-correcting themselves using external tools. The authors conduct experiments with question answering, program synthesis and toxicity reduction show that CRITIC consistently improves the performance of LLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is written clearly and easy to follow.\n2. I found the comparison with ReAct interesting, i.e. the role of parameter knowledge vs language feedback."
                },
                "weaknesses": {
                    "value": "1. I'm not convinced the CRITIC framework is novel enough to count as a contribution. The idea idea of using natural language feedback [1, 2, 3, 4] that guides LMs in revising their responses is pretty old as is the idea of using tools [5,6]. I agree the authors provide a nice unifying framework and some new downstream tasks (e.g. toxicity with PerspectiveAPI), but these don't seem to be pass the bar for ICLR. \n\n2. The authors don't compare with other frameworks endowing LMs with self-correction and tool use, like the ones listed above.\n\n3. I think the claim that tool use \"mimic[s] human thinking and behavior\" is overblown. Humans use think and work with tools very differently, typically not through a text-only interface. \n\n[1] https://arxiv.org/abs/2204.14146\n\n[2] https://arxiv.org/abs/2303.11366\n\n[3] https://arxiv.org/abs/2212.08073\n\n[4] https://arxiv.org/abs/2303.16749\n\n[5] https://arxiv.org/abs/2207.14502\n\n[6] https://arxiv.org/abs/2302.04761\n\n[7] https://openai.com/blog/function-calling-and-other-api-updates"
                },
                "questions": {
                    "value": "How does the paper compare with other frameworks endowing LMs with self-correction and tool use?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4343/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698778711238,
            "cdate": 1698778711238,
            "tmdate": 1699636404720,
            "mdate": 1699636404720,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iGRn1vq3bB",
                "forum": "Sx038qxjek",
                "replyto": "QHjVJbOXOx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for thoroughly reviewing our work! We have carefully considered all your concerns and addressed them in the following comments.\n\n## CRITIC Compared to Existing Works on Self-Correction and Tool-use\n\n> The idea idea of using natural language feedback [1, 2, 3, 4] that guides LMs in revising their responses is pretty old as is the idea of using tools [5,6]. I agree the authors provide a nice unifying framework and some new downstream tasks (e.g. toxicity with PerspectiveAPI), but these don't seem to be pass the bar for ICLR.\n\nThank you for raising these concerns! Here we discuss why we believe CRITIC is novel, and more than a trivial blend of existing works on self-correction [1, 2, 3] and tool use [4,5]. While these are crucial fast-moving areas of LLMs, CRITIC offers unique findings, integrated frameworks, and valuable insights, making CRITIC a distinct contribution.\n\nThe relevant papers in these two fields you mentioned actually **have very different research perspectives and stances from CRITIC**:\n\n### **Intrinsic Self-Correct with NL feedback**\n\nWorks include Self-Critique [6], CAI [7], Reflexion[1], Self-Refine [2], and others [8-10] you mentioned, they prompt or train language models to correct their results. In contrast, our study is the first to demonstrate that such a \"Self-Verification and Self-Correction\" approach has proven to be remarkably unreliable across diverse tasks and various LLMs (Sec. 4 and Appendix D.1). Specifically, **modest improvements or even deterioration are observed universally using self-correct without external feedback**. Consequently, CRITIC emphasizes the importance of feedback from external interactions for the consistent self-improvement of LLMs. The proposed framework is general and has proven effective across multiple tasks. These profound reflections on unreliable self-correction of LLMs and crucial findings on the importance of external feedback can be important learnings for the community, as recognized by Reviewer [rWYL].\n\n### **The Unreliability of Self-correction**\n\nMoreover, CRITIC further delves into the core reason behind the unreliability of self-verification from the perspective of uncertainty estimation, as shown in Appendix D.1. Essentially, the models are **incapable of accurately identifying \"what they know\" (i.e., LLMs don't know what they know)** [12] without relying on external tools. Therefore, without the aid of oracle verification (employed in many contemporary works such as Reflexion [1] and Self-Refine [2]), self-correction might surprisingly deteriorate performance for many tasks, even worsening the initial answer (as demonstrated in Table 2, 3 under CRITIC w/o Tool, and in Table 8 under Self-Refine).\n\nAs noted by Reviewer [dDfr], a nice recent follow-up work to CRITIC, titled \"LLMs cannot self-correct reasoning yet\" [14] extends the study of CRITIC (which cites CRITIC numerous times). It further validates and expands our findings on the unreliability of Self-Verify and Self-Correct in CRITIC using GPT-4 in more settings on many reasoning tasks.\n\n### **Tool-Use**\n\nAnother category of related work is Tools Augmented LMs, such as ReAct [4], PoT [5], and Toolformer [17]. We would like to clarify that these works significantly differ from CRITIC as they focus on tool learning. To the best of our knowledge, **none of them consider using tool-interaction as faithful feedback to iteratively improve the model, which is the key component of CRITIC.**\n\nIn conclusion, CRITIC is the first to unveil the unreliability of self-verification and self-correction across diverse tasks and LLMs of various families and sizes. By initially highlighting the challenges LLMs encounter in self-verification [12], self-correction [6,7], our goal is to rectify any potential overestimations of these LLM's abilities within the research community [1-3]. By emphasizing that feedback from external tool interaction is crucial for consistent self-improvement of LLMs, we hope our findings provide valuable insights and encourage further exploration and enhancement of self-improving LLMs."
                    },
                    "title": {
                        "value": "Author Response to Official Review by Reviewer hutJ (1/2)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4343/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699891233169,
                "cdate": 1699891233169,
                "tmdate": 1699974686814,
                "mdate": 1699974686814,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tK3hj4v2Zb",
                "forum": "Sx038qxjek",
                "replyto": "QHjVJbOXOx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer hutJ (2/2)"
                    },
                    "comment": {
                        "value": "## Experiments of CRITIC Comparing Other Self-Correct and Tool-Use Frameworks\n\n> The authors don't compare with other frameworks endowing LMs with self-correction and tool use, like the ones listed above.\n\nIn fact, CRITIC has been compared with the most advanced baselines of Self-Correct and Tool Use across multiple tasks in the original paper:\n\n- Firstly, the work you mentioned includes methods for self-correction based on natural language feedback [1-3, 6-10], which is akin to the \"CRITIC w/o Tool\" baseline implemented and compared in many tasks in our paper, which relies on self-correction without using external tools.\n- Moreover, in the QA task, CRITIC is even compared with a concurrent work, Self-Refine [2], as shown in Table 5. We also compared it with the Self-Correct [11] method that trains an additional corrector in the Toxicity Reduction task. We have conducted detailed comparisons and discussions on related baselines in all experiments and in Appendix C.1.\n- Additionally, you mentioned the tool-use works like Toolformer [17] and many others [3,4]. We have actually compared the most advanced tool-use methods in each domain, such as the search engine-based ReAct [4] in the QA tasks; PoT [3] that used a Code Interpreter in the Math Program Synthesis tasks; Quark [18] and Self-Correct [11] that utilized Perspective API for Toxicity Reduction, etc.\n\nWe have ensured fairness in tool use and self-correct settings for all baselines across all these comparative experiments covering a wide diversity of tasks. These comprehensive comparisons across various tasks, LLMs, and model sizes collectively demonstrate the generality and effectiveness of CRITIC.\n\n## Revising the phrase \u201callows LLMs to mimic human thinking and behavior\u201d\n\n> I think the claim that tool use \"mimic[s] human thinking and behavior\" is overblown. Humans use think and work with tools very differently, typically not through a text-only interface.\n\nWe sincerely appreciate your feedback regarding the wording used here!\n\nOur intended message is that CRITIC is a framework designed, inspired by the process of human interaction with tools, such as humans compiling and running their own programs, receiving feedback, reflecting on issues, and going through the debugging process. We also agree that humans may exhibit a much richer inner monologue and behavior in using tools and reflecting, and language is indeed a limited interface through which LLM interacts with the world. In response to your feedback, we will revise this phrase to \"allows for human-like verify-then-correct trajectories\", and we hope this address your concerns.\n\nIn light of these clarifications and revisions, we kindly request that you consider increasing the review score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4343/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699891386631,
                "cdate": 1699891386631,
                "tmdate": 1699892960664,
                "mdate": 1699892960664,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ajp9CpW0wG",
                "forum": "Sx038qxjek",
                "replyto": "QHjVJbOXOx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer hutJ (References)"
                    },
                    "comment": {
                        "value": "### References\n\n[1] Shinn, Noah, et al. \"Reflexion: Language agents with verbal reinforcement learning.\" Thirty-seventh Conference on Neural Information Processing Systems. 2023.\n\n[2] Madaan, Aman, et al. \"Self-refine: Iterative refinement with self-feedback.\" Thirty-seventh Conference on Neural Information Processing Systems. 2023.\n\n[3] Chen, Xinyun, et al. \"Teaching large language models to self-debug.\" arXiv preprint arXiv:2304.05128 (2023).\n\n[4] Yao, Shunyu, et al. \"React: Synergizing reasoning and acting in language models.\" arXiv preprint arXiv:2210.03629 (2022).\n\n[5] Chen, Wenhu, et al. \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.\" arXiv preprint arXiv:2211.12588 (2022).\n\n[6] Saunders, William, et al. \"Self-critiquing models for assisting human evaluators.\" arXiv preprint arXiv:2206.05802 (2022).\n\n[7] Bai, Yuntao, et al. \"Constitutional ai: Harmlessness from ai feedback.\" arXiv preprint arXiv:2212.08073 (2022).\n\n[8] Campos, Jon Ander, and Jun Shern. \"Training language models with language feedback.\" ACL Workshop on Learning with Natural Language Supervision. 2022.. 2022.\n\n[9] Chen, Angelica, et al. \"Improving code generation by training with natural language feedback.\" arXiv preprint arXiv:2303.16749 (2023).\n\n[10] Haluptzok, Patrick, Matthew Bowers, and Adam Tauman Kalai. \"Language models can teach themselves to program better.\"\u00a0*arXiv preprint arXiv:2207.14502*\u00a0(2022).\n\n[11] Welleck, Sean, et al. \"Generating Sequences by Learning to Self-Correct.\"\u00a0*The Eleventh International Conference on Learning Representations*. 2023.\n\n[12] Kadavath, Saurav, et al. \"Language models (mostly) know what they know.\" arXiv preprint arXiv:2207.05221 (2022).\n\n[13] Ni, Ansong, et al. \"Lever: Learning to verify language-to-code generation with execution.\" International Conference on Machine Learning. PMLR, 2023.\n\n[14] Huang, Jie, et al. \"Large language models cannot self-correct reasoning yet.\" arXiv preprint arXiv:2310.01798 (2023).\n\n[15] Stechly, Kaya, Matthew Marquez, and Subbarao Kambhampati. \"GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems.\" arXiv preprint arXiv:2310.12397 (2023).\n\n[16] Valmeekam, Karthik, Matthew Marquez, and Subbarao Kambhampati. \"Can Large Language Models Really Improve by Self-critiquing Their Own Plans?.\" arXiv preprint arXiv:2310.08118 (2023).\n\n[17] Schick, Timo, et al. \"Toolformer: Language models can teach themselves to use tools.\" arXiv preprint arXiv:2302.04761 (2023).\n\n[18] Lu, Ximing, et al. \"Quark: Controllable text generation with reinforced unlearning.\" Advances in neural information processing systems 35 (2022): 27591-27609."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4343/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699891429143,
                "cdate": 1699891429143,
                "tmdate": 1699891429143,
                "mdate": 1699891429143,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2BT0EHifms",
                "forum": "Sx038qxjek",
                "replyto": "QHjVJbOXOx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4343/Reviewer_hutJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4343/Reviewer_hutJ"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response, I appreciate engaging with all the points I raised.\n\nHowever, I remain somewhat unconvinced that using external feedback counts as a novelty. The idea of using external feedback in combination with language feedback was used before (e.g. [1, 2]). Moreover, the need to rely on external tools is generally a limitation: it means that LLM capabilities cannot scale up unless new tools are available. It seems that at least some method can overcome it in certain settings. I can imagine a paper focusing on this limitation (similar to [3]), but that would require a change of the story in the paper.\n\n[1] https://arxiv.org/abs/2303.16749 \n\n[2] https://arxiv.org/abs/2303.11366\n\n[3] https://arxiv.org/abs/2310.01798"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4343/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700566139998,
                "cdate": 1700566139998,
                "tmdate": 1700566151167,
                "mdate": 1700566151167,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZbjVmWEbNs",
                "forum": "Sx038qxjek",
                "replyto": "YlKg5mVXFQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4343/Reviewer_hutJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4343/Reviewer_hutJ"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the response. After carefully reading our discussion as well as discussions with other reviewers I decided to keep my score."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4343/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650506518,
                "cdate": 1700650506518,
                "tmdate": 1700650506518,
                "mdate": 1700650506518,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ia0G4Dfd33",
            "forum": "Sx038qxjek",
            "replyto": "Sx038qxjek",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4343/Reviewer_rWYL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4343/Reviewer_rWYL"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework called CRITIC to progressively validate and revise the output based on the feedback from tools. Six different external tools are used including Knowledge base, code interpreter, Text APIs, Wiki, Calculator and Search Engine. Evaluations are done on free-form question answering, mathematical program synthesis, and toxicity reduction. CRITIC was shown to have superior performance on these benchmarks compared to strong baselines including CoT, Self-Consistency, ReAct, and PoT."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. LLM Tool Use is a very timely research topic, and it is an important research area to use external feedback for the self-improvement of LLMs. This paper covers a wider range of tools compared to many prior works which typically employ one single type of tools.\n2. The results are rather strong with universal improvements across most tasks evaluated with several different model families and sizes.\n3. The ablation against CRITIC w/o Tool shows the importance of external feedback from Tools, which is an important learning for the community.\n4. The paper is very well written and is easy to understand with comprehensive comparisons to strong baselines."
                },
                "weaknesses": {
                    "value": "Error analysis is missing on what are the failure modes after using Tools for feedback."
                },
                "questions": {
                    "value": "1. It is unclear how important each Tool is to each task. Such analysis will provide further insight into where the improvements come from.\n2. The authors used different sampling config for the experiments for different tasks: e.g. p=0.9 was used for section 4.3 which is different from p=0.5 in 4.1 and 4.2."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4343/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801621943,
            "cdate": 1698801621943,
            "tmdate": 1699636404639,
            "mdate": 1699636404639,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wjlqdRIXPa",
                "forum": "Sx038qxjek",
                "replyto": "ia0G4Dfd33",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer rWYL (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your thorough review! We appreciate your acknowledgment of CRITIC's importance, clarity, novelty, effectiveness, and consistent performance gains. We're grateful for your recommendation to accept our paper.\n\n## Adding Detailed Error Analysis on QA and Mathematical Program Synthesis Tasks\n\n> Error analysis is missing on what are the failure modes after using Tools for feedback.\n\nThank you for your constructive suggestion! Here, we point out that the appendix of the original paper actually includes extensive case studies on failure modes of all tasks.\n\nMoreover, in light of your suggestion, we have included new Error Analyses for the QA and Mathematical Program Synthesis tasks, as illustrated below.\n\n### **1. Success and Failure Case Studies**\n\nDue to the limitation of space in the main text, we provide a representative success and failure case analysis for each task in Appendix E in the original paper, where typical failure modes include evidence search failure, reasoning errors for QA tasks, and verification or correction errors in GSM8k.\n\n### **2. Error Analysis on QA Tasks**\n\nFollowing your suggestion, in order to further understand the failure modes after using tools for feedback, we randomly selected 100 cases from the HotpotQA task, and manually annotated and analyzed the error types for both the initial CoT and CRITIC. The results are as follows:\n\n| Error Type                               | Explanations                                                             | CoT | CRITIC | \n|------------------------------------------|--------------------------------------------------------------------------|-----|--------|\n| Hallucination                            | Hallucination, including misinterpreting evidence or inconsistencies | 36% | 7%     |\n| Reasoning Error                          | Incorrect logical reasoning                                              | 5%  | 10%    |\n| Irrelevant Response                      | Answering a question that was not asked                                  | 9%  | 7%     |\n| Insufficient Evidence (Refusal)          | Refusal to answer the question due to lack of sufficient evidence    | 2%  | 12%    |\n| Undefined Answer                         | Providing an empty answer or failing to derive an answer                 | 18% | 5%     |\n| Incorrect Correction                     | CRITIC wrongly altered the correct initial CoT answer                    | -   | 10%    |\n| Label Ambiguity (FN)                     | The prediction is correct but not matching the label                     | 20% | 37%    |\n| Incorrect Label (FN)                     | The dataset answer is incorrectly labeled                                | 9%  | 10%    |\n| Outdated Label (FN)                      | The dataset answer label is outdated                                     | 0%  | 2%     |\n\nAs depicted in the table:\n\n- (1) CRITIC can significantly reduce hallucinations (36% vs. 7%), but not all of them. Even after utilizing CRITIC, hallucinations persist due to the inability to find useful evidence via a search engine or misunderstanding the evidence. This is illustrated in Appendix E.\n- (2) Most errors after applying CRITIC arise from reasoning mistakes, refusal to answer, and incorrect corrections. The refusal to answer occurs when CRITIC can't find enough evidence to support a response, which we consider an expected behavior to maintain truthfulness.\n- (3) In reality, CRITIC has effectively helped us identify a large number of label ambiguities, label errors, and outdated issues in the HotpotQA dataset (49% in CRITIC error samples). These false negatives (FN) indicate a certain bias in evaluating different methods on free-form QA tasks using automatic metrics like EM / F1. This has motivated subsequent research to design a more reliable LLM-based evaluation for QA tasks [1]."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4343/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699890717380,
                "cdate": 1699890717380,
                "tmdate": 1700122304978,
                "mdate": 1700122304978,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SlxqVi7IgW",
                "forum": "Sx038qxjek",
                "replyto": "ia0G4Dfd33",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer rWYL (2/2)"
                    },
                    "comment": {
                        "value": "### **3. Error Analysis on Mathematical Program Synthesis tasks**\n\nOn Mathematical Program Synthesis tasks, to offer readers a more comprehensive understanding of the specific corrections made by CRITIC and the specific benefits derived from tool feedback (also mentioned by reviewer ASEM), we carried out a manual statistical analysis of the types of corrections made by CRITIC on the GSM8k full test set (1319 samples).\n\nSpecifically, we identified four different categories of initial program errors: syntax errors, runtime errors, unreasonable outputs (such as irrational negative values), and other intrinsic reasoning errors. We calculated the accuracy of the initial PoT (Init), and CRITIC for each type of error. The settings for corrections are consistent with the non-oracle setting in the original paper, with up to four rounds of correction. The statistics are presented in the following table:\n\n| Error Type          | Init (Count) | Init (Acc) | CRITIC (Count)| CRITIC (Acc)|   \n|---------------------|-------------|----------|-------------|------|  \n| Intrinsic Error     | 281 (77.4%) | 0.0      | 206 (71.8%) | 26.7 |  \n| Unreasonable Output | 61 (16.8%)  | 0.0      | 26 (9.1%)   | 57.4 |  \n| Syntax Error        | 17 (4.7%)   | 0.0      | 11 (3.8%)   | 35.3 |  \n| Runtime Error       | 4 (1.1%)    | 0.0      | 3  (1.0%)   | 25.0 |  \n| All Init Errors     | 363         | 0.0      | 246 (85.7%) | 32.2 |\n| Wrong Correction    | -           | 100.0    | 41 (14.3%)  | 95.7 |\n\nAs can be seen in the table:\n\n- (1) The majority of error types in the initial PoT responses are intrinsic reasoning errors (77.4%), such as misunderstanding the question or omitting conditions. The initial responses also exhibit a relatively high proportion (16.8%) of unreasonable output errors, while syntax and runtime errors are less frequent but not absent (5.8%).\n- (2) CRITIC has a high success rate in correcting unreasonable output and syntax errors (57.4% and 35.3% respectively). However, the correction rate for intrinsic errors, for which reliable feedback cannot be obtained, is relatively low (26.7%). Overall, CRITIC reduces errors in the initial erroneous samples by 32.2% in a non-oracle setting.\n- (3) Notably, while CRITIC has corrected a substantial number of errors in the initial PoT, as can be seen from the last row of the table above, there is a decrease of -4.3% in the accuracy of CRITIC on originally correct outputs. This results in the error modes after tool feedback also including 14.3% wrong corrections.\n\nWe will incorporate these results into the paper in subsequent updates.\n\n## Discussing The Role of Each Tool in Different Tasks\n\n> It is unclear how important each Tool is to each task. Such analysis will provide further insight into where the improvements come from.\n\nThank you for your suggestion! We will incorporate an analysis of the significance of different tools in various scenarios within the CRITIC paper.\n\nTo be specific, e.g., in knowledge-intensive tasks such as commonsense QA (AmbigNQ and TriviaQA) and multi-hop knowledge reasoning tasks like HotpotQA, the leading roles are played by web tools, and CRITIC primarily utilizes Wikipedia page browsing and Google snippet, as demonstrated by the numerous case studies in Appendix E.1; for mathematical program synthesis tasks, external knowledge is usually not required, and a code interpreter can serve the same function as a calculator. Therefore, in these experiments, our external feedback comes from error messages and execution results from the interpreter, as shown in the cases in Appendix E.2.\n\n \n## Clarifying Sampling Config Variations\n\n> The authors used different sampling config for the experiments for different tasks: e.g. p=0.9 was used for section 4.3 which is different from p=0.5 in 4.1 and 4.2.\n\nCRITIC have applied p=0.9 in the toxicity reduction task, diverging from the p=0.5 used in other tasks. This decision is based on the standard set in the RealToxicityPrompts benchmark [2], and is consistent with the methodology employed across all baseline studies [3], ensuring fair comparison.\n\n### References\n\n[1] Shao, Zhihong, et al. \"Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy.\" arXiv preprint arXiv:2305.15294 (EMNLP 2023).\n\n[2] Gehman, Samuel, et al. \"Realtoxicityprompts: Evaluating neural toxic degeneration in language models.\" arXiv preprint arXiv:2009.11462 (2020).\n\n[3] Welleck, Sean, et al. \"Generating sequences by learning to self-correct.\" arXiv preprint arXiv:2211.00053 (2022)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4343/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699890810856,
                "cdate": 1699890810856,
                "tmdate": 1699891985834,
                "mdate": 1699891985834,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qi8r6zJ9oZ",
                "forum": "Sx038qxjek",
                "replyto": "ia0G4Dfd33",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4343/Reviewer_rWYL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4343/Reviewer_rWYL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you authors for adding the detailed error analysis on QA tasks, and my concerns are fully addressed."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4343/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700436555976,
                "cdate": 1700436555976,
                "tmdate": 1700615652727,
                "mdate": 1700615652727,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yzGMvIU2WY",
            "forum": "Sx038qxjek",
            "replyto": "Sx038qxjek",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4343/Reviewer_dDfr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4343/Reviewer_dDfr"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a framework called CRITIC that enables large language models (LLMs) to self-verify and self-correct their outputs by interacting with external tools. The authors demonstrate the effectiveness of CRITIC in improving the performance of LLMs across multiple tasks, including free-form question answering, mathematical program synthesis, and toxicity reduction. The paper highlights the importance of external feedback in promoting the ongoing self-improvement of LLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The paper introduces a novel framework, CRITIC, which addresses the limitations of LLMs by allowing them to verify and correct their outputs through interaction with external tools.\n\n2) The authors provide comprehensive evaluations of CRITIC on different tasks and datasets, demonstrating its consistent performance improvement over baseline methods.\n\n3) The paper highlights the crucial role of external feedback in the self-improvement of LLMs and emphasizes the unreliability of LLMs in self-verification."
                },
                "weaknesses": {
                    "value": "1) I think this is a good paper. The motivation is strong: utilizing external feedback to enhance the model's ability. However, some recent studies [1] reported that large language models cannot self-correct themselves.  I acknowledge that [1] did not involve external tools, which is different from CRITIC's setting and it is a paper after CRITIC which is not necessarily be included, but it would be more comprehensive to include a discussion with these new studies in such a fast-moving field.\n\n2) How much of the additional costs? Since calling external tools costs money. The authors should report the cost for each experiment.\n\n3) In Appendix C.2, an important work active-prompt [2] should be included, which applies uncertainty estimation to chain-of-thought prompting.\n\n[1] Large Language Models Cannot Self-Correct Reasoning Yet\n[2] Active Prompting with Chain-of-Thought for Large Language Models"
                },
                "questions": {
                    "value": "How much of the additional costs? Since calling external tools costs money. The authors should report the cost for each experiment."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4343/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698851580839,
            "cdate": 1698851580839,
            "tmdate": 1699636404581,
            "mdate": 1699636404581,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dOLQofpRxw",
                "forum": "Sx038qxjek",
                "replyto": "yzGMvIU2WY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer dDfr"
                    },
                    "comment": {
                        "value": "Dear Reviewer dDfr,\n\nThank you for your comprehensive and detailed review of our paper! We sincerely appreciate your thorough and well-considered suggestions, which will help us make our work stronger! We're grateful for your recommendation to accept our paper!\n\n### **Including a Discussion with Follow-up Works**\n\n> I think this is a good paper. The motivation is strong: utilizing external feedback to enhance the model's ability. However, some recent studies [1] reported that large language models cannot self-correct themselves. I acknowledge that [1] did not involve external tools, which is different from CRITIC's setting and it is a paper after CRITIC which is not necessarily be included, but it would be more comprehensive to include a discussion with these new studies in such a fast-moving field.\n\nSure and thanks for your nice words! There are numerous recent studies, such as [1] that you mentioned. It is actually a follow-up to CRITIC and it kindly cites the CRITIC paper numerous times. These papers further provide additional experiments on various tasks like reasoning [1],  graph coloring[2], and planning [3], further validating the findings of unreliable self-correction of LLMs from CRITIC and providing further insights [1,2,3]. Thank you for your very thoughtful suggestion, and we will include discussions on these new studies in our subsequent updates.\n\n### **Discussing Tool-Use Cost for Each Experiment**\n\n> How much of the additional costs? Since calling external tools costs money. The authors should report the cost for each experiment.\n\nNice suggestion! We\u2019ll include a new paragraph discussing the cost of tool use in the Appendix, which is actually all free!\n\n- For QA tasks, as mentioned in the footnote in Sec. 4.1, we build a Google Web Crawler to crawl the results of Google Search and web pages, and employ a caching mechanism for web search, storing about 9GB of search results from January to April 2023 during our experiments. This part of the code is separately open-sourced anonymously at\u00a0https://anonymous.4open.science/r/llm-agent-web-tools. The results of the Search Engine in the paper are all obtained using this code. In addition, we will also open-source all caches after the anonymous review period ends, to ensure stability, fairness, and reproducibility in our results.\n- For Mathematical program synthesis tasks, we use a local code interpreter, which is free of charge.\n- For toxicity reduction tasks, we adopt PERSPECTIVE API [4] kindly provided by Google, which is also free.\n\n### **Adding Active-Prompt to Uncertainty Estimation**\n\n> In Appendix C.2, an important work active-prompt [2] should be included, which applies uncertainty estimation to chain-of-thought prompting.\n\nThank you for your suggestion! \nWe concur that Active-Prompt is a successful application of uncertainty estimation in the context of demonstration selection. In response to your request, we have referenced this paper in Appendix C.2 in our paper.\n\nIn light of these clarifications and revisions, we kindly request that you consider increasing the review score.\n\n### References\n\n[1] Huang, Jie, et al. \"Large language models cannot self-correct reasoning yet.\" arXiv preprint arXiv:2310.01798 (2023).\n\n[2] Stechly, Kaya, Matthew Marquez, and Subbarao Kambhampati. \"GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems.\" arXiv preprint arXiv:2310.12397 (2023).\n\n[3] Valmeekam, Karthik, Matthew Marquez, and Subbarao Kambhampati. \"Can Large Language Models Really Improve by Self-critiquing Their Own Plans?.\" arXiv preprint arXiv:2310.08118 (2023).\n\n[4] https://www.perspectiveapi.com/"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4343/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699890209209,
                "cdate": 1699890209209,
                "tmdate": 1699892400931,
                "mdate": 1699892400931,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SZaAqbweE7",
            "forum": "Sx038qxjek",
            "replyto": "Sx038qxjek",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4343/Reviewer_ASEM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4343/Reviewer_ASEM"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces CRITIC, a method for improving  the outputs of language models by leveraging external feedback from various tools. The idea is to generate an initial output with the language model and then refine this output using feedback from an external tool, such as a Python interpreter, search engines, or toxicity detection APIs. Notably, this approach relies solely on in-context learning without the need for specialized training. Results across various tasks, including question answering, mathematical reasoning, and toxicity reduction, show that CRITIC improves over baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The experiments demonstrate the effectiveness of the proposed approach across a diverse set of tasks, indicating its potential to significantly improve the performance of large language models (LLMs).\n\n- Utilizing external feedback as a means of improving LLM outputs is practical. The simplicity of the approach is a plus, as it facilitates widespread application."
                },
                "weaknesses": {
                    "value": "- The primary concern with this work is its novelty. Several studies have previously demonstrated that external feedback can be instrumental in correcting LLM outputs. In fact, there is existing work within each domain addressed in this paper, such as Self-Correct ([1], using external APIs), Self-Ask ([2], employing a search engine), and Self-Debug ([3], via a Python interpreter). Notably, Self-Debug and Self-Ask have a striking resemblance to CRITIC but are not referenced.\n\n\n- The settings that rely on an oracle are somewhat idealistic, and detract from the core message of the paper. It may be more appropriate to move these results to an appendix (as done by other works) to facilitate a clearer understanding.\n\n\n\n[1] Welleck, Sean, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. \"Generating Sequences by Learning to Self-Correct.\" In The Eleventh International Conference on Learning Representations. 2022.\n\n[2] Press, Ofir, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. \"Measuring and narrowing the compositionality gap in language models.\" arXiv preprint arXiv:2210.03350 (2022).\n\n\n[3] Chen, Xinyun, Maxwell Lin, Nathanael Sch\u00e4rli, and Denny Zhou. \"Teaching large language models to self-debug.\" arXiv preprint arXiv:2304.05128 (2023)."
                },
                "questions": {
                    "value": "- The emphasis in Table 1 seems inconsistent. For instance, the AmbigNQ EM score of 50.0 is highlighted for Text-Davinci-003, but it is not the highest. Is this a bug or am I missing something?\n\n- Regarding the GSM task in a non-oracle setting, it appears that feedback from the interpreter is limited to syntactic correctness. Given the improvements, it suggests that many of the programs were initially syntactically wrong. Is this the case?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4343/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699056582158,
            "cdate": 1699056582158,
            "tmdate": 1699636404517,
            "mdate": 1699636404517,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "egGnDImGnF",
                "forum": "Sx038qxjek",
                "replyto": "SZaAqbweE7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer ASEM (1/3)"
                    },
                    "comment": {
                        "value": "Dear Reviewer ASEM,\n\nThank you for your comprehensive and detailed review of our paper!\n\nWe sincerely appreciate your thorough and well-considered suggestions, which will help us make our work stronger.\n\n## Novelty and Contributions in Comparison to Previous and Contemporary Works\n\n>  Several studies have previously demonstrated that external feedback can be instrumental in correcting LLM outputs. In fact, there is existing work within each domain addressed in this paper, such as Self-Correct ([1], using external APIs), Self-Ask ([2], employing a search engine), and Self-Debug ([3], via a Python interpreter). Notably, Self-Debug and Self-Ask have a striking resemblance to CRITIC but are not referenced.\n\nThank you for raising these concerns! We agree that there are numerous related works to CRITIC, given the importance and rapid development of LLM self-correction and tool-use topics. However, we would like to ****address the misconceptions and misunderstandings**** about CRITIC and its contributions when compared to many related works. This is because ****CRITIC's stance and findings significantly differ from these works****:\n\n### **Intrinsic Self-Verification and Self-Correction**\n\nThese works include Self-Critique [6], CAI [7], Reflexion[1], Self-Refine [2], and others [8-10], they prompt or train language models to correct their results. In contrast, our study is the first to demonstrate that such a \"Self-Verification and Self-Correction\" approach has proven to be **remarkably unreliable across diverse tasks and various LLMs**. Specifically, modest or even deteriorated performance is observed universally using self-correct without external feedback in Sec. 4 and Appendix D.1. Consequently, CRITIC emphasizes the importance of feedback from external interactions for the consistent self-improvement of LLMs. The proposed framework is general and has proven effective across multiple tasks. These profound reflections on unreliable self-correction of LLMs and crucial findings on the importance of external feedback can be important learnings for the community, as recognized by Reviewer [rWYL].\n\n- **Self-Correct** [11] uses the PERSPECTIVE API to build toxicity reduction pairs for training a corrector. Despite this, **its focus isn't on tool feedback but on creating value-improving pairs to train an additional corrector.** It doesn't use external feedback in math reasoning or constraint generation, primarily employing an intrinsic self-correction mechanism. It is mainly designed for smaller models like GPT-2, which have limited in-context learning capabilities. In contrast, our approach, CRITIC, iteratively optimizes outputs based on various external tool feedback, rather than solely relying on the model's own judgment. Additionally, CRITIC is a plug-and-play approach applicable to black-box LLMs, which further distinguishes it. We also provide an analysis and comparison to Self-Correct in our original paper's related works section and Sec 4.3's experiments.\n- You also referenced **Self-Debug** [3], which notably, debuted concurrently with the first version of CRITIC. Self-Debug emphasizes code tasks, proposing self-explanation **\u201cwithout any feedback on code correctness or error messages\u201d (intrinsic feedback)**, and applying unit tests similar to LEVER [13]. In contrast, results from CRITIC show that self-correct without external feedback is unreliable, and CRITIC addresses general challenges of truthfulness, trustworthiness, and toxicity, using various tools like Google and external APIs. In light of your valuable feedback, we'll include a comparison with Self-Debug in our updated paper.\n\n### **Tool-Use: Tool Augmented LMs**\n\nAnother category of very related work is Tool Augmented LMs, such as ReAct [4], PoT [5], and Toolformer [17]. We would like to clarify that **these works significantly differ from CRITIC as they focus on tool learning**. To the best of our knowledge, none of them consider using tool-interaction as faithful feedback to iteratively improve the LLMs, which is the key component of CRITIC.\n\n- You also mentioned a \"striking resemblance\" to the **Self-Ask** paper, which to our understanding, also does not involve self-correction with external feedback. In fact, Self-Ask \u201cuses a search engine to answer sub-questions\u201d and does not focus on verifying and correcting answers. However, we believe this work falls within the broader scope of RAG and will be referenced in the related works section."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4343/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699889605795,
                "cdate": 1699889605795,
                "tmdate": 1699974750145,
                "mdate": 1699974750145,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UpYx3gzEUN",
                "forum": "Sx038qxjek",
                "replyto": "SZaAqbweE7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer ASEM (2/3)"
                    },
                    "comment": {
                        "value": "### **Dive into The Unreliability of Self-correction**\n\nCRITIC further delves into the core reason behind the unreliability of self-verification and self-correction from the perspective of uncertainty estimation, as shown in Appendix D.1,. Essentially, language models are ****incapable of accurately identifying \"what they know\"**** (i.e., LLMs don't know what they know) [12] without relying on external tools. Therefore, without the aid of oracle verification (employed in many contemporary works such as Reflexion [1] and Self-Refine [2]), self-correction might surprisingly deteriorate performance for many tasks, and even incorrectly modify many initial answers (as demonstrated in Table 2, 3 under CRITIC w/o Tool, and in Table 8 under Self-Refine).\n\n- As noted by Reviewer [dDfr], a nice recent follow-up work to CRITIC, titled \"LLMs cannot self-correct reasoning yet\" [14] extends the study of CRITIC (which cites CRITIC numerous times). It further validates and expands our findings on the unreliability of Self-Verify and Self-Correct in CRITIC using GPT-4 in more settings on many reasoning tasks.\n- Another latest research [15, 16] experiments on graph coloring [15] and planing [16], demonstrating that \"LLMs are in fact very poor at verifying solutions\" and that \"iterative prompting can help when there is an external provably correct verifier in the loop\". These follow-ups further illustrates the significance and foresight of CRITIC's findings on the unreliability of Self-Verification and Self-Correction and the value of introducing external tool verification.\n\nIn conclusion, CRITIC is the first to unveil the unreliability of self-verification and self-correction across diverse tasks and LLMs of various families and sizes. By initially highlighting the challenges LLMs encounter in self-verification [12], self-correction [6,7], our goal is to rectify any potential overestimations of these LLM's abilities within the research community [1-3]. By emphasizing that feedback from external tool interaction is crucial for consistent self-improvement of LLMs, we hope our findings provide valuable insights and encourage further exploration and enhancement of self-improving LLMs.\n\nMoreover, we have discussed these related works and offered a thorough comparison to the latest research in Appendix C. We will also expand the discussion to include recent follow-up works [18], such as the \u201cLLMs cannot self-correct reasoning\u201d paper [14] as suggested by reviewer [dDfr].\n\n## Adjusting the Placement of Oracle Settings for Enhanced Clarity\n\n> The settings that rely on an oracle are somewhat idealistic, and detract from the core message of the paper. It may be more appropriate to move these results to an appendix (as done by other works) to facilitate a clearer understanding.\n\nIn fact, the oracle setting in CRITIC was established and included in the main text with reference to previous work (see Table 1 in [11]). It's worth mentioning that contemporaneous works to CRITIC such as Reflexion [1], Self-Refine [2], and RCI [19] *all* used the Oracle setting (i.e., trial-and-error) for their main results. Since you suggest that it may be more appropriate to move these results, we will adjust the placement of the Oracle setting in subsequent updates, hoping this will make things clearer for you.\n\n## Addressing Table 1 Emphasis Typos (Q1)\n\n> The emphasis in Table 1 seems inconsistent. For instance, the AmbigNQ EM score of 50.0 is highlighted for Text-Davinci-003, but it is not the highest. Is this a bug or am I missing something?\n\nThank you for pointing out this nuance in Table 1. We acknowledge the discrepancy in highlighting the AmbigNQ scores, despite it not being the highest. This was indeed an oversight and not a bug. We have corrected this in an updated version. Additionally, we have meticulously reviewed all the tables to ensure no further inconsistencies are present. We appreciate your rigorous attitude and constructive feedback."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4343/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699889638222,
                "cdate": 1699889638222,
                "tmdate": 1699979952506,
                "mdate": 1699979952506,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2jxBvaR32I",
                "forum": "Sx038qxjek",
                "replyto": "SZaAqbweE7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer ASEM (3/3)"
                    },
                    "comment": {
                        "value": "## Adding Additional Error Analysis on the Role of Interpreter Feedback (Q2)\n\n> Regarding the GSM task in a non-oracle setting, it appears that feedback from the interpreter is limited to syntactic correctness. Given the improvements, it suggests that many of the programs were initially syntactically wrong. Is this the case?\n\nThanks for raising this issue!\nThe feedback from the interpreter is not solely about syntactic correctness. As mentioned in Sec. 4.2 and demonstrated in the cases in Appendix E.2, CRITIC leverages both \"error messages and execution results\", enabling it to fix syntax errors, correct runtime errors (e.g., timeout error in Listing 9 in Appendix E.2) and self-reflect on unreasonable reasoning steps and outputs (e.g., Listing 7 in Appendix E.2). Notably, in non-oracle settings, we do not only apply correction on answers where the interpreter fails, but by default, we allow LLMs to verify and then correct if necessary and \"stop if the executed result remains unchanged for two consecutive revisions\", as detailed in Sec. 4.2.\n\nMoreover, to offer readers a more comprehensive understanding of the specific corrections made by CRITIC and the specific benefits derived from tool feedback, we carried out a manual statistical analysis of the types of corrections made by CRITIC on the GSM8k full test set (1319 samples).\n\nSpecifically, we identified four different categories of initial program errors: syntax errors, runtime errors, unreasonable outputs (such as irrational negative values), and other intrinsic reasoning errors. We calculated the accuracy of the initial PoT (Init), and CRITIC for each type of error. The settings for corrections are consistent with the non-oracle setting in the original paper, with up to four rounds of correction. The statistics are presented in the following table:\n\n| Error Type          | Init (Count) | Init (Acc) | CRITIC (Count)| CRITIC (Acc)|   \n|---------------------|-------------|----------|-------------|----------|  \n| Intrinsic Error     | 281 (77.4%) | 0.0      | 206 (83.7%) | 26.7 |  \n| Unreasonable Output | 61 (16.8%)  | 0.0      | 26 (10.6%)  | 57.4 |  \n| Syntax Error        | 17 (4.7%)   | 0.0      | 11 (4.5%)   | 35.3 |  \n| Runtime Error       | 4 (1.1%)    | 0.0      | 3  (1.2%)   | 25.0 |  \n| All Init Errors     | 363         | 0.0      | 246         | 32.2 |\n\nAs shown in the above table:\n\n- (1) The majority of error types in the initial PoT responses are intrinsic reasoning errors (77.4%), such as misunderstanding the question or omitting conditions. The initial responses also exhibit a relatively high proportion (16.8%) of unreasonable output errors, while syntax and runtime errors are less frequent but not absent (5.8%).\n- (2) CRITIC has a high success rate in correcting unreasonable output and syntax errors (57.4% and 35.3% respectively). However, the correction rate for intrinsic errors, for which reliable feedback cannot be obtained, is relatively low (26.7%). Overall, CRITIC reduces errors in the initial erroneous samples by 32.2% in a non-oracle setting.\n\nWe hope our responses adequately addresses all your concerns! We greatly appreciate your time and effort in reviewing our paper, and your constructive feedback!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4343/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699889946663,
                "cdate": 1699889946663,
                "tmdate": 1699892299390,
                "mdate": 1699892299390,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T6JWtZ2CFp",
                "forum": "Sx038qxjek",
                "replyto": "SZaAqbweE7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Official Review by Reviewer ASEM (References)"
                    },
                    "comment": {
                        "value": "### References\n\n[1] Shinn, Noah, et al. \"Reflexion: Language agents with verbal reinforcement learning.\" Thirty-seventh Conference on Neural Information Processing Systems. 2023.\n\n[2] Madaan, Aman, et al. \"Self-refine: Iterative refinement with self-feedback.\" Thirty-seventh Conference on Neural Information Processing Systems. 2023.\n\n[3] Chen, Xinyun, et al. \"Teaching large language models to self-debug.\" arXiv preprint arXiv:2304.05128 (2023).\n\n[4] Yao, Shunyu, et al. \"React: Synergizing reasoning and acting in language models.\" arXiv preprint arXiv:2210.03629 (2022).\n\n[5] Chen, Wenhu, et al. \"Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.\" arXiv preprint arXiv:2211.12588 (2022).\n\n[6] Saunders, William, et al. \"Self-critiquing models for assisting human evaluators.\" arXiv preprint arXiv:2206.05802 (2022).\n\n[7] Bai, Yuntao, et al. \"Constitutional ai: Harmlessness from ai feedback.\" arXiv preprint arXiv:2212.08073 (2022).\n\n[8] Campos, Jon Ander, and Jun Shern. \"Training language models with language feedback.\" ACL Workshop on Learning with Natural Language Supervision. 2022.. 2022.\n\n[9] Chen, Angelica, et al. \"Improving code generation by training with natural language feedback.\" arXiv preprint arXiv:2303.16749 (2023).\n\n[10] Haluptzok, Patrick, Matthew Bowers, and Adam Tauman Kalai. \"Language models can teach themselves to program better.\"\u00a0*arXiv preprint arXiv:2207.14502*\u00a0(2022).\n\n[11] Welleck, Sean, et al. \"Generating Sequences by Learning to Self-Correct.\"\u00a0*The Eleventh International Conference on Learning Representations*. 2023.\n\n[12] Kadavath, Saurav, et al. \"Language models (mostly) know what they know.\" arXiv preprint arXiv:2207.05221 (2022).\n\n[13] Ni, Ansong, et al. \"Lever: Learning to verify language-to-code generation with execution.\" International Conference on Machine Learning. PMLR, 2023.\n\n[14] Huang, Jie, et al. \"Large language models cannot self-correct reasoning yet.\" arXiv preprint arXiv:2310.01798 (2023).\n\n[15] Stechly, Kaya, Matthew Marquez, and Subbarao Kambhampati. \"GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems.\" arXiv preprint arXiv:2310.12397 (2023).\n\n[16] Valmeekam, Karthik, Matthew Marquez, and Subbarao Kambhampati. \"Can Large Language Models Really Improve by Self-critiquing Their Own Plans?.\" arXiv preprint arXiv:2310.08118 (2023).\n\n[17] Schick, Timo, et al. \"Toolformer: Language models can teach themselves to use tools.\" arXiv preprint arXiv:2302.04761 (2023).\n\n[18] Pan, Liangming, et al. \"Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies.\"\u00a0*arXiv preprint arXiv:2308.03188*\u00a0(2023).\n\n[19] Kim, Geunwoo,, et al. \"Language models can solve computer tasks.\" Thirty-seventh Conference on Neural Information Processing Systems. 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4343/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699890035730,
                "cdate": 1699890035730,
                "tmdate": 1699890035730,
                "mdate": 1699890035730,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1ksGurTZ8e",
                "forum": "Sx038qxjek",
                "replyto": "SZaAqbweE7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer ASEM, We hope that our revised paper and the above responses address your concerns. If there are any unresolved issues, please let us know. We are more than happy to answer any further questions you may have."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4343/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590231689,
                "cdate": 1700590231689,
                "tmdate": 1700590231689,
                "mdate": 1700590231689,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rxmIQT2ZrT",
                "forum": "Sx038qxjek",
                "replyto": "SZaAqbweE7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4343/Reviewer_ASEM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4343/Reviewer_ASEM"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response! Some clarifications"
                    },
                    "comment": {
                        "value": "Thanks for taking the time to respond! Let me reiterate my concerns and also rephrase my understanding of your responses.\n\n---\n\n### Weakness 1: Novelty\n\n> The primary concern with this work is its novelty. Several studies have previously demonstrated that external feedback can be instrumental in correcting LLM outputs. In fact, there is existing work within each domain addressed in this paper, such as Self-Correct ([1], using external APIs), Self-Ask ([2], employing a search engine), and Self-Debug ([3], via a Python interpreter). Notably, Self-Debug and Self-Ask have a striking resemblance to CRITIC but are not referenced.\n\n\n\n1. Self-Correct\n\n> \"its focus isn't on tool feedback but on creating value-improving pairs to train an additional corrector.\"\n\nThis doesn't seem to be correct?\n\nFrom [self-correct](https://arxiv.org/pdf/2211.00053.pdf): *As the value function, we use the Perspective API score, v(y) \u2208 [0, 1], which measures the toxicity of the completed sequence* They use the word \"value function,\" but essentially use the score from the API to decide if the improvements have any benefit.\n\nFurther:\n\n_We use additional fine-grained information from the toxicity API as natural language\nfeedback. Specifically, besides the overall toxicity score, Perspective API also provides scores for\nfine-grained attributes of toxicity (e.g., identity attack, profanity, flirtation, etc.). At training time,\nwe compare the attribute scores from a hypothesis and its selected correction, and use the attribute\nwith the largest decrease as natural language feedback (e.g. \"decrease toxicity in profanity\"). At\ninference time, we call the API on the current hypothesis, and use the attribute with the highest\nscore. Here **we use the API at inference time**, which is not required in our previous experiments._\n\n\n\n2. Self-Debug:\n\n\n> Self-Debug emphasizes code tasks, proposing self-explanation \"without any feedback on code correctness or error messages\" (intrinsic feedback)\n\nThis also doesn't seem to be precise. From [Self-Debug v1](https://arxiv.org/pdf/2304.05128v1.pdf): Figure 1: *The **code explanation** along with the execution results constitute the feedback message, which is then sent back to the model to perform more debugging steps...*. Please also see Section 3 and the examples in the Appendix.\n\nSo, I wonder if the novelty claim holds.\n\n*Regarding concurrency with Self-Debug:* Self-Debug came out on April 11 2023, so as per ICLR reviewer guidelines it is not concurrent work (only work after posted after **May 28 2023** is concurrent). Please see the [guidelines](https://iclr.cc/Conferences/2024/ReviewerGuide) here. I am not aware when CRITIC came out, and I do not want to look it up either. I'll let the AC decide on this.\n\n---\n\n###  Weakness 2: Including Oracle results in the main paper\n\n> The settings that rely on an oracle are somewhat idealistic,\n\nThanks for moving the Oracle results to the Appendix. \n\n> It's worth mentioning that contemporaneous works to CRITIC such as Reflexion [1], Self-Refine [2], and RCI [19] all used the Oracle setting (i.e., trial-and-error) for their main results.\n\nThis is incorrect, at least for some of the works. For example, [2] ([link](https://arxiv.org/pdf/2303.17651.pdf)) place the results from the Oracle setting in the Appendix. On this note, \n\n>  CRITIC is the first to unveil the unreliability of self-verification and self-correction across diverse tasks and LLMs of various families and sizes.\n\nThis statement may be toned down because [2] in the version posted on May 25th includes a discussion on this on Page 5 and also presents initial conjectures for why models may fail to reason (lack of good quality feedback)\n\n\nNevertheless, this weakness is addressed by moving the results to the Appendix.\n\n---\n\n### Question: Nature of feedback from the interpreter\n\nThanks for the analysis!\n\n1. How is \"Unreasonable Output\" determined? Is it a hard-coded rule that the output cannot be negative or an irrational number?\n\n2. From the analysis: *The majority of error types in the initial PoT responses are intrinsic reasoning errors (77.4%), such as misunderstanding the question or omitting conditions.* To clarify, there is no way to catch these errors with tools, right?"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4343/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597840844,
                "cdate": 1700597840844,
                "tmdate": 1700597985437,
                "mdate": 1700597985437,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ArhnFquimb",
                "forum": "Sx038qxjek",
                "replyto": "SZaAqbweE7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your reply!"
                    },
                    "comment": {
                        "value": "Thank you very much for your quick reply and for letting us know your remaining concerns!\n\n### Further Discussion on Novelty\n\n> **1. Self-Correct**\n> \n> > \"its focus isn't on tool feedback but on creating value-improving pairs to train an additional corrector.\"\n> \n> This doesn't seem to be correct?\n> \n> From\u00a0self-correct:\u00a0*As the value function, we use the Perspective API score, v(y) \u2208 [0, 1], which measures the toxicity of the completed sequence*\u00a0They use the word \"value function,\" but essentially use the score from the API to decide if the improvements have any benefit.\n> \n\nWe agree that Self-Correct uses the Perspective API in one of its tasks, but we maintain our assertion because it does not use external feedback in math reasoning or constraint generation, primarily employing an intrinsic self-correction mechanism. The focus of this paper is not on feedback based on tool-interaction, but on \"creating value-improving pairs to train an additional corrector.\u201d\n\nUnlike Self-Correct, we have validated the effectiveness of self-correction with external tool-interaction on diverse tasks and LLMs through extensive experiments. Moreover, we have demonstrated that self-correction based on external feedback does not require retraining an additional corrector and can be taught with few-shot in-context learning, making it applicable to black-box LLMs.  Furthermore, we provide an analysis and comparison to Self-Correct in our original paper's related works section and Section 4.3's experiments.\n\n> **2. Self-Debug:**\n> > Self-Debug emphasizes code tasks, proposing self-explanation \"without any feedback on code correctness or error messages\" (intrinsic feedback)\n>\n> This also doesn't seem to be precise. From\u00a0Self-Debug v1: Figure 1:\u00a0*The\u00a0**code explanation**\u00a0along with the execution results constitute the feedback message, which is then sent back to the model to perform more debugging steps...*. Please also see Section 3 and the examples in the Appendix.\n> \n> So, I wonder if the novelty claim holds.\n> \n\nWe clarify that this statement is from the abstract of [Self-Debug v1](https://arxiv.org/pdf/2304.05128v1.pdf) [2], which you also referenced. Thanks for you comment, we find that, despite Self-Debug focusing on code tasks and CRITIC concentrating on math reasoning program synthesis, we find no substantial difference in their core method of using program execution results as feedback.\n\nHowever, it's crucial to note that CRITIC was independently developed from January to April 2023, separate from Self-Debug. In fact, **Self-Debug and CRITIC were published within approximately a month** of each other, and Self-Debug is also [under review at ICLR\u201924](https://openreview.net/forum?id=KuPixIqPiq). While Self-Debug focuses on code tasks, CRITIC tackles broader challenges of truthfulness, trustworthiness, and toxicity, utilizing various tools such as Google and external APIs. Through extensive experimentation, we've demonstrated significant performance improvements offered by CRITIC across different base LLMs in tasks like QA, Math reasoning, and toxicity reduction. In light of your feedback, we will include a citation to this paper in our revision."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4343/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645672361,
                "cdate": 1700645672361,
                "tmdate": 1700646255861,
                "mdate": 1700646255861,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aNikJHPEGv",
                "forum": "Sx038qxjek",
                "replyto": "SZaAqbweE7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4343/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer ASEM,\n\nThank you once again for your review of our paper! We hope that our responses have adequately addressed all your concerns. As the deadline is approaching, we encourage you to reach out if you have any additional questions before the reviewer-author discussion period ends. We are more than happy to address any further questions or concerns you may have!\n\nBest Regards,\n\nThe Authors"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4343/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665216060,
                "cdate": 1700665216060,
                "tmdate": 1700665216060,
                "mdate": 1700665216060,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "flUpJ10sVH",
                "forum": "Sx038qxjek",
                "replyto": "SZaAqbweE7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4343/Reviewer_ASEM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4343/Reviewer_ASEM"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "> We agree that Self-Correct uses the Perspective API in one of its tasks, but we maintain our assertion because it does not use external feedback in math reasoning or constraint generation, primarily employing an intrinsic self-correction mechanism. \n\nEssentially, Self-Correct did tool-driven correction for one task, and CRITIC does this for multiple tasks. I agree with this, but I don't know if this is grounds for novelty.\n\n\n> Thanks for you comment, we find that, despite Self-Debug focusing on code tasks and CRITIC concentrating on math reasoning program synthesis, we find no substantial difference in their core method of using program execution results as feedback.\n\nIf I am reading this right, Self-Debug and CRITIC have the same method but focus on different tasks. I also agree with this statement.\n\nThis discussion reinforces my concerns regarding novelty: the ideas in CRITIC are already present in Self-Correct and Self-Debug. However, I agree that the idea is extended to a broader range of tasks in CRITIC. I will consult with other reviewers during the remainder of the discussion period and try to understand why they think the idea is novel.\n\n\nFinally, regarding the dates and arxiv versions: my job is to judge the version submitted to ICLR, and my feedback is solely based on the submitted/revised version. I will let AC/SAC judge the arxiv version of CRITIC when they make a decision.\n\nThanks"
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4343/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679155236,
                "cdate": 1700679155236,
                "tmdate": 1700679189775,
                "mdate": 1700679189775,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]