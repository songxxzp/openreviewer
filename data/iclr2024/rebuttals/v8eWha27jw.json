[
    {
        "title": "Accelerating Federated Learning with Quick Distributed Mean Estimation"
    },
    {
        "review": {
            "id": "j6xhTFUzbZ",
            "forum": "v8eWha27jw",
            "replyto": "v8eWha27jw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5142/Reviewer_fdHV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5142/Reviewer_fdHV"
            ],
            "content": {
                "summary": {
                    "value": "This works addresses the problem of communication-efficient distributed mean estimation (DME), a common subroutine in distributed optimization, and improves the SOTA of quantization techniques. It proposes QUIC-FL, the first quantization scheme that is efficient in both encoding and decoding and achieves the optimal NMSE, the metric of the estimation error, of $O(1/n)$ at the same time, where $n$ is the number of clients. QUIC-FL is based on two key ideas: 1) Bounded support quantization (BSQ), which sends a few large coordinates exactly and quantizes only the rest few. 2) An optimization framework that optimizes the set of quantization values to further reduce the estimation error, based on the limiting distribution of transformed coordinates of the client vectors, i.e., the normal distribution. Furthermore, QUIC-FL discusses the usage of client-specific shared randomness and the RHT rotation to practically gain constant improvement in the estimation error. Finally, extensive experiments show advantages of QUIC-FL compared to several SOTA quantization schemes in terms of encoding time, decoding time and NMSE."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "In terms of originality, this paper combines several existing approaches and improve the SOTA of quantization schemes.\n\nIn terms of quality, the paper analyzes and empirically shows an improved performance of the proposed QUIC-FL in terms of computational efficiency and the estimation error, against several SOTA baselines.\n\nIn terms of clarity, the paper conveys the key ideas in the design of QUIC-FL.\n\nIn terms of significance, the problem of communication-efficient DME the paper addresses is important. Improvement in quantization schemes is always welcomed."
                },
                "weaknesses": {
                    "value": "The presentation of this draft needs to be greatly improved.\n\n-\tThe abstract is not informative at all. The reader has no idea about the techniques this work uses to improve DME and the novelty of the techniques after reading it. It should at least mention, for example, one idea QUIC-FL builds on is BSQ which sends exactly a few large coordinates and quantizes the rest small ones.\n\n-\tIn Introduction, the paragraph starting with \u201cFor example, in Suresh et al. 2017 \u2026\u201d, it is mentioned the entropy encoding is \u201ccompute-intensive\u201d. How long does the decoding time of this approach take? Table 1 does not include entropy encoding as a baseline.\n\n-\tIt is mentioned in Introduction that the decoding procedure of \u201centropy encoding\u201d from a previous work is \u201ccompute-intensive\u201d. What is the time complexity of this decoding procedure? It seems this approach is not included in Table 1 as a baseline for comparison.\n\n-\tThe Lloyd-Max quantizer appears at several places in the work, e.g., in Introduction, and in serving as the Lower Bound in Section 3.5. However, this work does not introduce this quantizer properly. Can the authors give a brief introduction of this quantizer and in which cases is it optimal?\n\n-\tThe paragraph \u201cwhile the above methods suggest \u2026\u201d is abrupt and confusing. It\u2019d be better to move this paragraph before surveying existing quantization techniques in Introduction.\n\n-\tIn Section 2 preliminaries, it would be clearer if \u201cunbiased algorithms and independent estimators\u201d can be formally and clearly defined. Minor issue: \u201cwe that NMSE \u2026\u201d => \u201cwe want that NMSE \u2026\u201d\n\n-\tThe uniform random rotation appears at several places in the work. What exactly is this rotation? Is it a uniform Gaussian random rotation?\n\n-\tIn Section 3.5 \u201caccelerating QUIC-FL with RHT\u201d, \u201cadversarial vectors\u201d are mentioned but not introduced. It is confusing how the proposed approach compares against DRIVE and EDEN in terms of the application to \u201cadversarial vectors\u201d.\n\n-\tIn Section 3.5 \u201caccelerating QUIC-FL with RHT\u201d, it states \u201cthe result does not have the additive NMSE term is [because] we directly analyze the error for the Hadamard-rotated coordinates\u201d. Can the author be more formal and specific how the analysis here is different from the one in Theorem 3.1?\n\n-\tSeveral places in the work states in plain text the performance of QUIC-FL with different values of its hyperparameters. For example, in Section 3.5 \u201caccelerating QUIC-FL with RHT\u201d, it states the NMSE of QUIC-FL is bounded by \u2026 with $b = 1,2,3,4$. It\u2019d be easier for the readers if those numbers can be turned into figures. \n\nThis work claims the proposed QUIC-FL has NMSE $O(1/n)$.  However, by Theorem 3.1, the actual NMSE is indeed $O(1/n \\cdot \\sqrt{\\log d / d})$. It does not seem to be OK to omit the $O(\\sqrt{\\log d / d})$ term and directly write $O(1/n \\cdot \\sqrt{\\log d / d}) = O(1/n)$."
                },
                "questions": {
                    "value": "$\\textbf{Optimizing the quantization values. }$\nIn Section 3.3 \u201cdistribution-aware unbiased quantization\u201d, this work proposes two optimization problems to find the optimal quantization values to reduce NMSE. In the first optimization problem on page 4, the notations $S(z, x)$ and $R(x)$ are a bit confusing. Are $S$ and $R$ two functions to be optimized? Is $R(x)$ essentially a vector of $2^b$ variables? Is $S(z, x)$ a continuous function?\n\nSimilarly, in the second optimization problem (i.e., the discretized version of the first problem), is $S\u2019(i, x)$ essentially a vector of $m \\cdot 2^b$ variables to be optimized?\n\nSince the number of variables to be optimized is on the order of $2^b$, how efficient is the second optimization problem?\n\n$\\textbf{Communication cost. }$\nOne concern is that the proposed QUIC-FL requires extra bits to send a few large coordinates exactly along with their indices, while the baseline quantization schemes usually allocate a fixed number of bits per coordinate. This makes QUIC-FL use more communication cost compared to the baseline. And hence it might not be fair to directly compare QUIC-FL\u2019s NMSE against that of the baselines. How does the author compare in Table 1? Also, how does the author address this in the experiments?\n\n$\\textbf{Optimality. }$\nIt is mentioned at several places that the optimal NMSE of any quantization is $O(1/n)$ (this lower bound is in terms of the number of clients only, I presume). This is not rigorous in the draft. Can the authors cite the theorems that indicates the optimality? \n\nThe draft claims QUIC-FL achieves a \u201cnear-optimal\u201d NMSE. However, it seems this lower bound is only empirically obtained using the Lloyd-Max quantizer in Section 3.5. \u201cNear-optimal\u201d specifically means one can theoretically show the algorithm achieving optimality (e.g., close to a lower bound), up to a logarithmic factor. And so it might not be appropriate to claim the \u201cnear-optimality\u201d of QUIC-FL."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5142/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5142/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5142/Reviewer_fdHV"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5142/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697945919231,
            "cdate": 1697945919231,
            "tmdate": 1699636508171,
            "mdate": 1699636508171,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GVUEv220YX",
                "forum": "v8eWha27jw",
                "replyto": "j6xhTFUzbZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5142/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5142/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author's response"
                    },
                    "comment": {
                        "value": "Thank you for the review.\n\n---\n\n*Weaknesses*\n\n---\n\n**The abstract is not informative at all. The reader has no idea about the techniques this work uses to improve DME and the novelty of the techniques after reading it. It should at least mention, for example, one idea QUIC-FL builds on is BSQ which sends exactly a few large coordinates and quantizes the rest small ones.**\n\nWe will revisit the abstract to make it more informative.\n\n**In Introduction, the paragraph starting with \u201cFor example, in Suresh et al. 2017 \u2026\u201d, it is mentioned the entropy encoding is \u201ccompute-intensive\u201d. How long does the decoding time of this approach take? Table 1 does not include entropy encoding as a baseline.**\n\n**It is mentioned in Introduction that the decoding procedure of \u201centropy encoding\u201d from a previous work is \u201ccompute-intensive\u201d. What is the time complexity of this decoding procedure? It seems this approach is not included in Table 1 as a baseline for comparison.**\n\nIn theory, entropy encoding requires $O(d)$ time. However, this operation is not GPU-friendly and is thus much slower in practice.\nIn the paper \u201cDoCoFL: Downlink Compression for Cross-Device Federated Learning\u201d (ICML 2023), the authors performed timing measurements of entropy encoding schemes and found it to be ~ two orders of magnitude slower compared to EDEN, which runs on a GPU (QUIC-FL also runs on a GPU and is faster than EDEN as we show in the paper).\n\nThe Lloyd-Max quantizer is a biased quantizer that yields good quantization levels for a given distribution. In some cases, the outcome may be suboptimal (see [A], Example 3.2.1 ). However, the quantizer is known to be optimal for the log-concave distributions (see, e.g., [B]), including the Guassian, which is the focus of our work.\n\n[A] MIT lecture notes https://ocw.mit.edu/courses/6-450-principles-of-digital-communications-i-fall-2006/926689aaa62a0315473fa9b982de1b07_book_3.pdf\n\n[B] Huang, Bormin, and Jing Ma. \"On asymptotic solutions of the Lloyd-Max scalar quantization.\" 2007 6th International Conference on Information, Communications & Signal Processing. IEEE, 2007.\nhttps://ieeexplore.ieee.org/document/4449824.\n\n**The paragraph \u201cwhile the above methods suggest \u2026\u201d is abrupt and confusing. It\u2019d be better to move this paragraph before surveying existing quantization techniques in Introduction.**\n\n**In Section 2 preliminaries, it would be clearer if \u201cunbiased algorithms and independent estimators\u201d can be formally and clearly defined. Minor issue: \u201cwe that NMSE \u2026\u201d => \u201cwe want that NMSE \u2026\u201d**\n\nWe will revise these accordingly.\n\n**The uniform random rotation appears at several places in the work. What exactly is this rotation? Is it a uniform Gaussian random rotation?**\n\nRandom Matrix Theory (RMT) was introduced by Wishart [C] in 1928, and includes rotating the vector by multiplying it with a \u201crotation matrix\u201d, which is a random orthogonal matrix (see e.g., [D] for more details and methods for its generation. We will discuss this in the paper.\n\n[C] Wishart J. The generalised product moment distribution in samples from a normal multivariate population. Biometrika 20A, 32\u201352 (1928).\n[D] Martin A Tanner and Ronald A Thisted. Generation of Random Orthogonal Matrices. Journal of the Royal Statistical Society. Series C (Applied Statistics), 31(2):190\u2013192, 1982.\n\n**In Section 3.5 \u201caccelerating QUIC-FL with RHT\u201d, \u201cadversarial vectors\u201d are mentioned but not introduced. It is confusing how the proposed approach compares against DRIVE and EDEN in terms of the application to \u201cadversarial vectors\u201d.**\n\nWe meant that there are inputs for which DRIVE and EDEN fail to give an $O(1/n)$ NMSE when using RHT, as we explain in Appendix A. This is not a distinct application but rather an acknowledgment that DRIVE and EDEN are not suitable for worst-case situations.\n\n**In Section 3.5 \u201caccelerating QUIC-FL with RHT\u201d, it states \u201cthe result does not have the additive NMSE term is [because] we directly analyze the error for the Hadamard-rotated coordinates\u201d. Can the author be more formal and specific how the analysis here is different from the one in Theorem 3.1?**\n\nIn Theorem 3.1., we analyze the error that arises from approximating the distribution after a uniform random rotation and show that it is manifested in the $O(\\sqrt{\\log d / d})$ additive error term. The analysis in Section 3.5 directly analyzes the distribution that results from applying RHT, and thus no additional error term is needed.\n\n**Several places in the work states in plain text the performance of QUIC-FL with different values of its hyperparameters. For example, in Section 3.5 \u201caccelerating QUIC-FL with RHT\u201d, it states the NMSE of QUIC-FL is bounded by \u2026 with $b=1,2,3,4$. It\u2019d be easier for the readers if those numbers can be turned into figures.**\n\nWe will add the figures to the paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5142/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700073430515,
                "cdate": 1700073430515,
                "tmdate": 1700073430515,
                "mdate": 1700073430515,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JQT7FWRJ14",
            "forum": "v8eWha27jw",
            "replyto": "v8eWha27jw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5142/Reviewer_umvY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5142/Reviewer_umvY"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel quantization algorithm with application in federated learning. The key parameters are determined via a constraint optimization problem. Notably, coordinates above the determined threshold are explicitly transmitted to the server, while other values are quantized. To simplify decoding on the server, the clients apply a common preprocessing rotation to the local vectors. This step also modifies the distribution of the coordinates to reduce quantization errors."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The overview of the state-of-the-art is well presented, with comparisons with existing methods.\n\n+ The authors demonstrate that the encoding and decoding times of their method are comparable to those of competitors, but with greater precision."
                },
                "weaknesses": {
                    "value": "* The authors propose interesting contributions, although some ideas have similarities with existing work."
                },
                "questions": {
                    "value": "* The parameters of the quantization set $\\mathcal{Q}_{b,p}$ are obtained by solving a problem that considers the quantiles of a truncated Gaussian distribution. In practice, do the entries of the rotated vectors follow this Gaussian distribution? Instead of considering the standard normal distribution $\\mathcal{N}(0,1)$, would it be possible to approximate the coordinate distribution by a parametric distribution?\n\n* What is the complexity of the optimization problem for determining $b, p, t_p$?\n\n**Requested Changes:**\n\n* Page 3: \"have\" is probably missing between \"we\" and \"that\" at the end of the paragraph \"Problems and Metric\".\n\n* Page 5: in the definition of $\\mathcal{A}\\_{p,m}$, I think $A\\_{p,m}(i)$ should be replaced by $\\mathcal{A}_{p,m}(i)$.\n\n* Page 7: \"the sender sends the message $x$\", perhaps should be replaced by \"the sender sends the message $R(h,x)$\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I don't see any major concerns, the paper designs a novel quantization method with a particular interest in federated learning."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5142/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766061326,
            "cdate": 1698766061326,
            "tmdate": 1699636507964,
            "mdate": 1699636507964,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "99h8agfFfs",
                "forum": "v8eWha27jw",
                "replyto": "JQT7FWRJ14",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5142/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5142/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author's response"
                    },
                    "comment": {
                        "value": "Thank you for the review.\n\nRegarding the first question: Yes! One can use our approach to optimize the quantization for the actual distribution (shifted-Beta, as we explain in the paper - page 6). We knowingly chose to optimize it for the normal distribution instead as (1) it is independent of the dimension of the input vectors, (2) it is a good approximation and the error that arises from it diminishes very quickly with the dimension.\nThis way, we can run the solver once and get a solution that is applicable across a range of vector dimensions and learning tasks. By restricting the number of solver invocations, we can also run the solver to yield a better approximation (e.g., more quantiles and shared randomness values) for the same overall computation/time.\n\nThe runtime of the solver is exponential in $b$ (and is largely independent of $p$ and $t_p$), but we only need to run it once (offline) for each value of $b$, and the values of interest are small (e.g., $b\\in\\{1,2,3,4\\}$).\n\nWe will clarify these in the paper.\n\nRegarding the requested changes: \nWe will fix the first two. Thank you!\nWith respect to the third, it should indeed be $x$ and not $R(h,x)$, which is the value that the receiver estimates the coordinate at given the shared randomness value $h$ and message $x$."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5142/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072827435,
                "cdate": 1700072827435,
                "tmdate": 1700072827435,
                "mdate": 1700072827435,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ucn4k6O5sa",
            "forum": "v8eWha27jw",
            "replyto": "v8eWha27jw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5142/Reviewer_oj6v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5142/Reviewer_oj6v"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors propose a distributed mean estimator, namely QUIC-FL, to estimate the mean of n vectors in a distributed setting. Their method achieves the optimal $O(\\frac{1}{n})$ NMSE (normalized mean squared error). They provide asymptotic improvement to either encoding complexity or decoding complexity (or both) with respect to the existing methods providing $O(\\frac{1}{n})$ NMSE guarantees."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I found the introduction of bounded support quantization and its use to achieve $O(\\frac{1}{n})$ NMSE interesting. I generally liked the presentation and clarity of the paper. The claims have been repeated at times, I believe for emphasis, but otherwise, it is a well-written paper. I also liked the way authors have placed their work with respect to the existing works. They have also provided a good set of numerical experiments to validate their theory."
                },
                "weaknesses": {
                    "value": "The gain in accuracy seems marginal (if any) as compared to EDEN empirically. The proposed method does perform better in terms of decoding time, but decoding time is usually not a big concern when it is done in a centralized server with sufficient processing power."
                },
                "questions": {
                    "value": "I am slightly confused by the following statement on page 4.\n\n\"Empirically, sending the indices using ...as $p . \\log d << 1$ in our settings, resulting in fast processing time and small bandwidth overhead.\"\n\nDoes this mean that $p$ is not kept constant? If that is the case, then shouldn't NMSE have an order of $O(\\frac{\\log d}{n})$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5142/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5142/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5142/Reviewer_oj6v"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5142/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698991443852,
            "cdate": 1698991443852,
            "tmdate": 1699636507883,
            "mdate": 1699636507883,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JwjnZPodxQ",
                "forum": "v8eWha27jw",
                "replyto": "ucn4k6O5sa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5142/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5142/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author's response"
                    },
                    "comment": {
                        "value": "Thank you for the review.\n\nIndeed, we do not claim that QUIC-FL is generally more accurate than EDEN in practice. Our contribution is in getting a comparable accuracy with asymptotically faster decoding time. \n\nMoreover, our contribution is not just in improving the decoding complexity, but also in providing stronger **worst-case guarantees**. Namely, EDEN has an NMSE of $O(1/n)$ only when using uniform random rotation (which requires $O(d^3)$ time and is computationally intensive) and $O(1)$ when using RHT (as the quantization is biased; see Appendix A for details). In contrast, QUIC-FL has $O(1/n)$ NMSE even when using RHT. \n\nRegarding the decoding time, as we convey in the paper:\nAccording to recent sources (e.g., see the \u201cFederated Learning with Formal Differential Privacy Guarantees\u201d blog post by Google), cross-device FL training procedures can accommodate many thousands of devices per round. Current works (see \u201cFederated Learning with Formal Differential Privacy Guarantees,\u201d SysML 2019) even discuss using tens of thousands of clients per round. In such a setup, if one wishes to use the most accurate available DME techniques, asymptotically faster aggregation time may translate to significant savings in time and/or resources, even if the aggregation is performed on a powerful server.\n\nIn light of our consistent findings and the existing evidence that suggests such savings translate to reduced latency (\u201c..devices have significantly higher-latency, lower-throughput connections..\u201d, Federated learning: Collaborative machine learning without centralized training data, Google blog), we believe our approach will indeed lead to improved real-world deployment.\n\nRegarding the overhead of encoding the indices \u2013 we do consider $p$ to be a constant (e.g., 1/512). You are correct that if we encode the indices explicitly, this will cost $O(d\\log d\\cdot p)$ bits ($O(d\\log d)$ if $p$ is constant). In theory (and in practice), we can simply use $d$ bits to encode the indices in a 0-1 vector for $d$-bit overhead for all indices combined in all cases. In practice though, as $p$ is considerably smaller than $1/\\log d$, it is cheaper to send the indices, and this was the point of our statement. \n\nIn summary, the theoretical results can be achieved by sending a bit vector, and thus we achieve the $O(1/n)$ bound with $b=O(1)$. In practice, for reasonable $p$, $d$, explicit encoding of the $p$-fraction of the coordinates is cheaper. \n\nWe will clarify these points in the paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5142/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072419620,
                "cdate": 1700072419620,
                "tmdate": 1700072419620,
                "mdate": 1700072419620,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KIRLkM7KPm",
            "forum": "v8eWha27jw",
            "replyto": "v8eWha27jw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5142/Reviewer_13gx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5142/Reviewer_13gx"
            ],
            "content": {
                "summary": {
                    "value": "The authors study Distributed Mean Estimation problem (DME) where $n$ clients communicate a representation of a $d$-dimensional vector to a parameter server which estimates the vectors\u2019 mean.\n\nI think the overall presentation of the paper, for example providing Figure 1 is quite helpful to help readers understand the main contribution of this paper. To the best of my knowledge, related work have been covered sufficiently. \n\nIn terms of technical contribution, this paper is built on the previous literature DRIVE and EDEN and improves Encoding and Decoding complexity bounds under similar normalized mean squared error bounds. It seems the main advantage of QUIC-FL comes from the tailored random rotation preprocessing which reduces the constant in the NMSE error bound for small values of $p$.\n\nI have an overall positive impression about this work, while I think there are rooms for improvement that will be discussed in the following.  \n\nThe authors provide PuTorch and TensorFlow implementation and show improvements over QSGD Hadamard, and Kashin. The improvements over DRIVE and EDEN is somehow marginal. It will be also very helpful for the readers if the authors elaborate on the discussion after Theorem 3.1."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think the paper is overall quite well-written. \n\nThe related work is comprehensive. I also think it is very nice that that the authors show transparently the superiority of EDEN on low bit-width region in terms of NMSE.\n\nI also like the overall flow of the paper including the intuition provided by authors within the algorithmic description."
                },
                "weaknesses": {
                    "value": "The authors provide PuTorch and TensorFlow implementation and show improvements over QSGD Hadamard, and Kashin. \nThe improvements over DRIVE and EDEN is somehow marginal. \n\nI appreciate that the authors show the superiority of EDEN on low bitwidth region in terms of NMSE.\n\nI was just wondering whether the authors can come up with a hybrid type method that enjoys the NMSE of EDEN while have similar coding time improvements of QUIC-FL? \n\n------------------\n\nI appreciate the discussion after Theorem 3.1 regarding $\\mathrm{E}\\big[\\big(Z-\\hat Z\\big)^2\\big]$. However, it will be still great if the authors provide an explicit error bounds in terms of $b,p,d$. In the current form, it is a bit difficult to provably show the theoretical improvement."
                },
                "questions": {
                    "value": "I was just wondering whether the authors can come up with a hybrid type method that enjoys the NMSE of EDEN while have similar coding time improvements of QUIC-FL? \n\n\nCould the authors provide a more explicit bound in Theorem 3.1? \n\nI will be willing to increase my scores during the rebuttal period."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5142/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699137555090,
            "cdate": 1699137555090,
            "tmdate": 1699636507783,
            "mdate": 1699636507783,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZHxkjxh7Is",
                "forum": "v8eWha27jw",
                "replyto": "KIRLkM7KPm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5142/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5142/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author's response"
                    },
                    "comment": {
                        "value": "Thank you for the review.\n\nUnfortunately, QUIC-FL\u2019s faster decoding relies on the quantization itself being unbiased, while EDEN uses an optimized biased quantization of the transformed coordinates.  EDEN becomes unbiased for a single sender only after the inverse rotation and scaling are applied; however, this also requires users to use different rotations because otherwise, the senders\u2019 results are not independent. EDEN\u2019s better vNMSE is a direct implication of biased quantizations being capable of lower error rates than unbiased ones. We do not see an approach that allows QUIC-FL to apply different rotations to different senders and maintain its decoding time. In particular, because QUIC-FL uses the same rotation for all senders, the unbiasedness comes from a randomized rounding using the same rotation (unlike EDEN\u2019s deterministic rounding, with different random rotations). We do not see how a hybrid can be achieved.  \n\nRegarding 3.1, the term $\\mathbb E[(Z-\\widehat Z)^2]$ is independent of $d$ and only depends on $b,p$. We present it in this way to allow one to understand how further improvements in the unbiased quantization of normal random variables (e.g., if one has a more powerful solver or is able to solve the continuous variant) translate directly to improvements in the vNMSE bound. We can provide explicit tables that give bounds for meaningful values of $b,p$. One such result is given in Thm G.3 for running QUIC-FL with RHT. Here, we also give the vNMSE results for QUIC-FL with a uniform random rotation, which are closer to the error in practice even when using RHT:\n\np = 1/128:\n\nb = 1, $\\mathbb E[(Z-\\widehat Z)^2]$ = 1.408\nb = 2, $\\mathbb E[(Z-\\widehat Z)^2]$ = 0.201\nb = 3, $\\mathbb E[(Z-\\widehat Z)^2]$ = 0.0398\nb = 4, $\\mathbb E[(Z-\\widehat Z)^2]$ = 0.00859\n\np=1/512:\n\nb = 1, $\\mathbb E[(Z-\\widehat Z)^2]$  = 1.517\nb = 2, $\\mathbb E[(Z-\\widehat Z)^2]$ = 0.223\nb = 3, $\\mathbb E[(Z-\\widehat Z)^2]$ = 0.0444\nb = 4, $\\mathbb E[(Z-\\widehat Z)^2]$ = 0.00982\n\nIn order to get a general formula for uniform random rotations, we can consider uniform quantization (we do at least as good as this). The bound that depends on $b,T_p$ would be  $O(T_p^2 \\cdot 4^{-b})$.\nRecall that for $p>0$ we have $T_p = \\Phi^{-1}(1-p/2)$, which gives a bound in terms of $p$.\n\nWe will add this discussion to the paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5142/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072158288,
                "cdate": 1700072158288,
                "tmdate": 1700072158288,
                "mdate": 1700072158288,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PAshUnIdAX",
                "forum": "v8eWha27jw",
                "replyto": "ZHxkjxh7Is",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5142/Reviewer_13gx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5142/Reviewer_13gx"
                ],
                "content": {
                    "title": {
                        "value": "After Rebuttal"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their response. I went through Theorem G.3 and noticed that your upper bound is indeed \"per coordinate\" so the final MSE is in $O(d)$. \n\nOn Table 1, the authors claim that the NMSE for QSGD is $O(d/n)$, which is wrong. It is indeed $O(\\sqrt{d})$ (their $n$ in their Lemma 3.1 is your $d$). For this reason and also the comment raised by Reviewer fdHV regarding the claims in the paper, unfortunately, I do not recommend acceptance."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5142/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651023870,
                "cdate": 1700651023870,
                "tmdate": 1700651023870,
                "mdate": 1700651023870,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]