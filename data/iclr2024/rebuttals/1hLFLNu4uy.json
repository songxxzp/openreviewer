[
    {
        "title": "Split and Merge: Aligning Position Biases in Large Language Model based Evaluators"
    },
    {
        "review": {
            "id": "5UArZzXoKH",
            "forum": "1hLFLNu4uy",
            "replyto": "1hLFLNu4uy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1761/Reviewer_cPgm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1761/Reviewer_cPgm"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel evaluation system called Portia based on large language models (LLMs). Portia is designed to mitigate positional bias in pairwise comparison with LLMs. Instead of presenting candidate answers sequentially, Portia splits each answer into segments, aligns these segments on 2 sides based on their similarities, and then combines segment pairs to construct prompts for LLMs. Portia exhibits a notable enhancement in answer agreement compared to the baseline method, effectively rectifying most of the inconsistencies found in the baseline models. Furthermore, the evaluations generated by Portia demonstrate a closer alignment with both human evaluations and outputs from GPT-4. Lastly, the authors conduct a cost analysis of Portia, identifying an optimal segment number that strikes a balance between cost and effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well written with a clear description of the method and appropriate discussions.\n2. As outlined in the summary section, Portia can be practically useful as it improves the consistency of LLMs for pairwise comparison and improves the assessment quality, especially for smaller LLMs.\n3. The experiment design is reasonable, covering many popular LLMs and 3 comparison forms.\n4. The paper includes extensive appendices and supplementary materials, which makes the paper easy to reproduce and more understandable."
                },
                "weaknesses": {
                    "value": "1. It is in doubt about the semantic meaningfulness of alignment. The method assumes that answers can be deconstructed, allowing for rough alignment on a segment-by-segment basis. However, this assumption may not universally hold for open-ended answers. It is common for two models to generate entirely distinct yet equally valid responses. Additionally, they may produce similar content with variations in the order, as exemplified in the question section. In many scenarios, alignment may not yield meaningful results.\n2. Limited applicability: The proposed system, i.e. the pipeline of splitting, alignment, and merging, is a complicated rule-based system, which can be brittle. The authors only test their system on MT benchmark, so it is interesting to see if a system that is fine-tuned toward MT benchmark can be applicable to other forms of assessment tasks."
                },
                "questions": {
                    "value": "1. The paper mentions that the split should preserve the answer order, then it may cause an issue of alignment when the answer is order insensitive. What if 2 models generate segments of texts with different orders, but both are fine? E.g. the question is \"What should you do in the morning after waking up?\". Model A generates \"brush teeth and wash face\" and model B generates \"wash face and brush teeth\", then an oder-invariant alignment will not be able to find an optimal alignment.\n2. How does Portia work for short sequences, which cannot be split into shorter segments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1761/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697841045362,
            "cdate": 1697841045362,
            "tmdate": 1699636105364,
            "mdate": 1699636105364,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YklxZfo09U",
                "forum": "1hLFLNu4uy",
                "replyto": "5UArZzXoKH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1761/Reviewer_cPgm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1761/Reviewer_cPgm"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. I agree with other reviewers and think the major issue of this paper is still the generalizability. The experiments in the global response do not alleviate my concerns: It shows that the variable lengths do not affect the consistency, but the point of \"split and merge\" seems groundless in this scenario. Moreover, it does not explain how Portia works for semantically different responses."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1761/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587200085,
                "cdate": 1700587200085,
                "tmdate": 1700587200085,
                "mdate": 1700587200085,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GFmRlLwSel",
            "forum": "1hLFLNu4uy",
            "replyto": "1hLFLNu4uy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1761/Reviewer_SWLn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1761/Reviewer_SWLn"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of using LLMs as evaluators. They focus on pair-wise comparison, where the LLM is given a pair of responses to a question and it needs to select which one is better. They aim to mitigate the position bias of LLM evaluation, which is the case that an LLM evaluation result is different by swapping the two responses. \n\nThis paper proposes a pipeline, Portia, to mitigate the position bias in pair-wise LLM evaluation. This is done by the following process: Given two responses, they split each response into an equal number of fragments with roughly the same size, and they pair the fragments from the two responses together and let the LLM evaluate. \n\nThe results show that Portia can largely mitigate the positional bias of 6 LLMs including Claude-2, Llama2, and ChatGPT. They also show that Portia is a method that can close the gap between GPT-4 and ChatGPT while costing less resources (money and time)"
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Positional bias in LLM evaluation is an important problem that needs to be solved.\n- The proposed method can successfully reduce the positional bias of all the six LLMs they tested on."
                },
                "weaknesses": {
                    "value": "- **The method is not clearly described, making it hard for me to properly evaluate the soundness and contribution of the paper**. Specifically, the semantic alignment part is incomprehensible. The text on page 5 does not provide too much information, and Algorithm 1 also does not clearly explain what it is doing. The functions $partition$ and $n_s$ are not explained and are hard to understand. The appendix also does not provide any further elaborations. So I also cannot evaluate the correctness of the analysis on page 7. Considering that this is a core part of the proposed method, **I cannot assess this paper if this issue is not resolved.** I am willing to adjust my review based on the author's response.\n\n- The models used in this paper are not very well explained. I cannot see which Llama-2 model is used, the Llama-2 model or the Llama-2-chat model. \n\n- The experiment setting in Appendix B is not clear. The paper seems to require that the scores in different scales need to have a linear mapping relationship. I do not agree with this. As long as we can use the scores derived from LLMs ratings to obtain meaningful comparison. the scores are meaningful. So a more reasonable comparison may be calculating the correlation coefficient as prior works that use single-wise comparison. Refer to [1, 2] for more experiment details. \n\n- Missing references: [1] and [2] are the earliest two works that use LLMs as evaluators and should be included in the related works.\n\n\n[1] Cheng-Han Chiang and Hung-yi Lee. 2023. Can Large Language Models Be an Alternative to Human Evaluations?. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15607\u201315631, Toronto, Canada. Association for Computational Linguistics.\n\n[2] Liu, Yang, et al. \"Gpteval: Nlg evaluation using gpt-4 with better human alignment.\" arXiv preprint arXiv:2303.16634 (2023)."
                },
                "questions": {
                    "value": "Q1. What happens if the responses to be rated are very short, or what if the two responses that need to be rated have a very different length?\n\nI may have further questions after the authors respond to the weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1761/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698648849508,
            "cdate": 1698648849508,
            "tmdate": 1699636105269,
            "mdate": 1699636105269,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "ryIqS8JlY1",
            "forum": "1hLFLNu4uy",
            "replyto": "1hLFLNu4uy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1761/Reviewer_YB2D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1761/Reviewer_YB2D"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method for improving the consistency of using LLM to make choices between two generated texts. The method includes a split-merge process, where the two generated texts are split, aligned and merged into a single text, so that the position bias is reduced.\n\nThe writing and organization of the paper is pretty well."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method is simple. \n\nThe experiments demonstrated that the consistency is improved for different LLMs by a considerable margin."
                },
                "weaknesses": {
                    "value": "The proposed method seems to be problematic in the following cases:\n\nFor answers/texts that are significantly different in length. Or with almost the same length but significantly different in symantics or content. \n\nIf the answers/texts have different orders, changing the order of one of the text may affect the evaluation of the generation quality.\n\nBesides, is it possible that the merging operation makes it harder (longer context, in a comparing way) for the LLMs to understand and evaluate the output?"
                },
                "questions": {
                    "value": "See the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1761/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698666185070,
            "cdate": 1698666185070,
            "tmdate": 1699636105189,
            "mdate": 1699636105189,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "U2ZA3LYNdO",
            "forum": "1hLFLNu4uy",
            "replyto": "1hLFLNu4uy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1761/Reviewer_RHzc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1761/Reviewer_RHzc"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new method called PORTIA, which is designed to address position bias in large language model-based evaluators. The method uses semantic and length alignment to calibrate position bias in a lightweight and cost-effective manner. Specifically, PORTIA splits the answers into multiple segments, aligns similar content across candidate answers, and then merges them back into a single prompt for evaluation by LLMs. The authors present experiments with six LLMs and demonstrate the effectiveness of PORTIA in enhancing their consistency rates in serving as evaluators."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper addresses an interesting and important limitation of LLM-based evaluators, namely position bias, which has been known to affect the consistency and accuracy of evaluations. \n* The proposed method PORTIA is simple to execute."
                },
                "weaknesses": {
                    "value": "* The method design is a bit ad-hoc and does not seem to be generalizable enough for evaluating open-ended generation, considering how flexible/free-form the generation results can be. Specifically, the method requires splitting multiple answers being compared into the same amount of segments, and expecting that the answers are somewhat overlapping semantically. However, this may not work well when the generation results are largely different from each other, and this could be very common in open-ended generation. For example, in creative writing tasks, the answers being compared may not have any semantic overlap, and may even differ a lot in their lengths. It seems that the proposed method does not take these cases into account. Also, I'm not sure if splitting a response into multiple segments is really a good idea, since generation evaluation usually has to consider the answers' coherency (potentially spanning over long-range contexts that should be evaluated as a whole).\n* The experiment evaluation only focuses on the consistency rates and does not provide evidence of its impact on the accuracy (e.g., whether the calibrated prediction corresponds better to human judgments). Such evaluations are obviously necessary because any naively deterministic method (for example, a system that always prefers the longer answer) will have a consistent rate of 100% but will not be useful in practice.\n* I expect to see stronger baselines included (there are several methods aiming at enhancing LLM-as-evaluators, though they may not be completely addressing position bias). I'd encourage the authors to also discuss the new studies that came out after the ICLR submission deadline such as Zeng et al. (this is not a weakness but a suggestion)\n\nReference:  \nZeng et al. \u201cEvaluating Large Language Models at Evaluating Instruction Following.\u201d ArXiv abs/2310.07641"
                },
                "questions": {
                    "value": "Please address the weaknesses mentioned above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1761/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1761/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1761/Reviewer_RHzc"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1761/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816603640,
            "cdate": 1698816603640,
            "tmdate": 1700710327650,
            "mdate": 1700710327650,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]