[
    {
        "title": "Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning"
    },
    {
        "review": {
            "id": "qZSDid0UnX",
            "forum": "1jbh2e0b2K",
            "replyto": "1jbh2e0b2K",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission976/Reviewer_AgCa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission976/Reviewer_AgCa"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies multitask finetuning for adapting foundation models to new tasks in few-shot. It provides a theoretical justification to analyze the multitask finetuning and a practical task selection algorithm for choosing good finetuning tasks. Extensive experiments are conducted to validate the proposed algorithm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper proposes a theoretical framework to analyze multitask finetuning for foundation models, providing definitions and analysis of task diversity and consistency. Based on the theoretical concepts, the authors introduce a practical task selection algorithm.\n- The authors conducted extensive experiments and showed the effectiveness of the proposed method compared to the direct adaptation and full fine-tuning baseline.\n- The overall writing is good, which clearly explains the problem, approach, analysis and results."
                },
                "weaknesses": {
                    "value": "- The theoretical part discussed diversity and consistency, but the method part simplifies consistency to similarity and diversity to coverage. So the final algorithm is basically a similarity-based sorting. The simplicity is not a problem, but I wonder if it is really related to the theoretical concepts.\n- The authors evaluated the proposed algorithm on a wide range of datasets/models, but how does it compare with some stronger baselines besides direct adaptation and full fine-tuning, e.g. meta-learning algorithms?"
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission976/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697900560296,
            "cdate": 1697900560296,
            "tmdate": 1699636023527,
            "mdate": 1699636023527,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C5CSeZvEXB",
                "forum": "1jbh2e0b2K",
                "replyto": "qZSDid0UnX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission976/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission976/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission976/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission976/Reviewers",
                    "ICLR.cc/2024/Conference/Submission976/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission976/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission976/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarifications and revision"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and address the questions below.\n\n### Consistency to similarity and diversity to coverage\n\nWe formally show consistency is reduced to similarity and diversity to ellipsoid coverage in Gaussian distribution. We consider each latent class/representation as a feature vector. The non-zero entries contain the information/feature. The non-zero entries contain the information. Both consistency and diversity can be depicted through the variations in the entries of target tasks' latent classes, which are encoded by entries in the finetuning tasks' latent classes. \n\nTo meet consistency, the latent classes in finetuning classes should not include an excessive amount of unrelated varying entries. This is directly related to the similarity in the context of Gaussian distributed features. In our framework, consistency is quantified by how closely the task-related feature distributions align, which is effectively captured by the similarity between mean vectors. \n\nSimilarly, to meet the diversity criterion, we would want latent features in target tasks covered by that in finetuning tasks. Under our theoretical model, diversity equates to the spread of the feature vectors across the task space. The ellipsoidal coverage represents the extent to which the feature space of the finetuning tasks encapsulates the feature space of the target task. \n\n### Stronger baselines besides direct adaptation and full finetuning, e.g. meta-learning algorithms\n\nOur experiments were designed to demonstrate the effectiveness of the proposed method against common approaches like direct adaptation and full finetuning. We interpret that the reviewer is pointing to Table 2 regarding our multitask finetuning approach. It's important to recognize that our multitask finetuning strategy falls under the category of meta-learning algorithms, as it involves treating target few-shot tasks as individual tasks and creating analogous few-shot finetuning tasks to refine the model. This process assists the model in learning to tackle these few-shot challenges effectively. In response to the feedback, we aim to incorporate additional meta-learning algorithms into our comparison table to ensure a more equitable evaluation."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission976/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699981886411,
                "cdate": 1699981886411,
                "tmdate": 1699981886411,
                "mdate": 1699981886411,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TAvcHvMXyK",
                "forum": "1jbh2e0b2K",
                "replyto": "qZSDid0UnX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission976/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission976/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission976/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission976/Reviewers",
                    "ICLR.cc/2024/Conference/Submission976/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission976/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission976/Authors"
                ],
                "content": {
                    "title": {
                        "value": "New update: more experiments on meta learning baseline"
                    },
                    "comment": {
                        "value": "We incorporated the Model-Agnostic Meta-Learning (MAML) algorithm, as outlined by Finn et al. [1], as another baseline for our few-shot tasks.\n\nMAML operates in a two-step process: it initially updates parameters based on within-episode loss (the inner loop), then it evaluates and updates loss based on learned parameters (the outer loop). We follow the pipeline in [2] to implement MAML for few-shot tasks.\n\nHere we show the results of the comparison for DINOv2 ViT-S on tieredImageNet. We refer the reviewer to Table 14 in Appendix G.8 for complete results.\n\n| Method       | 1-shot       | 5-shot       |\n|--------------|--------------|--------------|\n| Adaptation   | 74.54 (0.32) | 89.20 (0.19) |\n| Standard FT  | 74.84 (0.32) | 89.30 (0.19) |\n| MAML         | 74.63 (0.34) | 89.60 (0.19) |\n| Ours         | **77.78 (0.32)** | **90.23 (0.18)** |\n\nAs illustrated in the table, MAML can slightly outperform Adaptation and Standard FT for DINOv2 ViT-S on tieredImageNet, but lags behind our multitask finetuning. The reason is in few-shot tasks, support samples (shot image) are very limited, preventing the model from learning a good set of parameters for adapting to new tasks.\n\n\n[1] Finn et al. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. PMLR 2017.\n\n[2] Triantafillou et al. Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples. ICLR 2020."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission976/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700090354403,
                "cdate": 1700090354403,
                "tmdate": 1700090719409,
                "mdate": 1700090719409,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YFJ8d43cjo",
            "forum": "1jbh2e0b2K",
            "replyto": "1jbh2e0b2K",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission976/Reviewer_mX1E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission976/Reviewer_mX1E"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents theoretical results that to adapt a pretrained model to a target task, it is beneficial to first finetune the model on a diverse set of related tasks and then tune the model on the target task. The authors prove the theorems by first defining the diversity and consistency between different tasks that the model is going to adapt to, where diversity refers to the coverage of the other tasks on the target task and consistency refers to the similarity between the other tasks and the target task. Then, under some Lipschitzness assumptions, sufficient consistency and diversity assumptions, and sufficient tasks and sufficient sample assumptions (on the finetuning tasks, not the target task), the model with multitask finetuning can achieve a reduced error on the target task compared to the setting without multitask finetuning. Moreover, the authors propose a multitask selection algorithm based on the consistency and diversity requirements. Experimental results indicate that with the proposed multitask selection, the ViT-B model achieves better results on multiple target datasets, and increasing the number of tasks and the number of samples per task are most effective in improving target task performance. Lastly, with multitask finetuning, various pretrained models achieve better results than direct adaptation / standard finetuning on multiple datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The presented theoretical results are quite intuitive -- if the tasks in multitask finetuning are diverse and consistent with the target task $T_0$, then the model with pretraining followed by multitask finetuning achieves better target prediction performance than the model only with pretaining. My intuition is that, through the training/finetuning on the proxy tasks, the model learns some knowledge related to the target tasks. It is nice that the authors present the theorems that support this intuition, although I didn't verify the proofs. \n- Although the theorems are not directly applicable to real problems, the authors propose a practical Consistency-Diversity Task Selection algorithm, which is effective in experiments. The algorithm is a nice method to try for target task adaption in the real world.\n- The overall experiments are quite extensive and echo the theoretical results, clearly demonstrating the advantages of multitask finetuning over standard finetuning and direct adaptation."
                },
                "weaknesses": {
                    "value": "- The Gaussian distribution assumption in Section 3.2 may not be realistic, leading to biased task selection that may be suboptimal to the model performance on the target task. However, it is okay to derive further refined algorithms in future work."
                },
                "questions": {
                    "value": "- I found the notations in Section 3.1 (Linear Data and Tasks) somewhat difficult to understand. $T_{z, z'}$ is discussed in the text but how is it related to the $T_i (i = 1, \\dots, M)$ in figure 1? It would ease the reader's understanding by revising the explanation in that part. \n- Does the Experiment Protocol apply to all three major experiments (4.1, 4.2, and 4.3)? It seems to me that the experiments have their own protocols, which causes confusion.\n- What are the multitasks for finetuning in Section 4.3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission976/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827093592,
            "cdate": 1698827093592,
            "tmdate": 1699636023439,
            "mdate": 1699636023439,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jr0dTq0GuE",
                "forum": "1jbh2e0b2K",
                "replyto": "YFJ8d43cjo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission976/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission976/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarifications and revision"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and address the questions below.\n\n### Gaussian distribution assumption\n\nPlease see the global rebuttal.\n\n### Relation between $\\mathcal{T}_{z,z\u2019}$ and $\\mathcal{T}_i (i = 1,\\ldots,m)$\n\nWe realize that the notation used in Section 3.1 could be made clearer. In Figure 1, we denote each task containing two latent classes, namely ($z$,$z\u2019$). Each task in Figure 1 can be represented as ($T_1$ to $T_{z_1,z_1^\\prime}$, $T_2$ to $T_{z_2,z_2^\\prime}$). We thank the reviewer for pointing this out. We have updated this in the new revised manuscript.\n\n### Confusions in the Experiment Protocol in Section 4\n\nThe paragraph **Experiment Protocol** outlined at the beginning of Section 4  applies to all three major experiments (Sections 4.1, 4.2, and 4.3).  We clarify further here: In Section 4.1, we retain the same finetuning setup and vary the sample complexity in tieredImageNet. In Section 4.2, we continue with the same finetuning process and apply the task selection algorithm on Meta-Dataset. In Section 4.3, we retain the same finetuning setup and include the additional standard finetuning where we append the encoder with a linear head to map representations to class logits and finetune the whole model.\n\nAll experiments focus on evaluating models using few-shot tasks. For multitask finetuning, the model's parameters are updated using the nearest centroid classifier. This involves encoding all samples, calculating class centroids, and using the cosine similarity between a query sample and these centroids as the class logits. For adapting to a specific target task, we employ the model's encoder along with the same nearest centroid classifier approach.\n\nWe will ensure that the protocol is clearly described for each experiment in the revised manuscript to avoid any confusion. \n\n### Multitasks for finetuning in Section 4.3\n\nIn Section 4.3, the multitasks are the few-shot tasks consisting of $K=15$ classes with $N=1$ support samples and $Q=10$ query samples per class (known as $K$-way $N$-shot). The multitasks selected for finetuning were chosen based on their relevance and consistency with the target tasks, as prescribed by our Consistency-Diversity Task Selection algorithm."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission976/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699981832647,
                "cdate": 1699981832647,
                "tmdate": 1699981832647,
                "mdate": 1699981832647,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NJ4TVO3xMg",
            "forum": "1jbh2e0b2K",
            "replyto": "1jbh2e0b2K",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission976/Reviewer_1yNr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission976/Reviewer_1yNr"
            ],
            "content": {
                "summary": {
                    "value": "Given a pretrained foundation model, a target task with limited labeled samples, and a bunch of auxiliary tasks possibly with many labeled data, this paper analyzes how to select good auxiliary tasks to finetune the foundation model first, then adapt the auxiliary-task-finetuned model to the target task (a.k.a multitask finetuning), such that the resultant model can outperform the model that directly adapted to the target task from pretraining, or the model that finetuned with all the auxiliary tasks before adaption. To this end, the authors derive several bounds which indicate that the model with multitask finetuning outperforms that without, and the selected auxiliary tasks should be sufficiently diverse and have good consistency w.r.t. the target task. According to this, the authors formulate an auxiliary-tasks-selection algorithm, whose outperformance over standard finetune is validated on various datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Several bounds about multitask finetuning are derived.\n2. The multitask finetuning with task selection outperforms that finetuned on all the tasks.\n3. The experiments have been extensively performed on a dozen of datasets."
                },
                "weaknesses": {
                    "value": "1. While several bounds have been derived, their implications are quite straightforward. For instance, the model with multitask fine-tuning outperforms the one without it, and the selected auxiliary tasks should exhibit sufficient diversity and strong consistency w.r.t. the target task, are well accepted by the community, despite those being derived based on the somewhat strict assumptions. It would be more valuable if these bounds could lead to novel insights that were previously overlooked by the community.\n2. Such limitation also lies in the task-selection algorithm, where the authors simply assume a Gaussian distribution of features, then use the angle of mean and variance-scaled L2 distance indicating the consistency and the diversity.\n3. As \\phi evolves during the training, should Algorithm 1 perform repeatedly during the training?\n4. The paper in its current version is quite difficult to read, it is suggested to give more intuition before diving into the derivation, and also simplify/brief the symbols if applicable."
                },
                "questions": {
                    "value": "1. Why does the performance decrease with the increase of sample shots in Fig. 2a?\n2. Is Standard FT in Table 2 the same as All in Table 1?\n3. First Paragraph on Page 4, In contrastive pretraining -> In contrastive to pretraining; First Paragraph on Page 7, the cosine similarity similarity -> the cosine similarity\n4. It seems that Theorem 3.3 (or the index 3.3) is missing in the main text."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission976/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698842205946,
            "cdate": 1698842205946,
            "tmdate": 1699636023359,
            "mdate": 1699636023359,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8jIIRDJEQS",
                "forum": "1jbh2e0b2K",
                "replyto": "NJ4TVO3xMg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission976/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission976/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarifications and revision"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and address the questions below.\n\n### Novel insights of theoretical bound\nWhile our derived bounds may coincide with what has been empirically observed, it's important to recognize that such empirical observation, until formalized, remains a hypothesis. By providing a rigorous theoretical foundation, we not only validate these empirical observations but also establish a framework upon which future hypotheses can be tested and understood. \n\nWithin the theoretical community, various concepts of diversity have been proposed to establish guarantees [1,2]. Yet, we propose an additional notion of consistency, highlighting the significance of aligning finetuning data with target data. Conversely, practitioners in the empirical community have predominantly concentrated on algorithms whose selection is based on similarity ([3,4] and many more). Our algorithm, which integrates both diversity and consistency, aims to enhance task selection when adapting foundation models to specific tasks.\n\n[1] Tripuraneni et al. On the theory of transfer learning: The importance of task diversity. NeurIPS 2020.\n\n[2] Zhao et al. Blessing of Class Diversity in Pre-training. AAAI 2023.\n\n[3] Liu et al. Learning customized visual models with retrieval-augmented knowledge. CVPR 2023.\n\n[4] Liu et al. What Makes Good In-Context Examples for GPT-3? arXiv 2021.\n\n### Gaussian distribution assumption\n\nPlease see the global rebuttal.\n\n### Evolved $\\phi$ in Algorithm 1\n\nWe appreciate this insightful observation. Currently, our algorithm does not perform repeatedly during training. Instead, we utilize a pre-trained $\\phi$ as a feature extractor to obtain the representation of each individual sample. Online update $\\phi$ is interesting and we leave it as a future work. \n\n### Difficulty in readability\nThank you for pointing out improvements in the presentation. To aid in the understanding of task selection, we further include a diagram in Figure 2 in Section 3.2 to show relation between tasks. We have a plan following to improve the readability:\n- Example illustration: We will hold a simplified example before introducing new notations, including the definition of tasks and loss in Section 2.\n- Intuitive explanations: We will introduce each major theoretical result with an intuitive explanation, including our main theorem and theorem for diversity and consistency in Section 3.\n- Symbol simplification: We will try to simplify the notation by reducing the number of symbols and by using more familiar notations, including different models $\\phi$, task $\\mathcal{T}$, loss $\\mathcal{L}$ in Section 3.\n\nWe hope these improvements will enhance the clarity and make our paper more approachable to broader range of readers.\n\n### Performance decreases with the increase of shots in Figure 2a\n\nWe illustrate this phenomenon below as we mentioned in paragraph **Results.** in Section 4.1.\nIn Figure 2a we fix the target task as a $1$-shot setting but vary the number of shots from $1$ to $4$ in finetuning. We recognize that the results may initially appear counterintuitive as they oppose the common belief that finetuning using meta-learning has to mimic the exact setting in the target task. A mismatch will hurt the performance [5]. However, in our study, we discovered that within the range of shots considered (from 1 to 4), the number of shots in finetuning did not significantly alter the accuracy. This observation underscores a key insight from our work: it is the total sample size $M \\times m$, not just the number of shots, that determines the performance gains. This aligns with our theoretical findings (Theorem 3.2).\n\n[5] Snell et al. Prototypical networks for few-shot learning. NeurIPS 2017.\n\n###  **Standard FT** in Table 2 and **All** in Table 1\nWe illustrate \u201cStandard FT\u201d in the Section 4.3 Setup paragraph. We use multitask finetuning for results in Table 1 (including \u201cAll\u201d) as illustrated in the Section 4 Experiment Protocol paragraph. Here we give a brief explanation.\n\nIn Table 2,  \"Standard FT'' refers to another finetuning method where an encoder is appended with a linear head that maps representations learned by encoders to the class logit probability, with the model finetuned based on logit loss. On the other hand, in multitask finetuning, the model is updated using the nearest centroid classifier. This approach differs from \"Standard FT\" in that it does not rely on the logits loss but rather uses class centroids computed from encoded samples. In Table 1, we use a multitask finetuning strategy and focus on comparing the task selection algorithm. \"All\" refers to results without a task selection algorithm and finetuning on all datasets.\n\nWe hope this explanation resolves the confusion and distinguishes between the two finetuning methods.\n\n### Typos of contrastive pretraining, cosine similarity, and missing index of theorem\nWe thank the identification of textual errors. We corrected these in the revised manuscript."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission976/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699983102449,
                "cdate": 1699983102449,
                "tmdate": 1699983125433,
                "mdate": 1699983125433,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "N32RQmi8m7",
            "forum": "1jbh2e0b2K",
            "replyto": "1jbh2e0b2K",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission976/Reviewer_72L9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission976/Reviewer_72L9"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores how to effectively adapt foundational models to new tasks, especially those with limited labeled data. The authors investigate a multi-task fine-tuning approach, wherein the foundational model is fine-tuned for a set of related tasks before fine-tuning it for the target task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper provides a theoretical analysis, revealing that fine-tuning with a diverse set of related tasks reduces errors in the target task compared to directly adapting a pre-trained model. The authors introduce diversity and consistency metrics to quantify the relationship between fine-tuning tasks and the target task and propose a practical task selection algorithm."
                },
                "weaknesses": {
                    "value": "1. In Section 1.1, it would be beneficial to provide a more detailed enumeration of recent advancements in the field of multitask fine-tuning.\n2. I suggest adding some diagrams in Section 3.2 to visually illustrate the entire process. Visual representation will help readers gain a clearer understanding and assess the effectiveness of task selection.\n3. I suggest conducting more detailed ablation experiments for TASK SELECTION to provide stronger evidence for its effectiveness.\n4. The experimental section lacks sufficient detail, such as the absence of descriptions for hyperparameter settings. Providing these details would make the findings more convincing."
                },
                "questions": {
                    "value": "1. In Section 1.1, it would be beneficial to provide a more detailed enumeration of recent advancements in the field of multitask fine-tuning.\n2. I suggest adding some diagrams in Section 3.2 to visually illustrate the entire process. Visual representation will help readers gain a clearer understanding and assess the effectiveness of task selection.\n3. I suggest conducting more detailed ablation experiments for TASK SELECTION to provide stronger evidence for its effectiveness.\n4. The experimental section lacks sufficient detail, such as the absence of descriptions for hyperparameter settings. Providing these details would make the findings more convincing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission976/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission976/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission976/Reviewer_72L9"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission976/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698859600470,
            "cdate": 1698859600470,
            "tmdate": 1699636023285,
            "mdate": 1699636023285,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dvMJOWJLJU",
                "forum": "1jbh2e0b2K",
                "replyto": "N32RQmi8m7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission976/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission976/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission976/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission976/Reviewers",
                    "ICLR.cc/2024/Conference/Submission976/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission976/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission976/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification and more results"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and address the questions below.\n\n### Recent advancements in the field of multitask finetuning\n\nWe included the detailed related work in Appendix C. We refer the reviewer to Section **Multitask Learning** and **Adapting Foundation Models** in Appendix C for multitask finetuning. Please let us know if the reviewer has further concerns or suggestions. \n\n### Visual diagrams in Section 3.2\n\nThe procedure of the proposed task selection method was illustrated in Algorithm 1 of the main paper. Per the reviewer\u2019s request, we further highlight the key intuition using the diagram in Figure 2 under Section 3.2 of the new revised manuscript.\n\n### \u200b\u200bDetailed ablation experiments for task selection\n\nWe conducted several ablation studies presented in Appendix G.4 that examine the task selection:\n-  G.4.1: Violate both consistency and diversity: We maintained the same level of sample complexity but incorporated data from unrelated tasks during finetuning. This resulted in a decline in performance.\n- G.4.2: Only retain diversity while violating consistency: Introducing additional unrelated data during the finetuning phase also led to a deterioration in performance.\n\nPer the reviewer\u2019s request, we further included an additional ablation study on our task selection algorithm, focusing solely on either consistency or diversity, while compromising on the other:\n\n- Ignoring Diversity: If the algorithm terminates early without fulfilling the stopping criteria, the data utilized in finetuning tasks fails to encompass all the attributes present in the target data. This leads to a breach of the diversity principle.\n- Ignoring Consistency: Conversely, if the algorithm persists beyond the stopping criteria, the finetuning tasks become overly inclusive, incorporating an excessive amount of unrelated data, thus breaching the consistency.\n\nWe include details in Appendix G.4.4 and Table 9 of the new revised manuscript. \n\n### Descriptions for hyperparameter settings\n\nWe had detailed experimental settings with hyperparameters in Appendix G.2, including the number of tasks, the number of classes (N) per task, as well as the number of shot samples (K), and query samples (Q). \n\nWe have further expanded the appendix to include specifics on acquiring image embeddings and the methods used to measure consistency and diversity. Additionally, we have included information on the hyperparameters used during the finetuning optimization process. This expansion ensures that our experimental procedures are reproducible and that our findings can be validated. All these additional details are available in Appendix G.2."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission976/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699981238975,
                "cdate": 1699981238975,
                "tmdate": 1699981319206,
                "mdate": 1699981319206,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]