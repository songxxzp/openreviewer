[
    {
        "title": "IMProv: Inpainting-based Multimodal Prompting for Computer Vision Tasks"
    },
    {
        "review": {
            "id": "s1mDr7gRpp",
            "forum": "eJFt8ZRQ9a",
            "replyto": "eJFt8ZRQ9a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4300/Reviewer_gzFg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4300/Reviewer_gzFg"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores in-context visual learning with multimodal prompting, i.e., automatically completing corresponding dense vision tasks according to either visual prompts, text prompts or a combination of both. To achieve this, the authors collected a new dataset of figures in computer vision papers and the associated caption, from the Semantic Scholar website. Based on the previous method MAE-VQGAN, the authors add text as input and use cross-attention layers to fuse the text tokens and image tokens. The model achieves higher performance than the previous self-supervised visual prompting method, and can follow both textual and visual prompts for novel tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- In-context visual learning is a very interesting and significant topic. This paper presents new capabilities of multimodal prompting along this direction.\n- The proposed Semantic Scholar Computer Vision dataset (S2CV) provides new angles to leveraging unlabeled in-context data with both text and images. The dataset can be useful for future research.\n- The method is simple and straightforward, not novel but can be easy to follow and adapt.\n- The papers show many qualitative cases and comparisons in studying the properties of the proposed IMProv model, e.g., comparison with stable diffusion, which provides a lot of insights."
                },
                "weaknesses": {
                    "value": "- It would be more convincing to have more quantitative experiments on more tasks, e.g., one-shot or few-shot tasks. Although it might be challenging, it will give readers a bigger picture.\n- What about using multiple prompts? For example, multiple images, or even multiple image-text pairs as the prompt. It would be interesting to see what happens and how can further unleash the potential of the model.\n- Failure cases should be analyzed and discussed."
                },
                "questions": {
                    "value": "- The authors use ViT-L be default. Did you tried backbone at different scales?\n- If we want to further scale up, how can we get more data like Semantic Scholar Computer Vision dataset? It would be interesting to have more discussion."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4300/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698648453227,
            "cdate": 1698648453227,
            "tmdate": 1699636398086,
            "mdate": 1699636398086,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kV0ibA2h1B",
                "forum": "eJFt8ZRQ9a",
                "replyto": "s1mDr7gRpp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4300/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4300/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thank you for the review and suggestions. We address your comments below.\n\n**Q**: It would be more convincing to have more quantitative experiments on more tasks, e.g., one-shot or few-shot tasks. Although it might be challenging, it will give readers a bigger picture.\n\n**A**: We provide results of more Image-to-X tasks in Table 5 and visualization results in Figure 7 and 8. \n\n**Q**: What about using multiple prompts? For example, multiple images, or even multiple image-text pairs as the prompt. It would be interesting to see what happens and how can further unleash the potential of the model.\n\n**A**: We provide the results of multiple prompts in Table 8 and Figure 14. It shows our model can handle and benefit from multiple prompts.\n\n**Q**: Failure cases should be analyzed and discussed.\n\n**A**: For some of the failure cases in Bar et al, our IMProv could successfully address them with text prompts, e.g. the cat colorization and bottle segmentation of \u201cTask ambiguity\u201d. And our model could successfully generate the image that moves the orange to the center, though fails the other Non-aligned input-output example.\n\n**Q**: The authors use ViT-L be default. Did you tried backbone at different scales?\n\n**A**: We tried a larger network (ViT-H) as encoder, but there was no significant improvement. We suspect that scaling up the decoder instead of the encoder may help, but we did not have sufficient computing budget to try it. We plan to explore it as a future work.\n\n**Q**: If we want to further scale up, how can we get more data like Semantic Scholar Computer Vision dataset? It would be interesting to have more discussion.\n\n**A**: One potential way to scale up the dataset is by collecting instructional videos and incorporating them during the training process (e.g. videos of academic talks, oral presentations from conferences, and paper summary videos). We will add it to the discussion and explore this direction in future work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700016329850,
                "cdate": 1700016329850,
                "tmdate": 1700718449353,
                "mdate": 1700718449353,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5i7nqmElf4",
            "forum": "eJFt8ZRQ9a",
            "replyto": "eJFt8ZRQ9a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4300/Reviewer_wsst"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4300/Reviewer_wsst"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a visual in-context learning framework termed IMProv. Based on an inpainting pipeline, IMProv incorporates both visual examples and task descriptions to prompt the model for different vision tasks. To train IMProv, a large-scale image-text dataset containing paired figures and captions from computer vision papers is collected by the authors. Experiments on different tasks are conducted to indicate the validity of IMProv to perform in-context inference."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. Integrating text prompts into the visual in-context learning framework is interesting and the experimental results demonstrate its effectiveness.\n\n2. This paper proposes a large-scale image-text dataset, the Semantic Scholar Computer Vision dataset (S2CV), which is beneficial for the multimodal learning community."
                },
                "weaknesses": {
                    "value": "1. Obviously, there is a large difference between figure captions from papers and the prompts used at inference time. For example, the text prompt may not appropriately describe the task during training. In the paper, I do not see any discussions regarding this issue.\n\n2. In Table 3, methods are trained with different datasets, thus leading to an unfair comparison. The authors could report the performance of IMProv trained on CCVF.\n\n3. For the tasks of X-to-images and images-to-X, the evaluation is not rigorous. For example, LPIPS cannot assess the performance of semantic segmentation. The reported results do not support the claim that IMProv can well generalize to these standard computer vision tasks. Commonly used metrics for these tasks should be adopted for evaluation. In addition, the results of X-to-images look poor, where the generated images are blurry and lack details.\n\n4. For the ablation study, I have a few concerns.\n\n    a) Dataset ablation: This is not a valid ablation. IMProv trained with different datasets can indicate the benefits brought from larger datasets, not different methods trained with different datasets.\n\n    b) The authors claim that Figure 4 suggests a trade-off between visual and text prompts. However, the visual prompts are the same for a query in Figure 4.\n\n    c) In Figure 5, what are the specific settings for these variants? \u201cw/o text prompt\u201d means no text prompt during 1) training and inference or 2) just inference? I think only the former could showcase the importance of incorporating text prompts. \n\n5. There are no comparisons with other SOTA in-context learning methods such as Painter [1*] (only a small comparison for the task of colorization is given in the appendix which is far less than enough) and Prompt Diffusion [2*].\n\n    Overall, I think the paper presented an interesting idea for multimodal in-context learning. However, significant flaws in terms of experiments (see above points 2-5) make the claims of this paper very unconvincing.\n\n    [1*] Images speak in images: A generalist painter for in-context visual learning. In CVPR, 2023.\n\n    [2*] In-context learning unlocked for diffusion models. arXiv, 2023."
                },
                "questions": {
                    "value": "See weaknesses. Questions are embedded into weakness points."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4300/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698725184113,
            "cdate": 1698725184113,
            "tmdate": 1699636398015,
            "mdate": 1699636398015,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e5nHUS1lC1",
                "forum": "eJFt8ZRQ9a",
                "replyto": "5i7nqmElf4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4300/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4300/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thank you for the review and suggestions. We address your comments below.\n\n**Q**: Dicussion on the difference between figure captions from papers and the prompts used at inference time.\n\n**A**: Although the training and testing prompts are different, the model is able to generalize to the testing prompts by training on large-scale datasets. During the pre-training, the model learns to extract meaningful representation from the noisy data.  For example, CLIP\u2019s image encoder and text encoder are pre-trained on noisy data as well, but it can zero-shot transfer to different images and texts at inference time. And according to Figure 17, the captions of our datasets include terms like \u201csegmentation\u201d \u201csketch\u201d, \u201ccolorization\u201d etc. So the gap between training and inference is not large. \n\n**Q**: In Table 3, methods are trained with different datasets, thus leading to an unfair comparison. The authors could report the performance of IMProv trained on CCVF. \n\n**A**: In Table 4, we report the accuracy of MAE-VQGAN and IMProv trained on the same dataset. \nCCVF just extends CVF by extracting the captions of the figures. CCVF and CVF share the same set of images. MAE-VQGAN only uses images, and IMProv is trained on both texts and images. So row 1 and 2 in Table 4 is a fair comparison between MAE-VQGAN and IMProv, indicating that text prompt can improve the model performance.\n\n**Q**: For the tasks of X-to-images and images-to-X, the evaluation is not rigorous. For example, LPIPS cannot assess the performance of semantic segmentation. The reported results do not support the claim that IMProv can well generalize to these standard computer vision tasks. Commonly used metrics for these tasks should be adopted for evaluation. In addition, the results of X-to-images look poor, where the generated images are blurry and lack details.\n\n**A**: In [2*], Wang et al. only reported MSE for all the vision tasks. We follow them to use a single metric for all the tasks. The images from InstructPix2Pix are generated from Stable Diffusion, there is no ground-truth annotation for the depth/edge/segmentation. We will report both LPIPS and MSE in the updated version.\n\n**Q**: Dataset ablation: IMProv trained with different datasets can indicate the benefits brought from larger datasets, not different methods trained with different datasets.\n\n**A**: We report the results of IMProv(S2CV+LAION) in the below table under the same \u201cRandom Class\u201d setting. CCVF just extends CVF by extracting the captions of the figures. CCVF and CVF share the same set of images. MAE-VQGAN only uses images, and IMProv is trained on both texts and images. IMProv(CCVF) improves over MAE-VQGAN (CVF) by 2 points, indicates text prompt helps when visual prompt is randomly sampled from different classes. IMProv(S2CV + LAION) outperforms IMProv(CCVF + LAION) by 1.7 points, which justifies the effectiveness of our proposed S2CV dataset. \n\n\n| Model                | Avg.  |\n|----------------------|-------|\n| MAE-VQGAN (CVF)      | 23.52 |\n| IMProv(CCVF)         | 26.13 |\n| IMProv(CCVF + LAION) | 36.29 |\n| IMProv(S2CV + LAION) | 38.07 |\n\n\n**Q**: The authors claim that Figure 4 suggests a trade-off between visual and text prompts. However, the visual prompts are the same for a query in Figure 4.\n\n**A**: In Figure 4, we keep the query the same, and change the support from \u201cwithout support\u201d (first 2 columns) to \u201cwith support\u201d (last 4 columns). We provide more examples of the trade-off between visual and text prompts in Figure 9, with different class random sample prompts, same class random sample prompts, and nearest neighbor prompts.\n\n**Q**: In Figure 5, what are the specific settings for these variants? \u201cw/o text prompt\u201d means no text prompt during 1) training and inference or 2) just inference? I think only the former could showcase the importance of incorporating text prompts.\n\n**A**: We imply without text prompt (empty string) during inference. Following MUSE, we drop 10% of the text prompt randomly during training, so our model is able to generalize to \"w/o text prompt\" during inference.\n\n**Q**: There are no comparisons with other SOTA in-context learning methods such as Painter [1*] (only a small comparison for the task of colorization is given in the appendix which is far less than enough) and Prompt Diffusion [2*] \n\n**A**: We acknowledge that our approach performs worse than them on their benchmarks because they are trained in a fully supervised way with ground truth annotations. In contrast, our model is trained on noisy data from the paper. We plan to explore the combination of supervised data and unsupervised in future works."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700016326302,
                "cdate": 1700016326302,
                "tmdate": 1700016326302,
                "mdate": 1700016326302,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F5tgwx8cph",
                "forum": "eJFt8ZRQ9a",
                "replyto": "5i7nqmElf4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4300/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4300/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer wsst,\n\nThank you so much again for the detailed feedback. We're approaching the end of the author-reviewer discussion period. However, there are no responses yet to our rebuttal.\nPlease do not hesitate to let us know if there is any further information or clarification we can provide. We hope to deliver all the information in time before the deadline.\n\nThank you!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629689135,
                "cdate": 1700629689135,
                "tmdate": 1700629689135,
                "mdate": 1700629689135,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AbLWl4J5Gm",
                "forum": "eJFt8ZRQ9a",
                "replyto": "F5tgwx8cph",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4300/Reviewer_wsst"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4300/Reviewer_wsst"
                ],
                "content": {
                    "title": {
                        "value": "remaining questions"
                    },
                    "comment": {
                        "value": "Thanks for the response. I still have some concerns as follows:\n\n1. For the tasks of images-to-X, since there is no ground truth for the dataset created by PromptDiffusion, why not adopt standard benchmarks (e.g., NYUv2 Dataset for depth estimation)? Current results evaluated via LPIPS or MSE do not make much sense to me.\n\n2. For the tasks of X-to-images, at least FID is an available and suitable metric to leverage. In addition, the authors claimed that they followed PromptDiffusion to only report MSE for all the vision tasks, but no comparison between this work and PromptDiffusion was given.\n\nGiven the unconvincing results and insufficient comparisons with existing baselines, I am still inclined to reject the current version of this paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660021969,
                "cdate": 1700660021969,
                "tmdate": 1700660021969,
                "mdate": 1700660021969,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nGhXHqYx9B",
                "forum": "eJFt8ZRQ9a",
                "replyto": "5i7nqmElf4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4300/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4300/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply! We address your concerns as follows.\n\nQ1: Since it\u2019s not trivial to convert the RGB outputs to standard benchmark formats due to the time limit, we follow PromptDiffusion to report MSE and perception scores only. We will evaluate on standard benchmark in the future version. \n\nQ2: We report the comparison with PromptDiffusion in the table below.\n|                 | Depth -> Image |      | Image -> Depth |      | HED -> Image |      | Image -> HED |      | Seg -> Image |      | Image -> Seg |      | Normal -> Image |      | Image -> Normal |      |\n|-----------------|----------------|------|----------------|------|--------------|------|--------------|------|--------------|------|--------------|------|-----------------|------|-----------------|------|\n|                 | LPIPS          | MSE  | LPIPS          | MSE  | LPIPS        | MSE  | LPIPS        | MSE  | LPIPS        | MSE  | LPIPS        | MSE  | LPIPS           | MSE  | LPIPS           | MSE  |\n| IMProv          | 0.61           | 4.60 | 0.52           | 2.69 | 0.51         | 3.83 | 0.46         | 1.54 | 0.59         | 3.35 | 0.50         | 2.61 | 0.56            | 3.63 | 0.48            | 2.03 |\n| PromptDiffusion | 0.36           | 1.76 | 0.31           | 1.13 | 0.30         | 1.41 | 0.20         | 0.59 | 0.39         | 2.11 | 0.51         | 2.10 | 0.44            | 2.42 | 0.74            | 2.17 |\n\nAlthough IMProv underperforms PromptDiffusion on most tasks (except Image -> Normal\t), we would like to emphasize that PromptDiffusion explicitly uses the paired dataset to train in a fully supervised way, including depth, HED and segmentation annotations, while ours only uses noisy data from Semantic Scholar and LAION. As mentioned in the fail cases in the PromptDiffusion paper, their model fails to generalize to some unseen computer vision tasks, e.g. Image -> Normal, since there are no such training data pairs in their training set. However, even though our model is trained with noisy data from the web, it still generalizes to output normal maps. \nMoreover, PromptDiffusion initializes the weights from ControlNet (Stable Diffusion 1.5), which is already a strong model for image generation, while our IMProv is trained from scratch without annotated data."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711591442,
                "cdate": 1700711591442,
                "tmdate": 1700730900797,
                "mdate": 1700730900797,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kaHnPxkm34",
            "forum": "eJFt8ZRQ9a",
            "replyto": "eJFt8ZRQ9a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4300/Reviewer_W54V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4300/Reviewer_W54V"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a model for solving computer vision tasks using multi-modal prompts based on inpainting. In addition to visual prompts, this method also utilizes text features encoded by CLIP as prompts, inputting them into the visual transformer using cross-attention mechanisms. To train the model, the authors collected a new dataset called Semantic Scholar Computer Vision (S2CV) dataset, which includes structured textual descriptions for precise task delineation, enabling the model to better handle distribution shifts. This model expands the range of tasks, it can perform to include images-to-X and X-to-images tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The author's incorporation of text as an additional cue into the practice of visual in-context learning is insightful. \n2. Abundant experimental results demonstrate the effectiveness of the approach. The paper, through a comparison of IMProv trained on CCVF data and MAE-VQGAN, provides evidence that multi-modal prompts consisting of both text and images outperform purely visual prompts. Furthermore, the paper shows that IMProv trained on a mixed dataset of S2CV + LAION, which includes structured textual prompts, can further enhance the model's performance.\n3. The paper extends the model's application scope, enabling it to accomplish image-to-X and X-to-image tasks, contributing to the further advancement of In-context learning in computer vision."
                },
                "weaknesses": {
                    "value": "1. Using only LPIPS as a quantitative metric for extending applications in image-to-X and X-to-image tasks may not be particularly convincing. While there may be significant variations in metrics for image-to-X tasks, the use of standardized metrics like FID for X-to-image tasks would provide more solid experimental results.\n2. The choice of sample images in Figure 2 of the paper, which showcases the pipeline, does not seem to be well-suited and may not have a strong relevance to the tasks addressed in the paper."
                },
                "questions": {
                    "value": "1.Do large language models that exclusively handle text, such as T5, outperform models that encode text using CLIP? \n2.In Tables 4 and 7, the models trained with CCVF and LAION exhibit significantly better metrics compared to models trained solely with CCVF. Could the authors provide some analysis, possibly accompanied by visual results, to explain the reasons behind this substantial improvement?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4300/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698742010785,
            "cdate": 1698742010785,
            "tmdate": 1699636397932,
            "mdate": 1699636397932,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0uy8muuEGs",
                "forum": "eJFt8ZRQ9a",
                "replyto": "kaHnPxkm34",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4300/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4300/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thank you for the review and suggestions. We address your comments below.\n\n**Q**: Metrics other than LPIPS for image-to-X and X-to-image tasks.\n\n**A**: We generally agree, however, in [2*] we followed Wang et al. that used the images from InstructPix2Pix. These images are generated using Stable Diffusion, and there is no ground-truth annotation for the depth/edge/segmentation. Therefore, like Wang et al. we report MSE and LPIPS (see the table below).\n\n|                                              | Depth -> Image |      | Image -> Depth |      | HED -> Image |      | Image -> HED |      | Seg -> Image |      | Image -> Seg |      | Normal -> Image |      | Image -> Normal |      |\n|----------------------------------------------|----------------|------|----------------|------|--------------|------|--------------|------|--------------|------|--------------|------|-----------------|------|-----------------|------|\n|                                              | LPIPS          | MSE  | LPIPS          | MSE  | LPIPS        | MSE  | LPIPS        | MSE  | LPIPS        | MSE  | LPIPS        | MSE  | LPIPS           | MSE  | LPIPS           | MSE  |\n| Supervised ICL (InstructPix2Pix)             | 0.65           | 4.48 | 0.60           | 3.91 | 0.59         | 4.36 | 0.62         | 3.26 | 0.64         | 3.65 | 0.61         | 3.11 | 0.62            | 3.91 | 0.55            | 2.55 |\n|IMProv~(CCVF+LAION)                 | 0.61           | 4.60 | 0.52           | 2.69 | 0.51         | 3.83 | 0.46         | 1.54 | 0.59         | 3.35 | 0.50         | 2.61 | 0.56            | 3.63 | 0.48            | 2.03 |\n| IMProv~(CCVF+LAION+InstructPix2Pix) | 0.55           | 3.67 | 0.43           | 2.63 | 0.47         | 3.26 | 0.37         | 1.59 | 0.54         | 3.16 | 0.46         | 2.38 | 0.51            | 3.05 | 0.44            | 1.90 |\n\n\n**Q**: The choice of sample images in Figure 2 of the paper, which showcases the pipeline, does not seem to be well-suited and may not have a strong relevance to the tasks addressed in the paper.\n\n**A**: The sample image in Figure 2 is a random sample from our dataset, which comes from one of the computer vision papers, and represents the common image pattern of our dataset.\n\n**Q**: Do large language models that exclusively handle text, such as T5, outperform models that encode text using CLIP?\n\n**A**: We tried both CLIP and T5 text encoder. And we found the larger T5 text encoder doesn't improve the performance. Under \u201cRandom Class\u201d visual prompt setting, T5 vs CLIP is 33.26 vs 36.29 mIoU. We choose to use CLIP for all the other experiments to save computation costs. \n\n**Q**: In Tables 4 and 7, the models trained with CCVF and LAION exhibit significantly better metrics compared to models trained solely with CCVF. Could the authors provide some analysis, possibly accompanied by visual results, to explain the reasons behind this substantial improvement?\n\n**A**: As reported in the \"Dataset effect.\" section from Bar et al., pre-training on a larger and more diverse dataset helps the model to generalize better."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700016321623,
                "cdate": 1700016321623,
                "tmdate": 1700016321623,
                "mdate": 1700016321623,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "D7HrDS6dJ4",
            "forum": "eJFt8ZRQ9a",
            "replyto": "eJFt8ZRQ9a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4300/Reviewer_yR1X"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4300/Reviewer_yR1X"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors expand upon Vision prompting via image inpainting by introducing a new textual prompt modality. Their method, IMProv, is capable of conducting image-to-image translation tasks based on a textual task description and a few input-output visual examples. The authors introduce two datasets to support their approach. They demonstrate that incorporating a text prompt and utilizing larger datasets results in improved performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method is straightforward without unnecessary complexity. The model architecture closely resembles Bar et al. to minimize additional variables.\n- The authors conduct a quantitative analysis of various factors influencing the performance of this method, such as training data size and source (including more data and diverse standard data), different prompt designs, and more.\n- The proposed method offers flexibility as it can accept either a textual prompt, a visual prompt, or both."
                },
                "weaknesses": {
                    "value": "- This paper exhibits limited novelty since the proposed method is primarily a straightforward extension of Bar et al.'s work.\n- Qualitative analyses are relatively scarce within the paper.\n- It appears that there may be a trade-off between textual and visual modalities, with the advantages of a text prompt being less pronounced when paired with better visual prompts."
                },
                "questions": {
                    "value": "Questions:\n\n- The authors explore various visual prompts, such as switching from a random class to a nearest neighbor (NN) class. It raises the question of how performance would be affected if the visual prompt is incorrect, (for example, an example for a different task), and how this compares to using a textual prompt alone.\n- In the appendix, the authors compare their method to SD. However, it may not be a fair comparison. It would be more equitable if the authors also fine-tuned on CCVF and S2CV datasets to assess whether the SD model can correctly inpaint in those scenarios.\n- Bar et al. discussed the limitations of their methods, including task ambiguity, non-aligned input-output, and out-of-distribution decoding. It would be interesting to explore whether these issues can be mitigated by using a textual prompt (for addressing task ambiguity and non-aligned input-output) and by employing more data (for handling out-of-distribution decoding).\n- Have the authors considered how different text encoders might affect the model's performance?\n- The authors examine the quality of visual prompts versus textual prompts. Both the authors and Bar et al. have investigated how performance changes with the number of support pairs versus performance. It would be valuable to understand the trade-off between the number of support pairs and the use of textual prompts.\n- It appears that the authors conclude S2CV+LAION results in better performance from Table 3. Although I do think the conclusion is likely true, however, it is not rigorous to say so since IMProv benefits from both textual prompts and data. An interesting experiment could involve retraining MAE-VGGAN with CCVF+LAION data and S2CV+LAION data for a fair comparison.\n\nMinor:\n\n- The \u201ctextual prompts ablation\u201d section is not only about textual prompts but both textual and visual.\n- TIt's worth mentioning that the loss used is similar to the MRM (Masked Region Modeling) in UNITER. A citation could provide relevant context. Chen, Yen-Chun, et al. \"Uniter: Universal image-text representation learning.\"\u00a0*European conference on computer vision*. Cham: Springer International Publishing, 2020.\n\n\nFinal rating:\nI maintain my original rating. While the authors provide numerous qualitative examples, which is good, the paper still lacks both qualitative and quantitative analyses, as I pointed out in my questions. Strengthening these aspects would enhance the paper's overall quality and impact."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4300/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4300/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4300/Reviewer_yR1X"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4300/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699172795428,
            "cdate": 1699172795428,
            "tmdate": 1700798161392,
            "mdate": 1700798161392,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6rRCUoaCkk",
                "forum": "eJFt8ZRQ9a",
                "replyto": "D7HrDS6dJ4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4300/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4300/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thank you for the review and suggestions. We address your comments below.\n\n**Q**: This paper exhibits limited novelty since the proposed method is primarily a straightforward extension of Bar et al.'s work.\n\n**A**: In this work, our goal is to explore the tradeoff between text and image prompts based on Bar et al. work. Nevertheless, we show that text prompts can be used to improve the model performance, and we also collect a large dataset for multimodal in-context learning.\n\n**Q**: Qualitative analyses are relatively scarce within the paper.\n\n**A**: We provided extensive qualitative analysis in Figures 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 13, 14, both in the main paper and in the supplementary material. \n\n**Q**: It raises the question of how performance would be affected if the visual prompt is incorrect, and how this compares to using a textual prompt alone.\n\n**A**: There are no guarantees whether the visual or text prompt would affect the model more. In Figure 6, we provide an example of inconsistent visual and text prompts, and in this example, the model is more likely to follow the text prompt (generate a letter \"T\"). \n\n**Q**: In the appendix, the authors compare their method to SD. However, it may not be a fair comparison. It would be more equitable if the authors also fine-tuned on CCVF and S2CV datasets to assess whether the SD model can correctly inpaint in those scenarios.\n\n**A**: We agree that the off-shelf SD is not directly comparable with our method. Our motivation for including the failure of SD is to motivate the necessity to collect S2CV for large-scale multimodal in-context learning.\n\n**Q**: Whether the limitation of Bar et al. can be mitigated by using a textual prompt.\n\n**A**: We show the results of failure cases in Figure 16. For some of the failure cases in Bar et al, our IMProv could successfully address them with text prompts, e.g. the cat colorization and bottle segmentation of \u201cTask ambiguity\u201d. And our model could successfully generate the image that moves the orange to the center, though fails the other Non-aligned input-output example. \n\n**Q**: Have the authors considered how different text encoders might affect the model's performance?\n\n**A**: We tried both CLIP and T5 text encoder. And we found the larger T5 text encoder doesn't improve the performance. Under the \u201cRandom Class\u201d visual prompt setting, T5 vs CLIP is 33.26 vs 36.29 mIoU. We choose to use CLIP for all the other experiments to save computation costs. \n\n**Q**: The trade-off between the number of support pairs and the use of textual prompts.\n\n**A**: We plot the mIoU w.r.t the number of support in Figure 15. We run the experiments under grid 4x4 setting in Table 8, with \u201cRandom Class\u201d support image. Similar to Bar et al., as we increase the number of support visual examples, mIoU goes up.  It\u2019s worth noting that when there are only 1 or 2 examples, the text prompt doesn\u2019t help. It is because the model couldn't generate meaningful segmentation with a small number of visual supports due to the resolution (see Figure 13 for details). Under the setting that visual support number greater than 3, the text prompt consistently improves the results. It implies that our text prompt is orthogonal to the number of visual support pairs.  \n\n**Q**: Fair comparison for S2CV+LAION results.\n\n**A**: In Table 4, we report the accuracy of MAE-VQGAN and IMProv trained on the same dataset.\nCCVF just extends CVF by extracting the captions of the figures. CCVF and CVF share the same set of images. MAE-VQGAN only uses images, and IMProv is trained on both texts and images. So row 1 and 2 in Table 4 is a fair comparison between MAE-VQGAN and IMProv, indicating that text prompt can improve the model performance. To compare S2CV with CCVF, we additionally report the results of IMProv(S2CV+LAION) in the below table (Table 10 in supplementary material) under the same \u201cRandom Class\u201d setting. We report the average mIoU over 4 splits. IMProv(S2CV + LAION) outperforms IMProv(CCVF + LAION) by 1.7 points, which justifies the effectiveness of our proposed S2CV dataset.\n\n| Model                | Avg.  |\n|----------------------|-------|\n| IMProv(CCVF + LAION) | 36.29 |\n| IMProv(S2CV + LAION) | 38.07 |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700016318527,
                "cdate": 1700016318527,
                "tmdate": 1700016318527,
                "mdate": 1700016318527,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wrQ5Y7xFqW",
                "forum": "eJFt8ZRQ9a",
                "replyto": "D7HrDS6dJ4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4300/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4300/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer yR1X,\n\nThank you so much again for the detailed feedback. We're approaching the end of the author-reviewer discussion period. However, there are no responses yet to our rebuttal.\nPlease do not hesitate to let us know if there is any further information or clarification we can provide. We hope to deliver all the information in time before the deadline.\n\nThank you!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629698584,
                "cdate": 1700629698584,
                "tmdate": 1700629698584,
                "mdate": 1700629698584,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d6F1IbZqEh",
                "forum": "eJFt8ZRQ9a",
                "replyto": "kV0ibA2h1B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4300/Reviewer_yR1X"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4300/Reviewer_yR1X"
                ],
                "content": {
                    "title": {
                        "value": "Anonymity"
                    },
                    "comment": {
                        "value": "Hi, I think some person's name appears in your reply."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718304591,
                "cdate": 1700718304591,
                "tmdate": 1700718304591,
                "mdate": 1700718304591,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ETraMUDgVL",
                "forum": "eJFt8ZRQ9a",
                "replyto": "6rRCUoaCkk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4300/Reviewer_yR1X"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4300/Reviewer_yR1X"
                ],
                "content": {
                    "title": {
                        "value": "Response to author feedback"
                    },
                    "comment": {
                        "value": "I appreciate the authors' reply: failure cases in Bar et al; the tradeoff between visual prompts and text prompts. Here are my followups.\n\n1. Visual Prompts vs. Text Prompts\nI don't fully understand the explanation that \"the model couldn\u2019t generate meaningful segmentation with a small number of visual supports due to the resolution\". One would expect with a small number of visual supports, text prompts can compensate some, it appears that, counterintuitively, they may have a detrimental effect.\n2. Additional Analyses\nThanks the authors for their comprehensive examination of failure cases from Bar et al. However, I do want the authors to delve deeper into the unique and unforeseen properties of their model. Exploring these aspects could provide valuable insights.\n3. Incorrect visual prompts\nI appreciate the insights presented in Figure 6 and that is why I came up with the question. It would be beneficial if the authors could furnish additional quantitative results to shed light on the model's choosing between visual prompts and textual prompts."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719957700,
                "cdate": 1700719957700,
                "tmdate": 1700719957700,
                "mdate": 1700719957700,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]