[
    {
        "title": "On Adversarial Training without Perturbing all Examples"
    },
    {
        "review": {
            "id": "Sgv4JKyw7p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5502/Reviewer_uHfk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5502/Reviewer_uHfk"
            ],
            "forum": "pE6gWrASQm",
            "replyto": "pE6gWrASQm",
            "content": {
                "summary": {
                    "value": "In this paper, the authors study the generalization of adversarial robustness from class-wise and sample-wise aspects."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. There are two different settings, i.e., CSAT and ESAT. For each of them, there are comprehensive experiments based on the proposed entropy metrics. On the other hand, the authors consider both $L_2$ and $L_\\infty$ attacks. All settings are studied on multiple datasets, which makes the results more convincing.\n\n2. Downstream task transferability is an interesting topic. The results indicate that the learned features can be transferred to other classes, which is aligned with the observation from CSAT.\n\n3. This paper is easy to follow. The writing is clear."
                },
                "weaknesses": {
                    "value": "1. This paper mainly contains various experiments and their results, but lack the important analysis. Specifically, there are no analysis of the transferability observed from CSAT and ESAT. I cannot get any insightful information after reading this paper, although the results are informative.\n\n2. There is no theoretical analysis to rethink the observation as a special property of adversarial training. Additionally, from Figure 4, we can find that the clean accuracy has a similar tendency as the robust accuracy, therefore, it is possible that CSAT and ESAT are just because of the generalizability of the deep learning models.\n\n3. Only PGD-AT is considered. More advanced methods, like TRADES and AWP, should be evaluated under the same settings.\n\n4. For downstream task transferability, it is similar, but not exactly the same, as contrastive adversarial training. The authors should discuss the similarity and difference between these two methods in this case.\n\n\nMinor:\n\na. some figures' labels are blocked."
                },
                "questions": {
                    "value": "1. I notice that the authors use stronger data augmentation than usual. For example, when training on CIFAR-10, we usually only use random crop and flip. I hope the authors can provide ablation studies on these data augmentation methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5502/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5502/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5502/Reviewer_uHfk"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5502/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697369629090,
            "cdate": 1697369629090,
            "tmdate": 1700628705393,
            "mdate": 1700628705393,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GXqPdJsO3Q",
                "forum": "pE6gWrASQm",
                "replyto": "Sgv4JKyw7p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5502/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Question 1 to 3"
                    },
                    "comment": {
                        "value": "1. **no analysis:**\nWe agree with the reviewer, that an additional analysis regarding the 'why' would be of interest. At this point, we cannot offer one but refer the reviewer to their own statement that the \"*results are informative*\". We also believe that our results are informative and are likely to spur follow up work. We believe this is a strong reason to publish this work. \nWe restate the contribution of our paper: in contrast to related work on robustness and its transfer capabilities (as discussed in related work and our method section), we conduct a systematic analysis where we perturb only a fixed subset of training examples. This reveals a surprising phenomenon: adv. training a single class, transfers robustness to all other classes above non-trivial levels. Moreover, the hardest class provides the best transfer. As a consequence, we observe that just perturbing $50$% of training examples is sufficient to reach baseline AT performance and only $30$% of examples is sufficient in the task transfer setting.\n\n2. \n    a) **no theory:** While additional theory is useful, we think that a lack of it does not reduce the importance of our work. Our work describes a phenomenon of adversarial training --few attacked examples/classes transfer robustness surprisingly well-- comprehensively.\n    This is confirmed by the reviewer itself and also R3h75, RgZFF and RQzqJ.\n  \n    b) **effect is just side-effect of generalizability:** We thank the reviewer for a highly relevant question that is addressed by providing additional figures in the new paper revision reporting the defense rate -- robustness independent of clean accuracy. While the trend between clean and robust accuracy seems highly correlative, in section A.6 and figure 17 in the appendix, we show that the increase in robustness is not due to clean accuracy alone. By providing robustness on accurately classified examples only, we remove the influence of clean accuracy entirely. We can see that robustness transfer is best when SAT is performed using the hardest examples.\n\n3. **no TRADES and AWP evaluation:** We agree with the reviewer that integration of TRADES is especially interesting in our setting, since it provides a means to trade-off clean with robust accuracy. However, we highlight that selecting the best trade-off between clean and robust accuracy with TRADES does not alter the main findings of our paper: adv. training on few examples/classes transfers robustness surprisingly well. Moreover, adversarial weight perturbation would be interesting to investigate, yet we believe that it goes beyond the scope of this work. Regarding TRADES, we have added results for CIFAR-10 for CSAT $L_2 \\epsilon=0.5$ $|A|=1$ and the general case. For $|A|=1$ we conducted a coarse hyperparameter search for TRADES $\\beta$ parameter and found $\\beta=12.0$ to provide good robustness transfer (see table below). This configuration achieves $42.9$% $(+10.7)$ robust accuracy on $B$ while also increasing rob. acc. on $A$ to $78.2$% $(+28.6)$, with the expected decrease in overall clean accuracy ($81.0$% $(-10.2)$).\n\n|$A=\\{cat\\}$|$\\beta$|clean acc on CIFAR-10|rob. acc on $A$|rob. acc on $B$|\n|--|--|--|--|--|\n|w/o TRADES|N/A|91.0|49.6|37.8|\n|w/ TRADES|1.0|92.4|41.5|32.0|\n||6.0|85.9|70.2|42.1|\n||12.0|**81.0**|**78.2**|**42.9**|\n||24.0|81.6|80.1|41.8|\n\nIn a second experiment, we use linearly decreasing $\\beta$, from $12$ down to $6$, for our ESAT $L_2 \\epsilon=0.5$ experiment on CIFAR-10, corresponding to figure 6.\nThat is, when $|A|$ is small, we trade off some of the gained clean accuracy for increased robustness with large $\\beta=12$, but decrease to $\\beta=6$ with increasing size of $|A|$. We choose $\\beta=6.0$ for $|A|=9$ based on the same parameter choice for the full CIFAR-10 set in (Zhang et al. 2019). We compare *ESAT-hardest first* with and without TRADES in the table below.\nWe observe quicker robust accuracy convergence to the baseline ($66$% robust accuracy is achieved at $20k$ samples, instead of at $25k$) with an expected drop in clean accuracy below full AT baseline clean accuracy ($85.6$% clean accuracy at $20k$ vs $89.7$% at $25k$ vs $89.2$% for full AT).\n\n\n|TRADES||5k|10k|15k|20k|25k|30k|35k|40k|45k|full AT|\n|--|--|--|--|--|--|--|--|--|--|--|--|\n|yes|Rob. acc|57.2|62.1|65.7|66.1|67.7|67.6|68.2|66.4|69.0|69.0|\n||Clean acc|87.0|87.4|84.2|85.6|84.3|87.0|84.0|88.0|88.4|88.6|\n|no|Rob. acc|52.1|59.1|63.3|64.9|66.3|66.8|67.7|68.7|68.5|69.0|\n||Clean acc|91.9|91.2|90.5|89.7|89.4|89.4|89.2|89.4|89.5|89.2|"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700421330705,
                "cdate": 1700421330705,
                "tmdate": 1700422218795,
                "mdate": 1700422218795,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gj9234Yi5k",
                "forum": "pE6gWrASQm",
                "replyto": "Sgv4JKyw7p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5502/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Question 4-6"
                    },
                    "comment": {
                        "value": "4. **what is difference to contrastive AT?:** We base our discussion around (Kim et al.), in which the authors provide a self-supervised adversarial training method, which does not require explicit labels. We highlight, that our proposed analysis is complementary to this work. (Kim et al.) perturb *all* training examples, similar to the vanilla AT approach. Ours, instead, investigates the impact of robustness transfer when we perturb only a *fixed subset*. Our SAT analysis is therefore applicable to the proposed contrastive learning approach.\n\n    Kim, Minseon, Jihoon Tack, and Sung Ju Hwang. \"Adversarial self-supervised contrastive learning.\" Advances in Neural Information Processing Systems 33 (2020).\n\n5. **some figures' labels are blocked:** We thank the reviewer for raising this issue. We have fixed the blocked labels in all figures.\n\n6. **data augmentation different to related work:** We adopted the adversarial training methodology from the GitHub repository *github.com/MadryLab/robustness*. Consequently, we use the same data augmentation as they did. That is, on CIFAR-10, the data augmentation involves cropping, flipping, color jittering and rotating (for details see *github.com/MadryLab/robustness/blob/master/robustness/data_augmentation.py*).\nIrrespectively, we repeated our CSAT $|A|=1$ experiments with the reviewers suggested data augmentation pipeline: only performing cropping and flipping. Results, reported in the table below, show similar results to our reported in our submission.\nWe conclude that the data augmentation is not the reason for our observed robustness transfer.\n\n|data augmentation|class in $A$ | clean acc on CIFAR-10|rob. acc. on $A$|rob. acc. on $B$|\n|--|--|--|--|--|\n|crop,flip|plane|88.6|88.8|26.7|\n||car|89.2|95.4|15.4|\n||bird|84.1|80.6|34.4|\n||cat|91.0|50.4|39.9|\n||deer|84.6|85.5|34.5|\n||dog|91.5|61.2|35.0|\n||frog|86.8|90.2|28.3|\n||horse|88.2|90.2|27.6|\n||ship|89.0|91.2|24.0|\n||truck|91.9|87.0|20.3|\n||\n|crop,flip,jitter,rotate|plane|87.2|89.4|25.7|\n||car|89.8|93.5|17.1|\n||bird|84.4|80.4|34.3|\n||cat|91.0|49.6|37.8|\n||deer|85.4|84.0|34.2|\n||dog|91.5|60.9|34.9|\n||frog|85.7|92.7|28.4|\n||horse|88.1|88.6|27.3|\n||ship|89.0|91.8|24.5|\n||truck|87.6|92.3|21.9|"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700421408454,
                "cdate": 1700421408454,
                "tmdate": 1700421408454,
                "mdate": 1700421408454,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EH7oab8qbU",
                "forum": "pE6gWrASQm",
                "replyto": "Sgv4JKyw7p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5502/Reviewer_uHfk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5502/Reviewer_uHfk"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response of the authors. I have read other reviewers' comments and responses. There are some additional questions.\n\n1. The authors mention that the proposed method can reduce the time cost. Could the authors discuss related works, such as Free-AT[1]?\n\n2. I still think it is important to give a deep analysis of the experiment results. For example, what is the reason that the model can obtain more robustness from the 'cat' class? Is it just because images in 'cat' classes are harder to learn? And why model obtain robustness for cars from cat images? They are very important questions to figure out the reason why we do not need to perturb all classes.\n\n3. There is no theoretical analysis is a main flaw. For CIFAR-10, it is okay to study images in each class. And find the most efficient strategy. However, if there is no theoretical guarantee, it is difficult to design strategies for larger datasets, like ImageNet-1k. I hope the authors can prove they can efficiently find proper methods for ImageNet even without theoretical analysis.\n\n4. I think that the authors misunderstand the question of generalizability. I mean that as the perturbed data are selected with the metrics entropy during the training process, it is possible that the perturbed data for the next training epoch will not be chosen. Because the model has the generalizability to give a clean version of the AE. Therefore, during the whole training process, all data could be selected to be perturbed, but they are selected in different epochs. If true, all data are perturbed, instead of only part of the training data.\n\n[1] [Adversarial training for free!](https://arxiv.org/pdf/1904.12843.pdf)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458167289,
                "cdate": 1700458167289,
                "tmdate": 1700458167289,
                "mdate": 1700458167289,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wcNeQXhu4h",
                "forum": "pE6gWrASQm",
                "replyto": "3MaQ9iwPSV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5502/Reviewer_uHfk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5502/Reviewer_uHfk"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses. I am sorry for the misunderstanding of how to divide the training set. \n\n1. If I correctly understand the paper, the division requires a pre-trained model to calculate the entropy. Am I right? Is there any requirement for the pre-trained model? Will it be important to use the same model structure or optimizer?\n\n2. It seems that Section A.9 and Fig. 22 are about transferring to other datasets. Could you provide the results of the same datasets?\n\n3. Considering that there are some previous works discussing the adversarial properties of data in the dataset, such as [1], [2], [3], [4], [5], and [6], the finding that some data in the training set help the model improve the robustness and others do not is not very surprising. Therefore, I hope the authors could provide more analysis.\n\n4. Thanks for the examples of AlexNet and CLIP. They are very excellent works. My concern is about the practical usage of this work. Because without a theoretical guarantee, we have no idea how it will work on a real-world large dataset. And AlexNet and CLIP achieve very impressive results on large datasets. Therefore, could the authors provide results on a large dataset, like ImageNet-1k, to address my concern?\n\n[1] Huang L, Zhang C, Zhang H. Self-adaptive training: beyond empirical risk minimization[J]. Advances in neural information processing systems, 2020, 33: 19365-19376.\n[2] Wang Y, Zou D, Yi J, et al. Improving adversarial robustness requires revisiting misclassified examples[C]//International conference on learning representations. 2019.\n[3] Zhang J, Xu X, Han B, et al. Attacks which do not kill training make adversarial learning stronger[C]//International conference on machine learning. PMLR, 2020: 11278-11287.\n[4] Zhang J, Zhu J, Niu G, et al. Geometry-aware instance-reweighted adversarial training[J]. arXiv preprint arXiv:2010.01736, 2020.\n[5] Dong Y, Xu K, Yang X, et al. Exploring memorization in adversarial training[J]. arXiv preprint arXiv:2106.01606, 2021.\n[6] Ge Y, Li Y, Han K, et al. Advancing Example Exploitation Can Alleviate Critical Challenges in Adversarial Training[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 145-154."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558080012,
                "cdate": 1700558080012,
                "tmdate": 1700558080012,
                "mdate": 1700558080012,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RqVSdENt4O",
                "forum": "pE6gWrASQm",
                "replyto": "DbyYKm75ut",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5502/Reviewer_uHfk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5502/Reviewer_uHfk"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the updates and additional results.\n\n1. Previous works [1,2,3,4,5,6] discuss data in the training set to help the model learn clean accuracy or robust accuracy. Therefore, I think they are related to this paper. It will be better to build a closer connection with these works. Especially, discussing the similarity of the data split in CSAT and ESAT between previous methods during the training process will be helpful.\n\n2. ImageNet-1k contains more labels with similar semantic information. That is the reason that I would like to know how data in these classes will affect CSAT.\n\n3. Based on the additional results, I would like to give a 6 for this paper and discuss it with AC and other reviewers. Nonetheless, I believe this will be an 8, if the authors could provide more analysis and theoretical guarantees."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700628679438,
                "cdate": 1700628679438,
                "tmdate": 1700628679438,
                "mdate": 1700628679438,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fiD7KTfX4Q",
            "forum": "pE6gWrASQm",
            "replyto": "pE6gWrASQm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5502/Reviewer_3h75"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5502/Reviewer_3h75"
            ],
            "content": {
                "summary": {
                    "value": "To investigate the transferability of adversarial robustness across different classes and tasks, the authors proposed Subset Adversarial Training (SAT), which splits the training data into A and B and constructs adversarial examples (AEs) on A only.  Using SAT, this paper shows that training on AEs of just one class (e.g., cars in CIFAR-10) can transfer a certain level of robustness to other classes. Hard-to-classify classes (like cats) tend to provide greater robustness transfer compared to easier ones. Moreover, using AEs generated from half of the training data can match the performance of full AT. These findings also apply to downstream tasks. This paper distinguishes itself from others by only creating AEs on a pre-defined subset of the training set, independent of the model's architecture or the specifics of the training process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper provides valuable insights into the transfer properties of adversarial robustness. The observation that adversarial robustness can transfer to classes that have never been adversarially attacked during training is intriguing.\n2. The finding that generating AEs on merely 50% of the data can recover most of the baseline AT performance, especially on large datasets like ImageNet is insightful. This could potentially lead to significant computational savings without compromising the robustness of the model.\n3. The paper's findings are not limited to a single task or dataset. The authors have undertaken a thorough experimental evaluation across multiple datasets."
                },
                "weaknesses": {
                    "value": "1. While this paper presents intriguing empirical results on the SAT approach, it falls short of providing a clear explanation for the observed transferability of adversarial robustness from subset A to subset B."
                },
                "questions": {
                    "value": "1. Can you provide more theoretical justification or intuitive explanations for the observed efficacy of constructing AEs on only a subset of the training data? Specifically, what underpins the phenomenon where harder examples offer better robustness transfer?\n2. You mentioned that as dataset complexity increases, the trend of harder examples providing better robustness transfer diminishes. Can you explain the reasons behind this observation? Are there specific characteristics or properties of complex datasets that might be influencing this behavior?\n3. Could you explain more on why the robustness transfer notably increases for smaller $\\epsilon$ and decreases for larger $\\epsilon$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5502/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5502/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5502/Reviewer_3h75"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5502/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698733591175,
            "cdate": 1698733591175,
            "tmdate": 1699636562605,
            "mdate": 1699636562605,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pCtnduYkal",
                "forum": "pE6gWrASQm",
                "replyto": "fiD7KTfX4Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5502/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. **no (theoretical) explanation for the observed transferability:**\nWhile we agree with the reviewer, that additional theoretical or empirical based explanations would be beneficial, we believe that our paper has a valuable contribution to the scientific community. We highlight a surprising phenomenon and counter-intuitive results -- e.g. (i) CIFAR-10 SAT on *cat* provides better robust transfer to *car* than *truck* on *car* or (ii) $30$% of training examples achieve baseline robust accuracies in the task transfer setting -- that could lead to new research directions.\n\n    1. **theoretical justification for why harder examples transfer robustness better:** At this point, we cannot provide an underlying theory. We speculate however, that classifying hard examples requires the detection of more diverse features, which are trained adv. robust. Given the compositional nature of deep networks, it is plausible to assume that these robust features are reused by other classes.\n\n    2. **reasons behind diminishing returns when dataset complexity increases:** We conjecture this to be similar to our point made for the previous question. Datasets with a larger number of classes and larger input images likely require the detection of more diverse features.\n\n    3. **why does robustness transfer increase for smaller $\\epsilon$?:** We conjecture, that with increasing $\\epsilon$ each feature becomes more class-specific and is thus less reusable. In contrast, with decreasing $\\epsilon$ the generalizability is improved."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700420916982,
                "cdate": 1700420916982,
                "tmdate": 1700420916982,
                "mdate": 1700420916982,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hZVN89ZYmL",
                "forum": "pE6gWrASQm",
                "replyto": "pCtnduYkal",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5502/Reviewer_3h75"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5502/Reviewer_3h75"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer 3h75"
                    },
                    "comment": {
                        "value": "Thanks for the response of the authors. I have checked the updated version of the manuscript. After revision, the experiments become more solid. Besides, I agree with the authors that the major observations of this paper could lead to new research directions. In my opinion, the contributions of this paper are sufficient. Therefore, **I would recommend accepting this paper**. However, I am not fully convinced by the explanations provided by the authors as there is no theoretical guarantee. Overall, I will retain my current score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481290601,
                "cdate": 1700481290601,
                "tmdate": 1700481290601,
                "mdate": 1700481290601,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JE3vz5YBWE",
            "forum": "pE6gWrASQm",
            "replyto": "pE6gWrASQm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5502/Reviewer_gZFF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5502/Reviewer_gZFF"
            ],
            "content": {
                "summary": {
                    "value": "This paper conducts extensive experiments to see how only perturbing a subset of adversarial examples in training impacts adversarial robustness.  The authors find that using adversarial examples from certain classes can lead to nontrivial gains in robustness in other classes (despite not training with those classes).  The authors find that the most useful examples/classes to train with are correlated with their difficulty which they measure using entropy."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- paper is very clear\n- great scope in experiments which encompass multiple datasets and model architectures\n- experiments clearly demonstrate correlation between entropy and robust accuracy on the subset\n- experiments suggest that in certain settings, we can train with a smaller subset of adversarial examples instead of all adversarial examples which can reduce the runtime of adversarial training, making it more feasible in practice.  Robustness also transfers to other datasets as well suggesting that this approach can be used with pretraining models."
                },
                "weaknesses": {
                    "value": "- while it's clear that training with smaller subsets of adversarial examples can be beneficial, are there guidelines for how to determine the size of this subset to use SAT in practice?"
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5502/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794083529,
            "cdate": 1698794083529,
            "tmdate": 1699636562491,
            "mdate": 1699636562491,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QEjF3as8f7",
                "forum": "pE6gWrASQm",
                "replyto": "JE3vz5YBWE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5502/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the encouraging scores on our manuscript.\n\n1. **guidelines for selecting subset size in practice?:**\n    We would like to emphasize that our proposed SAT methodology provides means to analyze the robustness transfer across classes and examples, yet for practical purposes we would give the following guidelines: for downstream task transfer, we recommend using the hardest $30$% of training data and otherwise $50$%. If calculating the entropies during training is intractable for whatever reason, it should be sufficient to use randomly sampled data."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700420829878,
                "cdate": 1700420829878,
                "tmdate": 1700420829878,
                "mdate": 1700420829878,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oUvCbrL4Sr",
            "forum": "pE6gWrASQm",
            "replyto": "pE6gWrASQm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5502/Reviewer_QzqJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5502/Reviewer_QzqJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the impact of adversarial training with a limited subset of data on the robust accuracy of computer vision models. The empirical study shows that a limited amount (30%) of carefully selected data is sufficient to achieve 90% of the robustness of the models. Additionally, the paper explores the transferability to other models of the robustness acquired with their method. Empirically, the paper shows that model robustness is best preserved with a custom-balanced loss and with hard-to-classify examples."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper addresses the relevant problem of the generalization of the acquired robustness to unseen classes, examples, and tasks.\n- The paper is well-written and easy to grasp.\n- The experimental protocol is well described and allows to reproduce the study.\n- The paper provides an in-depth empirical study to support its claim.\n- The paper proposes a method to estimate the transferability of the acquired robustness which can be useful for testing ML models, as new classes and examples can appear after adversarial training in real-world systems."
                },
                "weaknesses": {
                    "value": "I have a single but potentially critical concern regarding the significance of this work. In particular, I am unsure what are the benefits of considering a subset of the adversarial examples during adversarial training to improve the efficiency of the process. \n\nAs I understand, the key objective of this paper is to limit the size of the set of examples used for adversarial training, to achieve a similar robust accuracy than with full adversarial training. The reason is that full adversarial training is computationally expensive. Considering the settings described in the experimental protocol, I fail to understand why the proposed process is more efficient than full adversarial training. In both cases, the full datasets need to be labeled and the same amount of computational resources is needed, as only the set of available examples for adversarial training differs, not the total number of examples used during adversarial training (since the subset of adversarial examples is completed with the clean examples not used for generating adversarial examples. One could argue that we save 70% of the time to generate the adversarial examples, but the largest cost still come from model training. Considering the empirical results that demonstrate that the proposed method does not lead to better robust accuracy than full adversarial training, I do not see the added value for model robustness efficiency/effectiveness. It would be good if the authors could clarify the benefits of using only a subset of examples in the adversarial training process."
                },
                "questions": {
                    "value": "- What is the exact process of adversarial training used in the experiments? Are all examples adversarially perturbed? Are some examples perturbed and others clean? In what proportion? In total is the number of perturbation executions the same for adversarial training as subset adversarial training? \n\n- What is the objective of transferring the robustness across classes and tasks? We need to retrain models to adapt to these new classes and tasks, isn\u2019t it simpler to adversarial train them at retraining, therefore empirically obtaining a more robust model ?\n\nOther comments:\n\n- On page 6,  lines 6 and 11, of the paragraph left to fig 3, describe the results for the class \u201ccar\u201d while the numerical value corresponds to the line \u201cplane\u201d on fig 2.\n- Figures are not readable when printed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5502/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5502/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5502/Reviewer_QzqJ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5502/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828849246,
            "cdate": 1698828849246,
            "tmdate": 1699636562384,
            "mdate": 1699636562384,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XyBR1wkK9C",
                "forum": "pE6gWrASQm",
                "replyto": "oUvCbrL4Sr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5502/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. **what are the benefits of considering a subset:?** \n    \n    The benefits of considering a subset are twofold: (a) practical, and (b) analytical. \n    \n    **Practical:** We observe significant savings in terms of training cost with our protocol. For example, training our ResNet-18 models on CIFAR-10 takes approx. 3h without any adversarial perturbations. Full adversarial training takes about 7h, and only perturbing 50\\% of the data results in a training time of 5h. Thus  reducing the cost of training considerably. These savings are expected to be even larger at scale, that is when training large models on large datasets. E.g. consider the power consumption of training foundational models, which consume hundreds of Mega-Watt-hours without adversarial training (e.g. nearly 500MWh for LLaMa-65B [LLama]). Adding adversarial training roughly increases the training time -- and thus the power consumption -- by a factor of PGD-steps. Our insights suggest that training time/consumption could be at least halved.\n\n\n    **Analytical:**\n    While it is true that \"the proposed method does not lead to better robust accuracy than full adversarial training\", we show that you can trade-off a minor loss in robust accuracy against significant savings in training time -- by not perturbing all classes/examples. However, we also believe that this loss in robust accuracy should motivate further research into more effective defenses against adversarial attacks: Why **do** we need to perturb all classes/examples to achieve the full potential of adversarial training? Can we train models to be robust in a class-agnostic manner? These are important questions raised by our work, which we believe to be of interest to the scientific community.\n    \n    [LLama] Touvron, Hugo, et al. \"Llama: Open and efficient foundation language models.\" arXiv preprint arXiv:2302.13971 (2023).\n\n2. **adversarial training protocol:** For vanilla adversarial training, we perturb all training examples. There is no mix between clean and perturbed examples in a batch as described in earlier work on adv. training. For SAT on the other hand, we randomly sample a training batch and perturb only those examples that are contained in $A$. We have updated the appendix, section A.1 in our manuscript to make this clear.\n\n3. **during task transfer, why not retrain with full adv. training?** Our argument is similar to question 1. We see an increase in foundational models, which provide strong off-the-shelf performance which can be improved via fine-tuning. Given the computational demand that is required to train such large models, it is often sufficient to only train a shallow classifier on top of intermediate outputs.\nIn this case, adversarial robustness can only be provided when the foundational model is robust.\nWe have shown that it is sufficient to perturb only $30\\%$ of examples during the initial (foundational) training to achieve high adversarial robustness after fine-tuning.\n\n4. **mismatched reporting on page 6:** Thank you for highlighting this issue. We have fixed it in the new revision. Note that, now, the numerical values are actually better than what was reported in our initial text. \n\n5. **unreadable figures:** Thank you for highlighting a readability issue. We will improve the readability for the final revision."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700420666267,
                "cdate": 1700420666267,
                "tmdate": 1700420703186,
                "mdate": 1700420703186,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8T6q8kPgdV",
                "forum": "pE6gWrASQm",
                "replyto": "XyBR1wkK9C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5502/Reviewer_QzqJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5502/Reviewer_QzqJ"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you for the detailed answer. The full and partial AT protocols are now clearer. With the described protocol, the empirical and theoretical description of the efficiency gain of the approach makes sense. Subset AT has the potential to drastically reduce adversarial training costs (both in classic and transfer learning settings).\n\nI share the concerns of reviewers gZFF and uHfk on the practical application of this method on larger, unknown datasets, while maintaining the time benefits. Indeed we do not know a priori the size of the subset on which to apply AT. I am not convinced that the simple guidelines (30% and 50%) given as an answer to reviewer gZFF will generalize.\n\nAdditionally, from the answer given to reviewer uHfk (20.11.2323 14:13) on fast AT methods, there are - potentially more effective - methods that have been proposed for the same problem but the paper does not compare with them.\n\nI will continue following the discussion on these matters."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721626125,
                "cdate": 1700721626125,
                "tmdate": 1700721626125,
                "mdate": 1700721626125,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EfLBEsIRsI",
                "forum": "pE6gWrASQm",
                "replyto": "oUvCbrL4Sr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5502/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5502/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, thank you for the response.\n\nWe would like to emphasize that our paper should not **exclusively** be read as providing \u201cguidelines\u201c for more efficient adversarial training. As described earlier in this thread, there is also an analytical component to our paper centered on the question: Do we need to adversarially perturb images from every class to attain good robustness, and what are the implications? \n\nThere is a \u201cglass half full\u201d answer that we back up with our experiments, namely that one can get away with a minor loss of robustness by **not** perturbing examples from every class. This has positive practical implications that we have discussed at length.\n\nThere is however also an interesting \u201cglass half empty\u201d answer, namely that to achieve the full potential of adversarial training in terms of robustness, it would seem that one **indeed** has to perturb representative images from every class, but also that the choice of classes is inferior when random. This has all kinds of interesting implications and suggests further questions and directions for research, e.g. to name a few: While we provide some measures that correlate with robustness transfer, what really are the underlying mechanisms? What theoretical tools are needed to distinguish between individual object and scene classes in terms of their visual characteristics? What does this say about adversarial training as a general framework if robustness ultimately has to be learned for every class? Can we be satisfied with visual recognition methods that must gain robustness in this manner? Is this scalable?\n\nNote: Regarding the fast adversarial methods (esp Wong et al 2020) our initial experiments show that these may well be complementary but we acknowledge that further investigation is warranted."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5502/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740760818,
                "cdate": 1700740760818,
                "tmdate": 1700741761059,
                "mdate": 1700741761059,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]