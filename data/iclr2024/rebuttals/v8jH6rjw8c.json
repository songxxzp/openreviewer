[
    {
        "title": "Fairness Improves Learning from Noisily Labeled Long-Tailed Data"
    },
    {
        "review": {
            "id": "t26QZaZLst",
            "forum": "v8jH6rjw8c",
            "replyto": "v8jH6rjw8c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4841/Reviewer_Ed4g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4841/Reviewer_Ed4g"
            ],
            "content": {
                "summary": {
                    "value": "The paper assesses whether fairness methods can improve learning on noisy imbalanced data. A regularization approach is taken to promote similar losses across sub-populations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The claim is interesting and well-motivated: that fairness improves learning on noisy long-tail sub-populations.\n- Extensive experiments are conducted."
                },
                "weaknesses": {
                    "value": "- Table 3: empirical improvements seem marginal and lack confidence intervals or pairwise statistical significance tests.\n  - I'm not too convinced that, e.g., changes from 74.46 to 74.48-74.50 are significant;\n  - I suggest bolding only cells whose difference to the baseline are statistical significant, as opposed to a simple \"larger than\" test.\n  - As an example, you can get confidence intervals using bootstrapping on the test set, without retraining; or running k-fold cross-validation to obtain a measure of dispersion of results.\n\n- It's not clear whether access to group membership is useful given that the work is not targeting fairness improvements.\n  To target the noisy long-tail sub-populations couldn't we just up-weight samples with higher loss and down-weight samples\n  with lower loss? (an approach that is also quite common in the fairness literature)\n  - What is the intuition behind the use of group membership supposedly leading to better performance?\n\n- Fig. 6: there seems to be a lot of noise on the effect of FR across sub-populations;\n  the paper is missing a more in depth analysis of the mechanism by which overall performance is increased.\n  - Intuitively, it should improve performance on populations with lower representation in the dataset (tail populations);\n    one example plot would show performance change on the y-axis (acc. increase or decrease after introducing FR), and sub-population size on the x-axis.\n\n- Using a regularizer on the performance differences among sub-populations is not particularly novel.\n  - The novelty stems from the use of a standard fairness tool to improve learning performance\n    (regardless of fairness); but the fact that this is a standard fairness tool (and that FR is not\n    the focus of the paper in any way) should really be made more clear in the paper.\n  - Perhaps, if the goal is to show that standard fairness tools can have a positive impact on \"noisy\n    long-tailed sub-populations\", it'd make more sense to use unaltered fair ML methods (that aim to\n    bring sub-population performance closer) and show that the paper claim on improved learning holds.\n  - Essentially, most fair ML methods reduce to the same thing as FR: up-weighing samples whose loss\n    is larger than the average loss, or samples whose group has a larger loss than average."
                },
                "questions": {
                    "value": "- Fig. 3: does the index have any meaning? Should this be sorted by frequency?\n- Fig. 4: please use a more adequate font size, it's completely unreadable when printed.\n- Instead of holding all $\\lambda_i$ constant it would be interesting to use a standard dual ascent framework to jointly optimize the Lagrange multipliers.\n  - \"Intuitively, applying dual ascent is likely to result in a large $\\lambda_i$ on the worst sub-population, inducing possible negative effects\"\n  - It'd be interesting to back this up with actual results.\n- Eq. 3: given that the constraint that is actually used in practice is different from the definition of $\\text{Dist}_i$ given in Section 4.1., I'd suggest giving a different name to each of these constraints."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4841/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772609187,
            "cdate": 1698772609187,
            "tmdate": 1699636467744,
            "mdate": 1699636467744,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lh5F6Qvtjj",
                "forum": "v8jH6rjw8c",
                "replyto": "t26QZaZLst",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4841/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4841/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Ed4g (Part 1)"
                    },
                    "comment": {
                        "value": "Thanks for your time for reviewing our work! Please refer to our following responses regarding your insightful suggestions!\n\n```Weaknesses 1:```\n\n[Table 3: empirical improvements seem marginal and lack confidence intervals or pairwise statistical significance tests.]\n\n```Response:``` \n\n**Why there is no statistical test in Table 3** \n\nOur main results are summarized in Table 1, whose statistical tests are given in Table 2. Only Table 3 is not provided with hypothesis testing, since our aim for giving Table 3 is the ablation study \u2013 we want to show $\\lambda>0$ generally improves learning from the noisily labeled long-tailed data. What is more, Clothing 1M is not a standard benchmark like MNIST/CIFAR and it has a single realization.\n\n**New highlight criterion**\n\nWe recognize the need for a more robust method to highlight significant improvements. In light of your suggestion, we plan to revise our highlighting criteria. In the revised manuscript, we will only highlight cells in Table 3 where the increase in test accuracy is greater than 0.1. We believe this threshold represents a substantial difference, particularly in large-scale datasets such as ImageNet and Clothing-1M. This change aims to provide a clearer and more meaningful distinction of the results that are statistically significant.\n\n```Weaknesses 2:```\n\n[It's not clear whether access to group membership is useful given that the work is not targeting fairness improvements. To target the noisy long-tail sub-populations couldn't we just up-weight samples with higher loss and down-weight samples with lower loss? (an approach that is also quite common in the fairness literature). What is the intuition behind the use of group membership supposedly leading to better performance?]\n\n```Response:```\n\n**Addressing Sample-wise Reweighting** In our previous research, we have observed that in scenarios involving long-tailed data with label noise, samples with small losses often correlate with major populations and clean labels. Conversely, samples with large losses could either belong to minor populations or be incorrectly labeled. Therefore, simply up-weighting high-loss samples could inadvertently amplify the influence of mislabeled data, leading to skewed model predictions. This insight influenced our decision to not solely rely on sample-wise reweighting.\n\n**Explaining the Absence of Class-level Consideration** \n\nWe also steered clear of class-level reweighting, primarily due to the noisy nature of class labels in our dataset. Given this uncertainty in labeling, class-level adjustments could potentially introduce additional biases.\n\n**Justifying Sub-population Level Approach** \n\nOur focus on sub-population levels stems from their likelihood to be feature-dependent, circumventing the need to evaluate class or sample-wise performance directly. We aimed to illustrate this concept through Figure 1, which demonstrates how head sub-populations generally benefit from existing methods, while tail sub-populations could see improvements with the implementation of a fairness regularizer.\n\n**Clarifying the Novelty of Our Approach**\n\n Unlike the traditional fairness-accuracy tradeoff in resource-constrained settings, our findings, as presented in Figure 2 of the main paper, suggest that current methodologies for handling long-tailed noisy data fail to achieve Pareto optimality. This results in significant performance discrepancies across different classes/populations, hinting at an unfair distribution of model performance. We hypothesize that robust solutions might disproportionately affect sub-populations. Section 3 of our paper delves into this hypothesis by examining the influence of sub-populations on test data, revealing distinct impacts on model performance when dealing with noisily labeled long-tail data.\n\n**Presenting the Fairness Regularizer** \n\nMotivated by these observations, Section 4 introduces our Fairness Regularizer (FR). This novel approach is designed to prevent overemphasis on specific sub-populations and to minimize performance gaps across different groups. This is particularly crucial in the context of long-tailed, noisily labeled data, where certain populations might be significantly overlooked without fairness constraints. Our empirical studies (Section 5) and theoretical analysis (Appendix A) support the assertion that implementing FR can enhance the performance of tail sub-populations with minimal impact on head sub-populations.\n\n**Theoretical Underpinnings in Appendix A** \n\nIn Appendix A, we provide a theoretical exploration of the effects of sub-populations in learning with long-tailed noisy data, using a binary Gaussian example. Our theorem suggests that by enforcing fair performance across specific sub-populations (i.e., equal training error probabilities), a classifier trained on noisy data with a fairness regularizer can approximate the optimal classifier for the clean data distribution."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4841/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546432407,
                "cdate": 1700546432407,
                "tmdate": 1700546432407,
                "mdate": 1700546432407,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oJyeRpDdRy",
                "forum": "v8jH6rjw8c",
                "replyto": "9vtyn7XgJT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4841/Reviewer_Ed4g"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4841/Reviewer_Ed4g"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response.\n\nI think the paper's motivation is strong, but the experiments section should be significantly improved to support the main claim.\n\nIn my opinion it should position itself as an empirical paper that tests/shows that fairness-aware learning improves learning on noisily labelled long tailed data. Any focus given to the fairness regularizer distracts from the main point, as the FR is not novel. I also agree with W1 of Reviewer yYZa."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4841/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732934772,
                "cdate": 1700732934772,
                "tmdate": 1700732934772,
                "mdate": 1700732934772,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WjqJ37IpzY",
            "forum": "v8jH6rjw8c",
            "replyto": "v8jH6rjw8c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4841/Reviewer_MjTZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4841/Reviewer_MjTZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores the problem of learning from noisy labeled and long-tailed data. The paper starts by observing that existing methods to improve learning from noisy labeled or long-tailed data create accuracy disparities across different sub-populations in the dataset. The paper proposes a method called Fairness Regularizer (FR) that incentivizes the learned classifier to have uniform performance across subpopulations, aiming to mitigate accuracy disparities and improve learning. Experiments indicate that the proposed method improves the model performance in underrepresented populations in the dataset while increasing overall model performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "\u2022\tThe idea of connecting fairness with robust learning is exciting and could lead to interesting results.   \n\n\u2022\tThe discovery in Figure 2 of the disparate impact in underrepresented groups when using methods to learn from data with label noise or long-tail is interesting and should be communicated to the community.   \n\n\u2022\tThe conclusion that methods that incentivize statistical parity led to better model performance under noisy labels and long-tail data is very nice. The results contradict the folklore that fairness improvement methods *always* decrease the model performance.  \n\n\u2022\tThe results demonstrated in the paper had strong statistical guarantees, as shown in Table 2."
                },
                "weaknesses": {
                    "value": "\u2022\tThe proposed method, Fairness Regularizer, is similar to the reductions approach for statistical parity [1]. But I didn\u2019t find a citation for the paper.  \n\n\u2022\tThere are multiple existing in-processing methods to ensure fairness (statistical parity) across groups in the population check [2] Table 1 for multiple references. The authors should, ideally, compare their results with other fairness improvement methods. At least, the paper should mention other fairness improvement methods and discuss the difference between the proposed and existing approaches.  \n\n\u2022\tI believe the points in the lower-left corner of Figure 2 and 6 are the underrepresented groups. However, Figure 6 and Figure 2 could provide a more explicit message to the reader by adding one more axis of information (e.g., the intensity of the color of the points) with the group representation."
                },
                "questions": {
                    "value": "\u2022\tCould the authors please highlight the main differences between FR and the reductions approach [1]?\n\n\u2022\tWhy is the proposed method preferred compared to other fairness (statistical parity) improvement methods?\n\n\n\n[1] Agarwal et al. A Reductions Approach for Fair Classification. 2018.\n\n[2] Lowy et al.  A Stochastic Optimization Framework for Fair Risk Minimization. 2022"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4841/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4841/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4841/Reviewer_MjTZ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4841/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789585699,
            "cdate": 1698789585699,
            "tmdate": 1699636467658,
            "mdate": 1699636467658,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cUE1mSDBby",
                "forum": "v8jH6rjw8c",
                "replyto": "WjqJ37IpzY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4841/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4841/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MjTZ (Part 1)"
                    },
                    "comment": {
                        "value": "Thanks for your time for reviewing our work! Please refer to our following responses regarding your insightful suggestions!\n\n```Weaknesses 1:```\n\n[The proposed method, Fairness Regularizer, is similar to the reductions approach for statistical parity [1]. But I didn\u2019t find a citation for the paper.]\n\n```Response:```\n \nThanks for the great suggestions. In our revision, we will explicitly distinguish between our proposed Fairness Regularizer (FR) and the reductions approach for statistical parity, as outlined in Agarwal et al. (2018).\n\nTo briefly clarify, while both methods share a common goal of promoting fairness, they differ significantly in their implementation and outcomes. Traditional reduction approaches, including the one mentioned in your reference, predominantly focus on achieving statistical equality across different groups. This is often achieved by modifying the training process, specifically by addressing the Lagrangian:\n\n$$\\mathbb{E}\\_{(X,  \\widetilde{Y})\\sim \\widetilde{\\mathcal{D}}}[\\ell(f(X), \\widetilde{Y})] +\\sum\\_{i=1}\\^{N}\\lambda\\_{i} (\\text{Dist}\\_{i}-\\delta),$$\n\nwhich aims to offer an alternative solution to the original optimization problem presented in Equation (2):\n\n$$\\min\\_{f:  X\\rightarrow [K]} \\mathbf{E}\\_{(X, \\widetilde{Y})\\sim \\widetilde{\\mathcal{D}}} [\\ell(f(X), \\widetilde{Y})], \\qquad s.t. \\qquad \\text{Dist}\\_{i}\\leq \\delta, \\forall i \\in [N], \\text{[Eqn. (2)]} $$ \n\nIn contrast, our FR approach, while superficially similar, diverges in its core methodology. Specifically, it entails optimizing the Lagrangian in Equation (2) with uniform $\\lambda_i=\\lambda$ and $\\delta=0$. This distinction is crucial as it results in different optimal solutions. Our approach, aimed at enhancing overall performance on clean and balanced test data, strategically avoids overemphasis on the worst-performing sub-populations. Unlike reduction approaches that strive for strict fairness across all sub-populations, our method prioritizes a more balanced improvement across the board.\n\nWe will ensure to elaborate on this point more clearly in our revised manuscript, including appropriate citations to Agarwal et al. (2018) to acknowledge their foundational work in this area. Thank you once again for your valuable feedback, and we look forward to incorporating these clarifications in our revision.\n\n[1] Agarwal et al. A Reductions Approach for Fair Classification. 2018.\n\n```Weaknesses 2:```\n\n[There are multiple existing in-processing methods to ensure fairness (statistical parity) across groups in the population check [2] Table 1 for multiple references. The authors should, ideally, compare their results with other fairness improvement methods. At least, the paper should mention other fairness improvement methods and discuss the difference between the proposed and existing approaches.]\n\n```Response:```\n\nThank you for your valuable feedback. We appreciate your insights and agree that a comparison with existing fairness improvement methods would enrich our paper. Please allow us to clarify our approach in light of your comments.\n\n**Clarification on Our Focus:** Our primary objective is not solely to achieve fairness, but to demonstrate how fairness measures can enhance learning with noisy long-tail data, thereby improving the model's overall performance while reducing disparities across groups. Our method draws inspiration from popular in-processing fairness solutions, particularly those involving fairness definitions and constraints. However, our aim is distinct: we focus on enhancing overall performance rather than solely achieving the highest level of fairness.\n\n**Discussion on Methodological Differences:** We recognize the importance of situating our work within the broader context of fairness in machine learning. To this end, we will include a detailed discussion of various fairness improvement methodologies in our revised manuscript. Here is a brief overview of the additional related works we intend to incorporate:\n\n[More in Part 2 responses~]"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4841/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546009633,
                "cdate": 1700546009633,
                "tmdate": 1700546009633,
                "mdate": 1700546009633,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rwcYnuHQaf",
                "forum": "v8jH6rjw8c",
                "replyto": "WjqJ37IpzY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4841/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4841/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MjTZ (Part 2)"
                    },
                    "comment": {
                        "value": "We continue our response as below:\n\nHere is a brief overview of the additional related works we intend to incorporate:\n\n* Pre-Processing Techniques: These methods, like reweighing [1] and optimized pre-processing [2], aim to reduce bias in training data by adjusting training instance weights and modifying feature values for balanced representation.\n\n* Post-Processing Techniques: Techniques such as those proposed by Hardt et al. (2016) [5] adjust model predictions after training to achieve fairness objectives, often focusing on equal error rates across groups.\n\n* In-Processing Techniques: Central to our research, these strategies involve modifying the learning algorithm itself. Examples include fairness constraints within the optimization objective [3] and adversarial debiasing [4].\n\n* Our proposed Fairness Regularizer aligns with in-processing methods but is unique in its pursuit of optimal performance on clean and balanced test data. It aims for a balance in performance across sub-populations, diverging from reductionist approaches like Agarwal et al. (2018) [6] by prioritizing overall efficacy alongside fairness.\nWe hope this additional context addresses your concerns. We are also open to suggestions for any important related works in fairness machine learning that we may have overlooked.\n\n\n\nReferences\n\n[1] Kamiran, F., & Calders, T. (2012). Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems, 33(1), 1-33.\n\n[2] Calmon, F., Wei, D., Ramamurthy, K. N., Varshney, K. R., & Rosa, A. (2017). Optimized pre-processing for discrimination prevention. Advances in Neural Information Processing Systems, 30.\n\n[3] Zafar, M. B., Valera, I., Gomez Rodriguez, M., & Gummadi, K. P. (2017). Fairness constraints: Mechanisms for fair classification. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS).\n\n[4] Zhang, B., Lemoine, B., & Mitchell, M. (2018). Mitigating unwanted biases with adversarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society.\n\n[5] Hardt, M., Price, E., & Srebro, N. (2016). Equality of opportunity in supervised learning. Advances in Neural Information Processing Systems, 29.\n\n[6] Agarwal, A., Beygelzimer, A., Dud\u00edk, M., Langford, J., & Wallach, H. (2018). A reductions approach to fair classification. In International Conference on Machine Learning (ICML).\n\n\n```Weaknesses 3:```\n\n[I believe the points in the lower-left corner of Figure 2 and 6 are the underrepresented groups. However, Figure 6 and Figure 2 could provide a more explicit message to the reader by adding one more axis of information (e.g., the intensity of the color of the points) with the group representation.]\n\n```Response:```\n\nThanks for this insightful suggestion. We will revise the figure by adding one more axis of information with the group representation \n\n```Question 1:```\n\n[Could the authors please highlight the main differences between FR and the reductions approach [1]?]\n\n```Response:```\n\nPlease refer to our response to weakness 1.\n\n```Question 2:```\n\n[Why is the proposed method preferred compared to other fairness (statistical parity) improvement methods?]\n\n[1] Agarwal et al. A Reductions Approach for Fair Classification. 2018.\n[2] Lowy et al. A Stochastic Optimization Framework for Fair Risk Minimization. 2022\n\n```Response:```\n\nThank you for raising this insightful question. While the solutions proposed by Agarwal et al. (2018) and Lowy et al. (2022) are indeed efficient in targeting fair risk minimization, our approach diverges in its primary objectives. Our research is not focused on achieving fairness per se, but on demonstrating how fairness measures can be instrumental in learning from noisy, long-tail data. The goal is to enhance the model's overall performance, which incidentally also helps in reducing disparities across groups.\n\nFurthermore, our methodology draws inspiration from one of the most recognized in-processing fairness solutions\u2014implementing constraints based on fairness definitions. However, unlike these solutions, our emphasis is not solely on attaining the highest level of fairness but on leveraging these fairness measures as a means to improve overall model performance. This subtle but significant shift in focus distinguishes our method from the ones mentioned and aligns it more closely with our research objectives.\n\nWe appreciate your interest in our approach and hope this clarification helps in understanding the distinct positioning of our method in the landscape of fairness improvement strategies."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4841/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546187105,
                "cdate": 1700546187105,
                "tmdate": 1700549335432,
                "mdate": 1700549335432,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ipVsh66JAQ",
                "forum": "v8jH6rjw8c",
                "replyto": "rwcYnuHQaf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4841/Reviewer_MjTZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4841/Reviewer_MjTZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you, authors, for your response. \n\nGiven its claim, this paper could significantly impact the community. However, it still needs to be substantially improved to support its claims. \nFor this reason, I will maintain the scores I provided."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4841/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676926507,
                "cdate": 1700676926507,
                "tmdate": 1700676926507,
                "mdate": 1700676926507,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Mi71WDl1ML",
            "forum": "v8jH6rjw8c",
            "replyto": "v8jH6rjw8c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4841/Reviewer_ziso"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4841/Reviewer_ziso"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers a challenging learning problem under the existence of long-tailed and noisily labeled data. The paper makes efforts to understand and mitigate the heterogeneous effects of label noise for the imbalaenced data. The paper empirically examines the role of each sub-population and its influence on the test data. It proposes a fairness regularizer to encourage the learned classifier to achieve fair performance across different sub populations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is generally well written. The structure is clear and the motivation is strong. The Fairness Regularizer (FR) is easy to implement. The numerical results are relatively complete. Some theoretical insights are also included in the appendices."
                },
                "weaknesses": {
                    "value": "The paper overally has no big weakness.\n\nMinor suggestions: \n\n(i) How to tune $\\lambda_i$'s? Is there any recommendation or theory to support this?\n(ii) Some theory presented in the appendix can be moved into main content.\n(iii) Can author make some comments on distribution shift problem? If some sub-population in the test data does not appear in training data, how to guarantee its performance?"
                },
                "questions": {
                    "value": "See weakness points."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4841/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698918057362,
            "cdate": 1698918057362,
            "tmdate": 1699636467562,
            "mdate": 1699636467562,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sBq14R2WAt",
                "forum": "v8jH6rjw8c",
                "replyto": "Mi71WDl1ML",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4841/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4841/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ziso"
                    },
                    "comment": {
                        "value": "Thanks for your time for reviewing our work! Please refer to our following responses regarding your insightful suggestions!\n\n```Weaknesses 1:``` \n\n[How to tune lambda ? Is there any recommendation or theory to support this?]\n\n```Response:```\n\nThank you for your insightful query regarding the tuning of $\\lambda$. We appreciate the opportunity to clarify this aspect of our study.\n\nIn our research, we did not focus on providing a theoretical basis for $\\lambda$ tuning, primarily because our approach does not necessitate strict fairness across sub-populations. In our experimental framework, we opted for a straightforward approach by selecting constant $\\lambda$ values. This decision was driven by practical observations, where we found that using a constant $\\lambda$ generally yields a simple yet effective means to manage the performance disparity.\n\nFor a more comprehensive understanding, we included an ablation study of $\\lambda$ in Table 3, utilizing the Clothing 1M dataset. In this study, we explored various $\\lambda$ values, specifically chosen from the set {0.0,0.1,0.2,0.4,0.6,0.8,1.0,2.0}. It\u2019s important to note that a $\\lambda$ value of 0.0 corresponds to the baseline methods' training without the application of our Fairness Regularizer (FR).\n\nOur findings suggest that the default setting for FR ($\\lambda=1.0$) consistently achieves competitive performance compared to other $\\lambda$ values. This observation supports our recommendation to adopt $\\lambda=1.0$ as a practical default in similar applications. Additionally, our results indicate that $\\lambda$ values close to 1.0 generally outperform those nearer to 0.0. This trend not only underscores the effectiveness of our proposed fairness regularizer but also suggests a relative insensitivity of the model to hyper-parameter variations near the chosen $\\lambda$ value.\n\n```Suggestion 1:```\n\n[Some theory presented in the appendix can be moved into main content]\n\n```Response:```\n\nThanks for the suggestion. In our revision, we will consider summarizing main insights of our theoretical results in the appendix into the main content!\n\n```Question 1:``` \n\n[Can the author make some comments on the distribution shift problem? If some sub-population in the test data does not appear in training data, how to guarantee its performance?]\n\n```Response:```\n\nThis is a great question! When certain subpopulations $G_i$ does not appear in the training data, to improve the performance of $G_i$ at test time, it makes sense to assume that we have a held-out validation set for model selection/calibration. In that case, we could perform some post-hoc treatments on model weights scaling at test time, such weights could be inspired by referring to a validation set. Meanwhile, we tend to not intervene in the training process (except for FR) for the sake of fitting well on the existing sub-populations."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4841/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545579183,
                "cdate": 1700545579183,
                "tmdate": 1700545579183,
                "mdate": 1700545579183,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9FgMiGRo0P",
            "forum": "v8jH6rjw8c",
            "replyto": "v8jH6rjw8c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4841/Reviewer_yYZa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4841/Reviewer_yYZa"
            ],
            "content": {
                "summary": {
                    "value": "This paper is trying to address an interesting problem, that is, how to eliminate long-tailed effect and noisy labels simultaneously. Previous approaches have often tackled these issues independently and have not considered the combined effects of both. This paper empirically demonstrates that such isolated solutions are ineffective when dealing with long-tailed datasets that also have label noise. Furthermore, existing methods do not consistently improve accuracy across all sub-populations, leading to uneven performance improvements. In response to these observations, the paper introduces the Fairness Regularizer (FR), which aims to reduce performance disparities between sub-populations. FR is shown to enhance the performance of tail sub-populations and overall learning in the presence of label noise. Experiment results support the effectiveness of this approach, especially when combined with existing robust or class-balanced methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1: This paper introduces a novel perspective on addressing both long-tailed problem and noisy label problem, which is rarely discussed in previous studies.\n\nS2: This paper provides some empirical evidence, which helps to better interpret motivation.\n\nS3: This paper is well-organized and clearly written."
                },
                "weaknesses": {
                    "value": "W1: The logic of this paper is rather confusing; the author discusses the issues of noisy data and long tail problem together, but does not build a good bridge of them. In addition, the significance and necessity of the empirical evidence is not clear. Because it is also necessary to deal with the long tail problem on clean data, instead of only need to deal with the long tail problem on noisy data. The authors need to emphasize the differences and connections between dealing with long tail problems on noisy data and dealing with long tail problems under clean data (e.g., problem formalization or techniques).\n\nW2: Why observation 3.1 holds\uff1fSpecifically, if prediction model trained on noisy data has larger variance than the prediction model trained on clean data, the same pattern will be also observed. Therefore, observation 3.1 depends on the original variance of the prediction model. In addition, if we did not remove any sub-population and draw an Acc plot for clean data and noisy data, can we observe the similar pattern in Figure 4?  Similar arguments are also holds for observation 3.2.\n\nW3: The problem is handled by simply adding the distance of each class from the \"center\" to the original loss function, which is simple and lacks some novelty. For example, is it possible to calibrate clean data with noisy data or head with tail data to address this problem?\n\nW4: How can we choose $\\lambda_{i}$ for each sample in practice when it is not set to a constant?\n\nW5: This paper is missing some noisy label and long-tail baselines. There are many works trying to denoise, like [1-4], and many long-tail baselines like [5-6]. I didn't intend for the authors to compare all noisy label and long-tail baselines, but it is necessary to consider some of them.\n\n[1] Patrini, Giorgio, et al. \"Making deep neural networks robust to label noise: A loss correction approach.\" in CVPR 2017\n\n[2] Han, Bo, et al. \"Co-teaching: Robust training of deep neural networks with extremely noisy labels.\" in NeurIPS 2018.\n\n[3] Xia, Xiaobo, et al. \u201cAre anchor points really indispensable in label-noise learning?.\u201d in NeurIPS 2019.\n\n[4] Yong, Lin., et al. \"A Holistic View of Label Noise Transition Matrix in Deep Learning and Beyond.\" in ICLR 2022.\n\n[5] Chen, Wei, et al. \"Crest: A class-rebalancing self-training framework for imbalanced semi-supervised learning. in CVPR 2021.\n\n[6] Zhong, Zhisheng, et al. \"Improving Calibration for Long-Tailed Recognition\" in CVPR 2021."
                },
                "questions": {
                    "value": "Please refer to weakness part for the questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4841/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4841/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4841/Reviewer_yYZa"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4841/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699029247006,
            "cdate": 1699029247006,
            "tmdate": 1699636467490,
            "mdate": 1699636467490,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VWX602QiWz",
                "forum": "v8jH6rjw8c",
                "replyto": "9FgMiGRo0P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4841/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4841/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yYZa (Part 1)"
                    },
                    "comment": {
                        "value": "Thanks Reviewer yYZa for the efforts and time in reviewing our paper! Please see our response below regarding your concerns and mentioned weaknesses.\n\n```Weaknesses 1:``` \n\n[The authors need to emphasize the differences and connections between dealing with long tail problems on noisy data and dealing with long tail problems under clean data (e.g., problem formalization or techniques).]\n\n\n```Response:```\n\n**Regarding Problem Formalization:**\nWe appreciate the reviewer's point on differentiating between long-tail problems in clean versus noisy data contexts. In our perspective, addressing long-tail issues in clean data can be considered a subset of the broader challenge posed by noisy data. This relationship becomes apparent when considering the transition from clean to noisy labels, represented as $T_{i, j}(X)=\\mathbb{P}(\\widetilde{Y}=j | Y=i, X)$. In this framework, the scenario with clean data emerges when $T(X)$ simplifies to an identity matrix.\n\n**Concerning Techniques**\nFor illustrative purposes, we refer to the classical setting with class-dependent transition probabilities ($T_{i, j}(X)=T_{i, j}$, irrespective of $X$). Here, the learning or optimization tasks differ significantly between clean and noisy label scenarios. In the clean label context, the objective is to minimize: \n\n$\\mathbf{E}_{(X, Y)\\sim \\mathcal{D}} \\left[\\ell(f(X), Y)\\right],$ \n\nwhich is equivalent to \n\n$\\sum_{i} \\mathbf{E}_{X} ~ \\mathbb{P}(Y=i)  \\left[\\ell(f(X), Y=i)\\right].$\n\nIn contrast, with noisy labels, the objective function shifts to minimizing:\n\n$$\\mathbf{E}_{(X, \\widetilde{Y})\\sim \\widetilde{\\mathcal{D}}} ~\\left[\\ell(f(X), \\widetilde{Y})\\right].$$ \n\nThe objective under the label noise setting is equivalent to:\n\n$$\\sum_j  \\mathbf{E}_{X} ~\\mathbb{P}(\\widetilde{Y}=j) \\left[\\ell(f(X), \\widetilde{Y}=j)\\right].$$\n\nLeveraging the transition matrix, we could then derive:\n\n$$\\sum_j \\sum_i  \\mathbf{E}_{X} ~\\mathbb{P}(\\widetilde{Y}=j|Y=i) \\mathbb{P}(Y=i)  \\left[\\ell(f(X), \\widetilde{Y}=j)\\right],$$ \n\nIt is worthy noting that the impact of class-imbalanced distribution is indicated in the formulation as well (varied $\\mathbb{P}(Y=i)$). As we can see the differences, with the presence of label noise, the technique differences lie in how to further mitigate/correct the impact brought by $T_{i, j}=\\mathbb{P}(\\widetilde{Y}=j|Y=i)$.\n\n```Weakness 2:```\n\n [Why observation 3.1 holds\uff1fSpecifically, if prediction model trained on noisy data has larger variance than the prediction model trained on clean data, the same pattern will be also observed. Therefore, observation 3.1 depends on the original variance of the prediction model. In addition, if we did not remove any sub-population and draw an Acc plot for clean data and noisy data, can we observe the similar pattern in Figure 4? Similar arguments are also held for observation 3.2.]\n\n```Response:```\n\nThank you for your insightful comments and queries regarding Observation 3.1 in our manuscript. We appreciate the opportunity to clarify these points and ensure the accuracy of our findings.\nFirstly, we acknowledge your perspective on the potential variance of prediction models trained on noisy versus clean data. However, the focus of Observation 3.1 and Figure 4 in our study is not on the variance of model predictions but on the performance disparity, specifically the test accuracy drops at the sub-population level. Our analysis is centered on comparing the following metrics\uff1a \n\n$\\text{Acc}\\_{\\text{p}}(\\mathcal{A},S, i, j)$  v.s.  $\\text{Acc}\\_{\\text{p}}(\\mathcal{A}, \\widetilde{S}, i, j)$.\n\nIn Figure 4, the broader distribution of the box plot for  $\\text{Acc}\\_{\\text{p}}(\\mathcal{A}, \\widetilde{S}, i, j)$  (pertaining to the noisy-label scenario) as compared to $\\text{Acc}\\_{\\text{p}}(\\mathcal{A},S, i, j),$ indicates a more pronounced influence of noisy data, thereby reinforcing Observation 3.1.\n\nRegarding your question about the absence of sub-population removal and its effect on the accuracy plot for clean versus noisy data, we have addressed this in Figure 9 (located in the Appendix, lower right corner). It appears there may have been a misunderstanding regarding the definition of our influence measure depicted in Figure 4, which focuses on the accuracy drop. To elucidate, when calculating  $\\text{Acc}\\_{\\text{p}}(\\mathcal{A}, \\widetilde{S}, i, j)$ for each $i, j$, we assess the following difference:\n\n$$\\mathbb{P}\\_{f \\leftarrow \\mathcal{A} (\\widetilde{S}), (X\u2019, Y\u2019, G=j)} (f(X\u2019)=Y\u2019) - \\mathbb{P}\\_{f \\leftarrow \\mathcal{A}(\\widetilde{S}^{\\backslash i}), (X\u2019, Y\u2019, G=j)} (f(X\u2019)=Y\u2019)$$\n\nHere, the upper term represents the scenario where no sub-populations are removed in training. Following your query, if no subpopulation is excluded, the lower term becomes identical to the first, resulting in a net influence of zero."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4841/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543164989,
                "cdate": 1700543164989,
                "tmdate": 1700545165707,
                "mdate": 1700545165707,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HgWL5pUzei",
                "forum": "v8jH6rjw8c",
                "replyto": "9FgMiGRo0P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4841/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4841/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yYZa (Part 2)"
                    },
                    "comment": {
                        "value": "```Weakness 3:``` \n\n[The problem is handled by simply adding the distance of each class from the \"center\" to the original loss function, which is simple and lacks some novelty. For example, is it possible to calibrate clean data with noisy data or head with tail data to address this problem?]\n\n```Response:```\n\nThank you for your insightful feedback. We appreciate your suggestion to calibrate clean data with noisy data or head with tail data. Indeed, this approach offers an intriguing perspective. However, our decision to not pursue this direction was driven by several pragmatic considerations.\n\nFirstly, implementing such a solution would necessitate additional information, such as clean labels for training samples or a separate, clean validation set. Our study operates under the realistic assumption that these resources are not readily available. Estimating the necessary statistics without these could be particularly challenging in the context of long-tailed data, potentially leading to errors that might compromise robust learning.\n\nFurthermore, our empirical observations highlighted the importance of controlling the performance disparity among different populations. Introducing a fairness regularizer aligned more closely with our research narrative and objectives, offering a more feasible solution under the constraints of our study.\n\nRegarding the simplicity of our method, we believe that simplicity does not detract from novelty, particularly when it effectively addresses the complexities of long-tail distribution and label noise. The \"distance\" we refer to in our approach is not a traditional $L_{1}, L_{2}$ distance, but rather a measure of the performance gap across sub-populations. To our knowledge, our work is unique in employing fairness regularization to control this gap under such challenging conditions.\n\n```Weakness 4:```\n \n[How can we choose lambda for each sample in practice when it is not set to a constant?]\n\n```Response:``` \nThank you for your insightful question regarding the selection of lambda for each sample in our proposed method. Your query touches on a fundamental aspect of our approach, which we appreciate.\nIn addressing the optimization task presented in Equation (2):\n\n\n$$\\min_{f:  X\\rightarrow [K]} \\mathbf{E}_{(X, \\widetilde{Y})\\sim \\widetilde{\\mathcal{D}}} [\\ell(f(X), \\widetilde{Y})],  \\qquad s.t.  \\qquad\\text{Dist}_i\\leq \\delta, \\forall i \\in [N], \\qquad \\text{[Eqn. (2)]} $$ \n\nWe could find an empirical solution utilizing the augmented Lagrangian form. This approach transforms the constrained problem into a series of unconstrained minimization problems, as detailed in Equation (R1):\n\n$$\\min_{f:  X\\rightarrow [K]}  \\Phi(f_k):= \\mathbf{E}_{(X, \\widetilde{Y})\\sim \\widetilde{\\mathcal{D}}}  [\\ell(f\\_{k} (X), \\widetilde{Y})]+  \\mu\\_{k} \\cdot \\sum\\_{i\\in[N]}  g( \\text{Dist}\\_{i}-\\delta), \\text{[Eqn. (R1)]}$$\n\nwhere $g(x):=\\max(0, x)^2$. The iterative process involves updating the solution from each iteration to serve as the initial guess for the subsequent one. This method ensures that the solutions converge asymptotically to the optimization task defined in Equation (2).\n\nIn the augmented Lagrangian form, with $\\delta=0$, the formulation becomes:\n\n$$\\min_{f: X\\to [K]} ~\\Phi(f_k):=\\mathbb{E}_{(X,  \\widetilde{Y})\\sim \\widetilde{\\mathcal{D}}}[\\ell(f\\_{k}(X), \\widetilde{Y})] + \\frac{\\mu\\_{k}}{2} \\cdot \\sum\\_{i\\in[N]}{\\text{Dist}\\_{i}}^2+ \\sum\\_{i\\in[N]} \\lambda\\_{i} \\cdot  \\text{Dist}\\_{i}$$\n\nwith $\\lambda_i$ updating as $\\lambda\\_{i}\\leftarrow \\lambda\\_{i} + \\mu\\_{k}\\cdot \\text{Dist}\\_{i}$, and $\\text{Dist}\\_{i}$ takes the input classifier $f_k$ given by the solution to Eqn. (R1) at the $k$-th iteration. In such an augmented Lagrangian method, the variable $\\lambda$ could be viewed as an estimate of the Lagrange multiplier, and the accuracy of this estimate keeps improving as iteration progresses. We adopt the augmented format instead of the penalty method for the reason that: it is not necessary to take $\\mu \\rightarrow \\infty$ to solve Eqn. (2). And the use of the Lagrange multiplier term allows us to choose smaller $\\mu_k$, hence avoiding ill-conditioning.\n\nRegarding the adoption of a constant $\\lambda$, our approach stems from the lack of a strict requirement for equal performance across sub-populations. A constant $\\lambda$ offers a simpler implementation to manage performance disparities. Our ablation study (referenced in Table 3) supports this choice, demonstrating the efficacy of constant $\\lambda$ values in Fairness Regularization (FR)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4841/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545009784,
                "cdate": 1700545009784,
                "tmdate": 1700546988660,
                "mdate": 1700546988660,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]