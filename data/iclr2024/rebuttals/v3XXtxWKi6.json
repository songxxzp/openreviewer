[
    {
        "title": "RLCD: Reinforcement Learning from Contrastive Distillation for LM Alignment"
    },
    {
        "review": {
            "id": "fmbRpYvwxx",
            "forum": "v3XXtxWKi6",
            "replyto": "v3XXtxWKi6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4026/Reviewer_bkDY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4026/Reviewer_bkDY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Reinforcement Learning from Contrastive Distillation, which uses an unaligned language model to produce contrasting response pairs from both positive and negative prompts in terms of a desired attribute, e.g. harmlessness or helpfulness. It is argued that responses generated in this way are more distinguishable, producing a better signal-to-noise ratio. The produced rankings between pairs are then used for training a preference model, and subsequently, RLHF. Experiments are conducted on three alignment tasks, showing better performance than RLAIF and context distillation baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n2. The method is clearly motivated, and the authors make a detailed argument on the problems with the prior work.\n3. Experiments are conducted over multiple domains, demonstrating the generality of the method."
                },
                "weaknesses": {
                    "value": "1. The analysis on the preference model shows that the preference model produced by RLCD is, while better than the baseline, still not very good, especially on the harmlessness attribute (Tab. 5). It is not clear how this slight advantage over chance (2.4%~5.9%) translates into a much better downstream performance after RLHF.\n2. As shown in Appendix C, RLAIF-Few-30B produces both a better preference model and a better-aligned language model than RLCD-30B on the harmlessness benchmark, which is attributed to few-shot prompting by the authors. It seems that this technique can also be integrated into RLCD to enable a fairer comparison.\n3. The advantage of RLCD over RLAIF shrinks going from 7B to 30B (Tab. 2). It remains to be seen whether RLCD (or RLCD-Rescore) can scale to yet larger language models that are arguably better at differentiating responses near the decision boundary."
                },
                "questions": {
                    "value": "Not atm"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4026/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4026/Reviewer_bkDY",
                        "ICLR.cc/2024/Conference/Submission4026/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4026/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698745713189,
            "cdate": 1698745713189,
            "tmdate": 1700643346166,
            "mdate": 1700643346166,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3mblS8XW9f",
                "forum": "v3XXtxWKi6",
                "replyto": "fmbRpYvwxx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4026/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4026/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer bkDY"
                    },
                    "comment": {
                        "value": "Thank you for your helpful comments! We appreciate your comments about our idea being clearly motivated and about the generality of our method across multiple domains. We address your questions below.\n\n> **Concern 1: The analysis on the preference model shows that the preference model produced by RLCD is, while better than the baseline, still not very good, especially on the harmlessness attribute (Tab. 5). It is not clear how this slight advantage over chance (2.4%~5.9%) translates into a much better downstream performance after RLHF.**\n\nActually, we think that the preference model doesn\u2019t need to be that far above chance to enable good downstream performance. For example, Anthropic\u2019s original RLHF paper [1] observed only 63% agreement between their own researchers and crowdworkers on their helpful-honest-harmless labels. After all, the human labels are on pairs of model outputs generated from the same prompt, so there is often not a clear winner within a pair. \n\nWe observe that harmlessness is perhaps even a bit noisier than helpfulness: we checked GPT-4\u2019s agreement with humans and observed only 60.5% agreement on harmlessness compared to 73.0% on helpfulness. So on harmlessness, that would put e.g., 5.9% above chance as closing 56% of the gap between chance and GPT-4. So we think it\u2019s reasonable to observe decent downstream performance with our reward models even if the advantage over chance doesn\u2019t look like much.\n\n> **Concern 2: As shown in Appendix C, RLAIF-Few-30B produces both a better preference model and a better-aligned language model than RLCD-30B on the harmlessness benchmark, which is attributed to few-shot prompting by the authors. It seems that this technique can also be integrated into RLCD to enable a fairer comparison.**\n\nWe intentionally used zero-shot prompts in our main experiments to minimize the influence of prompt selection for fair comparison (more detailed discussion under Reviewer HiRS, Concern 1); adding few-shot examples would introduce an additional variable. We mainly included RLAIF-Few-30B in Appendix C to check that we can replicate the results from [2], to make sure that RLAIF\u2019s often poor performance when we do not use both (1) few-shot and (2) 30B scale is not due to implementation differences.\n\nTo be clear, we think the final language model learned by RLAIF-Few-30B is not good - as shown in Table 17 in Appendix C, the model distribution collapses to a generic but unhelpful output, with *nearly all* outputs being a variation of \u201cI'm sorry, but I'm not sure how I can help with that. Can I ask some questions to help me understand your problem better?\u201d. Quantitatively, the fraction of distinct 3-grams (Dist-3) within a sample of 10000 tokens is only 18% (!) for RLAIF-Few-30B, compared to over 90% for most other experiments (see new Appendix J.3; we\u2019ve also added more discussion on the mode collapse to Appendix C).\n\nIn any case, we\u2019ve now run RLCD-Few-30B on harmlessness too. RLCD-Few-30B also collapses to a single generic output: \u201cI'm here to help you get the information you need. Can I help you today?\u201d. GPT-4 happens to prefer RLAIF-Few-30B\u2019s generic output, but we think that it\u2019s more indicative to look at the preference model\u2019s performance since both models are so heavily mode-collapsed. RLCD-Few-30B\u2019s preference model\u2019s accuracy for agreeing with humans is 62.8% (avg. prob of agreement = 0.599), a good bit higher than the 57.0% for RLAIF-Few-30B\u2019s (avg. prob of agreement = 0.540). \n\n> **Concern 1: The advantage of RLCD over RLAIF shrinks going from 7B to 30B (Tab. 2). It remains to be seen whether RLCD (or RLCD-Rescore) can scale to yet larger language models that are arguably better at differentiating responses near the decision boundary.**\n\nWe totally agree with this point, as touched upon in our Discussion and our Limitations sections, and we\u2019d be happy to emphasize it more for clarity. However, even if it turns out that RLCD in its current form is not as useful as model scale exceeds 30B (which is still unclear), we believe that we have still made valuable research contributions, as described in \u201cAdditional Aspects of Our Contribution\u201d under the General Response.\n\n[1] Bai, Yuntao, et al. \"Training a helpful and harmless assistant with reinforcement learning from human feedback.\" arXiv preprint arXiv:2204.05862 (2022).\n\n[2] Bai, Yuntao, et al. \"Constitutional ai: Harmlessness from ai feedback.\" https://arxiv.org/abs/2212.08073"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700264043490,
                "cdate": 1700264043490,
                "tmdate": 1700264043490,
                "mdate": 1700264043490,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "poHiLvyvQZ",
                "forum": "v3XXtxWKi6",
                "replyto": "3mblS8XW9f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4026/Reviewer_bkDY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4026/Reviewer_bkDY"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your rebuttal"
                    },
                    "comment": {
                        "value": "Hi,\n\nThanks for your time spent on the rebuttal. I appreciate your honesty. Most of my concerns have been addressed and there are lots of evaluations added in. I agree to raise my review score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643327056,
                "cdate": 1700643327056,
                "tmdate": 1700643327056,
                "mdate": 1700643327056,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "whfqY2WCHi",
            "forum": "v3XXtxWKi6",
            "replyto": "v3XXtxWKi6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4026/Reviewer_Cuc6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4026/Reviewer_Cuc6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method called RLCD that incorporates the idea of context distillation into RLHF framework. Instead of using a single prompt to elicit preference data, RLCR does this by constructing two manually augmented prompts of positive and negative instructions. The authors show the effectiveness of RLCR by comparing it with RLAIF and SFT context distillation on a dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.The idea is interesting and the motivation seems clear.\n\n2.The paper is well-written."
                },
                "weaknesses": {
                    "value": "1.The proposed method is too simple that seems like a prompting trick in preference data construction.\n\n2.More comprehensive methodology and solid experiments (e.g., stronger baselines and deeper analyses) are needed to improve the contribution and soundness of this paper."
                },
                "questions": {
                    "value": "1. Will the proposed method reduce the diversity of preference data? \n\n2. What if the original prompt already contains positive or negative instructions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4026/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4026/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4026/Reviewer_Cuc6"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4026/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832791228,
            "cdate": 1698832791228,
            "tmdate": 1700568983130,
            "mdate": 1700568983130,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H9A2WuRv4q",
                "forum": "v3XXtxWKi6",
                "replyto": "whfqY2WCHi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4026/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4026/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Cuc6"
                    },
                    "comment": {
                        "value": "Thank you for your helpful comments! We appreciate your comments that our idea is interesting and well-motivated, and that our paper is well written. We address your questions below.\n\n> **Concern 1: The proposed method is too simple that seems like a prompting trick in preference data construction.**\n\nThe core idea of RLCD is to change the generation process of the preference outputs, moving away from noisy near-boundary examples to more reliable far-from-boundary examples; this tradeoff is both empirically effective and automatable. Please see \u201cAdditional Aspects of Our Contribution\u201d under the General Response for a more detailed discussion of our contributions; we would also like to highlight our theoretical motivation for RLCD in the new Appendix N, which we summarize in \u201cTheoretical Motivation for RLCD\u201d in the General Response.\n\nRLCD then relies on prompting because\u2014given that we want to change the generation process of the preference outputs\u2014prompting seems to be the simplest implementation of our idea. We view this simplicity of implementation as a strength which makes RLCD easier to reuse or adapt in new applications; we note that reviewers HiRS, UFW8, and RAFU mentioned simplicity / straightforwardness of implementation of RLCD under Strengths as well.\n\n\n> **Concern 2: More comprehensive methodology and solid experiments (e.g., stronger baselines and deeper analyses) are needed to improve the contribution and soundness of this paper.**\n\nWe\u2019ve now added experiments according to your questions. Was there something else in particular you were looking for?\n\n> **Question 1: Will the proposed method reduce the diversity of preference data?**\n\nWe\u2019ve now checked the preference data diversity by measuring the percentage of distinct 1-, 2-, and 3-grams among 10K tokens of preference data for both RLAIF and RLCD (see new Appendix K.2). The metrics for RLCD\u2019s preference data are overall very similar to those of RLAIF; if anything RLCD has higher diversity more often than not. \n\n\n\n> **Question 2: What if the original prompt already contains positive or negative instructions?**\n\nWe think adding positive or negative instructions to the original prompt is fundamentally orthogonal to the RLCD idea, which is mainly about creating *contrast* in the outputs $o_+$ and $o_-$ by using *contrasting* prompts $p_+$ and $p_-$. To confirm, we\u2019ve also added an experiment in the new Appendix L, where we observe that adding positive instructions to the prompt doesn\u2019t change RLAIF\u2019s performance compared to RLCD."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263958767,
                "cdate": 1700263958767,
                "tmdate": 1700263958767,
                "mdate": 1700263958767,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G2FpRdKw0P",
                "forum": "v3XXtxWKi6",
                "replyto": "whfqY2WCHi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4026/Reviewer_Cuc6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4026/Reviewer_Cuc6"
                ],
                "content": {
                    "title": {
                        "value": "Official Review of Submission4026 by Reviewer Cuc6"
                    },
                    "comment": {
                        "value": "The author indeed addresses the issues I was concerned about. However, I argue that the core innovativeness of the method (using LLM to generate pairwise preference data) is still insufficient for publication; it seems more like a variant of context distillation. Considering this work's extensive experimental validation and case analysis, I raise the score to 5."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568958721,
                "cdate": 1700568958721,
                "tmdate": 1700568958721,
                "mdate": 1700568958721,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xTOoPM2S8c",
            "forum": "v3XXtxWKi6",
            "replyto": "v3XXtxWKi6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4026/Reviewer_RAFU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4026/Reviewer_RAFU"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Reinforcement Learning from Contrastive Distillation (RLCD), a novel method to align language models with human values without relying on human feedback. It's designed to overcome limitations in previous approaches like Reinforcement Learning from AI Feedback (RLAIF) and context distillation. RLCD operates by generating two contrasting model outputs using positive and negative prompts, with the positive prompts encouraging adherence to desired principles (e.g., harmlessness) and negative prompts doing the opposite. This method creates clearer preference pairs for training a preference model, which is then used to improve a base unaligned language model via reinforcement learning. The paper demonstrates that RLCD outperforms existing methods across various tasks and model scales, confirming its effectiveness in aligning language models more closely with desired human values."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well written and easy to read\n- The proposed RLCD method seems straightforward to implement and can be generalised\n- By generating clearer preference pairs without human annotations, RLCD reduces the cost and time associated with collecting high-quality human preference data.\n- The paper provides empirical evidence that RLCD outperforms existing methods across different tasks and scales."
                },
                "weaknesses": {
                    "value": "- The GPT-4 evaluated the baseline RLAIF outperforming the proposed RLCD method for the 30B model size.\n- The contribution of the paper is limited"
                },
                "questions": {
                    "value": "In the GPT-4 evaluation results (Table 3), the authors mentioned that \"The gap between RLCD and all baselines is especially large when using LLaMA-7B for preference data simulation.\". Indeed, comparing row 2 and row 5: the RLCD-7B has a larger advantage over RLAIF-7B compared with RLCD-30B vs. RLAIF-30B. Similarly for the human evaluation in Table 2.\nAnd in the Helpfulness and Outlining tasks, the baseline RLAIF-30B scored higher than the proposed RLCD-30B). I have a few questions on this:\n- Why does the proposed RLCD algorithm gain a bigger advantage on the smaller LlaMA model when compared with its baselines? Is it due to the poor performance of RLAIF with smaller models?\n- For the helpfulness and outlining tasks, the baseline RLAIF-30B outperformed the proposed RLCD. What is the cause of this result and should we generally adopt RLAIF when a larger LLM is available?\n- The human evaluation always preferred RLCD compared with RLAIF, in contrast to GPT 4's preference on RLCD-30B. Can the authors please provide some insights into the cause of the difference in the GPT4 vs. human evaluation?\n\nThe downstream fine-tuning is using PPO. Although not the main contribution of the paper, can the authors please provide the details of the downstream fine-tuning procedures? For example, what are input and output of the RL model, what is the reward used by PPO derived from the upstream preference generation? And the overall process of the RL fine-tuning? This information would be useful for a general audience who would like to use the proposed RLCD method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4026/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4026/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4026/Reviewer_RAFU"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4026/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698959623359,
            "cdate": 1698959623359,
            "tmdate": 1699636365338,
            "mdate": 1699636365338,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SGmNxjpsD5",
                "forum": "v3XXtxWKi6",
                "replyto": "xTOoPM2S8c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4026/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4026/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer RAFU"
                    },
                    "comment": {
                        "value": "Thank you for your helpful comments! We appreciate your comments about the implementation ease and generalizability of our method and its empirical effectiveness. We address your questions below.\n\n> **Concern 1: The GPT-4 evaluated the baseline RLAIF outperforming the proposed RLCD method for the 30B model size.**\n\nPlease see Question 1b below.\n\n> **Concern 2: The contribution of the paper is limited**\n\nPlease see \u201cAdditional Aspects of Our Contribution\u201d under the General Response.\n\n> **Question 1: In the GPT-4 evaluation results (Table 3), the authors mentioned that \"The gap between RLCD and all baselines is especially large when using LLaMA-7B for preference data simulation.\". Indeed, comparing row 2 and row 5: the RLCD-7B has a larger advantage over RLAIF-7B compared with RLCD-30B vs. RLAIF-30B. Similarly for the human evaluation in Table 2. And in the Helpfulness and Outlining tasks, the baseline RLAIF-30B scored higher than the proposed RLCD-30B). I have a few questions on this:**\n\n> **Question 1a: Why does the proposed RLCD algorithm gain a bigger advantage on the smaller LlaMA model when compared with its baselines? Is it due to the poor performance of RLAIF with smaller models?**\n\nIndeed, RLAIF works quite poorly with smaller models, which struggle to label preference data accurately using the RLAIF scoring prompts (see Tables 24 and 25 for examples, if interested). Please see our \u201cTheoretical Motivation for RLCD\u201d under the General Response, which includes potential explanations for why RLCD works so much better on 7B, or the new Appendix N for complete details on these theoretical intuitions.\n\n> **Question 1b: For the helpfulness and outlining tasks, the baseline RLAIF-30B outperformed the proposed RLCD. What is the cause of this result and should we generally adopt RLAIF when a larger LLM is available?**\n\nTo be clear, we trust the human evaluations more; see Question 1c below for discussion of human-GPT4 disagreements. Nevertheless, you raise an important question of whether we should use RLCD with even larger LLMs, as the difference between RLCD and RLAIF is much smaller at 30B compared to 7B. Though we\u2019ve already tested up to 30B scale, it\u2019s indeed not yet clear whether RLCD will scale to even larger model sizes (as we touched upon in our Discussion and our Limitations sections too). \n\nHowever, even without considering whether RLCD in its current form scales well to >30B scale, we believe that we have still made useful research contributions; for instance, please see \u201cAdditional Aspects of Our Contribution\u201d under the General Response.\n\n> **Question 1c: The human evaluation always preferred RLCD compared with RLAIF, in contrast to GPT 4's preference on RLCD-30B. Can the authors please provide some insights into the cause of the difference in the GPT4 vs. human evaluation?**\n\nWe discussed the annotation disagreements briefly in Appendix G, where we observed quantitatively that GPT-4\u2019s agreement with humans is lower in two main cases. \n\nFirst, when evaluating helpfulness on the harmlessness prompt set, GPT-4\u2019s own harmlessness alignment often prevents it from saying which answer is more helpful. \n\nSecond, specifically when comparing RLCD-30B to RLAIF-30B, the disagreement is higher because both models are actually producing quite good outputs (see e.g., example helpfulness outputs for RLCD-30B and RLAIF-30B in Table 27), so the comparison may simply be more subjective. For example, with outlines, we noticed qualitatively that RLCD\u2019s outputs seemed a bit more surprising, which was fine for humans but somewhat dispreferred by GPT-4. So we think GPT-4 likely just subjectively prefers a particular style, and that it\u2019s a bit more reliable to survey humans as a result.\n\n> **Question 2: The downstream fine-tuning is using PPO. Although not the main contribution of the paper, can the authors please provide the details of the downstream fine-tuning procedures? For example, what are input and output of the RL model, what is the reward used by PPO derived from the upstream preference generation? And the overall process of the RL fine-tuning? This information would be useful for a general audience who would like to use the proposed RLCD method.**\n\nGood point, we previously included the implementation details and hyperparameters, but we\u2019ve now added a description of the actual PPO fine-tuning procedure in Section 3.1.1 in the main text."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263882584,
                "cdate": 1700263882584,
                "tmdate": 1700264433657,
                "mdate": 1700264433657,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WJuXy1rOYX",
                "forum": "v3XXtxWKi6",
                "replyto": "SGmNxjpsD5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4026/Reviewer_RAFU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4026/Reviewer_RAFU"
                ],
                "content": {
                    "title": {
                        "value": "thank the authors for their rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for their rebuttal and updating the PPO details. I have no further questions."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659099263,
                "cdate": 1700659099263,
                "tmdate": 1700659099263,
                "mdate": 1700659099263,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dfcXft7nd4",
            "forum": "v3XXtxWKi6",
            "replyto": "v3XXtxWKi6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4026/Reviewer_UFW8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4026/Reviewer_UFW8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method called Reinforcement Learning from Contrastive Distillation (RLCD) for aligning Language models (LMs) to follow principles expressed in natural language. RLCD utilizes contrasting prompts encouraging and discouraging adherence to principles, resulting in differentiated model outputs and cleaner preference labels, eliminating the need for human feedback. Based on the generated preference pairs, RLCD trains a preference model that captures desired behavior. The trained preference model guides a reinforcement learning process to refine an unaligned base LM, aligning it with the specified principles. RLCD outperforms existing methods like RLAIF and context distillation on diverse tasks including harmlessness, helpfulness, and story outline generation. RLCD demonstrates effectiveness with both small (7B) and large (30B) model sizes for simulating preference data. Overall, RLCD offers a novel method for human-free alignment of language models, surpassing existing techniques and demonstrating promising scalability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "RLCD is a neat idea that is simple yet effective. It requires some changes to the prompt to force the model to output more contrastive positive and negative outputs, which yield significant improvement in practice. I think the simplicity and effectiveness of the method is of value to the community.\n\nMoreover, the empirical results seem quite strong, which makes the method more convincing. \n\nThe paper is also easy to follow and understand."
                },
                "weaknesses": {
                    "value": "The method seems straightforward and is less technically strong. The main technical contribution is the simple changes on the prompt design when generating the pair of responses for the preference data. While such changes make sense intuitively, i.e. making both responses more contrastive, the authors didn't show much principled analysis on why such design can be better than direct RLAIF. I think it would be helpful to give some more technical/principled explanation on RLCD.\n\nMoreover, I wonder if this design is important when there's human feedback data presented in the preference dataset as well, which is more common in practice. It would be interesting to see if RLCD would still make such a big difference in practice with some human feedback data in the mix."
                },
                "questions": {
                    "value": "1. Please clarify the technical details of RLCD, ideally some theoretical justifications.\n2. Please show some experiments in scenarios where there's human preference data available in the data mixture."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4026/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699233607168,
            "cdate": 1699233607168,
            "tmdate": 1699636365266,
            "mdate": 1699636365266,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H8FBBg8pFN",
                "forum": "v3XXtxWKi6",
                "replyto": "dfcXft7nd4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4026/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4026/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer UFW8"
                    },
                    "comment": {
                        "value": "Thank you for your helpful comments! We appreciate your comments about the simplicity and effectiveness of our method in practice and our paper being easy to follow and understand. We address your questions below.\n\n> **Concern 1: The method seems straightforward and is less technically strong. The main technical contribution is the simple changes on the prompt design when generating the pair of responses for the preference data. While such changes make sense intuitively, i.e. making both responses more contrastive, the authors didn't show much principled analysis on why such design can be better than direct RLAIF. I think it would be helpful to give some more technical/principled explanation on RLCD.**\n\nThanks for this excellent suggestion! We\u2019ve now added a technical explanation for our intuitions; please see \u201cTheoretical Motivation for RLCD\u201d under the General Response, and the new Appendix N for complete details.\n\n\n> **Concern 2: Moreover, I wonder if this design is important when there's human feedback data presented in the preference dataset as well, which is more common in practice. It would be interesting to see if RLCD would still make such a big difference in practice with some human feedback data in the mix.**\n\nSince we have the original Anthropic human-labeled preference data for harmlessness and helpfulness, we\u2019ve now tested harmlessness and helpfulness on 7B model scale with human preference labels mixed in (see new Appendix M). Even with as many as 20% human labels, RLCD still outperforms RLAIF by a decent margin according to GPT-4, though the difference is naturally smaller than with no human labels.\n\n\n> **Question 1: Please clarify the technical details of RLCD, ideally some theoretical justifications.**\n\nPlease see \u201cTheoretical Motivation for RLCD\u201d under the General Response.\n\n> **Question 2: Please show some experiments in scenarios where there's human preference data available in the data mixture.**\n\nPlease see response to Concern 2."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263785889,
                "cdate": 1700263785889,
                "tmdate": 1700263785889,
                "mdate": 1700263785889,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "i7W8V6UP92",
            "forum": "v3XXtxWKi6",
            "replyto": "v3XXtxWKi6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4026/Reviewer_HiRS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4026/Reviewer_HiRS"
            ],
            "content": {
                "summary": {
                    "value": "Reinforcement learning (RL) from human feedback has been very effective in aligning large language models (LLM) to human preferences. In particular, RLHF requires human preference data to learn a reward model for optimizing the LLM with RL. However, collecting human preference data can be very expensive. This paper proposed to simulate human preferences using ideas from context distillation. Unlike context distillation, which only modifies the original prompt to be more positive, this paper modifies the original prompt to be both positive and negative, creating a preference dataset from the new prompt-generation pairs. The author's results show that this idea empirically performs better than two competitive baselines across various tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed idea is very simple, and it is intuitive why the idea should perform well in practice.\n- The authors thoroughly evaluated their approach to baseline approaches using both GPT-4 and human evaluation.\n- The paper is well-written, and it is easy to follow.\n- The authors perform experiments on a 7B and 30B model to show how robust their proposed technique is at scale."
                },
                "weaknesses": {
                    "value": "- It is hard to understand if the performance is coming from the prompts themselves or the proposed algorithm.\n- The authors only report GPT-4 and human evaluation but do not report RM-score or standard NLP metrics (e.g., perplexity or output-perplexity)\n- The authors do not provide a thorough description of the outlining prompts task, and there does not seem to be any references for this task, so it is very hard to understand the task's difficulty."
                },
                "questions": {
                    "value": "- How did you decide on the prompt affix pairs?\n- Why is having more than one prompt affix pair important? \n- Given that you automatically assume $o_{+}$ is preferred - how often does $o_{+}$ have a lower reward with respect to a held-out reward function?\n- Why would training examples far away from the boundary be better than training examples close to the boundary? I would assume that the points far away could be easy to classify.\n- Could you elaborate on how you performed your GPT-4 evaluation? What prompt did you use? How did you shuffle the data? Etc?\n- Could you provide other quantitative metrics for all algorithms considered in your experiments? (e.g., RM-score, perplexity, output-perplexity, etc.)\n - Did you perform GPT-4 evaluation using comparisons from the algorithms-generated output and human-generated data for a given prompt?\n- Could you provide other diversity metrics on the outputs of the text generated? (e.g., the ratio of distinct n-grams (Distinct-1, Distinct-2), average length of sentences, or count of n-grams in the generated text [1]). The text in Table 4 implies that RLCD generations are much longer than the base model sentences.\n- Could you elaborate on the RLCD-Resouce model setup? In particular, what does it mean to re-label the same scoring prompts as in RLAIF?\n- For RLAIF, did you run an experiment where you sample two outputs from the $p_{+}$ positive affix prompts? This provides RLAIF algorithms with modified prompts similar to RLCD and would reduce the advantage that RLCD has to strictly have the altered $p_{-}$ prompts.\n\n[1] A diversity-promoting objective function for neural conversation models by Li et al. 2015"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4026/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699397277353,
            "cdate": 1699397277353,
            "tmdate": 1699636365206,
            "mdate": 1699636365206,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Osy3UtTbn5",
                "forum": "v3XXtxWKi6",
                "replyto": "i7W8V6UP92",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4026/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4026/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer HiRS (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your helpful comments! We appreciate your comments about the intuitiveness of our method\u2019s usefulness in practice, the thoroughness of our experiments, and our paper being well-written. We address your questions below, including several new analyses following your suggestions.\n\n> **Concern 1: It is hard to understand if the performance is coming from the prompts themselves or the proposed algorithm.**\n\nAll RLAIF scoring prompts and RLCD affix pairs are shown in Appendix A, together with a description of how we selected them to maintain fairness of comparison with our baselines. \n\nRLAIF uses \u201cdiscriminative\u201d prompts for scoring while RLCD uses the prompt affix pairs in generation, so there are necessarily some differences where the language is unnatural when converting directly from one to the other, but we attempted to match the general meaning. For example, for harmlessness, we used the original 16 scoring prompts from [1] for RLAIF, and then constructed 16 corresponding affix pairs for RLCD to paraphrase the qualities being evaluated in those scoring prompts. Due to ensembling 16 prompts, we think that our harmlessness experiments should be particularly robust to the effect of prompt selection; the results also hold up when we run a version of harmlessness with a different set of more \u201cfocused\u201d prompts in Appendix D. For helpfulness and outlining we wrote our own scoring prompts for RLAIF, and each scoring prompt directly corresponds to an affix pair designed to encourage differences on the same axis being measured.\n\nMeanwhile, context distillation directly uses the same positive prompt $p_+$ as RLCD, to minimize the effect of prompt selection as much as possible.\n\n\n\n> **Concern 2: The authors only report GPT-4 and human evaluation but do not report RM-score or standard NLP metrics (e.g., perplexity or output-perplexity)**\n\nThanks for the suggested metrics, we\u2019ve now checked these. \n\nFor RM-score (new Appendix J.1), we have human preference data for harmlessness and helpfulness, so we train a held-out reward model on those. RLCD\u2019s outputs are the highest reward, except on helpfulness at 30B scale only, where the reward is slightly lower than RLAIF.\n\nFor the conditional perplexity of outputs (new Appendix J.2), RLCD\u2019s numbers are generally similar to those of baselines.\n\n> **Concern 3: The authors do not provide a thorough description of the outlining prompts task, and there does not seem to be any references for this task, so it is very hard to understand the task's difficulty.**\n\nWe were intending to provide a more thorough description along with examples, but regrettably were blocked due to data licensing restrictions. We\u2019re revisiting to see if we can release more details, though in any case we consider the harmlessness and helpfulness tasks to be the more \u201cstandard\u201d tasks for our main experiments. We\u2019ll still add some more information on outlining where possible\u2014e.g., the premises are usually 10 to 40 tokens, to get a sense of length. The outputs in our main experiments were anywhere from 20 to 200 tokens or more depending on the method. \n\n\n> **Question 1: How did you decide on the prompt affix pairs?**\n\nPlease see description under Concern 1.\n\n> **Question 2: Why is having more than one prompt affix pair important?**\n\t\nHaving more than one pair isn\u2019t required, actually\u2014in our helpfulness task we only use a single prompt affix pair. But we think using multiple affix pairs can be convenient when trying to optimize more complex multi-objective tasks such as outlining, where we are trying to optimize for interestingness as well as well-formedness and premise relevance (hence, we use 3 affix pairs). [1] also used 16 scoring prompts for harmlessness in RLAIF to increase robustness to prompt design, which we matched in our work.\n\n> **Question 3: Given that you automatically assume $o_+$ is preferred - how often does $o_+$ have a lower reward with respect to a held-out reward function?**\n\nWe\u2019ve now run this analysis for harmlessness and helpfulness using the same held-out reward models described under Concern 2; see new Appendix K.1. It\u2019s still noisy as we\u2019re relying on the base pretrained LLaMA: the fraction of correct labels w.r.t. the held-out reward ranges from 54% (harmlessness, 7B) to 74% (helpfulness, 30B). However, RLCD\u2019s label accuracy is always higher compared to RLAIF, with the difference ranging from 8% to 14% absolute."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263642278,
                "cdate": 1700263642278,
                "tmdate": 1700263642278,
                "mdate": 1700263642278,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lNooXyskqR",
                "forum": "v3XXtxWKi6",
                "replyto": "i7W8V6UP92",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4026/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4026/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer HiRS (part 2)"
                    },
                    "comment": {
                        "value": "> **Question 4: Why would training examples far away from the boundary be better than training examples close to the boundary? I would assume that the points far away could be easy to classify.**\n\nIf we could guarantee that the training examples were correctly labeled, then yes we would prefer examples close to the boundary. However, RLAIF/RLCD need to use a model to label the training examples in the first place before they can be used downstream. RLCD\u2019s examples may on average be farther from the boundary, but are more likely to be accurately labeled. In fact, RLCD\u2019s label accuracy may actually be higher *even when considering only examples that are close to the boundary,* as suggested by our theoretical analysis in the new Appendix N; please see \u201cTheoretical Motivation for RLCD\u201d under the General Response.\n\n> **Question 5: Could you elaborate on how you performed your GPT-4 evaluation? What prompt did you use? How did you shuffle the data? Etc?**\n\nThe procedure is described in Appendix F.2; the full prompts are shown in Tables 21 and 22. When comparing two outputs, we show them in random order. (Please let us know if you meant something else regarding data shuffling.)\n\n> **Question 6: Could you provide other quantitative metrics for all algorithms considered in your experiments? (e.g., RM-score, perplexity, output-perplexity, etc.)**\n\nPlease see Concern 2 above.\n\n> **Question 7: Did you perform GPT-4 evaluation using comparisons from the algorithms-generated output and human-generated data for a given prompt?**\n\nOur GPT-4 pairwise evaluation compares algorithm-generated outputs from two different methods (e.g., RLCD vs. a baseline) on the same prompt. There are no human-generated outputs being compared. (Note we don\u2019t have human-generated *outputs* in the Anthropic data we\u2019re using, only human *preference labels* on model-generated outputs.)\n\n> **Question 8: Could you provide other diversity metrics on the outputs of the text generated? (e.g., the ratio of distinct n-grams (Distinct-1, Distinct-2), average length of sentences, or count of n-grams in the generated text [1]). The text in Table 4 implies that RLCD generations are much longer than the base model sentences.**\n\nGood idea, we\u2019ve now checked these. \n\nThe n-gram diversity for RLCD is very similar to that of baselines (new Appendix J.3), except for on harmlessness on 7B scale only, where RLCD\u2019s diversity is lower due to repetitive wording when refusing to answer some requests (though it\u2019s still far from completely mode-collapsed, as shown in examples in Table 26). \n\nAs for length (new Appendix J.4), RLCD generations are indeed often longer than those of baselines particularly for helpfulness and story outlining, as RLCD correctly identifies that longer outputs better satisfy these alignment criteria on average (as you noticed in the Table 4 example for the helpfulness task).\n\n\n> **Question 9: Could you elaborate on the RLCD-Resource model setup? In particular, what does it mean to re-label the same scoring prompts as in RLAIF?**\n\nRLAIF: generates two outputs from the same prompt, and then uses a scoring prompt to decide which of the two outputs is better. \n\nRLCD: generates two outputs from different prompts $p_+$ and $p_-$, and then decides which of the two outputs is better simply based on which prompt $p_+$ or $p_-$ was used to generate each output. \n\nRLCD-Rescore: generates two outputs from different prompts $p_+$ and $p_-$ like RLCD, but then decides which of the two outputs is better by using the scoring prompt as in RLAIF, rather than simply based on which prompt $p_+$ or $p_-$ was used to generate each output.\n\n> **Question 10: For RLAIF, did you run an experiment where you sample two outputs from the $p_+$ positive affix prompts? This provides RLAIF algorithms with modified prompts similar to RLCD and would reduce the advantage that RLCD has to strictly have the altered $p_\u2212$ prompts.**\n\nThe core intuition of RLCD is to use the different prompts $p_+$ and $p_-$ to amplify the *contrast* between the outputs $o_+$ and $o_-$. In principle, $p_+$ isn\u2019t even needed for contrast\u2014one could imagine a version of RLCD that uses just the base prompt $p$ with a negative prompt $p_-$ to produce $o$ and $o_-$ for contrast instead. So we think that just directly sampling two outputs from the same $p_+$ wouldn\u2019t really change the advantage of RLCD over RLAIF at all, which we confirmed in 7B experiments in the new Appendix L.\n\n[1] Bai, Yuntao, et al. \"Constitutional ai: Harmlessness from ai feedback.\" https://arxiv.org/abs/2212.08073"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4026/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263726561,
                "cdate": 1700263726561,
                "tmdate": 1700263726561,
                "mdate": 1700263726561,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]