[
    {
        "title": "Discovering Mathematical Formulas from Data via LSTM-guided Monte Carlo Tree Search"
    },
    {
        "review": {
            "id": "UxOMudYbcJ",
            "forum": "1iKydVG6pL",
            "replyto": "1iKydVG6pL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6810/Reviewer_SBg1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6810/Reviewer_SBg1"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method to perform symbolic regression based on Monte-Carlo Tree Search (MCTS) with guidance from an LSTM."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I'm afraid I do not see much added value in this paper compared to existing work..."
                },
                "weaknesses": {
                    "value": "- Lack of novelty: the authors do not cite \u201cDeep Generative Symbolic Regression with Monte-Carlo-Tree-Search\u201d by Kamienny et al. The latter also performs MCTS for SR with guidance from a Transformer and should be cited \u2014 according to me, their method performs much better and is better validated empirically. There are also many other missing references in the SR literature.\n- Experimental validation is poor. For example, the authors only report results when sampling only 20 points in the interval [-1,1], which is very small. They do not evaluate on the mainstream benchmark SRbench.\n- Paper is particularly poorly written and presented, as detailed below."
                },
                "questions": {
                    "value": "Important comments:\n- I don\u2019t understand what the \\hat x_ij means in Eq 6; it does not seem to be defined anywhere. In symbolic regression, one typically predicts the labels \\hat y from the inputs x_ij, but I don\u2019t see how one can \u201cpredict the inputs\u201d\u2026 This is very important as the authors consider this new loss function to be among their main contributions.\n\nComments on presentation:\n- Many sentences are not capitalised \n- Many sentences are cut with inappropriate punctuation (e.g. \u201cwhich cleverly combines LSTM and MCTS. And outperforms several baselines\u201d or \u201cthereby avoiding situations where each symbol is predicted with a similar probability. Improved the search efficiency of the algorithm.\u201d)\n- References are not separated from the text with a space\n- Fig 3 is poorly described: what is the red line in panel (a) ? What exactly is plotted in panel (c) (what is compressive strength) ?\n- Lack of details in many parts: \n    - \u201c\"No constrain\" means no constraints are applied\u201d, what are these constraints ?\n    - Table 2 needs more details (\u201cYes/No\u201d->\u201dWith entropy regularisation\u201d/\u201cWithout\u201d, \u201cTime\u201d->\u201dTraining time\u201d etc)\nTypos : \n- \u201ctimes it is child\u201d \n- \u201cwith the following expression:6\u201d\n\nOther things:\n- \u201cAnti-noise\u201d->\u201ddemonising\u201d\n- \u201cthe algorithm\u2019s reward function fluctuation is illustrated in the line graph (convergence proof)\u201d : reward vs time is by no means a convergence proof\u2026\n- The computations after Eq. 7 do not make any sense : the partial derivatives are indicated as positive or negative without any justification on the range of the variables. Moreover, dy/dx>0 does not mean y is \u201cproportional\u201d to x."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6810/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6810/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6810/Reviewer_SBg1"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6810/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697636156457,
            "cdate": 1697636156457,
            "tmdate": 1700648169724,
            "mdate": 1700648169724,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HwGc24h2ks",
                "forum": "1iKydVG6pL",
                "replyto": "UxOMudYbcJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6810/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6810/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the esteemed reviewer."
                    },
                    "comment": {
                        "value": "Dear reviewers, thank you very much for your review.\n\n**Weaknesses-1:**\n\n1, There are essential differences between our algorithm and DGSR-MCTS in the way the policy network is trained. First, DGSR-MCTS trains the policy network inspired by the mutation in the GP algorithm, samples the expression as described in Figure 1 (by random mutation), and then predicts the following sequence with the previous sequence. A policy network is classically trained as a sequence-to-sequence encoder-decoder, using a cross-entropy loss. Instead, we train the policy network with the probability of $\\pi$, normalized by the number of times MCTS visited each child node during the search (since MCTS tends to make nodes on the correct path more visited). We make the output of the policy network as close as possible to the learning outcome $\\pi$ of MCTS; therefore, our training process is a numerical regression problem. And the updated policy network can give better guidance to MCTS. In our algorithm, the policy network and MCTS are coupled, helping each other and making progress together. Moreover, the training data of DGSR-MCTS is obtained using a similar random mutation, which has great randomness. However, our method is the real data obtained by MCTS through multiple simulations, which is more reliable and stable.\n\n2,  Pretrained models are less general and are only good for variables and symbols that you have seen in the dataset. For example, if we only have [sin, cos] in our training set, it will not perform well when our symbol base becomes [sin, cos, exp]. When the variables are only $x_1$, and $x_2$, then it will be at a loss when encountering $x_3$, and $x_4$, and even when the sampling interval of variable X changes, it may affect the performance of the algorithm. However, our algorithm does not suffer from these problems, and our symbol library and the number of variables can be added or deleted at will. Moreover, X can be sampled in any interval.\nTo sum up, we are very different from DGSR-MCTS, and our algorithm is more versatile and flexible than DGSR-MCTS. Moreover, our target data is more instructive in the training process of the policy network. In our algorithm, the policy network and MCTS are coupled together and make progress together in helping each other.\n\nWe will cite DGSR-MCTS in the related work section of the paper, as well as add some SR articles. Thanks again for your review.\n\n**Weaknesses-2:**\n\nFirst of all, I am very sorry for the wrong expression of this sentence in our article. Thank you very much for your careful review and criticism. We will correct this sentence. We do not only sample 20 points on [-1,1]. Please refer to the appendix for the specific sampling interval and sampling number. Thanks again. As for the SRbench you said, first of all, SRbench mainly contains two parts of data, with concrete expressions and without concrete expressions. Since the evaluation criterion of our algorithm is the full recovery rate, it is not a simple comparison $R^2$. So we only selected the part of SRbench with concrete expressions (AIfeyman dataset). We have tested 222 mainstream formulas, which I think can reflect the performance of our algorithm comprehensively. Thank you very much again for your review.\n\n**Questions-1:**\n\nWe propose a new loss function (Used to calculate rewards) called $S_{NRMSE}$. This innovation is rarely considered in previous papers. In the symbolic regression algorithm, when dealing with multivariate problems, if a variable is smaller than other variables, or the correlation between several variables is relatively high. At this time, the symbolic regression algorithm is prone to the problem of missing variables. Missing variables are not allowed because we want to fully recover the original expression. To solve this problem, we propose a new loss function $S_{NRMSE}$,\n$$\n\\mathcal{S_{NRMSE}} = \\frac{1}{\\sigma_{y}}\\sqrt{\\frac{1}{\\mathcal{N}}\\sum_{i=1}^{\\mathcal{N}} (y_{i} - \\hat{y_i})^2 }  + \\lambda \\sum_{j = 1}^{m}\\frac{1}{\\sigma_{x_j}} \\sqrt{\\frac{1}{\\mathcal{N}}\\sum_{i=1}^{\\mathcal{N}} (x_{ji}-\\hat{x_{ji}})^2 }\n$$\nIn $S_{NRMSE}$, instead of considering the difference between the true value y and the predicted value $y_{pred}$, we consider the difference between the true point and the predicted point in the geometric space. Suppose we have three variable $x_1, x_2, x_3 $, and forecasting formula is only $x_1, x_3 $so at this point, $\\hat {x_1} = x_1, \\hat {x_2} = 0, \\hat{x_3} = x_3 $. In this case, the loss is large because of the lack of $x_2$. Where $x_{ji}$denotes the $i^{th}$ element of the $j^{th}$ variable.  We add the regulation coefficient $\\lambda$ to the NRMSE between x and $x_{pred}$.  In the ablation experiments (Fig. 3(a)), we can see that $S_{NRMSE}$ can significantly improve the full recovery rate of the algorithm.\n\n**Questions-2:**\n\nThank you very much for your careful reading and review. We will definitely improve the deficiencies mentioned in this part. Thanks."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699767053366,
                "cdate": 1699767053366,
                "tmdate": 1699767053366,
                "mdate": 1699767053366,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UMIr2yTZ0E",
                "forum": "1iKydVG6pL",
                "replyto": "HwGc24h2ks",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6810/Reviewer_SBg1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6810/Reviewer_SBg1"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your response !\n- W1: These are fair points, and I think it is important to add them to the paper. Thanks!\n- W2: Indeed, this sentence is particularly misleading! \n- Q1: Sorry but I still do not understand precisely what $\\hat x$ means in this loss function. I strongly encourage the authors to formalize this better (the example with three variables is simply not clear enough -- what does \"forecasting formula is $x1, x3$ mean? please use precise mathematical language), and add this to the manuscript.\n\nFor now, I will increase my score up to 3. If the authors manage to explain better the loss function, I will consider bumping it up higher."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560998507,
                "cdate": 1700560998507,
                "tmdate": 1700560998507,
                "mdate": 1700560998507,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TscOYkn3IR",
                "forum": "1iKydVG6pL",
                "replyto": "UxOMudYbcJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6810/Reviewer_SBg1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6810/Reviewer_SBg1"
                ],
                "content": {
                    "title": {
                        "value": "Answer"
                    },
                    "comment": {
                        "value": "I'm sorry but this really does not make any sense to me.\n- Why would the penalty for missing out $x_i$ always be $\\sum (x_i)^2$, independently of the particular way we missed out the variable $x_i$ ?! There are a vast number of ways one could \"miss out\" a variable, and the loss should be depend on this. For example, missing out a $sin(x_i)$ term is very different to missing out an $exp(-x_i)$ term;\n- Why would the penalty depend on the magnitude of $x_i$ ? Missing a term $sin(x_i)$ should give the same penalty, irrespectively of the scale of the $x_i$ sample during training;\n- What happens if on the contrary, one predicts a term $x_i$ which isn't in the ground truth ? This loss does not handle this case at all.\n\nIn summary, this loss function is not principled at all in my opinion. I would like to hear the opinion of other reviewers or AC on this, but for now I will keep my score at 3."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586895356,
                "cdate": 1700586895356,
                "tmdate": 1700587037867,
                "mdate": 1700587037867,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wnAzZzE5Ze",
            "forum": "1iKydVG6pL",
            "replyto": "1iKydVG6pL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6810/Reviewer_YgjN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6810/Reviewer_YgjN"
            ],
            "content": {
                "summary": {
                    "value": "The submission examines the performance of an AlphaZero-like approach, which they call AlphaSymbol, to the symbolic regression problem."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I'm not very familiar with the symbolic regression, but I'm not aware of AlphaZero having been applied to this problem setting."
                },
                "weaknesses": {
                    "value": "The formatting of the submission makes it hard to read. There is not sufficient space between the paragraphs. There is clearly content that can be cut from the submission to make it easier to read. For example, the four phases of MCTS do not need to be enumerated in the introduction.\n\nThe structure and contextualization of the submission is poor. The submission is essentially applying AlphaZero to a new setting with problem-specific tweaks. However, the submission is written as if the AlphaZero methodology is largely original to the submission: AlphaGo Zero is cited one time for the definition of a running action value and AlphaZero is not cited at all. This lack of proper attribution is alone enough to disqualify the submission from acceptance. The appropriate way to structure the submission would be to include AlphaZero in a background section and describe problem specific tweaks in a methodology section.\n\nThere are also some strange deviations from AlphaZero that make me skeptical of whether the results should be taken seriously. For example, in equation (4), the submission seems to suggest that it uses the normalized logarithm of the visit counts as the policy (though it gives contradictory information elsewhere in the submission). If it is true that the submission is using the logarithm of the visit counts, it ought to better justify this modification (though I am skeptical that a justification exists). Also, it adds an entropy penalty to the loss function that is not typically present. The submission does ablations which seem to suggest that this entropy penalty is helpful. But these lead me to wonder whether this penalty is only necessary because of other unusual choices made by the submission. Overall, it is certainly possible that the submission's deviations from AlphaZero are necessary to achieve good performance, but the submission's poor presentation leaves the reader with the feeling that these deviations are haphazard rather than the product of careful study."
                },
                "questions": {
                    "value": "> Think of the things where a response from the author can change your opinion\n\nI think the submission requires significant revisions to improve readability, appropriately separate background from contribution, and discuss and investigate the reasoning behind deviations from AlphaZero."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6810/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698536159946,
            "cdate": 1698536159946,
            "tmdate": 1699636787311,
            "mdate": 1699636787311,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PBPi3lTWdk",
                "forum": "1iKydVG6pL",
                "replyto": "wnAzZzE5Ze",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6810/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6810/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the esteemed reviewer"
                    },
                    "comment": {
                        "value": "Dear reviewers, thank you very much for taking time out of your busy schedule to review my article.\n**Weaknesses-1:**\n\nDear reviewers, we will carefully cut and improve our article according to your requirements. The four stages of MCTS are included in the introduction so that readers who are not familiar with the MCTS process can better understand the article.\n\n**Weaknesses-2:**\n\nFirst of all, I apologize to the authors of AlphaZero[1], and to you, for forgetting to cite them in our article. We do not mean not to quote, please trust us. It would be extremely unwise for us to hide the fact that we are borrowing from AlphaZero by not citing its well-known work. Furthermore, our algorithm is called Alphasymbol to remind you of our connection to the Alpha family of algorithms. So trust us, we're not intentionally not citing AlphaZero articles. We will definitely clarify the relationship between AlphaZero and our algorithm in the final submission as per your request. Thanks again for your review and suggestions.\n\n[1] : Silver D, Hubert T, Schrittwieser J, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm[J]. arXiv preprint arXiv:1712.01815, 2017.\n\n**Weaknesses-3:**\n\nDear reviewers. We just use the log normalization of the access count in the self-search phase as the basis for selecting the next symbol. However, in the MCTS search process, we apply the UCT in Equation 1 to select the next node symbol.\n\nIn AlphaGo zero the article [1], $\\ pi_ {a_i} = N (s, a_i) ^ {1/ \\tau} / (\\sum_0 ^ N (N (s, a_j) ^ {1/ \\tau})) $. In Eq. 4, we use a small trick by taking log first and then normalizing it. The advantage is that the data can be transformed into a logarithmic scale to prevent the N value from fluctuating too much and affecting the training stability of the LSTM.\n\nFor the problem of entropy loss, we want LSTM to provide effective guidance for MCTS. Then we want the LSTM to be as \"confident\" as possible. For example, for the following two probability distributions, [0.25, 0.25, 0.3, 0.2] and [0.1, 0.1,0.7, 0.1], although both of these probability distributions ultimately choose the third symbol, we would prefer that the LSTM output the second probability distribution. Because it's more instructive. This operation can be proved to be effective by ablation experiments and Table 2. Thank you very much for your review.\n\n[1] : Silver, D., Schrittwieser, J., Simonyan, K. *et al.* Mastering the game of Go without human knowledge. *Nature* **550**, 354\u2013359 (2017). https://doi.org/10.1038/nature24270\n\n**Questions**\n\nDear reviewers, in the final version, we will carefully revise the article according to your and other reviewers' opinions, so as to improve the readability of the article. Once again, thank you for reviewing our article and wish you a happy life."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699796858783,
                "cdate": 1699796858783,
                "tmdate": 1699796858783,
                "mdate": 1699796858783,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gdeJckieUX",
                "forum": "1iKydVG6pL",
                "replyto": "ba65vNXhDd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6810/Reviewer_YgjN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6810/Reviewer_YgjN"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for the reply. I think I would've needed to see a revised version of the submission to have considered the possibility of changing my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588270505,
                "cdate": 1700588270505,
                "tmdate": 1700588270505,
                "mdate": 1700588270505,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Kp1jgL6nY1",
            "forum": "1iKydVG6pL",
            "replyto": "1iKydVG6pL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6810/Reviewer_Fmza"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6810/Reviewer_Fmza"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers using a Monte Carlo Tree Search variant for discovering mathematical formulas. The MCTS variant uses PUCT for selection with an LSTM network providing the prior.\nThe empirical evaluation show that the algorithm is competitive with the state-of-the-art on several benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The empirical results do show that the proposed algorithm can be a powerful tool for discovering mathematical formulas."
                },
                "weaknesses": {
                    "value": "The proposed algorithm is a fairly standard MCTS, LSTM being the only slight deviation from a standard architecture used in games."
                },
                "questions": {
                    "value": "Since the main deviation from the standard MCTS implementation (that uses deep neural networks as priors) is the use of LSTM, it would have been useful to explore the possible alternative architectures. LSTM seems a reasonable choice given previous suitability to formula discovery, but have you tested other architectures as well?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6810/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766307216,
            "cdate": 1698766307216,
            "tmdate": 1699636787119,
            "mdate": 1699636787119,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MUao0OnSb9",
                "forum": "1iKydVG6pL",
                "replyto": "Kp1jgL6nY1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6810/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6810/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the esteemed reviewers"
                    },
                    "comment": {
                        "value": "Dear reviewers, thank you very much for your careful and conscientious review.\n\nIn theory, other neural networks that process sequences can be used as well, but the big advantage of LSTM is that it has a \"forget gate\", which automatically selects the information to memorize during the learning process. In this way, when we only input the parents and siblings of the node to be predicted, the LSTM can remember the important information and discard the unimportant information from time to time. However, like Transformers, we might have to input all the nodes before the node we want to predict each time.  such as for sequence [1,2,3,4,5,6], for LSTM, we can enter [1,2] to predict [3], then [2,3] to predict [4]...  Although each input is a part of the sequence, the LSTM automatically remembers important information from each previous input sequence. With the transformer, we need [1,2] to predict [3], and then [1,2,3] to predict [4]...  Each time, you enter all the previous nodes.\n\nIn summary, other sequence prediction algorithms can also be theoretically used for our algorithm, but we think LSTM may be more suitable for some.\n\nOnce again, thank you very much for your careful review. Thank you very much. I wish you every success in your work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699768763999,
                "cdate": 1699768763999,
                "tmdate": 1699768763999,
                "mdate": 1699768763999,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E0CB6U7Fwg",
                "forum": "1iKydVG6pL",
                "replyto": "MUao0OnSb9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6810/Reviewer_Fmza"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6810/Reviewer_Fmza"
                ],
                "content": {
                    "comment": {
                        "value": "Indeed LSTM seems a reasonable choice, but my question was about testing. It seems not, but it was not a major issue."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648137293,
                "cdate": 1700648137293,
                "tmdate": 1700648137293,
                "mdate": 1700648137293,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cB1Jspm77c",
            "forum": "1iKydVG6pL",
            "replyto": "1iKydVG6pL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6810/Reviewer_27mA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6810/Reviewer_27mA"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents AlphaSymbol, a new approach for symbolic regression for the discovery of mathematical formulae. The proposed approach augments a monte-carlo tree search with an LSTM to guide the search, a new reward function that addresses the problem of variable omission, and a new loss function for training the LSTM such that it produces distributions with lower information entropy. The experiments show that the proposed approach has a significantly higher recovery rate compared to the baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths:\n- Important and well-motivated problem (symbolic regression for discovering mathematical formulae)\n- New approach for the problem that consists of using LSTM to guide the monte-carlo tree search, as well as using a specialized reward function and a specialized loss function for training the LSTM\n- Experiments show significantly higher recovery rate compared to the baselines"
                },
                "weaknesses": {
                    "value": "Weaknesses:\n- Evaluation of experiments is not entirely clear: When is the search stopped and counted as \"not recovered\"?\n- No comparison of running times between the proposed approach and the baselines. Or alternatively, comparison of rewards over time vs. the baselines.\n- Some details about the technical approach is not entirely clear:\n\t* It is not clear how is the self-search phase and the use of LSTM are coordinated. For example, is the self-search used for several epochs while LSTM is being trained and then the algorithm changes to using the trained LSTM (if so, when is the change done)? \n\t* There are two loss functions. Is the second one (S_{NRMSE}) only used for the reward computation (while the first one is used for the LSTM training)?\n- Writing can be improved as some details are missing (examples above), format is quite dense with some subtitles appear inside a paragraph (e.g., \"Ablation experiment for information entropy.\"), and several typos and inconsistencies (examples listed under \"Minor issues\" below). The appendix is used as part of the paper, simply transferring some figures there and referencing to them as if they are part of the main paper, which hinders the ability of the paper itself to be self-contained without the appendix and hurts the readability of the paper. Section 5 is entitled \"Discussion\" but reads much more like a \"Conclusion\".\n\n\nMinor issues:\n- in abstract: \"MCTS and LSTM hand in hand advance together, win-win cooperation until the target expression is successfully determined\" - this is a bit too informal and can be rephrased to be a bit more precise/clear.\n- \"which is not interpretable and analyzable\": there are many post-hoc interpretation techniques that can be applied\n- \" visit count N increase\": what is N?\n- \"regression. however\" -> \"regression. However\"\n- Section 4: the description of algorithms as \"excellent\", \"superior\" is not clear (is excellent better than superior?). It is also important to highlight the current state-of-the-art on this task.\n- \"method. the expression 5 shows\" -> \"method. Expression 5 shows\"\n- \"matrixE.1,\""
                },
                "questions": {
                    "value": "Please see \"weaknesses\" above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6810/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698883242796,
            "cdate": 1698883242796,
            "tmdate": 1699636786995,
            "mdate": 1699636786995,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dy3FGxHbCY",
                "forum": "1iKydVG6pL",
                "replyto": "cB1Jspm77c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6810/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6810/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the esteemed reviewers"
                    },
                    "comment": {
                        "value": "Dear reviewers, thank you very much for taking time out of your busy schedule to review our manuscript.  I wish you every success in your work.\n\n**Weaknesses-1:**\n\nDear reviewers, we use \"full recovery rate\" in our paper. For example, for the expression $sin(x) + x$, the formula binary tree is expanded to [+, sin, x, x] according to the preorder traversal, that is to say, the algorithm must obtain an expression that is the same as $sin(x) + x$to be restored. That is, the sequence [+,sin,x,x] is generated one by one. Secondly, in the algorithm, we will specify a threshold of the maximum number of expressions searched in the self-search phase, for example, 1000. If the target expression is not found after finding 1000 expressions in the self-search phase, the search will stop and it will be regarded as failing to recover the expression.\n\n**Weaknesses-2:**\n\nDear reviewers, due to time reasons, we only test the running time (in seconds) of each algorithm on the Nguyen dataset, run each expression 3 times, and then take the average, and the results are as follows.\n\n| Dataset   | AlphaSymbol | DSO     | SPL    | GP      |\n| --------- | ----------- | ------- | ------ | ------- |\n| Nguyen-1  | 14.22       | 7.05    | 18.29  | 87.55   |\n| Nguyen-2  | 115.34      | 96.59   | 188.23 | 236.27  |\n| Nguyen-3  | 132.64      | 108.88  | 316.35 | 443.20  |\n| Nguyen-4  | 268.42      | 222.41  | 589.54 | 857.35  |\n| Nguyen-5  | 624.45      | 1647.05 | 831.28 | 1087.55 |\n| Nguyen-6  | 136.24      | 99.79   | 174.61 | 236.27  |\n| Nguyen-7  | 36.24       | 808.38  | 84.99  | 1443.20 |\n| Nguyen-8  | 1.22        | 3.41    | 1.23   | 3.35    |\n| Nguyen-9  | 17.18       | 13.05   | 24.14  | 87.55   |\n| Nguyen-10 | 38.28       | 1299.79 | 53.24  | 1236.27 |\n| Nguyen-11 | 62.44       | 108.88  | 80.73  | 443.20  |\n| Nguyen-12 | 323.42      | 2857.35 | 487.90 | 2444.63 |\n| Average   | 147.5       | 606.05  | 237.54 | 717.23  |\n\n**Weaknesses-3-1:**\n\nFirst of all, we will perform multiple MCTS simulations in the self-search phase. For example, we have a total of three symbols [sin,cos,x], the currently selected symbol is [sin], and we want to predict the next symbol. After multiple simulations with MCTS (with LSTM guidance), of the three children of [sin], x has been visited 8 times, sin has been visited once, and cos has been visited once. At this point, we know that sin followed by x is probably the right choice, so we need to teach the LSTM to remember this lesson. So let's say $\\pi = softmax([1,1,8])$. The LSTM is trained with [sin] as input so that its output is closer to $\\pi$. The trained LSTM is then used to better guide MCTS to produce better $\\pi$ to train a more powerful LSTM network. This process continues until the end. During the self-search process, we will put {[sin], $\\pi$} training data like this in a library, the capacity of the library is [1000], if more than 1000, new in, old out. We take a batch of data from the library at a time to train the LSTM. As for when to replace the LSTM, after training the LSTM for a certain number of times, we will test the LSTM, specifically, let the LSTM with the new parameters guide MCTS to simulate multiple expressions, and if the maximum reward of the obtained expression is higher than that of the previous simulation, we will replace the old LSTM parameters.\n\n**Weaknesses-3-2:**\n\nDear reviewers, you are right, the first loss function is used to train LSTM; The second loss function, $S_{NRMSE}$, is used to compute the reward after the search yields an expression.  We will improve this in this article to make it more clear. Thank you again\n\n**Weaknesses-4:**\n\nDear reviewers, thank you very much for your careful review. In the final submission version, we will carefully improve the article one by one according to your requirements. Thank you very much for your valuable advice."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699781868086,
                "cdate": 1699781868086,
                "tmdate": 1699781868086,
                "mdate": 1699781868086,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wkJNGgSLl7",
                "forum": "1iKydVG6pL",
                "replyto": "cB1Jspm77c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6810/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6810/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Ask a respected reviewer for advice."
                    },
                    "comment": {
                        "value": "Dear reviewers, I do not know if you have any questions about our paper. If so, I sincerely hope you can raise them here, and we will be happy to answer your questions. Thank you again for taking time out of your busy schedule to review our manuscript. Wish you a happy life."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586921826,
                "cdate": 1700586921826,
                "tmdate": 1700586921826,
                "mdate": 1700586921826,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rnciIhA6ru",
                "forum": "1iKydVG6pL",
                "replyto": "dy3FGxHbCY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6810/Reviewer_27mA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6810/Reviewer_27mA"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comment. The results on runtime are interesting but should be provided for the other datasets. I think the processes described under Weaknesses-1 and Weaknesses-3-1 need to be explained more clearly in the paper and the formatting of the paper should be improved as mentioned before and by the other reviewers as well."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709978680,
                "cdate": 1700709978680,
                "tmdate": 1700709978680,
                "mdate": 1700709978680,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]