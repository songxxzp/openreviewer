[
    {
        "title": "Intrinsic Mesh CNNs"
    },
    {
        "review": {
            "id": "NzQgDrZDeu",
            "forum": "b0elDO9v31",
            "replyto": "b0elDO9v31",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3407/Reviewer_jqVq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3407/Reviewer_jqVq"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors relate three special cases of intrinsic convolutions on surfaces to the generic formulation in terms of gauge and template functions.\n\nThey further show how different choices of templates can influence the features learnt in conjunction with the weighting function, naming these choices prior as they may encode assumptions made about the surface properties the network can learn.\n\nFrom the formulation of Monti, they introduce a Dirac distribution prior taken as the zero variance limit of Gaussian functions. They characterise the set of learnable features for the Dirac prior and claim it is more general than others as it is \"not limited by an integral\".\n\nFinally, the authors conduct experimental evaluations using different choices of priors, taken to be the density functions of several usual probability distributions. In the experimental setting of learning dense point correspondences, they show the Dirac prior outperforms the other choices."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper relates three milestone formulations of intrinsic convolutions on surfaces to the general theory of convolution in the tangent space with a choice of gauge. The authors introduce a partial order to the set of priors through a notion of what prior is more powerful - i.e., can a prior learn the same features as another."
                },
                "weaknesses": {
                    "value": "The authors focus on a limited body of work in the geometric deep learning literature, namely, three early formulations of intrinsic convolutions. Furthermore, they only cite one reference regarding all the theoretical framework of gauge equivariant CNNs, Bronstein et al., 2021. Combined, these two factors mean the paper ignores important references on gauge equivariant CNNs published before (Cohen et al.) or at the same time (Weiler et al.) the book cited, as well as other related works on convolutions for surfaces and 3D shapes (e.g., Tangent Convolutions for Dense Prediction in 3D in CVPR 2018, MeshCNN, and others).\n\nThe mathematical derivations contain typos (the limit of Gaussian functions for decreasing variance should be for n -> 0 not n -> +oo), and the use of the notation $\\Delta_{\\theta}$ and $\\Delta_{\\theta, w}$ should be introduced. The authors should introduce the change of integration domain from [0, 1]^n to BR(0) before Theorem 1. The theorem itself is a direct application of Fubini's theorem and as such is not a particularly strong theoretical result of the paper.\n\nAs noted by the authors themselves, the Dirac distribution is not a function. This weakens the link with the theoretical results presented before.\n\nThe authors claim to \"we show that existing definitions for surface convolutions only differ in their prior assumptions about local surface information\". It is known in the community, and the authors show it for the formulation of Monti only, who had already shown their formulation encompasses the previous Geodesic CNN and Anisotropic CNN.\n\nFinally, the experimental evaluation yields important questions (detailed below)."
                },
                "questions": {
                    "value": "Can the theoretical framework be strengthened by reformulating it in terms of distributions and extended to the class of test functions?\n\nThe network architecture used in the experiments uses angular max pooling. Can the authors clarify why this is needed?\nIsn't it a step backwards compared to the work of Monti, or other mesh CNNs such as FeastNet that do not require angular max pooling?\n\nHow did the authors choose the priors compared against the Dirac prior?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3407/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3407/Reviewer_jqVq",
                        "ICLR.cc/2024/Conference/Submission3407/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3407/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698710202060,
            "cdate": 1698710202060,
            "tmdate": 1700941298883,
            "mdate": 1700941298883,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x1zGgXZTHx",
                "forum": "b0elDO9v31",
                "replyto": "NzQgDrZDeu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3407/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are very grateful for the provided review. In the following, we would like to address the points and questions.\n\n> The authors focus on a limited body of work in the geometric deep learning literature, namely, three early formulations of intrinsic convolutions. Furthermore, they only cite one reference regarding all the theoretical framework of gauge equivariant CNNs, Bronstein et al., 2021.\n\nThe theoretical contribution of our work focuses on the discovery of previously unknown but insightful\nproperties of intrinsic surface convolutions which depend only on the definition of priors and is as such\nmore fundamental and independent of gauge transformations. Therefore, while we think our theory\nalso applies to gauge-equivariant intrinsic surface convolutions, this topic exceeds the scope of this\nwork.\n\n> The mathematical derivations contain typos (the limit of Gaussian functions for decreasing variance should be for $n \\to 0$ not $n \\to \\infty$, and the use of the notation $\\Delta_\\theta$ and $\\Delta_{\\theta, w}$ should be introduced.\n\nWe thank the reviewer for pointing out typos in our manuscript and will of course correct those, as well as add\nthe full definition for the anisotropic Laplacian $\\Delta_{\\alpha \\theta}$ into the appendix to ensure completeness.\n\n> As noted by the authors themselves, the Dirac distribution is not a function. This weakens the link with the theoretical results presented before.\n\nWhile mathematically possible to expand the domain of priors to transition kernels and with that\nallowing for several new priors such as the Dirac prior, this redefines the Riemann integrals into\nLebesgue integrals which come with a significant amount of technical overhead, as stated in the manuscript,\nand whose description would not help to understand our contribution.\nThereby, our choice of describing the Dirac prior allows to compare it to all previous models that use\npriors, which can be expressed in terms of Riemann integrals, for example by comparing their sets of\nlearnable features. This, in turn, again shows the potential of reformulating IMCNNs in terms of using a\nprior rather than a patch operator.\n\n> The authors claim to \"we show that existing definitions for surface convolutions only differ in their prior assumptions about local surface information\". It is known in the community, and the authors show it for the formulation of Monti only, who had already shown their formulation encompasses the previous Geodesic CNN and Anisotropic CNN.\n\nWe generalize the framework of Monti et al., 2017, thereby obviously allowing for the definition of\nmodels mentioned by Masci et al., 2015, and Boscaini et al., 2016, but additionally also the model\nof Bronstein et al., 2021, and new models that use priors in the sense Theorem 1.\n\n> Can the theoretical framework be strengthened by reformulating it in terms of distributions and extended\nto the class of test functions?\n\nWhile the discussion about distributions and test functions definitely is possible, we are convinced\nthat the choice of transition kernels is more natural as we think of signals on the manifold in a\nstatistical fashion. If we assume that our IMCNNs only learn test functions, we would restrict our\nresearch on a comparably limited domain.\n\n> The network architecture used in the experiments uses angular max pooling. Can the authors clarify\nwhy this is needed? Isn\u2019t it a step backwards compared to the work of Monti, or other mesh CNNs such\nas FeastNet that do not require angular max pooling?\n\nAngular max-pooling (AMP) is required as a means to resolve the angular coordinate ambiguity\nproblem, a direct consequence of gauge-ambiguity, in practice. It was also used by Monti et al., 2017.\nFeastNet (Verma et al., 2018), while representing interesting work, is a fundamentally different\napproach to surface convolutions.\n\n>  How did the authors choose the priors compared against the Dirac prior?\n\nThe normal distribution has also been used by Monti et al., 2017. The other priors portray valid instances\nfor our class of IMCNNs and were selected for further comparison.\n\n**Sources**\n> *Masci, Jonathan, et al. \"Geodesic convolutional neural networks on riemannian manifolds.\" Proceedings of the IEEE international conference on computer vision workshops. 2015.*\n\n> *Boscaini, Davide, et al. \"Learning shape correspondence with anisotropic convolutional neural networks.\" Advances in neural information processing systems 29 (2016).*\n\n> *Monti, Federico, et al. \"Geometric deep learning on graphs and manifolds using mixture model cnns.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.*\n\n> *Verma, Nitika, Edmond Boyer, and Jakob Verbeek. \"Feastnet: Feature-steered graph convolutions for 3d shape analysis.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.*\n\n> *Bronstein, Michael M., et al. \"Geometric deep learning: Grids, groups, graphs, geodesics, and gauges.\" arXiv preprint arXiv:2104.13478 (2021).*"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574266570,
                "cdate": 1700574266570,
                "tmdate": 1700574266570,
                "mdate": 1700574266570,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VDhADlcmF2",
            "forum": "b0elDO9v31",
            "replyto": "b0elDO9v31",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3407/Reviewer_eZPj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3407/Reviewer_eZPj"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes unifying the intrinsic spatial convolution on manifolds from Bronstein et al 2021 with the spatial convolution and patch operators in Monti et al 2017. The authors show that there is an implicit prior by connecting the two formulations and propose a class of intrinsic CNNs with different priors."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors describe previous work (Masci et al 2015, Boscaini et al 2016, Monti et al 2017) in detail and provide a good background of the different patch operators."
                },
                "weaknesses": {
                    "value": "-My primary concern with this paper is that it\u2019s not clear to me why it is called a prior and how using a different prior helps. It simply seems like different instances of the patch operator, akin to the Gaussian in Masci et al. 2015. \n\n-If the Dirac prior is the most expressive, why should one consider other priors? The results don\u2019t seem to show much difference between the other priors.\n\n-Are the shot descriptors orientation invariant? Does a global gauge exist?\n\n-Why was a gauge-equivariant method such as [1] not considered? It seems very relevant to the proposed method and has the advantage that the output transforms accordingly with the gauge transformations.\n\nReferences:\n[1] De Haan, P., Weiler, M., Cohen, T., & Welling, M. (2020, October). Gauge Equivariant Mesh CNNs: Anisotropic convolutions on geometric graphs. In International Conference on Learning Representations."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3407/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3407/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3407/Reviewer_eZPj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3407/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805270123,
            "cdate": 1698805270123,
            "tmdate": 1699636291797,
            "mdate": 1699636291797,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jVN66R0eqa",
                "forum": "b0elDO9v31",
                "replyto": "VDhADlcmF2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3407/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the provided feedback of the reviewer and would like to address the raised points.\n\n> My primary concern with this paper is that it\u2019s not clear to me why it is called a prior and how using a different prior helps. It simply seems like different instances of the patch operator, akin to the Gaussian in Masci et al. 2015.\n\nThe prior can be seen as a formal tool for encoding assumptions that you have about the signal on the surface.\nOur contribution formally describes the effects of priors. Key to the description is the introduction\nof the notion of *learnable features* in Section 4 of our manuscript. Depending on the chosen prior, the set of learnable\nfeatures of an IMCNN can be more or less comprehensive. Given a problem, like for example classification, we search for\nfeatures within the set of learnable features that cause the IMCNN to perform well. If the set of learnable\nfeatures is smaller, we expect that good features within that set are found faster. \nHowever, a small set of learnable features might exclude features which lead to better performances.\nLarger sets of learnable features, in turn, imply more unstable training as searching for good features\nbecomes more complicated due to the increased amount of possible features to learn.\n\n> If the Dirac prior is the most expressive, why should one consider other priors? The results don\u2019t seem to show much difference between the other priors.\n\nOur paper introduced the Dirac prior as a means to connect the framework of Monti et al., 2017, with the definition of intrinsic surface convolutions from Bronstein et al., 2021. We subsequently discovered that the Dirac prior is very comprehensive as it is associated to a large set of learnable features. This comes with a higher probability of finding more suitable features for the underlying problem, as indicated by the experiment results in which Dirac outperforms the other priors. However, this comes at the cost of a large search space. If unstable training is observed and domain knowledge is available, we have the option to limit our network onto a certain set of learnable features to stabilize training by using a more restrictive prior.\n\n> Are the shot descriptors orientation invariant?\n\nSHOT-descriptors are rotation-invariant as they are computed with the help of relative angles between\nvertex normals, which do not vary under the application of isometries. However, our work\ndoes not discuss SHOT-descriptors. Therefore, we kindly refer to the original paper (Salti et al.,\n2014) for further questions about SHOT-descriptors.\n\n> Does a global gauge exist?\n\nA global gauge does not exist in the general case. A gauge $\\omega$ defines a local basis for a tangent\nspace $T_u M$ with $u \\in M$ of the manifold $M$. In the general case, $T_u M$ and $T_v M$ are different vector spaces for\n$u \\not = v$, which already shows that a gauge usually cannot be globally equal for multiple tangent spaces.\nHowever, in the special case that the underlying manifold is parallelizable, we at least have a global\ncontinuously changing gauge.\n\n> Why was a gauge-equivariant method such as [1] not considered? It seems very relevant to the proposed method and has the advantage that the output transforms accordingly with the gauge transformations.\n\nThe theoretical contribution of our work focuses on the discovery of previously unknown but insightful\nproperties of intrinsic surface convolutions which depend only on the definition of priors and is as\nsuch more fundamental and independent from gauge transformations. Therefore, while we think our\ntheory also applies to gauge-equivariant intrinsic surface convolutions, this topic exceeds the scope\nof this work.\n\n**Sources**\n> *Salti, Samuele, Federico Tombari, and Luigi Di Stefano. \"SHOT: Unique signatures of histograms for surface and texture description.\" Computer Vision and Image Understanding 125 (2014): 251-264.*\n\n> *Monti, Federico, et al. \"Geometric deep learning on graphs and manifolds using mixture model cnns.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.*\n\n> *Bronstein, Michael M., et al. \"Geometric deep learning: Grids, groups, graphs, geodesics, and gauges.\" arXiv preprint arXiv:2104.13478 (2021).*"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570651679,
                "cdate": 1700570651679,
                "tmdate": 1700570651679,
                "mdate": 1700570651679,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "utEdkHrnWl",
                "forum": "b0elDO9v31",
                "replyto": "jVN66R0eqa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3407/Reviewer_eZPj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3407/Reviewer_eZPj"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the detailed response. I'm afraid I still do not see the value of this work. I still do not understand how the use of priors is important for understanding intrinsic surface convolutions and the experiments do not seem to elucidate any additional insights. I therefore maintain my original score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712133264,
                "cdate": 1700712133264,
                "tmdate": 1700712133264,
                "mdate": 1700712133264,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KxwhOTN8Tz",
            "forum": "b0elDO9v31",
            "replyto": "b0elDO9v31",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3407/Reviewer_u4PC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3407/Reviewer_u4PC"
            ],
            "content": {
                "summary": {
                    "value": "The authors attempt to make new connections between prior works on convolutions on meshes. They explore a novel parametrization of the convolution operation, and find that it performs worse than the typical parametrization."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- I appreciate the attempt of drawing new connections between prior works."
                },
                "weaknesses": {
                    "value": "I am afraid I do not understand the point of this work.\n\nAny linear map from a scalar signal $s: M \\to \\mathbb R$ on a $n$-manifold, to another scalar signal can be represented by a function $w : M \\times M \\to \\mathbb R$, via the integral $(s \\star w)(x) = \\int\\_M w(x, y)s(y)dy$. Typically, for a convolution-style operator, one does not want the parameters to depend on $x$, so one chooses a frame/gauge $\\omega$ on the manifold and arrives at the \"intrinsic manifold convolution\" of def 1 of the manuscript, with a parameter $t: \\mathbb R^n \\to \\mathbb R$.\n\nMonti et al (2017) [4] recognize that the gauge can not be chosen uniquely, so choose $J$ frames apply a convolution on each of choice of frame, with different parameters each. In other words, they choose a parameter function $t : \\mathbb R^n \\to \\mathbb R^J$.\nThe alternative approach suggested by [2,3] is to be equivariant to the choice of frame, leading to constraints on the parameters (and typically the use of non-scalar features such as vectors).\n\nHowever, what's done in theorem 1 in the equation of $(s \\star t)\\_{\\Delta \\theta, w}(u)$, is very different from any of the above. Instead of computing the output signal at one point by a single integral (over either the manifold or over the tangent plane), the authors compute a convolution as two integrals over the tangent plane.\nAlso, they use one parameterization $\\mathbb R^n \\to \\mathbb R$, and another parametrization $\\mathbb R^n \\times \\mathbb R^n \\to \\mathbb R$.\n\nThe authors appear to suggest that this is similar to what Monti et al (2017) [4] does, but this appears to me as very different. As the authors themselves note in theorem 1, this notation is completely redundant and can be reduced to a single integral. In fact, the equation for $(s \\star t)\\_{\\Delta \\theta, w}(u)$ appears to do two convolutions, with different parameters, which should indeed reduce to a single convolution. In the rest of sec 3, the authors make the unsurprising observation that if one of the two convolutions contain a Dirac delta, that convolution is an identity operation, and the double convolution reduces to just the second convolution.\n\nSo what's the point of analyzing the double convolution, which no one uses? It's very different from what [4] proposes, so how does it bridge a gap between anything?\n\nIn section 5, the authors are considering parametrizations of the double convolution different from the Dirac prior (thus the single convolution) and find that they perform worse. Also, as they involve a double integral, I suspect that they are much slower to compute.\n\nIn short, the authors didn't make any new connection between prior works, and proposed a different parametrization of the convolution that performed worse. In case I completely misunderstood the work, I look forward to your clarifications and will reconsider my opinion.\n\nOther points:\n- The authors cite [1] for the gauge equivariant convolution. This should be a citation to [2] and also [3] in the context of meshes.\n- The authors should compare to [3] in their experiments. Those authors found close to 100% correspondence at 0 geodesic distance for the the FAUST experiment, much better than the numbers reported in the present manuscript.\n- It's very confusing to use $w$ (w) and $\\omega$ (omega) in the same equation, the first referring to weights and the latter to the gauge. Please choose an alternative notation.\n\n\nRefs:\n- [1] Bronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Veli\u010dkovi\u0107. 2021. \u201cGeometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges.\u201d http://arxiv.org/abs/2104.13478.\n- [2] Cohen, Taco S., Maurice Weiler, Berkay Kicanaoglu, and Max Welling. 2019. \u201cGauge Equivariant Convolutional Networks and the Icosahedral CNN.\u201d http://arxiv.org/abs/1902.04615.\n- [3] De Haan, P., M. Weiler, T. Cohen, and M. Welling. 2020. \u201cGauge Equivariant Mesh CNNs: Anisotropic Convolutions on Geometric Graphs.\u201d https://arxiv.org/abs/2003.05425.\n- [4] Monti, Federico, Davide Boscaini, Jonathan Masci, Emanuele Rodol\u00e0, Jan Svoboda, and Michael M. Bronstein. 2016. \u201cGeometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs.\u201d http://arxiv.org/abs/1611.08402."
                },
                "questions": {
                    "value": "see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3407/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699305987267,
            "cdate": 1699305987267,
            "tmdate": 1699636291724,
            "mdate": 1699636291724,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "luwLwQuufO",
                "forum": "b0elDO9v31",
                "replyto": "KxwhOTN8Tz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3407/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the undertaken efforts and want to address the raised concerns.\n\n> Any linear map $s: M \\to \\mathbb{R}$ from a scalar signal on a $n$-manifold, to another scalar signal can be represented by a function [...]\n\nThe formulation of the intrinsic manifold convolution follows a different intention. Intrinsic manifold \nconvolutions (Bronstein et al., 2021) represent a generalization from standard Euclidean convolutions\nto convolutions on Riemannian manifolds. For this we require a notion of relative direction between\npoints within the manifold. Bronstein et al., 2021, proposes to use tangent vectors. We \nassociate tangent vectors to coordinates by selecting one from multiple possible gauges for each\ntangent plane. Eventually, we map said tangent vectors via the exponential map\nonto the surface to retrieve manifold points on which the surface signal is defined.\n\n> Monti et al (2017) [4] recognize that the gauge can not be chosen uniquely, so choose $J$ frames apply [...]\n\nMonti et al., 2017, proposes a general algorithmic framework for the construction of previous geometric CNN\narchitectures such as, but not limited to, geodesic CNNs (Masci et al., 2015). Within said framework, when\nspecifying for the geodesic CNNs, $J$ refers to the amount of neighbors around surface point $x$,\nat which we calculate the convolution. In the continuous case, the sum becomes an integral over the disc\n$B_R(\\textbf{0})$, that contains geodesic polar coordinates which portray the input to the selected gauge. \nThereby, $R$ is pre-specified radius that typically refers to the injectivity radius of the earlier discussed\nexponential map.\n\n> However, what's done in theorem 1 in the equation of, is very different from any of the above. [...]\n\nOur convolution is equivalent to how Monti et al., 2017, and Bronstein et al., 2021, would compute\nintrinsic surface convolutions. We have shown this in Theorem 1. The proof is provided in the appendix\nand comes down to the application of Fubini\u2019s theorem. The function $t : \\mathbb{R}^n \\to \\mathbb{R}$\nrepresents the template that shall be learned, the function $w : \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}$\nreferences a hand-crafted prior.\n\n> The authors appear to suggest that this is similar to what Monti et al (2017) [4] does, but this appears to me as very different. [...] So what's the point of analyzing the double convolution, which no one uses? It's very different from what [4] proposes, so how does it bridge a gap between anything?\n\nAs noted before, Theorem 1 states that the framework of Monti et al., 2017, is equivalent to our\nnotation. Selecting Dirac as a prior does not make the intrinsic surface convolution an identity\noperation, but returns the formulation for intrinsic surface convolutions of Bronstein et al., 2021.\nThat is, selecting Dirac as a prior within our reformulation of Monti et al., 2017, which we have\nproven to be equivalent, closes the theoretical gap between the algorithmic framework of\nMonti et al., 2017, and the theoretical derivation of the intrinsic surface convolution from\nBronstein et al., 2021.\n\n> The authors cite [1] for the gauge equivariant convolution. This should be a citation to [2] and also [3]\nin the context of meshes.\n\nWe are aware of the literature by Taco et al., 2019, and De Haan et al., 2020. We thank the reviewer\nfor pointing out that these citations are missing and will of course add them to the manuscript.\n\n> The authors should compare to [3] in their experiments. Those authors found close to 100 correspondence\nat 0 geodesic distance for the the FAUST experiment [...]\n\nThe theoretical contribution of our work focuses on the discovery of previously unknown but insightful\nproperties of intrinsic surface convolutions and not on extensions of them such as gauge-equivariant\nconvolutions.\n\n> It\u2019s very confusing to use $w$ (w) and $\\omega$ (omega) in the same equation, the first referring to weights and the\nlatter to the gauge.\n\nThank you for your feedback on the readability of our paper. We can rephrase $w(\u00b7)$ to $p(\u00b7)$, \nhighlighting that $w(\u00b7)$ does not contain trainable weights but represents a handcrafted prior.\n\n**Sources:**\n> *Masci, Jonathan, et al. \"Geodesic convolutional neural networks on riemannian manifolds.\" Proceedings of the IEEE international conference on computer vision workshops. 2015.*\n\n> *Monti, Federico, et al. \"Geometric deep learning on graphs and manifolds using mixture model cnns.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.*\n\n> *Cohen, Taco, et al. \"Gauge equivariant convolutional networks and the icosahedral CNN.\" International conference on Machine learning. PMLR, 2019.*\n\n> *De Haan, Pim, et al. \"Gauge equivariant mesh CNNs: Anisotropic convolutions on geometric graphs.\" arXiv preprint arXiv:2003.05425 (2020).*\n\n> *Bronstein, Michael M., et al. \"Geometric deep learning: Grids, groups, graphs, geodesics, and gauges.\" arXiv preprint arXiv:2104.13478 (2021).*"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565672941,
                "cdate": 1700565672941,
                "tmdate": 1700565672941,
                "mdate": 1700565672941,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wqjizA1xm6",
                "forum": "b0elDO9v31",
                "replyto": "KxwhOTN8Tz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3407/Reviewer_u4PC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3407/Reviewer_u4PC"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nI thank you for your reply.\n\nCan you refer exactly to where Monti [4] refers to $J$ as the number of neighbours, in the context of their proposed approach? Under equation (9) in [4], they write\n\n> $J$ represents the dimensionality of the extracted patch\n\nLater they write that there are $2Jd$ parameters. The index $J$ is thus a statement about their parametrization and has nothing to do with the number of neighbours. In fact, it's the number of mixture components in their CNN, thus it's a discrete index. I therefore fail to see why it makes sense to take that index to have values in the tangent space, and also fail to see why your description theorem 1 has anything to do with Monti's formulation.\n\nCould you please elaborate on why your formulation in theorem 1 is a sensible \"continuous version\" of Monti [4]?\n\n[4] Monti, Federico, Davide Boscaini, Jonathan Masci, Emanuele Rodol\u00e0, Jan Svoboda, and Michael M. Bronstein. 2016. \u201cGeometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs.\u201d http://arxiv.org/abs/1611.08402."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581113468,
                "cdate": 1700581113468,
                "tmdate": 1700581369944,
                "mdate": 1700581369944,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9QGuqNZBWF",
                "forum": "b0elDO9v31",
                "replyto": "jKBtH6adr2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3407/Reviewer_u4PC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3407/Reviewer_u4PC"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nI guess we have a very different notion of the word \"neighbour\". In my terminology, the set of neighbours of a point $x$, is the set $\\mathcal N(x)$, of points $y$ that are close to / connected to point $x$ on the manifold. You're also using this terminology in section 2.3.\n\nIn contrast, what you call \"neighbour\" in this discussion I'd call \"patch\", namely a region of the tangent space that we want to apply the same weight to. The operator $D_j(x)f$ computes the value of the $j$'th patch at position $x$ of signal $f$. Subsequently, the weight vector $g$ weighs the different patches.\n\nMonti [4] considers a finite number of patches, $J$. In my eyes, it's very different to take the space of patches to be uncountably infinite and parametrize it by the tangent space. While I understand your construction, I fail to see why this is the obvious \"the continuous version of the parametric patch operator from Monti\", as you write in theorem 1. As such, I fail to see why this draws a meaningful connection to the Gaussian mixture CNN proposed by Monti [4], and the generic intrinsic mesh convolution.\n\nThe only meaningful connection I can see it the opposite from the one you draw: to interpret the Gaussian mixture CNN as an instance of the intrinsic mesh CNN. If the pseudo-coordinates $u(x,y)$ are Riemannian normal coordinates, simply set the intrinsic mesh CNN template to be the mixture of Gaussians: $t(v)=\\sum_{j=1}^J g_j w_j(v)$."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653543114,
                "cdate": 1700653543114,
                "tmdate": 1700653543114,
                "mdate": 1700653543114,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "edGsBb0A5i",
                "forum": "b0elDO9v31",
                "replyto": "H8VoFTyoAO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3407/Reviewer_u4PC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3407/Reviewer_u4PC"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response. I understand that if the patches have a \"mean\", then the patch can be identified with this mean point, and you could call the patch a neighbour. In your formulation in thm 1, however, you never required your patch $p_v(y)$ to be a Gaussian-like patch with mean $v$, so I don't see why this interpretation holds in general. As such, I still find it confusing to call the patch parametrized by $v$ a \"neighbour\".\n\nAlso, my opinion is unchanged that that the authors' departure from a finite number of Gaussian patches (as used by Monti) to an infinite number of patches, makes for a wholly different architecture. I'm afraid I thus don't see this paper as proposing a meaningful connection between the convolution proposed by Monti and the intrinsic mesh convolution. My score remains unchanged."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672558695,
                "cdate": 1700672558695,
                "tmdate": 1700672558695,
                "mdate": 1700672558695,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "idxDALXp4f",
            "forum": "b0elDO9v31",
            "replyto": "b0elDO9v31",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3407/Reviewer_jCDn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3407/Reviewer_jCDn"
            ],
            "content": {
                "summary": {
                    "value": "The paper builds a connection between a practical implementation of mesh CNN (Monti et al., 2017) to Intrinsic Mesh CNN (Bronstein et al., 2021). By defining a template, Intrinsic Mesh CNN reduces to the mesh CNN defined by Monti et al. with a Dirac prior. The paper later experiments with different choices of priors and show that Dirac prior leads to better results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper connects a practical model with the theoretical framework of Intrinsic convolution on meshes.\n- The intuition behind the idea is straightforward and easy to understand.\n- The paper proves that a partial order exists for comparing these priors.\n- The paper shows some quantitative results to compare the difference between different priors."
                },
                "weaknesses": {
                    "value": "- Although by defining priors the paper builds a connection between theory and practice, the resulted model is not that useful. In particular, the Dirac prior, which corresponds to Monti et al., 2017, is still the best solution at least in the experiments in the paper.\n- Indeed, instead of decoupling $w_t(\\cdot)$ (the  into $w(\\cdot)$ (the prior) and $t(\\cdot)$ (the template) and learning the template, one may simply treat $w_t(\\cdot)$ as the learnable parameter. If we are allowed to discretize $w_t(\\cdot)$ with sufficient amount of parameters, parametering $w_t$ is flexible enough. Making a (not quite accurate) analogy to regular Euclidean CNNs: it seems to me what the paper presents is to pre-convolve some handcrafted $H$ with the original convolutional kernel $K$, with $H$ being something very restrictive in the sense of both capacity and optimization. It therefore does not surprise me too much that the Dirac prior (equivalent to directly parametering $w_t$) is better than all other variants.\n- Continuing the point above, I believe there is no reason not to learn the priors at the same time. And doing this may lead to some additional benefits in optimization.\n- The presentation seems a bit messy in the experiment section. Many descriptions can be simplified: for instance, I do not think one needs to write down the exact formula of cross entropy (I guess it is well known by the majority of the audience).\n- The benchmark seems to be quite toy. I feel that if the paper includes empirical results on the tasks presented by MeshCNN, it will be much more convincing.\n\n[1] Hanocka, Rana, et al. \"Meshcnn: a network with an edge.\" ACM Transactions on Graphics (ToG) 38.4 (2019): 1-12."
                },
                "questions": {
                    "value": "- How much time does the propose implementation with non-Dirac priors consume, compared to Monti et al., 2017?\n- How much difference does it make to change the hyperparameters, including the ones for the GPC coordinate systems and template discretization, in Table 1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3407/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699344989979,
            "cdate": 1699344989979,
            "tmdate": 1699636291645,
            "mdate": 1699636291645,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5BW6XYXoMh",
                "forum": "b0elDO9v31",
                "replyto": "idxDALXp4f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3407/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback. In the following, we would like to briefly address the raised points and questions.\n\n> Point 1:\n\nWhile it is true that earlier experiments report better training results, a direct comparison with\nthese experiments is misleading as they applied post-processing methods to the network output,\nsuch as functional maps in Masci et al., 2015, and Boscaini et al., 2016, or intrinsic Bayesian filters\nin Monti et al., 2017. Furthermore, the Dirac prior is a proposal of our work, which is fundamental\nto establish a connection between the theory of Bronstein et al., 2021, to the practical framework of\nMonti et al, 2017. Notably, it was not discussed in Monti et al, 2017.\n\n> Point 2 and 3:\n\nLearning the parameters directly has been suggested by Bronstein et al., 2021. Pre-convolving the\ntemplate $t(\u00b7)$ with a handcrafted kernel $w(\u00b7)$ has implicitly been done in what previously has been\nreferred to as the patch operator (Masci et al., 2015; Monti et al, 2017). Our paper proposes a\nreformulation of the parametric framework from Monti et al, 2017, which stresses the theoretical\nimplications of handcrafted kernels $w(\u00b7)$ by giving rise to the notion of learnable features. This\nnotion not only explains why the handcrafted kernel encodes prior knowledge, which in previous\nwork has merely been referred to as either interpolation coefficients, weighting function or kernel\n(Masci et al., 2015; Monti et al, 2017), but also allows us to observe a partial order on priors in\nthe context of intrinsic mesh CNNs. Therefore, we now know that different priors can vary in their\ncomprehensiveness. One very interesting example is given by our proposed Dirac prior, which on the\none hand, when inserted into our reformulation, yields the theoretical definition for intrinsic surface\nconvolutions of Bronstein et al., 2021, and on the other hand renders the pre-convolution irrelevant.\n\n> Point 4:\n\nIn order to simplify the repeatability of our experiments, we want to be precise about our experiment\nsetup. \n\n> Point 5:\n\nWhile we agree that there exist more interesting benchmarks, we want to clarify that our benchmark\nis no artificial toy benchmark. For instance, compared to Hanocka et al., 2019, our input meshes\nhave 41328 edges. Their benchmark used considerable smaller meshes with roughly 750 edges for \nmesh classification and roughly 2250 edges for mesh segmentation. Additionally, in difference to\nHanocka et al., 2019, we have trained our IMCNN to predict dense point-correspondences. Predicting\ndense point-correspondences is commonly regarded as a notoriously hard problem in the computer\nvision community (Van Kaick et al., 2011). Lastly, the benchmark is a common benchmark in the\nbackground papers (Masci et al., 2015; Boscaini et al., 2016; Monti et al, 2017) .\n\n> Question 1:\n\nThe Dirac-prior effectively gets rid of a matrix multiplication, therefore requiring less computation\nthan the IMCNNs which use a specific prior.\n\n> Question 2:\n\nIn order to get an intuition for what difference the hyperparameter make, it is helpful to take a\nlook on Figure 4 in the appendix. While it is difficult to quantify how much difference varying\nhyperparameter cause in general, we expect certain aspects to hold true. For instance, the GPC-\nsystem radius should be chosen smaller or equal to the injectvity radius of the exponential map.\n\n**Sources:**\n\n> *Van Kaick, Oliver, et al. \"A survey on shape correspondence.\" Computer graphics forum. Vol. 30. No. 6. Oxford, UK: Blackwell Publishing Ltd, 2011.*\n\n> *Masci, Jonathan, et al. \"Geodesic convolutional neural networks on riemannian manifolds.\" Proceedings of the IEEE international conference on computer vision workshops. 2015.*\n\n> *Boscaini, Davide, et al. \"Learning shape correspondence with anisotropic convolutional neural networks.\" Advances in neural information processing systems 29 (2016).*\n\n> *Monti, Federico, et al. \"Geometric deep learning on graphs and manifolds using mixture model cnns.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.*\n\n> *Hanocka, Rana, et al. \"Meshcnn: a network with an edge.\" ACM Transactions on Graphics (ToG) 38.4 (2019): 1-12.*\n\n> *Bronstein, Michael M., et al. \"Geometric deep learning: Grids, groups, graphs, geodesics, and gauges.\" arXiv preprint arXiv:2104.13478 (2021).*"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562635747,
                "cdate": 1700562635747,
                "tmdate": 1700562635747,
                "mdate": 1700562635747,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]