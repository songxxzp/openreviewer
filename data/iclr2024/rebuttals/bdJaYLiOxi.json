[
    {
        "title": "Radar Spectra-language Model for Automotive Scene Parsing"
    },
    {
        "review": {
            "id": "WSd599eN3T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5702/Reviewer_Bffg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5702/Reviewer_Bffg"
            ],
            "forum": "bdJaYLiOxi",
            "replyto": "bdJaYLiOxi",
            "content": {
                "summary": {
                    "value": "This paper presents a study to investigate vision-language models for scene understanding in automotive scenes. To this end, a benchmark is created. For autonomous driving scene understanding, the benefits for downstream object detection and free space estimation are discussed. Ablation studies are well conducted and the paper is well structured."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The presented work is one of the first to train a radar-language model for autonomous driving scene understanding.\n2. The paper is overall well structured."
                },
                "weaknesses": {
                    "value": "1. Please consider comparing your proposed model against some existing adapted language models for driving scene understanding.\n2. Please consider directly comparing your proposed model against some object detection and segmentation models to verify the superiority of your proposed model.\n3. Most of the components from the proposed framework are from existing works. It is hard to find any novel technical designs in the presented framework. Please better clarify the technical novelty and theoretical contributions of the presented work.\n4. Would you consider giving an overview of your proposed framework at the beginning of the methodology section, which can help the readers better understand the work?\n5. The computation efficiency should be discussed, which is critical in automotive scene understanding.\n6. The writing style and presentation quality could be further enhanced. In the introduction, the space between different paragraphs should be enlarged.\n\nSincerely,"
                },
                "questions": {
                    "value": "Would you consider presenting some visualization results of object detection and free space segmentation to qualitatively verify the effectiveness of your proposed method?\n\nWhile there are not many works on radar-language models, there are extensive works on adapting large-language models for driving scene understanding. Would you consider discussing the relations and differences between your work and existing works in the related work section? \n\nSincerely,"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5702/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5702/Reviewer_Bffg",
                        "ICLR.cc/2024/Conference/Submission5702/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5702/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697372727166,
            "cdate": 1697372727166,
            "tmdate": 1700630746677,
            "mdate": 1700630746677,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fdhJ2I2YPN",
                "forum": "bdJaYLiOxi",
                "replyto": "WSd599eN3T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5702/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Thank you for your comments and suggestions!** We address the issues raised below \n\n> R1:S1 The presented work is one of the first to train a radar-language model for autonomous driving scene understanding. \n\nIt is the first to train a radar spectra \u2013 language model, and the first to make use of the shared language model embedding space to shed light on the semantic content of radar spectra.  \n\nRadar spectra is a unique form of measurement \u2013 it captures the spatial reflection intensity of the electomagnetic signal and thus the geometric structure of the environment. But it also captures the reflection intensity along the Doppler axis, i.e. it is (radial) motion-aware, and the reflection intensity depends on the material properties, as well as geometry. It is clear that radar spectra carry richer information than the widely used sparse radar reflection points, but it is not quite known what kind of information can be extracted. The current work aims to provide a way to shed light on this. \n \n> R4:W1 Please consider comparing your proposed model against some existing adapted language models for driving scene understanding. \n\nA review of VLMs for automotive scene understanding was added to the related work section (edited part highlighted in blue). All methods are quite new - concurrent to our work. None of the reviewed methods is quite applicable to our setting of retrieval \u2013 where data is scarce and semantic ground truth is absent (and off-the-shelf VLM performance is unsatisfactory).  \n \n> R4:W2  Please consider directly comparing your proposed model against some object detection and segmentation models to verify the superiority of your proposed model. \n \nThe comparison of our proposed model to an existing radar object detection and segmentation model is included in Table 3. The baseline model in Table 3  is FFT-RadNet of Rebut et al. (2022), the SOTA for the RADIal dataset, we clarified this in the table and the corresponding section 4.3 (updated portions of the manuscript are highlighted in blue). \nFollowing this comment we added to the comparison SotA results from two additional papers on the same dataset.  \n\n> R4:W3  Most of the components from the proposed framework are from existing works. It is hard to find any novel technical designs in the presented framework. Please better clarify the technical novelty and theoretical contributions of the presented work. \n\nOur paper introduces a novel task of extraction of semantic information from radar spectra \u2013 an otherwise generally non human - interpretable modality, and introduces an approach to tackle it. Both of the above are the main novelties of the paper. It is fortunate, that the encoder of an existing object detection and segmentation network (FFT-RadNet Rebut et al. (2022)) can be used as radar encoder of the radar spectra language model. This way, not another specific architecture has to be designed for one specific use case. \n\nIn addition, we describe a novel process of generating captions for and fine-tuning an image-language model to extract general semantic information from AD scenes.  \n\nFinally, to validate our radar-spectra model we modify an object detection model acting on spectra by  injecting the embedding vector from our spectra encoder, and demonstrate it improves SotA on the RADIal dataset. \n\nThe goal of the work is to explore the semantic content of radar spectra, which is otherwise very difficult.  \n\nIn the updated paper version, we explain our contributinos in a clearer manner (highlighted by blue text color). We will try to clarify this further in the final version.  \n\n> R4:W4 Would you consider giving an overview of your proposed framework at the beginning of the methodology section, which can help the readers better understand the work? \n\nDone. Overview provided at the begining of the \"Proposed Approach\" Section 3.  \n\n> R4:W5 The computation efficiency should be discussed, which is critical in automotive scene understanding. \n\nWe agree that computational efficiency is an important property for models running on an embedded device. Nevertheless, the main focus of the paper is to a) get a better understanding of the content of radar spectra and b) to investigate the benefits of the learnt features for downstream tasks. The focus is not on computational efficiency and the method is not intended to be run on an edge device, but rather is a research tool.  \n\nIn other words, the current work aims to provide a tool to examine the semantic content of radar spectra, rather than provide a perception algorithm for an autonomous vehicle. For this reason, computational efficiency is not a major consideration."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560258360,
                "cdate": 1700560258360,
                "tmdate": 1700560258360,
                "mdate": 1700560258360,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0dtFQ4Yz2S",
                "forum": "bdJaYLiOxi",
                "replyto": "Z0iLc1z2Sa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5702/Reviewer_Bffg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5702/Reviewer_Bffg"
                ],
                "content": {
                    "title": {
                        "value": "Comment"
                    },
                    "comment": {
                        "value": "The reviewer would like to thank the authors for the added responses and clarifications, which helped solve some concerns.\n\nFor this reason, the reviewer would like to elevate the rating accordingly.\n\nSincerely,"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630720932,
                "cdate": 1700630720932,
                "tmdate": 1700630720932,
                "mdate": 1700630720932,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TCJgPXrsvj",
            "forum": "bdJaYLiOxi",
            "replyto": "bdJaYLiOxi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5702/Reviewer_9UPS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5702/Reviewer_9UPS"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a radar spectra-language model (RSLM). The RSLM is built upon CLIP with image as a bridge between radar and text. The RSLM is evaluated by a retrieval task and two downstream tasks. Experiments show that the RSLM has good zero-shot retrieval ability and can boost the performance of two downstream tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. To the best of my knowledge, this is the first paper trying to build a radar spectra-language model.\n2. The fine-tuned VLM for autonomous driving scenes works much better than the off-the-shell CLIP.\n3. The zero-shot retrieval ability of RSLM is impressive, especially for the small objects such as pedestrian and cyclist."
                },
                "weaknesses": {
                    "value": "1. The author seems to lack paper writing skills. All the figures are unaesthetic bitmaps with low resolution and some of the figures are not necessary. For Figure 4a, it is better to use formulation instead of python code to describe the loss functions. For Figure 4b, such a simple architecture may be put in the supplement material. \n2. Changing the position encoding without finetuning may cause performance drop, and splitting the image may break some objects on the edge. A better and more common way is to pad black pixels to the top and bottom of the image. \n3. For the detection and segmentation downstream tasks, it is better to show some cases that the pretrained model helps improve the performance, not just numbers.\n4. For autonomous driving tasks, the localization abilitiy is more important than the classification. Could you provide some visualizations such as attention map or GradCAM to see if the retrieved objects are corresponding to the right location?\n\nOther issues:\n1. For Equation 4, a period should be added in the end of the formula. \n2. All the quotation marks are single quotes. Please use backquote in front of the quoted phrases. \n3. Some of the RADIal is mistaken by RADiaL.\n\nOverall, the proposed model is novel and the zero-shot results are interesting and impressive. However, the writing and lack of discussion lowered the final score. I would like to increase my score if the authors would polish the paper and redraw all the figures using vectorgraph."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5702/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5702/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5702/Reviewer_9UPS"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5702/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804203541,
            "cdate": 1698804203541,
            "tmdate": 1700730105227,
            "mdate": 1700730105227,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l9mD5dZHkt",
                "forum": "bdJaYLiOxi",
                "replyto": "TCJgPXrsvj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5702/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> R3:W1 The author seems to lack paper writing skills. All the figures are unaesthetic bitmaps with low resolution and some of the figures are not necessary. For Figure 4a, it is better to use formulation instead of python code to describe the loss functions. For Figure 4b, such a simple architecture may be put in the supplement material. \n\nFigures were re-exported in vector format.  \n\nFigure 4 has been moved to the appendix and re-numbered to Figure 6. Formulation of loss functions was added in Section A.2.  \n\nThe reason we used python code was to be consistent with the CLIP paper Radford et al. 2021, which defined its InfoNCE loss with python code.  \n\n> R3:W2 Changing the position encoding without finetuning may cause performance drop, and splitting the image may break some objects on the edge. A better and more common way is to pad black pixels to the top and bottom of the image. \n\nWe always fine-tuned the model when using each of the adaptation methods we reported ( (1) positional encoding interpolation, (2) multiple square crops). \n\nWhen creating crops we made sure there's an overlap, hopefully mitigating effects like mentioned - of breaking an object on the edge, i.e. an object would usually appear in its entirety at least in one of the crops.  \n\nFollowing this comment we tried using zero-padding, which worsened results (average weighted accuracy in the experiment did not exceed 0.5).  \n\nWe believe that the reason is that images in the RADIal dataset have aspect ratio of nearly 1:2, while the network input is 244x244 square. In this case (resize and) padding results in half the input image being black, with effective image height just 122 pixels, and many details are lost \u2013 resulting in the observed performance hit.  \n\n> R3:W3 For the detection and segmentation downstream tasks, it is better to show some cases that the pretrained model helps improve the performance, not just numbers. \n\nWe added a new appendix section A.6 with visualization of object detection and segmentation, and a brief discussion.  \n\n> R3:W4 For autonomous driving tasks, the localization abilitiy is more important than the classification. Could you provide some visualizations such as attention map or GradCAM to see if the retrieved objects are corresponding to the right location? \n\nWe added a new appendix section A.4 with visualization of VLM activations using GradCAM before and after fine-tunning.  \n\n> Other issues: \n> - For Equation 4, a period should be added in the end of the formula. \n> - All the quotation marks are single quotes. Please use backquote in front of the quoted phrases. \n> - Some of the RADIal is mistaken by RADiaL. \n\nAll done. Thanks for pointing out!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552820819,
                "cdate": 1700552820819,
                "tmdate": 1700552820819,
                "mdate": 1700552820819,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SKdRPM1QZ3",
                "forum": "bdJaYLiOxi",
                "replyto": "l9mD5dZHkt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5702/Reviewer_9UPS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5702/Reviewer_9UPS"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the effort of authors. Most of my concerns are addressed except the presentation skills including figure drawing. I will slightly raise my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730086833,
                "cdate": 1700730086833,
                "tmdate": 1700730086833,
                "mdate": 1700730086833,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EdxILrAmab",
            "forum": "bdJaYLiOxi",
            "replyto": "bdJaYLiOxi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5702/Reviewer_H2ZX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5702/Reviewer_H2ZX"
            ],
            "content": {
                "summary": {
                    "value": "The author proposes Radar-Spectra Language Model (RSLM) to help interpret the difficult modality (by humans).\nIn addition, the modality also does not have many datasets. Thus the approach is to leverage the expressive power Vision Language Model (VLM), and train a radar encoder to mimic the features produced by VLM.\n\nRSLM first fine-tunes CLIP image encoder to road scenes (from self-driving car research). \nThe best image encoder for the task is OpenCLIP.\nTo connect radar spectra to the resulting CLIP features, RSLM trains a radar encoder to output features that are as similar as possible to the CLIP features.\nThe best radar encoder network is Feature Pyramid Network (FPN), and it is trained using MSE loss on retrieval tasks.\nThe resulting features then are inputted to a network that is trained on two downstream tasks: object detection and free-space estimation.\nThe object detection losses are focal and smooth-L1 loss, while free-space estimation is trained using BCE loss.\n\nRSLM is tested to find the optimal components, e.g. usage of OpenCLIP, FPN.\nIn addition, it also analyzes the performance of RSLM on object detection and free-space estimation.\nRSLM is able to surpass the baseline, FFT-RadNet, these two tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The radar spectrum pre-training to optimize on similarity to fine-tuned OpenCLIP is novel. It allows for pre-training without a need for explicit Radar-spectra dataset."
                },
                "weaknesses": {
                    "value": "No discussion on what is still hard to do or not reliable.\nAlso analysis of the varying the difficulty of the input scenes would help answer the previous question."
                },
                "questions": {
                    "value": "What self-driving related take would Radar-spectra be able to do well, while other modality cannot or struggle with?\n\nTypos:\nPg. 3, \"Prompt Generation\" section: (e.g. a photo of a {}) -- {} symbol should be replaced."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5702/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830954375,
            "cdate": 1698830954375,
            "tmdate": 1699636596241,
            "mdate": 1699636596241,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ICX2IvVH0y",
                "forum": "bdJaYLiOxi",
                "replyto": "EdxILrAmab",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5702/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Thank you for the valuable feedback!** We would like to answer your questions:  \n\n> R2:W1 No discussion on what is still hard to do or not reliable. Also analysis of the varying the difficulty of the input scenes would help answer the previous question. \n\nA discussion section was incorporated (page 10). The changes in the updated paper version are highlighted with blue text color. \n \n> R2:Q1 What self-driving related take would Radar-spectra be able to do well, while other modality cannot or struggle with? \n\nRadar has several advantages compared to other modalities, e.g., it is robust to difficult weather conditions like rain or sun glare, and it can directly measure the relative radial velocity of other objects. Moreover it can detect objects at large distances, depending on the sensor for example up to about 350m.  Nevertheless, the focus of the paper is not a comparison of radar to other modalities. We investigate the semantic content of radar spectra, which is otherwise difficult to interpret, unlike other modalities. \n \n> R2:Q2 Typos: Pg. 3, \"Prompt Generation\" section: (e.g. a photo of a {}) -- {} symbol should be replaced. \n\nDone! Thank you for pointing this out!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534156525,
                "cdate": 1700534156525,
                "tmdate": 1700534254404,
                "mdate": 1700534254404,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oJhZqEZp1F",
                "forum": "bdJaYLiOxi",
                "replyto": "ICX2IvVH0y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5702/Reviewer_H2ZX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5702/Reviewer_H2ZX"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you so much for the replies. After reading them, and other discussions, I would like to keep my ratings as is."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690173339,
                "cdate": 1700690173339,
                "tmdate": 1700690173339,
                "mdate": 1700690173339,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cCaNTKN8n0",
            "forum": "bdJaYLiOxi",
            "replyto": "bdJaYLiOxi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5702/Reviewer_RqMG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5702/Reviewer_RqMG"
            ],
            "content": {
                "summary": {
                    "value": "Radar sensors are integral to driver assistance systems and the future of autonomous driving due to their cost-effectiveness, long-range capabilities, and resilience to adverse weather. Typically, radar data is processed in point cloud format, but raw radar spectra contain more detailed information, though they are harder to interpret. This research focuses on enhancing radar spectra interpretability in the automotive context. It introduces a radar spectra-language model that enables natural language queries about scene elements within radar spectra. To address data scarcity, the study aligns the embedding space of a vision-language model. By fine-tuning for automotive scenes, it improves performance. This learned representation benefits scene parsing, enhancing free space segmentation and object detection when integrated into a baseline model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper introduces the text information into feature fusion for radar spectra interpretability."
                },
                "weaknesses": {
                    "value": "1.\tThe framework seems to be a simple combination of existing methods. I didn\u2019t see the specific design for the radar spectra language model. \n2.\tThe experiment of detection is not compared with SOTA methods such as RODNet.\n3.\tWhat is [20] in Table 3?\n4.\tIf the description includes multiple object information, how do you align the text information with the corresponding object?"
                },
                "questions": {
                    "value": "If the description is not accurate will the information mislead the model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concerns."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5702/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5702/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5702/Reviewer_RqMG"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5702/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837472279,
            "cdate": 1698837472279,
            "tmdate": 1699636596076,
            "mdate": 1699636596076,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n07DTYzT4F",
                "forum": "bdJaYLiOxi",
                "replyto": "cCaNTKN8n0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5702/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5702/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**We thank you for your valuable comments and suggestions!** Below we address the issues raised.  \n \n> R1:W1 The framework seems to be a simple combination of existing methods. I didn\u2019t see the specific design for the radar spectra language model. \n\nOur paper introduces a novel task of extraction of semantic information from radar spectra \u2013 an otherwise generally non human - interpretable modality, and introduces an approach to tackle it. Both of the above are the main novelties of the paper.  \n\nIn addition, we describe a novel process of generating captions for and fine-tuning an image-language model to extract general semantic information from AD scenes.  \n\nFinally, to validate our radar-spectra model we modify an object detection model acting on spectra by  injecting the embedding vector from our spectra encoder, and demonstrate it improves SotA on the RADIal dataset. \n\n> R1:W2 The experiment of detection is not compared with SOTA methods such as RODNet. \n\nand  \n\n> R1 :W3 What is [20] in Table 3? \n\nThank you for pointing this out! The reference in table 3 is the FFT-RadNet model from Rebut et al. (2022) - it has been fixed in the updated paper version. FFT-RadNet is the SOTA model for the RADIal dataset, to which we compare our proposed methods. We consider the RADIal dataset for the evaluation of the proposed radar spectra language model. Unfortunatelly, RODNet cannot be applied to this dataset directly, since RODNet is designed to work on a sequence of complex valued range-azimuth-spectra, the data provided in the CRUW dataset. In contrast, the RADIal dataset contains range-Doppler spectra, where the azimuth dimension has not been processed yet.  \n\nFollowing this comment we've added results of two more SotA methods for RADIal to the comparison. We've extended Sec. 4.3, in particular the ``Results\" paragraph to accomodate for the changes. Modified portions of the manuscript are highlighted in blue.  \n\n> R1:W4 If the description includes multiple object information, how do you align the text information with the corresponding object? \n\nIn this work, we formulate queries as descriptions on scene level, we do not align text information with a specific object explicitly. When generating captions, we have the entire information of the scene. In conclusion \u2013 the alignment problem does not arise, at least for the setting described in the paper.  \n\n> R1:Q1 If the description is not accurate will the information mislead the model? \n\nIn general, yes -  imprecise descriptions may have a negative effect of training. For example, using inaccurate descriptions for highway scenes, e.g. \"there are 3 cars in the street\" (as opposed to \"on the highway\"), decreased the quality of the scene classification. That's why we don't use GPT augmentations of captions, as it sometimes adds information that is not relevant to the image like \"it's a sunny day, car goes down the street\" for a picture with cloudy weather. Meanwhile, we added augmentations that are supposed not to violate caption-image aligning, e.g. \"it's a photo of the environment with a car\" and \"there is a car\"."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533464324,
                "cdate": 1700533464324,
                "tmdate": 1700533464324,
                "mdate": 1700533464324,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y74QYA75RM",
                "forum": "bdJaYLiOxi",
                "replyto": "n07DTYzT4F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5702/Reviewer_RqMG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5702/Reviewer_RqMG"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your effort in the rebuttal. My major concern is that I dont trust your text formation since your CLIP finetuning is based on random caption generation. You also answered my concerns \"In general, yes - imprecise descriptions may have a negative effect of training.\". Therefore, I decided to keep my ratings."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5702/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641923402,
                "cdate": 1700641923402,
                "tmdate": 1700641923402,
                "mdate": 1700641923402,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]