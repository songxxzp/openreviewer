[
    {
        "title": "Towards Control-Centric Representations in Reinforcement Learning from Images"
    },
    {
        "review": {
            "id": "qFhBGddswr",
            "forum": "3ARfhjGfdF",
            "replyto": "3ARfhjGfdF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2613/Reviewer_sEtP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2613/Reviewer_sEtP"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose to improve the bisimulation principle methods in terms of (1) better latent dynamics prediction; (2) handle sparse reward environments. The authors propose ReBis, which combine bisimulation loss with a masked transformer model to learn the latent forward dynamics and latent reconstruction loss can prevent collapse from uninformative rewards."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The motivation is clear. The paper identifies the drawbacks of the existing bisimulation methods, and proposes corresponding algorithms to handle them. In general, the writing is clear and the paper is easy to follow."
                },
                "weaknesses": {
                    "value": "1. The novelty remains concerned. It seems that the algorithm combines MLR + a bisimulation loss.\n\n2. The algorithm designs (using masked inputs, momentum encoder, etc.) need ablation studies to prove its effectiveness.\n\n3. Though the motivation is to improve the bisimulation method, the proposed algorithm seems to be something between model-based RL (as cited in \u201cHighly Expressive Dynamics Model\u201d part). So I think it is necessary to discuss / compare with these works."
                },
                "questions": {
                    "value": "1. I find some notations might need more clarification:\n\na) By stacking 3 frames for each observation (derive $o_t \u2018$ from $o_t$), do you stack 3 frames $(o_t, o_t, o_t)$ with different masking, stack $(o_t, o_{t+1}, o_{t+2})$, or other setting?\n\nb) Can you please explain the relative position embedding $\\tau_K^p$ in detail? Do you add this position embedding to the state / action tokens, or concatenate this position embedding as an independent input?\n\nc) As for the block masking, I think it will be better to introduce how the images are masked to make the paper self-contained.\n\n2. What are the modifications on the proposed methods w.r.t. MLR besides the behavior loss (bisimulation part)? Most of the merits of the proposed methods (e.g., better forward dynamics, learn asymmetric reconstruction to handle uninformative reward) seems to come from the MLR backbone.\n\n3. I\u2019m also wondering why using the MLR as backbone to improve bisimulation and why not other methods (e.g., Dreamer, TransDreamer, Transformer world model) as cited in your related works."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2613/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2613/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2613/Reviewer_sEtP"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2613/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698645141959,
            "cdate": 1698645141959,
            "tmdate": 1700545425160,
            "mdate": 1700545425160,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "25WmRAIrQb",
                "forum": "3ARfhjGfdF",
                "replyto": "qFhBGddswr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sEtP - 1"
                    },
                    "comment": {
                        "value": "Thanks for your comments, we would like to clarify some points and explain your questions.\n\n**1. Explanation of frame stack.**\n\nWe use the commonly used frame stack method, which involves stacking $(o_t, o_{t+1}, o_{t+2})$. This helps capture information about object motion, velocity, and direction,  favoring the tasks with an underlying POMDP.\n\n**2. Explanation of the relative position embedding.**\n\nWe did utilize relative position embeddings in our model, drawing inspiration from the prevailing techniques in the domain of transformer architectures. By doing so, we adopt the relative positional embeddings to encode relative temporal positional information into both state and action tokens. We will consider offering a more detailed description in subsequent revisions.\n\n**3. Explanation of the Block-wise masking.**\n\nThe block masking technique we employed is inspired by the method described in [1], which utilizes both spatial and temporal masking blocks. We divide the sequence of image frames into several cubic segments. This segmentation considers both spatial and temporal dimensions, ensuring that each cube encompasses a portion of an image as well as its temporal extension over consecutive frames. The cubes are selected by initially defining a 2-D block at a random time step in the sequence of frames. Once a 2-D block is defined, it is extended along the temporal dimension. For the actual masking process, we randomly select these cubes until we achieve a predetermined masking ratio. Recognizing the importance of making the paper self-contained, we will provide a more detailed description in the future version of our paper.\n\n[1] Wei, C., Fan, H., Xie, S., Wu, C. Y., Yuille, A., & Feichtenhofer, C. (2022). Masked feature prediction for self-supervised visual pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 14668-14678).\n\n**4. The key difference between MLR and the proposed method ReBis.**\n\nWe have provided a detailed analysis in Appendix C, comparing the ReBis with existing methods, particularly MLR. Firstly, it is essential to clarify that while ReBis shares some similarities with MLR, such as the adoption of a mask strategy and a transformer-based encoder, these elements are not unique to MLR and are widely used in current models. The key to differentiate a model is how it defines its objectives and training process when applying these frameworks. In contrast to MLR, ReBis is fundamentally based on the principles of bisimulation. This influences our method to tailor its objectives and training process specifically to enhance bisimulation. As a result, ReBis achieves an effective balance between efficiency and capability, particularly in capturing control-centric (reward-associated) state dynamics. This is a significant departure from MLR, which tends to overlook task-related information.\n\nFrom the model architecture perspective, MLR employs a more complex network structure, including additional two projection heads, and one prediction head. While these components result in higher complexity and computational demands and are integral to MLR, ReBis does not require such complex structures to effectively perform its tasks.\n\nTherefore, both in terms of design target and model architecture, ReBis significantly diverges from MLR, showcasing its unique approach and contribution to the field.\n\n**5. Why not other methods (e.g., Dreamer)?**\n\nWe did consider and experiment with structures like Dreamer to enhance bisimulation performance. However, we encountered several issues. Firstly, methods in the Dreamer series are model-based, which inherently differs from the approach we aimed for in our research. Secondly, Dreamer involves pixel-level reconstruction.  Our goal was to minimize irrelevant information as much as possible.  Therefore, the reconstruction nature of Dreamer essentially conflicts with the objectives of bisimulation, which seeks to abstract important information away from such low-level details.\n\nGiven these considerations, it is currently not feasible to apply bisimulation to methods like Dreamer. On the other hand, MLR, with its focus on latent space reconstruction, aligns more closely with the principles of bisimulation. Furthermore, it is indeed possible that there are other approaches that could potentially yield equal or better results than ReBis. Our focus has been on establishing a paradigm in which certain modules are interchangeable. We plan to explore the potential of integrating ReBis with other methods in future work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235915013,
                "cdate": 1700235915013,
                "tmdate": 1700288572618,
                "mdate": 1700288572618,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "84mY7N4mTv",
                "forum": "3ARfhjGfdF",
                "replyto": "qFhBGddswr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sEtP - 2"
                    },
                    "comment": {
                        "value": "**6. Ablation studies.**\n\nThank you for your comment regarding the need for ablation studies to validate the effectiveness of the algorithm design, particularly the use of masked inputs and momentum encoder. Masking inputs is a fundamental step in our methodology, as it is integral to the subsequent reconstruction process, which plays a crucial role in our model's learning mechanism. We would like to emphasize that to explore its effectiveness, we have conducted thorough ablation experiments focusing on different masking ratios. The detailed results and analyses of these ablation studies are presented in Appendix D.3 of our paper.\n\nThe momentum update technique has become a common practice due to its effectiveness in enhancing the stability of the learning process. Our inspiration to use the momentum encoder for representation learning comes from its successful adaptation in references [2, 3, 4]. In response to your suggestion, we have conducted an ablation experiment focusing on the momentum coefficient $m$. The results of this experiment are as follows:\n\n|    momentum $m$     | 0.9  |      | 0.95 |      | 0.99 |      | \n| ----------------- | ---- | ---- | ---- | ---- | ---- | ---- | \n|                   | mean | std  | mean | std  | mean | std  | \n| Finger, Turn Easy | 596  |  6   | 652  | 34   | 508  | 12   | \n| Pendulum, Swingup | 440  |  11  | 458  | 7    | 403  | 6    | \n\n[2] He, K., Fan, H., Wu, Y., Xie, S., & Girshick, R. (2020). Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 9729-9738).\n\n[3] Laskin, M., Srinivas, A., & Abbeel, P. (2020, November). Curl: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning (pp. 5639-5650). PMLR.\n\n[4] Zang, H., Li, X., & Wang, M. (2022, June). Simsr: Simple distance-based state representations for deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 36, No. 8, pp. 8997-9005).\n\n**7. Comparison with model-based RL method.**\n\nIndeed, ReBis is a model-free method, as we does not construct the reward function and the explicit reconstruction in observation space. In our framework, we only reconstruct the state embeddings in latent space, which means categorizing our method into model-based RL seems not to be appropriate. Nevertheless, we still constructed an experiment on comparing our method and a model-based representation method TIA [5] (one method that incorporates Dreamer structure and also aims to learn task-specific information), to show the effectiveness of our model:\n\n|                    |  TIA |      | ReBis |      |\n| ------------------ | ---- | ---- | ---- | ---- |\n|                    | mean | std  | mean | std |\n| Ball in cup, Catch | 750  |  401 | 970  | 16 | \n| Cartpole, Swingup  | 108  |  18  | 859  | 21 | \n| Finger, Spin       | 485  |  27  | 893  | 35 | \n| Hopper, Hop        | 57   |  67  | 145  | 28 | \n\nPlease note that due to the tight rebuttal timeframe, these experimental results are preliminary. We plan to include more model-based methods into comparison in subsequent versions, to further demonstrate the advantages of our model.\n\n[5] Fu, X., Yang, G., Agrawal, P., & Jaakkola, T. (2021, July). Learning task informed abstractions. In International Conference on Machine Learning (pp. 3480-3491). PMLR."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236115373,
                "cdate": 1700236115373,
                "tmdate": 1700236115373,
                "mdate": 1700236115373,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c4DZf2DGTr",
                "forum": "3ARfhjGfdF",
                "replyto": "84mY7N4mTv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2613/Reviewer_sEtP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2613/Reviewer_sEtP"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response, I appreciate your response and I would like to raise the score. However, the major concern w.r.t. the novelty remains (W1). According to the author's feedback, the main difference is that: (1) ReBis basically use the MLR network but removes \"additional two projection heads, and one prediction head\", and (2) a bisimulation training objective. To me, this still seems like a combination of two existing methods as I commented previously, and this is why I don't raise my score further"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545395477,
                "cdate": 1700545395477,
                "tmdate": 1700545395477,
                "mdate": 1700545395477,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Lwmb5qzB72",
            "forum": "3ARfhjGfdF",
            "replyto": "3ARfhjGfdF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2613/Reviewer_6U7G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2613/Reviewer_6U7G"
            ],
            "content": {
                "summary": {
                    "value": "This paper builds on the idea of applying the bisimulation principle to shape the representation of deep RL with task-specific information. The authors first identify several limitations of prior work, for example, the expressiveness of the learned dynamics model and the ability to leverage reward-free control information when the reward is uninformative.\n\nThen, the authors propose ReBis which leverages spatiotemporal consistency and long-term behavior similarity. More specifically, they reduce spatio-temporal redundancy in observations via Siamese encoders with block-wise masking and use a transformer-based dynamics model to capture multi-modal behaviors.\n\nFinally, the authors demonstrate the superiority of ReBis on Atari and DeepMind control suites."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper compares with many prior works on learning a good representation for policy learning, including CURL, SimSR, etc., and shows their proposed method, ReBis, outperforms them in most tasks. Moreover, their learned representation is more robust to different backgrounds as demonstrated in the DeepMind control suite with different background distractions.\n\nExperiments and ablations are comprehensive and well-designed. The ablation of different loss components justifies the design of ReBis and shows both reconstruction loss and bi-simulation loss contribute to the gain in ReBis.\n\nThe paper writing is easy to follow and has a nice overview of related work."
                },
                "weaknesses": {
                    "value": "As pointed out by the author, the representation learning part adds non-trivial complexity to the RL algorithm. It would be interesting to experiment with different update frequencies of the representation learning part. For example, what\u2019s the score if we update the representation encoder every 1, 10, 100, or 1000 environmental interactions?\n\nIs the method complementary to other representation learning methods, such as augmentation, video prediction, time contrastive learning, etc? Does combining with other methods lead to better results?\n\nThe representation learning part relies on a reward specification by design, which prevents it from being used as a general pre-training method with unknown tasks. The authors should consider discussing this in limitations."
                },
                "questions": {
                    "value": "Do we need to update the encoder every RL update? Can we update it less frequently than the policy?\n\nIs the method complementary to other representation learning approaches?\n\nCan we use it for pre-training a representation without task specification?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2613/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2613/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2613/Reviewer_6U7G"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2613/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698649462950,
            "cdate": 1698649462950,
            "tmdate": 1699636200211,
            "mdate": 1699636200211,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A5IbVeq5cQ",
                "forum": "3ARfhjGfdF",
                "replyto": "Lwmb5qzB72",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6U7G"
                    },
                    "comment": {
                        "value": "Thanks for your comments, we would like to clarify some points and explain your questions.\n\n**1. Do we need to update the encoder every RL update? Can we update it less frequently than the policy?**\n\nOur current method does indeed require updating the encoder at every RL update. This frequent updating is integral to ensuring that the encoder accurately captures the evolving state representations, which is crucial for the effectiveness of our RL algorithm. However, inspired by your suggestion, we explored the possibility of updating the representation encoder less frequently. \n\nTo test this, we conducted experiments where the encoder was updated every 10, 100, or 1000 environmental interactions. The results of these experiments are as follows:\n|                          |  10  |      | 100  |      | 1000 |      |\n| ------------------------ | ---- | ---- | ---- | ---- | ---- | ---- |\n|                          | mean | std  | mean | std  | mean | std  |\n| Cartpole, Swingup Sparse | 492  | 8    | 290  | 46   | 141  | 53   |\n| Pendulum, Swingup        | 290  | 28   | 110  | 47   | 22   | 16   |\n\nThese findings suggest that while our current approach of updating the encoder at every RL update is effective, there may be potential in exploring alternative update frequencies. The need for frequent updates stems from the dynamic nature of reinforcement learning environments. Updating the encoder with every RL update ensures that the state representations remain current and relevant, thereby facilitating more effective learning and decision-making. Less frequent updates may lead to a decreased ability of the encoder to keep pace with the rapidly changing environment and agent dynamics. These findings open avenues for future research to optimize the balance between update frequency and computational efficiency, potentially through adaptive updating mechanisms or more efficient methods.\n\n**2. Is the method complementary to other representation learning approaches?**\n\nOur method can be effectively combined with data augmentation techniques, such as random crop.  Prior research [1, 2] has uncovered that applying proper data augmentation can significantly improve the sample efficiency of reinforcement learning models.\n\nThe integration of video prediction techniques with our method may offer additional contextual information, aiding in a better understanding of temporal dynamics. Similarly, time contrastive learning, which focuses on understanding the temporal relationships between different observations, could also complement our approach.  By leveraging the temporal contrasts, our method might further enhance its ability to discern meaningful patterns in temporal sequences.\n\nIn our future work, we believe that incorporating these approaches could potentially lead to significant improvements in the model's performance, particularly in scenarios with complex temporal dependencies.\n\n[1] Laskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., & Srinivas, A. (2020). Reinforcement learning with augmented data. Advances in neural information processing systems, 33, 19884-19895.\n\n[2] Kostrikov, I., Yarats, D., & Fergus, R. (2020). Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649.\n\n**3. Can we use it for pre-training a representation without task specification?**\n\nOur model is primarily designed to extract task-specific features, which is central to its effectiveness in sparse reward environments.   This focus ensures that the model could capture relevant information that directly contributes to the task.\n\nDespite this task-specific orientation, we conducted exploratory experiments to assess the model's adaptability to different environments.   In one such experiment, we used an encoder trained in the 'Finger Spin' environment to evaluate its performance across 'Finger Turn Easy\u2019 environment. The results of this experiment are as follows: \n\n| an encoder trained on Finger, Spin | 500k |      | \n| ---------------------------------- | ---- | ---- | \n|                                    | mean | std  | \n|       Finger, Turn Easy            | 611  | 21   | \n\nThese findings suggest that the learned representation will generalize to unseen reward functions, it also exhibits a degree of flexibility and can adapt to new environments to some extent. Based on these results, it appears that our model can be used for pre-training without explicit task specifications. However, the effectiveness of this pre-training may vary depending on the similarity between the training and target environments."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233944196,
                "cdate": 1700233944196,
                "tmdate": 1700288507491,
                "mdate": 1700288507491,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TWTFky8dJB",
                "forum": "3ARfhjGfdF",
                "replyto": "A5IbVeq5cQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2613/Reviewer_6U7G"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2613/Reviewer_6U7G"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the new experiments! This partially addresses my concern. However, the fact that the encoder needs frequent updates and its lack of generalizability across tasks made me worry about its applicability in more complex environments. Thus, I decided to keep my original score!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592527113,
                "cdate": 1700592527113,
                "tmdate": 1700592527113,
                "mdate": 1700592527113,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CNPAxFpIU3",
                "forum": "3ARfhjGfdF",
                "replyto": "Lwmb5qzB72",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6U7G"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s feedback to our replies and the concerns about the  generalizability and applicability of our method. \n\nIn terms of generalizability for in-domain tasks, our approach holds distinct advantages. These arise from our method's integration of both task-specific and spatiotemporal dependencies, a claim substantiated by our latest experiments mentioned in the Answer to Q3. These findings further demonstrate that the ReBis encoder maintains superior performance over current baselines, even after adopting the encoder from another in-domain task. Furthermore, we recognize that the reviewer's concern may arise from bisimulation theory's inherent focus on integrating behavioral similarity into representation learning, which naturally leads to task or domain specificity. Adhering to the 'no free lunch theorem', we understand that a model tailored for a specific task may not universally adapt to all environments, which is why our evaluations are concentrated solely on in-domain tasks. And models with higher generalizability with cross-domain tasks often necessitates more in-domain-specific tuning. Hence, the trade-off between generalizability and task-specific (or domain-specific) optimality is a direct consequence of the model's design principles.\n\nAdditionally, it is important to note that all baseline approaches used for comparison in our study update their encoder with the same frequency as ours. This frequency setting is also consistent with the theoretical requirement of a high update frequency for the encoder, given the dependence of the bisimulation objective on the policy, and it will not hinder the effectiveness of our model in complex environments. Besides, this does not affect inference complexity as we only use CNN encoder during inference time.  Nevertheless, the time complexity deserves attention here, as the incorporation of a transformer architecture in our training process results in considerable time requirements for each gradient update. Therefore, future work could interestingly focus on accelerating model forward speed and achieving model distillation without compromising accuracy."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631319374,
                "cdate": 1700631319374,
                "tmdate": 1700632987583,
                "mdate": 1700632987583,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "d60oqK67fi",
            "forum": "3ARfhjGfdF",
            "replyto": "3ARfhjGfdF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2613/Reviewer_TC9E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2613/Reviewer_TC9E"
            ],
            "content": {
                "summary": {
                    "value": "The paper deals with image-based reinforcement learning. Image-based RL requires the extraction of control-specific relevant information from the images while discarding the visual noise. The authors came up with a representation learning algorithm that uses the bisimulation metric to measure distances between states. Bisimulation suffers from issues such as possible collapse in sparse rewards and requiring dynamics modeling which the authors are able to alleviate. The produce competitive results on Atari and distracting dm control suite."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) They correctly identify the issues in using the bisimulation metric for quantifying the distances between states. They take measures to solve these.\n\n(2) They use a transformer to implicitly learn the dynamics which improves the expressibility of the dynamics models.\n\n(3) Their reconstruction objective ensures that the representations do not collapse even in a sparse rewards regime.\n\n(4) They produce impressive results in the distracting dm control suite."
                },
                "weaknesses": {
                    "value": "(1) Can this loss be used as a pretraining loss for learning an encoder? There should have been some experiment depicting this.\n\n(1) The method of representation learning seems way more complex than the RL algorithm itself. If this is an auxiliary loss, the transformer model capturing the dynamics is never used by the RL algorithm which seems a waste of resources."
                },
                "questions": {
                    "value": "(1) What if the distracting images in distracting dm control are added during training as well? Can the model get distracted then?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2613/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2613/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2613/Reviewer_TC9E"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2613/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831230256,
            "cdate": 1698831230256,
            "tmdate": 1699636200043,
            "mdate": 1699636200043,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dZOFKbS8Q1",
                "forum": "3ARfhjGfdF",
                "replyto": "d60oqK67fi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TC9E"
                    },
                    "comment": {
                        "value": "Thanks for your comments, we would like to clarify some points and explain your questions.\n\n**1. Can this loss be used as a pretraining loss for learning an encoder?**\n\nThank you for your question. We have experimented with this possibility. We used an encoder pre-trained in one environment (Finger Spin) to evaluate its performance in different environment (Finger Turn Easy). The result of the experiment is as follows: \n\n| an encoder trained on Finger, Spin | 500k |      | \n| ---------------------------------- | ---- | ---- | \n|                                    | mean | std  | \n|       Finger, Turn Easy            | 611  | 21   | \n\nThis experiment demonstrates that our method is versatile enough to be used for pre-training representations without task-specific constraints. The trained encoder shows promising adaptability and generalization across various environments, indicating its potential for broader applications in reinforcement learning. \n\n**2. The method of representation learning seems way more complex than the RL algorithm itself. If this is an auxiliary loss, the transformer model capturing the dynamics is never used by the RL algorithm which seems a waste of resources.**\n\nWhile the transformer model primarily focuses on capturing the dynamics for representation learning, its contribution extends beyond being just an auxiliary loss. The learned representations, albeit indirectly, play a vital role in the overall RL process. They provide the RL algorithm with a more nuanced understanding of the environment. Regarding the concern about resource utilization, Rebis aims to balance the computational overhead with the benefits brought by the transformer model. We understand the importance of efficient resource usage and continuously strive to optimize our model to make the best use of available resources. The current design reflects a trade-off where the added complexity is counterbalanced by the substantial improvements in learning quality.\n\nWe acknowledge the need to explore ways to directly leverage the dynamics captured by the transformer model within the RL algorithm. Future iterations of our research will focus on more tightly integrating these components, ensuring that the dynamics captured by the transformer are more directly utilized in decision-making processes.\n\n**3. What if the distracting images in distracting dm control are added during training as well? Can the model get distracted then?**\n\nIt is a consideration that aligning the noise levels in training data with those in testing data, could potentially lead to better adaptation and performance in noisy environments. This approach might allow the model to become more accustomed to distractions.\n\nIn our current research, we deliberately chose a more complex setup where the model is trained without distractions and tested in the presence of distracting images. This setup was intended to assess the model's ability to generalize and perform effectively in environments different from those encountered during training.\n\nIn response to your query, we conducted additional experiments where distracting elements were included during both training and testing.  We found that the results obtained from both experimental setups were similar. This outcome underscores the effectiveness of our method, indicating its robustness and generalizability across different levels of environmental noise."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233695445,
                "cdate": 1700233695445,
                "tmdate": 1700288557344,
                "mdate": 1700288557344,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "O71GilBPCN",
            "forum": "3ARfhjGfdF",
            "replyto": "3ARfhjGfdF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2613/Reviewer_hpbt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2613/Reviewer_hpbt"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents ReBis, a new method for state representation learning in image-based RL, based on the bisimulation metric. It utilizes a transformer architecture with block-wise masking to capture dynamics and address issues inherent in environments with sparse rewards. The authors claim that ReBis prevents feature collapse where standard bisimulation metrics fail, showcasing performance gains on benchmark tasks such as Atari and DMC."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper makes a notable attempt to tackle the shortcomings of bisimulation metrics in sparse reward settings by integrating a transformer-based dynamics model. The adoption of block-wise masking as a strategy for sampling stochastic dynamics is an interesting approach. Empirically, the method appears to offer improvements over existing techniques, as demonstrated in the results section which features performance gains across Atari, DMC, and DMC with distraction."
                },
                "weaknesses": {
                    "value": "The paper aims to address two challenges of bisimulation metrics: the reliance on Gaussian distribution for modeling and the issue of uninformative rewards. However, the justification for how the proposed ReBis method successfully overcomes these challenges remains unconvincing. The paper's method of using block-wise masking observation to simulate the sampling of stochastic dynamics is not fully explained. Various other techniques, such as random cropping as in DrQ, employing Dropout layers, or introducing noise to weights or latent features, could potentially serve a similar purpose. A detailed comparison of these methods within the experimental section would be beneficial for substantiating the need for block-wise masking.\nRegarding the challenge of sparse rewards, the explanation of how ReBis overcome this issue is not adequately addressed. The paper should provide a clearer rationale for why its methods would be more effective in such environments.\n\nThe paper\u2019s claim to novelty largely rests on the application of a transformer dynamics model, but this alone does not present a compelling case for novelty. The absence of a thorough comparative analysis, especially in the ablation studies, against non-transformer or non-sequential architectures, diminishes the strength of the argument for the proposed method\u2019s innovation. Furthermore, the paper borrows heavily from prior work for various aspects, such as the masked observation approach, the metric used, and the theoretical framework, which detracts from the original contribution.\n\nThe theoretical foundation of the paper also shows weaknesses. \n1. The Definition 1 provided is mislabeled as \"bisimulation-based\" since it lacks the Wasserstein distance. It defines metric like MICo or SimSR but misses an expectation operator in front of $\\bar{d}$. \n1. Theorem 2 and 3 for bisimulation metric, originated from (Kemertas & Aumentado-Armstrong, 2021), cannot be directly applied to the metric defined in the paper because their proofs rely on the Wasserstein distance, which is absent from the paper's metric. The authors need to develop new proofs that are pertinent to their specific metric definition.\n1. Theorem 4 does not seem intuitive if the metric includes an expectation operator. (Castro et al., 2021) mentioned it is \u0141ukaszyk\u2013Karmowski distance with non-zero distance. The transition from Equation (15) to Equation (16) in the proof is unclear and requires further clarification."
                },
                "questions": {
                    "value": "1. Could the authors clarify the term \"multi-modals\" within the paper?\n1. In Tables 1 and 2, the blue color is not described. What does it represent in the context of these tables?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2613/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699197317452,
            "cdate": 1699197317452,
            "tmdate": 1699636199939,
            "mdate": 1699636199939,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Mnsizhncgy",
                "forum": "3ARfhjGfdF",
                "replyto": "O71GilBPCN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2613/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments. We regret that the initial manuscript did not adequately convey our assumptions, partly due to page constraints. We deeply appreciate your feedback, which has prompted us to revisit these aspects and provide more clarity. In this paper, we assume that the dynamics in real-world environments tend to be nearly deterministic, and therefore we focus primarily on deterministic settings. We have thoroughly revised the related theories and descriptions, marking these updates in blue for your convenience.\n\n**1. Explanation of Definition 1.**\n\nIt is important to note that the term 'bisimulation-based' does not necessarily imply the inclusion of the Wasserstein distance. The bisimulation-based methods, such as MICo or SimSR, also do not incorporate the Wasserstein distance. Instead, they employ independent coupling as an alternative to mitigate the computational cost associated with the Wasserstein distance.  Besides, back to the early developments in bisimulation, as illustrated in [1, 2], they originally presented both Wasserstein-based and Total-Variation-based objectives.  Therefore, \u2018bisimulation-based' approaches are not inherently required to include the Wasserstein distance.\n\nAdditionally, we would also like to kindly remind the reviewer that the defined measurement does not strictly qualify as a 'metric', as it does not fulfill the requirements of the triangle inequality. Regarding the absence of the expectation operator in our definition, this omission stems from the assumption of deterministic transition dynamics, as outlined in Sections 3 and 4. This assumption influenced our choice of a transformer as the dynamics model, ensuring deterministic reconstruction and prediction within our framework. We appreciate your feedback on the need for clearer, self-contained explanations and have accordingly revised the relevant sections in our manuscript. \n\n[1] Ferns, N. F. (2003). Metrics for markov decision processes.\n\n[2] Ferns, N., Panangaden, P., & Precup, D. (2012). Metrics for finite Markov decision processes. arXiv preprint arXiv:1207.4114.\n\n**2. Explanation of Theorem 2 and 3.**\n\nWe're grateful for the chance to elucidate our theoretical framework and how it aligns with our definitions. In our paper, we operate under the assumption that the dynamics in real-world environments are nearly deterministic. Therefore, our focus is primarily on deterministic settings. Specifically, we assume that for any latent state  $s\\in\\mathcal{S}$, $a \\in \\mathcal{A}$, there exists a unique $\\kappa(s,a)\\in\\mathcal{S}$ such that $P_{s}^a(\\kappa(s,a))=1$. With this assumption in place, we show that the equivalence between the bisimulation measurement $d(\\kappa(s_i,a),\\kappa(s_j,a))$ and the W1 distance $\\mathcal{W_1}(d)(P{s_i}^a,P_{s_j}^a)$ in deterministic settings.\n\nAs such, we can effectively apply Theorems 2 and 3 to the measurement defined in our paper. The deterministic nature of the transitions in our model aligns well with the theoretical underpinnings of these theorems, ensuring their relevance and applicability to our work. We hope this explanation clarifies how our assumptions and theoretical developments are congruent with the application of Theorems 2 and 3 to our measurement. \n\n**3. Explanation of Theorem 4.**\n\nWe apologize for the typo error that we made. The right-hand-side of Equation 16 in our previous manuscript should be $\\max_{x,y}|R_x^\\pi-R_y^\\pi|$ instead of $|r_{s_i}^{\\pi} - r_{s_j}^{\\pi}|$. We have fixed this error in new manuscript. We also provide a more detailed derivation in the appendix now. Regarding Theorem 4, our intention was to address the challenges posed by sparse reward environments when using bisimulation. MICo distance is a special case, inherently possesses a non-zero distance. However, as we described in the beginning of Section 3, we \"primarily focus on a cosine distance-based bisimulation measurement\", where the issue that we referred indeed exists. Besides, although the characteristic of MICo having non-zero self-distance makes it unsuitable for Theorem 4, the author of SimSR[3] pointed out that this attribute may encounter the failure mode of representation collapse, even without considering the sparse reward settings.\n\n[3] Zang, H., Li, X., & Wang, M. (2022). Simsr: Simple distance-based state representations for deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 36, No. 8, pp. 8997-9005).\n\n**4. Explanation of \"multi-modals\".**\n\n\"Multi-modals\" here refers to the presence of multiple modes in the behavior or decision-making patterns of an agent.\n\n**5. Description of the blue color in Tables 1 and 2.**\n\nThe blue color in these tables is specifically used to highlight tasks that operate under sparse reward conditions. These tasks include Ball in Cup, Catch, Cartpole Swingup Sparse, Finger Turn Easy/Hard, and Pendulum Swingup. In the newly uploaded manuscript, we added the corresponding explanation."
                    },
                    "title": {
                        "value": "Response to Reviewer hpbt - 1"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232930636,
                "cdate": 1700232930636,
                "tmdate": 1700233262455,
                "mdate": 1700233262455,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cS1389RYKj",
                "forum": "3ARfhjGfdF",
                "replyto": "O71GilBPCN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2613/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**6. Explanation of Block-wise Masking.**\n\nWe would like to clarify that the use of block-wise masking is not to simulate the sampling of stochastic dynamics as the reviewer assumed.  As we have  discussed in Section 4, we consider the environment nearly deterministic, we employee the transformer as a deterministic dynamics model. Besides, block-wise masking in our approach does not  serve as the sampling of stochastic dynamics, it in fact is used to get rid of spatio-temporal redundance of the observations, coupled with bisimulation-based objective. Intuitively, we don't think random cropping, drop-out and injection of noise can serve the similar purpose, due to their inadequacy of capturing the observation-level spatio-temporal information. \n\nIntegrating random cropping with our method may enhance the model's ability to generalize and adapt to different visual inputs. However, employing random cropping as a standalone technique is not as effective in our specific application. While random cropping often results in large, contiguous sections of the image being visible, block-wise masking creates a more scattered pattern of visible and masked regions. This scattered pattern is more effective in our context for capturing control-centric information, as it challenges the model to reconstruct and understand fragmented visual inputs. \n\nFor dropout and injecting noise, although they can serve as sampling from stochastic dynamics, they are not applicable currently in our framework as we mainly focus on deterministic settings. We would like to investigate the potential of applying our framework in stochastic setting in our future work.\n\n**7. The explanation of how ReBis overcome the challenge of sparse rewards.**\n\nReBis overcomes the challenge of sparse rewards by employing an asymmetrical architecture in its dynamics model. As mentioned in our paper, particularly in Theorem 4, we acknowledge that bisimulation tends to collapse in sparse reward environments.  However, it's important to note that agents can still interact with these environments, which means the agent can still learn control relevant informations from the interactions.  This understanding forms the primary motivation behind our work with ReBis.\n\nIn the framework of ReBis, an asymmetric component within the Siamese architecture is specifically tailored to prevent feature collapse in environments where rewards provide limited information.  The effectiveness of this approach is underpinned by Theorem 5, which demonstrates how an asymmetrical architecture can alleviate feature collapse.  It does so by increasing the effective feature dimensionality throughout the training process. Specific details regarding Theorem 5 are presented in the appendix.\n\n**8. Concern about the novelty of our paper.**\n\nWhile our model utilizes structural components that are commonly recognized in the field, the true claim of our research lies in our unique application of these elements.  We've conducted an in-depth examination of bisimulation within the context of reinforcement learning, with a specific focus on its limitation in complex environments with sparse rewards. \n\nOur main contribution is the development of a straightforward yet highly effective approach, designed specifically to address the limitations we identified in traditional bisimulation methods. We have meticulously tailored the objectives and training processes of our model to cater to the unique challenges of reinforcement learning, with a particular focus on learning robust, control-centric representations.\n\nComplementing our theoretical advancements, our model has been empirically validated to demonstrate its effectiveness. Our research reveals that such theoretically-grounded solutions can effectively mitigate challenges that more complex traditional methods struggle to overcome. In essence, our model is an effective framework that can match or outperform existing algorithms, which may provide new insights in the field."
                    },
                    "title": {
                        "value": "Response to Reviewer hpbt - 2"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700233165358,
                "cdate": 1700233165358,
                "tmdate": 1700233284276,
                "mdate": 1700233284276,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RL8AqHtAPc",
                "forum": "3ARfhjGfdF",
                "replyto": "O71GilBPCN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kindly Reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nThanks again for the time spent on our manuscript and your valuable feedback. We have revised the paper to address your questions and comments. We believe we have responded to the questions and concerns in full, but if something is missing please let us know and we would be happy to add it. If we have addressed your concerns we would appreciate it if the reviewer could re-evaluate our work in light of these clarifications."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633350297,
                "cdate": 1700633350297,
                "tmdate": 1700633350297,
                "mdate": 1700633350297,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TUBN8Jje3z",
                "forum": "3ARfhjGfdF",
                "replyto": "RL8AqHtAPc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2613/Reviewer_hpbt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2613/Reviewer_hpbt"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment from Reviewer hpbt"
                    },
                    "comment": {
                        "value": "**Concern of the Deterministic Assumption**: The authors make an assumption of deterministic transition dynamics, however, it is not the sufficient condition to make $\\mathcal{W}_1(d) = d = \\mathbb{E}[d]$ is true. The next state distribution distance, either $\\mathcal{W}_1$ in bisimulation metric or expectation operator in MICo/SimSR, are computed over policy $\\pi$, i.e., distribution of action. Deterministic transition dynamics along with **deterministic policy** is the sufficient condition to authors' claim on Definition 1 and Theorem 2 and 3. \n\n**Alternation of Block-wise Masking**: Now I understand block-wise masking is not to simulate the sampling of stochastic dynamics because the dynamics is deterministic. But I don't understand why random crop like DrQ, dropout and injecting noise cannot serve the purpose similar to block-wise masking, because they all introduce randomness. \nAuthors mention that scattered pattern is more effective than random crop. Is it associated to sample efficiency or computational efficiency? If sample efficiency, comparison experiments are required to support your claim.\nDropout is also a kind of masking but on different dimension to block-wise masking. It is strange to make assumption of deterministic dynamics but use the block-wise masking approach to increase randomness.  Comparing with other randomness introduction methods can improve the motivation of block-wise masking, otherwise the method looks like heuristically proposed. \n\n**Concern on Theorem 5**: Theorem 5 seems about the effectiveness of learning siamese neural network. However the challenge of sparse rewards is due to non-informative rewards in most transitions data. I don't recognize the connection between Theorem 5 and MDP, and cannot understand that claim \"ReBis overcomes the challenge of sparse rewards by employing an asymmetrical architecture in its dynamics model. \" More explanation of Theorem 5 in the paper may improve to motivate ReBis.\n\n**Other Issues**:\n1. MICo or SimSR mentioned \"bisimulation\" and \"bisimulation metric\" in their paper, but they don't call themselves \"bisimulation\" and don't use the theoretical analysis from bisimulation metric.\n1. $\\kappa$ is mentioned in Equation 3 but defined after Equation 5.\n1. Missing definition of $\\Delta(S)$.\n1. Missing Equation reference in the proof of Theorem 5, in Page 17."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716834364,
                "cdate": 1700716834364,
                "tmdate": 1700716834364,
                "mdate": 1700716834364,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l1UjbyN3wh",
                "forum": "3ARfhjGfdF",
                "replyto": "O71GilBPCN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hpbt - 4"
                    },
                    "comment": {
                        "value": "**3. Concern on Theorem 5**\n\nWe apologize for the overclaim in our last reply of the sentence \"ReBis overcomes the challenge of sparse rewards by ...\". We meant to say ReBis can alleviate the representation collapse issues in sparse reward environments. \nIn our context, the term 'asymmetric' refers not only to the network architecture, such as in siamese networks, but also encompasses the asymmetry in inputs. This is implemented by applying block-wise masking to one branch with the online encoder, while the other branch with the momentum encoder remains unmasked. We underscore that block-wise masking, as a means to reduce redundancy, is a crucial element of this asymmetric setup. Without block-wise masking, siamese architectures struggle to extract useful information in environments with sparse rewards. This is because they lack the inherent capacity to disentangle environmental information, leading to a collapse into identical representations. Conversely, with asymmetric inputs, the effective feature dimensionality improves over time during training. This allows for some information capture, even in settings where environmental rewards are extremely sparse.\n\n**4. Other issues**\n\n(1) MICo or SimSR don't call themselves \"bisimulation\"\n\nWe appreciate the reviewer's perspective on the term 'bisimulation' and understand that it may be perceived as having strict limitations, potentially leading to disagreements. Our intention is to highlight that the motivation of our work is from bisimulation-like objectives (or bisimulation-like measurements), rather than the specific stricted \"bisimulation metric\". We will modify these terms in the camera-ready version to ensure clarity and precision.\n\n(2) $\\kappa$ is mentioned in Equation 3 but defined after Equation 5\n\nWe first introduced $\\kappa$ on the upper side of the second page, which might not be very noticeable. We have now reiterated in Equation 3.\n\n(3) Others\n\nThanks! We have now provided the corresponding revisions in our updated manuscript."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738183685,
                "cdate": 1700738183685,
                "tmdate": 1700738755752,
                "mdate": 1700738755752,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]