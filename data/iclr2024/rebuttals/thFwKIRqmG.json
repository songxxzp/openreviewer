[
    {
        "title": "ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages"
    },
    {
        "review": {
            "id": "3CZ5UiQfCz",
            "forum": "thFwKIRqmG",
            "replyto": "thFwKIRqmG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1495/Reviewer_kpwW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1495/Reviewer_kpwW"
            ],
            "content": {
                "summary": {
                    "value": "This paper made three simple modification to advantage actor-critic methods.\n(1) introduced a ReLU function to restrict policy update to the optimal policy while enable approximate Bayesian inference.\n(2) used spectral normalization to restrict the output of network\n(3) used Thopson sampling to do exploration via dropout.\nThe reported results indicate that, mostly, the proposed method achieves improved returns when compared to the popular on-policy algorithms and other off-policy baseline methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Minimal modification of the ac method to enable Bayesian inference is an interesting and valuable idea. However, similar discussions exist in previous work[1].\n\nUse Thompson sampling to replace passive exploration without introducing complex machanism and high computational cost is important for dealing with the Exploration-exploitation dilemma for on-policy algorithms.\n\n\nThe performance of this method is impressive.\n\n[1] Levine, Sergey. \"Reinforcement learning and control as probabilistic inference: Tutorial and review.\" arXiv preprint arXiv:1805.00909 (2018)."
                },
                "weaknesses": {
                    "value": "This paper is not well written. Neccessary background and discribtion of Thompson sampling is missing. The names of metrics shown in Figure 1(Median, IQM, Mean, OG) should be emphasized above.(e.g. change 'robust normalized median, interquartile mean, and mean metrics' to 'robust normalized median(Median), interquartile mean(IQM), mean(Mean) and optimal gap(OG) metircs'). The multiple use of some terms in many places made me confused, e.g. I think A3C represent two different algorithms respectively in Figure 1 and 2.\n\nExcept in the case of sparse rewards, it is generally not acceptable to assume that the difference between the expected value of the value function over the next states and the value function at the current state is zero.\n\nAccording to the equation (3), the paper assume that $h$ is independent from $\\theta$. However, the advantage function is strong depend on current policy and also depend on $\\theta$. And the advantage function may not be approximated via Gamma distribution. From my understanding, the $\\sigma^2\\geq 0$ is a more direct reason why $h$ is a non-negative value.\n\n\ntypo on (4): $\\frac{\\beta^\\alpha\\sqrt{\\tau}}{\\sqrt{\\Gamma(\\alpha)2\\pi}}\\rightarrow \\frac{\\beta^\\alpha \\sqrt{\\tau}}{\\Gamma(\\alpha)\\sqrt{2\\pi}}$, $exp(\\beta h_i)\\rightarrow exp(-\\beta h_i)$.\n\ntypo in appendix, an extra $\\nabla$ after 'Letting, $v^*_\\pi(s_0)$......'"
                },
                "questions": {
                    "value": "Can you explain in detail how to combine Thompson sampling with Bayesian inference? And why this is a better **state-aware** exploration method?\n\nIs it necessary to assume it as a gamma-normal distribution?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1495/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1495/Reviewer_kpwW",
                        "ICLR.cc/2024/Conference/Submission1495/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1495/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698417655240,
            "cdate": 1698417655240,
            "tmdate": 1700665133567,
            "mdate": 1700665133567,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BDNnQeZAeC",
                "forum": "thFwKIRqmG",
                "replyto": "3CZ5UiQfCz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1495/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1495/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kpwW"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your close reading of our paper! We are encouraged that you found the idea of our contribution interesting and valuable, that you recognized low computational overhead of our solution, and that your were impressed by our method's performance. The reviewer brings up some good and challenging points that we would like to address.\n\n### Questions\n\n> Is it necessary to assume it as a gamma-normal distribution?\n\nThe gamma-normal assumption allows us to interpret adding dropout and weight-decay regularization as sensible approximate Bayesian inference without adding complex computational overhead to the original A3C optimization algorithm. As such, this assumption primarily serves to ground Thompson sampling through approximate Bayesian inference and is not requisite for Theorem 1. As with the original result of the policy gradient theorem, the results in Equations (5-6) do not make any distributional assumptions on $\\pi$ and should hold for all policies with differentiable probability densities/distributions.\n\n### Methodoligical Concerns\n\n> Except in the case of sparse rewards, it is generally not acceptable to assume that the difference between the expected value of the value function over the next states and the value function at the current state is zero.\n\nThank you for this comment as your attention has led to a clear improvement. We hope that our comment to all reviewers above has addressed this concern. Specifically, upon a close re-examination of this assumption, we agree it would not generally hold. Fortunately, we can show that it is not necessary and that $C_\\pi(\\mathbf{s})$ can be smaller than under the zero-expectation assumption, resulting in a smaller additive:\n\n**Lemma.**\n\n$$\\text{ReLU}(a) <= |a|$$\n\n*Proof*.\n$$ \\text{ReLU}(a) = \\max(0, a)$$\n$$\\quad\\quad\\quad\\quad= \\frac{1}{2}a + \\frac{1}{2}|a|$$\n$$\\quad\\quad\\quad\\quad= \\\\{a  \\text{ if } a\\geq0\\text{ else }0\\text{ if } a<0$$\n$$\\quad\\quad\\quad\\quad\\leq \\\\{a  \\text{ if } a\\geq0\\text{ else }-a\\text{ if } a<0 (-a > 0)$$\n$$\\quad\\quad\\quad\\quad= |a| $$\n\nTherefore,\n\n$$C_\\pi(\\mathbf{s}) = \\iint \\left(v_\\pi(\\mathbf{s'}) - v^\\pi(\\mathbf{s}) \\right)^+ dP(\\mathbf{s}' \\mid \\mathbf{S}^\\mathrm{t} = \\mathbf{s}, \\mathbf{A}^\\mathrm{t} = \\mathbf{a}) d\\Pi(\\mathbf{a} \\mid \\mathbf{S}^\\mathrm{t} = \\mathbf{s})$$\n$$\\quad\\quad\\quad\\leq \\iint \\left|v_\\pi(\\mathbf{s'}) - v_\\pi(\\mathbf{s}) \\right| dP(\\mathbf{s}' \\mid \\mathbf{S}^\\mathrm{t} = \\mathbf{s}, \\mathbf{A}^\\mathrm{t} = \\mathbf{a}) d\\Pi(\\mathbf{a} \\mid \\mathbf{S}^\\mathrm{t} = \\mathbf{s}) $$\n$$\\quad\\quad\\quad\\leq \\iint K_\\pi\\left|\\left|\\mathbf{s'} - \\mathbf{s} \\right|\\right| dP(\\mathbf{s}' \\mid \\mathbf{S}^t = \\mathbf{s}, \\mathbf{A}^\\mathrm{t} = \\mathbf{a}) d\\Pi(\\mathbf{a} \\mid \\mathbf{S}^\\mathrm{t} = \\mathbf{s})$$\n\n\nPlease let us know if you have further concerns about this point."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1495/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338738167,
                "cdate": 1700338738167,
                "tmdate": 1700338738167,
                "mdate": 1700338738167,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8teUAjX6WG",
                "forum": "thFwKIRqmG",
                "replyto": "3CZ5UiQfCz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1495/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1495/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Presentation Concerns"
                    },
                    "comment": {
                        "value": "### Presentation Concerns\n\n> Neccessary background and discribtion of Thompson sampling is missing. Can you explain in detail how to combine Thompson sampling with Bayesian inference? And why this is a better state-aware exploration method?\n\nApproximate Bayesian inference over the parameters of the policy, $\\theta$, yields a distribution over those parameters, $p(\\theta \\mid \\mathcal{D})$. Sampling a policy from this distribution, $\\hat{\\theta} \\sim p(\\theta \\mid \\mathcal{D})$, is as easy as sampling a dropout mask and then running a forward pass of the network, yielding the likelihood, $\\pi(\\mathbf{a} \\mid \\mathbf{s}, \\hat{\\theta})$. Then sampling an action is done by sampling an action from the sampled policy, $\\mathbf{a} \\sim \\pi(\\mathbf{a} \\mid \\mathbf{s}, \\hat{\\theta})$. This is precisely the procedure descibed by Thompson sampling. We outline this procedure in lines 5-6 of Algorithm 1 and have a section on Thomspon sampling for RL in the related works section, but we agree with your assessment that this can be made more explicit in the main text and we are working on incorporating the above description to the manuscript. We would appreciate any feedback on it should you have time.\n\nWe hypothesize that this is a better state-aware exploration method for two reasons. First, for less frequently visited states the diversity of the sampled paramters of the policy will be greater promoting more exploration. As a state is visited more often under actions that yield positive advantages, the diversity of samples will concentrate promoting less exploration. Thus, we get more exploration for states that we have less experience of good actions, and less exploration in states where we know what actions lead to good expected returns. Second, this exploration is done around the mode of the policy distribution, so the model is less likely to explore actions that are far from the mode, which could be more likely to lead to failure.\n\n\n> The names of metrics shown in Figure 1(Median, IQM, Mean, OG) should be emphasized above.(e.g. change 'robust normalized median, interquartile mean, and mean metrics' to 'robust normalized median(Median), interquartile mean(IQM), mean(Mean) and optimal gap(OG) metircs'). \n\nWe are adding your suggestions on the metric names.\n\n> The multiple use of some terms in many places made me confused, e.g. I think A3C represent two different algorithms respectively in Figure 1 and 2.\n\nWe agree that we can be more clear here and propose making the following changes to the names in Figure 1 in order to help aleviate the confusion:\n\nRMPG -> VSOP \"all-actions\"\nA3C -> VSOP no-ReLU-Adv.\nNo Spectral -> VSOP no-spectral\nNo Thompson -> VSOP no-Thompson\n\n> typo['s] on ..\n\nNice catches. We have updated the manuscript with, $\\frac{\\beta^{\\alpha}\\sqrt{\\tau}}{\\Gamma(\\alpha)\\sqrt{2\\pi}}$ and 'Letting $\\nabla v_\\pi^*(\\mathbf{s}_0)$'."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1495/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338765920,
                "cdate": 1700338765920,
                "tmdate": 1700338765920,
                "mdate": 1700338765920,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s6WAiWlC0r",
                "forum": "thFwKIRqmG",
                "replyto": "3CZ5UiQfCz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1495/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1495/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you, again"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you again for sharing your valuable feedback with us. Your comments have clearly led to an improved manuscript! We strongly believe that the motivation, theory, and empirical results make this submission worthy of acceptance. Moreover, we think our response addresses your concerns and have updated the paper accordingly. Specifically, we have updated the $C_\\pi(\\mathbf{s})$ bound, included discussions on the normal-gamma assumption and Thompson sampling, and incorporated your formatting instructions. We also respectfully contend that our approach differs significantly from that of Sergey Levine's in the work you cite. Their max-entropy interpretation has optimal trajectories as the target of probabilistic inference rather than model weights (or functions induced by those weights). The practical implications of this difference become clear in comparing our objective function in Equation (5) of our work to the objective function in section 4.1 of their work, which is precisely the A3C objective. If you have no further concerns, we kindly ask you to consider raising your score. Otherwise, we would also be happy to discuss further, should you find the time. \n\nBest regards,\nThe Authors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1495/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615590359,
                "cdate": 1700615590359,
                "tmdate": 1700616355260,
                "mdate": 1700616355260,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "paHaWUtUQj",
                "forum": "thFwKIRqmG",
                "replyto": "s6WAiWlC0r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1495/Reviewer_kpwW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1495/Reviewer_kpwW"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for reply"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive response, which addressed most of my concerns. I will raise my rating to 5. However, I cannot assign a higher score because the method lacks the novelty required, and when compared to other exploration techniques, it fails to convince me that it is a better state-aware exploration method."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1495/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665122847,
                "cdate": 1700665122847,
                "tmdate": 1700665122847,
                "mdate": 1700665122847,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8E8IVgqlQb",
            "forum": "thFwKIRqmG",
            "replyto": "thFwKIRqmG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1495/Reviewer_tvnj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1495/Reviewer_tvnj"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a modification to asynchronous advantage actor-critic (A3C) that involves incorporating a ReLU function to the advantage estimates, using spectral normalization and incorporating dropout. The key idea in their work is that exploration is required in on-policy reinforcement learning. When there is no exploration, certain states may fail to get explored and the policy might get trapped. In order to provide a remedy to this, algorithms typically use methods that do not depend on the frequency with which states are visited, which can provide suboptimal results compared to using simply using a method that incorporates details of frequency with which states are visited. As a remedy to this, the algorithm incorporates using a ReLU function to the advantage function. The way this is performed is that in the critic step, dropout is employed and in the actor step, ReLU is used. The motivation behind using the ReLU is that it enables a Bayesian inference over the actor's parameters. The rationale for the changes are justified in the Methods section. The work incorporates a theoretical bound that illustrates how their methods allows a maximization of estimation of state value functions plus a constant. The constant is then massaged in the spectral norm refinement stage of the algorithm. There are also very extensive empirical studies."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The algorithm provides a remedy to an issue with asynchronous advantage actor-critic algorithms (or, more broadly, reinforcement learning algorithms that many algorithms do not take into account state visitation frequency. This issue has been noted by other works as a very important topic in theoretical RL. See the works of [@book{sutton2018reinforcement, title={Reinforcement learning: An introduction}, author={Sutton, Richard S and Barto, Andrew G}, year={2018}, publisher={MIT press} }, @article{tsitsiklis2002convergence, title={On the convergence of optimistic policy iteration}, author={Tsitsiklis, John N}, journal={Journal of Machine Learning Research}, volume={3}, number={Jul}, pages={59--72}, year={2002} }, @inproceedings{winnicki2023convergence, title={On The Convergence Of Policy Iteration-Based Reinforcement Learning With Monte Carlo Policy Evaluation}, author={Winnicki, Anna and Srikant, R}, booktitle={International Conference on Artificial Intelligence and Statistics}, pages={9852--9878}, year={2023}, organization={PMLR} }] for more on this. The work takes an interesting angle by looking at statistical techniques for improvement which in turn motivated other improvement to the algorithm. The work provides a theoretical intuition and bound as well as numerous empirical studies."
                },
                "weaknesses": {
                    "value": "In the theoretical component of the algorithm, while there is a theoretical result which shows how the value function improves as a result of the modifications, which is very nice, but I think that the work could shed light on the role of these parameters on the overall convergence of the modified A3C? I also noticed that neither the simulations nor the theoretical results shed light on the exact role of the choice of dropout, relu etc., on the impact of their bounds, both theoretical bounds and empirical bounds, with the exception of the justification of the spectral normalization step. Or perhaps, a comparison to A3C, since that is the algorithm the current paper is based on?"
                },
                "questions": {
                    "value": "A question I have is whether the constant K is policy dependent, in which case, what would the policy improvement step be optimizing over in (6)? I noticed that the K-Lipschitz assumption is introduced with respect to a particular value function over all $s\\in\\scriptS,$ \nwhich makes me wonder if K is dependent on policies \u03c0. Another question I have is if the assumptions on the action distribution follow the normal-gamma in the bound on the value function improvement in (6)? What other assumptions are incorporated in the bound on the value function improvement?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1495/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1495/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1495/Reviewer_tvnj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1495/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698845102641,
            "cdate": 1698845102641,
            "tmdate": 1699636078454,
            "mdate": 1699636078454,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RMwjGKwy5o",
                "forum": "thFwKIRqmG",
                "replyto": "8E8IVgqlQb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1495/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1495/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tvnj"
                    },
                    "comment": {
                        "value": "# Response to Reviewer tvnj\n\nWe are keen to thank the reviewer for their thorough and helpful review. We're glad that the reviewer finds that our work tackles an important topic and that our empirical results are extensive. Thank you for the insightful pointers on this topic, which we are incorporating into the updated manuscript! The reviewer raises many good questions that we would like to answer below.\n\n### Questions \n\n> I noticed that the K-Lipschitz assumption is introduced with respect to a particular value function over all which makes me wonder if K is dependent on policies \u03c0. A question I have is whether the constant K is policy dependent, in which case, what would the policy improvement step be optimizing over in (6)?\n\nThis is an excellent question and we are very thankful for your insight. We hope that the response to all reviewers above serves to answer it. In summary, we agree that K is policy dependent in general, and the dependence has given new insights into the role of spectral normalization as a regularizer of K. Please do not hesitate to follow up if you have furhter clarifying questions regarding this point. \n\n> Another question I have is if the assumptions on the action distribution follow the normal-gamma in the bound on the value function improvement in (6)?\n\nAs with the original result of the policy gradient theorem, the result in Equation (6) does not make any distributional assumptions on either $q_\\pi$, $v_\\pi$, or $\\pi$, and should hold for all policy distributions with differentiable densities. \n\n> What other assumptions are incorporated in the bound on the value function improvement?\n\nAs stated in Theorem 3.1, two assumptions are necessary to get the result in Equation (6): 1.) that rewards, $\\mathrm{R}_t$, are non-negative (which we make without loss of generality); and 2.) that the gradient of the policy is a conservative vector field. With the additional assumption that the value function is K-Lipschitz, we can bound the constant as in Equation (7). Note that in response to Reviewer kpwW's comment, we can drop the additional assumption that the expected difference in the value function between subsequent states is 0.\n\n### Concerns\n\n> In the theoretical component of the algorithm, while there is a theoretical result which shows how the value function improves as a result of the modifications, which is very nice, but I think that the work could shed light on the role of these parameters on the overall convergence of the modified A3C?\n\nWe are very interested in establishing convergence rates for our algorithm, but believe that such an analysis would consitute a self contained journal paper akin to the works of *Agarwal et al. 2021* or *Shen et al. 2023*. Note that these works were published several years after the acceptance of the original A3C paperto ICML in 2016.\n\nAgarwal, Alekh, et al. \"On the theory of policy gradient methods: Optimality, approximation, and distribution shift.\" The Journal of Machine Learning Research 22.1 (2021): 4431-4506.\n\nShen, Han, et al. \"Towards Understanding Asynchronous Advantage Actor-critic: Convergence and Linear Speedup.\" IEEE Transactions on Signal Processing (2023).\n\n> I also noticed that neither the simulations nor the theoretical results shed light on the exact role of the choice of dropout, relu etc., on the impact of their bounds, both theoretical bounds and empirical bounds, with the exception of the justification of the spectral normalization step.  Or perhaps, a comparison to A3C, since that is the algorithm the current paper is based on?\n\nWhile dropout does not factor into the bound in Equation (6), we can improve the clarity of Theorem 3.1 to emphasize that the bound is directly dependent on applying the ReLU function to the advantage estimates. In Theorem 3.1 we write, $(x)^+ := \\max(0, x)$, which is the definition of the ReLU function."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1495/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700324336683,
                "cdate": 1700324336683,
                "tmdate": 1700324336683,
                "mdate": 1700324336683,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kZCSXgnmkD",
            "forum": "thFwKIRqmG",
            "replyto": "thFwKIRqmG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1495/Reviewer_szF5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1495/Reviewer_szF5"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method for enhancing A3C by introducing state-aware exploration. The method has three components, a ReLu function for advantage estimation, a spectral normalization and dropout. Analysis is provided and experimental results show that the method achieves good performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problem in consideration appears interesting and timely."
                },
                "weaknesses": {
                    "value": "- It would be helpful if the authors could improve the motivation of the work. In particular, the Intro and the background do not provide an effective argument as to what problems are really being addressed in this work and why should one care.  \n- The reviewer would suggest moving the algorithm pseudocode to an earlier place. It is rather inconvenient that Theorem 3.1 is presented before the algorithm. \n- Equation (7) seems to assume a Lipschitz condition on v(s)? Please elaborate. \n- It might be helpful to explain how many training steps are implemented. Also, would it be possible to show the training curves?"
                },
                "questions": {
                    "value": "- It would be helpful if the authors could improve the motivation of the work. In particular, the Intro and the background do not provide an effective argument as to what problems are really being addressed in this work and why should one care.  \n- The reviewer would suggest moving the algorithm pseudocode to an earlier place. It is rather inconvenient that Theorem 3.1 is presented before the algorithm. \n- Equation (7) seems to assume a Lipschitz condition on v(s)? Please elaborate. \n- It might be helpful to explain how many training steps are implemented. Also, would it be possible to show the training curves?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1495/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699372266509,
            "cdate": 1699372266509,
            "tmdate": 1699636078370,
            "mdate": 1699636078370,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tCf1F6WO2U",
                "forum": "thFwKIRqmG",
                "replyto": "kZCSXgnmkD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1495/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1495/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer szF5"
                    },
                    "comment": {
                        "value": "We would like to thank the reivewer for their useful feedback! We are glad the reviewer finds that our work tackles an interesting and timely problem. The reviewer brought up several useful questions and points that we would like to address and shed light on.\n\n### Questions\n\n> Equation (7) seems to assume a Lipschitz condition on v(s)? Please elaborate.\n\nWe hope that the comment above to all reviewers has addressed this question. Please to not hesitate to follow up if you have further clarifying questions concerning this point.\n\n> It might be helpful to explain how many training steps are implemented. Also, would it be possible to show the training curves?\n\nFor the MuJoCo-Gym experiments, models interact with each environment for 3 million training steps; for the MuJoCo-Brax experiments, 50 million training steps; and for the ProcGen experiments, 25 million training steps. To save space, we show all training curves in Appendix E. We will make the references to these images in the main text more apparent.\n\n### Concerns\n\n> The reviewer would suggest moving the algorithm pseudocode to an earlier place. It is rather inconvenient that Theorem 3.1 is presented before the algorithm.\n\nWe are moving the algorithm closer to its first reference on Page 4 in the updated manuscript, which we are working to share shortly."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1495/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700324881336,
                "cdate": 1700324881336,
                "tmdate": 1700324881336,
                "mdate": 1700324881336,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VPIDustl8f",
                "forum": "thFwKIRqmG",
                "replyto": "kZCSXgnmkD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1495/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1495/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you, again"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you again for sharing your valuable feedback with us. We strongly believe that the motivation, theory, and empirical results make this submission worthy of acceptance. Moreover, we think our response addresses your concerns and have updated the paper accordingly. Specifically, we added commentary on the Lipschitz condition, included your formatting suggestions, and reworked the abstract and introduction to help communicate the motivation better. If you have no further concerns, we kindly ask you to consider raising your score. Otherwise, we would also be happy to discuss further, should you find the time. \n\nBest regards,\nThe Authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1495/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615720514,
                "cdate": 1700615720514,
                "tmdate": 1700615720514,
                "mdate": 1700615720514,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MzKr25Jq3q",
            "forum": "thFwKIRqmG",
            "replyto": "thFwKIRqmG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1495/Reviewer_QokJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1495/Reviewer_QokJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers a modified version of A3C algorithm called VSOP by constraining advantage estimates to be positive and applying dropout and spectral normalization both on the actor and the critic networks. Via the application of dropout, the authors tie their presented method to Bayesian inference over critic and actor parameters \u2013 such connection for the actor requires that the advantage estimates be gamma distributed. The authors note that since the sign of advantages can be whatever, the gamma requirement is not fulfilled. As a modification, the authors then propose to clip the advantages to only non-negative values and show theoretically that this change corresponds to the policy gradient maximizing a lower bound on the state-value function plus a bounded constant. Motivated by the constant's bound, the authors propose a spectral normalization for the critic weights. VSOP is evaluated Mujoco and ProcGen benchmarks and demonstrates strong performance against several baseline methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "VSOP demonstrates strong performance in multiple environments and is mostly justified, see questions. The presented lower bound view on the policy gradient optimization of clipped advantages is novel and can be used to motivate the choice of spectral normalization for the critic weights."
                },
                "weaknesses": {
                    "value": "Main issues: \n\nWhile the results presented in the paper are indeed strong, I unfortunately find that the paper is still premature in terms of analysis. Throughout the paper it remained unclear to me how much each added trick contributes to the overall performance. As far as I understand, VSOP was ablated with: \n- VSOP (dropout, spectral, relu, thompson)\n- A3C (dropout, spectral, thompson)\n- No Spectral (dropout, relu, thompson)\n- No Thompson (dropout, spectral, relu)\n\nFrom these ablations it is evident that one cannot determine the contribution of each of the tried tricks. Given the lack of other strong justifications for performance I feel confused by the results. Furthermore, I have some doubts about the theoretical connection between the clipped advantage and MAP estimation, please see questions section."
                },
                "questions": {
                    "value": "* Advantage clipping is motivated by the fact that regular advantages cannot be Gamma distributed, because Gamma has only positive support. While clipping does fix the advantage estimates' support issue, why should this operation make the estimates Gamma distributed as assumed by the theory?\n\n* As authors forthcomingly note, the spectral normalization proves to be detrimental to performance when run in a highly parallelized manner \u2013 what could be the reason? Does this also happen with ProcGen environments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1495/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1495/Reviewer_QokJ",
                        "ICLR.cc/2024/Conference/Submission1495/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1495/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699553905308,
            "cdate": 1699553905308,
            "tmdate": 1700653846731,
            "mdate": 1700653846731,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CSI2En4GYS",
                "forum": "thFwKIRqmG",
                "replyto": "MzKr25Jq3q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1495/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1495/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QokJ"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their thoughtful review. We are glad that the reviewer finds that \"the results presented in the paper are strong\" and that the lower bound perpsective for policy gradients is novel. The reviewer mentions some good points that we would like to address.\n\n### Questions\n\n>Advantage clipping is motivated by the fact that regular advantages cannot be Gamma distributed, because Gamma has only positive support. While clipping does fix the advantage estimates' support issue, why should this operation make the estimates Gamma distributed as assumed by the theory?\n\nThe intuition behind assuming a gamma distribution for the clipped advantages is that advantages ideally have zero mean by construction (we subtract the state-action value by its expected state value over actions), so clipping at zero will result in a heavy-tailed distribution. Gamma distributions are sensible hypotheses for heavy-tailed distributions. In the following linked image, for a training run of Humanoid-v4, we plot the marginal histograms for the advantages (left) and the clipped advantages (right) over each training update:\n\n[Histogram of Clipped Advantages](https://i.imgur.com/wExvlMC.png)\n\nThe clipped advantage histogram on the right lends evidence to the gamma assumption (at least for the marginal distribution). We may expect multi-modality at the state-action level, which integration over actions may marginalize out at the state level; however, we would still expect a heavy tail in both cases.\n\n> As authors forthcomingly note, the spectral normalization proves to be detrimental to performance when run in a highly parallelized manner \u2013 what could be the reason? Does this also happen with ProcGen environments? \n\nWe hope the anaysis in the comment to all reviewers helps address these questions. For your convenience we reiterate our thoughts on these specific points here. \n\nIn the single-threaded setting, a single agent collects data. This specific experience from a single initialization, coupled with the flexibility of Neural Networks, could result in the objective maximizing a policy that encourages spuriously high-frequency (rather than high-value) value functions when the data is sparse early in training. In this case, regularization from spectral normalization would be beneficial. \n\nConversely, the algorithm collects data from many agents with unique initializations in the highly parallel setting. Thus, with more diverse and less sparse data, we can expect more robust value function estimates, less likely to be spuriously high-frequency between state transitions. Then, the $K_\\pi=1$ assumption induced by spectral normalization may be too strong and lead to over-regularization.\n\nConcerning ProcGen, we report results for an agent trained with spectral normalization under the PPO default settings of 64 threads and 256 time steps per rollout.\n\nWe would like to emphasize that our novel theoretical perspective obtains strong results in multiple settings, which we believe is a valuable contribution to the community. Please do not hesitate to follow up if you have further clarifying questions.\n\n### Concerns\n\n>Throughout the paper it remained unclear to me how much each added trick contributes to the overall performance. As far as I understand, VSOP was ablated with:\n    VSOP (dropout, spectral, relu, thompson)\n    A3C (dropout, spectral, thompson)\n    No Spectral (dropout, relu, thompson)\n    No Thompson (dropout, spectral, relu)\nFrom these ablations it is evident that one cannot determine the contribution of each of the tried tricks. \n\nYour understanding of the ablation is mostly correct. To help with clarity, we propose to make the following changes to the ablated variant names in Figure 1.:\n\n- RMPG -> VSOP \"all-actions\"\n- A3C -> VSOP no-ReLU-Adv.\n- No Spectral -> VSOP no-spectral\n- No Thompson -> VSOP no-Thompson\n\nWe hyperparameter tune each variant according to the same procedure as for VSOP. The hyperparameter tuning search space includes the dropout rate in the range of 0.0-0.1. As such, we effectivelty ablate dropout rate in the hyperparamter tuning phase to isolate the effect of the added regularization of dropout from the that of Thompson sampling. The estimated optimal dropout rates are 0.025 (VSOP, VSOP no-Thompson, and VSOP no-ReLU), 0.05 (VSOP no-spectral), and 0.0 (VSOP \"all-actions\").\n\nWe respectfully maintain these ablations are sufficient to show that the combination of the four modifications is necessary for the performance of VSOP by demonstrating that the subtraction of any modification results in decreased performance and stability.\n\nWe are also interested in the results of seeing the effect size of the remaining nine combinations with respect to A3C, but we have not seen significant evidence that any subset would lead to the performance of VSOP. Lest to justify the substantial computational cost of the hyperparameter tuning and subsequent training runs required for a fair comparison."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1495/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700336375730,
                "cdate": 1700336375730,
                "tmdate": 1700336433316,
                "mdate": 1700336433316,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OPLEe24zoz",
                "forum": "thFwKIRqmG",
                "replyto": "MzKr25Jq3q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1495/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1495/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you, again"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you again for sharing your valuable feedback with us. We strongly believe that the motivation, theory, and empirical results make this submission worthy of acceptance. Moreover, we think our response addresses your concerns and have updated the paper accordingly. Specifically, we added commentary on the normal-gamma assumption and spectral normalization and strengthened our presentation of the ablation study. If you have no further concerns, we kindly ask you to consider raising your score. Otherwise, we would also be happy to discuss further, should you find the time. \n\nBest regards,\nThe Authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1495/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615749024,
                "cdate": 1700615749024,
                "tmdate": 1700615749024,
                "mdate": 1700615749024,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7nrOTOEYEr",
                "forum": "thFwKIRqmG",
                "replyto": "CSI2En4GYS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1495/Reviewer_QokJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1495/Reviewer_QokJ"
                ],
                "content": {
                    "title": {
                        "value": "Raising score"
                    },
                    "comment": {
                        "value": "I thank the authors for the detailed response. The added analysis provides more valuable insight to understanding the presented method, which as already stated demonstrates strong performance. Thus I comfortable with raising my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1495/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653829457,
                "cdate": 1700653829457,
                "tmdate": 1700653829457,
                "mdate": 1700653829457,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]