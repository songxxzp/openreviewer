[
    {
        "title": "GPAvatar: Generalizable and Precise Head Avatar from Image(s)"
    },
    {
        "review": {
            "id": "fr9Rkz9foM",
            "forum": "hgehGq2bDv",
            "replyto": "hgehGq2bDv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission965/Reviewer_mo8c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission965/Reviewer_mo8c"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a one-shot/few-shot NeRF-based talking face system. The contributions are twofold: (1) a point-based expression field (PEF) for 3D avatar animation; (2) a multi triplanes attention (MTA) that supports multiple images as input to handle hard cases like occlusion or closed eyes. I like some ideas in this paper. This is a overall well-written paper and is easy to follow. The experiment shows good performance over previous baselines. \n\nHowever, the identity similarity in the demo video is not as good as previous one-shot 3D talking face methods (such as HiDe-NeRF). Besides, I'm also curious about the performance of this method under large head poses, which is not revealed in the demo."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- I like the idea of PEF since it well utilizes the geometry prior of FLAME to help learn the avatar animation in the 3D space. \n- Also, it is the first one-shot 3D talking face paper that focuses on the few-shot setting.\n- The paper is well-written and is easy to follow."
                },
                "weaknesses": {
                    "value": "- the PEF could well handle the segment modeled by FLAME (such as head and torso), but it cannot handles other parts, such as hair and clothes. See question 1.\n- The identity similarity in the demo video is worse than some baseline (HideNeRF).\n- The image quality can be improved. For instance, in Figure 1, the predicted images in the second column seems blurry."
                },
                "questions": {
                    "value": "- In  PEF, the expression feature of facial part can be queried from the FLAME mesh, but the non-facial part, such as hair, clothes, and background is not modeled by FLAME. The authors said \"we instead search for neighboring points in the entire space\", but it is not clear how it bundles the non-facial part in the 3D space with the learnable features.\n- The identity similarity in the demo video is not as good as previous one-shot 3D talking face methods (such as HiDe-NeRF), what's the cause?\n- The head movement in the provided video is quite gentle. Is there any demo where the head pose is larger (such as side view)? Since one of the biggest advantage of 3D methods over the traditional 2D methods is the good quality under a large view angle, I think this is necessary for the reviewer to assess the performance of a 3D-based work.\n- Could you provide the visualization of the attention weights in the multi-reference setting? I'm also curious about the scalability of the MTA, how is the attention map looks like under the two-in/five-in/ten-in ?\n- In the demo video, the avatars are driven by audio, it is better to illustrate the way used to obtain the facial expression."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission965/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698659394000,
            "cdate": 1698659394000,
            "tmdate": 1699636022426,
            "mdate": 1699636022426,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aWcAKpQsAm",
                "forum": "hgehGq2bDv",
                "replyto": "fr9Rkz9foM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission965/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive feedback and detailed review. We have taken your suggestions and conducted experiments to improve the identity similarity. Our responses below address each of your points.\n### W1: About the PEF\nPlease refer to the Q1.\n### W2: About the identity similarity\nPlease refer to the Q2.\n### W3:  About the  image quality\nSince our model is only trained on VFHQ data (characters from TV interviews) and inferred on unseen IDs, for some inputs like CG characters, there are some cases where we cannot generate clear reconstruction. We hope that we can solve this problem by enhancing data or improving cross-domain robustness in future work. It is worth noting that even though we have not added these improvements, our model still has amazing generalization ability and is able to handle many cross-domain cases.\n### Q1: How the PEF handles some non-facial parts.\nOur feature space mainly consists of two parts, the canonical tri-planes feature space and the expression feature space based on the FLAME point cloud (PEF). Non-face areas are overall modeled by the tri-planes space, which is essentially a rigid body. If we limit the feature search of the expression field to the FLAME area, then parts such as hair and torso will be completely modeled as rigid bodies that change as the camera perspective rotates. In order to be able to solve some non-flame modeling areas related to the expressions, we relax the nearest neighbor search distance of the expression feature field, so that some hints of the current expression can be obtained in the entire tri-planes feature space, which makes some non-FLAME areas more flexible. Especially areas such as hair and glasses that are close to the face but are not modeled by FLAME. The quantitative ablation experiment on global sampling also confirmed that this can improve various metrics including reconstruction quality and expression control fineness.\n\n### Q2: About the identity similarity of our method and HideNeRF\nWe examined the technology and implementation used by HideNeRF in detail, and found that Arcface was used to establish identity loss to improve identity consistency. We also adopted the same method and conducted experiments, and we can observe that our identity consistency has also been improved, but at the same time, the reconstruction quality and expression control accuracy have decreased. We believe that we may need to conduct more experiments to find a better balance between multiple losses to obtain better results. We will continue to work on this part. But it is worth noting that the fine control of expressions and the substantial improvement in reconstruction quality are our main contributions and the features we most want to retain.\n\nIn the following is the result when we add Arcface-based identity loss L_ID. Among them, ours one-in w/o L_ID is the result in the original paper version, and w L_ID is the result with identity loss added.\n|   Exp on VFHQ              | PSNR\u2191 |  SSIM\u2191| LPIPS\u2193|CSIM\u2191  |   L1\u2193 | AED\u2193  | APD\u2193  | AKD\u2193 | CSIM\u2191 | AED\u2193  | APD\u2193  |\n|---                    |---    |---    |---    |---    |---    |---    |---    |---   |---    |---    |---    |\n| HideNeRF              | 20.07 | 0.745 | 0.204 | 0.794 | 0.056 | 0.521 | 0.031 | 5.33 | 0.558 | 1.024 | 0.044 |\n| Ours One-in w/o L_ID  | **22.08** | **0.765** | **0.177** | 0.789 | **0.039** | **0.434** | **0.017**| **3.53** | 0.558 | **0.910** | **0.034** |\n| Ours One-in w L_ID    | 21.48 | 0.759 | 0.187 | **0.797** | 0.043 | 0.464 | 0.017 | 3.84 | **0.574** | 0.932 | 0.035 |\n\n|     Exp on HDTF         | PSNR\u2191 |  SSIM\u2191| LPIPS\u2193|CSIM\u2191  |   L1\u2193 | AED\u2193  | APD\u2193  | AKD\u2193 | CSIM\u2191 | AED\u2193  | APD\u2193  |\n|---                    |---    |---    |---    |---    |---    |---    |---    |---   |---    |---    |---    |\n| HideNeRF              | 21.38 | 0.803 | 0.147 | 0.907 | 0.038 | 0.499 | 0.027 | 4.33 | 0.803 | 1.031 | 0.032 |\n| Ours One-in w/o L_ID  | **24.21** | **0.834** | **0.131** | 0.871 | **0.029** | **0.427** | **0.012** | **3.06**| 0.790 | **0.869** | **0.020** |\n| Ours One-in w L_ID    | 23.67 | 0.821 | 0.136 | **0.910** | 0.034 | 0.443 | 0.013 | 3.45 | **0.812** | 0.892 | 0.020 |"
                    },
                    "title": {
                        "value": "Reply to Reviewer mo8c (Part1)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507743086,
                "cdate": 1700507743086,
                "tmdate": 1700507802233,
                "mdate": 1700507802233,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c0RLZ4Ng21",
                "forum": "hgehGq2bDv",
                "replyto": "fr9Rkz9foM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission965/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer mo8c (Part2)"
                    },
                    "comment": {
                        "value": "### Q3: About the larger head pose\nWe agreed that a large change in the view angle is crucial for evaluating a 3D-based method. The animation sequence in our audio-driven demonstration is extracted from SadTalker, which always has smooth and gentle head movements. But in our paper numerous results show significant head movements, even in side views (as illustrated in Figure 7). These results show that our method effectively inherits the advantages of 3D methods in view angle changes.\n\n### Q4: The visualization of the attention weights\nWe appreciate the reviewer's insightful suggestion to show attention visualizations, which can better show how our MTA module works. For now, we have included these visualizations in the revised version (Sec.C and Figure.9). These attention visualization results are very interesting and show that our method can indeed pay attention to and combine different angles of people in different images.\n\n### Q5: How we obtain facial expression from audio\nWe employ a widely used audio-driven motion generator: SadTalker to generate head motion based on audio input, although other similar methods can also be used. SadTalker uses an image and a piece of audio to generate a video, and then we track the 3DMM parameters in that video. We then use these parameters to animate the GPAvatar. We will add descriptions in future demos."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507771152,
                "cdate": 1700507771152,
                "tmdate": 1700510225105,
                "mdate": 1700510225105,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UyghruVTvs",
                "forum": "hgehGq2bDv",
                "replyto": "c0RLZ4Ng21",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission965/Reviewer_mo8c"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission965/Reviewer_mo8c"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the reply. The authors have addressed most of my concerns, so I keep my initial rating of 6. By the way, do you have the plan to open-source the code in the future?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631852151,
                "cdate": 1700631852151,
                "tmdate": 1700631852151,
                "mdate": 1700631852151,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DsijxVgigI",
            "forum": "hgehGq2bDv",
            "replyto": "hgehGq2bDv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission965/Reviewer_q9Mp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission965/Reviewer_q9Mp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method that reconstructs 3D head avatars from images and synthesizes realistic talking head videos. It takes as input a single or a small number of face images and reconstructs 3D head avatars in a single forward pass. It extends the formulation of NERFS and proposes a dynamic point-based expression field that is driven by a point cloud, motivated by the need to have an accurate control of facial expressions. In addition, the proposed method adopts a Multi Tri-planes Attention (MTA) fusion module that facilitates the 3D representation of the scene and the incorporation of information from multiple input images. The proposed method is compared with several SOTA methods and achieves promising results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The proposed method achieves promising results and the supplementary videos show that the videos synthesized by the proposed method are in general realistic and visually pleasing. \n\n+ The experimental evaluation is detailed and systematic. The proposed method is compared with several recent SOTA methods that solve the same problem."
                },
                "weaknesses": {
                    "value": "- The presentation in several parts of the paper, especially in the methodology, is unclear and needs several clarifications. See detailed comments in Questions below. \n\n- The following paper is not cited, despite the fact that it is very closely related in terms of methodology:\n\nAthar, S., Shu, Z. and Samaras, D., 2023, January. Flame-in-nerf: Neural control of radiance fields for free view face animation. In 2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG) (pp. 1-8). IEEE.\n\nThe above un-cited paper also uses a FLAME-based representation of the 3D face and extends the formulation of NERFS to achieve realistic face animation with expression control. The similarities and differences with the proposed method are not discussed and the Flame-in-nerf is not included in the comparisons of the experimental section.  This raises concerns in terms of the real novelty and contributions of the proposed method. \n\n- Furthermore, there are also several other closely related works that are not cited. For example: \n\nJiaxiang Tang, Kaisiyuan Wang, Hang Zhou, Xiaokang Chen, Dongliang He, Tianshu Hu, Jingtuo Liu, Gang Zeng, and Jingdong Wang. Real-time neural radiance talking portrait synthesis via audio-spatial decomposition. arXiv preprint arXiv:2211.12368, 2022.\n\nYudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun Bao, and Juyong Zhang. Ad-nerf: Audio driven neural radiance fields for talking head synthesis. In ICCV, pp. 5784\u20135794, 2021.\n\nYu, H., Niinuma, K. and Jeni, L.A., 2023, January. CoNFies: Controllable Neural Face Avatars. In 2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG) (pp. 1-8). IEEE."
                },
                "questions": {
                    "value": "- Section 3.2: the paper fails to clearly explain how the expression information affects the process of building a point-based expression field.\n\n- Section 3.3: the paper provides insufficient details about about how the canonical encoder is defined and built.\n\n- Figure 4: For several columns with results, it is unclear which is the corresponding method. There is one column more than the number of methods in the caption and one column more than the columns of Figure 5. This creates confusions. \n\n- Equation (1): the definition of w_i does not seem to make sense. Is there a missing norm in the denominator?\n\n- After Equation (2): N is refereed to as the \"input number\", but apparently it should be referred to as the number of input images"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "As is the case for all methods of this field, the proposed method could be misused in order to create deep fake videos of a person without their consent. This raises issues of misinformation, privacy and security. Section 6 includes some discussion about this issues. However, this discussion could have been more detailed, with a more in-depth discussion of specific mitigation measures."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission965/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission965/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission965/Reviewer_q9Mp"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission965/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836823250,
            "cdate": 1698836823250,
            "tmdate": 1699636022340,
            "mdate": 1699636022340,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FFNny5GVET",
                "forum": "hgehGq2bDv",
                "replyto": "DsijxVgigI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission965/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer q9Mp"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback and time to review our paper. We have taken your suggestions and further refined our paper. Please find our detailed responses to each of your points raised below:\n### W1: Methodology needs several clarifications\nPlease refer to the responses to the corresponding questions.\n### W2-3: About the missing citations.\nThanks for pointing this out. We acknowledge the relevance of this paper and include it in our updated related work section.\nHowever, it is worth noting that our method is not comparable to these methods. Our approach focuses on oneshot and fewshot settings, meaning the input is only one or a few images, no training on new identities is required, and our model is driven by motion sequences based on FLAME parameters.\nAmong these papers, the paper [1] requires videos as input (may have thousands of images), and each ID-specific model needs to be trained based on the videos. This prevents the model from making inferences on unseen IDs, but requires data from that ID for training. This is different from our setting, where we use unseen IDs during inference to evaluate our method, which is not possible in paper [1]. At the same time, the way it uses FLAME is also different from us. Our motivation is to avoid the loss of expression information, but the paper [1] adopts a method similar to Figure.2(c), by directly inputting the expression vector to control the expression, which will obviously lead to expression Loss of information.\n\nPapers [2] and [3] use audio signals as the driving method, which makes it difficult to accurately control the avatar to make the desired expression, and since we use FLAME parameters to control the avatar to make expressions, the difference in control signals makes we differ from papers [2] and [3] and cannot evaluate in the same setting.\n\nThe setting of paper [4] is similar to paper [1], requiring the use of video as training data and cannot infer with an unseen ID. This makes paper [4] not comparable to our method.\n\n### Q1: Sec 3.2, how the expression information affects the PEF?\nOur feature space mainly consists of two parts, the canonical tri-planes feature space and the expression feature space based on FLAME point cloud (PEF). Expression information is given in the form of FLAME parameters, and the FLAME model dynamically forms a point cloud based on these parameters. That is, different FLAME parameters will generate point clouds with different positions. Since our PEF is based on these point clouds, the feature queried from PEF will also be changed with different expression parameters.\n\n### Q2: Sec 3.3, how do we build the canonical encoder?\nWe adopt the same Style-UNet structure as in GFPGAN with only modification of the input and output size. \nIn short\uff0cwe obtain the style code through 4 groups of ResBlock down-sampling, use 3 groups of ResBlock up-sampling to obtain the conditions, and then use StyleGAN to output the 3 \u00d7 32 \u00d7 256 \u00d7 256 tri-planes based on style code and conditions. For more implementration details please refer to Sec.A.1, and we will also release the code and checkpoints to facilitate reproducibility and further research. \n\n### Q3: Figure 4, missing column title\nThe last two columns actually correspond to \"Ours One-in\" and \"Ours Two-in\". For a fair comparison, we decided to only compare here the driving results of a single image as input. In the revised version we have removed \"Ours Two-in\" in Figure 4. Results of Ours Two-in or more can be referenced from other Figures (like Figure 6,7,8,9).\n\n### Q4-Q5: About the equations.\nWe appreciate the reviewers for identifying sign errors in our formulas. These mistakes have been rectified in the revised version. The corrected equation is $$f_{exp,x} = \\sum_{i}^{K} \\frac{w_i}{\\sum_{j}^{K} w_j} L_p(f_{i}, F_{pos}(p_i-x)), \\text{where}~w_i=\\frac{1}{p_i-x},$$ where $x$ is the coordinate to be queried, $K$ is the number of neighbors, $f_i$ is the corresponding feature, $p_i$ is the point coordinate, $L_p$ is the linear layers and $F_{pos}$ is the frequency positional encoding function. In this equation, the $w_i$ ensures that the nearest point has the highest weight when contributing to the feature. \n\nIn Equation (2): $N$ should be referred to the number of input images and we also correct it in the revised version.\n\n### Ethic concerns:\nWe agree with you that there are potential ethical risks with talking head generation. Please refer to the \"Response to Ethics Concerns\" for details. We have also added more discussion about ethics in Sec.7 of the revised paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505402396,
                "cdate": 1700505402396,
                "tmdate": 1700505467582,
                "mdate": 1700505467582,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u9kmW9LpNF",
            "forum": "hgehGq2bDv",
            "replyto": "hgehGq2bDv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission965/Reviewer_MrBB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission965/Reviewer_MrBB"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, a novel framework named GPAvatar is introduced, designed to reconstruct 3D head avatars seamlessly from one or multiple images in a single forward pass. The key novelty lies in the incorporation of a dynamic point-based expression field, guided by a point cloud, to intricately and efficiently capture facial expressions. The authors present the concept of a dynamic Point-based Expression Field (PEF), enabling accuracy and control of expressions across different identities. Additionally, they introduce a Multi Tri-planes Attention (MTA) fusion module, capable of handling a varied number of input images with precision."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Overall, the paper is well-organized and easy to follow. The motivation is clear. The figures and tables are informative.\n\n- Experimental results demonstrate that the proposed method achieves the most precise expression control and state-of-the-art synthesis quality (StyleHeat, ROME, OTAvatar, and Next3D) (based on NeRF and 3D generative models)n on multiple on VFHQ and HDTF benchmark datasets."
                },
                "weaknesses": {
                    "value": "- The model proposed has overall more trainable parameters compared to baseline models, which could potentially bring in some unfairness during comparison with other works.\n- No discussion about the limitations of the approach?"
                },
                "questions": {
                    "value": "- It is not clear to me how the model captures the subtle information such as closed eyes ? \n- why the normalization of the weight wi in the equation is required?\n- Several terms in the equation (page 3) I_t= R (....) are not defined. What R means?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Since the proposed framework allows of head avatars, it has a wide range of applications but also carries the potential risk of misuse. The authors are also considering methods like adding watermarks to synthetic videos to prevent misuse."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission965/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699208416262,
            "cdate": 1699208416262,
            "tmdate": 1699636022261,
            "mdate": 1699636022261,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "40FNhCilOy",
                "forum": "hgehGq2bDv",
                "replyto": "u9kmW9LpNF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission965/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission965/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MrBB"
                    },
                    "comment": {
                        "value": "Thank you for your kind feedback and your pointers and questions. We have refined our revised version paper based on your suggestions. Please find our answers for each of your points below.\n### W1. About trainable parameters\nGiven the wide range of parameters in these models, there is a concern regarding fair comparisons. However, it is quite difficult to evaluate these methods at the same parameter level due to different custom structures. Notably, our method does not have the most trainable parameter among all the baselines: our model has 96M parameters but HideNeRF has 569 million parameters.\n### W2. About the limitations of our approach\nWe appreciate the reviewer for pointing out this missing part. Specifically, our current FLAME-based model lacks a module to control the shoulders and body, resulting in limited control below the neck (the shoulder position generally aligns with the input image for now). Additionally, for other areas not modeled by FLAME, such as hair and tongue, explicit control is not feasible. Furthermore, while our aspiration is to achieve real-time reenactment of more than 30 fps, our current performance is pre-real-time for now (approximately 15 fps on the A100 GPU). We added the discussion about limitations in our revised paper (Sec.B), and will solve these problems as future work.\n### Q1. About how we captures the subtle information\nWe avoid the information loss resulting from over-processing using our PEF (illustrated in Figure 2). Consequently, any subtle details captured by FLAME directly influence the point positions in the PEF, thereby contributing to the rendering results. \n### Q2-Q3. About the normalization weights in equation 1 and R in equation (Page 3).\nWe appreciate the reviewers for identifying sign errors in our formulas. These mistakes have been rectified in the revised version. The corrected equation is\n$$f_{exp,x} = \\sum_{i}^{K} \\frac{w_i}{\\sum_{j}^{K} w_j} L_p(f_{i}, F_{pos}(p_i-x)), \\text{where}~w_i=\\frac{1}{p_i-x}, $$ where $x$ is the coordinate to be queried, $K$ is the number of neighbors, $f_i$ is the corresponding feature, $p_i$ is the point coordinate, $L_p$ is the linear layers and $F_{pos}$ is the frequency positional encoding function. In this equation,  the  $w_i$ ensures that the nearest point has the highest weight when contributing to the feature. While for the $R$ in equation (page 3), it refers to the volume rendering process in NeRF, which renders the merged feature space into the result image."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission965/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502855413,
                "cdate": 1700502855413,
                "tmdate": 1700505438884,
                "mdate": 1700505438884,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]