[
    {
        "title": "Test like you Train in Implicit Deep Learning"
    },
    {
        "review": {
            "id": "rkp1JwlCDv",
            "forum": "RzV7QRowUl",
            "replyto": "RzV7QRowUl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission150/Reviewer_uPNB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission150/Reviewer_uPNB"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates implicit deep learning, particularly in meta-learning and Deep Equilibrium Networks (DEQs). A common belief is that increasing the number of iterative solutions (inner iterations) during inference improves performance. This study challenges that notion, providing a theoretical analysis that highlights overparametrization as a crucial factor. The findings reveal that overparametrized networks, like DEQs, don't benefit from extra iterations at inference, while meta-learning does."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The topic is interesting, and the paper clearly states the motivation, system model, and assumptions."
                },
                "weaknesses": {
                    "value": "1. It is hard to interpret the meaning of main theorems. For example, Eq. (9) in Theorem 1 contains orthogonal projections and $E_N$ while their values are unknown.\n\n2. The tightness of the bounds shown in these theorems is not discussed. It will be beneficial to use a figure to compare the actual value and the analytical bound."
                },
                "questions": {
                    "value": "The major questions I have are about $D(N,\\Delta N)$, i.e., the change of the training loss after changing number of inner iterations by $\\Delta N$ for a fixed learned $\\theta^{\\*,N}$. First, this definition seems problematic because when $N$ changes, $\\theta^{\\*,N}$ should change. In other words, we should not fix $\\theta^{\\*,N}$. Second, this is on training loss, not the test loss. The relationship between training loss and test loss is not trivial, especially when overparameterized. In the paper, the authors claim that \"This quantity is a proxy for the increase in test loss, provided we have access to enough training data\", which I doubt since the meaning of \"enough training data\" is very vague (or even contradictory) in the context of overparameterization."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission150/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818236481,
            "cdate": 1698818236481,
            "tmdate": 1699635940228,
            "mdate": 1699635940228,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Eu4jpD0CUF",
                "forum": "RzV7QRowUl",
                "replyto": "rkp1JwlCDv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission150/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission150/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their review.\nWe tackled the question of the discrepancy between train and test in the common response (*C1*).\nWe will answer each point subsequently.\n- **Explanation of theorem 1**: Theorem 1 states that it is possible to improve the loss by changing the number of iterations only up to a \u201cpoint\u201d, and gives a quantitative result for this \u201cpoint\u201d (i.e. the lower bound). We even show in the subsequent Corollary 1, that in the most extreme case it is not possible to improve the loss.\nWe will add this clarification after theorem 1 in the camera-ready version of our paper.\nWe recall that all notations are defined within theorem 1 or its assumptions.\n- **Tightness of the bound**: The bound is tight and we will mention it in the camera-ready version of our paper. It comes from the fact that only triangular inequalities were used in the proof. Fig 1. (left) shows the tightness under the assumptions of Corollary 1. \n- **Adapting $\\theta^{\\star, N}$**: Indeed, we agree that this point is not intuitive, but two aspects justify such a framing of the problem: 1. It is the current practice not to refit the model and use it with a different number of inner iterations as we found in all the state of the art implementations. This is therefore the mathematical equation behind what happens in practice. 2. It has been claimed in the literature that DEQs have the possibility to generalize beyond the $N$ for which it was trained. We wanted to show that, indeed, it is important in the current state to use DEQs with the $N$ for which it was trained."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission150/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700309274707,
                "cdate": 1700309274707,
                "tmdate": 1700309274707,
                "mdate": 1700309274707,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7PHAEO40O3",
                "forum": "RzV7QRowUl",
                "replyto": "Eu4jpD0CUF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission150/Reviewer_uPNB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission150/Reviewer_uPNB"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the explanation. I keep my original rating."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission150/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709693219,
                "cdate": 1700709693219,
                "tmdate": 1700709693219,
                "mdate": 1700709693219,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "j7OWxEozQQ",
            "forum": "RzV7QRowUl",
            "replyto": "RzV7QRowUl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission150/Reviewer_YcSM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission150/Reviewer_YcSM"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the inner iterations overfitting problem of overparameterized models in implicit deep learning, and provides theoretical results in a simplified affine setting to show that increasing the number of iterations at test time cannot improve\nperformance for overparametrized models. Two typical implicit deep learning methods, DEQ and iMAML, are considered in the paper. Experiments on diverse tasks verifies the theorem on both DEQ and iMAML."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well written and easy to follow.\n2. The definition and analysis of inner iterations overfitting problem is novel and will be helpful for future researches on implicit deep learning."
                },
                "weaknesses": {
                    "value": "Experiments can be further improved. Fore example, \n- it only considers the case where $N$ is fixed while $\\Delta N$ varies (e.g., Figure 2 & 3). Does the conclusion hold for other choices of $N$?\n- Theorem 1 is validated on a small scale dataset. It will be helpful to validate it on real dataset."
                },
                "questions": {
                    "value": "See the section of Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission150/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698861466390,
            "cdate": 1698861466390,
            "tmdate": 1699635940094,
            "mdate": 1699635940094,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Nb3UyrCg4Y",
                "forum": "RzV7QRowUl",
                "replyto": "j7OWxEozQQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission150/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission150/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their review.\nWe will answer each point subsequently.\n- **Varying $N$**: In each experiment, $N$ is different, in other words we don\u2019t use the same number of inner iterations for different settings. The idea here was to reuse pre-trained DEQs, which generally have very fine hyperparameter tuning to maximize the performance on the task and for a specific number of inner iterations. Each $N$ is specified in Table 1. in the Appendix. \nUsing different $N$ would mean retraining all of these models ourselves, which is beyond our computational budget.\nWe also wanted to make sure to uncover this phenomenon for models that were trained by others in order to strengthen our findings.\n- **Validation of Theorem 1**: we tried to verify Corollary 2 on a more real dataset in Figure 4. Figure 1. was indeed meant as an illustration rather than a validation of the theorem, and we believe that our experiments on diverse real-world dataset demonstrate the applicability of our findings."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission150/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700309246639,
                "cdate": 1700309246639,
                "tmdate": 1700309246639,
                "mdate": 1700309246639,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iGHIzfUHMq",
            "forum": "RzV7QRowUl",
            "replyto": "RzV7QRowUl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission150/Reviewer_UkB2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission150/Reviewer_UkB2"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies when increasing the number of fixed-point iterations in implicit deep learning during test time improves performance. In implicit deep learning, the network must _implicitly_ minimize the objective $\\ell(z(\\theta))$. Specifically, the network parameters $\\theta$ control some intermediary output $z(\\theta)$ which tends to be the solution of some inner rootfinding problem $f(z, \\theta, D_{train}) = 0$. The solution $z(\\theta)$ is often identified through some $N$ fixed-point updates during training, then during test time, the solution is updated using test data using more fixed-point updates. This helps in certain applications like meta-learning and not others like DEQ. \n\nThe paper attempts to prove that this difference (helping in certain cases and not others) can be explained by the degree $\\theta$ is overparameterized for the inner problem. They prove lower bounds for how much the training loss can change when changing the number of fixed-point iterations for an affine inner problem."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is generally clear and easy to follow. \n- Excluding a couple minor things (bullet points below), the proofs in Appendix Section A for the main theorems in the main body seem to be correct. \n    - Between Eq. 22 and 23, it says \u201cTherefore $z_{N+1}$ is also in the range of $K^\\top$, should say $z_{N+1} - z_0$. \n    - Eq 30 has some problems with the signs, although the final form Eq 31 seems to be correct. \n- The empirical results in Figures 2/3 seem to support their hypothesis that overparameterization leads to less improvements with more inner updates."
                },
                "weaknesses": {
                    "value": "- As already pointed out by the authors, there is a major inconsistency between what the paper tries to prove and what is actually proven. During inference, the inner optimization is conducted on the _test data_. However, the paper\u2019s results are only showing how the _training_ loss can fluctuate with the number of inner optimization updates. The problem with this relaxation is because it interferes directly with the paper\u2019s core result, that network overparametrization is why more inner optimization steps do not help. One can imagine in the overparametrized setting, the test loss/inner problem could be significantly different from the training loss/inner problem, causing an analysis on just the training loss to fall apart. I may be misunderstanding the paper however, and so I vote for a low score with low confidence.\n\n- I think the authors can be more precise about what they mean by overparametrization. Formally, they do define it as $d_x < d_\\theta$ in Corollary 1, but it might be good to clarify how $d_x$ scales for the different applications (DEQ, meta-learning) in the main body. Is this definition related to the classic usage of the term \u201coverparametrization\u201d as the number of parameters being larger than the number of training data in linear models? And the word overparametrization is a bit thrown around loosely in the paper. \n\n- Figure 4 plots $D(N, \\Delta N)$ of the training loss for different levels of \u201coverparameterization\u201d which they measure using the training loss (lower training loss \u2192 more expressive model). They conclude from the experiment that \u201cThe lower the training loss, the higher the $D(N, \\Delta N)$\u201d. This comparison seems a bit unfair and requires normalizing $D(N, \\Delta N)$ with respect to the training loss. For a loss lower bounded by 0 (like most losses), if the training loss is already small, the lowest possible improvement with more fixed point iterations $D(N, \\Delta N)$ obviously gets smaller. \n\n\n- There\u2019s some minor spacing issue of Figure 2"
                },
                "questions": {
                    "value": "- What is \u201cconvergence\u201d plotted in FIgure 2?\n- I am not too familiar with meta-learning literature and am a bit confused about doing the fixed-point iterations during inference, specifically for meta-learning. As implied by Equation 5, it seems to require access to test labels..Is the inner optimization during inference conducted on the test data using labels?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission150/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699222222564,
            "cdate": 1699222222564,
            "tmdate": 1699635940012,
            "mdate": 1699635940012,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x6J7812HTn",
                "forum": "RzV7QRowUl",
                "replyto": "iGHIzfUHMq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission150/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission150/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for engaging in the review process benevolently.\nWe tackled the question of the discrepancy between train and test in the common response (*C1*).\nWe will answer all other raised points subsequently.\n\n- **Typos in the proofs**: Thank you for spotting these, we correct them in our revised version.\n- **Clarification of overparametrization**: \nWe take a clear definition of over-parameterization, which is that we can learn any vector in our latent space; this should be connected to the number of training points. This clear definition does not necessarily match the usual definition of #params > #samples, but it has the same intuitive flavour of having a large enough number of parameters to be able to model a wide class of functions.\nThen, there's the practical definition of overparameterization, in a practical network that does not necessarily have the same architecture as the one studied.\n- **Fig 4: fairness of comparison across losses**: We think this is a very good point, and have reworked Fig 4. to include this normalization. It should be noted that with the normalization the result is less striking for MAML, but we still obtain a negative correlation score of $-0.4$ with a p-value of $0.03$ (i.e. statistically significant) without rerunning the experiment. For iMAML the result is still very obvious with a negative correlation score of $-0.6$ and a p-value of $7.10^{-4}$.\n- **Fig 2**: spacing thanks for noticing, we will correct this in the camera-ready version of our work.\n- **Convergence**: In fig 2., convergence is the norm of the error on the fixed point, i.e. in our notations $\\|z_{N+\\Delta N}(\\theta^{\\star, N}) - f(z_{N+\\Delta N}(\\theta^{\\star, N}), \\theta^{\\star, N})\\|$. We clarify this in our revised version in the caption of Fig 2.\n- **Meta-learning with validation**: Indeed, in order to solve the meta-learning problem, we need to use a set of pairs of datasets: each pair has a training set and a validation set. During training, meta-learning use both the training and validation set in order to learn an initialization (or anchor weights) that when fitted on the training data allows to generalize well when evaluated on the validation data.\nWhen testing with weights $\\theta^{(\\text{meta})}$, we use other pairs of training and validation datasets, as done by Finn et al. 2017 and Rajeswaran et al. 2019."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission150/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700309171780,
                "cdate": 1700309171780,
                "tmdate": 1700309209792,
                "mdate": 1700309209792,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]