[
    {
        "title": "Conditional MAE: An Empirical Study of Multiple Masking in Masked Autoencoder"
    },
    {
        "review": {
            "id": "MnyVJ6O5iF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1594/Reviewer_xPJ6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1594/Reviewer_xPJ6"
            ],
            "forum": "JfcLYCqOkQ",
            "replyto": "JfcLYCqOkQ",
            "content": {
                "summary": {
                    "value": "The paper proposes a multiple masking strategy for MAE to enhance the local perception ability of Masked Image Modeling. In addition, the paper also summarizes several takeaways from the observation. Downstream experiments on classification, object detection and semantic segmentation show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper proposes an incremental masking strategy based on MAE, which may provide some experience for the community. \n\nThe experiments show the performance gains on multiple downstream tasks."
                },
                "weaknesses": {
                    "value": "Actually, the contribution is limited. The proposed method is more of an empirical technique. I do understand the authors have done lots of tuning experiments, however, most of them are not that significant, or even well-known for the community (e.g., the one-shot masking). \n\nIn addition, the three-shot masking is significantly worse than the baseline as shown in Table 5, further implying its limitation."
                },
                "questions": {
                    "value": "Please see the weakness. \n\nTypos: fine-grind --> fine-grained"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1594/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697437982938,
            "cdate": 1697437982938,
            "tmdate": 1699636088028,
            "mdate": 1699636088028,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zTL6c6bW7G",
                "forum": "JfcLYCqOkQ",
                "replyto": "MnyVJ6O5iF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1594/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1594/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xPJ6"
                    },
                    "comment": {
                        "value": "Firstly, we sincerely thank the reviewer for acknowledging the effort we have made to conduct extensive experiments.\n\n**However, we do not agree with the reviewer's comment that the contribution is limited.** In this paper, we are the first to present an in-depth analysis of masking which is important for masked autoencoder. Our study gives a comprehensive empirical analysis and sheds light on how multiple masking affects the optimization in training and performance of pretrained models. More importantly, we observe that multiple masking is capable of introducing locality bias to models. e.g., introducing more locality to models. **Our contributions are also acknowledged by Reviewer JZAj and GAuN.**\n\n**We also do not agree with the reviewer that the performance is not that significant.** In our experiments, our two-shot masking has the potential to significantly improve the baseline (MAE). Please see Figure 3(b). For example, Our two-shot masking L(0, 10) basically outperforms the baseline by 2% in ImageNet100. We also empirically demonstrate that multiple masking has the potential in fine-grained classification, outperforming baseline (MAE) in three widely-used fine-grained datasets by above 2%. Please see Table 2. Additionally, our best-performing setting with ViT-B has outperformed baseline by 0.3% in ImageNet1k. The reviewer comments that the three-shot masking is **significantly** worse than the baseline as shown in Table 5. As shown in Table 5, our three-shot masking is about 0.6% lower than the baseline. **If the 0.6% performance gap is significant, how about our enhancement of two-shot masking over baseline, which is over 2%?**\n\nWe acknowledge that the three-shot masking is worse than the baseline. However, the reviewer's comment diverges from the original goal of our three-shot masking experiments. **To give a comprehensive analysis to subsequent researchers, it is our responsibility to explore the boundary of improvement, hoping to inspire future work** Additionally, though three-shot masking is worse than the baseline in the general classification, we find that the three-shot model outperforms the baseline in three fine-grained datasets by around 0.5% in Table 9 in Appendix. These results demonstrate that the three-shot masking is also meaningful."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1594/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700271175560,
                "cdate": 1700271175560,
                "tmdate": 1700271175560,
                "mdate": 1700271175560,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bvNmNPnS5h",
                "forum": "JfcLYCqOkQ",
                "replyto": "MnyVJ6O5iF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1594/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1594/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer xPJ6\uff1a\n\nAs the rebuttal period is ending soon, we wonder if our response answers your questions and addresses your concerns. \n\nThanks again for your very constructive and insightful feedback!  Looking forward to your post-rebuttal reply!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1594/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639271913,
                "cdate": 1700639271913,
                "tmdate": 1700639271913,
                "mdate": 1700639271913,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "m6QFKJNyBz",
            "forum": "JfcLYCqOkQ",
            "replyto": "JfcLYCqOkQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1594/Reviewer_MagS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1594/Reviewer_MagS"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies masking strategies in masked autoencoders. Moreover, the paper proposes a multi-shot masking strategy for MAE,  and shows the results of downstream tasks, such as image classification, object detection, and semantic segmentation. However, the experiments mainly focus on ImageNet-100 and several downstream tasks to demonstrate the effectiveness. Meanwhile, I am confused about the motivation of this paper."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper explores the application of masking in MAE training and validates its effectiveness through a series of comprehensive experiments. The problem they aim to address seems reasonable to me. The writing in the paper is clear and comprehensible."
                },
                "weaknesses": {
                    "value": "1. Readers are confronted with a perplexing dilemma due to the lack of support for motivation in the given context. The second paragraph in the introduction is particularly confusing. The reasons behind the failure of these methods and the evidence supporting the interior robustness of MAE are uncertain. The original MAE paper indicates a more comprehensive evaluation of robustness. These statements appear to be fabricated.\n\n2. In fact, VideoMAE v2 is the first to propose to mask MAE. Therefore, this paper is not the first research to explore the impact of multiple shots masking in MAE.\n\n3. The experimental setting of this paper does not convince me of its results. The dataset selection has several issues. For the main experiments, the author selected the less commonly used benchmark, ImageNet-100. Since the scale and diversity of the data can significantly influence the experimental results, the validity of comparing with MAE is uncertain. Furthermore, the author also failed to choose common datasets for the downstream tasks. Moreover, the article makes adjustments under the original experimental setup conditions of MAE, thus rendering the conclusions drawn from the experiments inaccurate.\n\n4. The proposed method appears to lack technical contributions, and I fail to discern any tangible practical benefits in terms of performance or speed."
                },
                "questions": {
                    "value": "Implementing more experiments on the benchmark ImageNet-1k dataset with the same experimental setting is necessary."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1594/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1594/Reviewer_MagS",
                        "ICLR.cc/2024/Conference/Submission1594/Senior_Area_Chairs"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review needed."
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1594/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698205984739,
            "cdate": 1698205984739,
            "tmdate": 1700651912971,
            "mdate": 1700651912971,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wIx5iUJR6W",
                "forum": "JfcLYCqOkQ",
                "replyto": "m6QFKJNyBz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1594/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1594/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer MagS (1/2)"
                    },
                    "comment": {
                        "value": "**Q1:** Readers are confronted with a perplexing dilemma due to the lack of support for motivation in the given context. The second paragraph in the introduction is particularly confusing. The reasons behind the failure of these methods and the evidence supporting the interior robustness of MAE are uncertain. The original MAE paper indicates a more comprehensive evaluation of robustness. These statements appear to be fabricated.\n\n**A1: We do not agree with the reviewer.** In the second paragraph, we do not mention the robustness of MAE and other methods. **How did the reviewer obtain the content about \"robustness\" out of thin air in the second paragraph?**\n\nIn fact, the logic of our motivation is below: Firstly, we emphasize the importance of masking operation. Then we describe the fact how other methods including MAE perform masking and select a suitable mask ratio. Afterward, in light of that masking is an important and flexible operation that can be performed at different stages, we present our concern and pose a question about a different masking strategy, i.e., multiple masking, and ask how it affects the optimization in training and performance.\n\n**It is also acknowledged by Reviewer GAuN and JZAj that our article presents a well-organized and well-argued thought process with clear logic and structure.**\n\nWe feel sorry that the reviewer is confused. We have further revised the second paragraph carefully with better clarity as below:\n\n- A crucial component of the masked autoencoder is the mask ratio, which directly impacts the model's performance. For instance, in MAE, the performance gap for fine-tuning accuracy may vary by up to 2\\% with different mask ratios.\n  However, current methods, including MAE, mostly ablate the mask ratio only on the input image: they mask the input image with various ratios and select the best-performing ratio after training those model variants. Considering that masking is an important and flexible operation that can be performed at different stages (\\eg, the input image and different levels of representations) and with different ratios, these approaches may fail to fully exploit the potential of the autoencoder. Hence, a question naturally raises: **Can the masked autoencoder handle multiple rounds of masking at different levels, and how does multiple masking affect its optimization in training and performance?**\n\n**Q2:** In fact, VideoMAE v2 is the first to propose to mask MAE. Therefore, this paper is not the first research to explore the impact of multiple shots masking in MAE.\n\n**A2:** By carefully going through VideoMAE v2, especially section 4.2 (Main result, Results on dual masking, https://arxiv.org/pdf/2303.16727.pdf), we agree that VideoMAE v2 is the first to propose to mask MAE. **But VideoMAE v2 is not the first work to dive deep into the impact of multiple shots masking in MAE.** We list three discrepancies between VideoMAE v2 and our work:\n\n**Firstly, the application is different.** As the name suggested, VideoMAE v2 is designed for videos while our work is for images.\n\n**Secondly, the motivation is different.** VideoMAE v2 is to improve the overall efficiency of computation and memory so as to perform scaling in model and data. As one can see in Table 2 Page 7 of VideoMAE v2, it primarily wants to tell the story about the benefits in computational cost, memory consumption, etc. However, the in-depth analysis of potential impact of dual masking on encoder is absent in VideoMAE v2. In contrast, our work dives deep into it and reveals what multiple shots of masking bring to the encoder and shows how it affects the encoder's optimization in training. In other words, VideoMAE v2 focuses on application while our work focuses more on analysis. This is the biggest difference between VideoMAE v2 and our work.\n\n**Thirdly, in terms of specific implementation, the masking place and times are different.** VideoMAE v2 only performs two times of masking and employs them on encoder and decoder respectively. Differing from VideoMAE v2, our method involves three times of masking that are all performed on the encoder.\n\nFinally, we express profound respect for all the contributors to VideoMAE v2, recognizing their efforts in scaling models and datasets within the video domain. Simultaneously, we hope that the reviewer can objectively assess the distinctions between our work and VideoMAE v2 and realize the contribution we have made in the in-depth analysis of multiple masking.\n\nWe also thank the reviewer for pointing out this problem. To avoid misunderstanding, we have cited VideoMAE v2, added the discussion in related work, and revised our description of our contribution as follows to distinguish with VideoMAE v2:\n\n- Building on our proposed flexible framework, i.e., Conditional MAE, we are the first to make an in-depth analysis of multiple masking and reveal its impact on masked autoencoder's optimization in training and performance."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1594/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700271023515,
                "cdate": 1700271023515,
                "tmdate": 1700271023515,
                "mdate": 1700271023515,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sTjEN9xvi3",
                "forum": "JfcLYCqOkQ",
                "replyto": "m6QFKJNyBz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1594/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1594/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer MagS (2/2)"
                    },
                    "comment": {
                        "value": "**Q3:** The experimental setting of this paper does not convince me of its results. The dataset selection has several issues. For the main experiments, the author selected the less commonly used benchmark, ImageNet-100. Since the scale and diversity of the data can significantly influence the experimental results, the validity of comparing with MAE is uncertain. Furthermore, the author also failed to choose common datasets for the downstream tasks. Moreover, the article makes adjustments under the original experimental setup conditions of MAE, thus rendering the conclusions drawn from the experiments inaccurate.\n\n**A3: We do not agree with the reviewer's such biased and ridiculous comments.**\n\nFirstly, the subset of ImageNet1k, i.e., ImageNet-100, is widely used in analysis in papers [1] [2] [3] [4] [5] [6] [7] [8] published in top conferences and Journal, e.g, ICLR, NeurIPS, ICML, JMLR, etc. Besides, we also conduct experiments on large ImageNet1k to verify the effectiveness and scalability. Please see Section 3.4.\n\nSecondly, our selected datasets for downstream tasks are widely used in the field of self-supervised learning. For example, we choose ADE20K for semantic segmentation, MSCOCO for object detection. The two datasets are also used in MAE[9], BEiT[10], CAE[11], iBOT[12], and other methods published in top conferences and Journal, e.g., CVPR, ICLR, IJCV, etc. For transfer learning, we use CIFAR100 and CIFAR10 that are also widely used in iBOT[12], MoCo v3[13], DINO[14], SimCLR[15].\n\nFinally, parameter tuning is common and reasonable in the field of deep learning because different methods may have different fitting abilities and optimization spaces. By parameter tuning, we can find the best configuration and unleash model capabilities better. For example, iBOT [12] and DINO [14], the code of iBOT is based on DINO while in ViT-B, iBOT sets local crop scale to (0.05, 0.32) and DINO uses (0.05, 0.25) for local crop scale. Both of them perform parameter tuning for better performance.\n\n**Q4:** The proposed method appears to lack technical contributions, and I fail to discern any tangible practical benefits in terms of performance or speed.\n\n**A4:** As we have mentioned in the last paragraph of the Introduction, we are not to propose a state-of-the-art method, but to enhance both the understanding and performance of MAE by exploring the potential of masking and to inspire future research. And our study actually has two aspects of contribution:\n\nFrom the aspect of understanding MAE, our study shows that multiple masking introduces more locality to models, and summarizes several takeaways from our findings. Please see our Introduction. **Reviewer GAuN and JZAj also acknowledge our findings and contribution.**\n\nFrom the aspect of enhancing the performance of MAE, we also empirically demonstrate that multiple masking has the potential to further improve the performance of MAE as shown in Table 6 and Figure 8. More importantly, we find multiple masking has significant superiority over MAE on fine-grained classification, which is missing in previous studies.\n\n[1] Whitening for Self-Supervised Representation Learning. ICML2021\n\n[2] Adversarial Masking for Self-Supervised Learning. ICML2022\n\n[3] Mosaic Representation Learning for Self-supervised Visual Pre-training. ICLR 2023 Spotlight\n\n[4] Self-Supervised Learning with an Information Maximization Criterion. NeurIPS 2022.\n\n[5] solo-learn: A Library of Self-supervised Methods for Visual Representation Learning. JMLR2022\n\n[6] Improving Transferability of Representations via Augmentation-Aware Self-Supervision. NeurIPS2021\n\n[7] A Simple Data Mixing Prior for Improving Self-Supervised Learning. CVPR 2022.\n\n[8] DLME: Deep Local-flatness Manifold Embedding ECCV2022\n\n[9] Masked Autoencoders Are Scalable Vision Learners CVPR2023\n\n[10] BEiT: BERT Pre-Training of Image Transformers. ICLR2022 Oral\n\n[11] Context Autoencoder for Self-Supervised Representation Learning. IJCV2023\n\n[12] iBOT: Image BERT Pre-Training with Online Tokenizer. ICLR2022\n\n[13] An Empirical Study of Training Self-Supervised Vision Transformers. ICCV 2021\n\n[14] Emerging Properties in Self-Supervised Vision Transformers. ICCV 2023\n\n[15] A Simple Framework for Contrastive Learning of Visual Representations. ICML2020"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1594/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700271119425,
                "cdate": 1700271119425,
                "tmdate": 1700271119425,
                "mdate": 1700271119425,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c09mTjdyCf",
                "forum": "JfcLYCqOkQ",
                "replyto": "m6QFKJNyBz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1594/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1594/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer MagS\uff1a\n\nAs the rebuttal period is ending soon, we wonder if our response answers your questions and addresses your concerns. \n\nThanks again for your very constructive and insightful feedback!  Looking forward to your post-rebuttal reply!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1594/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639237709,
                "cdate": 1700639237709,
                "tmdate": 1700639237709,
                "mdate": 1700639237709,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qdcDvFtXDK",
                "forum": "JfcLYCqOkQ",
                "replyto": "c09mTjdyCf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1594/Reviewer_MagS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1594/Reviewer_MagS"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Thanks for the response from authors. \n\nAfter reviewing the response and other reviewer's questions carefully, I think the author's response fails to convince me for the following reasons:\n\n1.I continue to question the author's motivation, such as the introduction of the multi-shot masking in Video MAE V2. While the proposal aims to save computational resources, the author's strategy of employing multiple masks lacks a clear motivation and appears to address similar issues. Moreover, the rationale behind this approach is not well articulated.\n\n2.Concerning the experiments conducted on ImageNet-100 as the authors proposed, the articles either present original methods or possess a clear motivation with proposed solutions. However, the author's work does not align with these standards.\n\n3. Additionally, the majority of methods validate their effectiveness on ImageNet-1K. Many self-supervised methods that are effective in small data scenarios show contradictory results when the dataset size increases. The author's experimentation on MAE, given that the original MAE paper conducted experiments on ImageNet-1K, raises concerns. While it may be acceptable for an original self-supervised method to be tested solely on ImageNet-100, incremental work or findings based on existing methods necessitate a comparison under the same experimental conditions. This is essential for a convincing conclusion, and the current conclusions based on small-scale experiments are difficult to trust.\n\n4. Similarities exist between Video MAE v2 and the author's experiments. However, while Video MAE v2 aims to reduce computational load in large-scale data settings at the expense of model performance, the model's performance in this article purportedly improves. This inconsistency raises doubts about the conclusions drawn in this paper.\n\nIn summary, the author's rebuttal lacks sufficient support to address my concerns about the motivation, experimental design, and conclusions presented in the paper. Further clarification and additional evidence are needed for the arguments to be more compelling in the context of academic discourse. Therefore, I keep my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1594/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651881559,
                "cdate": 1700651881559,
                "tmdate": 1700651881559,
                "mdate": 1700651881559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Saci8E55Ge",
                "forum": "JfcLYCqOkQ",
                "replyto": "wawj8fOOTW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1594/Reviewer_MagS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1594/Reviewer_MagS"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Thanks for the response from authors. Firstly, I would like to clarify that I indeed do not hold any bias against this paper. The following is my response\uff1a\n\nIt seems that the author has consistently misunderstood my points and evaded addressing critical questions. \n\n1. Regarding question 1, my emphasis was on the existing motivation failing to convince me. I hope the authors can identify a perspective that convinces me, demonstrating how the method can address a certain issue, akin to videomae v2's resolution of computational load concerns. Presently, the current explanation lacks inspiration and noteworthy aspects. \n\n2. Concerning the experiments, while it's true that a small amount of work has published papers using small-scale datasets, almost all research papers use ImageNet-1K as a benchmark due to the greater persuasiveness of results derived from such datasets. Papers conducting experiments on small datasets require stronger innovation, a quality lacking in this article. Furthermore, based on the author's ImageNet experiments, I pointed out that the current method is likely to be ineffective on a large-scale dataset. The author's experiments indicate a very marginal improvement in finetuning (a mere 0.1% increase with an increase in model size and epochs consistent with the MAE benchmark). I believe this marginal improvement is more likely due to random seed fluctuations. Meanwhile, the author's results lack universally accepted experiments like linear probing, which better reflect model performance and require significantly less evaluation time than finetuning. \n\n3. Lastly, I did not claim that a win-win scenario is impossible. I merely highlighted the conflict between the conclusion and existing works. The author's method introduces \"mask findings\" for MAE, while videomae v2 also utilizes MAE. The latter demonstrates that multi-shot masks can degrade MAE performance, conflicting with the conclusions drawn in this paper. Both works utilize visual data, and videomae v2 does not specifically process the temporal dimension. I believe that the conflicting conclusions cannot be explained merely by differences in data and would prefer to see more compelling experimental results."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1594/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711123922,
                "cdate": 1700711123922,
                "tmdate": 1700711123922,
                "mdate": 1700711123922,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fEDrf3Eddm",
            "forum": "JfcLYCqOkQ",
            "replyto": "JfcLYCqOkQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1594/Reviewer_GAuN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1594/Reviewer_GAuN"
            ],
            "content": {
                "summary": {
                    "value": "This article focuses on the mask strategy issue in MAE, which adopts a multi-stage mask approach instead of a fixed mask method. The optimal mask ratio and layer are selected in different stages to mask both the input image and feature layers. The author believes that this method effectively enhances the attention of the mask on locality, and the effectiveness of this method is verified through a series of experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Originality**: Although this article proposes new improvements based on the masking strategy of MAE, the problem addressed is a commonly overlooked issue. Through a series of experiments, the author reflects on the mask strategy and verifies the specific role of the mask in SSL.\n\n**Quality**: The article presents a well-organized and well-argued thought process, and the effectiveness of condition-MAE is demonstrated through a series of comparative experiments.\n\n**Clarity**: The article is highly readable, with clear logic and structure.\n\n**Significance**: The further exploration of the role of the mask and the unexpected findings in the experiments are also worth attention."
                },
                "weaknesses": {
                    "value": "1. The essence of this article is based on MAE and explores the mask strategy. However, this setting lacks novelty as there have been many studies on mask strategies (e.g., [1-3]), and the related work mentioned is not sufficient. It is hoped that the author can conduct further comparisons and supplements to evaluate performance.\n\n2. Some findings mentioned in the article, such as the non-positive correlation between linear probing and finetune performance, are relatively straightforward. Linear probing mainly evaluates the model's discriminative generalization ability, while finetune focuses on the model's fitting ability to the data. However, there is still a positive correlation, mainly depending on the model's scale. The larger the model, the less obvious the positive correlation.\n\n3. The article proposes that the second-stage mask introduces more locality. However, I am concerned that the feature-level mask may have caused this result, as the input image is low-level, while the feature is high-level. Therefore, the combination of the two naturally makes SSL focus on global and local representations.\n\n\n[1] Choe, J., Shim, H.: Attention-based dropout layer for weakly supervised object localization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2219\u20132228 (2019) \n\n[2] Shi, Y., Siddharth, N., Torr, P.H., Kosiorek, A.R.: Adversarial masking for self-supervised learning. arXiv preprint arXiv:2201.13100 (2022)\n\n[3] Wei, C., Fan, H., Xie, S., Wu, C.Y., Yuille, A., Feichtenhofer, C.: Masked feature prediction for self-supervised visual pre-training. arXiv preprint arXiv:2112.09133 (2021)"
                },
                "questions": {
                    "value": "1. The mask ratio in the first stage of the article follows the MAE setting, while the best ratio in the second stage is selected manually. However, it is unclear whether this setting will be effective if a different backbone is used instead of VITs. Would it still require manual tuning?\n\n2. There are some minor changes that need to be noted. For example, the sentence \"The dashed line denotes the one-shot baseline with masking ratios of 0.75 and 0.9 respectively\" is unclear in terms of the baseline used in the graph. Additionally, the experiment with a mask ratio of 0.9 is missing, and if it is included in the appendix, it should be mentioned to improve readability. In the \"Potential Application\" section, \"fine-grained\" should be used instead of \"fine-grind\", and there are inconsistencies in the use of nouns throughout the article.\n\n3. In Figure 4, it can be seen that the difference between layer 0-3 and layer 0-6 is significant, and according to your statement, there should be an improvement. However, the experimental results show a decrease from 84.1 to 84.0. On the other hand, the improvement from layer0-6 to layer0-9 is significant, but the graph shows that the difference from the baseline is not as significant. This has caused some confusion, and it would be helpful if the author could explain this discrepancy."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1594/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675518506,
            "cdate": 1698675518506,
            "tmdate": 1699636087863,
            "mdate": 1699636087863,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HreB5Etgsm",
                "forum": "JfcLYCqOkQ",
                "replyto": "fEDrf3Eddm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1594/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1594/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GAuN (1/2)"
                    },
                    "comment": {
                        "value": "**Q1:** The essence of this article is based on MAE and explores the mask strategy. However, this setting lacks novelty as there have been many studies on mask strategies (e.g., [1-3]), and the related work mentioned is not sufficient. It is hoped that the author can conduct further comparisons and supplements to evaluate performance.\n\n**A1:** We partially agree with the reviewer's comments that there have been many studies on mask strategies but our work is still meaningful. Because these studies primarily focus on **studying how to further improve the performance** while as we mentioned in the last paragraph of the Introduction, our work is to **reveal how multiple masking affects masked autoencoder's behavior, e.g., introducing more locality, and further enhance the understanding of masked autoencoder**. For example, [1] leverages self-attention mechanism to hide the most discriminative part and highlight the informative region to improve the accuracy of weakly supervised object Localization (WSOL). [2] uses an adversarial objective to consistently improve on state-of-the-art self-supervised learning (SSL) methods. [3] uses Histograms of Oriented Gradients (HOG), a hand-crafted feature descriptor, as reconstruction target,  and verifies its effectiveness on video recognition.\n\nWe thank the reviewer's suggestion and have cited and discussed them in our related work section to show the discrepancy.\n\n**Q2:** The article proposes that the second-stage mask introduces more locality. However, I am concerned that the feature-level mask may have caused this result, as the input image is low-level, while the feature is high-level. Therefore, the combination of the two naturally makes SSL focus on global and local representations.\n\n**A2:** Thank you for the comment, but we cannot fully agree with the comment. We may need to remind the reviewer that the reconstruction target In MAE is normalized pixel value in RGB space. Even though we perform the feature-level masking, the model still needs to focus on the recovery of image pixels. Consequently, the discrepancy of the masking type is not the essence of the introduced locality. In fact, the locality is introduced because the presence of the second masking necessitates that **patches that interacted in previous layers must recover their corresponding masked neighbors in the forward pass**. As a result, the model needs to dedicate a portion of its capacity to learning how to infer local neighbors.\n\nWe have added this discussion to our paper to better clarify how the locality is introduced.\n\n**Q3:** The mask ratio in the first stage of the article follows the MAE setting, while the best ratio in the second stage is selected manually. However, it is unclear whether this setting will be effective if a different backbone is used instead of VITs. Would it still require manual tuning?\n\n**A3:** Considering that the masking ratio is a hyperparameter, similar to other hyperparameters, it may not always be compatible with various backbones., e.g.,  ConvNeXt V2 (0.6) and MAE (0.75),  and various masking algorithms, e.g, block (SIMMIM 0.5) and random (MAE 0.75), even without our strategy. Hence, we are afraid that it would possibly require some manual tuning. But since the essence of masked image modeling is invariant, **this process would be significantly shorten with the help of the summarized takeaways in our paper.** Please see the Introduction."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1594/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700270758531,
                "cdate": 1700270758531,
                "tmdate": 1700270939182,
                "mdate": 1700270939182,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W5YHanqtJ5",
                "forum": "JfcLYCqOkQ",
                "replyto": "fEDrf3Eddm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1594/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1594/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GAuN (2/2)"
                    },
                    "comment": {
                        "value": "**Q4:** There are some minor changes that need to be noted. For example, the sentence \"The dashed line denotes the one-shot baseline with masking ratios of 0.75 and 0.9 respectively\" is unclear in terms of the baseline used in the graph. Additionally, the experiment with a mask ratio of 0.9 is missing, and if it is included in the appendix, it should be mentioned to improve readability. In the \"Potential Application\" section, \"fine-grained\" should be used instead of \"fine-grind\", and there are inconsistencies in the use of nouns throughout the article.\n\n**A4:** We feel sorry for this confusion. We have highlighted the dashed line, gone through the whole article, and revised similar issues of inconsistencies. Thank the reviewer for the kind suggestion.\n\n**Q5:** In Figure 4, it can be seen that the difference between layer 0-3 and layer 0-6 is significant, and according to your statement, there should be an improvement. However, the experimental results show a decrease from 84.1 to 84.0. On the other hand, the improvement from layer0-6 to layer0-9 is significant, but the graph shows that the difference from the baseline is not as significant. This has caused some confusion, and it would be helpful if the author could explain this discrepancy.\n\n**A5:** We sincerely thank the reviewer for this constructive suggestion. This is easy to explain. First of all, we may need to emphasize that the disparity in the heatmap does not necessarily imply whether the learned representation is advantageous or detrimental. It only reflects **how the representation learned by our two-shot masking model varies from that of the baseline.** Hence, it would be unreasonable to use the significance of heatmap to assess the performance after fine-tuning.\nFor L0-6, we think it is an awkward transitional state. It doesn't resemble L0-3, which learns a significantly superior discriminative ability (2.2% better than L0-6) as the initial state for fine-tuning. It also differs from L0-9 and L0-10, which exhibit stronger fitting capabilities to the data. As a result, L0-6 is relatively inferior to others. As shown in Figure 18 in Appendix, we compare the attention distance of two-shot model variants L(0;3/6/9/10/11;0.75,0.1), the results indicate that the adjustment of L0-6 is relatively inconspicuous.\n\nWe thank the reviewer again and have added this discussion to our paper to avoid confusion.\n\n[1] Choe, J., Shim, H.: Attention-based dropout layer for weakly supervised object localization. CVPR2019\n\n[2] Shi, Y., Siddharth, N., Torr, P.H., Kosiorek, A.R.: Adversarial masking for self-supervised learning. arXiv2022\n\n[3] Wei, C., Fan, H., Xie, S., Wu, C.Y., Yuille, A., Feichtenhofer, C.: Masked feature prediction for self-supervised visual pre-training. arXiv2021"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1594/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700270844305,
                "cdate": 1700270844305,
                "tmdate": 1700270893333,
                "mdate": 1700270893333,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eetTeoICBn",
                "forum": "JfcLYCqOkQ",
                "replyto": "fEDrf3Eddm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1594/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1594/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer  GAuN\uff1a\n\nAs the rebuttal period is ending soon, we wonder if our response answers your questions and addresses your concerns. \n\nThanks again for your very constructive and insightful feedback!  Looking forward to your post-rebuttal reply!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1594/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639193109,
                "cdate": 1700639193109,
                "tmdate": 1700639193109,
                "mdate": 1700639193109,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OiuqMXbuii",
            "forum": "JfcLYCqOkQ",
            "replyto": "JfcLYCqOkQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1594/Reviewer_JZAj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1594/Reviewer_JZAj"
            ],
            "content": {
                "summary": {
                    "value": "This work is a systematic empirical study of multi-shot masking in MAE by applying additional masking on hidden representations on chosen layers of the encoder on top of input masking in standard MAE. The ratios of masking (different ratios used at different layers) and how many of these additional masking (\u201cshots\u201d) are extensively studied. There are several interesting insights from these studies, as clearly stated in the introduction by the authors: Masking at the beginning is always beneficial for task performance, with 75% being optimal; building on one-shot masking, increasing the interval of two-shot masking with a large first ratio and a small second ratio is helpful; a small third ratio is helpful for three-shot masking; and more. Extensive and crucial analyses have been performed, such as linear probing / fine-tuning on models with different numbers of masking shots, different masking ratios, and masking levels, Centered Kernel Alignment, attention distance and entropy, visualization, robust analysis, and classification on fine-grained datasets, and transfer learning.\n\nGiven the strong, comprehensive empirical analysis and the great presentation of this submission, the reviewer recommends an acceptance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: as the author rightfully cited and compared, there have been numerous works analyzing and studying masked image modeling. However, to the reviewer\u2019s knowledge, this is the first paper to study the effect of multiple masking ratios across different layers in the MAE encoder. The work is, in this sense, original.\n\nQuality: the quality of the paper is high in terms of experimental results and analysis. First, four layers at equal intervals, five masking ratios, and two model sizes (ViT-S/16 and ViT-B/16) are considered, and the representations of one-shot, two-shot vs. three-shot maskings are carefully compared via linear probing/fine-tuning,  Centered Kernel Alignment of layer representation, attention distance and entropy, and fine-grained locality oriented datasets such as Flower102, Stanford Dog and CUB-200. The transfer learning analysis is also done on COCO for detection and ADE20K for semantic segmentation. There are further studies on robustness measured by classification results after occlusion and shuffling perturbation and the scalability of performance under different model sizes.\n\nClarity: this paper is very well written, with key messages clearly presented, results from each section nicely summarized, and key figures carefully designed.\n\nSignificance: given that masking image modeling is a popular topic, the research is beneficial to the community by creating new potentials to improve existing approaches with multi-shot masking."
                },
                "weaknesses": {
                    "value": "Post rebuttal update: the authors successfully clarified the differences between this work and previous methods and provided extensive discussions. Therefore, the rating was updated.\n\n----------------------------------\n\nOriginality: progressive masking has been extensively studied in generative modeling or the combination of SSL and generative modeling [1-3]; adaptive masking strategies on images or languages have also been proposed [4-5]. However, the authors did not cite or discuss them.\n\nQuality: Considering the nature of ICLR, the biggest weakness of this paper is not having any theoretical results, insights, or even discussions about the proposed approach. The insights are not theoretically backed, reducing the quality of the work.\n\nThe scalability discussion in Sec. 3.4 is weaker because there are only two model sizes. Also, the pre-training is done using ImageNet-100, creating a gap between the conclusions of the submission and the hypothetical behaviors of MAE trained on ImageNet-1K using multi-shot masking.\n\n[1] Chang, Huiwen, et al. \"Maskgit: Masked generative image transformer.\" CVPR 2022.\n\n[2] Chang, Huiwen, et al. \"Muse: Text-to-image generation via masked generative transformers.\" 2023.\n\n[3] Li, Tianhong, et al. \"Mage: Masked generative encoder to unify representation learning and image synthesis.\" CVPR 2023.\n\n[4] Bandara, Wele Gedara Chaminda, et al. \"AdaMAE: Adaptive Masking for Efficient Spatiotemporal Learning with Masked Autoencoders.\" CVPR 2023.\n\n[5] Xiao, Yisheng, et al. \"AMOM: adaptive masking over masking for conditional masked language model.\" AAAI 2023."
                },
                "questions": {
                    "value": "1. The authors may not need to perform empirical experiments on this, but what would the authors conjecture about using a low masking ratio (e.g., 0.1) on the beginning position for one-shot masking?\n\n2. \"We reconstruct it primarily conditioned on the 'borrowed' information through such interaction.\" The authors may rephrase this sentence to be more precise. \n\n3. Minor grammar errors: \"We need reconstruct two targets\", and \"pretaining loss\" in Figure 7."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1594/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1594/Reviewer_JZAj",
                        "ICLR.cc/2024/Conference/Submission1594/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1594/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790203987,
            "cdate": 1698790203987,
            "tmdate": 1700624055447,
            "mdate": 1700624055447,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XXkVHSUJgb",
                "forum": "JfcLYCqOkQ",
                "replyto": "OiuqMXbuii",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1594/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1594/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JZAj"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for acknowledging the contribution of our work and the kind and constructive suggestions.\n\n**Q1:** Progressive masking has been extensively studied in generative modeling or the combination of SSL and generative modeling [1-3]; adaptive masking strategies on images or languages have also been proposed [4-5]. However, the authors did not cite or discuss them.\n\n**A1:** We thank the reviewer for this kind suggestion. The discussion is below:\n\nChang et.al. introduce MaskGIT[1], which employs a bidirectional transformer decoder and is capable of learning to predict randomly masked tokens via attending to tokens in all directions during training. When inference, MaskGIT first generates all tokens of an image and then refines the generated image iteratively based on the previous generation. Recently, Chang et.al. propose Muse[2] and train it to predict randomly masked image tokens given the text embedding extracted from a pre-trained large language model (LLM). Leveraging LLM enables Muse to understand fine-grained language, translate to high-fidelity image generation, etc. Moreover, Muse directly enables inpainting, outpainting, and mask-free editing without the need to fine-tune or invert the model. Li et al. [3] propose to use semantic tokens learned by a vector-quantized GAN at inputs and outputs and combine this with masking to unify representation learning and image generation. Bandara et al. propose an adaptive masking strategy called AdaMAE [4]. AdaMAE samples visible tokens based on the semantic context using an auxiliary sampling network and empirically demonstrates the efficacy. Xiao et al. introduce a simple yet effective adaptive masking over masking strategy called AMOM [5] to enhance the refinement capability of the decoder and make the encoder optimization easier.\n\nWe have cited these works in our paper and added this discussion in our related work section. We sincerely hope to obtain support from the reviewer.\n\n**Q2:** The scalability discussion in Sec. 3.4 is weaker because there are only two model sizes. Also, the pre-training is done using ImageNet-100, creating a gap between the conclusions of the submission and the hypothetical behaviors of MAE trained on ImageNet-1K using multi-shot masking.\n\n**A2:** We acknowledge that the scalability is slightly weak. We may need to explain that due to the limitation of computation resources (as we have pointed out in Section 6 limitation), we could not conduct experiments on larger ones, e.g., ViT-Huge to further verify the capability of scaling. But it is worth mentioning that **our scaling experiments are all conducted on large ImageNet1K** instead of ImageNet100, which could verify the effectiveness of scaling model size of multi-shot masking in **large datasets** to some extent.\n\n**Q3:** The authors may not need to perform empirical experiments on this, but what would the authors conjecture about using a low masking ratio (e.g., 0.1) on the beginning position for one-shot masking?\n\n**A3:** It is a very interesting question. When using a small low masking ratio, e.g., 0.1, that is, more information is left and could be used to infer the masked patches, according to our empirical experience, this would lead to an easier reconstruction task, failing to make the encoder learn sufficient inference knowledge (or capability).  The masked autoencoder is inclined to degenerate into (or resemble) a vanilla autoencoder (masking ratio is 0). Above is our speculation before performing experiments.\n\nSince this question is quite interesting,  we are willing to perform such an experiment and share the result with the reviewer. We perform it on ImageNet100 using ViT-S/16. In this experiment, we find that after 300 epoch pretraining, the model is inferior to that of 0.75 masking ratio (31.7 v.s. 45.0 for linear probing and 80.2 v.s. 82.5 for fine-tuning), supporting our speculation above.\n\n**Q4:** Sentence issue and Minor grammar errors.\n\n**A4:** We thank the reviewer for pointing out these issues. We have fixed the grammar issue and rephrased the sentence in our paper.\n\n[1] Chang, Huiwen, et al. \"Maskgit: Masked generative image transformer.\" CVPR 2022.\n\n[2] Chang, Huiwen, et al. \"Muse: Text-to-image generation via masked generative transformers.\" 2023.\n\n[3] Li, Tianhong, et al. \"Mage: Masked generative encoder to unify representation learning and image synthesis.\" CVPR 2023.\n\n[4] Bandara, Wele Gedara Chaminda, et al. \"AdaMAE: Adaptive Masking for Efficient Spatiotemporal Learning with Masked Autoencoders.\" CVPR 2023.\n\n[5] Xiao, Yisheng, et al. \"AMOM: adaptive masking over masking for conditional masked language model.\" AAAI 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1594/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700270566824,
                "cdate": 1700270566824,
                "tmdate": 1700270566824,
                "mdate": 1700270566824,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P2hj5BcaPg",
                "forum": "JfcLYCqOkQ",
                "replyto": "OiuqMXbuii",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1594/Reviewer_JZAj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1594/Reviewer_JZAj"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "The reviewer sincerely appreciates the authors' response. The reviewer is satisfied with the response, and the revision made the paper stronger, addressing multiple concerns from different reviewers. Therefore, the reviewer raises the score to 8."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1594/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623891547,
                "cdate": 1700623891547,
                "tmdate": 1700623932678,
                "mdate": 1700623932678,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]