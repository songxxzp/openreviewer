[
    {
        "title": "FusionFormer: A Multi-sensory Fusion in Bird's-Eye-View and Temporal Consistent Transformer for 3D Object Detection"
    },
    {
        "review": {
            "id": "N5pUsbCrGK",
            "forum": "itKMOWSP6K",
            "replyto": "itKMOWSP6K",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2240/Reviewer_5zjx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2240/Reviewer_5zjx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a multi-modal fusion transformer-based framework for 3D object detection. The idea of this paper is intuitive and this paper is easy to understand. The performance on nuScenes is good."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.This paper is easy to read.  \n2.The performance of the model in this paper show superiority in nuScenes.  \n3.The idea is intuitive."
                },
                "weaknesses": {
                    "value": "1.Only one dataset is used in this paper. The generality of the framework should be analyized.  \n2.In introduction, Fig.1(b) is not compared with their method in details.  \n3.The most  baselines only consider LiDAR and camera, which are not fair to compare. Can you compare with them without temporal information? And this can also further verify the superiority of  Multi-modal fusion encoder.  \n4. The two topics, i.e., multi-modal fusion and temporal consistence, are different, causing that the focus of this paper is unclear."
                },
                "questions": {
                    "value": "Refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/a"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2240/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697535412486,
            "cdate": 1697535412486,
            "tmdate": 1699636157245,
            "mdate": 1699636157245,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xuv1yL1HjR",
                "forum": "itKMOWSP6K",
                "replyto": "N5pUsbCrGK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2240/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments!"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments and suggestions, and they are exceedingly helpful for us to improve our paper. We have carefully incorporated them in the revised paper. In the following, your comments are first stated and then followed by our point-by-point responses.\n## Q1 More experiments\n> Only one dataset is used in this paper. The generality of the framework should be analyized.\n\n**Author response:**  \nIt is worth noting that previous studies on multimodal fusion have primarily focused on the nuScenes dataset, and there is little research conducted on other dataset, such as Waymo. This may be attributed to the differences between the datasets since the Waymo dataset primarily consists of LiDAR data, and the available images predominantly capture a forward-facing view while lacking rearward images.\n\nWe are trying to evaluate the generalizability of our method on the Waymo dataset and other tasks such as segmentation. The experiments are still ongoing. We will include relevant experimental results in the final version of our paper. \n\n## Q2 Compared with CMT\n> In introduction, Fig.1(b) is not compared with their method in details.\n\n**Author response:**  \nThank you for your feedback. We have added a more detailed comparison in Figure 1(b).  \n\nWe also list the key differences with CMT here:\n\n- CMT **tokenizes** image and LiDAR features directly and uses a transformer decoder to generate object detection predictions, while our method generates **fused BEV features** that are then connected to the head for prediction.\n- CMT uses compressed **BEV lidar features**, while we preserve the Z-axis information by using **voxel features**. \n- CMT have challenges in achieving long term temporal fusion, while our method supports **long term temporal fusion**.\n\n## Q3 Single frame BEV feature result\n> The most baselines only consider LiDAR and camera, which are not fair to compare. Can you compare with them without temporal information? And this can also further verify the superiority of Multi-modal fusion encoder.\n\n**Author response:**  \nWe present the results as requested on nuScenes test set. Note that we do not use any test-time augmentation. Our single-frame FusionFormer (FusionFormer-S) achieves competitive performance. Nonetheless, we emphasize we aim to propose a uniform architecture for multi-modal, temporal consistent architecture.\n\n*Performance comparison on the nuScenes test set. \"L\" is LiDAR. \"C\" is camera. \"T\" is temporal. The results are evaluated using a single model without any test-time-augmentation or ensembling techniques.*\n|Methods|Modality|NDS\u2191|mAP\u2191|mATE\u2193|mASE\u2193|mAOE\u2193|mAVE\u2193|mAAE\u2193|\n|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n|PointPainting|CL|61.0|54.1|38.0|26.0|54.1|29.3|13.1|\n|PointAugmenting|CL|71.1|66.8|25.3|23.5|35.4|26.6|12.3|\n|MVP|CL|70.5|66.4|26.3|23.8|32.1|31.3|13.4|\n|FusionPainting|CL|71.6|68.1|25.6|23.6|34.6|27.4|13.2|\n|TransFusion|CL|71.7|68.9|25.9|24.3|35.9|28.8|12.7|\n|BEVFusion|CL|72.9|70.2|26.1|23.9|32.9|26.0|13.4|\n|BEVFusion|CL|73.3|71.3|**25.0**|24.0|35.9|25.4|13.2|\n|UVTR|CL|71.1|67.1|30.6|24.5|35.1|**22.5**|12.4|\n|CMT|CL|74.1|72.0|27.9|23.5|30.8|25.9|11.2|\n|DeepInteraction|CL|73.4|70.8|25.7|24.0|32.5|24.5|12.8|\n|BEVFusion4D-S|CL|73.7|71.9| - | - | - | - | - |\n|BEVFusion4D|CLT|74.7|**73.3**| - | - | - | - | - |\n|FusionFormer-S|CL|73.8|70.8|26.7|**23.4**|28.9|25.8|10.7|\n|FusionFormer|CLT|**75.1**|72.6|26.7|23.6|**28.6**|**22.5**|**10.5**|\n\n\n## Q4 Multi-modal and temporal fusion\n> The two topics, i.e., multi-modal fusion and temporal consistence, are different, causing that the focus of this paper is unclear.\n\n**Author response:**  \nThank you for your comments. Both multimodal fusion and temporal fusion are currently hot topics in the field of autonomous driving research, with significant implications for improving perception accuracy and stability. This study aims to present a unified Transformer-based multimodal temporal fusion framework. Our proposed Transformer-based multimodal fusion encoding module  adapts to various multimodal feature inputs, resulting in fused BEV features. Our approach outperforms fusion methods such as BEVFusion in terms of multimodal fusion performance. Furthermore, compared to previous transformer-based methods that do not support temporal fusion such as CMT and DeepInteraction, our approach, based on the proposed temporal fusion module, is capable of supporting temporal fusion and achieves state-of-the-art performance on the nuScenes dataset.  \nIn conclusion, we introduces a unified Transformer-based multimodal temporal fusion framework that achieves state-of-the-art results on the nuScenes dataset."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491501585,
                "cdate": 1700491501585,
                "tmdate": 1700491501585,
                "mdate": 1700491501585,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oxTPn1o0Lk",
            "forum": "itKMOWSP6K",
            "replyto": "itKMOWSP6K",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2240/Reviewer_bQU3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2240/Reviewer_bQU3"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a new spatial-temporal multi-modal fusion framework for 3D object detection. The proposed framework leverages 2D image features and 3D voxel features to generate BEV features and refine the features with temporal memory banks, which are then fed to the detection head to generate 3D object predictions. The method achieves leading performance on the nuScenes dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Overall the paper is well-written with clear technical pipeline. \n* The proposed framework can work even when missing modality inputs, showing better robustness. \n* The proposed method archives new SOTA results."
                },
                "weaknesses": {
                    "value": "* The motivation is not clear or sufficient. The author claims that \" state-of-the-art multi-modality frameworks need explicitly compressing the voxel features into BEV space\" and the proposed approach is aimed to address this issue. However, existing works like DeepInteraction maintain per-modality representations (2D perspective for camera and voxel space for LiDAR) and learn the interactions between these cross-modality interactions without the need for BEV intermediate representations.  What's the difference between the proposed approach and DeepInteraction? Please fully discuss the differences and highlight the novelty of the proposed methods with SOTA methods. \n* Lack of novelty. The proposed framework has a large overlay with both DeepInteraction and Bevformer. The BEV grid representation, temporal BEV feature fusion, and deformable attention have already been used in Bevformer and the cross-modality interaction of 2D features and 3D features have also been proposed in DeepInteractions. I would encourage the author to highlight the novelty and differences. \n* The memory banks contain all the previous BEV features, which is very time-consuming for learning the interactions of current BEV feature maps and previous ones. Why not choose a recurrent-style temporal fusion mechanism? \n* In the abstract and introduction, the author claims one of the main contributions is the residual architecture. However, in the method section, it is rarely mentioned. \n* When comparing with other SOTAs, is temporal information used for all the other methods for a fair comparison?"
                },
                "questions": {
                    "value": "* The BEV visualization in Figure 6 (b) of the proposed method looks very good. Is this the result of a pure camera branch or generated from both LiDAR and camera inputs? What is the feature visualization of LiDAR BEV feature map?  How are the visual improvements after adding the camera modality?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2240/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2240/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2240/Reviewer_bQU3"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2240/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698459994947,
            "cdate": 1698459994947,
            "tmdate": 1700864131626,
            "mdate": 1700864131626,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K7NugbMQXp",
                "forum": "itKMOWSP6K",
                "replyto": "oxTPn1o0Lk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2240/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments!"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments and suggestions, and they are exceedingly helpful for us to improve our paper. We have carefully incorporated them in the revised paper. In the following, your comments are first stated and then followed by our point-by-point responses.\n## Q1 Differences with DeepInteraction\n> The motivation is not clear or sufficient. The author claims that \" state-of-the-art multi-modality frameworks need explicitly compressing the voxel features into BEV space\" and the proposed approach is aimed to address this issue. However, existing works like DeepInteraction maintain per-modality representations (2D perspective for camera and voxel space for LiDAR) and learn the interactions between these cross-modality interactions without the need for BEV intermediate representations. What's the difference between the proposed approach and DeepInteraction? Please fully discuss the differences and highlight the novelty of the proposed methods with SOTA methods. \n\n**Author response:**  \n\nWe have clarified our novelty and the difference with SOTA methods in the general response. And we detail the response to your question here.\n\nWe apologize for any misunderstanding, however, we again list the key differences with DeepInteraction here:\n\n- DeepInteraction focus on the **interaction** between multimodal features, while our method focuses on **modal fusion**.   \n- DeepInteraction preserves the original form of each modality feature after interaction, while our method generates **fused features**. \n- DeepInteraction uses compressed **BEV lidar features**, while we preserve the Z-axis information by using **voxel features**.\n- DeepInteraction relies on both modal inputs simultaneously, while our method exhibits **robustness** in the presence of **missing modalities**.\n- DeepInteraction is not easily applicable for temporal fusion, while our method supports **temporal fusion**."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491077179,
                "cdate": 1700491077179,
                "tmdate": 1700491077179,
                "mdate": 1700491077179,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A6g2I1Jav5",
                "forum": "itKMOWSP6K",
                "replyto": "oxTPn1o0Lk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2240/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Q2 Clarification of the novelty"
                    },
                    "comment": {
                        "value": "## Q2 Clarification of the novelty\n> Lack of novelty. The proposed framework has a large overlay with both DeepInteraction and Bevformer. The BEV grid representation, temporal BEV feature fusion, and deformable attention have already been used in Bevformer and the cross-modality interaction of 2D features and 3D features have also been proposed in DeepInteractions. I would encourage the author to highlight the novelty and differences.\n\n**Author response:**  \n\nWe have clarified our novelty and the difference with SOTA methods in the general response. And we detail the response to your question here.\nWe also take this opportunity to emphasize the main contributions of our paper here:\n\n- **We advocate using point cloud features in __voxel format__ as opposed to using compressed LiDAR BEV features along the Z-axis.** The recent approaches, BEVFusion, DeepInteraction and CMT, rely on LiDAR BEV features which is compressed along the Z-axis. Compared to fusing LiDAR features in the form of BEV features, our voxel feature fusion approach exhibits remarkable improvements in both object center position prediction and orientation prediction accuracy as shown in Table 5 in our paper (we provide the table here for your convenience). \n\n*Study for the representation of the LiDAR feature on the nuScenes val-set, as in Table.*\n|LiDAR|mAP\u2191|NDS\u2191|mATE\u2193|mAOE\u2193|\n|:----:|:----:|:----:|:----:|:----:|\n|BEV|61.3|66.1|35.7|36.9|\n|Voxel|**62.7**|**67.3**|**34.4**|**31.4**|  \n\n- **Based on our voxel feature fusion approach, we have discovered that it is possible to enhance visual-based 3D object detection by replacing LiDAR features with voxel features generated from *monocular depth predictions*.** This novel idea provides a new direction for improving the performance of visual-based 3D object detection.\n\n*Results of camera based 3D detection on the nuScenes val set.*\n|Method|Backbone|mAP|NDS|\n|:----:|:----:|:----:|:----:|\n|BEVFormer|R101-DCN|41.6|51.7|\n|FusionFormer-Depth|R101-DCN|**43.9**|**53.5**|\n\n- **We have proposed a unified multimodal temporal fusion method based on Transformers, which has achieved a new state-of-the-art performance on the nuScenes dataset.**\n\nWe apologize for any misunderstanding, however, we again list the key differences between our method with BEVFormer & DeepInteraction here:  \n**The difference between our method with BEVFormer**\n- BEVFormer only supports **single-modality** visual input. In contrast, our method focuses on providing a novel **multimodal** fusion solution.\n- BEVFormer adopts a recurrent-based temporal fusion approach that does not support long-term temporal fusion. In contrast, our method efficiently achieves **long-term temporal fusion**.\n\n**The difference between our method DeepInteraction**\n- DeepInteraction focus on the **interaction** between multimodal features, while our method focuses on **modal fusion**.   \n- DeepInteraction preserves the original form of each modality feature after interaction, while our method generates **fused features**. \n- DeepInteraction uses compressed **BEV lidar features**, while we preserve the Z-axis information by using **voxel features**.\n- DeepInteraction relies on both modal inputs simultaneously, while our method exhibits **robustness** in the presence of **missing modalities**.\n- DeepInteraction is not easily applicable for temporal fusion, while our method supports **temporal fusion**."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491128185,
                "cdate": 1700491128185,
                "tmdate": 1700619470309,
                "mdate": 1700619470309,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "urOD4LYsGm",
                "forum": "itKMOWSP6K",
                "replyto": "oxTPn1o0Lk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2240/Reviewer_bQU3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2240/Reviewer_bQU3"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed feedback. I am still not convinced about the novelty part. From the current response, the voxel representation is novel and distinct with deepinteraction and bevformer. However, all the other argued novelty in comparisons are basically the differences between deepinteraction and bevformer which the proposed method builds upon. I would encourage the author to highlight the differences between the proposed method and deepinteraction&bevformer."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586983854,
                "cdate": 1700586983854,
                "tmdate": 1700587035155,
                "mdate": 1700587035155,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X7EYX3NOiu",
                "forum": "itKMOWSP6K",
                "replyto": "oxTPn1o0Lk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2240/Reviewer_bQU3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2240/Reviewer_bQU3"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. Previously you are comparing proposed method vs Bevformer and then compare the proposed method vs Deepinteraction. However this is not sufficient for highlighting the novelties. I would encourage the author to list the differences between the proposed method vs (Bevformer+DeepInteraction). In other words, Bevformer and DeepInteraction is treated as a group. You should highlight the differences and novelties that neither BEVFormer nor DeepInteraction have. Otherwise, still it is unclear if the proposed method is just a combination of these two papers. For example, temporal fusion is not included in deepinteraction but is included in bevformer and if the temporal fusion is the same as the bevformer, then I think the technical contribution and novelty would be limited."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622532302,
                "cdate": 1700622532302,
                "tmdate": 1700622600360,
                "mdate": 1700622600360,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ho3U7kFb4Y",
            "forum": "itKMOWSP6K",
            "replyto": "itKMOWSP6K",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2240/Reviewer_q7W1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2240/Reviewer_q7W1"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a transformer-based framework for 3D multi-modality object detection. It mainly contains spatial fusion and temporal fusion modules to fuse cross-modality features and temporal features, respectively. Experiments prove the effectiveness of the proposed modules."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The direction of cross-modality fusion for 3D object detection is promising, which could bring potential effect to practical application.\n2. The whole method is simple and easy to follow.\n3. The presentation and writing is clear."
                },
                "weaknesses": {
                    "value": "1. The core idea of utilizing BEV queries for temporal and cross-modality fusion is widely used in previous methods like BEVFormer. Although the proposed method is different in detailed design, the core application of BEV queries is unchanged. This harms the technical contribution of the proposed method.\n2. The runtime comparisons are missing. Because this work incorporated several attention modules in different encoders, it's essential to report the latency of each module.\n3. It's interesting that the proposed method can also support the camera-only setting in Figure 4. However, the performance in Table 3 seems not good enough compared with the clear BEV modeling like BEVDepth. Does it mean the fusion-based method in Figure 4 is not a good choice for the camera-only setting?"
                },
                "questions": {
                    "value": "Please refer to the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2240/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698569507955,
            "cdate": 1698569507955,
            "tmdate": 1699636157034,
            "mdate": 1699636157034,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "En6V4q6o1S",
                "forum": "itKMOWSP6K",
                "replyto": "ho3U7kFb4Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2240/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments!"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments and suggestions, and they are exceedingly helpful for us to improve our paper. We have carefully incorporated them in the revised paper. In the following, your comments are first stated and then followed by our point-by-point responses.\n## Q1 The technical contribution\n> The core idea of utilizing BEV queries for temporal and cross-modality fusion is widely used in previous methods like BEVFormer. Although the proposed method is different in detailed design, the core application of BEV queries is unchanged. This harms the technical contribution of the proposed method.  \n\n**Author response:**\nWe first clarify our novelty in the general response. And we detail the response to your question here.\n\nWe apologize for any misunderstanding, however, we again list the key differences with BEVFormer here:\n- BEVFormer is a **visual only** method. As such, it is not possible for them to use BEV queries to fuse cross-modality information.\n- BEVFormer adopts a recurrent-based temporal fusion approach that does not support long-term temporal fusion. In contrast, our method efficiently achieves **long-term temporal fusion**.\n\nWe also take this opportunity to emphasize the main contributions of our paper here:\n\n- **We advocate using point cloud features in __voxel format__ as opposed to using compressed LiDAR BEV features along the Z-axis.** The recent approaches, BEVFusion, DeepInteraction and CMT, rely on LiDAR BEV features which is compressed along the Z-axis. Compared to fusing LiDAR features in the form of BEV features, our voxel feature fusion approach exhibits remarkable improvements in both object center position prediction and orientation prediction accuracy as shown in Table 5 in our paper (we provide the table here for your convenience). \n\n*Study for the representation of the LiDAR feature on the nuScenes val-set, as in Table.*\n|LiDAR|mAP\u2191|NDS\u2191|mATE\u2193|mAOE\u2193|\n|:----:|:----:|:----:|:----:|:----:|\n|BEV|61.3|66.1|35.7|36.9|\n|Voxel|**62.7**|**67.3**|**34.4**|**31.4**|  \n\n- **Based on our voxel feature fusion approach, we have discovered that it is possible to enhance visual-based 3D object detection by replacing LiDAR features with voxel features generated from *monocular depth predictions*.** This novel idea provides a new direction for improving the performance of visual-based 3D object detection.\n\n*Results of camera based 3D detection on the nuScenes val set.*\n|Method|Backbone|mAP|NDS|\n|:----:|:----:|:----:|:----:|\n|BEVFormer|R101-DCN|41.6|51.7|\n|FusionFormer-Depth|R101-DCN|**43.9**|**53.5**|\n\n- **We have proposed a unified multimodal temporal fusion method based on Transformers, which has achieved a new state-of-the-art performance on the nuScenes dataset.**\n\n## Q2: Runtime comparison of module. \n> The runtime comparisons are missing. Because this work incorporated several attention modules in different encoders, it's essential to report the latency of each module.  \n\n**Author response:**  \nWe have added a time analysis for different modules, and the results are presented in the table below on a single A100 GPU. The time for the multimodal fusion encoding module represents the total time for 6 layers of encoding, while the time for the temporal fusion encoding module represents the total time for 3 layers of encoding. The time for each attention module represents the time for a single module in a single layer of encoding.\n|Latency (ms)|Camera Backbone|LiDAR Backbone|Multi-Modal Fusion Encoder|Points Cross-Attention|Image Cross-Attention|Self-Attention|Temporal Fusion Encoder|Head|FPS|\n|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n|FusionFormer|20|124|79|1|5|6|22|23|3.8|\n|FusionFormer-S|20|124|80|1|5|6|-|23|4.0|\n\nNonetheless, we already provide the runtime analysis against other baselines in Table 7 of Appendix. We replicate this table here for your convenience. Our method has a competitive runtime compared to those method.  \n\n*Appendix Table 7 Efficiency comparison on the nuScenes val set. \u201dL\u201d is LiDAR. \u201dC\u201d is camera. \u201dT\u201d is temporal. The \u201d-S\u201d indicates that the model only utilizes single-frame BEV features without incorporating temporal fusion techniques.*\n|Methods|Modality|mAP|NDS|FPS|\n|:----:|:----:|:----:|:----:|:----:|\n|TransFusion|CL|67.5|71.3|3.2|\n|BEVFusion|CL|68.5|71.4|4.2|\n|UVTR|CL|65.4|70.2|2.6|\n|CMT|CL|70.3|72.9|**6.0**|\n|DeepInteraction|CL|69.8|72.6|1.7|\n|FusionFormer-S|CL|70.0|73.2|4.0|\n|FusionFormer|CLT|**71.4**|**74.1**|3.8|"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490853024,
                "cdate": 1700490853024,
                "tmdate": 1700490853024,
                "mdate": 1700490853024,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kEIJcxqM23",
            "forum": "itKMOWSP6K",
            "replyto": "itKMOWSP6K",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2240/Reviewer_Qpuj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2240/Reviewer_Qpuj"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a novel sensor fusion technique that generates the fused BEV feature from LiDAR voxel features and image features without compressing the Z-axis information. Unlike prior works that generate separate point BEV features and image BEV features and then fuse them, the proposed method directly generates the fused BEV feature using queries in the BEV space and deformable attention modules to interact with point voxel features and 2D image features. For each BEV query, the authors generate multiple reference points with different heights and project them back to the voxel space or image feature space for deformable attention. Besides, a temporal fusion encoder is proposed to include temporal information. The proposed method can also be used for pure image BEV feature generation with additional depth estimation networks. Experiments show that the proposed method provides competitive performance with SOTA in 3D object detection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The paper is well-written and well-organized.\n2) The multi-modal fusion problem studied in this paper is interesting and timely in Autonomous Driving. \n3) The proposed method is simple and interesting. It can be seen as an extension of BEVFormer in the multi-modal settings."
                },
                "weaknesses": {
                    "value": "1) The proposed method is only tested in one dataset (nuScenes) and one task (3D object detection), which may not be enough to show the generalizability of the proposed sensor fusion scheme. It would be better to include more datasets (Waymo) and tasks (e.g., segmentation)\n\n2) Though the proposed method makes use of the z-axis information, it seems to greatly increase the model complexity. To make fair comparisons with other baselines, the authors may better include complexity analysis such as FLOPS, #of parameters, and FPS \n\n3) From Tables 1 and 2, the proposed method only provides a marginal improvement."
                },
                "questions": {
                    "value": "In Table 1, the authors only show the result that combines temporal information. How about the one that only uses single-frame BEV features? (i.e., FusionFormer-S in Table 2). This result is important to evaluate the sensor fusion mechanism when compared with other CL baselines."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2240/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786913329,
            "cdate": 1698786913329,
            "tmdate": 1699636156956,
            "mdate": 1699636156956,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "et3935pjIi",
                "forum": "itKMOWSP6K",
                "replyto": "kEIJcxqM23",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2240/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2240/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comments!"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments and suggestions, and they are exceedingly helpful for us to improve our paper. We have carefully incorporated them in the revised paper. In the following, your comments are first stated and then followed by our point-by-point responses.\n\n## Q1 More experiments.\n> The proposed method is only tested in one dataset (nuScenes) and one task (3D object detection), which may not be enough to show the generalizability of the proposed sensor fusion scheme. It would be better to include more datasets (Waymo) and tasks (e.g., segmentation)  \n\n**Author response:**  \nIt is worth noting that previous studies on multimodal fusion have primarily focused on the nuScenes dataset, and there is little research conducted on the Waymo dataset. This may be attributed to the differences between the datasets since the Waymo dataset primarily consists of LiDAR data, and the available images predominantly capture a forward-facing view while lacking rearward images.\n\nWe are trying to evaluate the generalizability of our method on the Waymo dataset and other tasks such as segmentation. The experiments are still ongoing. We will include relevant experimental results in the final version of our paper. \n\n\n## Q2 Complexity comparison with other baselines\n> Though the proposed method makes use of the z-axis information, it seems to greatly increase the model complexity. To make fair comparisons with other baselines, the authors may better include complexity analysis such as FLOPS, #of parameters, and FPS  \n\n**Author response:**  \nDue to the constraints on the page limit of the main text, we present the complexity efficiency comparison in Appendix A.2. \n\nAs shown in Table 7, we compare the efficiency of FusionFormer and existing methods. The efficiency and performance are tested on a single Tesla A100 GPU with the best model setting of official repositories. In comparison to BEVFusion, FusionFormer demonstrates superior performance with notable improvements of 3.1% in mAP and 2.7% in NDS, while maintaining a similar processing speed.\n\n*Appendix Table 7 Efficiency comparison on the nuScenes val set. \u201dL\u201d is LiDAR. \u201dC\u201d is camera. \u201dT\u201d is temporal. The \u201d-S\u201d indicates that the model only utilizes single-frame BEV features without incorporating temporal fusion techniques.*\n|Methods|Modality|mAP|NDS|FPS|\n|:----:|:----:|:----:|:----:|:----:|\n|TransFusion|CL|67.5|71.3|3.2|\n|BEVFusion|CL|68.5|71.4|4.2|\n|UVTR|CL|65.4|70.2|2.6|\n|CMT|CL|70.3|72.9|**6.0**|\n|DeepInteraction|CL|69.8|72.6|1.7|\n|FusionFormer-S|CL|70.0|73.2|4.0|\n|FusionFormer|CLT|**71.4**|**74.1**|3.8|\n\nIn addition, we conducted a comparative experiment on computation cost between our method and the previous state-of-the-art method, CMT. As shown in Table 8, our method has similar FLOPS and parameters as CMT.\n\n*Appendix Table 8 Computation cost comparison on the nuScenes val set. \u201dL\u201d is LiDAR. \u201dC\u201d is camera. \u201dT\u201d is temporal. The \u201d-S\u201d indicates that the model only utilizes single-frame BEV features without incorporating temporal fusion techniques.*\n|Methods|Modality|mAP|NDS|FLOPS|Params|\n|:----:|:----:|:----:|:----:|:----:|:----:|\n|CMT|CL|70.3|72.9|2.17T|77.73M|\n|FusionFormer-S|CL|70.0|73.2|2.33T|77.55M|\n|FusionFormer|CLT|71.4|74.1|2.42T|78.54M|\n\n\n## Q3 Marginal improvement\n> From Tables 1 and 2, the proposed method only provides a marginal improvement.  \n\n**Author response:**  \nThe nuScenes 3D object detection leaderboard has currently reached a high level. In this scenario, our method still achieves overall performance improvement and significant improvement in certain aspects.   \n- We achieves significant improvements in terms of reducing the **orientation error**\uff08mAOE\uff09and **classification error** (mAAE). Our method reduces the mAOE and mAAE compared to the previous state-of-the-art methods by 7.1% and 6.3%, respectively. These two errors are often considered challenging in point cloud-based detection. This indicates that our method is capable of better fusing image features to compensate for the shortcomings of point cloud features.  \n- Our method exhibits better detection performance in scenarios where point clouds are sparse, particularly over long distances. This superior performance can be observed in Figure 6(b) of our paper.  \n\nIn summary, our method successfully achieves unified multimodal temporal fusion and delivers superior detection performance."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2240/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490601821,
                "cdate": 1700490601821,
                "tmdate": 1700490601821,
                "mdate": 1700490601821,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]