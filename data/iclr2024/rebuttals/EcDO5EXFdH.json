[
    {
        "title": "SiGeo: Sub-One-Shot NAS via Information Theory and Geometry of Loss Landscape"
    },
    {
        "review": {
            "id": "bKaRz6aOdj",
            "forum": "EcDO5EXFdH",
            "replyto": "EcDO5EXFdH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5974/Reviewer_RQP1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5974/Reviewer_RQP1"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the sub-one-shot paradigm that connects zero-shot and one-shot NAS through information theory and the geometry of loss landscapes. It proposes a proxy metric called SiGeo and a theoretical framework that connects the supernet warm-up with the efficacy of zero-cost proxy. Experimental results show that SiGeo exhibits good consistency on NAS benchmarks and performs comparably to one-shot NAS in recommendation systems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. SiGeo can connect the performance evaluation of zero-shot and one-shot NAS.\n2. The paper provides theoretical analysis from the perspectives of convergence and generalization.\n3. The authors provide theoretical empirical justification and experimental validation in the field of recommendation systems."
                },
                "weaknesses": {
                    "value": "1. One concern is the novelty of the paper. SiGeo feels like a combination proxy of ZiCO, FR norm, and loss functions.\n\n2. SiGen is Sub-One-Shot, but the authors did not use the warm-up to analyze the correlation and search accuracy in the main experiments, including NAS Benchmarks and CIFAR-10/CIFAR-100. As the authors mentioned, SiGen is equivalent to a simplified ZiCO without warm-up, so the performance improvement in Tables 2 and 3 is also marginal.\n\n3. It would have been preferable to conduct experiments on NAS-Bench-201 benchmark and ImageNet dataset."
                },
                "questions": {
                    "value": "1. It seems a bit inconsistent that SiGeo utilizes both gradient mean and variance since Theorem 1 only provides gradient variance in the bound.\n2. Theorem  2 only offers a lower bound analysis. It would be better to provide an upper bound analysis of $L(\\hat{\\theta}^*)$.\n3. In section 4.2, is there a trade-off between a longer warm-up period (e.g., 60%, 80%) and consistency of SiGeo, or does a longer warm-up period consistently improve consistency?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5974/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762757644,
            "cdate": 1698762757644,
            "tmdate": 1699636638824,
            "mdate": 1699636638824,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j7VRePoEUB",
                "forum": "EcDO5EXFdH",
                "replyto": "bKaRz6aOdj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5974/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comprehensive review and constructive feedback on our manuscript. We appreciate the recognition of SiGeo's ability to bridge zero-shot and one-shot NAS, as well as the theoretical analysis we provided.\n\n1. **Novelty of SiGeo**: We understand your concern regarding the novelty of SiGeo. We aim to clarify that SiGeo is not merely a combination of these elements but an innovative integration that leverages each component to address specific challenges in NAS. SiGeo's novelty lies in how these elements are unified into a coherent sub-one-shot framework, offering new insights and capabilities beyond their individual applications.\n\n    - We clarify that the contribution of this work is not a new zero-shot proxy. Instead, we focus on showing the significance of the sub-one-shot and warm-up procedures, along with SiGeo's capacity to leverage information acquired as the candidate network improves with training.\n    - We clarify that the objective of our zero-shot NAS benchmark experiments was not necessarily to demonstrate superior performance to the zero-shot approaches. Rather, our aim was to validate its \"backward\" comparability to the zero-shot setting.\n    - We further included additional experiments; see the comment [\"New Experiments\"](https://openreview.net/forum?id=EcDO5EXFdH&noteId=CSjlIALRuD). To summarize, our experiments have shown an enhancement in the performance of the Fisher-Rao norm and training loss as the warm-up level increases.\n    - Our key results in Table 1 and Figure 3 demonstrate that SiGeo with a 1% warm-up of supernet can achieve significantly better performance than ZiCo and comparable performance to one-shot NAS methods with a significant reduction in computational costs.\n    - We also direct the reviewer to [General Clarification of Contribution](https://openreview.net/forum?id=EcDO5EXFdH&noteId=AwltH9CFe2).\n    \n2. **Utilization of Warm-Up in Main Experiments**: Your observation about the lack of warm-up analysis in the main experiments is well-taken. \n    - In response, we included an in-depth analysis of the correlation between warm-up periods and search accuracy in NAS benchmarks and CIFAR datasets. This will provide a clearer understanding of SiGeo's performance in various warm-up scenarios and its distinction from the ZiCo approach.\n    - We also want to emphasize the novelty of this paper lies in the coherent integration of SiGeo proxy with weight-sharing NAS.\n\n3. **Experiments on NAS-Bench-201 and ImageNet**: We agree that including experiments on NAS-Bench-201 and the ImageNet dataset would strengthen our findings. Some of our experiments in Section 4.3 were conducted on NAS-Bench-201 benchmark and datasets.\n\n**Responses to Questions**\n\n1. *It seems a bit inconsistent that SiGeo utilizes both gradient mean and variance since Theorem 1 only provides gradient variance in the bound.*\n    \n   - **Response**: We acknowledge the need for better clarity on this aspect. The absolute value of the gradient mean was appeared in the **Theorem 2.** as part of the lower bound of the minimal achievable training loss.\n    \n2. *Theorem 2 only offers a lower bound analysis. It would be better to provide an upper-bound analysis*\n    \n    - **Response**: Your suggestion for an upper bound analysis is valuable. We will explore this in our future work, thereby providing a more balanced theoretical perspective.\n    \n3. *In section 4.2, is there a trade-off between a longer warm-up period (e.g., 60%, 80%) and consistency of SiGeo, or does a longer warm-up period consistently improve consistency?*\n    \n    - **Response**: As two reviewers asked the same question, we have performed a side experiment study to illustrate the consistency of SiGeo. The results have been posted in the comment above [\"New Experiments\"](https://openreview.net/forum?id=EcDO5EXFdH&noteId=CSjlIALRuD)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459615345,
                "cdate": 1700459615345,
                "tmdate": 1700610013352,
                "mdate": 1700610013352,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ieHJ6Twbbg",
            "forum": "EcDO5EXFdH",
            "replyto": "EcDO5EXFdH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5974/Reviewer_DixF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5974/Reviewer_DixF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to employ sub-one-shot NAS to trade off the zero-shot  and one-shot NAS. Furthermore, this paper designs a novel proxy called SiGeo, which is based on the information\t and geometry of the loss landscape. SiGeo can achieve better performance\ton CV tasks compared with zero-shot NAS and on the RecSys domain with less computational costs compared with one-shot NAS."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper provides the theoretical analysis of SiGeo by jointly considering the minimum achievable training loss and generalization error. \n2. The sub-one-shot NAS framework can be better than zero-shot NAS, and can also consume\t less search cost than one-shot NAS."
                },
                "weaknesses": {
                    "value": "1. The proxy is mainly borrowed from ZiCo, so the novelty is not enough.\n2. The experimental results show no significant gain, especially compared with ZiCo on CV tasks."
                },
                "questions": {
                    "value": "1. The theoretical verification in Figure 2 is mainly conducted on a two-layer MLP-ReLU network, so whether this theory applicable to more complex networks? For example, for the architectures in NAS-Bench-201, what about the ranking consistency when warming up with 0%, 10%, and 40% data?\n2. The experiments on the CV task only consider the zero-shot settings. What about the performance when involving SiGeo proxy under the sub-one-shot NAS framework?\n3. Similarly, in the RecSys experiments of Figure 3, the zero-shot NAS with SiGeo should also be compared.\n4. Can the SiGeo proxy search for more complex networks on ImageNet, such as in the MobiletNet search space or the Transformer search space?\n5. Since when $\\lambda_2$ and $\\lambda_3$ in Formula (7) are set to zero, SiGeo is simplified to ZiCo. Can other existing zero-shot proxies also improve performance by adding the two items?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5974/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764069145,
            "cdate": 1698764069145,
            "tmdate": 1699636638694,
            "mdate": 1699636638694,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IZmynbxDRW",
                "forum": "EcDO5EXFdH",
                "replyto": "ieHJ6Twbbg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5974/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Addressing Weaknesses:**\n\n1. *Novelty of SiGeo*: We understand your concern regarding the novelty of SiGeo, particularly in relation to its similarities with ZiCo, Fisher-Rao (FR) norm, and loss functions. We aim to clarify that SiGeo is not merely a combination of these elements but an innovative integration that leverages each component to address specific challenges in NAS. SiGeo's novelty lies in how these elements are unified into a coherent sub-one-shot framework, offering new insights and capabilities beyond their individual applications.\n    - We clarify that the contribution of this work is **not** a new zero-shot proxy. Instead, we focus on showing the significance of the **sub-one-shot** and **warm-up** procedures, along with SiGeo's capacity to leverage information acquired as the candidate network improves with training. \n    - We clarify that the objective of our zero-shot NAS benchmark experiments was not necessarily to demonstrate superior performance to the state-of-the-art (SOTA) zero-shot approaches. Rather, our aim was to validate its \"backward\" comparability to the\u00a0**zero-shot setting**.\n    - To better demonstrate the contribution of this work, we included additional experiments; see the comment above [\"**New Experiments**\"](https://openreview.net/forum?id=EcDO5EXFdH&noteId=CSjlIALRuD). It provides a more comprehensive comparison of our method against ZiCo under sub-one-shot setting in various NAS benchmarks. To summarize, our experiments have shown an enhancement in the performance of the Fisher-Rao norm and training loss as the warm-up level increases. Specifically, in the NAS-201 benchmark, the ranking correlation of the Fisher-Rao norm rose from 0.35 to 0.78 with an increase in warm-up from 0% to 20%. In contrast, the Zico proxy shows limited improvement with additional training.\n    - Our key results are presented in Table 1 and Figure 3. It demonstrates that SiGeo with a 1% warm-up of supernet can achieve comparable performance to one-shot NAS methods but with a significant reduction in computational costs. In addition, SiGeo shows significant performance improvement over the SOTA zero-shot proxy ZiCo with a negligible increase in computational time.\n2. **Experimental Results Comparison**: We acknowledge your observation about the lack of significant gains over ZiCo in CV tasks. In response, we have conducted additional experiments to more comprehensively compare our method against ZiCo **under sub-one-shot setting**; see the results in the comment above [\"**New Experiments**\"](https://openreview.net/forum?id=EcDO5EXFdH&noteId=CSjlIALRuD).\n\n**Responses to Questions:**\n\n1. *The theoretical verification in Figure 2 is mainly conducted on a two-layer MLP-ReLU network, so whether this theory applicable to more complex networks? For example, for the architectures in NAS-Bench-201, what about the ranking consistency when warming up with 0%, 10%, and 40% data?*\n    \n     - **Response**: we have performed the side experiment study to illustrate the consistency of SiGeo. The results have been posted in the comment above [\"**New Experiments**\"](https://openreview.net/forum?id=EcDO5EXFdH&noteId=CSjlIALRuD).\n\n2. *The experiments on the CV task only consider the zero-shot settings. What about the performance when involving SiGeo proxy under the sub-one-shot NAS framework?*\n    - **Response**: new experiments have been added to indicate the performance of SiGeo under the sub-one-shot NAS.\n    \n3. *Similarly, in the RecSys experiments of Figure 3, the zero-shot NAS with SiGeo should also be compared.*\n    \n   - **Response**: The zero-shot result of SiGeo is listed in Table 5 in Appendix I. We observe that without warm-up SiGeo performs on par with ZiCo in RecSys benchmarks.\n    \n4. *Can the SiGeo proxy search for more complex networks on ImageNet, such as in the MobiletNet search space or the Transformer search space?*\n    \n   - **Response**: Recognizing that the regularity assumptions of initiation, local convexity, and almost everywhere differentiability are generally easily met, we are confident in the applicability of SiGeo across various search spaces within the sub-one-shot setting. In our future work, we aim to thoroughly explore and validate this applicability. In the revised version of our manuscript, we will include preliminary findings and insights that underscore the potential and versatility of SiGeo in these broader contexts.\n    \n5. *Since when\u00a0$\\lambda_2$ and\u00a0$\\lambda_2$\u00a0in Formula (7) are set to zero, SiGeo is simplified to ZiCo. Can other existing zero-shot proxies also improve performance by adding the two items?*\n    \n   - **Response**: Theoretically speaking, when the candidate architecture has been warmed up, we believe that the integration of the two terms from Formula (7) could offer improvements in other contexts as well."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459023056,
                "cdate": 1700459023056,
                "tmdate": 1700603977613,
                "mdate": 1700603977613,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "neeV2jYNuv",
                "forum": "EcDO5EXFdH",
                "replyto": "ieHJ6Twbbg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely expecting further discussions with reviewer DixF"
                    },
                    "comment": {
                        "value": "Dear Reviewer DixF,\n\nThank you for your insightful and constructive feedback. As the discussion period is drawing to a close, we wish to make the most of this remaining time to engage in a fruitful dialogue regarding our paper. We hope you have had the opportunity to review our responses to your comments, where we endeavored to thoroughly address each of your concerns.\n\nShould you require any further information or clarifications, please do not hesitate to let us know. We are eager to provide any additional details that might assist in your evaluation.\n\nThanks again! Authors of Paper 5974"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604159998,
                "cdate": 1700604159998,
                "tmdate": 1700604159998,
                "mdate": 1700604159998,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "i7mhZEhOCg",
            "forum": "EcDO5EXFdH",
            "replyto": "EcDO5EXFdH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5974/Reviewer_Hu1G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5974/Reviewer_Hu1G"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes SiGeo, a new proxy to conduct neural architecture search. The main contributions include some theoretical insights, upon which the proxy is designed. Numerical experiments validate the efficacy of the proposed method."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper is written well and easy to follow.\n- The way of using theorem to design proxy makes sense."
                },
                "weaknesses": {
                    "value": "My main concerns are\n\n- Too strong/wrong assumptions.  The foundations of their theorems, e.g., A3-A4, are rarely held for DNNs. For examples, there exists a lot operators in the search space that results in non-differentiability. Meanwhile, the Hessian matrix being positive-definitive implies the objective function as convex at least while is non-convex for Deep learning. In addition for A2, the authors used $\\ell_1$-norm to bound point in a ball, which is wrong since the domain is not a ball at all under $\\ell_1$ norm. \n\n- Wording is a bit big. The paper is titled as information theory, yet I did not find a specific area that actually leverages it significantly, except the usage of Fisher information matrix, while is commonly used as an alternative to Hessian matrix in other literatures.\n\n- Theorem is not necessary for the proxy designs. I would expect that the theorem could indeed provide some novel insights to design some unique and innovative proxy. However, the present proxy seems quite standard for me that researchers should be able to design it out without deriving theorem under strong assumptions beforehand. \n\nDue to the above main reasons, I could not accept the paper at the moment, though the routine of using theorem to design algorithm is what I agree on."
                },
                "questions": {
                    "value": "See the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5974/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5974/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5974/Reviewer_Hu1G"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5974/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699112905307,
            "cdate": 1699112905307,
            "tmdate": 1700033393070,
            "mdate": 1700033393070,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NcmvJPArCk",
                "forum": "EcDO5EXFdH",
                "replyto": "i7mhZEhOCg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5974/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed review and constructive feedback on our manuscript. We value your comments and would like to address weaknesses you've identified.\n\n1. **Assumptions and Theoretical Foundations**: We acknowledge your concerns about the assumptions (A3-A4). However, we would like to offer some clarifications that might shed light on our approach: \n    - Assumption 2: The \u201cball\u201d is a typo, we will change it to \u201cset\u201d in our revised manuscript.\n    - Assumption 3: We would like to clarify our differentiability assumption by using a rigorous definition of \"differentiable almost everywhere.\" This means that, apart from a negligible set with zero measure, the loss function associated with candidate architectures maintains differentiability. The entire theoretical framework would still hold. In fact, the assumption is commonly implicitly assumed in the field of modern deep learning, as without it, the gradient of loss function might not exist and the foundation of gradient descent-based optimization methods would be undermined.\n    - Assumption 4: the Hessian matrix being positive-definitive locally in a compact set $\\mathbb{B}$ (a small region near a local minimum) only implies the objective function is locally convex (instead of global convexity). This assumption is justified by the recent theoretical works that have proven \u201cthe loss will escape the saddle points and reside near a certain local minimum\u201d; see the discussion in the first paragraph of Section 3.\n2. **Title and Information Theory Application**: We appreciate your critique regarding the title's reference to information theory. The development of SiGeo was based on a significant amount of knowledge and existing work from Information theory in statistics. For example, the use of Fisher information matrix, the loss landscape, and the Fisher-Rao norm.\n3. **Theoretical Necessity and Proxy Design**: Thank you for your thought-provoking feedback regarding the necessity of the theorem in our proxy design. \n    - Indeed, the development of a new proxy might not necessarily require a theoretical framework. However, we are of the firm belief that having such a framework greatly helps both researchers and practitioners in comprehending the scope and limits of the method's applicability. This understanding is crucial for effectively applying the proxy in various contexts and for driving further innovations in the field. We have observed that the current STOA zero-shot proxies predominately focus on convolutional neural networks (such as ZiCo [1] and ZenNAS [2]) and current theoretical frameworks often lack the generality needed to effectively encompass diverse model architectures, such as Transformers. Our approach aims to bridge this gap, offering a more flexible theoretical framework that can adapt to a broader range of architectures such as RecSys domain.\n    - The theoretical framework has been instrumental in guiding our search for a new proxy. Initially, our project nearly discarded the Fisher-Rao norm, as it seemed to offer no substantial improvement over ZiCo. However, further theoretical exploration suggested that the Fisher-Rao norm could be effective, provided the weight is close to the local minimum of the loss function. It motivates us to reassess the Fisher-Rao norm, particularly in the context of pre-warming the network. Indeed, without these theoretical insights, the potential of the Fisher-Rao norm might have been overlooked.\n\nWe are committed to addressing these concerns in our revised manuscript and hope that our efforts will bring the paper closer to meeting the acceptance criteria. Thank you again for your thorough review and valuable insights.\n\n**References**\n1. [Li, Guihong, et al. \"ZiCo: Zero-shot NAS via Inverse Coefficient of Variation on Gradients.\" ICLR (2023).](https://openreview.net/pdf?id=rwo-ls5GqGn)\n2. [Lin, Ming, et al. \"Zen-nas: A zero-shot nas for high-performance image recognition.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.](https://arxiv.org/pdf/2102.01063.pdf)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700417948688,
                "cdate": 1700417948688,
                "tmdate": 1700417948688,
                "mdate": 1700417948688,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2RB0sPfhtG",
                "forum": "EcDO5EXFdH",
                "replyto": "i7mhZEhOCg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5974/Reviewer_Hu1G"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5974/Reviewer_Hu1G"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses which resolve my concerns to some extent, while I am still concerned for the assumptions and theorems. \n\nAt first, if the authors assume local convexity, it typically requires training sufficiently many epochs rather than holding in the initial stage or warm-up for a short period of time. Meanwhile, the paper is written to discuss loss function landscape, which is a concept describing the overall loss function, but the local convexity actually concentrates on a small region. \n\nSecondly, the theorems proposed in the paper are not new for me. If this is a theoretical optimization paper, theorem 1 & 2 actually serve as lemmas, but not main contributing theorems. The underlying story presented in the paper need improvements. \n\nDue to the above, I decide to keep my rating though I appreciate the authors' throughout responses."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543124963,
                "cdate": 1700543124963,
                "tmdate": 1700543872356,
                "mdate": 1700543872356,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QsOAzyIVes",
            "forum": "EcDO5EXFdH",
            "replyto": "EcDO5EXFdH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5974/Reviewer_wPAs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5974/Reviewer_wPAs"
            ],
            "content": {
                "summary": {
                    "value": "In this paper the authors propose a zero-cost proxy for NAS that is composed by 3 terms, namely the average gradient estimate divided by the gradient standard deviation, a Fisher-Rao norm and the training loss. The first terms is the same as the ZiCo zero-cost proxy, and the 2 other terms are included as an extension that takes into consideration the training dynamics in the non-zero-cost proxy regime. The authors evaluate their method in standard image classification benchmarks, as well as some recommended systems datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The theoretical motivation for proposing SiGeo is valid.\n\n- The paper is easy to read and well-written.\n\n- The empirical results show comparable or better performance previous ZC proxies."
                },
                "weaknesses": {
                    "value": "- Most of the paper is providing a lot of theory that is already well known in the deep learning community. Then it is using that to add a Fisher-Rao norm and training loss to the ZiCo [1] ZC proxy. This is a marginal contribution and not novel enough in my opinion for the paper to be over the acceptance threshold.\n\n- The improvements are also marginal compared to ZiCo based on the empirical evaluations reported in the paper.\n\n- The authors should also consider evaluating their ZC proxy on NAS-Bench-Suite-Zero [2], that contains more diverse benchmarks than the image classification ones the authors evaluated on.\n\n- No code available.\n\n**References**\n\n[1] https://arxiv.org/pdf/2301.11300.pdf\n\n[2] https://arxiv.org/pdf/2210.03230.pdf"
                },
                "questions": {
                    "value": "- I am puzzled how valid is the theory regarding the generalization and convergence of neural networks in the case of NAS with inheritance of the one-shot model weights. Can the authors say a few more words on this?\n\n- What is the standard deviation of multiple runs for the reported metrics in the experiments section?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5974/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699214827202,
            "cdate": 1699214827202,
            "tmdate": 1699636638502,
            "mdate": 1699636638502,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bWuabOvAhs",
                "forum": "EcDO5EXFdH",
                "replyto": "QsOAzyIVes",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5974/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. **Theoretical Background and Novelty**: We acknowledge your concerns about the theoretical aspects of our paper. We clarify our theoretical contribution:\n    - Our primary theoretical contribution is on expressing minimal achievable training loss/expected training loss in terms of gradient variance, Fisher-Rao norm, and gradient mean. It provides a foundation for our proxy and offers a justification for the necessity of network warm-up.\n    - Our theoretical results stand out in their flexibility compared to state-of-the-art (SOTA) methods. For example, the theory of [ZiCo](https://openreview.net/pdf?id=rwo-ls5GqGn) is applicable under strong assumptions of linear models or MLPs with ReLu activation functions while [ZenNAS](https://arxiv.org/pdf/2102.01063.pdf) is tailored to convolutional neural networks. In contrast, our framework requires only local convexity and differentiability almost everywhere (a.e), significantly broadening its scope.\n    -  We revised the manuscript to condense the theoretical background and focus more on the novel aspects of our work, specifically the new experiment of the Fisher-Rao norm and training loss. \n2. **Improvement upon ZiCo Metric**: Regarding the marginal improvements compared to ZiCo, we appreciate your observation.  We aim to clarify that SiGeo is not merely a combination of these elements but an innovative integration that leverages each component to address specific challenges in NAS. In specific,\n    - We clarify that the contribution of this work is **not** a new zero-shot proxy. Instead, we focus on showing the significance of the **sub-one-shot** and **warm-up** procedures, along with SiGeo's capacity to leverage information acquired as the candidate network improves with training. \n    - We clarify that the objective of our zero-shot NAS benchmark experiments was not necessarily to demonstrate superior performance to the zero-shot approaches. Rather, our aim was to validate its \"backward\" comparability to the\u00a0**zero-shot setting**.\n    - We further included additional experiments; see the comment [\"**New Experiments**\"](https://openreview.net/forum?id=EcDO5EXFdH&noteId=CSjlIALRuD). To summarize, our experiments have shown an enhancement in the performance of the Fisher-Rao norm and training loss as the warm-up level increases. \n    - Our key results in Table 1 and Figure 3 demonstrate that SiGeo with a 1% warm-up of supernet can achieve significantly better performance than ZiCo and comparable performance to one-shot NAS methods but with a significant reduction in computational costs. \n    - We also direct the reviewer to [General Clarification of Contribution](https://openreview.net/forum?id=EcDO5EXFdH&noteId=AwltH9CFe2).\n\n3. **Evaluation on NAS-Bench-Suite-Zero**: Your suggestion to evaluate our ZC proxy on NAS-Bench-Suite-Zero is well-taken. In fact,  we indeed used NAS-Bench-Suite-Zero in our current benchmark (ref. **Section 4.3.1**).\n4. **Code Availability**: We apologize for the absence of the code. We have attached the code as supplementary material alongside the revised manuscript.\n\nRegarding your specific questions:\n\n- **Generalization and Convergence Theory in NAS**: Our approach builds on the premise that inherited weights can provide good initial points for subnets (near some local minimum). If the inherited weight is within a **locally convex** region around a local minimum, then **Theorem 1** and **2** can be used to derive a proxy to evaluate a candidate architecture. \n    - **Theorem 1** provides an upper bound for the optimality gap, and indicates that a candidate architecture with a smaller gradient variance tends to have a smaller optimality gap. \n    - **Theorem 2** provides a lower bound for minimal achievable training loss and indicates that the minimal achievable training loss of candidate arch could be low when the expected absolute sample gradients and the FR norm are high, or the current training loss is low. \n   - Section 3.4 provides an informal analysis to clarify how the sample gradient variance is connected with the trace of Hessian and thus can serve as an indicator of a generalization error.\n- **Standard Deviation in Experiments**: Thank you for your feedback, which we value greatly. In our In the revised manuscript, we will try to incorporate the standard deviation as much as possible. However, we must also consider the significant resource demands of these experiments and the current practice. For instance, although the search is reasonably fast, validating all top-15 candidate architectures in one ResSys experiment alone requires approximately five days per A100 GPU (we have 30 experiments in total), illustrating the extensive nature of these tasks. Given these constraints and common practice, we may need to defer some of these extensive experiments to future work. \n\nWe hope that these revisions and clarifications will adequately address your concerns and enhance the overall contribution of our work. Thank you!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700457195892,
                "cdate": 1700457195892,
                "tmdate": 1700603766257,
                "mdate": 1700603766257,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WUwTg2ERjE",
                "forum": "EcDO5EXFdH",
                "replyto": "QsOAzyIVes",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely expecting further discussions with reviewer wPAs"
                    },
                    "comment": {
                        "value": "Dear Reviewer wPAs,\n\nThank you for your insightful and constructive feedback. As the discussion period is drawing to a close, we wish to make the most of this remaining time to engage in a fruitful dialogue regarding our paper. We hope you have had the opportunity to review our responses to your comments, where we endeavored to thoroughly address each of your concerns.\n\nShould you require any further information or clarifications, please do not hesitate to let us know. We are eager to provide any additional details that might assist in your evaluation.\n\nThanks again!\nAuthors of Paper 5974"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602892579,
                "cdate": 1700602892579,
                "tmdate": 1700602892579,
                "mdate": 1700602892579,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hXsesnKF6g",
                "forum": "EcDO5EXFdH",
                "replyto": "WUwTg2ERjE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5974/Reviewer_wPAs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5974/Reviewer_wPAs"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "I thank the authors for the work they put in the rebuttal. However, I still find that my main concerns are not fully addressed. For instance, NAS-Bench-Suite-Zero consists of 28 tasks in total and not only 5 used in Section 4.3.1. Moreover, reinforcing my other point, I also agree with the concerns that the other reviewers noted, in particular reviewer Hu1G. Citing them: \"the theorems proposed in the paper are not new for me. If this is a theoretical optimization paper, theorem 1 & 2 actually serve as lemmas, but not main contributing theorems.\"\n\nUnfortunately, I decide to keep my score and recommend the authors to extend further the theoretical contributions of the paper and the empirical evaluation on standard benchmark suites."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684350149,
                "cdate": 1700684350149,
                "tmdate": 1700684350149,
                "mdate": 1700684350149,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MgRWtpdB1z",
            "forum": "EcDO5EXFdH",
            "replyto": "EcDO5EXFdH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5974/Reviewer_JuJh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5974/Reviewer_JuJh"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a joint use of the zero-cost and loss metrics for NAS search, aiming to reduce the computational cost of One-Shot NAS and enhance the stability of the Zero-Cost method. The authors also improve upon the shortcomings of the ZiCo metric and validate the proposed approach through experiments on multiple datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The research motivation is clear and well-defined.\n- The application of the proposed method in the field of recommender systems is commendable, addressing practical needs."
                },
                "weaknesses": {
                    "value": "- The paper lacks significant innovation. The concept of Sub-One-Shot has already been mentioned in Prenas, and this paper does not bring many additional insights.\n- The improvement upon the ZiCo metric seems to be inconspicuous, and the contribution appears to be limited. Moreover, the experimental results in Table.2 are very similar to those of ZiCo.\n- The paper lacks a comparison with Prenas in the experimental evaluation."
                },
                "questions": {
                    "value": "Please see the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5974/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699364179976,
            "cdate": 1699364179976,
            "tmdate": 1699636638408,
            "mdate": 1699636638408,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FAcNjWAiuv",
                "forum": "EcDO5EXFdH",
                "replyto": "MgRWtpdB1z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5974/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful review and constructive comments on our manuscript. We appreciate your recognition of the clear research motivation and the practical application of our method in recommender systems. In response to your concerns, we would like to address the points you raised:\n\n1. *Innovation and Relation to Prenas*: We understand your concern regarding the perceived lack of significant innovation compared to Prenas. In our revised manuscript, we will emphasize the distinct aspects of our approach.\n    - **Compared to PreNAS:**\u00a0Although both methods consider combining weight-sharing NSA with zero-shot proxies. SiGeo is essentially different from PreNAS. (1) SiGeo performs the weight-sharing one-shot training to warm up the supernet first and then applies a proxy to search. In contrast, PreNAS reduces the sample space by a zero-cost selector and then performs weight-sharing one-shot training on the preferred architectures. (2) SiGeo is developed based on the idea that the warm-up of the supernet will initialize all subnets simultaneously while PreNas is developed based on the idea of using a proxy to select preferred subnets to alleviate update conflicts.\n2. *Improvement upon ZiCo Metric*: We acknowledge your feedback regarding the incremental improvement over the ZiCo metric. While our primary contributions focus on the\u00a0**sub-one-shot approach**, the SiGeo proxy, and novel experiments on RecSys models/benchmark, we recognize the importance of your concerns.\n    - It is crucial to clarify that the contribution of this work is **not** a new zero-shot proxy. Instead, we focus on showing the significance of the **sub-one-shot** and **warm-up** procedures, along with SiGeo's capacity to leverage information acquired as the candidate network improves with training. \n    - We clarify that the objective of our zero-shot NAS benchmark experiments was not necessarily to demonstrate superior performance to the state-of-the-art (SOTA) zero-shot approaches. Rather, our aim was to validate its \"backward\" comparability to the\u00a0**zero-shot setting**.\n    - To better demonstrate the contribution of this work, we included additional experiments; see the comment above [\"**New Experiments**\"](https://openreview.net/forum?id=EcDO5EXFdH&noteId=CSjlIALRuD). It provides a more comprehensive comparison of our method against ZiCo under sub-one-shot setting in various NAS benchmarks.\n    - Our key results are presented in Table 1 and Figure 3. It demonstrates that SiGeo with a 1% warm-up of supernet can achieve comparable performance to one-shot NAS methods but with a significant reduction in computational costs. In addition, SiGeo shows significant performance improvement over the SOTA zero-shot proxy ZiCo with a negligible increase in computational time.\n3. *Comparison with Prenas*: We acknowledge and appreciate the suggestion to include a comparative analysis with Prenas in our experimental evaluation. We understand the potential value this comparison could bring. However, due to the significant computational and implementation resources required, we are currently unable to incorporate this comparison into the present study. We are committed to conducting this comparison and plan to include it in our future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700454700324,
                "cdate": 1700454700324,
                "tmdate": 1700600114817,
                "mdate": 1700600114817,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6N1uENkEID",
                "forum": "EcDO5EXFdH",
                "replyto": "MgRWtpdB1z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5974/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5974/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely expecting further discussions with reviewer JuJh"
                    },
                    "comment": {
                        "value": "Dear Reviewer JuJh,\n\nThank you for your insightful and constructive feedback. As the discussion period is drawing to a close, we wish to make the most of this remaining time to engage in a fruitful dialogue regarding our paper. We hope you have had the opportunity to review our responses to your comments, where we endeavored to thoroughly address each of your concerns.\n\nShould you require any further information or clarifications, please do not hesitate to let us know. We are eager to provide any additional details that might assist in your evaluation.\n\nThanks again!\nAuthors of Paper 5974"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5974/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602829128,
                "cdate": 1700602829128,
                "tmdate": 1700602829128,
                "mdate": 1700602829128,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]