[
    {
        "title": "Is Training Necessary for Representation Learning"
    },
    {
        "review": {
            "id": "hqdsnsOcy3",
            "forum": "BURvGotSLz",
            "replyto": "BURvGotSLz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5143/Reviewer_YmUG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5143/Reviewer_YmUG"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents LagrangeEmbedding, a method for obtaining a data representation based on a first-order Lagrange basis. The authors present an algorithm for generating a mesh on the input space and show how to form the Lagrange basis in an efficient way using parallelization. The effectiveness of the method was demonstrated on several datasets, mostly in low-dimensional settings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* A novel method for obtaining representations in an unsupervised that doesn't require any learning.\n* The paper is written well and clearly. The visual illustrations are good and help to convey the main points in the paper.\n* The method is grounded by theoretical justifications, such as the universal approximation, which adds a nice flavor to the paper.\n* Code was provided and the results seems to be reproducible."
                },
                "weaknesses": {
                    "value": "* The main limitation of the proposed approach, in my opinion, is that it misses the goal of the paper (or at least the one that was presented). I acknowledge that in some cases, mainly in low-dimensional settings or on simple datasets (e.g., MNIST) it works fine. However, the main goal of learning transferrable representations is from big, complex, and high-dimensional datasets. Currently, this approach doesn't fit these types of data, and I am skeptical if it will ever will as FEM is not a new idea. Hence, at least currently, I do not think this paper can make an impact as the authors imply in the paper. Nevertheless, I do appreciate novel and non-standard directions, even if they are not mature yet, and I give the paper credit for that.  \n* Another possible limitation relates to the fact that the method doesn't have any learnable parameters. Often there is a domain shift between the dataset for learning representations and the dataset of interest, or between the training set and test set. Standard NN-based approaches can work well in such cases (depending on the magnitude of the shift) or can be adjusted to these shifts by fine-tuning the feature extractor, for example. Yet, as the proposed method heavily relied on the algorithm for obtaining a multiscale mesh based on the training set, it is not clear how it will work in such cases.  \n* I expected to see a broader reference to kernel methods, but the paper seems to miss this related research direction entirely. Kernels also form a basis function and are also universal approximators, and perhaps there is some connection to the Lagrange basis function. More importantly, I find two studies particularly relevant (to address and compare to). First, the line of research on infinite-width NNs [1, 2] which also hinges on inner products in the input space. Second, the method presented in [3] also suggests to use simplices for approximating the full data kernel. When does the proposed approach preferred over these modeling choices? \n* Regarding the experiments, it is not clear what is the test performance of the proposed approach on MNIST. If it is 97.25% as implied in the text, how is that equivalent to 6-layer CNN when even a standard LeNet reaches ~99.3% accuracy?  \n* In my opinion, some information is missing. Specifically,\n  * A reference (or a proof) for the two properties of the Lagrange basis function in Section 2.\n  * Intuition on the dimensionality reduction technique in section 3.2.1.\n  * Why is the following true $(n_t/d!)^{1/d} = \\mathcal{O}(h^{-1})$?\n\n[1]  Lee, J., Bahri, Y., Novak, R., Schoenholz, S. S., Pennington, J., & Sohl-Dickstein, J. (2018, February). Deep Neural Networks as Gaussian Processes. In International Conference on Learning Representations.  \n[2]  Matthews, A. G. D. G., Hron, J., Rowland, M., Turner, R. E., & Ghahramani, Z. (2018, February). Gaussian Process Behaviour in Wide Deep Neural Networks. In International Conference on Learning Representations.  \n[3] Kapoor, S., Finzi, M., Wang, K. A., & Wilson, A. G. G. (2021, July). Skiing on simplices: Kernel interpolation on the permutohedral lattice for scalable gaussian processes. In International Conference on Machine Learning (pp. 5279-5289). PMLR."
                },
                "questions": {
                    "value": "* At the beginning of Section 2.1, what is $m$, did you mean $N-1$?\n* Perhaps I didn't understand something, but to me the definition of the Lagrange function $\\mathcal{L}_i$ seems to be the indicator function and not a linear function picking at $p^{(i)}$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5143/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5143/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5143/Reviewer_YmUG"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5143/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698591430064,
            "cdate": 1698591430064,
            "tmdate": 1700983913546,
            "mdate": 1700983913546,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AlbTfKTubM",
                "forum": "BURvGotSLz",
                "replyto": "hqdsnsOcy3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer YmUG (W1, Part 1/2)"
                    },
                    "comment": {
                        "value": ">**W1: I acknowledge that in some cases, mainly in low-dimensional settings or on simple datasets (e.g., MNIST) it works fine. However, the main goal of learning transferrable representations is from big, complex, and high-dimensional datasets. Currently, this approach doesn't fit these types of data.**\n\nWe sincerely appreciate your thorough review of our paper and the concerns you raised regarding our experiments. Here is our response to your questions:\n\nTheoretically, as long as there are enough computational resources, LagrangeEmbedding can extract features from high-dimensional data. According to the important conclusion of FEM and the approximation theory, the linear combination of Lagrange basis functions must adhere to the **error-bound formula** Eqn (1):\n$$\\\\frac{1}{m} \\\\sum_{i=0}^{m-1} |f(x^{(i)};\\\\theta)-y^{(i)}|^2 = O(h) = O(n_t^{-1/d}) = O(n^{-1/d})$$\nThe formula indicates that for $d$-dimensional inputs and MSE loss function, the test $MSE$ of LagrangeEmbedding-based models will decrease to $2^{\u22121/d} \u2217 MSE$ when we double the number of parameters in last linear layer. Therefore, **we can continuously improve the model performance by increasing the number of parameters in the linear layer**. We have experimentally validated this in Section 3.1.3.\n\nFor the applied scenario, when computational resources are limited, two solutions are available:\n1. Design more efficient dimensionality reduction tools as preprocessing layers for LagrangeEmbedding-based networks.\n2. Utilize LagrangeEmbedding as a low-rank mapping, decomposing the $d$-dimensional input space into the product of $d$ one-dimensional spaces and applying LagrangeEmbedding in each one-dimensional space\u200b\u200b:\n\n$$\\\\mathrm{Original~LagrangeEmbedding}: \\\\mathbb{R}^d \\\\rightarrow [0, 1]^n $$\n\n$$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\\\boldsymbol{x} \\\\mapsto (\\\\mathcal{L}_1(\\\\boldsymbol{x}), \\\\cdots, \\\\mathcal{L}_n(\\\\boldsymbol{x}))$$\n\n$$\\\\mathrm{LowRank~LagrangeEmbedding}: \\\\mathbb{R} \\\\times \\\\cdots \\\\times \\\\mathbb{R} \\\\rightarrow [0, 1]^{nd}$$\n\n$$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\\\boldsymbol{x} \\\\mapsto (\\\\mathcal{L}_1(\\\\boldsymbol{x}_1), \\\\cdots, \\\\mathcal{L}_n(\\\\boldsymbol{x}_1), \\\\cdots, \\\\mathcal{L}_1(\\\\boldsymbol{x}_d), \\\\cdots, \\\\mathcal{L}_n(\\\\boldsymbol{x}_d))$$\n\nTo assess the effectiveness of the second approach, we conducted ablation studies. **Table R2-1** (see RtZ7-W1) shows that using a 4-layer LeNet (RtZ7's instance https://github.com/pytorch/examples/tree/main/mnist) as a frozen preprocessing layer, the LagrangeEmbedding-based network achieved higher test accuracy than pre-trained models. Hence, LagrangeEmbedding can extract better features from MNIST, Fashion MNIST, CIFAR-10, and CIFAR-100. This approach has been added to Appendix E. Reviewers can reproduce our experimental results by running **run.sh** in the revised supplementary material.\n\nFinally, due to the limited time of the rebuttal phase, we have not yet tuned the training hyper-parameters on ImageNet and COCO-related experiments. Once we obtain SOTA results, we will promptly provide the corresponding code in our open-source project and publish it as a new paper. We again express our gratitude for your valuable feedback and look forward to your further review."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458517859,
                "cdate": 1700458517859,
                "tmdate": 1700464991094,
                "mdate": 1700464991094,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "46xXeG4b79",
                "forum": "BURvGotSLz",
                "replyto": "hqdsnsOcy3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer YmUG (W1, Part 2/2)"
                    },
                    "comment": {
                        "value": ">**W1: FEM is not a new idea. Hence, at least currently, I do not think this paper can make an impact as the authors imply in the paper.**\n\nIt's true that FEM is not a new concept. However, the current FEM-related papers (e.g., [4][5]) are using neural networks to simulate the FEM without implementing FEM explicitly. Here are three pieces of evidence:\n1. A hallmark of successfully constructing FEM is that the model must adhere to the **error-bound formula** Eqn. (1). This formula indicates that as long as the model's scale is sufficiently large, its performance can reach the theoretical maximum. However, in the FEM-related papers published so far, no one has demonstrated that they have achieved this. In contrast, Our experiments in Section 3.1.3 illustrate that we did it.\n2. In FEM, the support of the basis functions is required to be independent of targets. However, In these papers, they train encoders, which means that the support of the basis functions is altered during training, which is not permitted in standard FEM. In contrast, we derive the fixed basis functions by mathematical deduction to make the train-free encoder.\n3. FEM requires continuous variables as input, but images and text belong to discrete distributions. Therefore, using FEM directly for classification tasks cannot guarantee the **error-bound formula**, and will result in very poor results. In contrast, we design a multi-scale domain decomposition algorithm, as it ensures that FEM can be applied to neural network modeling.\n\nFEM-based software has been successfully used in various advanced engineering fields as the most outstanding algorithm in numerical simulation. As long as the computing resources are sufficient, there is almost no upper limit to the performance of this method. The current implementation of FEM is mature on CPU clusters but challenging on GPU-based parallel platforms. We precisely implement the Lagrange basis functions on PyTorch as one of the main contributions of our paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458654008,
                "cdate": 1700458654008,
                "tmdate": 1700464953482,
                "mdate": 1700464953482,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "98kb1NqwpH",
                "forum": "BURvGotSLz",
                "replyto": "hqdsnsOcy3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer YmUG (W2)"
                    },
                    "comment": {
                        "value": ">**W2: Standard NN-based approaches can adjust the shifts between the dataset for learning representations and the dataset of interest by fine-tuning the feature extractor. Yet, as the proposed method heavily relied on the algorithm for obtaining a multiscale mesh based on the training set, it is not clear how it will work in such cases.**\n\n1. Our LagrangeEmbedding is a universal encoder that can be applied to multiple tasks without fine-tuning. In contrast with standard transfer learning, LagrangeEmbedding is train-free, ensuring lossless memory and avoiding catastrophic forgetting. Admittedly, due to its generality, LagrangeEmbedding-based networks may not outperform task-specific neural networks in terms of accuracy for a particular recognition task. However, our models adhere to the **error-bound formula**, meaning that we can reduce this performance gap by increasing the scale of LagrangeEmbedding.\n2. Our algorithm for generating multiscale meshes is designed to ensure that LagrangeEmbedding-based networks satisfy the **error-bound formula**. This algorithm determines the size of LagrangeEmbedding and the shape of its Lagrange basis functions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458746887,
                "cdate": 1700458746887,
                "tmdate": 1700464894708,
                "mdate": 1700464894708,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yeKbHXVhsg",
                "forum": "BURvGotSLz",
                "replyto": "hqdsnsOcy3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer YmUG (W3)"
                    },
                    "comment": {
                        "value": ">**W3: I expected to see a broader reference to kernel methods, but the paper seems to miss this related research direction entirely.**\n\nThank you for your comments. We have added Appendix F to our paper to discuss the differences between LagrangeEmbedding and recommended papers [1][2][3]. Our LagrangeEmbedding-based network and kernel methods are both linear models. However, their underlying principles are different: kernel methods map low-dimensional linearly inseparable data to high-dimensional and linearly separable data, whereas our method only maps weak-correlated data to be linearly separable. Specifically, if two input data are very close, the inner product of their projections will be close to 1, but if their similarity exceeds the threshold (i.e., longer than one simplex), their projections will be orthogonal. From Figure 1 of our paper:\n\n$$\\mathrm{Projection}(\\frac{1}{3} (p^{(1)} + p^{(2)} + p^{(3)})) = [0, \\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}, 0, 0, 0, 0]$$\n\n$$\\mathrm{Projection}(\\frac{1}{3} (p^{(2)} + p^{(3)} + p^{(4)})) = [0, 0, \\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}, 0, 0, 0]$$\n\n$$\\mathrm{Projection}(\\frac{1}{3} (p^{(0)} + p^{(5)} + p^{(6)})) = [\\frac{1}{3}, 0, 0, 0, 0, \\frac{1}{3}, \\frac{1}{3}, 0]$$\n\nThe first point $\\frac{1}{3} (p^{(1)} + p^{(2)} + p^{(3)}) \\in \\bigtriangleup p^{(1)}p^{(2)}p^{(3)}$ and the second point $\\frac{1}{3} (p^{(2)} + p^{(3)} + p^{(4)}) \\in \\bigtriangleup p^{(2)}p^{(3)}p^{(4)}$ belong to adjacent triangles, so their LagrangeEmbedding projections are close, and the inner product is close to 1. However, the first point $\\frac{1}{3} (p^{(1)} + p^{(2)} + p^{(3)}) \\in \\bigtriangleup p^{(1)}p^{(2)}p^{(3)}$ and the third point $\\frac{1}{3} (p^{(0)} + p^{(5)} + p^{(6)}) \\in \\bigtriangleup p^{(0)}p^{(5)}p^{(6)}$ belong to distant triangles, so their projections are orthogonal.\n\nLagrangeEmbedding has two characteristics:\n1. The inner product of two data in the input space does not necessarily correlate with the inner product in the projection space. In the projection space, their inner product is solely determined by the multi-scale mesh, and the mesh structure is determined by learning the data distribution in the input space. Therefore, for LagrangeEmbedding, two data with inner products close to 1 in the input space may potentially be orthogonal in the projection space.\n2. The LagrangeEmbedding-based network can have unlimited width if we continuously increase its degrees of freedom.\n\nFinally, LagrangeEmbedding has a cost-efficient solution for extracting features from high-dimensional raw data (see W1, Part 1, **Low-Rank LagrangeEmbedding**). Furthermore, LagrangeEmbedding can be used independently or as a parameter-free module inserted into a neural network to enhance the performance (see RtZ7-W1 **Table R2-1**). Therefore, we believe our approach holds advantages over [1][2][3]."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458985655,
                "cdate": 1700458985655,
                "tmdate": 1700464863480,
                "mdate": 1700464863480,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9e99MIL6G3",
                "forum": "BURvGotSLz",
                "replyto": "hqdsnsOcy3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer YmUG (W4)"
                    },
                    "comment": {
                        "value": ">**W4: Regarding the experiments, it is not clear what is the test performance of the proposed approach on MNIST. If it is 97.25% as implied in the text, how is that equivalent to 6-layer CNN when even a standard LeNet reaches ~99.3% accuracy?**\n\nThank you for your feedback. Several LeNet models (from GitHub and PyTorch tutorial https://github.com/pytorch/examples/blob/main/mnist/main.py) achieve ~99.1% test accuracy on MNIST and have parameter counts ranging from 0.8 million to 1.2 million. In contrast, our classifier contains only 0.09 million model parameters. For a fair comparison, our LagrangeEmbedding-based model maintains its advantage in the same parameter counts. In **Table R2-1** (see RtZ7-W1), we show the better performance of LagrangeEmbedding-based networks on other datasets. In Section 3.3, the LagrangeEmbedding-based network contains only 256 parameters but reaches 90% test accuracy on the text classification task, which is the same accuracy performance as the word2vec model (see the instance https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459572885,
                "cdate": 1700459572885,
                "tmdate": 1700464844169,
                "mdate": 1700464844169,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EsvAonrIlK",
                "forum": "BURvGotSLz",
                "replyto": "hqdsnsOcy3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer YmUG (W5)"
                    },
                    "comment": {
                        "value": ">**W5: In my opinion, some information is missing. Specifically, (1) A reference (or a proof) for the two properties of the Lagrange basis function in Section 2. (2) Intuition on the dimensionality reduction technique in section 3.2.1. (3) Why is $(n_t/d!)^{1/d} = O(h^{-1})$ true?**\n\n1. We have now included relevant references in the revised paper, such as [6]. The property \"Universal Approximation\" is from approximation and linear interpolation theory, while the \"Similarity Calculation\" property is based on the theory of first-order linear finite elements.\n2. The dimensionality reduction tool (the untrainable preprocessing layer) in Section 3.2.1 is not well-designed. Replacing it with random linear weighting for dimensionality reduction does not significantly impact the experimental results. Therefore, in Section 4, we stated that our future research direction focuses on constructing a better dimensionality reduction tool that compresses information without loss.\n3. This is a conclusion from FEM. When $d=1,2$, the formula can be derived from the standard uniform mesh (see https://en.wikipedia.org/wiki/Triangular_tiling). In higher dimensions, it can be proven via mathematical induction."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459632064,
                "cdate": 1700459632064,
                "tmdate": 1700464825885,
                "mdate": 1700464825885,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "imKNrfHQwl",
                "forum": "BURvGotSLz",
                "replyto": "hqdsnsOcy3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer YmUG (Q1 & 2)"
                    },
                    "comment": {
                        "value": ">**Q1: At the beginning of Section 2.1, what is $m$, did you mean $N-1$?**\n\nThank you for correcting me. A clear statement would be: ``For any given simplex, select a subset $\\{ (x^{(k_0)}, y^{(k_0)}), \\cdots, (x^{(k_{m'-1})}, y^{(k_{m'-1})}) \\}$ from the training set $\\{ (x^{(0)}, y^{(0)}), \\cdots, (x^{(m-1)}, y^{(m-1)}) \\}$ where $m'$ is the cardinality of subset, $m$ is the cardinality of subset, and all subset elements reside within the given simplex.\"\n\n>**Q2: Perhaps I didn't understand something, but to me the definition of the Lagrange function $\\mathcal{L}_i$ seems to be the indicator function and not a linear function picking at $p^{(i)}$.**\n\nThe Lagrange basis function is globally continuous and piecewise linear, not an indicator function. Please refer to Figure 6 (Middle) in Appendix B, It's a function image demo for 2D Lagrange function $L_7$ and $L_{20}$. Note: unlike some kernel methods, the support of Lagrange bais are not isolated from each other."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459688948,
                "cdate": 1700459688948,
                "tmdate": 1700464806407,
                "mdate": 1700464806407,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KQqd1kicOF",
                "forum": "BURvGotSLz",
                "replyto": "hqdsnsOcy3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer YmUG (References)"
                    },
                    "comment": {
                        "value": "[1] Lee, J., Bahri, Y., Novak, R., Schoenholz, S. S., Pennington, J., & Sohl-Dickstein, J. (2018, February). Deep Neural Networks as Gaussian Processes. In International Conference on Learning Representations.\n\n[2] Matthews, A. G. D. G., Hron, J., Rowland, M., Turner, R. E., & Ghahramani, Z. (2018, February). Gaussian Process Behaviour in Wide Deep Neural Networks. In International Conference on Learning Representations.\n\n[3] Kapoor, S., Finzi, M., Wang, K. A., & Wilson, A. G. G. (2021, July). Skiing on simplices: Kernel interpolation on the permutohedral lattice for scalable gaussian processes. In International Conference on Machine Learning (pp. 5279-5289). PMLR.\n\n[4] Hashash, Y. M. A., Jung, S., & Ghaboussi, J. (2004). Numerical implementation of a neural network based material model in finite element analysis. International Journal for numerical methods in engineering, 59(7), 989-1005.\n\n[5] Javadi, A. A., Tan, T. P., & Zhang, M. (2003). Neural network for constitutive modelling in finite element analysis. Computer Assisted Mechanics and Engineering Sciences, 10(4), 523-530.\n\n[6] Zienkiewicz, O. C., Morgan, K., & Morgan, K. (2006). Finite elements and approximation. Courier Corporation."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459743873,
                "cdate": 1700459743873,
                "tmdate": 1700459743873,
                "mdate": 1700459743873,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LDoSNcGsxn",
                "forum": "BURvGotSLz",
                "replyto": "KQqd1kicOF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Reviewer_YmUG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Reviewer_YmUG"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for the answers and empirical evaluations following the comments made by myself and other reviewers. I have a follow-up question that also relates to my point on OOD data. You state that your model adheres to the error-bound formula, yet, and perhaps I misunderstood something, the algorithm for generating the mesh (Algo. 1) depends on the training data locations. Is that right? If indeed that is true, then some regions, especially those that are not located near the training data points, will be sparsely covered by the mesh (or will not be covered at all). Hence, there is another factor here besides the FEM, your algorithm, which may introduce another error. So, even if you increase the resolution of the mesh (which to my understanding will be denser only in regions of the training data), the proposed bound will not be correct, no?\n\nAlso, regarding Q2 in my original comment. I am sorry for the confusion. What I meant is that on page 2, you define the Lagrange function in terms of the node only which can be perceived as an indicator function picking at the corresponding node. But now, after a second read, I understand what you meant."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649190024,
                "cdate": 1700649190024,
                "tmdate": 1700649190024,
                "mdate": 1700649190024,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4p9k96iLgK",
                "forum": "BURvGotSLz",
                "replyto": "hqdsnsOcy3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YmUG"
                    },
                    "comment": {
                        "value": "> I have a follow-up question that also relates to my point on OOD data. You state that your model adheres to the error-bound formula, yet, and perhaps I misunderstood something, the algorithm for generating the mesh (Algo. 1) depends on the training data locations. Is that right? If indeed that is true, then some regions, especially those that are not located near the training data points, will be sparsely covered by the mesh (or will not be covered at all). Hence, there is another factor here besides the FEM, your algorithm, which may introduce another error. So, even if you increase the resolution of the mesh (which to my understanding will be denser only in regions of the training data), the proposed bound will not be correct, no?\n\nWe sincerely appreciate the reviewer's feedback.  As confirmed in Section 3.1.3, the LagrangeEmbedding-based network adheres to the error-bound formula of FEM and approximation theory on the test set. These experimental results demonstrate that our approach handles out-of-distribution (OOD) data well.\n\nBased on Figure 1 or Eqn (3), **if a test data point is located in any simplex of the mesh, LagrangeEmbedding will map it to a non-zero vector, whether it resides in a coarse or fine simplex.** Specifically, If a test data point is within a coarse simplex, there are none or rare training data points similar to this data, indicating lower confidence in predicting this data.\n\nConversely, if the test data point is outside the mesh, LagrangeEmbedding will map it to a zero vector. To prevent this from happening, the initial mesh must be sufficiently large. As we introduced at the beginning of Alg 1, \"Initial Node Matrix $P$ containing coordinates of $d + 1$ points forming a simplex covering all training raw data\", it refers to the following large simplex (see `utils.py`/`utilsv2.py`) $T$ given by its vertices:\n\n$$V(T) = \\\\{ \\boldsymbol{v}^{(j)} | v_k^{(j)} = 1_{j \\neq k} [(1+\\epsilon) \\min_i x_k^{(i)} - \\epsilon \\max_i x_k^{(i)}] + 1_{j = k} [(2+3\\epsilon) \\max_i x_k^{(i)} - (1+3\\epsilon) \\min_i x_k^{(i)}], j=0, \\cdots d-1; v_k^{(d)} = (1+\\epsilon) \\min_i x_k^{(i)} - \\epsilon \\max_i x_k^{(i)} \\\\}, \\epsilon=0.1.$$\n\n\nIn a particular scenario, if the empirical distribution of the training set and test set are significantly different in the boundary area, causing some test data points still lie outside the mesh. The error-bound formula Eqn (1) will still hold when such abnormal data are in the minority:\n\n$$\\frac{1}{m} \\sum_{i=0}^{m-1} |f(x^{(i)};\\theta)-y^{(i)}|^2 = \n\\frac{1}{m} \\sum_{x^{(i)} \\in T} |f(x^{(i)};\\theta)-y^{(i)}|^2 + \\frac{1}{m} \\sum_{x^{(i)} \\notin T} |f(x^{(i)};\\theta)-y^{(i)}|^2 = \\Theta (\\frac{1}{m} \\sum_{x^{(i)} \\in T} |f(x^{(i)};\\theta)-y^{(i)}|^2) = O(n^{-1/d})$$"
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666455730,
                "cdate": 1700666455730,
                "tmdate": 1700669015933,
                "mdate": 1700669015933,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RabVWXkEdf",
            "forum": "BURvGotSLz",
            "replyto": "BURvGotSLz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5143/Reviewer_RtZ7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5143/Reviewer_RtZ7"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a training-free approach for generating a feature vector, where each coordinate value corresponds to the output of a Lagrange basis function. The proposed method enjoys theoretical guarantees on its approximation error as a function of the number of parameters. The effectiveness of the resulting embedding is evaluated on fitting data drawn from known distributions, as well as classification/super-resolution on the MNIST dataset, and classification on AG News dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method does not require training/backpropagation to generate input embeddings.\n- The method is evaluated across multiple tasks from traditional data fitting, to computer vision and NLP tasks.\n- The method enjoys theoretical bounds on the approximation error given the number of model parameters.\n- Limitations are discussed"
                },
                "weaknesses": {
                    "value": "- The paper over-promises and under-delivers. Among other broad claims - e.g. first paragraph of the conclusion, ``unparalleled level of explainability\u201d - the title itself \u201cIs training necessary for representation learning\u201d suggests that the proposed method can be comparable to training-based approaches such as neural networks. Yet, there exists few, if any, quantitative comparisons between the proposed method and neural network approaches, especially for the (toy) computer vision and NLP experiments.\n- In fact, the basic 2-layer convolutional network, for instance taken from the PyTorch tutorial page (https://github.com/pytorch/examples/tree/main/mnist), already achieves 98% accuracy on MNIST in the first epoch (outperforming the proposed approach), which completes in under a minute on a CPU and presumably orders of magnitude faster on GPU. \n- Sec 3.1.2 compares against neural networks when fitting distributions drawn from 2-dimensional distributions, but it is not stated what network parameters nor training parameters are used other than the fact that it is a MLP. \n- It seems that in Table 1, Random Forest is already highly effective at achieving almost perfect R^2 scores, and performance on most of the distributions considered appears to have already saturated.\n- How did the projection layer in Sec 3.2.1 arise? There is no explanation for why this specific projection equation was introduced, and while it claims to contain \u201cno trainable model parameters\u201d, it appears to require careful hand-crafting as well.\n- Speed is touted as an advantage of the method, but there exists no wall-clock timing comparisons for computing the proposed embedding.\n\n\nMinor comments\n- Eqn (2) $y^{(j)}$ should be $y^{(i)}$ instead\n- In Sec 3.3, does \u201cthe neural network\u201d refer to the proposed method (i.e. typo)? If not, are there quantitative results and comparisons for the proposed method? \n- Also in Appendix D.2., I assume \u201cRemarkably, after just 4 epochs of training, the neural network outputs close approximate the target values\u201d is also a typo?"
                },
                "questions": {
                    "value": "- Sec 3.3 - can you elaborate on how the pre-processing layer is implemented?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5143/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5143/Reviewer_RtZ7",
                        "ICLR.cc/2024/Conference/Submission5143/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5143/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698712937054,
            "cdate": 1698712937054,
            "tmdate": 1700674959733,
            "mdate": 1700674959733,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xXlG9eNeeo",
                "forum": "BURvGotSLz",
                "replyto": "RabVWXkEdf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer RtZ7 (Part 1/3)"
                    },
                    "comment": {
                        "value": "> **W1: The paper over-promises and under-delivers. Among other broad claims - e.g. first paragraph of the conclusion, ``unparalleled level of explainability\u201d - the title itself \u201cIs training necessary for representation learning\u201d suggests that the proposed method can be comparable to training-based approaches such as neural networks. Yet, there exists few, if any, quantitative comparisons between the proposed method and neural network approaches, especially for the (toy) computer vision and NLP experiments.**\n\n1. Thank you for your feedback. We place a strong emphasis on the explainability of our LagrangeEmbedding. As we demonstrated in Section 2 & 3.1.3, LagrangeEmbedding-based models strictly follow the complete FEM theory due to adhering to the **error-bound formula**:\n$$\\frac{1}{m} \\sum_{i=0}^{m-1} |f(x^{(i)};\\theta)-y^{(i)}|^2 = O(h) = O(n_t^{-1/d}) = O(n^{-1/d})$$\nFor $d$-dimensional inputs and MSE loss function, this formula shows that when we double the parameters counts of the last linear layer, the training error $MSE$ of LagrangeEmbedding-based models will get reduced to $2^{-1/d}*MSE$. **Therefore, we can continuously improve the model performance by increasing the number of parameters in the linear layer.** \n\n2. In Sections 3 & 5, when we mentioned that \"LagrangeEmbedding can be comparable to neural network-based encoders,\" we meant that \"the LagrangeEmbedding-based network performs equally well or even better than models of similar scale.\" To avoid any misunderstanding, we have included additional experiments for image classification tasks:\n\n**Table R2 - 1**: Ablation studies. We present the experimental results for image classification tasks conducted on MNIST, FashionMNIST, CIFAR-10, and CIFAR-100 datasets. The model instances used in these experiments are obtained from the PyTorch tutorial page (https://github.com/pytorch/examples/tree/main/mnist). When the \"LagrangeEmbed\" column is marked as \"False,\" it indicates that we trained the entire CNN. When \"LagrangeEmbed\" is marked as \"True,\" we freeze the CNN (excluding the last linear layer) as a preprocessing layer and insert our parameter-free LagrangeEmbedding before the final linear layer, then only train the final linear layer. The \"data_aug=True\" signifies using random horizontal flip as a data augmentation technique.\nThe \"init time\" column reports the wall-clock time taken for initializing our LagrangeEmbedding on an RTX 2080ti GPU. All training configurations are taken from the source PyTorch tutorial page, and all experiments can be reproduced by running `bash run.sh` in our revised supplementary material.\n| dataset  | LagrangeEmbed | data aug | test acc | init time |\n| -------- | ------------- | -------- | -------- | --------- |\n| minist       | False     | False    | 99.14%   | N/A       |\n| minist       | True      | False    | 99.17%   | 1.55s     |\n| fashionmnist | False     | False    | 92.47%   | N/A       |\n| fashionmnist | True      | False    | 92.56%   | 1.55s     |\n| fashionmnist | False     | True     | 92.14%   | N/A       |\n| fashionmnist | True      | True     | 92.14%   | 2.08s     |\n| cifar10      | False     | False    | 71.43%   | N/A       |\n| cifar10      | True      | False    | 71.54%   | 1.83s     |\n| cifar10      | False     | True     | 71.93%   | N/A       |\n| cifar10      | True      | True     | 72.09%   | 2.29s     |\n| cifar100     | False     | False    | 36.50%   | N/A       |\n| cifar100     | True      | False    | 36.78%   | 1.84s     |\n| cifar100     | False     | True     | 36.58%   | N/A       |\n| cifar100     | True      | True     | 36.97%   | 2.30s     |\n\n**Table R2-1** shows that LagrangeEmbedding can be inserted as a parameter-free module into a frozen model, where the improved model size remains almost unchanged, yet achieves better model performance. For text classification tasks, our LagrangeEmbedding-based network in Section 3.3 contains only 256 parameters but performs on par with the 6.13 million-parameter word2vec model."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459848260,
                "cdate": 1700459848260,
                "tmdate": 1700463742139,
                "mdate": 1700463742139,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DBN3Yov023",
                "forum": "BURvGotSLz",
                "replyto": "RabVWXkEdf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer RtZ7 (Part 2/3)"
                    },
                    "comment": {
                        "value": ">**W2: In fact, the basic 2-layer convolutional network, for instance taken from the PyTorch tutorial page (https://github.com/pytorch/examples/tree/main/mnist), already achieves 98% accuracy on MNIST in the first epoch (outperforming the proposed approach), which completes in under a minute on a CPU and presumably orders of magnitude faster on GPU.**\n\nThank you to the reviewer for providing the model instance (https://github.com/pytorch/examples/tree/main/mnist). We checked it and found it has 11 times more parameters than our model used in Section 3.2. To evaluate the Lagrange-based network fairly, we revised the implementation of our multi-scale domain decomposition method to run on GPU and conducted the experiments as shown in **Table R2-1**.\n\nIn these experiments, the PyTorch model instance has the same size as the LagrangeEmbedding-based network. However, while the former trains the entire model, the latter only trains the final linear layer. Notably, the latter performs equal time efficiency during inference, and outperforms in evaluation results. Although the LagrangeEmbedding-based network requires initialization before use, its actual initialization time is minimal.\n\n>**W3: Sec 3.1.2 compares against neural networks when fitting distributions drawn from 2-dimensional distributions, but it is not stated what network parameters nor training parameters are used other than the fact that it is a MLP.**\n\nThank you for your feedback. The neural network we described in Figure 3 is a 5-layer MLP using the ReLU activation function. No matter how we increase the number of parameters of this MLP or tune the training parameter configuration, the MLP cannot fit the high-frequency area in dataset $\\mathbb{C}$ well. This experimental phenomenon is consistent with the description in the paper [1].\n\n>**W4: It seems that in Table 1, Random Forest is already highly effective at achieving almost perfect $R^2$ scores, and performance on most of the distributions considered appears to have already saturated.**\n\nYes. For data fitting tasks, random forests exhibit similar performance to our LagrangeEmbedding-based network and overcome the challenge of fitting multi-frequency data. However, unlike our LagrangeEmbedding-based network, in most cases, random forests are not suitable for image and text-related recognition tasks.\n\n>**W5: How did the projection layer in Sec 3.2.1 arise? There is no explanation for why this specific projection equation was introduced, and while it claims to contain \u201cno trainable model parameters\u201d, it appears to require careful hand-crafting as well.**\n\nWe appreciate the feedback from the reviewer. The architecture of our LagrangeEmbedding-based network consists of a \"preprocessing layer -> LagrangeEmbedding -> linear layer,\" where the static preprocessing layer serves as a dimensionality reduction tool, and the train-free LagrangeEmbedding acts as the projection layer.\n\nAs we explained in the **Remark** of Section 3.2.1, the dimensionality reduction tool (the untrainable preprocessing layer) in Section 3.2.1 wasn't well-designed. Replacing it with random linear weighting for dimensionality reduction had minimal impact on the experimental results. To prevent any misunderstanding and highlight the advantages of LagrangeEmbedding, please refer to the preceding **Table R2-1**.\n\n>**W6: Speed is touted as an advantage of the method, but there exist no wall-clock timing comparisons for computing the proposed embedding.**\n\nWe have updated our supplementary materials, and the multi-scale domain decomposition method can now be executed on GPU. In terms of image classification tasks, our model demonstrates similar time efficiency compared to other networks (refer to **Table R2-1** for details. To reproduce these results, please run `bash run.sh`).\n\nFor text classification tasks, our model achieves faster performance, with an epoch taking 74.99 seconds (online training on RTX 2080Ti), compared to the word2vec model (https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html), which requires 91.24 seconds per epoch (online training on RTX 2080Ti). To reproduce these results, please run `textv2.py` and `textv2_comparion.py`."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459879201,
                "cdate": 1700459879201,
                "tmdate": 1700465039565,
                "mdate": 1700465039565,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AlPB8pJs0d",
                "forum": "BURvGotSLz",
                "replyto": "RabVWXkEdf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer RtZ7 (Part 3/3)"
                    },
                    "comment": {
                        "value": "# Minor comments\n\n>**C1: Eqn (2) $y^{(j)}$ should be $y^{(i)}$ instead.**\n\nThank you for reminding us of the typo!\n\n>**C2: In Sec 3.3, does \u201cthe neural network\u201d refer to the proposed method (i.e. typo)? If not, are there quantitative results and comparisons for the proposed method?**\n\nIt is not a typo. Here, \"the neural network\" refers to our LagrangeEmbedding-based network. **Our LagrangeEmbedding has only one hyperparameter: the degrees of freedom (dof).** When we set the dof to 64, the test accuracy on AG news dataset exceeds 90%. For lower dof, the test accuracy will decrease. \n\n>**C3: Also in Appendix D.2., I assume \u201cRemarkably, after just 4 epochs of training, the neural network outputs close approximate the target values\u201d is also a typo?**\n\nThis is not a typo. The meaning of this sentence is that the LagrangeEmbedding-based network converged after the first epoch training and did not obtain more test accuracy in the following four epochs.\n\n# Questions\n\n>**Q1: Sec 3.3 - can you elaborate on how the pre-processing layer is implemented?**\n\nYes, from our supplementary materials, the pre-processing layer outputs the TF [2] value where\n$$\\\\mathrm{TF} = \\\\frac{\\\\mathrm{number\\\\ of\\\\ times\\\\ the\\\\ term\\\\ appears\\\\ in\\\\ the\\\\ document}}{\\\\mathrm{total\\\\ number\\\\ of\\\\ terms\\\\ appears\\\\ in\\\\ the\\\\ document}}$$"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460367107,
                "cdate": 1700460367107,
                "tmdate": 1700465246235,
                "mdate": 1700465246235,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f6tnnJaa6z",
                "forum": "BURvGotSLz",
                "replyto": "RabVWXkEdf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer RtZ7 (Reference)"
                    },
                    "comment": {
                        "value": "[1] Xu, Z. Q. J., Zhang, Y., & Xiao, Y. (2019). Training behavior of deep neural network in frequency domain. In Neural Information Processing: 26th International Conference, ICONIP 2019, Sydney, NSW, Australia, December 12\u201315, 2019, Proceedings, Part I 26 (pp. 264-274). Springer International Publishing.\n\n[2] Sparck Jones, K. (1972). A statistical interpretation of term specificity and its application in retrieval. Journal of documentation, 28(1), 11-21."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460673236,
                "cdate": 1700460673236,
                "tmdate": 1700460673236,
                "mdate": 1700460673236,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PE5xLU4cb8",
                "forum": "BURvGotSLz",
                "replyto": "f6tnnJaa6z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Reviewer_RtZ7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Reviewer_RtZ7"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the rebuttal"
                    },
                    "comment": {
                        "value": "Thank you authors for the response. I still have some remaining concerns, especially regarding the main claim of the paper \n\nRegarding authors' latest experiments\n\n> **Therefore, we can continuously improve the model performance by increasing the number of parameters in the linear layer.**\n\nThis claim needs to be empirically backed, since it is expected that generalization performance saturates or decreases after some point. Model performance, or in general how good the representations are, should not be measured on the training set (as done in the theory, since NNs can overfit to even random finite training data given enough parameters), but on unseen data. The latter case needs to be experimentally determined, since the theory does not bound this generalization error. \n\n> **Table R2 - 1: Ablation studies**\n\nThank you for the additional comparison. The additional results indeed reflect that the proposed method can perform similarity to the MNIST CNN architecture. However, CIFAR-10 etc. are problems which have already been solved by training-based representation learning methods. The proposed method still yields poor performance on such datasets (and it is not clear whether they can even scale especially regarding generalization, as noted by previous comment), which suggests the proposed method is still far from competing with learnt representations especially since the overall computation time is still similar.\n\nA minor comment: It might be confusing to readers to use \"the neural network\" interchangeably with the proposed network, since it appears that NNs in general are comparison baselines for the proposed approach. \n\nI raised my score to 5, since the experiments conclude that the proposed method performs comparably to toy NNs, and better than random embeddings. I did not raise it further because \n\n- The proposed method has not be shown to be able to scale, in terms of generalization, with number of parameters to better close the gap to larger NNs even on the toy tasks of CIFAR-10/100 (see first part of this reply)\n- As such, the claim implied by the title seem to contradict the experiment results, since the proposed method performs poorly on tasks already solved by NNs\n- On the toy vision models, the method does not seem to provide any significant advantage in terms of speed.\n- The claim that the method provides \"unparalleled level of explainability\" is not backed by any evidence nor comparisons."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674900436,
                "cdate": 1700674900436,
                "tmdate": 1700674900436,
                "mdate": 1700674900436,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d8hPuY1dXX",
                "forum": "BURvGotSLz",
                "replyto": "RabVWXkEdf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RtZ7"
                    },
                    "comment": {
                        "value": "> This claim needs to be empirically backed, since it is expected that generalization performance saturates or decreases after some point. Model performance, or in general how good the representations are, should not be measured on the training set, but on unseen data. The latter case needs to be experimentally determined, since the theory does not bound this generalization error.\n\nThank you for the valuable insights. We have the same point: generally, the relationship between model capacity and its performance exhibits Marginal Utility. People cannot continuously improve their model performance on the test set by increasing the model size without limits.\n\nHowever, this relationship is explicit to our model, and the optimal capacity (the point of generalization performance saturation) is related to the number of training examples: the relationship between the capacity of the LagrangeEmbedding-based network and its test error adheres to the **error-bound formula** until it reaches optimal capacity. To substantiate this claim, in Section 3.1.3, we used a more precise metric, the $l^2$ error, to assess the model's generalization performance:\n\n$$l^2 = E_{(x,y) \\sim p_{data}} [|f(x;\\theta) - y|^2] = \\int |f(x;\\theta) - y|^2 \\ \\mathrm{d}x.$$\n\nHere, we evaluate the model's performance on the given data-generating distribution $p_{data}$ where the $l^2$ error is a numerical integral over the whole input space. The entire input space contains not only the test set but also more unseen data. Therefore, if the $l^2$ error is small, the test MSE will be smaller.\n\nIn Section 3.1.3, we conducted experiments on one-dimensional and two-dimensional data fitting tasks. The datasets consisted of 1000 training examples and 200 test examples for the one-dimensional case and 7500 training examples and 1500 test examples for the two-dimensional case. These datasets are sufficient for low-dimensional regression tasks. Figure 4 shows, that we observed that \"the relationship between the number of linear layer parameters and performance on test set adheres the **error-bound formula**.\" Furthermore, we also observed this phenomenon in the MNIST super-resolution task.\n\n> The additional results indeed reflect that the proposed method can perform similarity to the MNIST CNN architecture. However, CIFAR-10 etc. are problems which have already been solved by training-based representation learning methods. The proposed method still yields poor performance on such datasets (and it is not clear whether they can even scale especially regarding generalization, as noted by previous comment), which suggests the proposed method is still far from competing with learnt representations especially since the overall computation time is still similar.\n\nThank you for the suggestions, and we will do more experiments in our future work. The ablation experiments in **Table R2-1** demonstrate that we can improve the performance of a given model by using LagrangeEmbedding, which validates that LagrangeEmbedding indeed extracts better features. **Table R2-1** shows low test accuracy because the given baseline model is a simple 4-layer CNN.\n\n---\n\nFinally, we regret that we couldn't provide more extensive experimental results before the rebuttal deadline, especially the experiments for large models and large datasets. Here is what we are currently working on:\n\n1. The explainability of LagrangeEmbedding is manifested in its adherence to the **error-bound formula**, which we have experimentally proved in data fitting and image super-resolution tasks. We are in the process of validating this on better models and bigger datasets.\n2. The **error-bound formula** of FEM requires the loss function to be MSE, while classification tasks generally use cross-entropy as the loss function. Although our experiments in Table 1 and **Table R2-1** demonstrate the effectiveness of LagrangeEmbedding, we are still studying the theoretical principles in an exploratory stage."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689296158,
                "cdate": 1700689296158,
                "tmdate": 1700690345977,
                "mdate": 1700690345977,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3z1U4MB7hy",
            "forum": "BURvGotSLz",
            "replyto": "BURvGotSLz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5143/Reviewer_P3jk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5143/Reviewer_P3jk"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the author proposed a feature extraction method termed LagrangeEmbedding, which can extract features from simple image and text datasets. LagrangeEmbedding fits a function with many piecewise linears. The proposed method is validated with regressor and classification tasks.\n\nOverall, the ideal is novel, which can inspire further development of unsupervised representation learning. However, the related works that are closely related to the thinking of LagrangeEmbedding should be given. The proposed method seems to only work on simple datasets. What's more, the performance comparison is not provided."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea is novel. It provides a novel perspective for unsupervised representation learning.\n2. Some detailed examples and analyses are provided."
                },
                "weaknesses": {
                    "value": "1. The proposed method seems to only work on some toy tasks. \n2. The related work sections or some closely related works are not provided.\n3. The proposed method is only validated on simple image and text datasets. The comparison results with SOTA methods are not given. Even the proposed method achieves lower accuracy than SOTA methods. The comparison experiment with SOTA methods can assist the reader in finding the gap between the proposed and SOTA methods.\n4. The proposed method only runs in a non-parallel manner, as mentioned in the future directions section."
                },
                "questions": {
                    "value": "1. It seems that the proposed method can only extract low-level features, unlike the deep learning-based methods. The extracted features seem only suitable for toy tasks. Does the proposed method can extract non-low-level features? \nThe author is suggested to add some analysis and discussion.\n2. How can we extend the proposed method for complex tasks in actual situations? The author is suggested to add some discussion.\n3. I have not seen the author mention some closely related works. Is the proposed method totally original? If not, please provide the detailed related works and the difference between the proposed method and the related works.\n4. In section 2.1,  the definition of m in x^{(m)} is not given. What's the difference bettween the x^{(N-1)} and  x^{(m)} ?\n\nOther suggestions:\na\uff09 In Eqn(2), the \u2018i\u2019 is suggested to be replaced with 'n';\nb\uff09 \u201dgiven function F (x) to be fitted\u201d  ->``given function F (x) to be fitted\u201d\nc\uff09 The definition of SVR is not given."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5143/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698999196857,
            "cdate": 1698999196857,
            "tmdate": 1699636507824,
            "mdate": 1699636507824,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AynD2zegFn",
                "forum": "BURvGotSLz",
                "replyto": "3z1U4MB7hy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer P3jk (Part 1/2)"
                    },
                    "comment": {
                        "value": ">**W1: The proposed method seems to only work on some toy tasks.**\n\nWe sincerely appreciate the reviewer's feedback. We have now updated the GPU implementation of the multi-scale domain decomposition method in the supplementary materials. Additionally, for other datasets or more complex tasks, we have proposed several approaches and conducted further experiments to analyze their impact. The experimental results demonstrate that our LagrangeEmbedding is also fast and effective for other datasets (Please refer to our first response to YmUG-W1 and RtZ7-W1 for details). \n\nDue to the limited time of the rebuttal phase, we have not yet tuned the training hyper-parameters on ImageNet and COCO-related experiments. Once we obtain SOTA results, we will promptly provide the corresponding code in our open-source project and publish it as a new paper. We again express our gratitude for your valuable feedback and look forward to your further review.\n\n>**W2: The related work sections or some closely related works are not provided.**\n\nThank you for the reviewer's reminder. Our LagrangeEmbedding encoder is original. Unlike other other FEM-related models, it is an explicit implementation of FEM rather than an imitation, and it is the first model that satisfies the FEM **error-bound formula**. Then, our approach differs from kernel methods, it maps two raw data to orthogonal vectors only when the two raw data have significant differences. We have included Appendix F in the revised paper to discuss the distinctions between LagrangeEmbedding-based networks and kernel methods.\n\n>**W3: The proposed method is only validated on simple image and text datasets. The comparison results with SOTA methods are not given.**\n\nThank you for your feedback. Admittedly, due to the train-free property and generality of LagrangeEmbedding, LagrangeEmbedding-based networks may not outperform task-specific neural networks in terms of accuracy for a particular recognition task. However, our LagrangeEmbedding, can be inserted into any frozen neural network as a plug-and-play parameter-free module (Please refer to **Table R2-1** in our response to RtZ7-W1). It barely changes the model scale, only requires training the final linear layer, but enhances the model's performance. These experiments indicate that LagrangeEmbedding has the potential to help models achieve SOTA results.\n\n>**W4: The proposed method only runs in a non-parallel manner, as mentioned in the future directions section.**\n\nWe have recently upgraded the GPU implementation of the multi-scale domain decomposition method in the revised supplementary materials. It is available to run the whole data pipeline of LagrangeEmbedding-based networks in GPU parallel mode."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460850005,
                "cdate": 1700460850005,
                "tmdate": 1700520795705,
                "mdate": 1700520795705,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "spWoD3d7zY",
                "forum": "BURvGotSLz",
                "replyto": "3z1U4MB7hy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer P3jk (Part 2/2)"
                    },
                    "comment": {
                        "value": ">**Q1: It seems that the proposed method can only extract low-level features, unlike the deep learning-based methods. The extracted features seem only suitable for toy tasks. Does the proposed method can extract non-low-level features? The author is suggested to add some analysis and discussion.**\n\nThank you for your feedback. To extract non-low-level features from large raw data, we can utilize LagrangeEmbedding as a low-rank mapping, decomposing the $d$-dimensional input space into the product of $d$ one-dimensional spaces and applying LagrangeEmbedding in each one-dimensional space\u200b\u200b:\n$$\\\\mathrm{Original~LagrangeEmbedding}: \\\\mathbb{R}^d \\\\rightarrow [0, 1]^n $$\n\n$$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\\\boldsymbol{x} \\\\mapsto (\\\\mathcal{L}_1(\\\\boldsymbol{x}), \\\\cdots, \\\\mathcal{L}_n(\\\\boldsymbol{x}))$$\n\n$$\\\\mathrm{LowRank~LagrangeEmbedding}: \\\\mathbb{R} \\\\times \\\\cdots \\\\times \\\\mathbb{R} \\\\rightarrow [0, 1]^{nd}$$\n\n$$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\\\boldsymbol{x} \\\\mapsto (\\\\mathcal{L}_1(\\\\boldsymbol{x}_1), \\\\cdots, \\\\mathcal{L}_n(\\\\boldsymbol{x}_1), \\\\cdots, \\\\mathcal{L}_1(\\\\boldsymbol{x}_d), \\\\cdots, \\\\mathcal{L}_n(\\\\boldsymbol{x}_d))$$\nThe ablation studies in **Table R2-1** (see RtZ7-W1) demonstrate that this approach is indeed effective. It successfully reduces the 12,544-dimensional features (the projection dimension of a 4-layer LeNet) to 2 * 12,544 one-dimensional features.\n\n>**Q2: How can we extend the proposed method for complex tasks in actual situations? The author is suggested to add some discussion.**\n\nIf users have sufficient computing resources, they can address this issue by increasing the scale of LagrangeEmbedding. As LagrangeEmbedding-based networks adhere to the error-bound formula, **we can continuously improve the model performance by increasing the number of parameters in the linear layer**. For users with limited computational resources, we can consider the solution described in the previous question by partitioning the high-dimensional input space to reduce computational costs.\n\n>**Q3: Is the proposed method totally original?**\n\nYes, our multi-scale domain decomposition method (DDM) and the construction method of the Lagrange basis are totally original. Specifically, we construct the LagrangeEmbedding architecture by using Eqn (3) and initialize LagrangeEmbedding size via the multi-scale domain decomposition method. Unlike existing FEM-related neural networks [1][2], we have implemented FEM explicitly, rather than imitating it. The evidence lies in the fact that our model adheres to the **error-bound formula**.\n\n>**Q4: In section 2.1, the definition of $m$ in $x^{(m)}$ is not given. What's the difference bettween the $x^{(N-1)}$ and $x^{(m)}$?**\n\nThanks to the reviewer's reminder. Here, $m$ is the cardinality of the training set, and $N$ is the cardinality of the subset. A clear statement would be: ``For any given simplex, select a subset $\\{ (x^{(k_0)}, y^{(k_0)}), \\cdots, (x^{(k_{m'-1})}, y^{(k_{m'-1})}) \\}$ from the training set $\\{ (x^{(0)}, y^{(0)}), \\cdots, (x^{(m-1)}, y^{(m-1)}) \\}$ where $m'$ is the cardinality of subset, $m$ is the cardinality of subset, and all subset elements reside within the given simplex.\"\n\n>**Other suggestions: a\uff09 In Eqn(2), the \u2018i\u2019 is suggested to be replaced with 'n'; b\uff09 \u201dgiven function F(x) to be fitted\u201d ->``given function F(x) to be fitted\u201d c\uff09 The definition of SVR is not given.**\n\nWe want to express our gratitude for the reviewer's valuable suggestions. We have incorporated these recommendations into the content of the newly revised paper."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460903868,
                "cdate": 1700460903868,
                "tmdate": 1700464647272,
                "mdate": 1700464647272,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "crP7miPO3Y",
                "forum": "BURvGotSLz",
                "replyto": "3z1U4MB7hy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5143/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer P3jk (References)"
                    },
                    "comment": {
                        "value": "[1] Hashash, Y. M. A., Jung, S., & Ghaboussi, J. (2004). Numerical implementation of a neural network based material model in finite element analysis. International Journal for numerical methods in engineering, 59(7), 989-1005.\n\n[2] Javadi, A. A., Tan, T. P., & Zhang, M. (2003). Neural network for constitutive modelling in finite element analysis. Computer Assisted Mechanics and Engineering Sciences, 10(4), 523-530."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5143/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461215225,
                "cdate": 1700461215225,
                "tmdate": 1700461215225,
                "mdate": 1700461215225,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]