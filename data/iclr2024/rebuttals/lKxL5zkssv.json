[
    {
        "title": "CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic"
    },
    {
        "review": {
            "id": "4fbHPXelTK",
            "forum": "lKxL5zkssv",
            "replyto": "lKxL5zkssv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6614/Reviewer_UuaC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6614/Reviewer_UuaC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a CLIP-guided Multi-sUbject visual neural information SEmantic Decoding (CLIP-MUSED) method which generalizes single-subject decoding models to multiple subjects in visual neural decoding tasks. Different from other multi-subject decoding methods, CLIP-MUSED uses a Transformer-based fMRI feature extractor to effectively extract global features of neural responses. CLIP-MUSED uses low-level and high-level tokens to encode individual differences and uses the topological relationship of visual stimuli in CLIP representation space based on RSA to guide the representation learning of tokens."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is easy to read, and generally well written.\n2. The use of RSA is novel and contributes to the topological relationships of visual stimuli in the CLIP representation space as prior knowledge to guide neural representation learning in the shared space.\n3. The experiments have clearly verified the benefits of the proposed methods."
                },
                "weaknesses": {
                    "value": "1. The use of letters in the article needs to be consistent. For example, the representation of X.\n2. In order to reduce the computational cost, using 3D-CNN to reduce the dimension will have an impact on the results? The results of this comparison are not shown.\n3. The high-level feature in RSMs uses the average of the image and text features from the last layer of CLIP. What are the advantages of using the CLIP image encoder or text encoder alone compared to directly using it alone? Can you provide corresponding results for an explanation?"
                },
                "questions": {
                    "value": "1. How to set labels for semantic classification of N subjects after concatenating low-level and high-level token representations of different image stimuli?\n2. In Section 2.1 \"X^{(n)}\" is recommended to be consistent with the previous notation. The rest of the article should also be consistent.\n3. The meaning of B in \"B visual stimuli\" in Section 2.2 should be explained.\n4. Is the underline in the AUC results of MS-SMODEL-ViT in Table 1 redundant? It should remain in the same format as other tables."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6614/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6614/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6614/Reviewer_UuaC"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6614/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740706827,
            "cdate": 1698740706827,
            "tmdate": 1699636754535,
            "mdate": 1699636754535,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9NI1MKmbY0",
                "forum": "lKxL5zkssv",
                "replyto": "4fbHPXelTK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6614/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6614/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response_1-Part I"
                    },
                    "comment": {
                        "value": "We are grateful for your constructive comments! Here are our responses:\n\n\n**Response to the mentioned weaknesses**:\n\n**W1: Letter consistency**: \n\nThe expressions $\\mathcal{X}^{(n)}$ and $\\mathbf{X}^{(n)}$ actually represent two different meanings. $\\mathcal{X}^{(n)}$ represents the neural response space of the n-th subject, while $\\mathbf{X}^{(n)}$  represents the samples in the dataset, which is a subset of the former. We also checked the other symbols.\n\n**W2: The impact of 3D-CNN**\uff1a\n\nIn previous studies, 3D-CNN has frequently been employed for extracting features from volumetric BOLD signals, including works in neural encoding/decoding [1] and medical image processing [2][3]. Even in recent studies that introduced Transformers [4][5], 3D-CNN is still utilized at the model's foundation for dimensionality reduction and feature extraction. These investigations emphasize not only the computational efficiency of CNN but also its capability to smooth out noise in BOLD signals.\n\nDuring the initial experiments of this study, we attempted to directly input volumetric BOLD signals, split into patches, into a Transformer. Specifically, we divided the signals into 6x7x6 patches, each containing $19^3$ voxels. Despite employing a 24-layer ViT with an embedding size of 1024, the loss function on the training set does not exhibit a convergent trend during training. This suggests that directly inputting responses in the voxel-space into a Transformer is less feasible, while preliminary feature extraction through 3D-CNN contributes to stable convergence and enhanced classification performance.\n\nWe refrained from extensive optimization on the network architecture of the 3D-CNN used in our model and instead drew inspiration from the study in [5] buiding a model composed of 3D-CNN and Transformer. In the experimental phase, we maintain consistency in the CNN architectures used by all baseline methods based on 3D-CNNs (except for method-specific modules), ensuring a fair comparison to validate the effectiveness of our approach.\n\n\n**W3: Results of single-modal feature guidance**\uff1a\n\nAlthough CLIP maximizes the similarity between image and text modal embeddings through contrastive learning, there still exist differences in the last-layer embeddings of images and text. By considering the embeddings from both modalities and averaging them, the model can obtain more information.\n\nWe also conduct related experiments. As you can see, Table E6 shows the results only using the last-layer features extracted by CLIP's image encoder to guide the learning of higher-level representations, which is not better than CLIP-MUSED guided by the multi-modal features.\nThe experimental results using only the last-layer features from CLIP's text encoder for guidance have also been added to Table E6 (See CLIP-Text). These results further highlight the advantages of using the RSM of the multimodal features as the guidance."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699980240779,
                "cdate": 1699980240779,
                "tmdate": 1699980240779,
                "mdate": 1699980240779,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xP16muu6jU",
                "forum": "lKxL5zkssv",
                "replyto": "4fbHPXelTK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6614/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6614/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe appreciate your valuable revisions and suggestions. In our response, we have taken the time to address your concerns and provide relevant clarifications. As the open discussion phase is drawing to a close, we are eagerly awaiting your feedback.\n\nThank you for your time and effort in revising our paper.\n\nBest regards,\n\nSubmission6614 Authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587161314,
                "cdate": 1700587161314,
                "tmdate": 1700587181396,
                "mdate": 1700587181396,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oWk46Yh2mv",
            "forum": "lKxL5zkssv",
            "replyto": "lKxL5zkssv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6614/Reviewer_qr5q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6614/Reviewer_qr5q"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a transformer based cross-subject fMRI alignment method that uses learned special tokens (an approach also used in other ViT/language works which perform classification). \n\nSpecifically, the paper discusses the following issues in traditional hyperalignment approaches:\n1. Mapping functions are restrictive (I assume this is in reference to orthogonal procrustes, linear mappings, or kernelized mappings)\n2. Requirements for a per-subject mapping function, which can be expensive when deep networks are used\n3. Need for identical stimuli or stimuli from the same semantic category\n\nThey propose the following approach:\n1. Using a transformer to model long-range dependencies\n2. Using special tokens to model subject-wise information\n3. CLIP distances to guide the model, with the average of the text & visual branches used for the high level feature."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "CLIP-MUSED is an interesting approach that shares model parameters and only varies a per-subject token. The use of CLIP for feature extraction and feature alignment is solid. The method allows for the use of multi-subject data without a linear increase in the number of model parameters, which is beneficial for scalability\n\nThe paper provides a detailed methods section that covers the technical aspects of the CLIP-MUSED, including the use of Transformers and CLIP and the design of an RSA based loss. The high level approach is clear.\n\nThe paper is well-organized, making it relatively easy to follow the arguments and understand the methodology."
                },
                "weaknesses": {
                    "value": "**In my view there are two weaknesses of the paper:**\n1. The relatively weak decoding the authors perform, which is implied to be just category decoding. It is very strange to me that they use so much compute relative to traditional methods, and end up just doing category decoding. I believe the results would be strengthened by performing visual decoding in the form of full image reconstruction, at least for NSD (single image passive viewing task). If this is not possible, I think an alternative would be perform image retrieval (show top-5) on a test set, where the test set contains images not used for the training of any subject. This would likely require training the decoder on a single subject's stimuli only, then testing on other subjects.\n2. The fact that the subject-wise token is not computed via amortized inference (via an encoder conditioned on a few BOLD responses). But instead, they are fixed per subject. In my view this weakens the approach somewhat as you cannot take this approach and apply it to a new subject in a few-shot fashion, after the paper talks about how their method does not require subjects to view the same stimulus.\n\n**Math typos:**\n1. In both the paper eq 10, the authors discuss using the F-norm (frobenius norm), but in practice they are using the squared frobenius norm in the code (line 42 of `losses.py`). This is fine, as the squared F-norm is everywhere differentiable, but I ask the authors clarify/fix this.\n2. In the paper eq 15, the authors apply a orthogonal constraint such that matrix $z_\\text{llv}$ and matrix $z_\\text{hlv}$ are orthogonal to each other via the regularization loss. Same issue listed above applies. You should probably also clarify here that you want the low/high level representations for each stimuli to be orthogonal, otherwise it is not super clear.\n\n**Minor typos:**\n1. Page 3. Original `The contributes are summarized as follows`, should be `The contributions are summarized as follows`\n2. Page 3. It seems that $X^{(n)} \\in \\mathbb{R}^{n_i \\times d} $ should be $\\mathcal{X}^{(n)} \\in \\mathbb{R}^{n_i \\times d} $\n\nOverall I think the proposed method is interesting, but the experiments currently do not fully demonstrate the strength of the method. I would gladly reconsider if the authors can clarify my questions, and provide additional experiments to support their method."
                },
                "questions": {
                    "value": "Questions:\n1. For Figure 1 in the paragraph below you mention `two bird images t1 and t2` as well as `t1 and t2 are birds, while t3 (duck) and t4 (building blocks) are not`.\n\n    **a**. I encourage the authors to clarify this, or use other examples in the figure and paper. Ducks should be birds.\n\n2. In the paper, it is discussed that `Linear transformation and MLP are unsuitable for high-dimensional voxel responses`. However no justification on the MLP aspect is given, and the citation (Yousefnezhad & Zhang) explicitly uses a kernelized MLP approach. Could you clarify?\n\n3. In section 3.2, you never define what a backbone network is. Does it just take as input the fMRI BOLD responses and output a category? In that case, is the ViT a 3D model?\n\n4. It is not clear what exactly you are decoding. You mention that HCP has a wordnet label, and NSD has the 80 categories (I assume the binary 0/1 mask on if an object from a category is present in the image).\n\n5. Can you clarify if you use ROI based patching for NSD, and the CNN based patching approach for HCP? Is the CNN jointly trained with the transformer?\n\n6. How do you extract visual features for HCP given that the stimulus set consists of video clips?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/a"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6614/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6614/Reviewer_qr5q",
                        "ICLR.cc/2024/Conference/Submission6614/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6614/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781819807,
            "cdate": 1698781819807,
            "tmdate": 1700601745954,
            "mdate": 1700601745954,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FGmzDSNb7m",
                "forum": "lKxL5zkssv",
                "replyto": "oWk46Yh2mv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6614/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6614/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response_1-Part I"
                    },
                    "comment": {
                        "value": "We really appreciate your insightful review! Here are our responses:\n\n**Response to the mentioned weaknesses**:\n\n**W1: Category decoding task**: \n\nThank you for your valuable suggestions. We are supplementing with retrieval experiments. The experimental results will be promptly released once the experiments are completed. Although reconstruction tasks also have significant research value, reconstruction involves deep generative models such as diffusion models, whereas our method is a discriminative model. Research on algorithms for reconstruction tasks is beyond the scope of this paper. We will consider conducting further research in the future.  \n\n**W2: The learning of subject-wise token**: \n\nThe amortized inference you mentioned is indeed a good idea, but our method also has advantages compared to the amortized inference. \n1. Amortized inference typically relies on assumptions about the prior distribution. If the prior assumptions are inappropriate or inaccurate, it may affect the effectiveness of inference and the quality of results. Our method does not rely on prior distribution assumptions.\n2. Our method eliminates the need for an encoding network, reducing the number of parameters that need to be learned and preventing overfitting. \n\nApplying to new subjects in a few-shot fashion still is a common challenge faced in the multi-subject neural decoding field. We have minimized the parameters that new subjects need to learn as much as possible.\n\n**W3: Math typos**\n\nIn the latest manuscript PDF, we have revised the original statement to \"the squared F-norm of the difference matrix normalized by the matrix size,\" and made the corresponding modification to Eq 10. Based on your suggestion, we have also modified the description of the orthogonal constraint, \"To encourage the low-level and high-level token representations for each stimuli to represent as different features as possible, the proposed method applies an orthogonal constraint by ...\".\n\n**W4: Minor typos**\n1. In the latest manuscript PDF, we have corrected \"Original The contributes are summarized as follows\" to \"The contributions are summarized as follows\" in Page 3. \n2. The expressions $\\mathcal{X}^{(n)}$ and $\\mathbf{X}^{(n)}$ actually represent two different meanings. $\\mathcal{X}^{(n)}$ represents the neural response space of the n-th subject, while $\\mathbf{X}^{(n)}$  represents the samples in the dataset, which is a subset of the former."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699980067691,
                "cdate": 1699980067691,
                "tmdate": 1699980067691,
                "mdate": 1699980067691,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BdSfUnC4a1",
                "forum": "lKxL5zkssv",
                "replyto": "GLQhHtFAQb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6614/Reviewer_qr5q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6614/Reviewer_qr5q"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "I'm writing to thank the authors for their clear and concise response.\n\nI have read the current revision, and I think broadly the clarifications are helpful.\n\nHere are a few minor typos introduced in the revision:\n* `Our method comprises a Transformer-based` -> `Our method is consists of a Transformer-based`\n* ` that facilitate the model to aggregate multi-subject data` -> `that facilitates the aggregation of multi-subject data`\n* `without linear increase` -> `without a linear increase`\n* `To encourage the low-level and high-level token representations for each stimuli to represent as different features as possible, the proposed method applies an orthogonal constraint` -> `To encourage low-level and high-level token representations for each stimulus to differ as much as possible, the proposed method applies an orthogonal constraint`\n\nI've decided to maintain the current score for now. I think the method that the authors propose is interesting, but I still believe the current \"decoding\" is relatively weak.\n\nIn recent fMRI work for decoding, those papers have demonstrated incredible results using linear decoders. Examples of these papers include :\n\n* \"Brain-Diffuser: Natural scene reconstruction from fMRI signals using generative latent diffusion\" -- which utilizes (linear) ridge regression models and demonstrate state-of-the-art performance in image decoding,\n* \"High-resolution image reconstruction with latent diffusion models from human brain activity\" -- which also utilizes linear regression models\n\nThe authors propose a transformer based method, while it is interesting, I'm unsure if their category decoding experiments are super convincing."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245143815,
                "cdate": 1700245143815,
                "tmdate": 1700245143815,
                "mdate": 1700245143815,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HnRJ4I7c1E",
                "forum": "lKxL5zkssv",
                "replyto": "sZzSfJ1dkh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6614/Reviewer_qr5q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6614/Reviewer_qr5q"
                ],
                "content": {
                    "title": {
                        "value": "Score increase"
                    },
                    "comment": {
                        "value": "I've increased the score to a 6. \n\nI still believe the results are very weak in context of the compute used. But the approach is broadly interesting."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601724185,
                "cdate": 1700601724185,
                "tmdate": 1700601724185,
                "mdate": 1700601724185,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FzHtpfrmD1",
            "forum": "lKxL5zkssv",
            "replyto": "lKxL5zkssv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6614/Reviewer_mWYc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6614/Reviewer_mWYc"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of alignment of the response to visual stimuli across multiple subjects. This is an important problem for visual neural decoding tasks. In general, models for visual neural decoding tasks are either trained on a single subject (leading to problems such as overfitting) or must correct for differences across anatomical structure and functional topography of the brain in different subjects. The paper proposes a new method for multi-subject functional alignment that goes beyond the SOTA, namely hyperalignment and category-based methods. This new method uses transformer-based models to capture long-range dependencies that exist in the functional connectivity between brain regions. Intersubject differences are modelled through extra subject-specific tokens and the CLIP representation space is used to achieve a high consistency between cortical representations of visual stimuli."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper addresses an important problem in neurosciences and the proposed multi-subject alignment method is novel and interesting. Both the use of transformers to model long-range associations in brain regions and the use of the CLIP representation space to guide neural representation learning in a shared representation space is a nice idea! The experimental results on the two fMRI datasets are somewhat limited (esp. due to the small size) but demonstrate a proof-of-concept nicely."
                },
                "weaknesses": {
                    "value": "I found that the abstract is not as clearly written as is the introduction. From the abstract, it is unclear what the paper sets out to achieve nor how this is done. The introduction does much better job at this. Perhaps the authors could try to formulate the problem addressed and the contributions more clearly (I appreciate that this is more difficult to do in the space available). The evaluation is rather limited, especially since the size of the datasets is quite small."
                },
                "questions": {
                    "value": "- Why do extract low-level and high-level feature RSM seperately?\n\n- The HCP dataset contains more than 158 subjects. Why did you not use all subjects? How were the 158 subjects selected from the HCP? Randomly? Why is this further reduced to 9? What is the bottleneck here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6614/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831743759,
            "cdate": 1698831743759,
            "tmdate": 1699636754273,
            "mdate": 1699636754273,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uzDlhup1gH",
                "forum": "lKxL5zkssv",
                "replyto": "FzHtpfrmD1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6614/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6614/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the positive feedback! Here are our responses:\n\n**Response to the mentioned weaknesses**:\n\n**W1: Abstract modification**: Thank you to the reviewer for the modification suggestions on the abstract. We have made revisions to the abstract in the updated manuscript PDF to better highlight the problem addressed and our contributions.\n\n**W2: The evaluation is rather limited, especially since the size of the datasets is quite small**:\nAs mentioned in the Introduction, fMRI data collection is costly, resulting in relatively small sizes for publicly available datasets. Among the available visual fMRI datasets, the number of stimuli presented to each subject typically ranges from several hundred to a few thousand and the number of subjects always is no more than five. For example, Vim-1 [1] has 1750 stimuli and two subjects; the dataset in [2] has 6000 stimuli but only three subjects. Compared to other available datasets, NSD and HCP datasets are the largest datasets because they include much more stimuli and subjects. Based on them, we trained and evaluated our method and the comparative methods in the paper. Notably, our evaluation is not limited to classification metrics alone; we also demonstrate the interpretability of the algorithm through visualizing attention maps (See Fig. 4 and Fig. F8). In conclusion, our evaluation is not limited but rather comprehensive and extensive.\n\n\n**Q1. Why do extract low-level and high-level feature RSM seperately?**\n\n**Response**: \n\nThe task of object classification requires encoding both low-level and high-level features of stimuli, and there are differences in how subjects encode these two types of features. Therefore, we introduce low-level and high-level tokens to account for these differences. To encourage these tokens to encode low-level and high-level features, we utilize the guidance of low-level and high-level features from CLIP, necessitating two feature RSMs. Furthermore, extracting low-level and high-level feature RSM seperately has two advantages:\n\n1. The topological structures of stimuli in the low-level feature (e.g., shape, color) space and high-level feature (e.g., semantics) space exhibit differences. By encoding different topological structures in the two RSMs and incorporating the orthogonal constraint of tokens, it facilitates the tokens to learn better representations.\n2. By visualizing attention maps, we can observe the correspondence between primary brain regions and low-level tokens, as well as between high-level brain regions and high-level tokens, thereby ensuring the interpretability of the algorithm.\n\n**Q2. The HCP dataset contains more than 158 subjects. Why did you not use all subjects? How were the 158 subjects selected from the HCP? Randomly? Why is this further reduced to 9? What is the bottleneck here?**\n\n**Response**:\n    \nFirstly, we would like to clarify that our proposed algorithm itself is not a bottleneck. Our method can be easily scaled to more subjects because when training on one more subject, the model only needs to learn two additional N-dimensional vectors, where N equals 512. We selected the same ten subjects as the literature [3] that conducts experiments with randomly-selected ten subjects. However, the data format for one of the subjects has an issue. As a result, we proceeded with the experiment using the remaining nine subjects. \n\nWhile the algorithm itself is not a bottleneck, increasing the number of subjects and training data, on the other hand, may extend the training time and require additional resources. \nGiven that the stimuli presented to different subjects in the HCP dataset are the same, the data diversity is more limited compared to that of NSD. When the number of subjects reaches a certain threshold, it is akin to training the model with more iterations, as the model repeatedly processes similar responses from different subjects under the same stimuli. Therefore, training with all 158 subjects simultaneously may not significantly improve the decoding performance for individual subjects but will demand more training resources.\nAs a result, we have currently conducted experiments on nine subjects. \n\nIn order to illustrate the broader applicability of our method to a larger number of subjects, we are presently conducting multi-subject decoding experiments with a cohort of 30 subjects. We will expediently provide updates on the experimental outcomes.\n\n**References**\n\n[1] Kay et al. (2008) Identifying natural images from human brain activity. Nature.\n\n[2] Shen et al. (2019) Deep image reconstruction from human brain activity. PLOS Computational Biology.\n\n[3] Khosla et., al.(2020) A shared neural encoding model for the prediction of subject-speci\ufb01c fMRI response. MICCAI."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699979793960,
                "cdate": 1699979793960,
                "tmdate": 1699979844924,
                "mdate": 1699979844924,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "syimxGdsaB",
                "forum": "lKxL5zkssv",
                "replyto": "FzHtpfrmD1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6614/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6614/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you for your comments and questions. In our response, we have made efforts to address your questions and provided relevant clarifications and supplementary materials. Considering that the open discussion phase is coming to an end, we would appreciate your feedback.\n\nThank you for your time and effort, and we eagerly await your response.\n\nBest regards,\n\nSubmission6614 Authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587030592,
                "cdate": 1700587030592,
                "tmdate": 1700587204616,
                "mdate": 1700587204616,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]