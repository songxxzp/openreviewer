[
    {
        "title": "The Distributional Reward Critic Architecture for Reinforcement Learning Under Confusion Matrix Reward Perturbations"
    },
    {
        "review": {
            "id": "AUpZGIbDo3",
            "forum": "BSqVfAFJWz",
            "replyto": "BSqVfAFJWz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8244/Reviewer_Jfuy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8244/Reviewer_Jfuy"
            ],
            "content": {
                "summary": {
                    "value": "The authors study RL algorithms under corrupted rewards. By considering the reward distribution and using the discretizing techniques, the confusion matrix can be better estimated by incorporating a reward critic. They also extend their methods to recover the noisy rewards from the known interval and partitions to the unknown one to cater to practical scenarios. Experiments are conducted on a few classical control and Mujoco environments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* Considering the reward distribution in the corrupted reward setting is natural and well-motivated.\n\n* The paper also studies the unknown interval and partition settings, which is more practical."
                },
                "weaknesses": {
                    "value": "* **Potential concept misuse**. The reward is a naturally random variable and considering its distribution in the corrected reward setting is reasonable. However, it does not mean it is strongly correlated with distributional RL, which focuses on the return distribution instead of its expectation: value function.  I agree some density estimation techniques, such as categorical parameterization from C51 paper, can also be used here from distributional RL, but most of the technical issues, including the theoretical parts, between these two scenarios are very different. Emphasizing too much about the so-called distributional reward seems not natural.\n\n* **Inaccurate theoretical statements**. Proposition 1 seems incorrect. Assume r equals $r_{min}$ and it is corrupted to be close to $r_{max}$, which exceeds the upper bound. Also, the proof in Appendix B.1 seems not complete, which I cannot directly follow. Also, Theorem 1 is a descriptive statement without rigorous mathematical statements. Thus, the results are not convincing in a rigorous way. \n\n* **Limited contribution.** The confusion matrix is also well-studied in robust generative models, based on my knowledge, which can be naturally updated by a (critic) network. Thus, this contribution seems not sufficient. For avenues to cope with the continuous rewards, the discretization method this paper used is a direct application of C51 algorithm via the categorical parameterization. However, it seems the authors did not acknowledge that clearly. In distributional RL, C51 is typically inferior to quantile-based algorithms mainly due to its inability to handle unknown reward range and partitions. As far as I can tell, the paper mainly focuses on the known cases, with only heuristics strategies in the unknown settings. Hence, I can not recognize the sufficient contribution behind it and doubt its practical power.\n\n* **Technical questions.** I am very confused by selecting the most probable reward label in step 7 of algorithm 1 instead of doing a reweighted sampling from the distribution. Note that if we only choose a single one, the optimal label tends to be deterministic rather than truly learning reward distributions. In the current version, along with the computation of the predicted reward value step, I think it is very similar to just doing the regression instead of a real multi-class classification since the $\\arg\\max$ does not consider the other probabilities when the reward belongs to other bins. \n* **Empirical significance** Although the authors claim the empirical performance is desirable, I am afraid I cannot truly agree with that as I find most learning curves are not significantly better than other baselines instead of only making conclusions based on the final return. I may also have a question: although the authors claim that the proposed method can recover the reward better, is this benefit directly related to better performance? More importantly, Atari games are suggested to test the value-based RL algorithms. \n\n* **Computation issues.** Learning a confusion matrix mapping is OK in generative models, but learning a reward critic (similarly viewed as a confusion matrix mapping) can be costly in RL as RL algorithms typically lack sample efficiency. Based on this knowledge, determining $n_o$ via the training loss is normally intractable, especially in a large environment with huge state and action spaces. Last but not least, using the training loss is a typical cross-validation strategy, which is straightforward and even trivial for me."
                },
                "questions": {
                    "value": "Please refer to the weakness part.\n\n### Minors.\n* For continuous perturbations, the discretization technique should always induce a reconstruction error, which seems to have conflicts with some statements in the paper.\n* The writing can also be improved as there are many grammar errors and typos."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8244/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8244/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8244/Reviewer_Jfuy"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8244/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698368352010,
            "cdate": 1698368352010,
            "tmdate": 1699637024971,
            "mdate": 1699637024971,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OVGIeNw8WR",
                "forum": "BSqVfAFJWz",
                "replyto": "AUpZGIbDo3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8244/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8244/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(1/4) Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their review. Nevertheless, we believe they have misunderstood critical elements of the paper that are stated clearly.\n\n>Q1: Concept misuse of distributional RL\n\nA1: We do not think there is a concept misuse. As the reviewer says, distributional RL focuses on the return distribution. In our methods, our distributional reward critic focuses on reward distribution. \"Reward Critic\" is to predict the reward instead of value functions. \"Distributional\" means we apply the idea of distributional RL to predict the distribution instead of a single value as RE does. We would be happy to provide clarification if we have misunderstood the comment.\n\n>Q2: Proposition 1 seems incorrect. Assume r equals $r_{min}$ and it is corrupted to be close to $r_{max}$, which exceeds the upper bound.\n\nA2: Proposition is correct. Proposition 1 is to prove the coverage of the GCM setting by analyzing the maximum error of discretizing \\bar_{r} into \\tilde_{r}, where there is no discussion of the perturbation definitions. Please go to Section 3.2 for the definition of GCM. The field of the perturbed reward \\tilde_{r} is $[r_{min}, r_{max}]$. We kindly request the reviewer to clarify what incorrect is with Proposition 1. \n\n>Q3: The proof in Appendix B.1 seems not complete.\n\nA3: The proof in Appendix B.1 is complete. The reviewer might not notice the first paragraph on Page 15.\n\n>Q4: Theorem 1 is a descriptive statement without rigorous mathematical statements.\n\nA4: The assertion that \u201cthe prediction from the distributional reward critic (DRC) is the distribution of the perturbed reward label\u201d is a precise mathematical statement. This problem differs from conventional classification problems, where a given input (e.g., an image) has a fixed label. In our perturbed environment, a given state-action pair might have varying labels. It is difficult to prove the minimum of the loss function for all the samples directly so we wisely pick the samples of a state-action pair and prove the minimum of the loss function for these samples in its proof. Our goal is to prove what the output of the trained DRC using Cross-Entropy (CE) loss would be in our case.\n\n>Q5: The confusion matrix is also well-studied in robust generative models, which can be naturally updated by a (critic) network. Thus, this contribution seems not sufficient.\n\nA5: The general confusion matrix (GCM) perturbation is a whole concept. \u201cConfusion Matrix\u201d cannot be understood solely as its meaning has changed from its original one. The confusion matrix in GCM perturbations is to perturb the reward signals instead of being used to evaluate the performance. The confusion matrix perturbation is first used by Wang et al. to name their perturbation. We also have a detailed definition of GCM perturbations in Section 3.2.\nBesides, our distributional reward critic is not to update the confusion matrix but to predict the distribution of the perturbed reward labels of a given state-action pair. And the introduction of GCM perturbations is not our main contribution. Our main contribution is we propose a method (GDRC) that performs well in different settings without any information about the perturbation settings.\nWe want the reviewer to clarify the relation between our work and the study of the confusion matrix in robust generative models. If we have misunderstood the comment, we would be happy to provide clarification if the review could elaborate further on it."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8244/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700194909014,
                "cdate": 1700194909014,
                "tmdate": 1700194909014,
                "mdate": 1700194909014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CI9YGa6NGq",
                "forum": "BSqVfAFJWz",
                "replyto": "AUpZGIbDo3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8244/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8244/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(3/4) Rebuttal by Authors"
                    },
                    "comment": {
                        "value": ">Q9: Most learning curves are not significantly better than other baselines instead of only making conclusions based on the final return.\n\nA9: First, we do not present any learning curves in the paper. Each point in Figures 5, 6, and 7 represents the average performance of 10 seeds running for 20 episodes each after training. We also provide learning curves in Figures 14-19 in Appendix H, where every line is the average of 10 seeds.\n\nSecond, we do not agree that the outperformance is mostly insignificant. It is understandable the outperformance becomes more and more significant as we increase the noise ratio because any method handling perturbations inevitably suffers from performance loss. Besides, GDRC wins/ties the baseline methods in 40/57 (compared to 16/57 for the best baseline) of all kinds of perturbation settings (including small and large noise ratios) in Figures 5 and 6 as we present in the introduction, demonstrating the general outperformance of our methods. Even when considering the statistically significant win rate of GDRC (compared with PPO and RE) in Table 4, GDRC wins 29/48 cases with significance with the best baseline wins 10/48 cases with significance.\n\nThird, we unwrap the results more for the reviewer. In Figure 5, we win a small edge or achieve comparable performance as baseline methods when the noise ratio is small and the outperformance becomes larger as the noise ratio increases. In Figure 6, we can tell our methods almost always perform better than the baseline methods in Hopper and Reacher. As the noise ratio becomes larger, the outperformance becomes more significant as well. For Walker2d, the outperformance might not be that large although we almost always win over other baseline methods as well. This is because the benchmark performance is 1286.6 in Walker2d as we discuss in the second-last paragraph of Page 8, restricting the top performance we can achieve. What\u2019s more, there are two factors rising from the original confusion matrix perturbations contributing to the not-too-bad performance of the baseline methods, which will be discussed in the next paragraph. For HalfCheetah, we win only when the noise ratio is large. We have a detailed analysis in the last paragraph of Page 8 and Figure 10, which is interesting and opens a door for further improvement. To summarize, we mostly win over the baseline methods and the outperformance gets more significant as the settings get perturbed more.\n\nFourth, as we mention in the last paragraph, the two characteristics of the original confusion matrix perturbations lead to the relative not-too-bad performance of the baseline methods: the perturbed reward range remains unchanged as the one before perturbations, and the noise ratio is randomly assigned without intentional manipulations. The two factors together restrict the variance brought by the perturbation and cause less influence on the optimal policy. In reality, they can happen with high probability and the performance of the baseline methods will get destroyed severely, but our methods (DRC and GDRC) will not. To tell it in another way, our GCM perturbation settings can be more generalizable. We provide the results of the case the range of the perturbed rewards becomes twice as the clean ones, from $[r_{min}, r_{max}]$ to $[2r_{min}, 2r_{max}]$ where $r_{min}<0$ and $r_{max}>0$, in Figure 13 in Appendix G. GDRC wins/ties the best performance in 36/48 cases with larger outperformance. As for intentionally generating difficult GCM perturbations, we need to do statistics on the distribution of clean rewards, which we leave for future work.\n\n>Q10: Is this benefit directly related to better performance?\n\nA10: Yes, the only difference between our method and others lies in the use of different reward values for fitting the value function and improving the policy."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8244/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700195335156,
                "cdate": 1700195335156,
                "tmdate": 1700201723928,
                "mdate": 1700201723928,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "J9acUhEZBc",
            "forum": "BSqVfAFJWz",
            "replyto": "BSqVfAFJWz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8244/Reviewer_wJ8T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8244/Reviewer_wJ8T"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies reinforcement learning with perturbed reward, where the reward perceived by the agent is injected with noise. Inspired by previous work, this paper proposes a new form of reward perturbation based on a confusion matrix, which shuffles the rewards of discretized intervals and can preserve local continuity within each interval. Under the mode-preserving assumption, which guarantees the true reward will be observed the most frequently, the paper proposes to learn a classifier to identify the interval index of the true reward when the discretization is known. When the number of intervals or even the reward range is unknown, the paper proposes to use an ensemble or a streaming technique to address the respective unknown parameter. Experimental results on four continuous control tasks show the effectiveness of the proposed techniques."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strength of the paper is that it provides a comprehensive study of the proposed problem. As stated in the above summary, to address the new form of confusion matrix perturbation, the paper proposes a new algorithm as well as extends the new algorithm to handle scenarios in which some/all parts of the discretization are unknown. In addition, it also provides the guarantee that the correct reward can be predicted asymptotically and the theoretical justification for the voting mechanism of the ensemble approach when the number of intervals is unknown. \n\nIn terms of presentation, the paper is well-written and organized."
                },
                "weaknesses": {
                    "value": "In my opinion, the major potential weakness of the paper is that it has a restrictive type of perturbation. The paper assumes the rewards in different intervals are shuffled and the true reward is still the most frequently observed (mode-preserving property). However, I find this assumption too artificial and not natural. On the other hand, while the paper claims the assumption of RE (Romoff et al. 2018) to be stringent, I found it to be more natural. I understand the argument that RE can\u2019t properly handle non-order-preserving perturbation. Can you provide more justification for this assumption? Also, how robust would the method be if GCM is not mode-preserving?"
                },
                "questions": {
                    "value": "See Weaknesses for major questions.\n\nClarification questions or typos that do not impact assessment:\nWhat is SR_W? How is it different from SR?\nThe reference style is a bit messy and inconsistent throughout the paper.\nOn page 3, by convention, $\\beta : \\Delta(S)$ should be $\\beta \\in \\Delta(S)$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8244/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698633584711,
            "cdate": 1698633584711,
            "tmdate": 1699637024866,
            "mdate": 1699637024866,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Yzuj5UePRz",
                "forum": "BSqVfAFJWz",
                "replyto": "J9acUhEZBc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8244/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8244/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable review. Here are our answers to the questions:\n\n>Q1: The mode-preserving assumption is too artificial and not natural. The assumption of RE is more natural.\n\nA2: Mode-preserving is the assumption of the surrogate reward (SR) method instead of being created by us as shown in Table 1. When recovering the confusion matrix, they use majority voting to decide the true reward of the samples for a state-action pair assuming mode-preserving. Besides, the forms of $\\tilde{R}$ of the continuous perturbation functions (e.g. Gaussian perturbations) people pay attention to in RE necessitate mode-preserving although the space is continuous, which shows mode-preserving is a weaker assumption in the cases people are interested in. Following this idea, we propose a method that combines the advantages of both SR and RE under the assumption of mode-preserving.\n\nOne thing to notice is that we do not guarantee mode-preserving in the generation of GCM we experiment with. Nevertheless, our methods perform effectively even under high noise ratios (e.g., $\\omega =0.7$), demonstrating that our assumption of mode-preserving is weak and our methods are robust. Furthermore, our method outperforms baseline methods even under the perturbation (continuous perturbation) tailored for RE shown in Section 5.4, indicating our method's broad applicability and mode-preserving is a weak assumption again.\n\nIn a word, with a weak mode-preserving assumption, our methods are more broadly applicable under many kinds of perturbations. We clarify the motivation of mode-preserving in the last paragraph of Section 3 of the revised paper in blue.\n\n>Q2: How robust would the method be if GCM is not mode-preserving?\n\nA2: Any method dealing with perturbations needs an assumption to work. If mode-preserving, the only assumption of our methods, does not hold, there is no way for a method to work theoretically because we have no assumption about the perturbation for our reference to recover the clean rewards.\n\n>Q3: Clarification questions: How is SR_W different from SR? The reference style is a bit messy and inconsistent throughout the paper. On page 3, by convention, $\\beta:\\Delta(S)$ should be $\\beta\\in\\Delta(S)$.\n\nA3: SR can learn the confusion matrix within a simple environment space (e.g. Pendulum), but SR_W needs to know the whole confusion matrix before being applied, which is the last sentence of the first paragraph of Page 3. We uniform the use of the reference style and modify the notation as suggested."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8244/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700193352304,
                "cdate": 1700193352304,
                "tmdate": 1700193352304,
                "mdate": 1700193352304,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EhAhFn0gmy",
                "forum": "BSqVfAFJWz",
                "replyto": "Yzuj5UePRz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8244/Reviewer_wJ8T"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8244/Reviewer_wJ8T"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your answers and clarifications. Your answers to the questions have helped me understand your argument better. While Gaussian noise is mode-preserving, the order-preserving perturbation, in general, is not necessarily mode-preserving (for example, when there is a non-zero shift in the reward). Thus, I don\u2019t perceive mode-preserving as a weak assumption compared to the order-preserving assumption. They are not directly comparable. However, I do agree that mode-preserving is a useful assumption. \n\nAs for the example with a high noise ratio ($\\omega > 0.7$), the true reward will still be the most presented with a high probability. In other words, the example will likely be mode-preserving. Thus, I don\u2019t consider this as support for the argument that the proposed methods are robust and work even when the assumption does not hold. Including results with total perturbed environments ($\\omega = 1$) may help to provide the support.\n\nAfter reading other reviews, I noticed the concern Reviewer XsR2 raised that the method only works for MDPs with deterministic rewards, while it may suffer from unavoidable bias when the rewards are stochastic. I think the paper should be upfront about the potential bias introduced by ignoring the stochasticity of the rewards. I think incorporating the adaptation of the proposed methods to the stochastic rewards case would make the paper much stronger.\n\nOverall, I retain my positive assessment of the paper for now."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8244/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700628491948,
                "cdate": 1700628491948,
                "tmdate": 1700628491948,
                "mdate": 1700628491948,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UBejHT76Ew",
                "forum": "BSqVfAFJWz",
                "replyto": "J9acUhEZBc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8244/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8244/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the valuable reply from the reviewer. We just want to add some potentially useful information for the reviewer\u2019s further reference.\n\n\n>Point 1: While Gaussian noise is mode-preserving, the order-preserving perturbation, in general, is not necessarily mode-preserving (for example, when there is a non-zero shift in the reward).\n\nReply 1: That is a very good point. Indeed, our assumption can be even weaker as the mode of the perturbed rewards follows a positive affine transformation with some conditions. In comparison, RE follows that the expectation of the perturbed rewards follows the same rules. RE also cannot correct the systematic bias, but it works whenever the bias does not influence the optimal policy. To make the story simpler, we do not talk too many details.\n\n\n>Point 2: Thus, I don\u2019t perceive mode-preserving as a weak assumption compared to the order-preserving assumption. They are not directly comparable. However, I do agree that mode-preserving is a useful assumption.\n\nReply 2: We thank the reviewer for recognizing the usefulness of the mode-preserving assumption. Actually, all the baseline methods do not explicitly state their applicable scenarios and needed assumptions. Instead, we analyze them thoroughly and present the assumption of our methods clearly. We agree they might not be directly comparable, but the outperformance of our methods under two kinds of perturbations might support the broader application of our methods.\n\n\n>Point 3: The example ($\\omega=0.7$) will likely be mode-preserving. Thus, I don\u2019t consider this as support for the argument that the proposed methods are robust and work even when the assumption does not hold. Including results with total perturbed environments ($\\omega=1$) may help to provide the support.\n\nReply 3: We do not propose our methods work even when the assumption does not hold. We want to convey that our methods even work under large perturbations even without guaranteeing mode-preserving, showing mode-preserving is not a stringent assumption and our methods are robust. As we answered in A2 above, our methods do not work when mode-preserving does not hold, which is the same case for the other methods if their assumptions do not hold.\n\n\n>Point 4: The method only works for MDPs with deterministic rewards, while it may suffer from unavoidable bias when the rewards are stochastic. I think the paper should be upfront about the potential bias introduced by ignoring the stochasticity of the rewards. I think incorporating the adaptation of the proposed methods to the stochastic rewards case would make the paper much stronger.\n\nReply 4: Incorporating the adaptation of the proposed methods to the stochastic rewards case is a very good point. We want to clarify the motivation of the methods of handling perturbations. In MDPs with deterministic rewards, which is always the case for SR, RE, and ours, the methods of handling perturbations aim to correct the stochastics of the perturbed rewards because of perturbations (e.g., human errors, sensor noise, etc.). The MDPs with random rewards are not the targeted MDPs of our three works, meaning the stochastics inside the random rewards are not what we want to correct. As for the performance of different methods, it depends on their assumptions and the method designs. In the defined MDPs, which are also what people usually define, of the three methods, ours is the best not only from the theoretical perspective but from the experimental one.\n\nHowever, we agree to discuss the adaptation of our methods in another kind of MDP. It might be hard to combine them together directly in this work as they are under two kinds of MDPs, but we promise to have a separate part to discuss the adaptation of our methods in MDPs with random rewards.\n\n\nAgain, we really appreciate the insightful thoughts from the reviewer."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8244/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633661784,
                "cdate": 1700633661784,
                "tmdate": 1700633697844,
                "mdate": 1700633697844,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "G3KC7kQ7YZ",
            "forum": "BSqVfAFJWz",
            "replyto": "BSqVfAFJWz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8244/Reviewer_XsR2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8244/Reviewer_XsR2"
            ],
            "content": {
                "summary": {
                    "value": "The authors consider the problem of training a reinforcement learning agent under a perturbed reward signal. Similar to Romoff et al. (2018), they propose to learn a reward estimator and then train the agent from the predicted rewards rather than the environment rewards. However, unlike Romoff et al., who strive to learn the expected reward, the authors propose to learn the reward distribution (assuming a categorical distribution), and then train the RL agent from the mode of the predicted distribution. This purportedly allows the approach to handle a broader range of reward perturbations, including those that are not order-preserving."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of predicting the reward via a categorical distribution is novel, and it's an interesting adaptation of ideas from distributional RL. One thing I like about the approach is that, unlike the C51 agent (Bellemare et al., 2017), the number of intervals under GDRC is adaptive. The analysis in Section 5.2 supporting this adaptive approach is very nice. In fact, on the whole, all of the extra analysis that is done beyond the headline results is quite helpful. The authors are upfront about some of the shortcomings of the approach, e.g., the mode collapse problem described at the end of page 8 (where they again include extra, helpful analysis in the Appendix). While the content of the paper is fairly technical, it's mostly well-presented and I think I understood most of the main details. Lastly, I agree that this is an important research area, especially given that finetuning LLMs via RL with human feedback has become such a hot topic recently."
                },
                "weaknesses": {
                    "value": "One desirable property of any approach to this problem is that it should never decrease performance on tasks where the rewards are *unperturbed*. However, this doesn't hold for the proposed approach. For example, consider a simple betting game where the reward of some bet is -1 with probability 0.9 and +10 with probability 0.1. Romoff et al.'s approach will (in theory) learn that the expected reward is 0.1, and thus the agent will learn that the bet is worth taking. On the other hand, the proposed approach will learn that the mode reward is -1, and hence the agent will learn to avoid the bet. Generalising from this, if we want the approach to be \"safe\" then it ought to preserve the expected reward.\n\nIf we somehow knew that the clean rewards were deterministic and the perturbation was mode-preserving then this would no longer be an issue, but this is a big assumption and it's hard to see how we'd ever know this in a realistic application. For example, human feedback for LLMs arguably is stochastic, not just \"perturbed\", since different humans have different preferences.  \n\nLess pressing issues:\n\n- The difference between the proposed approach and Romoff et al.'s can be broken into two parts: (1) Learning a categorical estimate, rather than a scalar one; (2) The agent is trained from the mode reward, rather than the mean. I'd like to see a comparison versus an approach that uses (1) but not (2), i.e., the reward distribution is learned, but the agent is trained from the mean of the distribution. This would disengangle how much of the performance impact comes from assuming a categorical distribution and how much comes from assuming that the perturbation is mode-preserving.\n- Page 8 states: \"The agents in Hopper and Walker2d are encouraged to live longer because of the positive expectation of perturbed rewards\". However, this contradicts the statement on page 2 that affine transformations preserve optimal policies. The statement on page 2 should be refined to say that this only holds for non-episodic tasks.\n- In my opinion, the proof of Theorem 1 is unnecessary and just overcomplicates the paper. All it really says is \"under perfect conditions, the cross entropy loss will go to zero, and hence the approach learns what it's supposed to\". We'd have AGI already if he had \"sufficiently expressive\" neutral networks + unlimited training samples + the necessary compute :p"
                },
                "questions": {
                    "value": "I was slightly confused by the statement on page 3: \"we assume that the distribution of the perturbed reward depends only on the true reward, i.e., $\\tilde{r}_t \\sim \\tilde{R}(R(s_t, a_t))$\". Couldn't the proposed approach handle state- and action-dependent perturbations too?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8244/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8244/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8244/Reviewer_XsR2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8244/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792164251,
            "cdate": 1698792164251,
            "tmdate": 1699637024754,
            "mdate": 1699637024754,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZfkQt64u2t",
                "forum": "BSqVfAFJWz",
                "replyto": "G3KC7kQ7YZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8244/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8244/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(1/2) Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable review. Here are our answers to the questions:\n\n>Q1: One desirable property of any approach to this problem is that it should not decrease performance on tasks where the rewards are unperturbed. Romoff et al.'s approach works for the betting game, but the proposed approach does not.\n\nA1: It is overly stringent to expect a method designed for dealing with perturbations to not suffer from any performance loss in a clean environment. If there is always no degradation in performance for handling perturbations, there is no need for the original method.\n\nIn the definition of MDP in the first paragraph of Section 3, the reward $r_t$ is from states and actions $(s_t, a_t)$ expressed as $r_t = R(s_t, a_t)$, but the reward $r_t$ is also allowed to depend on the transition, meaning $r_t = R(s_t, a_t, s_t^\\prime)$. To make the betting game MDP, we need to include $s_t^\\prime$ as the input to the reward function to tell whether we win or lose if we choose to play the betting machine. Otherwise, it even does not accord with the definition of MDP as the same state-action pairs can generate different rewards. To handle the betting game, there is only a small adaption needed \u2013 input $s_t^\\prime$ to the distributional reward critic other than $(s_t, a_t)$. Therefore, our methods also work in the betting game.\n\n\n>Q2: If we knew that the clean rewards were deterministic and the perturbation was mode-preserving, it would no longer be an issue, but mode-preserving is a significant assumption, and it's hard to see how we'd ever know this in a realistic application. For example, human feedback for LLMs is arguably stochastic and not just \"perturbed,\" as different humans have different preferences.\n\nA2: First, deterministic clean rewards are not needed for our method. In the environments we experiment with, without considering any randomness in the environments, the rewards are indeed fixed given state-action. However, the clean rewards are not static because of the randomness.\n\nSecond, there is still a big issue even if mode-preserving holds. Perturbations can introduce variance, slowing the process of value function fitting, which is what RE addresses. GCM can induce bias breaking the preferences of the taken actions (order-breaking bias), directly affecting policy performance by altering the optimal policy measured by the perturbed rewards.\n\nThird, any work dealing with perturbation invariably requires assumptions of perturbations, whether mode-preserving or order-preserving. Mode-preserving is not created by us, which is the assumption of SR as shown in Table 1. When recovering the confusion matrix, they use majority voting to decide the true reward of the samples for a state-action chunk while assuming mode-preserving. Besides, the forms of $\\tilde{R}$ of the continuous perturbation functions (e.g. Gaussian perturbations) people pay attention to in RE necessitate mode-preserving although the space is continuous, which shows mode-preserving is a weaker assumption in the cases people are interested in. Also as we discuss in the second paragraph of A2, our methods can deal with non-order-preserving perturbations with mode-preserving assumption but RE cannot with a positive affine transformation. One thing to notice is that we do not guarantee mode-preserving in the generation of GCM we experiment with. Nevertheless, our methods perform effectively even under high noise ratios (e.g., $\\omega =0.7$), demonstrating that our assumption of mode-preserving is weak and our methods are robust. Furthermore, our method outperforms baseline methods even under the perturbation (continuous perturbation) tailored for RE shown in Section 5.4, indicating our method's broad applicability and mode-preserving is a weak assumption again.  We clarify the importance of mode-preserving in the last paragraph of Section 3 of the revised paper in blue.\n\nFourth, regarding RLHF, we still need to have some pre-knowledge of the distribution of perturbed labels, and then apply appropriate methods to handle perturbations. In RLHF, the samples collected from humans are usually with discrete labels (e.g. integers from 1 to 5), which can be regarded as problems dealing with noisy labels. Mode-preserving is the most used assumption for the methods to handle noisy labels except for the case you have other assumptions of the label distributions. Even if the assumption/pre-knowledge is about the distribution instead of the mode, our methods can still be adapted correspondingly as they can predict the perturbed labels, but RE cannot because the information on the distribution is not kept. Within the discrete space of RLHF, the outperformance of our methods is more significant compared with the baseline methods.\n\nIn a word, we want to clarify an assumption is needed for any work dealing with perturbation and mode-preserving is a weak one, which explains our methods work under many kinds of perturbations."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8244/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700192440388,
                "cdate": 1700192440388,
                "tmdate": 1700192440388,
                "mdate": 1700192440388,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gCXAEEOCkI",
                "forum": "BSqVfAFJWz",
                "replyto": "G3KC7kQ7YZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8244/Reviewer_XsR2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8244/Reviewer_XsR2"
                ],
                "content": {
                    "title": {
                        "value": "Stochastic rewards"
                    },
                    "comment": {
                        "value": "Thanks very much for your reply!\n\n> Otherwise, it even does not accord with the definition of MDP as the same state-action pairs can generate different rewards.\n\nCorrect me if I'm wrong, but isn't the perturbed reward $\\tilde{R}(s, a)$ stochastic? I thought that the main assumption in the problem setting was that the same state-action pairs *could* generate different rewards. Is the assumption that the clean rewards are deterministic but the perturbed rewards aren't? If so, what's the justification for this?\n\nEdit: I'm also not so sure about the assertion that MDPs can't have stochastic rewards. Even in Sutton and Barto's classic *Reinforcement Learning: An Introduction*, they state \"In general, reward signals may be stochastic functions of the state of the environment and the actions taken\" (Chapter 1)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8244/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700193829099,
                "cdate": 1700193829099,
                "tmdate": 1700195040817,
                "mdate": 1700195040817,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MTPTGCuJOI",
                "forum": "BSqVfAFJWz",
                "replyto": "JhSBYfxahk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8244/Reviewer_XsR2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8244/Reviewer_XsR2"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response, I appreciate the inclusion of the DRC_EX results, which look convincing.\n\nRegarding Q5, you're right, I should have said that the loss will be \"minimised\", not \"go to zero\". My point really was that the proof amounts to \"under unrealistically perfect conditions, the approach works\", which isn't surprising IMO and just over-mathsifies the paper. I'm not staunchly opposed to including it though; I guess it could be helpful for some readers.\n\n> Under continuous perturbations, the statement on Page 2 is about the assumption of RE expressed as $\\mathbb{E}(\\tilde{r}) = \\omega_0 \\cdot r + \\omega_1 (\\omega_0 > 0)$, which ensures the unchanged optimal policy for both episodic and non-episodic tasks after the perturbation.\n\nThis isn't true for episodic tasks. Consider an environment where an agent can choose to continue or terminate the task. If all rewards are positive, the optimal policy is to continue, but if they're shifted to negative values by setting $\\omega_1 \\lt 0$ then it's optimal to terminate the task. This isn't a major issue, but I'd suggest correcting the sentence."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8244/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700198037728,
                "cdate": 1700198037728,
                "tmdate": 1700198037728,
                "mdate": 1700198037728,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f8NcKGTSan",
                "forum": "BSqVfAFJWz",
                "replyto": "wdvJ8zrdSn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8244/Reviewer_XsR2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8244/Reviewer_XsR2"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your further comments, I think we're on the same page now. At this point I think I just need to chat with the other reviewers about the case where the reward is modelled as a random variable. I feel like the LLM scenario in the intro is not a particularly good motivating example, since I would argue that the reward *is* a random variable in that case. (The reward isn't corrupted per se; it's just that different people have different tastes.) If, say, four people are mildly pleased with the output of an LLM, but one person is deeply offended, I'm not sure it's a good idea to go with the majority reward; that seems unsafe. If, on the other hand, one user accidentally hits the wrong button when scoring, then the approach seems more applicable. At the very least, I agree with Reviewer wJ8T that \"the paper should be upfront about the potential bias introduced by ignoring the stochasticity of the rewards\"."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8244/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654790075,
                "cdate": 1700654790075,
                "tmdate": 1700654790075,
                "mdate": 1700654790075,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gw4jwFAVAx",
            "forum": "BSqVfAFJWz",
            "replyto": "BSqVfAFJWz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8244/Reviewer_m2dC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8244/Reviewer_m2dC"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of performing reinforcement learning (RL) when the true rewards have been perturbed. Like previous works, the paper considers problems where the reward function is perturbed in a specific way: the range of the reward function is discretized into equal-width subintervals, which are then perturbed according to a row-stochastic \"confusion matrix\". In particular, this work assumes that the confusion matrix is mode-preserving in that \"true reward is the most frequently observed class after perturbation\" (see abstract). Under this condition, a method is proposed that learns a classifier mapping state-action pairs to bins corresponding to observed modes and uses it to recover the true reward. These recovered rewards are then used when training the user's RL algorithm of choice. Versions of the proposed method are provided for (a) when the number of subintervals and reward range is known, (b) when the reward range but not the number of subintervals is known, and (c) when neither is known. The proposed method improves on existing works that either apply only to reward perturbations that preserve the order of the true rewards or require that the true rewards take only finitely many values. Experimental results are provided indicating superior performance over existing methods on a variety of tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper addresses an interesting and important problem that is likely of interest to the community. The proposed method generalizes previous work in the area (see summary above) and provides a practical scheme for performing RL with a certain class of perturbed rewards. Though the class of perturbations assumed is potentially restrictive (see weaknesses section below), the method proposed for training a classifier and recovering the true reward is clever and appears to be novel. The experimental evaluation is quite extensive and indicates superior performance to existing methods on the perturbed-reward environments tested."
                },
                "weaknesses": {
                    "value": "Two important weaknesses of this paper include the following. If these issues can be clarified, it will be easier to more accurately judge the significance of this work.\n1. Though the proposed method avoids some of the drawbacks of previous works, such as directly estimating the confusion matrix or the true reward function, or assuming order-preservation of the perturbation, the mode-preserving condition seems quite restrictive. Mode-preserving perturbations are not a generalization of order-preserving perturbations, since the latter may not be order-preserving. Also, methods that learn the confusion matrix may be more widely applicable (though more computationally expensive), since the perturbations can be quite general. Without further motivation of the mode-preserving condition and its advantages over other conditions imposed in the literature, the broader impact of this work is difficult to judge.\n2. The experimental comparisons are extensive, but there are some issues regarding the fairness of comparisons with previous methods as well as their interpretability. Due to the lack of order-preservation mentioned at the end of the first paragraph of Sec. 5.3, the results in Fig. 5 and 6 may be unfair to the RE method. Comparisons on problems that enjoy conditions to which all methods are applicable would provide a fairer comparison. In addition, though the proposed DRC and GDRC methods show better performance than previous methods on a number of problem instances, they decidedly underperform on many others. Further discussion of this discrepancy is warranted to clarify when the proposed methods should be preferred. Finally, the impact of $n_o$ studied in Sec 5.2 is an important issue, but the meaning of Fig. 3 is difficult to interpret. This leaves the question of how to choose $n_r$ open, which is a key component to the applicability of GDRC, in particular."
                },
                "questions": {
                    "value": "* the proposed method replaces the computational effort of estimating the confusion matrix with the computational cost of training a classifier estimating the bin to which the reward at a given state-action pair belongs; can you comment on how these two costs compare?\n* does the classifier trained in line 7 of Alg. 1 -- which is called the \"distributional reward critic\" -- correspond to any specific action value function, like the standard critics used in actor-critic algorithms? if not, it is a little misleading to call it a critic.\n* what is the main difficulty in proving Theorem 1? it seems like it should follow from a straightforward application of the law of large numbers.\n* what should the reader take away from Fig. 3? it is currently tough to interpret.\n* at the end of the **Algorithms** subsection of Sec. 5.2, it says the rewards are averaged over \"10 seeds and 20 random trials\"; independent trials are usually each associated with one seed -- can you clarify what you mean?\n* in Fig. 6, why do you think DRC's performance improves as noise improves?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8244/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8244/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8244/Reviewer_m2dC"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8244/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699052329689,
            "cdate": 1699052329689,
            "tmdate": 1699637024647,
            "mdate": 1699637024647,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q2dqTOqvou",
                "forum": "BSqVfAFJWz",
                "replyto": "gw4jwFAVAx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8244/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8244/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(1/3) Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable review. Here are our answers to the questions:\n\n>Q1: Order-preserving perturbations might not be mode-preserving and what the motivation of the mode-preserving assumption is.\n\nA1: Order-preserving is necessary for RE to work but is not sufficient. After analyzing all the methods in detail, we find the assumption of RE is $\\mathbb{E}(\\tilde{r})=\\omega_0\\cdot r +\\omega_1(\\omega_0 > 0)$, a positive affine transformation. That means RE might not work if order-preserving holds although mode-preserving might not hold as well. Here we clarify the difference between mode-preserving and order-preserving. Mode-preserving and order-preserving are two kinds of concepts although they look similar. Mode-preserving is the assumption of our methods and order-preserving is a property assumptions might necessitate. The assumption of RE is positive affine transformation instead of order-preserving. It is not that clear which method is more broadly applicable with different assumptions. Then we find positive affine transformation necessitates order-preserving, making RE not work under non-order-preserving cases as we discuss in the last paragraph of Page 2. However, mode-preserving does not necessitate order-preserving, which means our methods can work in cases where the reward order is totally broken, and then the optimal policy gets changed as long as mode-preserving holds, demonstrating the larger applicability and capability of our methods. That is the reason we have a separate column for order-preserving in Table 1 to help understand the applicable scenarios of different methods. To ensure there is no confusion, we change the term from \"order-preserving\" to \"optimal-policy-unchanged\", which is comparable to the positive affine transformation and is easy to tell the applicable scenarios as well. Besides, mode-preserving is not created by us, which is the assumption of SR as shown in Table 1. When recovering the confusion matrix, they use majority voting to decide the true reward of the samples for a state-action chunk while assuming mode-preserving.\n\nOne more thing to notice is that the forms of $\\tilde{R}$ of the continuous perturbation functions (e.g. Gaussian perturbations) people pay attention to in RE necessitate mode-preserving although the space is continuous, which shows mode-preserving is a weak assumption in the cases people are interested in. With mode-preserving, our methods win over the baseline methods under both GCM perturbations and continuous perturbations, which is the strongest evidence that mode-preserving is a weak assumption and our methods are broadly applicable.\n\nWe clarify the motivation of mode-preserving in the last paragraph of Section 3 of the revised paper in blue.\n\n>Q2: Methods that learn the confusion matrix may be more widely applicable since the perturbations can be quite general.\n\nA2: Although the general confusion matrix (GCM) perturbation is general, the method (SR) learning the confusion matrix is not widely applicable.\n\nAs we analyze in Table 1 and the last paragraph of Page 4, SR requires some unrealistic information about GCM settings, making it inapplicable to perturbations without the concept of a confusion matrix like continuous ones. Besides, although a correctly recovered confusion matrix tells us how perturbations are applied, it is not the best solution to have the fixed substitute reward for a corresponding observed reward using $\\hat{R}=C^{-1}\\cdot R$ because they might be perturbed from different clean rewards, which ignores the valuable state-action information and could even cause significant variance compared with the one caused by perturbations. Last but not least, SR discretizes the state-action space and regards the clean rewards of a given state-action chunk as the same without any assumption of the smoothness / Lipschitz-ness of rewards, making itself incapable of dealing with the environments with large state-action space and complicated reward functions.\n\n>Q3: The results in Figures 5 and 6 may be unfair to the RE method. Comparisons on problems that enjoy conditions to which all methods are applicable would provide a fairer comparison.\n\nA3: As mentioned in A1, a positive affine transformation is the assumption of RE instead of order-preserving and mode-preserving is what we need to work under all the perturbations people are interested in of both RE and SR. For Figures 5 and 6, the generation of GCM perturbations does not ensure any condition including mode-preserving as we say in the Perturbations paragraph of Page 7, meaning they are fair comparisons for all the methods. Additionally, we have evaluated the methods under continuous perturbations of RE with a positive affine transformation and found that our method still performs better although they are even not our targets, as discussed in Section 5.4."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8244/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191595264,
                "cdate": 1700191595264,
                "tmdate": 1700608988736,
                "mdate": 1700608988736,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qmGGnMuriL",
                "forum": "BSqVfAFJWz",
                "replyto": "5gESWkgA9T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8244/Reviewer_m2dC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8244/Reviewer_m2dC"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for their response, which helped to clarify some of my questions. However, I remain concerned about the motivation and justification for the restrictive mode-preserving condition and still lack a clear picture of the types of problems for which this is a reasonable assumption. In addition, I am not convinced that the proposed method truly generalizes existing approaches, as claimed in the abstract. Indeed, the response leads me to believe that, just as the proposed method may work for problems where order preservation fails, alternative methods may work when mode preservation fails. I will therefore keep my score."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8244/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718182322,
                "cdate": 1700718182322,
                "tmdate": 1700718182322,
                "mdate": 1700718182322,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lqVkQFcxxa",
                "forum": "BSqVfAFJWz",
                "replyto": "gw4jwFAVAx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8244/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8244/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the valuable reply from the reviewer. We just want to have a quick clarification for the reviewer\u2019s further reference.\n\n>Point 1: I remain concerned about the motivation and justification for the restrictive mode-preserving condition and still lack a clear picture of the types of problems for which this is a reasonable assumption.\n\nReply 1: We are happy to tell why we choose mode-preserving as the assumption of our methods. We want to design a method that can deal with a general perturbation. Then we generalize the perturbation in SR to form our generalized confusion matrix (GCM) perturbations, which can cover any reward perturbation as we propose in Section 3.2. In the step of deciding on the assumption of our methods, there are three factors contributing to our decision. First, the assumption of RE, optimal-policy-unchanged, is very strict and the optimal policy after this perturbation can change easily. Second, SR seems to work under this perturbation as long as its assumption works, which is mode-preserving. Third, what GCM perturbations do is transform the discrete bias caused by perturbed labels into a continuous space. In the field of handling perturbed (noisy) labels, mode-preserving is the most common assumption. Therefore, we choose mode-preserving as our assumption and we do not find a better choice than it under GCM perturbations.\n\nMode-preserving is not a restrictive assumption. Mode-preserving is a comparable assumption of optimal-policy-unchanged at least, where the former cares about mode and the latter cares about expectation. Besides, assuming the mode of perturbed labels under GCM perturbations makes more sense as we say in the second point above. Even for the continuous perturbations people pay attention to in RE, mode-preserving weakly holds, explaining why our methods work there. What\u2019s more, tuning LLMs is also an appropriate scenario to apply our methods for handling noisy labels. As we discuss with other reviewers, our methods can be even adapted to customize to different needs in LLMs, meaning the assumption can be anything besides mode-preserving.\n\n\n>Point 2: I am not convinced that the proposed method truly generalizes existing approaches, as claimed in the abstract.\n\nReply 2: We do not say our methods generalize existing approaches in the abstract. What we convey is that our GCM perturbations generalize the existing perturbations and the baseline methods do not work because of either unreasonable needed information or poor performance.\n\n\n>Point 3: The proposed method may work for problems where order preservation fails, alternative methods may work when mode preservation fails.\n\nReply 3: Yes, that is what we conveyed in the previous rebuttal. It is impossible to decide the better one between assuming the expectation and assuming the mode directly. However, we do not choose to assume the expectation because it ensures the optimal policy does not change regarding the expectation of the perturbed rewards, which greatly restricts the influence that can be caused by perturbations. To tell it simply, such perturbations can only cause variance but no bias to influence the optimal policy. Another point to mention is as we always say in the previous rebuttal and Reply1 above, mode-preserving weakly holds under the continuous perturbations people are interested in, which is not an unreasonable point. Generally speaking, when we consider a perturbation only causing variance, what people usually choose is something like Gaussian perturbation with one mode instead of the perturbation of a multimodal distribution, meaning those perturbations are more realistic to some extent."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8244/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727738104,
                "cdate": 1700727738104,
                "tmdate": 1700727991369,
                "mdate": 1700727991369,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]