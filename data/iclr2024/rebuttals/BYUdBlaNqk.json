[
    {
        "title": "System Identification of Neural Systems: Going Beyond Images to Modelling Dynamics"
    },
    {
        "review": {
            "id": "EfGxr4NR1G",
            "forum": "BYUdBlaNqk",
            "replyto": "BYUdBlaNqk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3872/Reviewer_wu71"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3872/Reviewer_wu71"
            ],
            "content": {
                "summary": {
                    "value": "This paper uses established regression techniques to determine how different models, including video-trained ones, relate to fMRI data collected during video viewing. It also first shows that such analyses can identify the input domain (image vs video) that models were trained on. Further comparisons explore architectures and layers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Demonstrates 'system identification' ability\n\nMany models are tested and compared\n\nIncludes analysis of hierarchical correspondence between brain and models\n\nResults are clearly presented in figures"
                },
                "weaknesses": {
                    "value": "Novelty is somewhat overstated (other work has compared video-trained networks to the brain:\nhttps://pubmed.ncbi.nlm.nih.gov/29436055/\nhttps://proceedings.neurips.cc/paper/2018/hash/9d684c589d67031a627ad33d59db65e5-Abstract.html\nhttps://arxiv.org/abs/2306.01354)\n\nWriting is unclear at points.\nFor example, these are not complete/correct sentences:\n\"Since we can identify its modelling scheme,\nwhich acts as a form of ground-truth to be used when comparing different models. \"\n\"Towards this we investigate one model\nthe OmniMAE its pretrained model in a self-supervised manner compared to the finetuned one to a\ndownstream task with full supervision. \"\n\n\"We use the modelling scheme to refer to the\nmodel\u2019s ability to learn from dynamic information provided in an input clip and/or static information\nfrom a single image.\" I'm still unclear on what modeling scheme means. Is it the same thing that is later labeled as the (i) input?\n\n\"Since we have established the feasibility of identifying the target system to an extent with regression\nscores, it brings the question of how can we use this information to identify the underlying mechanisms\nin biological neural systems.\" This was only established for video vs image trained networks, so that should be clear."
                },
                "questions": {
                    "value": "How do the authors understand their work in comparison to previous work that has showed self-supervised models to be equivalent to fully supervised in terms of neural prediction? e.g. https://www.pnas.org/doi/abs/10.1073/pnas.2014196118"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3872/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3872/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3872/Reviewer_wu71"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3872/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698353668251,
            "cdate": 1698353668251,
            "tmdate": 1700664244515,
            "mdate": 1700664244515,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1XXcOJjASU",
                "forum": "BYUdBlaNqk",
                "replyto": "EfGxr4NR1G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3872/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3872/Authors"
                ],
                "content": {
                    "title": {
                        "value": "R4 Response"
                    },
                    "comment": {
                        "value": "Thanks for your comments and feedback, we incorporate additional results in supplementary materials (Appendix A1-5) with Figs and Tables referenced with the initial S:\n\n1. Novelty highlighted in common response. We also incorporate referenced works in the revised submission Sec. 2 (in red). However, the referenced papers do not conduct large-scale study of video understanding models which is necessary when tackling the question of how do they compare to biological neural systems. In our work, we study around 30 video understanding models. We also include new and comprehensive aspects in our study missing from previous literature: e.g., single/two-stream, convolutional/transformer and self/fully supervised. We conduct a system identification of image vs. video understanding models through a simulated experiment which is missing from the literature.\n\n2. We updated the unclear paraphrases with a new one, and the updated paraphrases are written in red.\n\n3. We substituted the term \"modelling scheme\" with \"dynamics modelling\" referring to the representation of dynamic information from multiple frames, as opposed to the static information derived from a single image.\n\n4. Clarified in the revised submission.\n\n5. The paper referred to, has focused on single image stimuli in Macaque, and compared image understanding models to predict their responses. While they include one self supervised video understanding model, they lack the large-scale and comprehensive aspect of our study and the fact that your study is focused on video stimuli. Nonetheless, our main conclusions on self supervised models is similar in that most of their self supervised methods were on-par or less than the supervised ones. Except for contrastive learning, which we didn\u2019t investigate in our study and leave for future work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612785726,
                "cdate": 1700612785726,
                "tmdate": 1700659775217,
                "mdate": 1700659775217,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eaZS9kPtik",
                "forum": "BYUdBlaNqk",
                "replyto": "1XXcOJjASU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3872/Reviewer_wu71"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3872/Reviewer_wu71"
                ],
                "content": {
                    "comment": {
                        "value": "I am happy with the way the authors have re-phrased their contributions and included relevant citations. I have increased my score by a point."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664231064,
                "cdate": 1700664231064,
                "tmdate": 1700664231064,
                "mdate": 1700664231064,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pIi1M4gphY",
            "forum": "BYUdBlaNqk",
            "replyto": "BYUdBlaNqk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3872/Reviewer_vj7K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3872/Reviewer_vj7K"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on two aspects of brain-machine modeling: (1) processing dynamic information (e.g., video clips) instead of typical static images; (2) whether system identification is feasible. In this study, the authors used the Algonauts fMRI dataset where cortical responses for 1000 video clips are avalible. To test the feasibility of system identification, a simulated and a realistic environment were created. In the simulated environment, I3D ResNet-50, ViT-B, and MViT-B were used astarget systems, and several computational models were used as source systems to regress on the targets. The results showed that targets trained for image and video understanding can be successfully differentiated using this regression approach. In the realistic environment, brain data were used as target systems. Using the regression approach, differences between image and video understanding, between convolutional and transformer operation, between fully-supervised and self-supervised training, can be revealed in brain responses.\n\nI appreciate this approach but the results need more explanation"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This study utilizes the movie fMRI datasets and extends past work from image to video understanding\n2. This study extends past work and investigates the system identification problems in video undertanding\n3. The results are informative\n4. The writing is very clear and easy to follow\n5. The selection of candidate models is representative and complete."
                },
                "weaknesses": {
                    "value": "Weakness\n\n1. In the simulated environment, the authors claimed to focus on three aspects: (1) image/video understanding, (2) fully-supervised/self-supervised, and (3) convolution/transformer. However, Figure 1 only shows the result for (1). I am wondering what the results are for (2) and (3)?\n2. In the simulated environment, I3D ResNet-50, ViT-B, and MViT-B can be used for purposes (1) and (2), but not for (3). I would suggest including more target models for the purpose (3).\n3. In the realistic environment, Figure 2A shows the advantages of two-stream models over single-stream models. However, why should we compare them??  Two-stream vs. single-stream is not the part in the simulated environment nor the part in the introduction.\n4. Figure 4. if I understand correctly, OmniMAE-B pretrained is indeed self-supervised. But OmniMAE-B finetuned should be self-supervised + supervised finetuning. Is this comparison fair to show the differences between fully-supervised vs. self-supversied??"
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3872/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3872/Reviewer_vj7K",
                        "ICLR.cc/2024/Conference/Submission3872/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3872/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698488039147,
            "cdate": 1698488039147,
            "tmdate": 1699947487059,
            "mdate": 1699947487059,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9N3wVQkRTp",
                "forum": "BYUdBlaNqk",
                "replyto": "pIi1M4gphY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3872/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3872/Authors"
                ],
                "content": {
                    "title": {
                        "value": "R3 Response"
                    },
                    "comment": {
                        "value": "Thanks for your comments and feedback, we incorporate additional results in supplementary materials (Appendix A1-5) with Figs and Tables referenced with the initial S:\n\n1. We added Appendix A.4 Fig. S3 for (2) and (3) shows these results, for I3D it shows statistically significant results in favour of the groundtruth model (i.e., convolutional and fully supervised).\n\n2. We leave it for future work to expand on target models due to the limited scope of the rebuttal period. \n\n3. Our system identification is mainly used to establish the ability to differentiate image vs. video understanding models. It is the real experiments that give us insights on how video understanding models compare to brain computations, e.g., comparing single/two-stream models. Also to affirm the consistency of our results across different training datasets.\n\n4. For fair comparison, we added TimeSformer to Fig. 3b (which is trained fully-supervised on SSV2 relying on ViT base without self supervised pre-training). It still confirms that fully supervised models surpass self supervised ones. Below is a table showing the regression scores of OmniMAE Pretrained compared to TimeSformer. \n\n|     | OmniMAE Pretrained | TimeSformer |\n|-----|--------------------|-------------|\n| V1  | 0.23               | 0.28        |\n| V2  | 0.21               | 0.27        |\n| V3  | 0.19               | 0.26        |\n| V4  | 0.18               | 0.26        |\n| LOC | 0.20               | 0.28        |\n| EBA | 0.23               | 0.33        |\n| FFA | 0.21               | 0.27        |\n| STS | 0.14               | 0.20        |\n| PPA | 0.15               | 0.21        |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611950573,
                "cdate": 1700611950573,
                "tmdate": 1700615954364,
                "mdate": 1700615954364,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VrOo63EDC5",
                "forum": "BYUdBlaNqk",
                "replyto": "9N3wVQkRTp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3872/Reviewer_vj7K"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3872/Reviewer_vj7K"
                ],
                "content": {
                    "title": {
                        "value": "read, but keep my score"
                    },
                    "comment": {
                        "value": "I really appreciate the authors' efforts to provide additional analyses and more interpretations. But I still have some concerns.\n\nThe Fig. S3 is great. However, convolutional operations seem to outperform transformer operations. There seems no clear interpretation for the reason. The new analyses on TimeSformer are great.\n\nHere is my major concern. The intro clearly states that the aims of this study are to compare (1) video/image, (2) convolution/transformer, and (3) fully-supervised/self-supervised. But in the Appendix. 4, it states that \"although our focus in the system identification is on the ability to differentiate image vs. video understanding models, we provide additional results for other families of models\". I am confused why the scope is suddenly narrowed down to only (1). Also, to systematically address the three aspects, in theory there should be 2^3 = 8 target models. I understand that the rebuttal period is limited but the results seem not sufficient as claimed in the intro.\n\nAlso, I understand that the one-stream/two-stream comparison only occurs for brain data. However, my question is why should we care about this ??? The intro never mentioned the rationale of one-stream/two-stream comparison and this is also not included in the three-fold contributions as listed in intro. \n\nI share the feelings of the two reviewers above. The rationale of this study is unclear or, at least, this study is incomplete given its current form. I suggest that the authors consider revising the manuscript substantially and think about the potential rationale."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710085800,
                "cdate": 1700710085800,
                "tmdate": 1700710085800,
                "mdate": 1700710085800,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WDmvZHcva7",
            "forum": "BYUdBlaNqk",
            "replyto": "BYUdBlaNqk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3872/Reviewer_N4F4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3872/Reviewer_N4F4"
            ],
            "content": {
                "summary": {
                    "value": "In this manuscript the authors use DNNs for videos to predict fMRI responses and other networks in a pretest. As data they use the 2021 Algonauts challenge for predicting responses to 3 second video clips."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "It is\u2014in principle\u2014a step in the right direction to include temporal dynamics to better understand biological brains. We should move towards predicting responses to videos, not only images and the authors do that. Also they evaluate a selection of newer model architectures that were not available in 2021 when the Algonauts challenge with videos ran officially."
                },
                "weaknesses": {
                    "value": "I think the results of this study are underwhelming though for three major reasons:\n\nFirst, the DNN models are not intended as models of biological vision and do not contain any interpretable dynamics that would enable conclusions about theories. Also, they usually run at so slow timescales (typically the frame rate of the video), that they could never have the temporal dynamics of biological networks in the first place. Thus, it is is not surprising that the conclusions are not particularly clean.\n\nSecond, fMRI is not able to resolve dynamics of visual processes. Thus, it cannot provide evidence about these dynamics.\n\nThird, the relationship to the Algonauts challenge remain unclear to me. The challenge website is open for post challenge submissions, so the authors could have submitted their models to the competition to get scores for the official test set. If that was not desirable for some reason, I think we would like to see the results for the top entires of this challenge to get an idea how close to the state of the art the models from this paper perform. Unfortunately, I do not see a substantial step of this manuscript beyond the Algonauts papers. \n\nThus, this manuscript does not provide the promised insights into dynamics and instead becomes an incremental step repeating things that have been done for image neural networks with video networks without providing substantial new insights."
                },
                "questions": {
                    "value": "My main question for the authors is: Why this dataset and without the official evaluations? And to convince me of a better view of this work the main ingredient would have to be a substantial insight in how we might capture the dynamics of human visual perception better."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3872/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789695454,
            "cdate": 1698789695454,
            "tmdate": 1699636345398,
            "mdate": 1699636345398,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OfJeyvMd2y",
                "forum": "BYUdBlaNqk",
                "replyto": "WDmvZHcva7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3872/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3872/Authors"
                ],
                "content": {
                    "title": {
                        "value": "R2 Response"
                    },
                    "comment": {
                        "value": "Thanks for your comments and feedback, we incorporate additional results in supplementary materials (Appendix A1-5) with Figs and Tables referenced with the initial S:\n\n1. **\u201cUnderwhelming results\u201d** our findings are significant in that it helps improve video understanding models relying on ViTs and discussing the reasons for the superiority of multiscale ViTs. Also our results on system identification are necessary for all consequent works studying video understanding models in neural encoding. \n\n\n2. **1st & 2nd reason**: Several studies (Lahner et al., 2023; Zhou et al., 2022) have used DNNs models compared to brain computation, and it is well established to use fMRI in these studies, we are not the first to. Despite the slower timescales, our results demonstrate the superiority of video understanding over image ones in neural encoding.\n\n\n3. **3rd reason**: Challenge website prevents new submissions on the test set. Although our focus is on comparing existing SOA video models in neural encoding, we still compare to the challenge winner on the validation set, Fig. S4. It shows comparable results, without employing an ensemble of models, different modalities or fine-tuning the backbone as the winning entry.\n\n\n4. Novelty concern is addressed in the common response above.\n\n\n5. We provide a large-scale study of deep video understanding from a neuroscience perspective. Thus, the dataset is driven by the characteristics of short video stimuli, which is in-line with the typical datasets used for training deep video understanding models for a better comparison."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611891319,
                "cdate": 1700611891319,
                "tmdate": 1700615104462,
                "mdate": 1700615104462,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mBts3ADCbl",
                "forum": "BYUdBlaNqk",
                "replyto": "OfJeyvMd2y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3872/Reviewer_N4F4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3872/Reviewer_N4F4"
                ],
                "content": {
                    "title": {
                        "value": "Read, but no change in opinion"
                    },
                    "comment": {
                        "value": "I read the authors' response, but it did not change my opinion of the manuscript. \n\nIt still seems like a small incremental step beyond the existing DNN evaluations on fMRI data that did not yield deep insights. The distinctions the authors are able to make still seem quite trivial to me."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695245954,
                "cdate": 1700695245954,
                "tmdate": 1700695245954,
                "mdate": 1700695245954,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "f6Jnq4yZJa",
            "forum": "BYUdBlaNqk",
            "replyto": "BYUdBlaNqk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3872/Reviewer_4dZd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3872/Reviewer_4dZd"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors perform a system identification study that focuses on modeling dynamics in perception by investigating multiple video models and comparing them with image models.\n\nThis study attempts to answer following research questions:\n\n1. Is it possible to distinguish video models from image models? \n2. Which models better predict human fMRI responses to videos?\n    1. Video vs. image models\n    2. Convolutional vs. transformers\n    3. Fully supervised vs. self-supervised\n\nThe authors perform extensive experiments using multiple models on simulated (predict other model\u2019s responses) and real (predict human fMRI responses) to answer these questions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Layer-weighted encoding to compare models. This makes comparison easier removing the steps of layer selection for individual models. \n2. Varieties of models investigated in this study. The authors have carefully chosen a wide variety of models (conv vs. transformers ; self-supervised vs. supervised; image vs. video) using which they are able to answer multiple questions in this paper \n3. Statistical analysis to compare whether one family of model predict better than others.\n4. System Identification study(Figure1) investigating whether models from one modality (image/video) can predict the models from same modality better than models from other modality. The result showing that I3D early layers can be predicted equally well suggests I3D does not use temporal information well in early layers"
                },
                "weaknesses": {
                    "value": "1. The authors claim \u201cprevious work did not consider the time\naspect and how video and dynamics (e.g., motion) modelling in deep networks\ncompare to these biological neural systems\u201d . This is incorrect. Several previous works [i-iv] have investigated modeling temporal aspects of videos and comparing it to brain responses. These works have been completely overlooked and not cited. Further seminal works on encoding and neural system identification from  Jack Gallant\u2019s group and Marcel Van Gerven\u2019s group are not cited. \n2. Several important details are missing\n    1. When comparing convolutional vs. transformer or self-supervised  in Figure 2 b,c ; did you consider both video and image models ?\n        1. If yes what was the reasoning, because if video models better predict brain activity doesn\u2019t it make sense to restrict only to video models. If both the video and image models are considered for comparison do you see same pattern for video and image family of models? \n    2. When you compare OmniMAE-B Pretrained/Finetuned what was the task OmniMAE finetuned on and on which dataset (Figure 3b)\n3. In Figure 3, it is not clear whether the results are statistically significant or not\n4. Some of the results require a deeper dive to gain better understanding of exactly what is happening\n    1. In Figure 1a(MViT-B) and 1c (I3D R-50), it can be clearly seen that variance in regression score by video models is quite high compared to image models suggesting some models are better predictor and some are worse. Which ones are worse/best predictors and why? This answer is important to understand how temporal information in video should be modelled. \n    2. Similar variance can be observed in Figure 2a-c as well raising the question why these models  are one family? A simple classification such as transformer vs conv or self-supervised vs supervised is not helpful here when there is so much variance within a family of models. The  conclusion that can be derived here are\n        1. From Figure 2a: 3 video models predict brain responses similar to or worse than image models while others predict better. Which are similar to image models and which are better is not answered.\n        2. From Figure 2b: some transformer models predict as well as conv models\n        3. The above conclusions are quite weak and less helpful and informative for readers without a deeper analysis.\n5. Overall, I find paper containing multiple results with unclear findings. \n\nReferences\n\ni) Nishimoto, Shinji, et al. \"Reconstructing visual experiences from brain activity evoked by natural movies.\"\u00a0*Current biology*\u00a021.19 (2011): 1641-1646.APA\n\nii) Khosla, Meenakshi, et al. \"Cortical response to naturalistic stimuli is largely predictable with deep neural networks.\"\u00a0*Science Advances*\u00a07.22 (2021): eabe7547.\n\niii) Nishimoto, Shinji. \u201cModeling movie-evoked human brain activity using motion-energy and space-time vision transformer features\u201d ; biorxiv 2021\n\niv) Lahner, Benjamin, et al. \"BOLD Moments: modeling short visual events through a video fMRI dataset and metadata.\" bioRxiv (2023): 2023-03."
                },
                "questions": {
                    "value": "Suggestions: \n\n1.  Please add relevant citations \n2.  Refer to weakness point 2-4 and please address those."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3872/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698847390434,
            "cdate": 1698847390434,
            "tmdate": 1699636345308,
            "mdate": 1699636345308,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "d97BJJg3AZ",
                "forum": "BYUdBlaNqk",
                "replyto": "f6Jnq4yZJa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3872/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3872/Authors"
                ],
                "content": {
                    "title": {
                        "value": "R1 Response"
                    },
                    "comment": {
                        "value": "Thanks for your comments and feedback, we incorporate additional results in supplementary materials (Appendix A1-5) with Figs and Tables referenced with the initial S:\n\n**1.** We incorporate the requested works in the revised submission Sec. 2 (in red). Note that the last two works mentioned are not published and are concurrent. These works do not provide a large-scale study of different deep video understanding models. We study around 30 video understanding models and include new and comprehensive aspects in our study missing from previous literature: e.g., single/two-stream, fully/self supervised and transformer/convolutional. We conduct a system identification of image vs. video understanding models through simulated experiments which is missing from the literature. \n\n**2.1.1.** For Fig.2 b,c, yes we considered all models, to incorporate more models in the statistical significance test for a stable result. Nonetheless, we add Appendix A.2 results showing the consistency of our conclusions in video understanding only excluding image models in Fig. S1.\n\n**2.2.**  Training task: Action recognition, Dataset used: Something Something-v2 (SSv2). We added a line in section 3.3 (in red). \n\n**3.** Figure 3(a, b, c) statistical significant results are added and re-iterated in Appendix A.3 Tables S5-7 confirming in most regions there is statistically significant comparing single and two-stream models (Fig. 3a & Tab. S5), comparing self and fully supervised ones (Fig. 3b & Tab. S6), and comparing pairs of video understanding models  (Fig. 3c & Tab. S7).\n\n**4.1.** Tab. S1-3 were included for Fig. 1a-c, highlighting the best and worst source models. The alignment between the source and target model architecture and supervision signal is evident, except for the MViT target model, clarified in Appendix A.2 showing consistency to MViT performance in real experiments.\n\n**The best source models for I3D target model are shown below:**\n\n|    | Max (Video)   | Max (Image) |\n|----|---------------|-------------|\n| B1 | SlowFast-R101 | ResNet-18   |\n| B2 | C2D           | ResNet-50   |\n| B3 | C2D           | ResNet-50   |\n| B4 | C2D           | ResNet-18   |\n| B5 | R(2+1)D       | ResNet-50   |\n| B6 | C2D           | ResNet-18   |\n| B7 | SlowFast-R101 | ResNet-50   |\n\n**The best source models for MViT-B-16x4 target model are shown below:**\n\n|    | Max (Video) | Max (Image) |\n|----|-------------|-------------|\n| B1 | MViT-B-32x3 | ResNet-50   |\n| B2 | MViT-B-32x3 | ResNet-18   |\n| B3 | MViT-B-32x3 | ResNet-50   |\n| B4 | MViT-B-32x3 | ResNet-50   |\n| B5 | MViT-B-32x3 | ResNet-50   |\n\n**4.2.** The use of \u201cmodels families\u201d is established in previous literature, Han et. al. (2023), where they studied convolutional vs. transformer models. Computing statistical significance among populations of models is standard practice, regardless of the degree of variance within the population.\n\n&nbsp; **4.2.1.** The 3 video models performing worse and comparable to image models are the self supervised ones (stMAE and omniMAE variants). The best video models are the SlowFast variants (two-stream). Both results align with findings in Fig. 3.\n\n&nbsp; **4.2.2.**  Fig. S2 compares slow (conv) and timesformer (transformer) models, trained using two different datasets. Evidently, within the early-mid regions of the brain, the convolutional model consistently yields higher scores. This reaffirms our finding that convolutional models excel in capturing orientation and frequencies due to their early local connections.\n\n&nbsp; **4.2.3.** Strong findings are shown in Sec.4.5 and Appendix A.1,2 and 4 provide deeper analysis showing strong alignment to the results in the main submission.\n\n**5.** Findings summary and emphasis added to the Abstract and Sec. 4.5 and the main response for all reviewers."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611566831,
                "cdate": 1700611566831,
                "tmdate": 1700659510675,
                "mdate": 1700659510675,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0Z2oQCDmtq",
                "forum": "BYUdBlaNqk",
                "replyto": "f6Jnq4yZJa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3872/Reviewer_4dZd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3872/Reviewer_4dZd"
                ],
                "content": {
                    "comment": {
                        "value": "I thank authors for their responses to all the reviewers and making an attempt to address the weaknesses mentioned. Please find my responses to individual clarifications below:\n\n1. Thank you for adding these references. My intention was not to say that this contribution is similar to mentioned works, it was to argue that the claim authors make that this is the first study to model video and dynamics is not true. \n2. Thank you for adding these results. \"For Fig.2 b,c, yes we considered all models, to incorporate more models in the statistical significance test for a stable result\" What does this mean exactly? You can get stable results even if you compare two models and chose an appropriate statistical test. I do not find this justification valid. \n3. Thanks for adding the details\n4. Thank you for adding new results. Why the best models are different for each block and how different are they?\n5. Again, I find too many results without a unifying theme about what this paper is trying to do.\n\nWhile I appreciate authors responses to individual comments, I am still not convinced by their responses to some of mine and other reviewer's questions (e.g. Q3 and Q4 from Reviewer vj7K and Q1 and Q2 from Reviewer N4F4 ). I do not think this paper is ready for acceptance in current format and therefore will keep my original rating."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687570908,
                "cdate": 1700687570908,
                "tmdate": 1700688296485,
                "mdate": 1700688296485,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]