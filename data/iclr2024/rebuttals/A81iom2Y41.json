[
    {
        "title": "Be Your Own Neighborhood: Detecting Adversarial Example by the Neighborhood Relations Built on Self-Supervised Learning"
    },
    {
        "review": {
            "id": "xs45oae0ZV",
            "forum": "A81iom2Y41",
            "replyto": "A81iom2Y41",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7036/Reviewer_J3f8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7036/Reviewer_J3f8"
            ],
            "content": {
                "summary": {
                    "value": "The paper tackles the problem of defenses against adversarial examples (AE) targeting deep neural networks (DNN). The main contribution is \"BEYOND\", i.e., a novel countermeasure which seeks to _detect_ AE by leveraging self-supervised learning (SSL) techniques. The intuition behind BEYOND is that, given a sample, it is possible to create \"augmented\" versions of such a sample, thereby creating the \"neihborhood\" of a given sample: then, by using SSL, it is possible to predict the ground truth of the original sample and its neighbors: if the class is the same, then the sample is clean; however, if the class is different, then the sample is an AE. Despite leveraging a simple intuition, the proposed BEYOND is theoretically grounded, and empirically evaluations demonstrate its effectiveness.\n\nAll in all, I believe this paper to be a valuable contribution to ICLR."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ Well written and easy to follow\n+ The intuition is simple, but sensible\n+ The code is disclosed (and it is well written!)\n+ The method is theoretically grounded\n+ The evaluation is comprehensive (both from the \"attacker\" perspective, as well as from the considered defenses)\n+ An ablation study is carried out\n+ Considerations on the implementation costs are provided\n\nI thank the authors for carrying out this work and submitting this paper to ICLR! I really enjoyed reading it. Specifically, I commend them for being able to condense all the \"relevant\" parts of their contribution into a 9-pages long paper. I could not find any redudancy in the arguments, and the underlying intuition, theoretical explanations, and extensive evaluation clearly demonstrate that the proposed method is valuable. What I particularly appreciated, however, was the discussion/analysis of the implementation costs, wherein the authors acknowledge that the proposed method may have some computational overhead (as a \"tradeoff\").\n\nIt is my belief the work described in this paper has abundant scientific merit."
                },
                "weaknesses": {
                    "value": "## High Level\n\n- The method appears to be limited to the Computer Vision domain \n- Some unclear details in the evaluation\n\nThe first weakness is what prevents me from assigning \"only\" an 8 (instead of a 10 -- albeit my score is more leaning towards a 7 than an 8). The entire paper is tailored for DNN applications for Computer Vision. It would be enticing to see how well the proposed method could be \"adapted\" to cover other domains in which DNN have found applications (e.g., audio, finance, cybersecurity).\n\nFor the second weakness (which is the reason why I am \"more leaning towards a 7 than an 8\"), I was unable to determine if the results provided in the paper refer to a \"single\" run of a given method, or are provided after performing multiple runs and then averaging the results (if so, please provide the amount of runs as well as the standard deviation). \n\n## Minor comments and suggestions\n\nIn the Introduction, the text states the following:\n\n> This vulnerability prevents DNN\nfrom being deployed in safety-critical applications such as autonomous driving Cococcioni et al.\n(2020) and disease diagnosis Kaissis et al. (2020), where incorrect predictions can lead to catastrophic\neconomic and even loss of life.\n\nTone this down. Aside from it being incorrect (i.e., DNN _are_ deployed in those applications, e.g., [https://spectrum.ieee.org/self-driving-cars-2662494269]), it is an unnecessary overstatement, and there is no need to mention that incorrect predictions can lead to ```loss of life``` -- especially since there is little evidence that such catastrophic events are due to incorrect predictions stemming from \"adversarial examples\" (and not just due to misconfigurations of the overall system)\n\nAlso, I invite the authors to refrain from using \"white-/gray-box\" terminology to denote the envisioned attacker. According to some recent works from IEEE SaTML23, these terms are ambiguous. I encourage the authors to use \"perfect/limited knowledge\" attackers. Plus, I also invite the authors to provide a section (in the Appendix) which clearly defines the knowledge/capabilities of the attacker w.r.t. the attacked system. Such a threat model can be justified with a practical example which elucidates a practical use case.\n\nFinally, the reference by \"Dan Hendrycks\" appears twice in the bibliography."
                },
                "questions": {
                    "value": "While I appreciated the paper, I am willing to increase my score if the authors provide compelling evidence that:\n\nQ1) the method can be applied to different domains (potentially substantiated with some proof-of-concept experiment)\n\nQ2) the results encompass various repetitions (which have been averaged)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7036/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697919185856,
            "cdate": 1697919185856,
            "tmdate": 1699636826567,
            "mdate": 1699636826567,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dWwhQBHYIt",
                "forum": "A81iom2Y41",
                "replyto": "xs45oae0ZV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7036/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7036/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your detailed and constructive comments, and we are encouraged that you find our work \u201cwell written, easy to follow\u201d, and \u201cvaluable contribution\u201d. We explain your questions as follows:\n\n## Q1 Different Domain\nWe understand the reviewer is interested in exploring the use of BEYOND in new domains. We try to migrate our approach to different domains like Natural Language Processing (NLP) and Speech Recognition (SR). But we note some critical differences: existing self-supervised models in NLP or SR are not based on diversified data augmentation strategies. For instance, in NLP, masked token prediction is the primary method. However, having diverse augmentation types is important for BEYOND, because it needs good data augmentation to create neighboring data. Nonetheless, we believe that our approach pioneers an idea of detection based on testing time-augmented examples rather than reference examples. And we will also continue to explore ways to apply this idea to different domains.\n\n## Q2 Repeated Experiment\nIn this paper, our experimental results come from a single run. To further verify the effectiveness of BEYOND, We repeat the experiments in Tables 1 and 2 of the main paper five times. On Cifar10, the average detection performance for the five attacks is 99.16% with a standard deviation of 0.0117. On ImageNet, the average performance against three attacks is 96.44% with a standard deviation of 0.0426.\n\n## Minor Comments\nApologies for the uncritical description, we will correct it to\u201c where incorrect predictions can lead to unpredictable losses\u201d. \nAnd about the \u201cwhite-/gray-box\u201d terminology, we appreciate your experienced suggestions. According to your advice, we refer to the latest work **[RR1]** from IEEE SaTML23, and change the terminology to \u201climited/perfect knowledge\u201d attack in the the latest pdf. \n\n**[RR1]** Apruzzese G, Anderson H S, Dambra S, et al. \u201cReal Attackers Don't Compute Gradients\u201d: Bridging the Gap Between Adversarial ML Research and Practice. 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). IEEE, 2023: 339-364.\n\n\nOur responses to all questions have been updated to the latest pdf and highlighted in blue."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7036/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700451028638,
                "cdate": 1700451028638,
                "tmdate": 1700499357754,
                "mdate": 1700499357754,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "grE78p374X",
                "forum": "A81iom2Y41",
                "replyto": "dWwhQBHYIt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7036/Reviewer_J3f8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7036/Reviewer_J3f8"
                ],
                "content": {
                    "title": {
                        "value": "Ack"
                    },
                    "comment": {
                        "value": "Dear authors, thanks for responding to my review.\n\nHowever, it is still unclear to me if the \"latest pdf\" already integrates the answers to Q1 and Q2. Specifically, for Q1, would the authors be able to do these experiments before the end of the discussion phase? Whereas, for Q2, I'd like to see an updated version of Table 1 and 2 wherein the cells report the average results (and standard deviation) over the five runs. \n\nA way to do so is by reporting, e.g., $99.16 {\\scriptsize \\pm 0.0117}$ (code: ```$99.16 {\\scriptsize \\pm 0.0117}$```)\n\nAlso, there is a typo in the reporting in the paper of [RR1]."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7036/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482418982,
                "cdate": 1700482418982,
                "tmdate": 1700482418982,
                "mdate": 1700482418982,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HfZ20A7i5c",
                "forum": "A81iom2Y41",
                "replyto": "xs45oae0ZV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7036/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7036/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nFor the possibility of migrating BEYOND to other domains, within the limited time, we did a simple experiment on adversarial text detection for NLP. Using the text-attack tool  **[RR1]**, we generated 1000 adversarial texts on a BERT model trained on the MR sentiment classification dataset using the TextFooler attack **[RR2]**. We then used the chatgpt_paraphraser tool to generate ten paraphrases for each text as augmentations. Finally, we compare the embedding similarity between the samples and their augmentations on BERT. The AUC of the detection is 73.63%. Although there is certainly room for performance improvement, we believe that this pilot study confirms the possibility that BEYOND can be migrated to other domains.\n\n**[RR1]** Morris J X, Lifland E, Yoo J Y, et al. Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp[J]. arXiv preprint arXiv:2005.05909, 2020.\n\n**[RR2]** Jin D, Jin Z, Zhou J T, et al. Is BERT really robust? A strong baseline for natural language attack on text classification and entailment[C]//Proceedings of the AAAI conference on artificial intelligence. 2020, 34(05): 8018-8025."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7036/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690866652,
                "cdate": 1700690866652,
                "tmdate": 1700690979720,
                "mdate": 1700690979720,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J3aF9kbLy8",
                "forum": "A81iom2Y41",
                "replyto": "HfZ20A7i5c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7036/Reviewer_J3f8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7036/Reviewer_J3f8"
                ],
                "content": {
                    "title": {
                        "value": "Ack"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nthank you for performing this additional proof-of-concept experiment. I will take it into account when discussing the paper with the other reviewers."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7036/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692601297,
                "cdate": 1700692601297,
                "tmdate": 1700692601297,
                "mdate": 1700692601297,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gA4bKqIhco",
            "forum": "A81iom2Y41",
            "replyto": "A81iom2Y41",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7036/Reviewer_qcuA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7036/Reviewer_qcuA"
            ],
            "content": {
                "summary": {
                    "value": "In this study, a simple yet effective adversarial detection method is proposed. This method applies label consistency check and representation similarity check in the embedding space derived by performing contrastive learning over training data samples and their augmented samples. The experimental study involves state-of-the-art adversarial attack and adversarial sample detection methods. Furthermore, it considers both gray and white-box attack scenarios. The results confirm the superior performance of the proposed detection method over the other adversarial sample detection algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1/ The algorithmic design is simple, yet delivering superior adversarial detection performances. It is interesting to use the contrastive learning technique to enlarge the difference between clean samples and adversarially crafted samples. More specifically, the distance between a clean sample and its augmented samples is much smaller than an adversarially perturbed sample and the corresponding augmented variants in the embedding space derived by contrastive learning. This is the first core contribution in this study. \n\n2/ The second core contribution is to make use of the label consistency step to boost the detection accuracy given varying adversarial attack budge levels. \n\n3/ The experiments offer a comprehensive coverage over different attack settings, attack approaches and adversarial sample detection algorithms."
                },
                "weaknesses": {
                    "value": "1/ Theoretical study is not rigorous. The whole study is based on the assumption that data augmentations can effectively weaken adversarial perturbation. However, this is only an empirical observation, yet without any theoretical justification. It is not convincing to set up further deduction based on this hypothesis. \n\n2/ The proposed detection algorithm requires to set many threshold values. In Algorithm.1, three threshold $T_{cos}$, $T_{label}$ and $T_{rep}$ are applied.  The choice of these threshold values are dataset dependent.  This makes the proposed method difficult to be generalized across different datasets / learning scenarios."
                },
                "questions": {
                    "value": "It would be useful to discuss the sensitivity of the thresholds' values over the detection results."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7036/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698621323363,
            "cdate": 1698621323363,
            "tmdate": 1699636826443,
            "mdate": 1699636826443,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eNLC5NtpEG",
                "forum": "A81iom2Y41",
                "replyto": "gA4bKqIhco",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7036/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7036/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express sincere gratitude for your valuable feedback and constructive comments.\n\n## Q1 Theoretical study\n\nFirst, our analyses in the main paper are for the case where augmentation can weaken adversarial perturbations, and it shows that if augmentation can weaken adversarial perturbations, it can be used for adversarial sample detection on the SSL representation space.\n\nWe note that it is difficult to conduct an overall theoretical analysis to verify that data augmentation as a whole can always effectively weaken adversarial perturbations because there are many data augmentations and they have different effects on adversarial perturbations. We address your question as to how to determine whether one given augmentation type weakens the adversarial perturbation or not.  Therefore, we analyze which data augmentation can weaken adversarial perturbations, i.e., what conditions must be met for augmentations to be effective. \n\nEffective data augmentation makes the augmented label $y_{aug}$ tend to the ground-truth label $y_{true}$ and away from the adversarial label $y_{adv}$ as follow:\n\n$ ||y_{aug}-y_{true}||_2 $  \n\n$ \\leq ||y_{aug}-y_{adv}||_2 $\n\n$ \\leq ||y_{adv}-y_{true}||_2 $\n\nSince $y_{true}$ is the one-hot encoding, the range of $ ||y_{adv}-y_{true}||_2 $ is $(\\sqrt{2}/2, \\sqrt{2})$. \n\nThe distance is $\\sqrt{2}$ when the item corresponding to $y_{adv}$ is 1 in the logits of $y_{adv}$, and $\\sqrt{2}/2$ when the item corresponding to $y_{adv}$ and $y_{true}$ both occupy 1/2. Given a SSL-based classifier, $C$, we have:\n\n$\nC(W(x+\\delta)) = C(Wx)+\\nabla C(Wx)W\\delta = y_{true} + \\nabla C(Wx)W\\delta = y_{aug}\n$\n\nTherefore, the distance between $y_{aug}$ and $y_{true}$ is:\n\n$ ||y_{aug}-y_{true}||_2 = ||\\nabla C(Wx)W\\delta||_2 $\n$ \\leq ||\\nabla C(Wx)W||_2 ||\\delta||_2 $\n\n$ \\leq ||\\nabla C(Wx)W||_{2} \\epsilon $\n\nwhere $ ||\\delta||_2 $ is bounded by $ \\epsilon $ according to the definition of adversarial examples. \n\nTo make sure $ ||y_{aug}-y_{true}||_2$\n\n$ \\leq ||y_{aug}-y_{adv}||_2 $, then:\n\n$ ||\\nabla C(Wx)W||_2 \\epsilon \\leq \\frac{\\sqrt{2}}{2} \\Rightarrow  ||\\nabla C(Wx)W||_2 \\leq \\frac{\\sqrt{2}}{{2\\epsilon}} $\n\nIn summary, an augmentation can mitigate adversarial perturbation when it satisfies $||\\nabla C(Wx)W||_2 \\leq \\frac{\\sqrt{2}}{{2\\epsilon}}$.\n\nTo further validate our analysis, we generate 1000 adversarial examples by PGD with $\\epsilon=8/255$ on Cifar10. The following table shows the ratios for different data augmentations meeting the threshold $\\frac{\\sqrt{2}}{{2\\epsilon}}$.\n\n| Aug | Ratio | Aug | Ratio |\n|:---:|:---:|:---:|:---:|\n| Rotation | 99.9% | Vertical | 25.9% |\n| Crop | 40.7% | Color Jitter | 99.0% |\n| Resize | 74.0% | Gray | 40.6% |\n| Horizontal | 25.9% | Compose | 99.7% |\n\nA higher ratio means the augmentation is more effective. Meanwhile, we perform t-sne visualizations of the representations of clean examples and AEs processed by different augmentation methods in Fig. 10 in the latest pdf. As seen in the figure, the effective augmentation methods with the high ratio in the above table can effectively increase the distance between AEs and their neighbors. For example, Rotation has the highest ratio in the above table, and the distance between AE and its neighbors in the figure is larger than that of clean samples. While Horizontal and Vertical have the lowest ratio, and the distance between AE and its neighbors is still close in the above figure. \n\nMoreover, we test the detection performance of high-ratio augmentations and low-ratio augmentations as follows:\n\n|Augmentation|FGSM|PGD|CW|AutoAttack|Average|\n|:---:|:---:|:---:|:---:|:---:|:---:|\n|ColorJitter&Resize&Rotation| 97.11% | 96.55% | 98.15% | 96.56% | 97.09% |\n|Gray&Horizaotal&Crop&Vertical| 92.44% | 91.36% | 94.70% | 91.87% | 92.59% |\n\nIt can be seen that the average detection performance of the effective augmentations obtained by our analysis is 5% higher than that of the other augmentations. \n\nFinally, BEYOND adopts various augmentations instead of a single type to generate multiple neighbors for AE detection. This allows the detection performance to circumvent the effects of bad augmentation methods, which reduces randomness and makes the estimates more robust.\n\n## Q2 Threshold\nThe evaluation metric in our paper is mainly area-under-curve (AUC). BEYOND includes three thresholds. For any threshold set, we compute a True Positive Rate (TPR) and a False Positive Rate (FPR). High AUC on different datasets shows that: 1) BEYOND can get a high TPR when FPR is low; 2) when we choose a bad threshold set that has a high FPR, we also get a high TPR. 3) The sensitivity of BEYOND to the threshold is weaker than baselines. Hence, BEYOND is not threshold-sensitive. \nMoreover, in practice, we can choose a threshold with FPR5% only based on clean examples, which is not allowed for baselines that require adversarial examples as a reference set or training. \n\nOur responses to all questions have been updated to the latest pdf and highlighted in blue."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7036/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700457330444,
                "cdate": 1700457330444,
                "tmdate": 1700457358124,
                "mdate": 1700457358124,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LbY4QzOQGe",
                "forum": "A81iom2Y41",
                "replyto": "eNLC5NtpEG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7036/Reviewer_qcuA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7036/Reviewer_qcuA"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the resposne"
                    },
                    "comment": {
                        "value": "Dear authors, \n\nI appreciate your efforts in addressing my question and I would apologise for my late reply. \n\nI have gone through the rebuttal texts. I feel the theoretical study you offered over the condition of the augmentation still needs further investigation. \n\nIn the inequality \\epsilon \\leq \\frac{2}{2\\|\\nabla{C}(Wx)W\\|_{2}}, it indicates that a larger gradient magnitude of C at Wx requires to reduce the magnitude of the injected augmentation. \n\nLet's assume W is well normalized, so we can ignore the effects imposed by \\|W\\|, the inequality seems to show that it is more difficult to differentiate the impact of injecting an augmentation noise from that caused by adversarial manipulation in highly insensitive area in the input space, i.e. the area with large \\|\\nabla{C}(Wx)\\|. For example, in the extreme case, the augmentation noise that we can impose without changing the label will become marginally small if \\|\\nabla{C}(Wx)\\| becomes very large. In such a case, both adversarial noise and data transformation-based augmentation may cause the drastic output change. Can we say in this scenario that the idea of adding data augmentation to detect adversarial noise fails ? \n\nAll in all, I think the success of this detection-by-augmentation strategy may also be determined by the sensitivity / curvature of the input area where we apply this detection method. \n\nRegarding the threshold question, I understand the threshold values can be chosen by checking the AUC curves. This is the standard protocol that we would also follow in our study. However, my concern is over the situation that this detection algorithm needs three threshold $T_{cos}$, $T_{label}$ and $T_{rep}$, tuning all of them makes the deployment of this work in practices difficulty, more freedom of adjustion, more challenging to maintain / debug the system. \n\nAgain, I thank the authors' efforts in shaping this study -- it is impressive. But from my perspective, it still needs some improvement to make this study more solid for the top venue like ICLR."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7036/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664851089,
                "cdate": 1700664851089,
                "tmdate": 1700664851089,
                "mdate": 1700664851089,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IOyP8BZtMq",
                "forum": "A81iom2Y41",
                "replyto": "gA4bKqIhco",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7036/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7036/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer, \n\nWe deeply appreciate your valuable time and the comprehensive feedback you have provided. However, upon reviewing your response, we have identified some misunderstandings that we would like to address and clarify in relation to our work:\n\n1. The assumption we made in our theoretical analysis regarding the \"benign perturbation, i.e. random noise, $\\hat{\\delta}$, with bounded budgets causing minor variation, $\\hat{\\epsilon}$, on the feature space\" is intuitive and aligns with prior literature **[RR1][RR2][RR3]**. We believe this assumption is reasonable and aligns with established research in the field.\n\n2. As for your concern about ''$|\\nabla{C}(Wx)|$ becoming very large'', we believe that such a scenario is not realistic. This is because the SSL-based classifier $C(\\cdot)$ that we employ has already undergone parameter optimization using \\textbf{gradient descent} during the training process. Therefore, in situations where $C(\\cdot)$ can achieve high accuracy, it is unlikely to encounter a situation where $|\\nabla{C}(Wx)|$ becomes large.\n\n3. Furthermore, it is important to note that the utilization of the SSL model offers the advantage of producing stable features even under common augmentations. This characteristic helps to mitigate the occurrence of extreme cases mentioned earlier to a significant extent. The use of SSL models allows for more robust and reliable performance, thereby enhancing the overall stability of the system.\n\nWe apologize for any misunderstandings that may have arisen. \nFinally Thank you again for the truly splendid discussion, and if you have any further concerns or questions, please do not hesitate to let us know.\n\n**[RR1]** Nesti F, Biondi A, Buttazzo G. Detecting adversarial examples by input transformations, defense perturbations, and voting[J]. IEEE Transactions on neural networks and learning systems, 2021.\n\n**[RR2]** Tian S, Yang G, Cai Y. Detecting adversarial examples through image transformation[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2018, 32(1).\n\n**[RR3]** Xu W, Evans D, Qi Y. Feature squeezing: Detecting adversarial examples in deep neural networks[J]. arXiv preprint arXiv:1704.01155, 2017."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7036/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737322682,
                "cdate": 1700737322682,
                "tmdate": 1700739144241,
                "mdate": 1700739144241,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vUpdqRrNtp",
                "forum": "A81iom2Y41",
                "replyto": "IOyP8BZtMq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7036/Reviewer_qcuA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7036/Reviewer_qcuA"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the follow up discussion"
                    },
                    "comment": {
                        "value": "However, even for a converged classifier, there are still high curvature areas, for which the gradient magnitudes regarding to individual data points remaining high. Usually they correspond to data points difficult to fit by the model (can be rare-class samples or containing natural noise inside). \n\nBesides, there are also vulnerable data points where a slight change to the data points would cause large fluctuation to the decision output. We can use influence score [LiangICML2017] to identify such data points. \n\nLiangICML2017. Liang et al, Understanding Black-box Predictions via Influence Functions, ICML 2017. \n\nEven for SSL, these vulnerable data points exist. Of course, these cases are usually in the long-tailed area of data distribution. However, in these cases, using data augmentation to detect the difference between augmented data points and adversarially perturbation can fail to give a correct answer. The related discussion is absent from the current discussion. I don't see how the current algorithmic design can offer a guarantee to which extent it can handle / avoid such a situation. \n\nAgain, I am not denying the effectiveness of your approach. My concern is over the completeness of the analysis over your approach presented in this paper. If any practitioner uses this approach for their applications, it is important for them to understand the feasibility condition of the proposed method and when/how this method may fail."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7036/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740370576,
                "cdate": 1700740370576,
                "tmdate": 1700740370576,
                "mdate": 1700740370576,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9eJUyKpsT0",
            "forum": "A81iom2Y41",
            "replyto": "A81iom2Y41",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7036/Reviewer_cka9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7036/Reviewer_cka9"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an adversarial detection method called BEYOND, which detects the adversarial examples using label consistency and representation similarity with neighbors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The mathematical analysis is logical and convincing when combining with the proposed detection structure.\n\n2. The conflicting goals for adaptive attacks against the proposed method is original."
                },
                "weaknesses": {
                    "value": "1. The baselines selected in the paper are somewhat old. Using baselines with the same properties such as neighbors and representations is reasonable, while we believe that comparisons with newer methods with or without such properties are necessary, such as SimCLR for catching and categorizing (SimCat) [1] which also use representations effectively and Erase-and-Restore (E&R) [2].\n\n2. The format of citations is incorrect. For example, \"kNN Dubey et al. (2019)\" in Baselines of Section 4 should be \"kNN (Dubey et al., 2019)\".\n\n3. The detection ability for various types of attacks is beneficial for its applications, thus I am concerned about the evaluations of detection the adversarial samples generated by attacks based on different norm.\n\n[1]Moayeri M, Feizi S. Sample efficient detection and classification of adversarial attacks via self-supervised embeddings[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2021: 7677-7686.\n\n[2]Zuo F, Zeng Q. Exploiting the sensitivity of L2 adversarial examples to erase-and-restore[C]//Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security. 2021: 40-51."
                },
                "questions": {
                    "value": "Please see the Weaknesses section.\n\n==============After rebuttal===============\nThe explanations and results provided by authors address most of my concerns. Thus, I am willing to raise the rating score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7036/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7036/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7036/Reviewer_cka9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7036/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757889050,
            "cdate": 1698757889050,
            "tmdate": 1700643465285,
            "mdate": 1700643465285,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qCHR0UQN5A",
                "forum": "A81iom2Y41",
                "replyto": "9eJUyKpsT0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7036/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7036/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express sincere gratitude to your valuable feedback and constructive comments.\n\n## Q1 Baselines\nOur proposed BEYOND focuses on detecting adversarial examples by leveraging the relationship between the input and its neighbors. So, the baseline methods we chose are all related to detectors that are built on the distance between the input sample and its neighbors. And since our method uses the SSL model, we also compare BEYOND with Mao et al, which is a SSL-based adaptive-robust defense method.\n\nMoreover, we also included the suggested baselines SimCat from \u201c**[RR1]**\u201d and  E&R from  \u201c**[RR2]**\u201d. The comparison on ImageNet for CW L2 attack is as follows:\n| AUC(%) | BEYOND | SimCat | E&R |\n|:---:|:---:|:---:|:---:|\n| C&W L2 | **96.32%** | 81.85 | 93.57 |\n\nPlease note that SimCat is not open source, so the corresponding results come from our own replication. Erase-and-Restore (E&R) is only for L2 norm attacks.\nIt can be seen that BEYOND outperforms these two baselines. In addition, both SimCat and E&R require training a detector, which is not required inBEYOND. \nFor adaptive attacks, Table 5 in **[RR1]** shows that as a few-shot detector, SimCat itself is not robust to adaptive attacks, and its adaptive robustness can only be improved by combining a robust model (e.g. Adversarial training model). \nE&R in **[RR2]** evaluated its robustness to adaptive attack (BPDA), but **[RR3]** shows BPDA is not as effective as APGD used in BEYOND.\n\n**[RR1]** Moayeri M, Feizi S. Sample efficient detection and classification of adversarial attacks via self-supervised embeddings[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2021: 7677-7686.\n\n**[RR2]** Zuo F, Zeng Q. Exploiting the sensitivity of L2 adversarial examples to erase-and-restore[C]//Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security. 2021: 40-51.\n\n**[RR3]** Francesco Croce, Sven Gowal, Thomas Brunner, Evan Shelhamer, Matthias Hein, and Taylan Cemgil. Evaluating the adversarial robustness of adaptive test-time defenses. In International Conference on Machine Learning, pp. 4421\u20134435. PMLR, 2022\n\n## Q2 Citation Format\nThanks for your careful check, we have corrected the citation format in the latest pdf. \n\n## Q3 Various Types of Attacks\nYour concern about robustness to various types of attacks is warranted. Following the reviewer\u2019s comments we test the most representative method that supports multiple norm attacks, AutoAttack. AutoAttack supports $L_{\\infty}$, $L_2$ and $L_1$ norm attacks. In the main paper, we only report the detection performance of BEYOND against AutoAttack $L_{\\infty}$. The following table shows the performance of BEYOND against AutoAttack with different norms.\n\n| AUC(%) | $L_{\\infty}$ | $L_2$ | $L_1$ |\n|:---:|:---:|:---:|:---:|\n| Cifar10 | 99.18 | 99.13 | 99.07 |\n| ImageNet | 97.14 | 97.26 | 97.18 |\n\nThe perturbation budgets($\\epsilon$) on Cifar10 are 8/255 ($L_{\\infty}$), 0.5 ($L_2$), and 8 ($L_1$); and on ImageNet are 8/255 ($L_{\\infty}$), 3 ($L_2$), and 64 ($L_1$). The result shows BEYOND is still effective against attacks based on different norms.\n\nOur responses to all questions have been updated to the latest pdf and highlighted in blue."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7036/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700449570398,
                "cdate": 1700449570398,
                "tmdate": 1700449570398,
                "mdate": 1700449570398,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QjvM6uKy5t",
                "forum": "A81iom2Y41",
                "replyto": "qCHR0UQN5A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7036/Reviewer_cka9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7036/Reviewer_cka9"
                ],
                "content": {
                    "title": {
                        "value": "Response to authos"
                    },
                    "comment": {
                        "value": "Dear authors\n\nThanks a lot for your careful responses. I believe the provided explanations and results address most of my concerns. Thus, I will raise the rating score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7036/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643239328,
                "cdate": 1700643239328,
                "tmdate": 1700643239328,
                "mdate": 1700643239328,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "waiLuiNJWG",
            "forum": "A81iom2Y41",
            "replyto": "A81iom2Y41",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7036/Reviewer_SDmS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7036/Reviewer_SDmS"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes BEYOND, an adversarial example (AE) detection method which is based on label and representation consistency of augmented neighbor samples using a pretrained SSL model.\n\nThe method builds on ideas from DkNN [A] and LNG [B].\n\n[A] Nicolas Papernot and Patrick McDaniel. Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning. arXiv preprint arXiv:1803.04765, 2018.\n\n[B] Ahmed Abusnaina, Yuhang Wu, Sunpreet Arora, Yizhen Wang, Fei Wang, Hao Yang, and David Mohaisen. Adversarial example detection using latent neighborhood graph. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision, pp. 7687\u20137696, 2021.\n\nThe paper claims that the above-mentioned AE detection methods have limitations. Some AEs required to build the graph, and they cannot generalize to unseen attacks. They can be bypassed by adaptive attacks.\n\nThere's a theoretical analysis provided in the paper which explains the reasoning behind the applicability of the core idea of the paper. The conclusion of the analysis is that the imperceptible perturbation \u03b4 in the image space can be significantly enlarged in SSL\u2019s feature space, and this can be detected by referring to the original image's neighbors.\n\nThe proposed method can be used with Adversarially Trained models and is robust to adaptive attacks. The paper claims that the robustness to adaptive attacks comes from the conflicting optimization goals for the attacker where there is a cancelation of gradients, leading to poor adaptive attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The AE detection method proposed in the paper uses a novel SSL model-based approach. The experiments are thorough to support the claims. The method proposed is robust to adaptive attacks."
                },
                "weaknesses": {
                    "value": "The writing for the experiments section can be substantially improved. The main conclusions from the analysis can be highlighted better in text by shortening details. The figure captions should be made self-contained. It's hard to parse the figures independently of the text."
                },
                "questions": {
                    "value": "1. (Section 2.1) The paper claims, \u201cNote that BEYOND is not based on random data augmentation.\u201d But in Section 4.1, the paper says, \u201cAugmentations BEYOND uses for generating neighbors are consistent with SimSiam, including horizontal flips, cropping, color jitter, and grayscale.\u201d Aren't SimSiam augmentations random? A clarification will be helpful.\n\n2. (Typo) Section 4.1 \u201ca more IMAGENET\u201d\n\n3. (Repeated citation names) \u201cHu Hu et al. (2019)\u201d and \"Mao Mao et al. (2021)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7036/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7036/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7036/Reviewer_SDmS"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7036/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806588221,
            "cdate": 1698806588221,
            "tmdate": 1699636826195,
            "mdate": 1699636826195,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ebIQb22qqd",
                "forum": "A81iom2Y41",
                "replyto": "waiLuiNJWG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7036/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7036/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Weakness:\nThanks for your useful comments, we will adjust the paper to enhance readability. For the figure caption, we have added the necessary details to make it self-contained in the final paper.\n\n## Q1 Augmentation Type \nWe apologize for the confusion. The type of augmentations used by BEYOND is consistent with SimSiam's, including horizontal flips, cropping, color jitter, and grayscale. Since we need to generate multiple different neighbor samples for each input image, we use different combinations of augmentation parameters. However, unlike SimSiam's random augmentations, we fix the random seed to ensure that our method does not benefit from randomization. \nWe have modified the corresponding part to make the method setting clearer. \n\n## Q2 & Q3 Typo and Citation\nSorry for the discomfort caused by typos and repeated citation names, we have carefully proofread our paper.\n\nOur responses to all questions have been updated to the latest pdf and highlighted in blue."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7036/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700449112785,
                "cdate": 1700449112785,
                "tmdate": 1700449112785,
                "mdate": 1700449112785,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]