[
    {
        "title": "It HAS to be Subjective: Human Annotator Simulation via Zero-shot Density Estimation"
    },
    {
        "review": {
            "id": "8VyKuatZPT",
            "forum": "JB3lbDtsFS",
            "replyto": "JB3lbDtsFS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission782/Reviewer_Pupg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission782/Reviewer_Pupg"
            ],
            "content": {
                "summary": {
                    "value": "The papers proposed a meta-learning framework to make Human annotator simulation as a zero-shot density estimation problem, which allows for the generation of human-like annotations for unlabelled data. Moreover, conditional integer flows and conditional softmax flows can account for ordinal and categorical annotations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) The paper is writting and organzation are clearly.\n(2) The idea of adopting meta-learning into human annotations for zero-shot unlabeled data is sensible.\n(3) The experimental results on three real-world human evaluation tasks seems promising."
                },
                "weaknesses": {
                    "value": "(1) The main weakness is the limited compared method in the experiments. In the Tables in the paper, the state-of-the-art meta-learning methods are missing and the latested human annnotation simulators are also disregarded. Please consider add more SOTA methods for comparsion.\n(2) The other weakness is that the costing time is not reported in the paper. Since the authors claimed that the proposed method is effective, the training and test time should be list compared to other SOTA in the experiments. Please consider to add more details here."
                },
                "questions": {
                    "value": "Please see Weakness Part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission782/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758341481,
            "cdate": 1698758341481,
            "tmdate": 1699636005686,
            "mdate": 1699636005686,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VZ6uyrJ7QM",
                "forum": "JB3lbDtsFS",
                "replyto": "8VyKuatZPT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission782/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission782/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Pupg"
                    },
                    "comment": {
                        "value": "We thank Reviewer Pupg for the detailed comments. We address some general concerns in the [general response](https://openreview.net/forum?id=JB3lbDtsFS&noteId=4uz2kSCyhf) and response to your specific comments below:\n\n> (1) The main weakness is the limited compared method in the experiments. In the Tables in the paper, the state-of-the-art meta-learning methods are missing and the latested human annnotation simulators are also disregarded. Please consider add more SOTA methods for comparsion.\n\nHuman annotation simulation (HAS) is a new task proposed in our paper, so there is not, to our knowledge, latest human annotation simulator available - all baselines in our paper are adapted from suitable existing methods from other domains. We could not find suitable existing meta-learning techniques to adapt because our method is the first method that aims to meta-learn density estimators, whereas existing meta-learning methods focus on meta-learning (single-output) predictors. We\u2019d be grateful if the reviewer could give citations to suitable prior meta-learning methods for HAS.\n\n> (2) The other weakness is that the costing time is not reported in the paper. Since the authors claimed that the proposed method is effective, the training and test time should be list compared to other SOTA in the experiments. Please consider to add more details here.\n\nThe computational time cost of all of the methods that have been compared for the four tasks studied is shown in Table (i) - (iv) in general response (2). Please refer to general response for more details and analysis.\n\nWe hope our response resolves your concerns."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission782/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700010822221,
                "cdate": 1700010822221,
                "tmdate": 1700010822221,
                "mdate": 1700010822221,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rbhuOmZaT2",
            "forum": "JB3lbDtsFS",
            "replyto": "JB3lbDtsFS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission782/Reviewer_2YK4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission782/Reviewer_2YK4"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on human annotator simulation (HAS), which is the task of generating human-like annotations for unlabelled inputs. \n\nThe paper proposes a novel meta-learning framework that treats HAS as a zero-shot density estimation problem, which can capture the variability and subjectivity in human evaluation. \n\nThe paper also introduces two new model classes, conditional integer flows, and conditional softmax flows, to handle ordinal and categorical annotations respectively.\n\nThe paper evaluates the proposed method on three real-world human evaluation tasks: emotion recognition, toxic speech detection, and speech quality assessment. The paper shows that the proposed method can better predict the aggregated behaviors of human annotators, match the distribution of human annotations, and simulate inter-annotator disagreements."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper proposes a novel meta-learning framework that treats human annotator simulation (HAS) as a zero-shot density estimation problem, which can capture the variability and subjectivity in human evaluation. The proposed method is evaluated on three real-world human evaluation tasks: emotion recognition, toxic speech detection, and speech quality assessment. The paper shows that the proposed method can better predict the aggregated behaviors of human annotators, match the distribution of human annotations, and simulate inter-annotator disagreements. The paper also introduces two new model classes, conditional integer flows and conditional softmax flows, to handle ordinal and categorical annotations respectively. The proposed method is efficient and capable of generating human-like annotations for unlabelled test inputs. \n\nThe strengths of the paper are:\n\n- The proposed method is capable of generating human-like annotations for unlabelled test inputs with higher accuracy than the baseline methods.\n\n- The proposed method can better predict the aggregated behaviors of human annotators, match the distribution of human annotations, and simulate inter-annotator disagreements.\n\n- The paper introduces two new model classes, conditional integer flows, and conditional softmax flows, to handle ordinal and categorical annotations respectively.\n\n- The paper discusses the ethical implications and potential applications of HAS.\n\n- Training code is available to reproduce the results in this paper.\n\nOverall, this paper presents a novel approach to HAS that can capture the variability and subjectivity in human evaluation. The paper also provides insights into how to handle ordinal and categorical annotations."
                },
                "weaknesses": {
                    "value": "- The proposed method is evaluated on only three human evaluation tasks, which may not be sufficient to generalize the effectiveness of the proposed method to other domains.\n\n- The paper does not compare the proposed method with the latest state-of-the-art methods for HAS. The methods compared in this paper include deep ensemble (Ensemble) (Lakshminarayanan et al., 2017), Monte-Carlo dropout (MCDP) (Gal & Ghahramani, 2016), Bayes-by-backprop (BBB) (Blundell et al., 2015), conditional variational autoencoder (CVAE) (Kingma & Welling, 2014), conditional argmax flow (A-CNF) (Hoogeboom et al., 2021), Dirichlet prior network (DPN) (Malinin & Gales, 2018), Gaussian process (GP) (Williams & Rasmussen, 2006), and evidential deep learning (EDL) (Amini et al., 2020). The most recent method A-CNF was proposed two years ago.\n\n- The paper does not provide a detailed analysis of the robustness of the proposed method.\n\nDespite these limitations, the paper presents a novel approach to HAS that can capture the variability and subjectivity in human evaluation. The paper also provides insights into how to handle ordinal and categorical annotations. However, further research is needed to evaluate the proposed method on more diverse datasets and tasks."
                },
                "questions": {
                    "value": "I am a computer vision researcher, and I do not know the state-of-the-art of the HAS. However, the most recent method A-CNF compared in this paper was proposed two years ago. Is there any other recent work proposed in the past two years?\n\nEnsemble achieves the best performance in Table 1 and Table 2. The proposed method is much more efficient than the ensemble method. Could you please provide a detailed complexity comparison?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA, the proposed method is designed to alleviate ethic problems."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission782/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698778095533,
            "cdate": 1698778095533,
            "tmdate": 1699636005574,
            "mdate": 1699636005574,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2fegLBXy3Z",
                "forum": "JB3lbDtsFS",
                "replyto": "rbhuOmZaT2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission782/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission782/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2YK4"
                    },
                    "comment": {
                        "value": "We are thankful for Reviewer 2YK4\u2019s feedback. We address some general concerns in the [general response](https://openreview.net/forum?id=JB3lbDtsFS&noteId=4uz2kSCyhf) and response to your specific comments below:\n\n> (1) The proposed method is evaluated on only three human evaluation tasks, which may not be sufficient to generalize the effectiveness of the proposed method to other domains.\n\nThe three tasks used to evaluate the proposed method in our paper are real-world tasks from three very representative domains (emotion class annotation, toxic speech detection, and speech quality assessment) that concern human evaluations. In addition, we\u2019ve added an extra emotion attribute prediction task in Appendix K of the revised manuscript, which, instead of estimating discrete emotion labels such as happy, sad, neutral, predicts emotion attributes (i.e. valence \u2013 whether the speaker is positive or negative, arousal \u2013 whether the speaker is excited or calm, dominance \u2013 whether the speaker is weak or dominant). Please also refer to general response for more detail.\n\n> (2) The paper does not compare the proposed method with the latest state-of-the-art methods for HAS...... The most recent method A-CNF was proposed two years ago.\n\nHuman annotation simulation (HAS) is a new task setting proposed in our paper, for which no directly applicable prior method is available. All baselines considered in our paper are adapted from suitable existing methods from other domains. In addition, although we cite (Malinin & Gales, 2018) and (Amini et al., 2020) for DPN and EDL in the main paper, they are actually adapted from variants proposed in recent papers (Wu et al., 2022b) and (Wu et al., 2023) which are cited in Appendix E.\n\n> The paper does not provide a detailed analysis of the robustness of the proposed method.\n\nThe proposed method is evaluated for four different real-world tasks (three in the paper plus one newly added). Results reported in Table 1-3 are the mean and standard error from three independent runs with different random seeds, which shows the robustness of the proposed method.\n\n> Ensemble achieves the best performance in Table 1 and Table 2. The proposed method is much more efficient than the ensemble method. Could you please provide a detailed complexity comparison?\n\nThe complexity of all methods for the three tasks studied in the paper and one newly added task is shown in Table (i)-(iv) in the general response (2). Please refer to general response for more detail and analysis. \n\nIn addition, we\u2019d like to clarify that achieving the highest accuracy in Table 1 and Table 2 doesn\u2019t indicate that the ensemble has the best performance. As explained in Section 2.1, due to subjectivity in human interpretation, deriving a deterministic \u201cground truth\u201d in subjective tasks is not feasible. Therefore, we advocate for modelling the label distribution to account for human perception variability instead of only predicting the majority opinion. Various metrics are adopted in the paper to evaluate the performance of distribution matching and human variability simulation, as described in Section 4. The proposed method outperforms all baselines by yielding the smallest $\\text{NLL}^\\text{all}$ (indicating its superior distribution matching capability) and the smallest  $\\text{RMSE}^s$, $\\mathcal{E}(\\bar{s})$, $\\mathcal{E}(\\hat{\\kappa})$ (indicating its superior simulation of inter-annotator variability).\n\n\nWe hope our response resolves your concerns."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission782/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700010625091,
                "cdate": 1700010625091,
                "tmdate": 1700101568382,
                "mdate": 1700101568382,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v4NxcTkIdS",
            "forum": "JB3lbDtsFS",
            "replyto": "JB3lbDtsFS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission782/Reviewer_js5b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission782/Reviewer_js5b"
            ],
            "content": {
                "summary": {
                    "value": "Supervised learning tasks require annotation that can be done with high certainty, such as annotating the presence of an object, drawing rough bounding boxes, or deciding scene attributes; however, many tasks require subjective labeling that is influenced by a variety of factors, cognitive biases, or personal preferences. This paper proposes a human annotator simulation to incorporate the variabilities in this second group of labeling tasks.\n\nThe method is a meta-learning framework, a zero-shot density estimator that models the agreement and disagreements among human annotators using a latent variable model. It does not require any human effort and can be used to use unlabeled samples efficiently.\n\nThe experimental results are performed on different modalities and domains that demand various levels of subjective annotations, such as emotion category, toxic speech, and speech quality assessment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Various machine learning problems require subjective labeling, and it is not easy to use crowdsourcing due to privacy concerns. The proposed approach makes such highly sensitive data to be labeled and used in learning problems.\n\n* The proposed approach is a latent variable model. $p(z | x)$ encodes the information in the input $x$, however, the interesting part of the formulation is to introduce another intermediate variable $v$, instead of directly taking $p(y | z)$. Conditional normalising flow (CNF) formulation gives more flexibility instead of a particular distribution choice.\n\n* Conditional integer flows and conditional softmax flows are introduced to accommodate to ordinal and categorical annotation tasks."
                },
                "weaknesses": {
                    "value": "* In the experiments, test performances are reported, however, considering the problem as \"simulating subjective human annotations\", I would expect to see (i) a training to learn the latent model in a labeled training subset, (ii) generate simulated labels on a held-out training subset and (iii) training a classifier only with these simulated labels and evaluating on test set. Instead, the current evaluation is more like supervised evaluation.\n\n*  I found the problem address highly similar to the following paper [1,2] that aims to model the label space in each step iteratively using Gibbs sampling (or previous like of work that used MCMC). This work may require annotation process in a more dynamic setting, still very relevant to subjective labelling task and I think, they are needed to discussed.\n\n    [1] Harrison, P., Marjieh, R., Adolfi, F., van Rijn, P., Anglada-Tort, M., Tchernichovski, O., ... & Jacoby, N. (2020). Gibbs sampling with people. Advances in neural information processing systems, 33, 10659-10671. https://proceedings.neurips.cc/paper_files/paper/2020/file/7880d7226e872b776d8b9f23975e2a3d-Paper.pdf.  \n\n    [2] Sanborn, A., & Griffiths, T. (2007). Markov chain Monte Carlo with people. Advances in neural information processing systems, 20. https://papers.nips.cc/paper_files/paper/2007/file/89d4402dc03d3b7318bbac10203034ab-Paper.pdf\n\n* How does the proposed approach tackle the highly imbalanced data domains? For instance, in one of the tasks, MSP podcast dataset contains angry, sad, happy, neutral, and other. When continuous labels (valence-arousal annotations of the same dataset) is used, the skewed labeled distribution will be more visible. I suspect the proposed method to cause higher uncertainty (interrupter disagreement in less discovered part of label parameter space).\n\n* Different performance metrics are used. Particularly, Fleis' kappa for reliability of categorical labels is good. However, why did not you used Intraclass correlation coefficient (ICC) in continuous labels instead of RMSE and the absolute error of the average standard deviations? In continuous/ordinal subjective labelling tasks, ICC reliability is one of the golden standards that define the labelling quality or difficulty of the task."
                },
                "questions": {
                    "value": "Overall, I liked the Human Annotator Simulation approach to tackle learning domains that necessitates subjective labelling. Please see my comments in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review needed. The proposed paper, in contrast, aim to model human annotation processes and mitigate existing biases."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission782/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823650173,
            "cdate": 1698823650173,
            "tmdate": 1699636005494,
            "mdate": 1699636005494,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x1bIikPy6q",
                "forum": "JB3lbDtsFS",
                "replyto": "v4NxcTkIdS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission782/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission782/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer js5b (1)"
                    },
                    "comment": {
                        "value": "We appreciate Reviewer js5b\u2019s positive feedback and insightful comments. We address some general concerns in the [general response](https://openreview.net/forum?id=JB3lbDtsFS&noteId=4uz2kSCyhf) and response to your specific comments below:\n\n> (1) In the experiments, test performances are reported, however, considering the problem as \"simulating subjective human annotations\", I would expect to see (i) a training to learn the latent model in a labeled training subset, (ii) generate simulated labels on a held-out training subset and (iii) training a classifier only with these simulated labels and evaluating on test set. Instead, the current evaluation is more like supervised evaluation.\n\nThe purpose of the HAS system is to simulate annotations that match the distribution of human annotations. Since the underlying annotation distribution is unknown, it\u2019s not easy to directly assess the distribution matching performance by a supervised-learning downstream task. Therefore, we adopted various metrics in the paper to evaluate the distribution estimation performance, as described in Section 4.\n\nAs a concrete example, evaluating the performance of HAS with a classifier, which usually requires the training data to have a single ground truth (i.e. majority vote), is not suitable for our modelling purpose, because there might not exist a single ground-truth label in human evaluation tasks.\n\n> (2) I found the problem address highly similar to the following paper [1,2] that aims to model the label space in each step iteratively using Gibbs sampling (or previous like of work that used MCMC). This work may require annotation process in a more dynamic setting, still very relevant to subjective labelling task and I think, they are needed to discussed.\n\nThanks for pointing out these related works. We agree that delving into the differences between these tasks and our method is enlightening.\n\nPaper [2] developed a MCMC method for sampling from a particular subjective probability distribution which allows a person to act as an element of an MCMC algorithm by choosing whether to accept or reject a proposed change to an object. Paper [1] generalises paper [2] to a continuous-sampling paradigm. Despite both modelling subjective distributions, our method differs from these methods in the following two aspects.\n\nFirstly, these methods require an online annotation process where the annotators need to label data points based on the current state of the Markov chain, whereas in our setting the annotations can be obtained offline. \n\nSecondly, in those traditional density estimation frameworks, distributions are estimated based on observations while our meta-learning method does not require observations to be provided. Extending the notation in the paper, we denote an event as $d_i$, which consists of  a descriptor (i.e. an utterance)  $x_i$ and $M_i$ associated observations (i.e. human annotations) $D_i=\\\\{\\eta_i^{(m)}\\\\}_{m=1}^{M_i}$. For a test event $d^*$, the test descriptor and observations are denoted as $x^*$ and $D^*$. The target to estimate is the distribution of $D^*$, denoted as $p^*$.\n\nGiven an event of interest $d^*$, MCMC-based methods present the descriptor $x^*$ to human participants who are asked to provide sequence of decisions $D^*$ following a Markov Chain Monte Carlo acceptance rule. The distribution $p^*$ is then estimated based on $D^*$. In other words, observations $D^*$ are necessary in order to estimate each subjective probability distribution.  Each Markov chain only targets a specific subjective probability distribution and there is no obvious way to transfer information between different Markov chains. Therefore, these methods cannot be applied to simulate annotation distribution for unlabelled data.\n\nAn advantage of our method is that only the descriptor $x^*$ is needed to simulate the distribution of event $d^*$ and no $D^*$ is needed which are often unavailable in real-world settings. As described at the beginning of Section 3.1, the proposed approach meta-learns a conditional density estimator across $D_{meta}=\\\\{D_i\\\\}_{i=1}^N$. \n\nIn other words, given $\\\\{x_i, D_i\\\\}_{i=1}^N$, the model is trained to learn how to learn the underlying distribution of $D_i$ given $x_i$. During testing,  given the test descriptor $x^*$, the model directly estimates $p^*$ in a zero-shot fashion.\n\nWe\u2019ve added this discussion in Appendix M in the revised manuscript. \n\nThe rest of your comments will be addressed in Response (2)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission782/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700009651469,
                "cdate": 1700009651469,
                "tmdate": 1700009663672,
                "mdate": 1700009663672,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zssOWXbbAE",
            "forum": "JB3lbDtsFS",
            "replyto": "JB3lbDtsFS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission782/Reviewer_szvo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission782/Reviewer_szvo"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of human annotator simulation (HAS) as a density estimation problem, where marginal distribution of how the labels would be generated by a group of annotators given a particular sample is learnt."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors argue that existing methods take a majority vote of among multiple annotators. This is not reflective in case of majority bias, and is instead well captured by treating it as a density estimation problem. Fig 1, 2 clearly show that the predictions made by the proposed CNF algorithm  lie on the sample means and have enough variability to capture the diversity in human annotations."
                },
                "weaknesses": {
                    "value": "[1] The whole motivation of paper is that it should be possible to capture the data bias (when majority of labelled samples are wrong), and correct for it in some way. However, in all of the tables, the ensemble (tab 1,2)/gaussian process(tab 3) seem to be performing better than the proposed cnf method. While it is qualitatively visible that CNF is learning a better density distribution, the quantitative numbers dont justify the intuition. Perhaps, the authors would be well served by explicitly identifying the samples which possess such labelling bias, correct for it (by forcing the distribution to skew), and show better performance than all other methods. Right now, i see such form of analysis as lacking.\n\n[2]  What is the motivation behind latent-diffusion model. in my understanding, the marginal p(y|z) is already encoding information on different annotators z1,z2....z_m sampled over z. what does additional variable v inject into the model conceptually (apart from additional representational capacity).\n\n[3]  What is the reason for separate analysis of ordinal and categorical variables? I understand that ordinal categories are ordered, and that probability estimation then reduces to summing over the continuous space of latent variable v. However, I haven't seen standard classification setups explicitly enforce such ordering constraints. Also are there any cases where annotations are continuous (for eg, annotation by a speech etc.), which could be used to explore continuous cases? That could act as an interesting toy experiment.......\n\n[4] Possible extensions to cases where only single annotation is available for each sample:\n\t- how does this work compare to works which aim to filter noisy labels from the network. perhaps this could be mentioned in the related work. \n\nI like the fresh perspective  of learning distribution over labels, and how such system could act as an auto-labeller. However, the motivation does not reflect better performance on real-world metric (i.e. accuracy). Right now it feels like a fancy technique (i.e. density estimation) whose PRACTICAL real-world experiments i cannot see. Also, i dont see any experiments on zero-shot learning, which is the main title of the paper. \n\nFinally, this paper seems to present a chicken and egg problem. Most of the datasets in the real- world contain labels of only one annotator but might be biased. However, this paper does not work on them. Instead, it requires datasets where each sample has been annotated by many people. But that might not be how labelling happens in real-world. So, perhaps authors could discuss how to extend their method to single label cases where distributions could not be learnt. \n\nHowever, this work shows promise, and I would recommend the authors to improve the paper by addressing the above concerns and consider a future resubmission.\n\n-----\nPOST REBUTTAL\n-----\nThe authors were able to address most of the comments. Three main issues are still open,\n\n[1] The seeming unexplainable negative correlation between RMSE and Accuracy.\n\n-> Standard intuition suggests that lower RMSE should lead to higher accuracy in classification setups. If one accepts, that there are certain annotators (i.e. group of people) who are collectively biased, and that the sample might be incorrectly labelled, then, it should be possible to \u2018correct\u2019 for it. One way, could be perhaps reestimating the correct label and retrain the said classifier and SHOW better RMSE. I very much appreciate the authors efforts to add a new metric ICC to the setup. However, the actual increase on the accuracy (which is a well-accepted metric), would have convinced further and shown applicability on real world setup. Note that i am not asking for additional experiments on datasets which have only one label per sample, but only the performance improvements in the context of experiments the authors have already performed.\n\n[2] Zero shot density estimation.\n\n-> standard learning assumes that the neural net fits a density (which does not change) after the machine has learnt. during inference, a sample (from same/different distribution) is fed to the model and evaluated. The machine cant adapt its learnt density, since that is encoded in the weights which remain static.\n->the authors clarify zero shot as the ability to predict density of annotator responses p*, from a single phrase (x*).\n\t-> this might work, if the annotators (say A1) labelled a particular sample x1 (which was SEEN during training), and network is asked to predict A1\u2019s beliefs for a new sample x* (which has not been seen). However, the problem then does not remain zero shot since A1 was already seen by the network.\n\t-> If we accept (free will), i.e. one\u2019s personal beliefs are independent of statistical treatments of how other people respond, then merely sampling from the learnt distribution of annotator response, to \u2018simulate\u2019 how a prospective unseen annotator shall respond might not work.\n\n[3] Dynamic density adaptation\n\nThe idea of giving  networks the ability to adapt their densities dynamically to if a given sample is OOD seems promising. The only issue is that personal beliefs cant be given a density treatment. If so, then it doesn't remain zero shot.\n\nOverall, this is an interesting work, along above three points, it is not clear in what context this will be useful if it can not be used to improve the performance."
                },
                "questions": {
                    "value": "- relevance to title of the paper.\n\t- paper is titled zero shot density estimation, by zero shot i understand that a sample which is different from the original dataset on which the estimator was fit, could be used. given a set of gt classes, the network should dynamically predict distribution of labels over annotators. However, i see no such experiments, with training/inference being done over SAME dataset of speech, toxicity, and emotion annotations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission782/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission782/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission782/Reviewer_szvo"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission782/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698939144456,
            "cdate": 1698939144456,
            "tmdate": 1701056846184,
            "mdate": 1701056846184,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oyMw9Ob185",
                "forum": "JB3lbDtsFS",
                "replyto": "zssOWXbbAE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission782/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission782/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer szvo (1)"
                    },
                    "comment": {
                        "value": "We appreciate Reviewer szvo's thoughtful comments. We address some general concerns in the [general response](https://openreview.net/forum?id=JB3lbDtsFS&noteId=4uz2kSCyhf) and response to your specific comments below: \n\nFirst, we\u2019d like to clarify that the motivation of the paper is to model label distribution rather than correcting the labelling bias. Human perception and interpretation is subjective. There may not always exist a single \u201cground truth\u201d for tasks that involve human evaluation (i.e., how expressive the synthesised speech is? What\u2019s the correct score for an ICLR paper review?). In those tasks, multiple human annotators are commonly employed to label each sample (e.g., each ICLR submission is assigned at least three reviewers). The provided labels can be different. The difference (\u201cbias\u201d) reflects different human perceptions of the same event. No particular perception is correct nor wrong and the difference is valuable. As stated in Section 2.1, we propose modelling annotators\u2019 subjective interpretations rather than seeking to reduce the variability in annotations by enforcing a single correct answer. \n\n> (1) The whole motivation of paper is that it should be possible to capture the data bias (when majority of labelled samples are wrong), and correct for it in some way. However, in all of the tables, the ensemble (tab 1,2)/gaussian process(tab 3) seem to be performing better than the proposed cnf method. While it is qualitatively visible that CNF is learning a better density distribution, the quantitative numbers dont justify the intuition. Perhaps, the authors would be well served by explicitly identifying the samples which possess such labelling bias, correct for it (by forcing the distribution to skew), and show better performance than all other methods. \n\nAs mentioned in the clarification above, there may not exist a single correct answer for such subjective tasks. We thus frame the task as label distribution estimation. Therefore, accuracy/RMSE is no longer the proper metric for the human annotation simulation (HAS) task as these measures can only measure the majority/mean prediction. In order to better assess the model\u2019s ability to model label distribution, we adopted multiple metrics as discussed in Section 4. \nThe quantitative numbers in Table 1-3 indeed justify that the proposed method outperforms all baselines in label distribution estimation. It produces the smallest $\\text{NLL}^\\text{all}$, indicating better distribution matching, and the smallest  $\\text{RMSE}^s$, $\\mathcal{E}(\\bar{s})$, $\\mathcal{E}(\\hat{\\kappa})$, indicating a superior simulation of inter-annotator variability. Besides, we have added an additional metric ICC following Reviewer js5b\u2019s suggestion, for which our method also outperforms all baselines.\n\n> (2) What is the motivation behind latent-diffusion model. in my understanding, the marginal p(y|z) is already encoding information on different annotators z1,z2....z_m sampled over z. what does additional variable v inject into the model conceptually (apart from additional representational capacity).\n\nRegarding variable $v$. This is because the flow-based model is an invertible transformation, as mentioned in the paragraph above Eqn. (2), which cannot directly transform the continuous latent variable $z$ into discrete output $y$. Therefore, $z$ is first transformed into $v$ (continuous) with a flow and then discretized into $y$ (discrete).\n\n> (3) What is the reason for separate analysis of ordinal and categorical variables? I understand that ordinal categories are ordered, and that probability estimation then reduces to summing over the continuous space of latent variable v. However, I haven't seen standard classification setups explicitly enforce such ordering constraints. Also are there any cases where annotations are continuous (for eg, annotation by a speech etc.), which could be used to explore continuous cases? That could act as an interesting toy experiment.\n\nRegarding separate analysis of ordinal and categorical variables. You are correct that the standard classification setups cannot explicitly enforce ordering constraints. That\u2019s why we conduct separate analysis of ordinal and categorical variables. \n\nOrdinal rating schemes (e.g., the  5-point Likert scale) are commonly employed by human evaluation tasks (e.g.,  the speech quality assessment task in the paper, ICLR review) . Directly applying standard classification setups to ordinal annotations would result in poor performance due to model mis-specification. Therefore, we propose I-CNF to separately handle such cases. \n\nContinuous annotations can be handled by the identity transformation $p(y|v)=\\delta(y-v)$ as discussed at the bottom of page 3. In addition, we\u2019d like to point out that continuous annotations are less common in human evaluation tasks (e.g., ICLR reviewers would not rate papers with a score of 6.12).\n\nThe rest of your comments will be addressed in Response (2)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission782/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700007434175,
                "cdate": 1700007434175,
                "tmdate": 1700007434175,
                "mdate": 1700007434175,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]