[
    {
        "title": "Predicated Diffusion: Predicate Logic-Based Attention Guidance for Text-to-Image Diffusion Models"
    },
    {
        "review": {
            "id": "7z735VPCi0",
            "forum": "EKEcYL7gaf",
            "replyto": "EKEcYL7gaf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3255/Reviewer_YsNP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3255/Reviewer_YsNP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed Predicated Diffusion, a comprehensive framework designed to articulate users' intentions effectively. This approach leverages predicate logic and utilizes pixels within attention maps as fuzzy predicates, with propositions serving as the textual representation. By employing this methodology, it transforms these intentions into a differentiable loss function. The experimental findings demonstrate a heightened faithfulness to the provided prompts. Furthermore, the paper introduces the concept of 'possession failure,' which expands the scope of inquiry to encompass the existence or non-existence of objects and attributes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.By using the Predicate Logic method, this paper easily converts the intentions of prompts into a differentiable loss function, which is a simple, intuitive, and effective method.\n\n2.This paper proposes the term \u201cpossession failure\u201d to describe the situation of missing attributes of prompts in T2I models. It provides a detailed direction for modeling the missing attributes question."
                },
                "weaknesses": {
                    "value": "1. This paper represents an increment in the field, integrating additional predicate logics into the Attend-and-Excite framework. However, it's worth noting that the quality of the generated images appears to be subpar. For instance, Figure 1 illustrates issues such as a blurred bird in the first column, a cat with distorted textures in the second column, and a figure with missing eyes in the last column. These results suggest that the introduction of additional loss functions may have had a detrimental effect on the original model. Thus, a crucial question arises: How can we retain the benefits of the original model while addressing these shortcomings?\n\n2. Despite the proposed method, the problem of \"attribute leakage\" persists. For example, in Figure 3, the model still generates two apples in response to the prompt \"an apple and a lion,\" and this issue remains evident in Figure 4 with the prompt \"a green balloon and a purple clock.\"\n\n3. While this method effectively addresses the \"possession failure\" issue, it primarily focuses on Stable-Diffusion v1.4, rather than the latest SDXL model or the DALLE3 model. As a result, there may be limited instances of \"possession failure,\" prompting a need to evaluate the overall contribution of this paper.\n\n4. The paper conducts four distinct experiments to showcase the effects of integrating predicate logics. However, the question remains: If all predicate logics were learned simultaneously, what impact would this have on the original model's performance? Could it further deteriorate the model's results?\n\n5. The proposed method lacks a discussion of its limitations. It is imperative to address and acknowledge the limitations of this approach in order to provide a comprehensive evaluation.\n\n6. Several typographical errors require attention, such as the statement, \"the existing models rarely fail to generate an object with a specified color.\" In subsection \"One-to-One Correspondence\" of the \"METHOD\" section, it appears that the intended message may be the opposite. Please clarify this statement for greater clarity."
                },
                "questions": {
                    "value": "Please see the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3255/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739186315,
            "cdate": 1698739186315,
            "tmdate": 1699636273902,
            "mdate": 1699636273902,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uJcOhY9xcG",
                "forum": "EKEcYL7gaf",
                "replyto": "7z735VPCi0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3255/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3255/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We hope that our explanation will address your concerns (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive review and valuable suggestions. We are grateful for your recognition of the simplicity, intuitiveness, and effectiveness of our method. To address your concerns and questions, we have attached our responses below and will update the final manuscript accordingly.\n\n**1. This paper represents an increment in the field, integrating additional predicate logics into the Attend-and-Excite framework. However, it's worth noting that the quality of the generated images appears to be subpar. (...) How can we retain the benefits of the original model while addressing these shortcomings? (...)**\n\n**2. Despite the proposed method, the problem of \"attribute leakage\" persists. (...)**\n\nWe understand your concerns regarding the generation quality. In our presentation of results, we consciously avoided cherry-picking only the most visually appealing images. Instead, we chose to showcase both the successes and limitations of our method. Our method does not completely resolve the issues but reduces their occurrences while keeping image quality.\n\nTo objectively evaluate the generation quality, we have performed the CLIP image quality assessment (CLIP-IQA), proposed in (Wang et al., AAAI, 2023). We summarized the results in the following table.\n\nCLIP image quality assessment (higher is better)\n\n| Models               | Experiment (i) | Experiment (ii) | Experiment (iii) |\n|:---------------------|:---------------|:----------------|:-----------------|\n| Stable Diffusion     | 0.761          | 0.756           | 0.762            |\n| Composable Diffusion | 0.764          | 0.757           | --               |\n| Structure Diffusion  | 0.763          | 0.760           | --               |\n| Attend-and-Excite    | 0.766          | 0.761           | 0.760            |\n| SynGen               | --             | 0.750           | --               |\n| Predicated Diffusion | **0.775**      | **0.769**       | **0.765**        |\n\nThis metric evaluates whether the image is closer to the text \"good photo\" or \"bad photo\" in the embedding space, and has been proven to be highly correlated with the human perception of image quality. It does not require the text prompts used for image generation and measures purely the image quality. Predicated Diffusion achieved the best scores in all three Experiments. Therefore, we can conclude that the proposed Predicated Diffusion rather improved the image quality, while SynGen degraded it.\n\nFor the fidelity of generated images to the text prompts, we measured similarities between text prompts and generated images using CLIP and BLIP. The results, presented in Tables III and IV, demonstrate the superiority of our method.\n\n(Wang et al., AAAI, 2023) Jianyi Wang, Kelvin C.K. Chan, Chen Change Loy, Exploring CLIP for Assessing the Look and Feel of Images, AAAI Conference on Artificial Intelligence, 2023.\n\n**3. While this method effectively addresses the \"possession failure\" issue, it primarily focuses on Stable-Diffusion v1.4, rather than the latest SDXL model or the DALLE3 model.**\n\nThe studies of these models are concurrent with ours, making integration challenging. For reference, we have included some images generated using Stable Diffusion XL at the end of the revised manuscript. These additional images demonstrate that Stable Diffusion XL still faces the same issues including the possession failure. Therefore, we believe that our method is helpful even for cutting-edge models.\n\n**4. (...) If all predicate logics were learned simultaneously, what impact would this have on the original model's performance?**\n\nExperiment (i), using Eq. (2), is a subset of Experiment (ii), using Eqs. (2) and (5), or Experiment (iii), using Eqs. (2) and (6). All three cases of Experiment (iv) in Fig. 6 simultaneously used Eqs. (2), (5), and (6) and succeeded in generating a variety of faithful and high-quality images.\n\nNote that, in Experiment (ii), we have already incorporated eight different propositions (as listed below). In Experiment (iv), sometimes more than ten propositions are incorporated, and no particular problems have been observed. Thus, our experiments suggest that even with the simultaneous learning of many propositions, the model's performance remains robust and effective.\n\nEight different propositions: $\\exists x. Dog(x)$, $\\exists x. Cat(x)$, $\\forall x. Dog(x)\\rightarrow Black(x)$, $\\forall x. Black(x)\\rightarrow Dog(x)$, $\\forall x. Cat(x)\\rightarrow White(x)$, $\\forall x. White(x)\\rightarrow Cat(x)$, $\\forall x. Dog(x)\\rightarrow \\neg White(x)$, and $\\forall x. Cat(x)\\rightarrow \\neg Black(x)$"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3255/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699891371967,
                "cdate": 1699891371967,
                "tmdate": 1699892719975,
                "mdate": 1699892719975,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EVLPFOL9QU",
                "forum": "EKEcYL7gaf",
                "replyto": "7z735VPCi0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3255/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3255/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We hope that our explanation will address your concerns (2/2)"
                    },
                    "comment": {
                        "value": "**5. The proposed method lacks a discussion of its limitations.**\n\nThroughout the manuscript, we showcased both the successes and failures of our method in figures, demonstrating that it does not completely resolve the issues but reduces their occurrences. As discussed on page 16, when the backbone, Stable Diffusion, can generate images faithful to the prompt, the additional guidance might disturb the generation process. Although this is not a frequent occurrence, as evidenced by the overall improvement in average CLIP-IQA scores, we acknowledge these instances. Furthermore, our method inherits some of the general limitations of Stable Diffusion. These include challenges like the inability to count objects accurately and a tendency for bias towards typical examples. These issues will be addressed in future work.\n\n**6. Several typographical errors require attention, such as the statement, \"the existing models rarely fail to generate an object with a specified color.\" In subsection \"One-to-One Correspondence\" of the \"METHOD\" section, it appears that the intended message may be the opposite. Please clarify this statement for greater clarity.**\n\nSorry for the confusion. What we intended to express was the following: 'If a text prompt specifies only one object and one color, then the vanilla Stable Diffusion works fine in most cases, and hence the loss function (3) is not needed. Therefore, this section focuses on cases where multiple objects and multiple colors are specified.'\n\nTo eliminate unclear explanations and improve readability, we will restructure and revise the entire manuscript for the final draft."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3255/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699891386225,
                "cdate": 1699891386225,
                "tmdate": 1699892736589,
                "mdate": 1699892736589,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hcoFFc6HHj",
            "forum": "EKEcYL7gaf",
            "replyto": "EKEcYL7gaf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3255/Reviewer_5oLz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3255/Reviewer_5oLz"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduced a novel approach to guide text-to-image diffusion models in order to improve relation consistency within a generative image given a prompt. To this end, the introduced guidance links predicate logic and attention maps of diffusion models. The authors motivated the utilization of logic based on four issues, namely missing objects, unintended mixture of objects, attribute leakage between objects, and possession failure. After introducing the methodology, these issues are used to assess the performance of the proposed guidance and other baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper addresses a currently unresolved issue of text-to-image diffusion models.\n- The issues addressed are well explained and motivated.\n- The implementation of propositions via attention maps is well introduced and well supported with examples, which makes it easy to understand.\n- While the evaluation only considers Stable Diffusion, the approach can be transferred to other diffusion models without the need to adapt parameters and any additional training."
                },
                "weaknesses": {
                    "value": "- While the methodology is well introduced, the experiments lack clarity. \n\t- It is, for example, unclear how the prompts were selected. Are they extracted from existing datasets? \n\t- In the case of experiment 3, why is the similarity metric missing? \n\t- In Tables 3 and 4, is the fidelity corresponding to the rating from the user study? Are the reported values normalized? The authors describe that fidelity is assessed by human evaluators and two automated similarity approaches. However, it is not clear to which column these different metrics correspond. Further, it is unclear how the human ratings were aggregated; how many images were assessed by each human evaluator? What is reported, e.g., majority decision?\n\n- The limitations are not well discussed. E.g., the compute overhead of the additional predicate logic-based guidance is unclear. Compared to, e.g., autoregressive image generative models, diffusion models\u2019 inference time is rather slow. While approaches exist tackling these issues, I assume that the additional guidance introduced increases computation.\n\n- Missing related work:\n\t- Universal Guidance for Diffusion Models. Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, Tom Goldstein. CVPR Workshops 2023.\n\t- SEGA: Instructing Diffusion using Semantic Dimensions. Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, Kristian Kersting. In Proceedings of NeurIPS 2023\n\n\nMinor comment:\n\nTypo: Section 3 second paragraph \"Predicate Logic in Attention Map and Resulting Gauidance\" -> Resulting Guidance"
                },
                "questions": {
                    "value": "Next to the questions raised above:\n\t\n- Can you provide the computation costs you observed in your experiments, especially the additional overhead of using the introduced guidance?\n\t\n- Which Stable Diffusion version is used in the experiments?\n \t\n- You mentioned that the text encoder causes the addressed issues. Did you evaluate your method on diffusion models not relying on the CLIP text encoder and instead using, e.g., a more complex LM such as T5? For example, IF or Stable Diffusion XL? And could the introduced guidance be utilized during training or fine-tuning the text encoder?\n\t\n- Why is the fidelity increasing when using Predicate Diffusion? Is this because of resolving the issues of object mixtures?\n- How were the human ratings aggregated? \n- Can you provide more details on the conducted user study? How many images were assessed by each human evaluator? What is reported, e.g., majority decision? Why are eight raters an appropriate and sufficient number of participants?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3255/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756984929,
            "cdate": 1698756984929,
            "tmdate": 1699636273822,
            "mdate": 1699636273822,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "c4aI5NZ5eJ",
                "forum": "EKEcYL7gaf",
                "replyto": "hcoFFc6HHj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3255/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3255/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you very much for your important suggestions (1/2)"
                    },
                    "comment": {
                        "value": "Thank you very much for your detailed review and important suggestions. We are very pleased that you find our target issues well explained and motivated and the implementation well introduced and supported. To address your concerns and questions, we have attached our responses below and will update the final manuscript accordingly.\n\n**It is, for example, unclear how the prompts were selected. Are they extracted from existing datasets?**\n\nWe explained that in Appendix A.2. In Experiments (i) and (ii), we followed the experimental settings used in the Attend-and-Excite paper, with minor modifications. Roughly speaking, we randomly selected nouns and adjectives from candidates for every generation. The SynGen paper also conducted similar experiments. In Experiments (iii), we manually selected prompts to ensure diversity.\n\n**In the case of experiment 3, why is the similarity metric missing?**\n\nWe initially considered that automatic evaluations were not suitable for our self-prepared prompts. However, to improve clarity and objectivity in Experiment (iii), we will incorporate the following table that includes the similarity metrics and quality measures (CLIP-IQA). The results demonstrate that our method achieved better similarities and quality than comparison methods.\n\n\n| Models               | similarity            | quality (CLIP-IQA) |\n|:---------------------|:----------------------|:-------------------|\n| Stable Diffusion     | 0.320 / 0.811         | 0.762              |\n| Attend-and-Excite    | 0.334 / 0.843         | 0.760              |\n| Predicated Diffusion | **0.345** / **0.855** | **0.765**          |\n\nCLIP-IQA was proposed in Jianyi Wang, Kelvin C.K. Chan, Chen Change Loy, Exploring CLIP for Assessing the Look and Feel of Images, AAAI Conference on Artificial Intelligence, 2023.\n\n\n**In Tables 3 and 4, is the fidelity corresponding to the rating from the user study? (...) However, it is not clear to which column these different metrics correspond. (...) How were the human ratings aggregated?**\n\nWe appreciate your suggestion for clarity. We will add appropriate headers to Tables 3 and 4 for readability.\n\nEach image was evaluated by a single human evaluator. This approach was chosen to prioritize evaluating a larger number of images rather than stabilizing the evaluation of a single image. We assigned 3, 3, and 2 human evaluators for Experiments (i), (ii), and (iii), respectively. Each evaluator assessed the same number of images; e.g., images generated using 133-134 prompts and five models in Experiment (ii).\n\n**The limitations are not well discussed. E.g., the compute overhead of the additional predicate logic-based guidance is unclear.**\n\nThe computational overhead of attention guidance methods (Attend-and-Excite, SynGen, and ours) is approximately 25% of the main Stable Diffusion process. This holds regardless of the number of loss functions used. Typically, Stable Diffusion employs classifier-free guidance, needing the computation of the full U-Net with and without a prompt. Therefore, the total computational cost for Stable Diffusion is double that of U-Net. Since Stable Diffusion already generates attention maps during its reverse process, attention guidance methods can use these pre-existing maps to define loss functions, incurring negligible additional computational cost. To update the image, the gradient of the total loss function is backpropagated through the first half of U-Net, as the attention mechanism is located in the middle layer. This process incurs a computational cost about half that of U-Net. Given these considerations, we can estimate the additional computational overhead introduced by attention guidance methods to be around 25%.\n\nFurthermore, our method inherits some of the general limitations of Stable Diffusion. These include challenges like the inability to count objects accurately and a tendency for bias towards typical examples. These issues will be addressed in future work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3255/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699891052008,
                "cdate": 1699891052008,
                "tmdate": 1700247425185,
                "mdate": 1700247425185,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "A5xcgKtLA9",
            "forum": "EKEcYL7gaf",
            "replyto": "EKEcYL7gaf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3255/Reviewer_2NoA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3255/Reviewer_2NoA"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the misalignment between image and text in text-to-image generation. The paper proposes a framework that represents the input text prompt using predicate logic. The attention weight of each pixel is then considered as a continuous value that indicates the level of fulfillment of a pixel for a specific proposition. The intermediate image at each denoising step is then updated in order to maximize the level of fulfillment of the input prompt. Experiments show that the proposed method outperforms several baselines on generating more complete objects and objects with correct colors."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper proposes a novel framework for generating images that are faithful to the input text prompt. The framework is generic in that it covers various issues that have been studied in previous works, such as missing objects and mistakenly bonded colors.\n2. The experiments show that the proposed method outperforms existing baselines on four evaluated settings."
                },
                "weaknesses": {
                    "value": "1. Some of the assumptions that are used for representing text prompt as predicate logic do not make sense. For example, the prompt \"There is a black dog\" is interpreted as \"There is a dog\" AND \"All dogs are black,\" which won't work for prompts such as \"A black dog and a white dog.\" Similarly, prompts that have possession relationships such as \"a man holding a bag\" is interpreted as \"all pixels of the bag is also part of the man,\" which is not necessarily correct.\n2. The proposed optimization method will not guarantee that all predicates are satisfied. When multiple predicates exist in the text prompt, their conjunction is used as the objective function. However, since this is a multi-objective optimization problem, the optimization used in the paper is not guaranteed to find optimal solution for all predicates.\n3. The visualized images in the paper seem to not have as good quality as the baselines. No metrics (either automatic ones such as FID or subjective evaluations) are reported in the paper."
                },
                "questions": {
                    "value": "1. Should $P(x) \\rightarrow Q(x)$ be $1-A_P[i] \\times (1-A_P[i] \\times A_Q[i])$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3255/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3255/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3255/Reviewer_2NoA"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3255/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794655495,
            "cdate": 1698794655495,
            "tmdate": 1699636273753,
            "mdate": 1699636273753,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E2ejMvZSkA",
                "forum": "EKEcYL7gaf",
                "replyto": "A5xcgKtLA9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3255/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3255/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We hope that our explanation will help you understand (1/2)"
                    },
                    "comment": {
                        "value": "Thank you very much for your detailed review and important suggestions. We are very pleased that you find our method novel and generic. To address your concerns and questions, we have attached our responses below and will update the final manuscript accordingly.\n\n**1. Some of the assumptions that are used for representing text prompt as predicate logic do not make sense. For example, the prompt \"There is a black dog\" is interpreted as \"There is a dog\" AND \"All dogs are black,\" which won't work for prompts such as \"A black dog and a white dog.\"**\n\nThe prompt \"There is a black dog\" can be interpreted as \"There is a dog\" AND \"All dogs are black,\" only if it is the entire prompt.\nIf your prompt includes other statements, you may require additional considerations. For the prompt \"A black dog and a white dog,\" you can apply the loss function for one-to-one correspondence using the proposition $(\\forall x. Dog_1(x)\\leftrightarrow Black(x))\\land (\\forall x. Dog_2(x)\\leftrightarrow White(x))$. Here, we represent the first and second occurrences of the word \"dog\" as $Dog_1$\u200b and $Dog_2$\u200b, respectively. However, due to the limitations of the attention mechanism, the attention maps for $Dog_1$ and $Dog_2$\u200b may overlap, causing a failure to differentiate between the two dogs.\nThis issue is not an intrinsic flaw of our method but reflects a broader challenge: diffusion models still struggle to accurately capture the number of instances, and addressing this remains an unresolved question in the field.\n\n**1. Similarly, prompts that have possession relationships such as \"a man holding a bag\" is interpreted as \"all pixels of the bag is also part of the man,\" which is not necessarily correct.**\n\nWe would like to clarify that our method does not strictly enforce \"all pixels of the bag is also part of the man.\" We employed fuzzy logic, specifically product fuzzy logic, to capture tendencies rather than absolute properties. The attention maps are typically lower in resolution compared to the original image, which allows slight displacements in object positioning. This flexibility is evident in examples such as \"a frog wearing a hat\" or \"a girl holding a suitcase\" shown in Fig. A4. In these cases, the objects are close to but do not entirely overlap with their possessors. Additionally, our results in Fig. A8 demonstrate that the operator $\\rightarrow$ more accurately represents the possession relationship compared to $\\leftarrow$ or $\\land$, further supporting the validity of our method.\n\n**2. The proposed optimization method will not guarantee that all predicates are satisfied.**\n\nIndeed, it is true, but not a problem. The reverse process of the diffusion model is similar to gradient descent in finite steps, which may converge to a local minimum or stop early. However, a local minimum is often sufficient to improve the fidelity and quality of the generated images. The challenge arises with poor initial values, which can lead to poor local minima. To mitigate this, we have introduced an iterative refinement at the beginning of the reverse process, as detailed in Appendix A.1. This process helps adjust the initial values for better outcomes. While we acknowledge that more advanced optimization methods are helpful, the current approach does not limit the performance of our method."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3255/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699890886620,
                "cdate": 1699890886620,
                "tmdate": 1699892579985,
                "mdate": 1699892579985,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EVOAz1xBVz",
                "forum": "EKEcYL7gaf",
                "replyto": "A5xcgKtLA9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3255/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3255/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We hope that our explanation will help you understand (2/2)"
                    },
                    "comment": {
                        "value": "**3. The visualized images in the paper seem to not have as good quality as the baselines. No metrics (either automatic ones such as FID or subjective evaluations) are reported in the paper.**\n\nThank you for your valuable suggestion. Fr\u00e9chet Inception Distance (FID) requires a dataset of real images, so it is unavailable for our Experiments, which do not have corresponding ground truth images. Instead, to objectively evaluate the generation quality, we have performed the CLIP image quality assessment (CLIP-IQA), proposed in (Wang et al., AAAI, 2023). We summarized the results in the following table.\n\nCLIP image quality assessment (higher is better)\n\n| Models               | Experiment (i) | Experiment (ii) | Experiment (iii) |\n|:---------------------|:---------------|:----------------|:-----------------|\n| Stable Diffusion     | 0.761          | 0.756           | 0.762            |\n| Composable Diffusion | 0.764          | 0.757           | --               |\n| Structure Diffusion  | 0.763          | 0.760           | --               |\n| Attend-and-Excite    | 0.766          | 0.761           | 0.760            |\n| SynGen               | --             | 0.750           | --               |\n| Predicated Diffusion | **0.775**      | **0.769**       | **0.765**        |\n\n\nThis metric evaluates whether the image is closer to the text \"good photo\" or \"bad photo\" in the embedding space, and has been proven to be highly correlated with the human perception of image quality. It does not require the text prompts used for image generation and measures purely the image quality. Predicated Diffusion achieved the best scores in all three Experiments. Therefore, we can conclude that the proposed Predicated Diffusion rather improved the image quality, while SynGen degraded it.\n\nFor the fidelity of generated images to the text prompts, we measured similarities between text prompts and generated images using CLIP and BLIP. The results, presented in Tables III and IV, demonstrate the superiority of our method.\n\n(Wang et al., AAAI, 2023) Jianyi Wang, Kelvin C.K. Chan, Chen Change Loy, Exploring CLIP for Assessing the Look and Feel of Images, AAAI Conference on Artificial Intelligence, 2023.\n\n**Q1. Should $P(x)\\rightarrow Q(x)$ be $1-A_P[i]\\times (1-A_P[i]\\times A_Q[i])$?**\n\nYour expression aligns with classical Boolean logic but not product fuzzy logic. Yours can derive from the logical expression $\\neg(P(x)\\land(\\neg(P(x)\\land Q(x))))$, whereas ours derived from $\\neg(P(x)\\land \\neg Q(x))$. These two expressions are equivalent in classical Boolean logic, but they yield different outcomes under product fuzzy logic. Our expression is based on the strong conjugation and material implication of the product fuzzy logic, as mentioned on page 4, and consistent with the theory underlying product fuzzy logic."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3255/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699890911850,
                "cdate": 1699890911850,
                "tmdate": 1699892595769,
                "mdate": 1699892595769,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WmiqVJgT0D",
            "forum": "EKEcYL7gaf",
            "replyto": "EKEcYL7gaf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3255/Reviewer_h1q7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3255/Reviewer_h1q7"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Predicated Diffusion which combines predicate logic with the intuition of cross-attention layers in diffusion-based text-to-image. The paper draws connections between several propositions and attention map operations. Language prompts can be seen as a combination of these propositions and have corresponding loss functions that can be optimized in the diffusion process.  Experimental results show that the method outperforms several baselines including a recent SOTA method in this direction."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method is novel. It is very interesting to see how first-order logic can be connected to compositionality in text-to-image generation, specifically the attention maps. Some of the propositions and losses are reasonable and interpretable. \n- The proposed method tackles a wide range of problems, including well-studied ones and also an underaddressed problem, i.e. possession failure. \n- Experimental results show that the method outperforms previous methods in many aspects."
                },
                "weaknesses": {
                    "value": "- Some of the losses are not intuitive or cannot be easily verified. I am not sure if this is due to the presentation of the method section. For example, how does eq (2) prevent the two objects from highlighting the same pixels or regions? It would be better to give straightforward intuition behind the equations in terms of the behavior of attention maps. For example, if I understand correctly, eq 6 encourages the attention maps of \"bag\" to be partially overlapped with the attention maps of \"man\" yet does not force all pixels of \"bag\" to be part of the \"man\". \n- Predicated Diffusion requires manual or pre-defined use of different propositions for different prompts. As stated in Sec. 4, the authors applied different losses for different types of prompts. However, this is not practical for applications where prompts can be arbitrary. The authors manually extracted propositions for each prompt in Experiment (iv) which, I think, really downgrades the overall value of the work. Is there an automatic way to extract propositions for each prompt?\n- Writing could be improved. I think Sec. 3 could be improved in structure and contents. Some paragraphs have too many logical equations that make them a bit hard to follow. Perhaps the authors could find a more organized way to explain every proposition (e.g. start with a simple derivation of logic equations, then provide the attention equation, and finally give some intuition in words. ). The authors could attempt to group contents into subsections to illustrate propositions from easy ones to hard ones and distinguish the novel propositions over A&E or SynGen. There are other trivial flaws like using \"Experiment (x)\" in tables/captions without specifying the experiment domain, making it hard to follow. \n\nWhile I really like the novelty and perspectives presented by the work, there are major weaknesses. I will adjust my rating accordingly depending on how well these concerns are resolved."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3255/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3255/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3255/Reviewer_h1q7"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3255/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698865764491,
            "cdate": 1698865764491,
            "tmdate": 1699636273689,
            "mdate": 1699636273689,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9ItCChu14c",
                "forum": "EKEcYL7gaf",
                "replyto": "WmiqVJgT0D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3255/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3255/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you very much for your valuable suggestions."
                    },
                    "comment": {
                        "value": "Thank you very much for your detailed review and valuable suggestions. We are very pleased that you recognize the novelty and perspectives. To address your concerns and questions, we have attached our responses below and will update the final manuscript accordingly.\n\n**Some of the losses are not intuitive or cannot be easily verified.**\n\nSorry for our unclear explanation. Please allow me to respond as follows.\n\n> **how does eq (2) prevent the two objects from highlighting the same pixels or regions?**\n\nThe attention maps are generated through softmax operations, which inherently prevent the two objects from highlighting the same pixels or regions. When one object's attention map is activated, it naturally suppresses overlapping regions in other objects' maps. We will clarify this in the implementation details of the revised manuscript.\n\n> **eq 6 encourages the attention maps of \"bag\" to be partially overlapped with the attention maps of \"man\" yet does not force all pixels of \"bag\" to be part of the \"man\".**\n\nEq. (6) DOES force all pixels of \"bag\" to be part of the \"man\".\nIf there exists a pixel of \"bag\" ($Bag(x)=True$), our method enforces the same pixel represents \"man\" ($Man(x)=True$) or suppresses the response to the word \"bag\" ($Bag(x)=False$).\nIf a pixel does not respond to \"bag\" ($Bag(x)=False$), our method does nothing.\nPlease refer to the following True-False table:\n\n| Bag | Man | Bag->Man |\n|:----|:----|:---------|\n| F   | F   | T        |\n| F   | T   | T        |\n| T   | F   | F        |\n| T   | T   | T        |\n\nDue to the softmax operations mentioned above, the pixel intensities in both attention maps never reach 1.0 but instead converge to moderate values, as shown in Fig. A1. In practice, Eq. (6) encourages overlap but does not completely enforce it. Our goal is to guide the image generation process to fulfill the intended meaning; this implementation is deemed sufficient for that purpose.\n\n**Predicated Diffusion requires manual or pre-defined use of different propositions for different prompts. (...) Is there an automatic way to extract propositions for each prompt?**\n\nYes, one can employ a syntactic dependency parser to automatically identify key words. For validation, we used spaCy v3.0 to extract:\n\n- Nouns for propositions of concurrent existence in Eq. (2),\n- Modifier-noun pairs for propositions of one-to-one correspondence in Eq. (5), and\n- Subject-object pairs connected by verbs indicating possession (have, hold, grasp, and wear, in this experiment) for propositions of possession in Eq. (6).\n\nFor example, from the prompt \"Woman wearing a black coat holding up a red cellphone,\" we extracted:\n\n- Nouns: Woman, coat, cellphone\n- Modifier-noun pairs: [black, coat], [red, cellphone]\n- Subject-object pairs: [Woman, coat], [Woman, cellphone]\n\nThese results enabled us to automatically create propositions using a simple script.\nIn Figs. 6 and A5, we successfully extracted all propositions (shown below the images) from the prompts, with two exceptions: the possession relationships in prompts \"A black bird with a red beak\" and \"A white teddy bear with a green shirt and a smiling girl.\" In these cases, the preposition \"with\" expresses possession, but it can express merely existence in other cases. Hence, some prompts need more advanced syntactic analysis. Nonetheless, our findings generally support the sufficiency of simple syntactic analysis.\n\nFurthermore, we believe it is crucial for users to explicitly use predicate logic to clarify their intentions that cannot be fully expressed in text. The meaning of \"with,\" whether indicating a possession relationship or merely concurrent existence, can sometimes be ambiguous even for human readers. By employing our proposed method, users can clearly express their intentions and eliminate such ambiguities.\n\n**Writing could be improved.**\n\nThank you for your valuable suggestion. To improve the manuscript, we plan to restructure Section 3. We will group the content into subsections and provide supportive descriptions for tables and captions. Due to page limitations, some content will be moved to the Appendix."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3255/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699890739271,
                "cdate": 1699890739271,
                "tmdate": 1699891557745,
                "mdate": 1699891557745,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]