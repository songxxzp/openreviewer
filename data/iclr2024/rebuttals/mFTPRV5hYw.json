[
    {
        "title": "Where have you been? A Study of Privacy Risk for Point-of-Interest Recommendation"
    },
    {
        "review": {
            "id": "7Alfn60AHn",
            "forum": "mFTPRV5hYw",
            "replyto": "mFTPRV5hYw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2078/Reviewer_KUtq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2078/Reviewer_KUtq"
            ],
            "content": {
                "summary": {
                    "value": "The paper emphasizes the importance of assessing privacy risks in mobility data-based ML models. The authors propose a threat model for evaluating privacy risks and provide a comprehensive privacy risk assessment for such models. They also suggest a privacy-preserving solution for point-of-interest recommendation models to mitigate privacy risks. Their contributions include designing four different attacks, such as common location extraction (LOCEXTRACT), training trajectory extraction (TRAJEXTRACT), location-level membership inference attack (LOCMIA), and trajectory-level membership inference attack (TRAJMIA), developing a privacy-preserving training method that protects against data extraction and membership inference attacks aimed at point-of-interest recommendation models. Overall, the authors identify potential privacy risks in mobility data-based machine learning models and propose solutions to address these risks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The paper presents a privacy attack suite that is specifically designed for POI recommendation models. This suite consists of data extraction and membership inference attacks. By conducting experiments with real-world mobility datasets, the authors demonstrate the vulnerability of current POI recommendation models to these attacks.\n\n2) The paper investigates the effectiveness of existing defense mechanisms, such as L2 regularization and differential privacy, against the proposed attacks. However, it concludes that these mechanisms have limitations in providing comprehensive protection.\n\n3) The impact of training and attack parameters on attack performance is analyzed. The effect of training epochs on information leakage and the influence of query timestamps on data extraction attack performance are discussed."
                },
                "weaknesses": {
                    "value": "1) The study only evaluates the privacy risks of POI recommendation models in a controlled setting. They didn't provide some insights into how these models might perform in real-world scenarios, where more complex factors are at play.\n\n2) The author did not provide insights on how these risks compare to privacy risks associated with ride-sharing or food-delivery apps."
                },
                "questions": {
                    "value": "1) According to the paper, no definitive defense mechanism can protect against all the proposed attacks simultaneously. Can you please explain why this is the case? Additionally, could you provide insights into the challenges that must be addressed to develop more effective defenses against such attacks?\n\n2) How do the proposed attacks and defense mechanisms compare to existing methods in the literature?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2078/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2078/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2078/Reviewer_KUtq"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2078/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698403692507,
            "cdate": 1698403692507,
            "tmdate": 1699636139915,
            "mdate": 1699636139915,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nOyVhpoNcF",
                "forum": "mFTPRV5hYw",
                "replyto": "7Alfn60AHn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2078/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2078/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer KUtq,\n\nWe are grateful for the time you took to review our paper and for your valuable feedback. We would like to present our explanations and responses to your concerns below!\n\n__W1 Only Evalutes the Privacy Risk in A Controlled Setting But Not Real-World Scenarios__  \nOur attack is performed on real-world data sets collected by leading social media companies. Since the real-world POI recommendation models are also trained on a similar data source, we believe that our attack on the current dataset should be able to simulate realistic scenarios where the attack might happen.\n\nAlso, as mentioned in *Section 2.2*, our attacks do have real-world effects, majorly in two cases where the adversary should have the abilities mentioned in our paper: \n\n(1) In a scenario mentioned by service providers [1,2,3], user data is collected and used to train models, which are then provided as an API to third parties for application development. In this context, a malicious third party might gain access to the confidence scores from the model's output\n\n(2) In a second scenario, a situation arises on the service provider side during the retention period of the training data expires. Still, the model owner retains the model, and an adversary (e.g., a malicious insider of location service providers) can aim to use the model to perform our attack aiming to infer information about the data that has been deleted.\n\nAs discussed in the *\u201cLimitation and Future Work\u201d, Section 6*, we also plan to adapt our attack method to some public real-world services with more limitations (i.e. Map APIs which might only provide the predicted label) using methods like label-only attacks.\n\n\n__W2 Comparison to Privacy Risks Associated With Ride-sharing or Food-delivery Apps__  \nThe prior ride-sharing and food-delivery problems mostly lie in solving the different problems of different privacy stakeholders compared to the POI Recommendation Models. \n\nFirstly, prior ride-sharing and food-delivery research targets the following problems: \n\n(1) The privacy of company-owned data. For example, Lyft and Uber can use the spatial distribution information of their instrumented fleets for better placement and routing [4]. \n\n(2) Privacy attacks targeting passengers or companies' service providers, such as drivers. For instance, attackers could steal continuous trajectories and analyze behaviors. However, our paper focuses on attacking a different data format: the user's POIs in location formats, which can be combined into sparse trip trajectories closely related to the user's daily life.\n\nSecondly, the defense mechanisms are different. The studies of Ride-sharing or Food-delivery primarily focus on the data aggregation process, with most relying on secure system design utilizing methods like cryptographic approaches or differential privacy. Our attacks, however, target the privacy leakage of deep learning models, representing a new attack surface. The defense mechanisms for ride-sharing or food delivery are not suitable to defend against our attack.\n\n__Q1 Explanation for Defense Mechanism Not Working and Insights Into More Effective Defenses__  \nThanks for asking this question. This is a great question. We have listed the results of commonly used defense mechanisms in *Appendix E*. These defense mechanisms(e.g. differential privacy), often result in significant drops in utility, primarily due to the nature of the task. The reason is that the utility of existing POI recommendation models heavily depends on memorizing user-specific trajectory patterns from limited user-level data, which lacks sufficient semantic information as in CV/NLP. However, defense mechanisms like differential privacy can diminish the level of memorization.\n\nWe believe that to solve this problem, we need a better-designed scheme that can help with generalizing the POI recommendation model. For example, more information in the dataset, such as POI categories or trip purposes, should be included, and this information should be better utilized. Also, we believe that using a pre-trained model on a public dataset and fine-tuning it on a smaller private dataset might help solve this problem. This approach has been successfully used in other tasks in CV [6,7] and NLP [8,9]. This can be a potential future direction, and we plan to work on this in the future."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2078/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700121688362,
                "cdate": 1700121688362,
                "tmdate": 1700121717818,
                "mdate": 1700121717818,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GBlFI9uejT",
            "forum": "mFTPRV5hYw",
            "replyto": "mFTPRV5hYw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2078/Reviewer_sUA6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2078/Reviewer_sUA6"
            ],
            "content": {
                "summary": {
                    "value": "This paper offers a privacy attack suite (including data extraction and membership inference attacks) on point-of-interest recommendation models, tailored specifically for mobility data. Experiments are performed on three models trained on two distinct datasets. This suite could in principle be used as a privacy auditing tool."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is very clearly presented and the thoroughness of experimental results is commendable to the point where I am left with view remaining questions. \n\nThe paper demonstrates clear privacy vulnerabilities in POI recommendation systems that should inform future defenses and auditing."
                },
                "weaknesses": {
                    "value": "It would be valuable to understand how attack vulnerability changes with sample size. For very large datasets, where there are generally a larger number of unique users to all locations, does the attack success decline? This theory seems to be somewhat supported by Fig 4. \n\nMore detailed descriptions of datasets size and dimensionality would be valuable to understand whether the emulate real-world production systems. \n\nThe paper could benefit from explicitly contextualizing how attack performance compares to attack performance for other type of data/models (e.g. text & image are referenced)."
                },
                "questions": {
                    "value": "How do we know if the utility privacy trade-off inherent or a limitation of existing DP algorithms? You note the privacy-utility trade-off does not strictly hold in your experiments but can you show that practically acceptable utility and privacy can both be achieved?\n\nCould you explain why having more total check-ins seems to help protect a user against a MIA? This seems surprising in the context of differential privacy where worst-case privacy loss degrades with sensitivity. \n\nHave you explored the feasability of DP synthetic data for this type of application?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2078/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2078/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2078/Reviewer_sUA6"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2078/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698437021321,
            "cdate": 1698437021321,
            "tmdate": 1699636139844,
            "mdate": 1699636139844,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4mCGwc6jGU",
                "forum": "mFTPRV5hYw",
                "replyto": "GBlFI9uejT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2078/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2078/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer sUA6,\n\nWe thank you for your valuable feedback and for finding our work meaningful! We would like to respond to your concerns and improve this paper based on your suggestions.\n\n__W1 Effect of Larger Dataset__   \nThank you for pointing this out. We believe the dataset size does not affect our key insights, as our attacks are derived primarily from the task's inherent characteristics. For example, POIs frequently visited by a user are typically more unique to that user (e.g., home address). These locations are always more susceptible to our attack since they are well memorized by the models as shown in Figure 4.\n\nTo confirm this, we conducted additional experiments using a larger FourSquare dataset from Tokyo, containing about *4x* more check-ins than the NYC dataset. Our LocExtract and LocMIA attacks on models trained on this dataset are still effective, showing that our attack is agnostic to the dataset scale. Due to the time limitation in the rebuttal period, we have not finished the experiment on the trajectory-level attacks but we will add them to our paper when finished.\n\n\n*LocExtract Result:*\n|          | ASR-1 | ASR-3 | ASR-5 |\n|----------|-------|-------|-------|\n|**4SQ(TKY)** | 40.4% | 66.0% | 72.9% |\n| **4SQ(NYC)** | 33.0% | 57.3% | 64.9% |\n\n\n*LocMIA Result:*\n|          | TPR@%10FPR | AUC  | ACC  |\n|:--------:|:----------:|:----:|:----:|\n| **4SQ(TKY)** |     0.72     | 0.88 | 0.81 |\n| **4SQ(NYC)** |     0.74     | 0.80 | 0.87 |\n\n\n__W2  More Descriptions of Dataset__  \nThanks for pointing out this. We have included some dataset-related information in *Table 3 in the Appendix*, and we will make sure we highlight this information further. Our attack uses real-world datasets that are collected from NYC social media users. \n\n__W3  Attack Comparison with Other Types of Data/Models__   \nThanks for pointing this out! We will add a table for a comparison of MIAs here, as the data extraction attack we proposed is specific to POI recommendation models. As mentioned in [1], LiRA can achieve an *8.4% TPR @0.1%FPR* on image classification models trained on the cipher-10 dataset. In our case, the attack achieved a *20+% TPR @0.1%FPR* for LocMIA on all models trained on the FourSqure dataset, but it\u2019s relatively lower for the Gowalla dataset with a *1+%  TPR @0.1%FPR*. As demonstrated in [1], the difference in the attack success rate can be attributed to the level of memorization in the model. \n\nHowever, it should be noted that the current POI recommendation model utility still heavily relies on the memorization of the historical visit of a user, which makes the attack relatively easier and leaves the protection of POI-recommendation models more difficult, as we will discuss in the next question.\n\n__Q1  Reason for the Utility Privacy Trade-off and Potential Practical Solution__  \nThis is a great question/ We have listed the results of commonly used defense mechanisms in *Appendix E*. These defense mechanisms(e.g. differential privacy), often result in significant drops in utility, primarily due to the nature of the task. The reason is that the utility of existing POI recommendation models heavily depends on memorizing user-specific trajectory patterns from limited user-level data, which lacks sufficient semantic information as in CV/NLP. However, defense mechanisms like differential privacy can diminish the level of memorization.\n\nWe believe that to solve this problem, we need a better-designed scheme that can help with generalizing the POI recommendation model. For example, more information in the dataset, such as POI categories or trip purposes, should be included, and this information should be better utilized. Also, we believe that using a pre-trained model on a public dataset and fine-tuning it on a smaller private dataset might help solve this problem. This approach has been successfully used in other tasks in CV [6,7] and NLP [8,9]. This can be a potential future direction, and we plan to work on this in the future."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2078/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700120666224,
                "cdate": 1700120666224,
                "tmdate": 1700120666224,
                "mdate": 1700120666224,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FJX8fSqZsp",
                "forum": "mFTPRV5hYw",
                "replyto": "GBlFI9uejT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2078/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2078/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "__Q2  Explanation Why More Total Check-ins Help Protect a User From MIA__  \nThanks for asking! Sensitivity in DP measures the maximum difference between samples in a dataset. When this measurement is at the user level, a user with more check-ins can be more easily distinguished from others, as suggested in [2]. However, this kind of user-level inference attack might not be suitable for this task. The POI recommendation model takes the user ID as input. If such a user does not exist in the dataset, there will be no query results.\nRather than focusing on a user-level inference attack, our attack aims to infer the membership of individual POIs or trajectories. In this context, if a user has visited many locations, the model may have lower confidence in predicting a single POI, leading to less distinction in confidence scores between members and non-members. This scenario can be likened to the idea that hiding a location among a large number of locations is easier than concealing it among a few. Consequently, the more locations a user has visited, the lower the performance of the attack.\n\n__Q3 Feasibility of DP Synthetic Data for POI Recommendation__   \nSynthetic data with DP guarantee is a direction that is worth more investigation. There are some existing studies that discuss the use of synthetic data to protect mobility information, with a primary focus on safeguarding continuous trajectories. For instance, [3] explores the use of differentially private synthetic data to protect POIs. However, they do not address the utility and performance of POI recommendation tasks, as their main target is the private release of data.\n\nOur paper mostly focuses on measuring the privacy leakage from the POI recommendation models. However, Synthetic data generation might require different threat models and attack settings, Thus, we do not include them in our paper. Note that privacy attack for synthetic data is a promising direction that needs independent work to investigate [4, 5]. As a future work, it is interesting to see if the privacy attacks in these works can be applied to mobility data synthetic models like [3]. \n\n__References:__  \n[1] Carlini et al. Membership Inference Attacks From First Principles.  \n[2] Montjoye et al. Unique in the crowd: The privacy bounds of human mobility.  \n[3] Rao et al. LSTM-TrajGAN: A Deep Learning Approach to Trajectory Privacy Protection.  \n[4] Breugel et al. Membership Inference Attacks against Synthetic Data through Overfitting Detection.  \n[5] Hyeong et al. An Empirical Study on the Membership Inference Attack against Tabular Data Synthesis Models.  \n[6] Yu et al. ViP: A Differentially Private Foundation Model for Computer Vision.  \n[7] Sander et al. Tan without a burn: Scaling laws of dp-sgd.  \n[8] Anil et al. Large-scale differentially private bert.  \n[9] Yu et al.  Differentially private fine-tuning of language models."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2078/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700121039117,
                "cdate": 1700121039117,
                "tmdate": 1700122010072,
                "mdate": 1700122010072,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OMo9Lm3H5G",
            "forum": "mFTPRV5hYw",
            "replyto": "mFTPRV5hYw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2078/Reviewer_1v9T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2078/Reviewer_1v9T"
            ],
            "content": {
                "summary": {
                    "value": "This paper evaluates the privacy risks of POI recommendation models by introducing an attack suite and conducts extensive experiments to demonstrate the effectiveness of these attacks. Additionally, it analyzes which types of mobility data are vulnerable to the proposed attacks and further adapts two mainstream defense mechanisms to the task of POI recommendation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. The scenario of this paper, POI recommendation, has real-world applications.\n\nS2. The paper proposed several attack methods.\n\nS3. Extensive experiments are conducted on two real datasets."
                },
                "weaknesses": {
                    "value": "W1. The motivation of this work is not convincing enough. \n\nW2. The definition of sensitive information is unclear.\n\nW3. Experiments are inadequate and some insights are not surprising.\n\nW4. There are some typos in this paper. For example, at line 16 in algorithm 4,   $f_out$ should be $f_\\theta$."
                },
                "questions": {
                    "value": "Q1. The definition of sensitive information and privacy guarantee should be formally defined and well justified. Then, it may become meaningful to conduct adversary attacks.   \nQ2. There have been quite a few studies on protecting spatial/location/trajectory privacy. However, most of them was not reviewed/evaluated by this paper. Thus, it was uncertain whether existing privacy preservation mechanisms could help on the mentioned limitation of POI recommendation.    \nQ3. Take private spatial data publish as an example. Based on GDPR, a LBS platform can only collect user\u2019s check-in data that has been well protected (e.g., by differential privacy). Under this practical setting, deriving the platform\u2019s data will not leak the sensitive information of users, which makes the attacker model proposed in this work less meaningful.    \nQ4. I am also curious: if the input data has been well protected by existing privacy mechanism and then trained by POI recommendation model, is there any sensitive information leakage?    \nQ5. Since privacy preserving learning has been well studied in recent years, it is sometimes possible to extend existing POI recommendation models with privacy guarantee (e.g., by adding differential privacy noise in the gradients). Does this fact significantly change the main insight?   \nQ6. In Appendix E, the epsilon setting of DP-SGD is a little large. Please provide more justifications.    \nQ7. Both datasets are a little outdated and relatively smaller-scale than the current LBS platform. However, the major insights are strongly related to the data sparsity. Maybe, it would be better to conduct experiments on large-scale datasets.    \nQ8. Why does the curve in Figure 5(b) first rise and then drop when the number of POI increases?    \nQ9. It is mentioned that k is usually 1, 5, and 10 when using top-k to measure accuracy in page 3. However, only 1, 3, and 5 were tested in the experiment as shown in Figure 1. What is the rationale of this experimental setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2078/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2078/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2078/Reviewer_1v9T"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2078/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699346231924,
            "cdate": 1699346231924,
            "tmdate": 1700708499220,
            "mdate": 1700708499220,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RBHbSLzaq8",
                "forum": "mFTPRV5hYw",
                "replyto": "OMo9Lm3H5G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2078/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2078/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 1v9T,\n\nWe thank you for taking the time to review our paper and provide valuable feedback and questions. We would like to present our explanations below and hope they can address your concerns.\n\n__W3  Experiment and Insights Are Not Surprising__  \nWe are willing to adjust the problems or explain our experiments in more detail. Can you please provide more suggestions on the experiments that could further improve the experiment?\n\n__W4  Typos In The Paper__  \nThank you for pointing out the typos. We will fix them in the next version.\n\n __Q1\\W2  Need Formally Define Sensitive Information and Privacy Guarantee__  \nWe have included the notations and definitions of the objectives for each attack in *Appendix Table 2*, accompanied by their respective mathematical definitions. For your convenience, the corresponding definition table is also attached below for reference.\n\n| **Attack**     | **Adversary Objective** | **Adversary Knowledge** |\n|----------------|-------------------------|-------------------------|\n| `LocExtract`   | Extract the most frequently visited location $l$ of a target user $u$ | -- |\n| `TrajExtract`  | Extract the location sequence of a target user $u$ with length $n$: $x_L=$\\{$l_0, \\dots, l_{n-1}$\\} | Starting location $l_0$ |\n| `LocMIA`       | Infer the membership of a user-location pair ($u$,$l$) | Shadow dataset $D_{\\mathrm{s}}$ |\n| `TrajMIA`      | Infer the membership of a trajectory sequence $x_T=$\\{$(l_0,t_0),\\dots,(l_n,t_{n})$\\} | Shadow dataset $D_{\\mathrm{s}}$ |\n\n\nOur definitions of sensitive information in our attack suite are similar to previous works studying privacy risks on general machine learning models [1,2]. We appreciate it if you could provide additional suggestions and clarifications on improving the definition of sensitive information.\n\n\n\n\n__Q2  Need Defense-Related Literature Survey__  \n We have listed the attack-related works in *section 5* and defense-related works in *appendix F*.\n\nAs mentioned in *Section 5*, prior research primarily focused on exploiting side channels such as social relationships and historical trajectories to extract sensitive information during the data aggregation and release processes. However, these studies have not yet concentrated on potential leakages through deep neural networks. Given that more recent LBS relies on deep neural networks, which present a new attack surface for adversaries seeking to access private user information, these earlier attack studies are insufficient. We are the first to investigate privacy leakages in POI recommendation models based on deep neural networks. Our work highlights the need for a privacy-preserving ML-based POI recommendation system in both industry and academia.\n\nAs for defense, prior works primarily focused on data aggregation and release processes tailored to the attack mentioned above. Since our paper focuses on evaluating the privacy leakage associated with the model, we only focus on the defense mechanisms during the training process, which is widely ignored in prior works. We also will discuss more defenses, e.g., LDP) in the related work section.\n\nWe also appreciate it if you could identify some missing related work, and we will cite and compare them accordingly. \n\n__Q3/W1  GDPR and Attack Motivation__  \nWe would like to emphasize that the threat model of the paper is well-motivated, and the privacy study of the work is urgently needed, as required by GDPR. \n\nFirst of all, GDPR does not mention requirements for using specific protection mechanism mechanisms like differential privacy in collecting user data like location data. GDPR only requires \u201cthe collected content is needed, minimized, and stored safely\u201d in articles *#5 and #25* [3,4]. Since the potential threats in POI recommendation models have not been well studied, companies currently have not adopted adequate protection mechanisms when receiving the data [8,9]. Even if there are protection mechanisms like differential privacy on training data, existing works lacks measurement study to quantify whether those data protection mechanisms are properly applied to the data. Our work could bridge this gap by using our proposed attack suite to perform such kind of measurement.\n\nMoreover, no prior work has studied how POI recommendation models leak sensitive information about users\u2019 data, and our work is the first one to study this problem. This is an important problem since POI recommendation models could unintentionally leak user\u2019s sensitive information."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2078/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700118783791,
                "cdate": 1700118783791,
                "tmdate": 1700159772191,
                "mdate": 1700159772191,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1aze2BBO3m",
                "forum": "mFTPRV5hYw",
                "replyto": "OMo9Lm3H5G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2078/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2078/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "__Q8  Explanation of Fluctuation in Figure 5(b)__  \nWe believe that the fluctuation in Figure 5(b) can be attributed to two main reasons.\n\n(1) We hypothesize that when the trajectory is very short, it may be more easily influenced by other factors, such as increased location sharing. Additionally, as mentioned in our paper, many other characteristics can impact the performance of the attack. So they can also contribute to the fluctuations.\n\n(2) When the trajectory is longer, it is more difficult to be memorized by the model. Since our attack considers all location points in the trajectory, it is more likely some locations are less well memorized than others and cause a drop in attack success rate. This also explains why the overall trend is that longer trajectories are less vulnerable to our MIA, and a similar result can also be observed in Figure 17, where we conduct a study from the attack parameter selections.\n\n__Q9  Different Selection of \u201ctopk-accuracy\u201d and \u201dtopk-asr\u201d__  \nThanks for pointing out this notation confusion. The *topk-accuracy* of (1,5,10) on page 3 is in evaluating the performance of the POI recommendation task itself, whereas in the later part of the paper, we also use the same setting (and mostly k=10) to evaluate the utility of the model with/without protection.\n\nHowever, the result in Figure 1 is actually *topk-ASR* for the data extraction attacks, where a smaller k makes more sense as it represents the reliability of the attack. That\u2019s why we use  k=1/3/5 to evaluate the attack results. We will state this clearly in the next version of the paper.\n\n__References:__  \n[1] Liu et al. ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models.  \n[2] Carlini et al. Membership Inference Attacks From First Principles.  \n[3] https://gdpr.eu/article-5-how-to-process-personal-data/  \n[4] https://gdpr-info.eu/art-25-gdpr/\n[5] Andr\u00e9s et al. Geo-indistinguishability: Differential privacy for location-based systems.  \n[6] Bao et al. Successive Point-of-Interest Recommendation With Personalized Local Differential Privacy.  \n[7] Xu et al. An efficient privacy-preserving point-of-interest recommendation model based on local differential privacy.  \n[8] Chen et al. Curriculum Meta-Learning for Next POI Recommendation.  \n[9] Liu et al. STGIN: Spatial-Temporal Graph Interaction Network for Large-scale POI Recommendation.  \n[10] Luo et al. Timestamps as Prompts for Geography-Aware Location Recommendation.  \n[11] Yang et al. GETNext: Trajectory Flow Map Enhanced Transformer for Next POI Recommendation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2078/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700119702120,
                "cdate": 1700119702120,
                "tmdate": 1700159745980,
                "mdate": 1700159745980,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XGAHj2PtUA",
                "forum": "mFTPRV5hYw",
                "replyto": "OMo9Lm3H5G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2078/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2078/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 1v9T,\n\nThank you very much for taking the time to review our submission. We deeply appreciate your thorough feedback and evaluation of our work. In our rebuttal response, we have carefully considered your comments. We would be grateful for your acknowledgment of our responses and your feedback on whether they address your concerns. We are happy to engage in further discussions if needed. Once again, thank you for your invaluable input and attention to our work."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2078/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700256914873,
                "cdate": 1700256914873,
                "tmdate": 1700257535918,
                "mdate": 1700257535918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P3ZxIN50Iv",
                "forum": "mFTPRV5hYw",
                "replyto": "EYXc9vJZJp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2078/Reviewer_1v9T"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2078/Reviewer_1v9T"
                ],
                "content": {
                    "title": {
                        "value": "Response to the author feedback"
                    },
                    "comment": {
                        "value": "Thank you for the detailed respnse and sorry for the late reply. I have raised my score."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2078/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708487414,
                "cdate": 1700708487414,
                "tmdate": 1700708487414,
                "mdate": 1700708487414,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dpc3b9dHzy",
            "forum": "mFTPRV5hYw",
            "replyto": "mFTPRV5hYw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2078/Reviewer_iRwY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2078/Reviewer_iRwY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes data extraction and membership inference attacks to POI recommendations involving location data. The experiments are conducted in two datasets and the empirical results show the effectiveness of the proposed attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The research problem is important.\n2. The paper analyzes the factors in the data that affect the attack performance.\n3. The proposed attacks for data extraction and membership inference attacks are simple yet effective."
                },
                "weaknesses": {
                    "value": "1. The appropriate baselines are missing. Can the existing data extraction or membership inference attacks be applied to the POI recommendation models, e.g., [1]?\n2. There is no qualitative result analysis on the data extraction attacks. It would be better if the authors could conduct these analyses on the data extraction attacks.\n3. The threat models assume that the adversaries are capable of accessing the confidence scores, which makes them impractical. In practice, the model owner only releases the final result to the users. In this case, whether the proposed attacks are still effective is unknown.\n\n\n\n\n[1] R. Shokri, M. Stronati, C. Song and V. Shmatikov, \"Membership Inference Attacks Against Machine Learning Models,\" 2017 IEEE Symposium on Security and Privacy (SP), San Jose, CA, USA, 2017, pp. 3-18, doi: 10.1109/SP.2017.41."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2078/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699393743190,
            "cdate": 1699393743190,
            "tmdate": 1699636139724,
            "mdate": 1699636139724,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2zX6YkazyG",
                "forum": "mFTPRV5hYw",
                "replyto": "dpc3b9dHzy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2078/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2078/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer iRwY,\n\nWe thank you for taking the time to review our paper and for acknowledging the importance of our research problem. We attach our responses to the concerns below and hope to adequately address them.\n\n__W1  Baselines Missing__  \nThanks for the suggestions! We believe that the existing inference attacks can work, but we have not included them in the paper for the following reasons:\n\n(1) Since our paper primarily focuses on evaluating the privacy risks in POI recommendation models. Thus, we select the SOTA attack \u2013 LiRA [2], as it has been shown to outperform other MIAs, including [1].  \n\n(2) Our main contribution is in adapting the existing methods to the POI context. Our proposed mechanism should also work across different attack approaches like [1] as well, as they both utilize similar techniques like shadow model training. \n\n__W2  Qualitative Result__  \nWe will add some qualitative result analysis on the data extraction attack and corresponding visualization (e.g., map visualization) to help readers better understand the analyses of the attack. We also appreciate it if you could provide suggestions on the qualitative study that we can conduct.\n\n__W3  Threat Model and Confidence Scores__  \n Thanks for asking. We define the corresponding threat model and realistic cases in *section 2.2 (Adversary Knowledge)*. We highlight the scenarios below:\n\n(1) Instead of focusing on publicly available models like Google Maps, which might only provide the final predicted label, we are targeting the scenarios of service providers [3,4,5], who have mentioned in their service that collected user data and trained models, then providing these trained models as services to 3rd party to build applications. Under this setting, a malicious 3rd party might have the confidence scores from the model output.\n\n(2) Also, we have mentioned another scenario where the retention period of the training data expires. Still, the model owner keeps the model, and an adversary (e.g., a malicious insider of location service providers) can aim to use the model to perform our attack to infer about the deleted data.\n\n__References:__  \n[1] Shokri et al. Membership Inference Attacks against Machine Learning Models  \n[2] Carlini et al. Membership Inference Attacks From First Principles  \n[3] Chen et al. Curriculum Meta-Learning for Next POI Recommendation  \n[4] Liu et al. STGIN: Spatial-Temporal Graph Interaction Network for Large-scale POI Recommendation  \n[5] https://www.localogy.com/2021/03/foursquares-power-play-continues-with-relaunched-places-and-new-api/"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2078/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700117249001,
                "cdate": 1700117249001,
                "tmdate": 1700117249001,
                "mdate": 1700117249001,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]