[
    {
        "title": "ShareFormer: Share Attention for Efficient Image Restoration"
    },
    {
        "review": {
            "id": "9tu2v2APSc",
            "forum": "HXZK1Z8tHa",
            "replyto": "HXZK1Z8tHa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission977/Reviewer_DFQF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission977/Reviewer_DFQF"
            ],
            "content": {
                "summary": {
                    "value": "This paper tried to solve two problems: how to make Transformer faster and how to make Transformer's optimization faster.\nFor the first problem, they propose shared portion stripe attention (SPSA) to reduce the network latency up to 7x speedup.\nFor the second problem, they introduce residual connections to the value of SPSA.\nIn summary, they build a novel Transformer network: ShareFormer by SPSA with residual connections and gated united."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper reduced the computational complexity of stripe attention by Shared Portion Stripe Attention.\n\n2. They proposed Residual Connections on Value, which offset the obstruction of the information flow throughout the network.\n\n3. They reached similar or even better performance and fewer parameters compared with other methods."
                },
                "weaknesses": {
                    "value": "1. In order to implement shared attention, they had to introduce residual connections on value and gated unit to control the extra complexity. I am concerned that the greater the complexity of the system, the higher the likelihood of training instability.\n\n2. In Tables 4, 5, and 6, DRUNet reached similar performance and much lower latency compared with ShareFormer. To be honest, I prefer DRUNet in real applications with nearly 5x speedup, even though there is a 0.1~0.2 performance loss."
                },
                "questions": {
                    "value": "1. Could you please also list the number of parameters of each model in Tables 4 and 6?\n\n2. By adding V directly to the output, you're effectively giving more weight to the original values irrespective of the computed attention scores. This might dilute the effect of the attention mechanism, especially if the values in V dominate the weighted sum. Thus, what if introducing a trainable parameter to scale the residual connection instead of directly adding a residual connection?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission977/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission977/Reviewer_DFQF",
                        "ICLR.cc/2024/Conference/Submission977/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission977/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698555679116,
            "cdate": 1698555679116,
            "tmdate": 1700026043485,
            "mdate": 1700026043485,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JRTxvk1HNi",
                "forum": "HXZK1Z8tHa",
                "replyto": "9tu2v2APSc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission977/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments for your questions."
                    },
                    "comment": {
                        "value": "Thank you sincerely for your thorough review of our paper. We highly value the questions you raised. Below, we have provided detailed responses to each of them and made corresponding improvements to the paper. We hope our responses and revisions sufficiently address your concerns, and we genuinely appreciate your consideration for an improved rating of our paper.\n\n> For Q1, Could you please also list the number of parameters of each model in Tables 4 and 6?\n\nCertainly. We have included the parameter number and latency information for each model in the table below. The size of the model used for image denoising (corresponding to Table 4 in the main paper) and JPEG CAR (corresponding to Table 6 in the main paper) is the same. Therefore, we present these in a single table. Additionally, the details from this table have also been added to Appendix C.\n\n| Method            | Params | FLOPs  | Latency | PSNR on image denoising | PSNR on JPEG CAR |\n| ----------------- | ------ | ------ | ------- | ----------------------- | ---------------- |\n| DnCNN             | 0.56M  | 0.55T  | 83.18 ms  | 31.24                   | -                |\n| DRUNet            | 32.64M | 2.15T  | 172.39 ms | 31.69                   | 34.41            |\n| RNAN              | 8.96M  | N/A    | N/A     | -                       | -                |\n| SwinIR            | 11.46M | 11.28T | 5689.86 ms | 31.78                   | 34.52            |\n| Restormer         | 26.10M | 2.12T  | 1201.53 ms | 31.79                   | -                |\n| EDT-B             | 11.63M | N/A    | N/A     | 31.76                   | -                |\n| ShareFormer(ours) | 17.64M | 1.35T  | 806.97 ms | 31.80                   | 34.53            |\n\nPSNR on image denoising was calculated on the CBSD68 dataset, $\\sigma=25$. PSNR on image JPEG CAR was calculated on the Classic5 dataset, $q=40$. \"N/A\" indicates that the corresponding models are too heavy for the NVIDIA RTX 3090 GPU. \"-\" means that the result is not available.\n\n> For Q2, Thus, what if introducing a trainable parameter to scale the residual connection instead of directly adding a residual connection?\n\nThank you for your insightful suggestion. In fact, the introduction of a trainable residual connection is a design we had considered and experimented with earlier. However, we opted not to pursue this approach. The table below presents the experimental results on the classical $4 \\times$ image super-resolution task, the test set is Urban100, where the residual scale was initialized to 1.\n\n| Methods                 | PSNR & SSIM on Urban100 |\n| ----------------------- | ---------- |\n| Vanilla residual        | 27.64 \\| 0.8294 |\n| Learnable residual scale | 27.62 \\| 0.8283 |\n| Learnable residual channel-wise | 27.68\uff5c0.8293 |\n\nIt is evident that incorporating diverse types of residuals does not result in a significant performance enhancement for ShareFormer in the classical $4 \\times$ super-resolution task. This could be attributed to ShareFormer's intrinsic capacity to regulate value whereby supplementary trainable residual coefficients will not advance the network's performance (or a more detailed network initialization might be necessary)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410627699,
                "cdate": 1700410627699,
                "tmdate": 1700411959698,
                "mdate": 1700411959698,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QmpAIedY4e",
                "forum": "HXZK1Z8tHa",
                "replyto": "9tu2v2APSc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission977/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments for your proposed weaknesses."
                    },
                    "comment": {
                        "value": "> For W1, I am concerned that the greater the complexity of the system, the higher the likelihood of training instability\n\nThank you for raising an interesting research question. The correlation between model complexity and training stability is a topic of great importance. We have identified three initial perspectives on the issue:\n1. Firstly, it is important to note that trainability is primarily intended to demonstrate that the model can converge rapidly[1], rather than stably[2]. In terms of stability of convergence, it may be more appropriate to use the sharpness as a metric [3].\n2. In the three tasks discussed in this article, we did not notice any apparent variation in the training steadiness of models that differ in size. Nonetheless, we suspect that if the number of parameters is exceedingly high, the training instability will intensify as the intricacy of the model increases. Indeed, many studies relevant LLM scaling laws [4] have established that the more extensive the number of parameters, the less stable the training is.\n3. It is a consensus within the deep learning field that more complex networks require more training tricks. there is an Figure 1 And Table 3&4 in RCAN-it[5] for complex RCANs within CNN networks, Figure 2 in ConvNeXT[6] also makes use of a number of tricks to give a pump in the performance of convolutional networks, and Table 8. in DeiT[7] proposes a series of tricks to make Transformer more stable to train. All these instances underscore the need for meticulously designed tricks in training to enhance the stability of complex networks.\n\n> For W2, In Tables 4, 5, and 6, DRUNet reached similar performance and much lower latency compared with ShareFormer.\n\nOn this point of your concern, the choice between traditional CNNs and emerging Transformers is a widespread concern in the whole field. Here are a few of our perspectives:\n\n1. As previously mentioned, DRUNet has approximately double the number of parameters compared to ShareFormer. This has led to a limitation in its practical application due to insufficient lightweight capabilities.\n2. Unlike methods based on CNNs, models based on Transformers in the field of image restoration are still in the early and exploratory stages, with a considerable gap from practical application in industrial settings. Current efforts to improve inference speed and trainability, such as ELAN and our ShareFormer, represent steps toward achieving an optimal and practical Transformer network in the future. While it's just a small step, we believe our work holds significant relevance.\n3. In reality, our method has made significant advances with progress of 0.38dB on the notoriously difficult Urban100 dataset, compared to DRUNet. We believe this improvement is significant and should be noteworthy in the selection of methods.\n4. Unlike tasks like image classification and detection, numerical metrics are not the sole determinant of an image restoration method's effectiveness. Refer to Figure 10 in the main manuscript for a close-up view, which shows that our method excels in restoring the rich and intricate texture of the original image.\n\n### References\n\n[1] Jacot A, Gabriel F, Hongler C. Neural tangent kernel: Convergence and generalization in neural networks[J]. Advances in neural information processing systems, 2018, 31.\n\n[2] Berlyand, Leonid and Jabin, Pierre-Emmanuel and Safsten, C. Alex \"Stability for the training of deep neural networks and other classifiers\" Mathematical Models and Methods in Applied Sciences 2021, 31, 11, 2345-2390.\n\n[3] Philip M. Long, Peter L. Bartlett. \"Sharpness-Aware Minimization and the Edge of Stability\". arXiv preprint arXiv:2309.12488.\n\n[4] Qin, Zhen and Li, Dong and Sun, Weigao and Sun, Weixuan and Shen, Xuyang and Han, Xiaodong and Wei, Yunshen and Lv, Baohong and Yuan, Fei and Luo, Xiao and others \"Scaling transnormer to 175 billion parameters\" arXiv preprint arXiv:2307.14995.\n\n[5] Lin, Zudi and Garg, Prateek and Banerjee, Atmadeep and Magid, Salma Abdel and Sun, Deqing and Zhang, Yulun and Van Gool, Luc and Wei, Donglai and Pfister, Hanspeter \"Revisiting RCAN: Improved Training for Image Super-Resolution\" arXiv preprint arXiv:2201.11279.\n\n[6] Zhuang Liu and Hanzi Mao and Chao-Yuan Wu and Christoph Feichtenhofer and Trevor Darrell and Saining Xie \"A ConvNet for the 2020s\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2022. \n\n[7] Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve \"Training data-efficient image transformers &amp; distillation through attention\" International Conference on Machine Learning (ICML) 2021."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410681117,
                "cdate": 1700410681117,
                "tmdate": 1700455097765,
                "mdate": 1700455097765,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k5o8hHPF6o",
                "forum": "HXZK1Z8tHa",
                "replyto": "9tu2v2APSc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission977/Reviewer_DFQF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission977/Reviewer_DFQF"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. The answer solved my concern about the comparison with DRUnet. It's glad to see you have already done the experiments of learnable residual scale which addressed my concern regarding the excessively high weights of value.\n\nI agree with reviewer nYLY's point W3, which is similar to my point W1: The ablation study is limited and fails to comprehensively assess the contribution of each component. Providing additional relevant ablation studies would make the argument more convincing"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631523491,
                "cdate": 1700631523491,
                "tmdate": 1700632032028,
                "mdate": 1700632032028,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OT9Bb9KJMa",
            "forum": "HXZK1Z8tHa",
            "replyto": "HXZK1Z8tHa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission977/Reviewer_nYLY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission977/Reviewer_nYLY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new transformer architecture called ShareFormer for image restoration tasks like super-resolution and denoising. The key idea is to share attention maps between neighboring layers, which reduces computational cost and speeds up inference. Residual connections are added to preserve information flow. Experiments show ShareFormer achieves state-of-the-art accuracy with lower latency and better trainability than prior transformers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "\u2022\tThe proposed ShareFormer delivers substantial improvements in efficiency, reducing latency by up to 2x compared to CNN models without compromising accuracy. This is achieved through an innovative technique of sharing attention maps between transformer layers to avoid redundant computations. \n\u2022\tThe method enhances trainability over other transformer architectures by introducing beneficial inductive biases, allowing faster convergence. \n\u2022\tAn additional strength is the generalizability of the approach, which is shown to be compatible with different attention mechanisms like shifted windows."
                },
                "weaknesses": {
                    "value": "\u2022\tThe performance exhibited by the proposed ShareFormer is indeed commendable, adeptly striking a balance between quality and speed. Nonetheless, I must express some reservations regarding the motivation behind the proposed backbone. To put it candidly, certain elements of the core module in ShareFormer appear reminiscent of concepts present in existing methodologies. For instance, the notions of Residual on V and the Reuse of Attention map are echoed in methods like Restormer and ELAN. Similarly, the group split strategy bears similarities to the one found in EfficientViT [1]. Thus, at a glance, ShareFormer seems to be a thoughtful amalgamation of pre-existing techniques. I would strongly recommend emphasizing the unique aspects and contributions of ShareFormer to underscore its originality within the broader landscape.\n[1] Xinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, Yixuan Yuan: EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention. CVPR 2023: 14420-14430.\n\u2022\tThe gains in trainability from the residual connections could use more detailed analysis and intuition. The paper currently lacks insight into the underlying mechanisms enabling faster convergence.\n\u2022\tThe ablation study is limited and does not thoroughly evaluate the contribution of each component. More experiments could help tease apart the individual impact of techniques like SPSA and the gated units.\n\u2022\tImportant implementation details like dataset splits for training, validation, and testing are not provided. This makes reproducibility difficult.\n\u2022\tBesides the Image Super-Resolution, the overall improvements appear relatively incremental over strong prior work like SwinIR and Restormer. The advances are not radically transformative."
                },
                "questions": {
                    "value": "Some concerns are raised in Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission977/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission977/Reviewer_nYLY",
                        "ICLR.cc/2024/Conference/Submission977/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission977/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698679612271,
            "cdate": 1698679612271,
            "tmdate": 1700708564556,
            "mdate": 1700708564556,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qAArqU8Nwc",
                "forum": "HXZK1Z8tHa",
                "replyto": "OT9Bb9KJMa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission977/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comment for your proposed weakness 1."
                    },
                    "comment": {
                        "value": "Thank you for dedicating time and effort to review our paper. Below, we address each of your points and have made corresponding adjustments to the manuscript based on your valuable suggestions. If our responses sufficiently address your queries, would you kindly consider enhancing the rating for our paper?\n\n> For W1, ShareFormer seems to be a thoughtful amalgamation of pre-existing techniques.\n\nThank you for your commendation on the balance achieved between quality and speed in our proposed method. Regarding the innovation of the method, we would like to provide further details on a few points.\n\n1. About Residual on V: I would like to clarify that ELAN and Restormer did not propose this innovation. There may be some misunderstanding. Further, we will demonstrate that residual connections on the feature value facilitate the shared attention operation to counteract its inherent drawbacks, while also ensuring higher performance and no impact on inference speed.  Experiments about $4 \\times$ classical image super resolution were conducted with different numbers of shared layers, exploring two scenarios of residual connection on Value and other configurations. The results revealed the following findings.\n\n| Shared Layer | With residual v | Without residual value |\n| ------------ | --------------- | ---------------------- |\n| 2            | 27.64 \\| 0.8294 | 27.43 \\| 0.8224        |\n| 4            | 27.48 \\| 0.8241 | 27.25 \\| 0.8199        |\n| 6            | 27.45 \\| 0.8234 | 27.20 \\| 0.8181        |\n\nAs you can see, if the attention maps are only shared between nearby layers, the model's performance will significantly decrease. This demonstrates why it's important and effective to have a residual connection on value.\n\n2. About the group split strategy: the feature splitting approach in this paper differs from EfficientViT, which we will illustrate with a piece of PyTorch-style pseudo-code.\n\nFollowing the EfficientViT's [official code](https://github.com/microsoft/Cream/blob/main/EfficientViT/classification/model/efficientvit.py#L165-L179), they add the previous group's output to the next group's input. It also performs an additional grouping convolution on the query. The group split strategy of EfficientViT is:\n\n```python\nfor i, qkv in enumerate(self.qkvs):\n    if i > 0: # add the previous output to the input\n        feat = feat + feats_in[i]\n    feat = qkv(feat)\n    q, k, v = qkv.chunk(3, dim=1)\n    q = self.dws[i](q)\n    attn = (q.transpose(-2, -1) @ k) * self.scale\n    attn = attn.softmax(dim=-1)\n    feat = v @ attn.transpose(-2, -1)\n    feats_out.append(feat)\nx = self.proj(torch.cat(feats_out, 1))\n```\n\nHowever, we refrained from adding the output of the former group to the latter, as we believe it is an ineffective (or detrimental) operation for the image restoration task due to the following reason. The feature groups will be inputted into window attentions of different window-sizes. The first group of features will be inputted into a window of size [32, 8] by default, while the second group of features will be inputted into a window of size [8, 32]. The windows will analyse and process texture information in different directions. According to Figure 4 in GRL [3], we could see that \"image features in natural images are anisotropic.\" Therefore, two sets of features will be utilized for extracting heterogeneous information. Overlapping these features through addition will restrict the second set of window attention from learning useful knowledge, as its input includes texture information in the opposite direction. Here it's the group split strategy of ShareFormer:\n\n```python\nqkvs = self.qkvs(feat).chunk(split_num, dim=1)\nfeats_out = []\nfor qkv in qkvs:\n    q, k, v = qkv.chunk(3, dim=1)\n    attn = (q.transpose(-2, -1) @ k) * self.scale\n    attn = attn.softmax(dim=-1)\n    feat = v @ attn.transpose(-2, -1)\n    feats_out.append(feat)\nx = self.proj(torch.cat(feats_out, 1))\n```\n\nWe used EfficientViT's group split strategy to train ShareFormer on the classical $4 \\times$ image super-resolution task, and the experimental results proved our point.\n\n| Group Split Strategy | PSNR on Urban100 | SSIM on Urban100 |\n| -------------------- | ---------------- | ---------------- |\n| EfficientViT         | 27.48            | 0.8250           |\n| ShareFormer (ours)   | 27.64            | 0.8294           |\n\n[1] Yang G. Tensor programs ii: Neural tangent kernel for any architecture[J]. arXiv preprint arXiv:2006.14548.\n\n[2] Yang G, Littwin E. Tensor programs iib: Architectural universality of neural tangent kernel training dynamics[C]//International Conference on Machine Learning. PMLR, 2021: 11762-11772.\n\n[3] Y. Li, et al., \"Efficient and Explicit Modelling of Image Hierarchies for Image Restoration,\" in 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Vancouver, BC, Canada, 2023 pp. 18278-18289."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410289072,
                "cdate": 1700410289072,
                "tmdate": 1700412285886,
                "mdate": 1700412285886,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ruYSZIz1TW",
                "forum": "HXZK1Z8tHa",
                "replyto": "OT9Bb9KJMa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission977/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments for your proposed weaknesses 2-4."
                    },
                    "comment": {
                        "value": "> For W2, The gains in trainability from the residual connections could use more detailed analysis and intuition.\n\nSharing Attention reduces redundancy while also reducing the efficiency of information flow by restricting the channels for layer-by-layer information flow (you can think of the operation `Share` as a residual connection without residual branches, i.e., $y = f(x) + x \\ \\text{and} \\ f(x) = 0$). In order to improve the efficiency of information flow while controlling the amount of computation, we introduce residual on V.\n\nHowever, due to space constraints, we are only able to explain the trainability of residual on V from an experimental point of view rather than a mathematical one. The NTK spectral analysis[1,2] of the Transformer with shared attention and residual on V is exactly where we will direct our future work.\n\n> For W3, The ablation study is limited and does not thoroughly evaluate the contribution of each component.\n\nWe add ablation experiments on whether CSAU is more effective, please see the table below:\n\n|                       | Param | Latency | PSNR on Urban100 SR x4   |\n| --------------------- | ----- | ------- | --------------- |\n| Shared Attention+MLP  | 9.43M | 238.47 ms | 27.59 \\| 0.8275 |\n| Shared Attention+GDFN | 8.91M | 249.32 ms | 27.61 \\| 0.8280 |\n| CSAU(ours)            | 7.67M | 211.10 ms | 27.64 \\| 0.8294 |\n\nIt is evident that CSAU is more efficient when fewer parameters are used. In fact, CSAU serves as an efficient approach to reducing excessive computation in the Feed Forward Network (FFN) section. It accomplishes this by relocating the gating operation before the FFN computation process, which streamlines the FFN into an identity module, resulting in faster execution.\n\n> For W4, Important implementation details like dataset splits for training, validation, and testing are not provided. This makes reproducibility difficult.\n\nThe specific training datasets, training and testing details have been provided in Appendix B. Here, we present the hyperparameters for training for the four tasks outlined in the article. This is intended to facilitate the reproduction of this work by others and to address any questions that may arise.\n\n| Task           | Classical Super Resolution        | Lightweight Super Resolution      | Image Denoising & JPEG CAR        |\n| -------------- | --------------------------------- | --------------------------------- | --------------------------------- |\n| Batch-size     | 8                                 | 64                                | Progressive Learning in Restormer |\n| Learning-rate  | 2e-4                              | 5e-4                              | 3e-4                              |\n| Schedule       | Cosine                               | Cosine                               | Cosine                               |\n| Optim          | AdamW (0.9, 0.999)                | AdamW (0.9, 0.999)                | AdamW (0.9, 0.999)                |\n| Training Iters | 500k                              | 500k                              | 300k                              |\n| Warmup         | None                              | None                              | None                              |\n| Patch-size     | 48                                | 64                                | Progressive Learning in Restormer |\n| Init           | Default Initialization of PyTorch | Default Initialization of PyTorch | Default Initialization of PyTorch |\n| augment        | flip and rot 90 & 270             | flip and rot 90 & 270             | flip and rot 90 & 270             |\n| grad clip      | None                              | None                              | 0.01                              |\n| EMA            | 0.999                             | 0.999                             | 0.999                             |\n\n[1] Yang G. Tensor programs ii: Neural tangent kernel for any architecture[J]. arXiv preprint arXiv:2006.14548.\n\n[2] Yang G, Littwin E. Tensor programs iib: Architectural universality of neural tangent kernel training dynamics[C]//International Conference on Machine Learning. PMLR, 2021: 11762-11772."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410460461,
                "cdate": 1700410460461,
                "tmdate": 1700412314839,
                "mdate": 1700412314839,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cdvFe0eB35",
                "forum": "HXZK1Z8tHa",
                "replyto": "OT9Bb9KJMa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission977/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comment for your proposed weakness 5."
                    },
                    "comment": {
                        "value": "> For W5, Besides the Image Super-Resolution, the overall improvements appear relatively incremental over strong prior work like SwinIR and Restormer.\n\nYour concern pertains to a wider challenge within the field of image restoration. Let me elaborate on a couple of aspects:\n\n1. Image restoration poses unique challenges compared to other high-level visual tasks in terms of defining metrics and achieving improvements. As stated in the discussion of this survey [4]: \"More accurate metrics need to be found for image denoising. PSNR and SSIM are popular metrics for the task of image restoration. PSNR suffers from excessive smoothing, which is very difficult to recognize indistinguishable images. SSIM depends on brightness, contrast and structure, and therefore cannot accurately evaluate image perceptual quality.\" Metrics such as PSNR are not as absolute as accuracy in classification tasks or IoU in detection tasks.\n2. The assessment of image quality remains challenging due to the lack of a flawless objective metric. Conventional measures, such as PSNR and SSIM, oversimplify the multidimensional evaluation of image quality. Therefore, the numerical differences among cutting-edge techniques, especially those founded on Transformers, become less significant. However, Fig. 10 in our paper and detailed perceptual comparisons in Appendix C reveal our approach's ability to recover finer details, including dense dots in real images. This demonstrates the visual superiority of our method over those of the competition.\n3. It is crucial to emphasize that we aim to achieve an optimal Pareto balance between performance and latency rather than focusing solely on performance gains. In this respect, our method has a profound impact on the efficient use of algorithms, notably extensive Transformer-based models, in authentic industrial environments.\n\n[4] Chunwei Tian, Lunke Fei, Wenxian Zheng, Yong Xu, Wangmeng Zuo, Chia-Wen Lin, Deep learning on image denoising: An overview, Neural Networks, Volume 131, 2020, Pages 251-275, ISSN 0893-6080."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410539412,
                "cdate": 1700410539412,
                "tmdate": 1700412332184,
                "mdate": 1700412332184,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J2TUwlwPOL",
                "forum": "HXZK1Z8tHa",
                "replyto": "cdvFe0eB35",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission977/Reviewer_nYLY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission977/Reviewer_nYLY"
                ],
                "content": {
                    "comment": {
                        "value": "Anyway, PSNR and SSIM are still very good evaluation metrics. A good and effective method generally performs well in both of these metrics. If there is no significant improvement in these two metrics, it is actually difficult to convince me. Additionally, your paper does not demonstrate the advantages of the proposed method in other metrics."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707857244,
                "cdate": 1700707857244,
                "tmdate": 1700707857244,
                "mdate": 1700707857244,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2pm70j8gbV",
                "forum": "HXZK1Z8tHa",
                "replyto": "qAArqU8Nwc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission977/Reviewer_nYLY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission977/Reviewer_nYLY"
                ],
                "content": {
                    "comment": {
                        "value": "The authors are attempting to use code differences to illustrate the differences between the proposed method and previous methods, which means that the major idea is essentially consistent with the previous methods, with only minor differences in implementation details. In this case, such innovation is insignificant. I noticed that one reviewer mentioned that shared attention map mechanisms have already been proposed in ELAN, but the authors do not seem to have provided any response."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708393750,
                "cdate": 1700708393750,
                "tmdate": 1700708393750,
                "mdate": 1700708393750,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DUB68LAQkv",
            "forum": "HXZK1Z8tHa",
            "replyto": "HXZK1Z8tHa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission977/Reviewer_F9tc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission977/Reviewer_F9tc"
            ],
            "content": {
                "summary": {
                    "value": "This paper mainly proposes Shared Portion Stripe Attention (SPSA), which shares attention map in $(l-1)$-th and $l$-th layers and residually connects $\\textit{value}$ of attention to intermediate attention output. To show the importance of the proposed residually connected $\\textit{value}$, a Lesion study has been conducted. The authors also propose Combine SPSA with Gated Unit (CSGU) to enhance the existing GDFN module. ShareFormer shows comparable or improved performance when compared to state-of-the-art image restoration methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "[S1] The references and related works cited by the authors are very recent and relevant.\n\n[S2] The Lesion study presented in Sec.4.1 is comprehensive for demonstrating the proposed shared attention is highly related to ensemble behavior of the sequentially placed attention mechanism. Deleting or permutating some self-attention layers could smoothly increase the reconstruction loss. This can be the evidence that the shared attention is not the ensembles of shallow networks.\n\n[S3] The performances of ShareFormer for large SR and lightweight SR are notably enhanced, compared to recent SOTA methods. These results were achieved with smaller model size, fewer computations, and faster speed than the others. (But an unfair issue remain. See question Q4.)"
                },
                "weaknesses": {
                    "value": "[W1] The presentation of text and figures in this paper should become clearer and more understandable. See Q1, Q2, and Q3.\n\n[W2] The paper lacks some ablation studies proving the importance of the proposed components, such as CSAU. The authors should study architecture variants if they hope to clarify that CSAU comes not from insufficient considerations but from careful construction. And the efficiency and effectiveness differences between the original GDFN and the enhanced CSAU must be provided.\n\n[W3] Most importantly, the core parts of ShareFormer, shared attention, seem not novel. In ELAN, one of the sota methods, shared attention map mechanisms have been already proposed. Moreover, it has been already shown how sharing attention maps in more than one layers can impact on the performance and efficiency of the model (related to your Tab.8).\n\n[W4] A potential unfair issue is observed with respect to SR. See Q4.\n\n[W5] Despite the improved inference speed, the performance gains of grayscale denoising, color denoising, and JPEG CAR are not significant."
                },
                "questions": {
                    "value": "[Q1] Where is the exact part that the shared attention is operated? Eq.(4), (5) apparently reveals that this operates in $(l-1)$-th and $l$-th SPSA blocks. However, Fig.2 illustrates attention map of SPSA is shared to CSAU, while Fig.3 depicts sharing attention map appears after residual connection on $\\textit{value}$. The reviewer thinks that the explanation of Eq.(4), (5) is the correct case the authors intended, while the phrase, \"**sharing attention map**\", in Fig.2 and Fig.3 is confusing. If it\u2019s right, \u201csharing\u201d can be omitted. Or not, please let me know what your first intention with respect to the exact parts of sharing attention map is.\n\n[Q2] This question is related to Sec.4.2. From ConViT, how can you draw the conclusion that the concentrated ERF implies an amplified locality bias of the network? The reviewer thinks that it is not sufficient for the authors to claim that this fact shows the locality bias of the residually connected $\\textit{value}$ in ShareFormer. I cannot find the acceptable evidence from ConViT paper. Don\u2019t you have any other evidence justifying your claim, such as visualization?\n\n[Q3] In Appendix D, why did you compare the attention maps of layers 1 and 3? ShareFormer shares attention map in layer-order-number pairs (0, 1), (2, 3), \u2026, (2n-2, 2n-1), respectively. Thus, comparison of attention maps in \u201clayers 0 and 1\u201d or \u201clayers 2 and 3\u201d is more compelling to demonstrate attention map redundancy. Additionally, I recommend the authors to compare attention redundancies of the cases where the shared attention is \u201cemployed\u201d and \u201cnot employed\u201d in the proposed ShareFormer, instead of SwinIR cases.\n\n[Q4] Did you apply the Progressive Learning to large and lightweight SR tasks with training patch size of from 128 to 384? However, the comparative methods, SwinIR, ELAN, and DLGSA, used smaller patch size, such as 48x48 and 64x64. I am concerned that this leads to a potential unfairness issue.\n\n\n\n**Minor issue (not necessary to be mentioned in author rebuttal, if the authors struggle to a limit on the number of characters of rebuttal.)**\n\n(1) Fig.2 omits (a) and (b) mark, while the caption uses (a) and (b).\n\n(2) In the last to second sentence of Tab.1 caption, Restormer never used window attention. Moreover, you should mention what is MHDA, which seems a typo. (In Restormer, MDTA was used, and MHDA was not shown in SwinIR.)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission977/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission977/Reviewer_F9tc",
                        "ICLR.cc/2024/Conference/Submission977/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission977/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754643493,
            "cdate": 1698754643493,
            "tmdate": 1700791735844,
            "mdate": 1700791735844,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "25txl3btoU",
                "forum": "HXZK1Z8tHa",
                "replyto": "DUB68LAQkv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission977/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments for your Q1, Q2, and Q3."
                    },
                    "comment": {
                        "value": "Thank you for your meticulous review and valuable insights into our paper. Below, we will respond to each of the questions you raised and make corresponding modifications to the manuscript. If our responses address your concerns and the revised manuscript meets your satisfaction, we kindly ask for your consideration in potentially enhancing the rating of our paper. Your feedback is immensely appreciated.\n\n\n> For Q1: Where is the exact part that the shared attention is operated?\n\nWe have made appropriate changes to Figures 2 and Figure 3 and their captions in the article to make them clearer.\n\nIn fact, both Attention Map and Feature Value obtained in SPSA are fed into CSAU. For easier understanding, we summarise the whole procedure as Share Attention Block (SAB).In SAB, there exist two layers, the first one contains two modules: SPSA, GDFN, and the second one contains one module, which is CSAU. Among them, SPSA is used to compute the Attention Map $Attn\\_{l-1}$ and Feature $V\\_{l-1}$, while GDFN is classically used as Feed Forward Network (FFN); for the $l$th layer, which is the CSAU module, it receives the output of GDFN $X\\_{l}$, SPSA's attention map $Attn\\_{l-1}$ and feature $V\\_{l-1}$, and obtains the output according to Eq. 5 and Eq. 7 in the article.\n\nSpecifically, we can use the following formulas for further explanation:\n\n$$\nQ_{l-1}, K_{l-1} = W_q(X_{l-1}), W_k(X_{l-1}); \n$$\n$$\nAttn_{l-1} = Softmax((Q_{l-1}K_{l-1}^T)/s); \n$$\n$$\nV_{l-1} = W_v(X_l{l-1}); \n$$\n$$\nX_{l} = W_{l-1}(Attn_{l-1} V_{l-1}) + X_{l-1};\n$$\n\nThe above four lines of equations represent the specific work of the SPSA;\n\n$$\nX_{l} = GDFN(X_{l});\n$$\n\nThe formula above represents the specific work of the GDFN;\n\n$$\nG_{l}, V_{l} = W_g X_{l}, W_v{X_{l}} + V_{l-1}; \n$$\n$$\nAttn_l = Attn_{l-1}; \n$$\n$$\nX_{l+1} = W_{l}(G_{l} \\odot (Attn_l V_l)) + X_l;\n$$\n\nThe above three lines of equations represent the specific work of the CSAU.\n\nWe hope this explanation gives you a clearer understanding of our network structure, and you can also view the updated Figure 2 and Figure 3 for a more intuitive understanding of the model's design options.\n\n> For Q2: From ConViT, how can you draw the conclusion that the concentrated ERF implies an amplified locality bias of the network? \n\nWe suspect there might be a misunderstanding in our presentation. In Section 4.2 of our paper, the statement is as follows: \"As elucidated by Barzilai et al. (2023), the ensemble behavior of the network results in its ERF carrying more weight towards the central region of the receptive field.\" Therefore, we did not draw this conclusion from ConViT. This particular conclusion was obtained from Section 5.2 of \"A kernel perspective of skip connections in convolutional networks\" (Barzilai et al. 2023) [[1]](https://openreview.net/forum?id=6H_uOfcwiVh), a result that has been rigorously proven mathematically.\n\n> For Q3, In Appendix D, why did you compare the attention maps of layers 1 and 3?\n\nThank you for your inquiry, which brings up a crucial point.\n\n1. In Appendix D, we compared layers 1 and 3 because these two layers happened to exhibit an apparent redundancy, making it more beneficial to illustrate for readers' comprehension. However, we did not anticipate that this would lead to misunderstandings, and we sincerely apologize for that. To enhance clarity and aid understanding, we have now provided a complete set of attention maps visualization for all 24 layers of SwinIR-L in Appendix D. In reality, redundancy in attention computation might occur between adjacent layers (as shown in layer 4 and layer 5 of block 3 in the updated Appendix D) or even across distant layers (such as the previously compared layer 1 and layer 3 of block 1, and the layer 0 and layer 5 of block 4 in the updated Appendix D). The reasons for redundancy appearing across distant layers and how to determine which two layers exhibit significant redundancy are valuable questions that we acknowledge and plan to address in our future works.\n2. The use of SwinIR for comparison was not intended to convey any specific meaning. We simply aimed to illustrate that redundancy in attention computation is prevalent in various Transformers, and SwinIR, being one of the most famous and representative Transformer networks in the field of image restoration, served as a natural example to demonstrate this phenomenon. However, we also think your suggestion is meaningful. We are currently training ShareFormer without shared attention maps and observing the computational redundancy it introduces. We plan to update these results in the Appendices within the next two days.\n\n[1] Daniel Barzilai, Amnon Geifman, Meirav Galun, Ronen Basri. \"A Kernel Perspective of Skip Connections in Convolutional Networks\" ICLR 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409977799,
                "cdate": 1700409977799,
                "tmdate": 1700410962141,
                "mdate": 1700410962141,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eOB4QhgJxy",
                "forum": "HXZK1Z8tHa",
                "replyto": "DUB68LAQkv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission977/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments for your proposed Weaknesses"
                    },
                    "comment": {
                        "value": "> For W1, The presentation of text and figures in this paper should become clearer and more understandable. See Q1, Q2, and Q3.\n\nI hope the above answers on Q1-Q3 have allayed your concerns.\n\n> For W2, The paper lacks some ablation studies proving the importance of the proposed components, such as CSAU.\n\nThanks to your kind reminder, we supplemented the ablation experiment with this module of CSAU on classical image $4 \\times$ super-resolution task. The test dataset is Urban100. The results are as follows:\n\n|                       | Param | Latency | PSNR & SSIM     |\n| --------------------- | ----- | ------- | --------------- |\n| Shared Attention+MLP  | 9.43M | 238.47 ms  | 27.59 \\| 0.8275 |\n| Shared Attention+GDFN | 8.91M | 249.32 ms | 27.61 \\| 0.8280 |\n| CSAU(ours)            | 7.67M | 211.10 ms | 27.64 \\| 0.8294 |\n\nIt's clear that CSAU works better and faster with fewer parameters. In fact, CSAU is an efficient way to avoid excessive computation in the Feed Forward Network (FFN) part, because it moves the gating operation from the middle of the FFN computation process to before the FFN computation process and simplifies the FFN into an identity module.\n\n> For W3, shared attention seems not novel.\n\nThe core of our approach is to implement residual connections on the Value element while completely sharing the attention feature map of the network among adjacent layers. This allows the shared attention operation to offset the shortcomings brought about by it, all the while ensuring that there isn't excessive loss of accuracy and running speed remains unaffected.  We conducted experiments on varying numbers of shared layers for the two scenarios of residual connection on Value or otherwise in classical image $4 \\times$ super-resolution task, and the findings are as follows (the test set is the Urban100):\n\n| Shared Layer | With residual value | Without residual value |\n| ------------ | --------------- | ---------------------- |\n| 2            | 27.64 \\| 0.8294 | 27.43 \\| 0.8224        |\n| 4            | 27.48 \\| 0.8241 | 27.25 \\| 0.8199        |\n| 6            | 27.45 \\| 0.8234 | 27.20 \\| 0.8181        |\n\nAs you can see, if the attention maps are only shared between nearby layers, the model's performance will significantly decrease. This demonstrates why it's important and effective to have a residual connection on value.\n\n> For W4, A potential unfair issue is observed with respect to SR. See Q4.\n\nI hope the above answer on Q4 has solved your problem.\n\n> For W5, Despite the improved inference speed, the performance gains of grayscale denoising, color denoising, and JPEG CAR are not significant.\n\nYour concern addresses a broader challenge inherent to the entire domain of image restoration. Allow me to shed light on a few aspects:\n\n1. Image restoration, distinct from other high-level visual tasks, faces unique challenges in defining metrics and achieving improvements. As stated in the discussion of this review [1]: \"More accurate metrics need to be found for image denoising. PSNR and SSIM are popular metrics for the task of image restoration. PSNR suffers from excessive smoothing, which is very difficult to recognize indistinguishable images. SSIM depends on brightness, contrast and structure, and therefore cannot accurately evaluate image perceptual quality.\" Metrics such as PSNR are not as absolute as accuracy in classification tasks or IoU in detection tasks.\n2. The evaluation of image quality remains a complex task, given the absence of a perfect objective metric. Traditional measures like PSNR and SSIM oversimplify the multidimensional assessment of image quality. Consequently, the numeric variations among state-of-the-art methods, particularly those based on Transformers, become less prominent. However, as illustrated in Fig. 10 of our paper and detailed perceptual comparisons in Appendix C, our approach excels in recovering finer details, such as dense dots in real images. This showcases the visual superiority of our method over others.\n3. Importantly, our emphasis lies in striking an optimal Pareto balance between performance and latency, rather than pursuing absolute performance gains. In this regard, our approach holds significant implications for the effective application of algorithms, especially large Transformer-based models, in real-life industrial settings.\n\n[1] Chunwei Tian, Lunke Fei, Wenxian Zheng, Yong Xu, Wangmeng Zuo, Chia-Wen Lin, Deep learning on image denoising: An overview, Neural Networks, Volume 131, 2020, Pages 251-275, ISSN 0893-6080."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410138904,
                "cdate": 1700410138904,
                "tmdate": 1700610775140,
                "mdate": 1700610775140,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SLpq4rXpqv",
                "forum": "HXZK1Z8tHa",
                "replyto": "DUB68LAQkv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission977/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments for your proposed minor issues."
                    },
                    "comment": {
                        "value": "> Fig.2 omits (a) and (b) mark, while the caption uses (a) and (b).\n\nThank you sincerely for your careful review of our manuscript. We have addressed the identified issue in the paper accordingly.\n\n> Restormer never used window attention.\n\nThank you for pointing out this issue. It was, in fact, a typo and a description that had been deprecated in the early iterations of the manuscript. It was not relevant to the content discussed in the paper. We have now removed it from the manuscript."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410170045,
                "cdate": 1700410170045,
                "tmdate": 1700412263268,
                "mdate": 1700412263268,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "82fIi4QEcD",
                "forum": "HXZK1Z8tHa",
                "replyto": "SLpq4rXpqv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission977/Reviewer_F9tc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission977/Reviewer_F9tc"
                ],
                "content": {
                    "title": {
                        "value": "comments for the author rebuttal"
                    },
                    "comment": {
                        "value": "[Q1] The revised Figs. 2 and 3 enhance the clarity of the proposed mechanism compared to the previous version, addressing my initial concern.\n\n[Q2] My concern has been fully addressed and resolved. However, there is a potential for misunderstanding due to the placement of the reference of sentence in Sec. 4.2: \"This observation implies anamplified locality bias of the network, which has been proven to be capable of facilitating the training(d\u2019Ascolietal.,2021).\". It appears that the conclusion is drawn from ConViT. Therefore, I recommend a revision for clarity.\n\n[Q3] The revised figure in Appendix D compares attention maps in SwinIR. However, the redundancy of attention maps in adjacent layers does not seem apparent. While some are similar, there are actual differences when considering the scales of color bars. I believe the revised figure falls short of explaining the main problem highlighted by this paper.\n\n[Q4] My concern has been fully addressed and resolved.\n\n[W1] See the comments on Q1,Q2,Q3.\n\n[W2] The supplemented ablation study on the proposed CSAU does not sufficiently demonstrate the significance of that architecture. The improved efficiency and effectiveness appear marginal.\n\n[W3] The importance of value residual connection is proved. However, its novelty does not seem notable when compared to the shared attention map of ELAN.\n\n[W4] My concern has been fully addressed and resolved.\n\n[W5] I sincerely agree with the authors' claims as a researcher in this field. However, since only PSNR and SSIM are reported in this paper, it is inevitable that assessment for the performance gains must be based on these metrics. Moreover, compared to the improvements in efficiency, the performance gains are not significant enough for acceptance at this conference. That is, the time complexity should have been more reduced, or performance should have been more improved than other IR methods.\n\n\nIn summary, although the rebuttal addresses some of my concerns, the critical issues, including the urgency of the main problem, novelty, and performance gains, remain unresolved. Therefore, in my view, this paper still falls below the acceptance threshold."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632629232,
                "cdate": 1700632629232,
                "tmdate": 1700632629232,
                "mdate": 1700632629232,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]