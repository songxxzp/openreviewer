[
    {
        "title": "ArchLock: Locking DNN Transferability at the Architecture Level with a Zero-Cost Binary Predictor"
    },
    {
        "review": {
            "id": "aKawfMQhnD",
            "forum": "e2YOVTenU9",
            "replyto": "e2YOVTenU9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8804/Reviewer_boEZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8804/Reviewer_boEZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a methodology to degrade NN performance on new tasks. Specifically, the paper mentions that adversaries may want to adapt a pretrained NN to a new task while violating its terms of use. To mitigate this issue, the presented methods performs a form of neural architecture search to find NN architectures that degrade performance on the tasks for which the NN was not trained."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Demonstrates results on transnasbench.\n- Leverages zero-cost proxies in a new way."
                },
                "weaknesses": {
                    "value": "While this paper is interesting, I am not quite convinced of the motivation behind it. If you want to prevent others from fine-tuning a NN, why even release its parameters to begin with? perhaps in this use-case, the model should not even be released? Also, the definition of \"task\" is very broad. Would additional data be considered a new task? for example, more 32x32 images to be classified into cifar 10 classes for a cifar-10 NN? Or is it just when the classification head is modified?\n\nOther weaknesses include:\n- a limited evaluation on NAS benchmarks, making it harder to appreciate the motivation of the paper. If evaluation was done on some NN that someone wants to protect, then it would've helped.\n- 2% performance degradation on CIFAR-leve NNs is actually quite large.\n- Zero-cost proxies are meant to guage accuracy in general and have not been verified to work for this  task of minimizing out-of-training-distribution accuracy."
                },
                "questions": {
                    "value": "Can you please provide responses to the weaknesses above. The work is interesting but not fully convincing in its current form."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8804/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698689035030,
            "cdate": 1698689035030,
            "tmdate": 1699637107129,
            "mdate": 1699637107129,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "a990eVHieP",
                "forum": "e2YOVTenU9",
                "replyto": "aKawfMQhnD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Motivation"
                    },
                    "comment": {
                        "value": "Thanks for your question and valuable input. However, we think there is a **misunderstanding about our work** and we would like to clarify below.\n\nOur objective is **not to degrade NN performance on new tasks** as you stated in Summary. Instead, our focus is to find model architectures that perform well on their intended tasks while exhibiting low transferability, or degraded performance, on unauthorized target tasks.\n\n**[Motivation]**\n\nThe motivation for our approach **stems from ethical considerations and the potential misuse of machine learning models**, as highlighted in recent works such as [1,2,3,4].  For example, a recent work [1] has demonstrated the open-sourced models, including Stable Diffusion, Latent Diffusion, DALL\u00b7E 2-demo, and DALL\u00b7E mini can be misused to generate unsafe images and hateful memes. Also, another recent work has revealed that fine-tuning aligned language models (e.g., Llama-2) compromises safety [2]. \nThus, model owners are willing to release their models to contribute to the research community while still seeking to control and limit their usage to prevent potential misuse or abuse. \n\nOur proposed method, which reduces transferability, can thus serve as a tool to safeguard the intellectual property (IP) of the model. It **provides a way to protect against unauthorized use, particularly in scenarios where there may be ethical or safety concerns associated with the application of the model on certain tasks**. We will clarify our motivation in the final version.\n\n---\nReference:\n\n[1] Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models, ACM SIGSAC Conference\non Computer and Communications Security (CCS). ACM, 2023.\n\n[2] Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint, arXiv preprint arXiv:2310.03693, 2023.\n\n[3] Self-destructing models: Increasing the costs of harmful dual uses of foundation models. In Proceedings of the 2023 AAAI/ACM\nConference on AI, Ethics, and Society, pages 287\u2013296, 2023.\n\n[4]  Identifying and mitigating the security risks of generative ai. arXiv preprint arXiv:2308.14840, 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8804/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699937138997,
                "cdate": 1699937138997,
                "tmdate": 1699937138997,
                "mdate": 1699937138997,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1xsJUJ9NAp",
                "forum": "e2YOVTenU9",
                "replyto": "aKawfMQhnD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Address Weaknesses"
                    },
                    "comment": {
                        "value": "**[Task Definition]**\n\nIn our work, we did not consider adding more 32x32 images into CIFAR-10 (e.g., via augmentation) for classification as constituting a new task. In the supervised learning setting, a task is defined by both its domain space (data distribution, e.g., CIFAR-10 vs. ImageNet) and its label space (e.g., classification with different classes, classification vs. object detection). Therefore, a task is considered a new task when either its domain space is different or its label space is different.\n\n**[Evaluation and Performance]**\n\nWe would like to emphasize that our objective in this paper is **not centered on protecting specific neural networks (NNs)**. Instead, we aim to discover secure architectures for individuals who seek to build NNs for various tasks. The obtained secure architecture can then be deployed with reduced risk of misuse.\n\nRegarding performance degradation, it is the \"security budget\" for transferability reduction, which is simply a design choice that is faced by any security solutions, i.e., the tradeoff between security and performance. As a generic solution, our method can adjust this budget by modifying $\\lambda$ in the fitness score. A higher $\\lambda$ will result in lower performance degradation on the source task.\n\n**[Zero-cost Proxies]**\n\nIt appears there might be **a misunderstanding regarding our use of zero-cost proxies**. In our work, we employed them to train a performance predictor. For instance, some studies use architectures' accuracy to train NAS predictors [1], whereas we leverage zero-cost proxies. To tackle out-of-training-distribution challenges, the predictor undergoes meta-training. This approach aligns with the rationale presented in [2], where meta-learning has been demonstrated to be effective in out-of-training-distribution scenarios [3].\n\n---\nReference:\n\n[1] Brp-nas: Prediction-based nas using gcns. Advances in Neural Information Processing Systems, 33:10480\u201310490, 2020.\n\n[2]  Rapid neural architecture search by learning to generate graphs from datasets. In International Conference on Learning Representations, 2021.\n\n[3] Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126\u20131135. PMLR, 2017."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8804/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699937430958,
                "cdate": 1699937430958,
                "tmdate": 1699937430958,
                "mdate": 1699937430958,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f0z0EVQvz7",
                "forum": "e2YOVTenU9",
                "replyto": "aKawfMQhnD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind Request"
                    },
                    "comment": {
                        "value": "Considering you are the **sole reviewer who has provided a negative rating**, we value your feedback and would like to ensure that all your concerns are addressed. We sincerely appreciate your acknowledgment that our work is interesting. **Our work represents a pioneering effort in providing a defense against unauthorized transfer at the architectural level, and the compelling results obtained underscore the effectiveness of our approach.** We kindly request your reconsideration of the rating, taking into account the clarification and context provided."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8804/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699937502786,
                "cdate": 1699937502786,
                "tmdate": 1699937502786,
                "mdate": 1699937502786,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HxHJlTsdkN",
                "forum": "e2YOVTenU9",
                "replyto": "aKawfMQhnD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your response"
                    },
                    "comment": {
                        "value": "We would like to emphasize the practical significance of our contribution to the field. The problem we tackled holds substantial practical relevance, and our motivation is thoroughly justified. We understand the importance of a fair evaluation and believe that the current rating may underestimate the true value of our work. We hope that you can consider raising the rating score based on the information we provided."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8804/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362330174,
                "cdate": 1700362330174,
                "tmdate": 1700362330174,
                "mdate": 1700362330174,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6Ie4WrIt9T",
                "forum": "e2YOVTenU9",
                "replyto": "HxHJlTsdkN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8804/Reviewer_boEZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8804/Reviewer_boEZ"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors' clarifications"
                    },
                    "comment": {
                        "value": "Thank you for responding to my concerns regarding the motivation of your work. The stable diffusion example that you gave makes sense and I do have a better understanding of the work's purpose thanks to your response. I do hope that this makes it to your updated manuscript.\n\nHowever, there are still some issues:\n1- If image generation and diffusion models are the key motivating example, why not try your method on this class of models instead of the NB201 and TransNB evaluations?\n2- I still maintain that a 2% accuracy degradation is too high on a CIFAR-level task. \n\nNot that I do understand that you want to \"degrade performance on unauthorized target tasks\" - this is what I meant by \"new tasks\" in my summary.\n\nThe presented work performs NAS with an additional objective of _low_ accuracy on target or unseen tasks. For the case that the target task is known, directly comparing to conventional multiobjective NAS (e.g. table 3) with an additional objective of degrading accuracy on target task would make sense. This would make for a more fair comparison to Archlock-TK.\n\nI'm reluctant to increase my score because of the concerns above. If there was a 4 rating, perhaps I would increase to that. The work in its current form has missing comparisons, and unrealistic evaluation only on NAS benchmarks which may not necessarily be transferrable to the real-world scenarios described by the authors."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8804/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583145141,
                "cdate": 1700583145141,
                "tmdate": 1700583145141,
                "mdate": 1700583145141,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ixj5yaT8NA",
                "forum": "e2YOVTenU9",
                "replyto": "aKawfMQhnD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply.\n\n**[For issue 1]**\n\nWe used image generation and diffusion models as examples to address your question, \"Why even release its parameters to begin with?\" and to provide a better understanding. However, the vulnerability is general and not limited to generative tasks. Also, like all other work conducted on NAS Benchmarks, the objective is to provide a fair comparison. Your current concern seems centered on the existing NAS Benchmarks being less meaningful (or are you questioning the practical significance of NAS?). However, it's crucial to note that constructing NAS benchmarks is not the primary focus of our work. If someone proposes a diffusion model-based benchmark for generative tasks, we are willing to test our method on that. Additionally, our method holds promise for real-world scenarios as NAS has already been employed to design high-performance models for tasks. With our approach, the resulting models can not only exhibit superior performance but also boast enhanced security measures. \n\n**[For issue 2]**\n\nWe have explained that accuracy degradation is a design choice, and one can adjust it by modifying \n\u03bb in the fitness score. We would like to provide more experiments as a discussion in the appendix.\n\n**[For the newly raised question: comparison for TK]**\n\nAs we mentioned in the introduction, our focus is on how to determine and simulate the potential target task if it is not specified, and our method centers on this case and proposes TU. **Archlock-TK is already a comparison method designed for TU, and is not meaningful to include other comparison methods for our comparison method.** Also, we have explained why current cross-task or multi-objective NAS cannot be compared with TU in Section 2.1. \n\n**[Summary]**\n\n**Our formulation and methodology constitute the primary contribution we aim to make to the field, which has been acknowledged by other reviewers who think that our formulation is solid.**  We are the first to address this vulnerability by proposing a methodology leveraging NAS, thus we experiment with NAS benchmarks. Since more advanced benchmarks have not been proposed,  the experiment design you suggested is challenging at this time. However, our current experiments are sufficient to demonstrate the effectiveness of our method, and it not undermine our contribution. It's crucial to emphasize that our motivation is well-justified, the vulnerability is practical, and our methodology is well-formulated.\n\nMoreover, **considering that we were the first to identify this vulnerability and propose an effective mitigation method, our work has the potential to raise awareness among other researchers regarding this vulnerability. This could pave the way for them to propose more advanced solutions, building upon the foundation laid by our work.** \n\n**If you also agree that a rating of 3 is underrated, we kindly encourage you to raise it to the next level.**"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8804/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589476382,
                "cdate": 1700589476382,
                "tmdate": 1700598117869,
                "mdate": 1700598117869,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PD4dX4zhRs",
            "forum": "e2YOVTenU9",
            "replyto": "e2YOVTenU9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8804/Reviewer_umc9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8804/Reviewer_umc9"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a cross-task NAS framework to find an architecture to mitigate unauthorized DNN transferability.  A binary predictor using multiple zero-cost proxies is proposed to accelerate the NAS procedure. The results and the ablations demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The whole formulation of reducing transferability at the architecture level involved with architecture search is solid.\n2. The proposed nas method based on binary predictor is efficient and effective in designing model architectures with low transferability."
                },
                "weaknesses": {
                    "value": "1. Since the proposed method is based on a predictor, maybe it is better to cite a series of predictor-based NAS work. For example, PINAT: A Permutation INvariance Augmented Transformer for NAS Predictor AAAI 2023 TNASP: A Transformer-based NAS Predictor with a Self-evolution Framework NeurIPS 2021 and so on.\n2. It seems that there is no training details about the binary predictor, What is the training cost for this predictor? Is one pre-trained predictor suitable for processing the architectures from different search spaces?"
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8804/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698859741340,
            "cdate": 1698859741340,
            "tmdate": 1699637107023,
            "mdate": 1699637107023,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nRqnyNpb5L",
                "forum": "e2YOVTenU9",
                "replyto": "PD4dX4zhRs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for pointing out these references; we will cite these works in the proper place in the final version.\n\n**[Training details]**\n\nThe training process of our binary predictor has been provided in Section 4.3. Specifically, the predictor is constructed using a four-layer multi-layer perceptron (MLP). We also conducted experiments with various designs, including different architecture parameters and optimizers. Some of the experiment records are presented in the table below. The selected design, highlighted in bold, achieved an accuracy of 87% with a training cost of approximately 5 GPU hours. We would like to include this information as supplemental material in the final version.\n\n\n| # of Layers | Layer Configuration | Optimizer | Accuracy |\n|:-------------:|:---------------------------:|:-----------:|:----------:|\n| 3           | [256, 64, 32]        | SGD       | 85%      |\n| 4           | [256, 64, 32, 16]    | SGD       | 83%      |\n| 4           | [256, 64, 32, 16]    | Adam      | 87%      |\n| **4**       | **[128, 64, 32, 16]**   | **Adam**     | **88%**     |\n|5\t|[128, 64, 32, 16, 8]\t|Adam\t|87%|\n\nThe experiments in the above table share common settings, which are as follows:\n\n- Loss function: BCEWithLogitsLoss.\n- Learning rate: set to 0.05.\n- Learning rate scheduler: StepLR with a step_size of 5 and a gamma of 0.5.\n\nSince the input of the pre-trained predictor consists of the architecture embedding (see Fig. 1), once the search space is defined differently (e.g., DARTS vs NAS-Bench-201), the pre-trained predictor would not fit anymore. It is worth noting that it is common to train a new predictor for different search spaces, including the two papers you mentioned.  \nHowever, given that the cell-level search space of TransNAS-Bench-101 is a subset of NAS-Bench-201, our pre-trained predictor can be applied to both.\n\nGiven that **training detail is the only concern raised**, and considering **your acknowledgment that our formulation is solid and the proposed framework is efficient and effective, we kindly request that you reconsider your rating for this work**. We believe the additional information provided addresses the concern and contributes to a more thorough understanding of our approach. We appreciate your time and consideration."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8804/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699938057827,
                "cdate": 1699938057827,
                "tmdate": 1699938057827,
                "mdate": 1699938057827,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lnFe64lSi3",
                "forum": "e2YOVTenU9",
                "replyto": "PD4dX4zhRs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would appreciate confirmation that we have adequately addressed all your concerns. Thank you once again for your time."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8804/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362548422,
                "cdate": 1700362548422,
                "tmdate": 1700362548422,
                "mdate": 1700362548422,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VKtwvyoThI",
            "forum": "e2YOVTenU9",
            "replyto": "e2YOVTenU9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8804/Reviewer_ySE1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8804/Reviewer_ySE1"
            ],
            "content": {
                "summary": {
                    "value": "This paper prposes ArchLok to mitigate unauthorizaed DNN transfer. ArchLock first encodes the NN architecture to evaluate rank two architectures with task embeddings, then perform neural architecture search to find archs that are good on source tasks but bad on target tasks. Evaluation on NAS-Bench-201 and Trans-Bench-101 demonstrate that ARchLock significantly reduces the transferbility."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "By addressing security at the architecture level, ArchLock potentially fills a gap left by other security measures that focus on the model parameter level. This provides a more holistic defense strategy for DNN models.\n\nArchLock focuses more on the architecture rankings rather than the actual performance numbers, and utilize efficient zero-cost proxies as supervision. This approach can be scaled to any size of architecture pool and reduces the cost of training several architectures from scratch.\n\nExperiments on NAS-Bench-201 and TransNAS-Bench-101 demonstrate the effectiveness of ArchLock. It can effectively degrade the performance on target tasks by up to 30% while preserving the performance on source tasks."
                },
                "weaknesses": {
                    "value": "The details of S / TU / TK are not clearly described. Algorithm 1 shows the cross-task search when the target task is known, but it does not discuss how the other two baselines are performed. Additionally, it is still unclear how the GraphEncoder (Figure.1) is executed and how task embedding is extracted. How much overhead does  task embedding take for each new task?\n\nAre the numbers in Tab 1 and 2 real measurements, or are they directly taken from NAS-Bench/TransNAS-Bench? The zero-cost proxies/predictors are trained on the same set of datasets, which may lead to potential overfitting. Evaluating on unseen and large-scale datasets (e.g., ImageNet [can be a subset with ful 224x224 resolution], Miniplaces) is necessary to demonstrate effectiveness.\n\nArchLock aims to design architectures that show less transferability on new tasks, but it does not discuss what kind of architecture leads to poor transferability. For example, do different datasets dislike different architecture designs, or there is an type of architecture that transfers bad generally on all tasks? It would be beneficial to provide visualizations and discussions of general un-transferable architectures so that further work can gain inspiration and insights."
                },
                "questions": {
                    "value": "See weakness above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8804/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699259390677,
            "cdate": 1699259390677,
            "tmdate": 1699637106916,
            "mdate": 1699637106916,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vHZDfo0qLB",
                "forum": "e2YOVTenU9",
                "replyto": "VKtwvyoThI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your valuable comments. We would like to address your concerns one by one. \n\n**[Details of S/TU/TK]**\n\nFor S (source-only search scheme), it indeed follows the common NAS search scheme and does not consider the target task during its search process. Specifically, instead of using Eq. (5), its fitness score can simply be $f=-\\\\mathcal{S}_{rank}$, where higher ranks correspond to lower values, resulting in a higher fitness score.\n\nMoreover, Algorithm 1 indeed encapsulates the cross-task search process for both TU and TK, while the difference between them can be better understood with details in Line 11-18 of Algorithm 2 in Appendix C. Specifically, as mentioned in Section 4.2, the target task is known for TK, so it only has one $\\\\mathcal{T}$ and goes through one loop in Lines 11-18 with $z_\\\\mathcal{T}$ extracted from the known/true target task. In contrast, for TU with no knowledge of the true target task, we simulated 10 $z_{\\\\mathcal{T}_i}$ in our experiments following the method described in Section 3.1. Thus, TU has to go through 10 loops according to Lines 11-18 in Algorithm 2. After the search terminated, we then examined the performance of the searched architecture on the true target task, which remained unknown during the search. This is a more challenging defense scenario, but TU still strikes a better balance between the performance on the source task and transfer vulnerability compared to S.\n\n**[Graph Encoder]**\n\nThe graph encoder converts the directed acyclic graph of the architecture cell into an adjacency matrix, which will be flattened into a 1-D array as the architecture embedding.\n\nFor example, in NAS-Bench-201, each architecture consists of a predefined skeleton with a stack of the searched cell (see Fig 1 in [1]). Architecture search is thus transformed into the problem of finding a good cell. Each cell is represented as a directed acyclic graph with 6 edges, each associated with an operation selected from a predefined set (i.e., [zeroize, skip-connect, 1x1 conv, 3x3 conv, 3x3 avg pool]). To illustrate, consider the following cell configuration:\n\n- node-0: the input tensor\n- node-1: conv-3x3( node-0 )\n- node-2: conv-3x3( node-0 ) + avg-pool-3x3( node-1 )\n- node-3: skip-connect( node-0 ) + conv-3x3( node-1 ) + skip-connect( node-2 )\n\nWe apply one-hot encoding to encode the cell, and get its adjacency matrix (with the row indicating the edge and the column indicating the operation):\n\n$$\n\\\\left(\\\\begin{array}{ccccc} \n 0 & 0 & 0 & 1 & 0 \\\\\\\\\n  0 & 0 & 0 & 1 & 0 \\\\\\\\\n  0 & 0 & 0 & 0 & 1 \\\\\\\\\n  0 & 1 & 0 & 0 & 0 \\\\\\\\\n  0 & 0 & 0 & 1 & 0 \\\\\\\\\n  0 & 1 & 0 & 0 & 0 \\\\\\\\\n\\\\end{array}\\\\right) $$\n\nThus, our graph encoder can convert an architecture into a 6x5 matrix, which is then flattened into a 1-D array of size 30 as the architecture embedding.\n\n**[Task Embedding]**\n\nRegarding task embedding, we followed the method outlined in [2] as we mentioned in the paper (we refer readers to Section 3.1 in [2] for more details), with its implementation available at [this link](https://github.com/awslabs/aws-cv-task2vec). In our implementation, we used ResNet50 pre-trained on ImageNet as a feature extractor, and the overhead for computing a task embedding mainly comes from training the \"head\" layer of the given task. Given the pre-trained feature extractor already provides rich representation, only re-training the head can converge fast and the whole process can be done efficiently.\n\n\n---\nReference:\n\n[1] Nas-bench-201: Extending the scope of reproducible neural architecture search. In International Conference on Learning Representations (ICLR), 2020.\n\n[2] Task2vec: Task embedding for meta-learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6430\u20136439, 2019."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8804/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699939564977,
                "cdate": 1699939564977,
                "tmdate": 1699939564977,
                "mdate": 1699939564977,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "I3oa5tLGqQ",
                "forum": "e2YOVTenU9",
                "replyto": "VKtwvyoThI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**[Measurement and Overfitting]**\n\nThe reported numbers in Tables 1 and 2 are indeed derived from the benchmark to ensure a fair comparison by avoiding the influence of hyperparameter tuning.\n\nRegarding the zero-cost predictor, we would like to emphasize that it is not trained on the same set of datasets. Instead, it is meta-trained on ImageNet, as explicitly mentioned in Section 4.3. This intentional separation from the datasets used for the main evaluation mitigates concerns about overfitting, as the predictor is exposed to a diverse range of data during meta-training. We believe this design choice strengthens the generalization capabilities of our approach.\n\n**[Transferability Discussion]**\n\nBased on the observation from our experiments, there is not a type of architecture that transfers badly for all tasks. However, we appreciate your constructive suggestion, which brings up an interesting direction that is worth exploring in the future.\n\n---\nWe hope we have addressed all your concerns. Please let us know if you have any further questions. **If not, we kindly request that you reconsider the rating, considering your acknowledgment of the novelty of our work and its significance as a holistic defense strategy for DNN models.** Thanks for your time!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8804/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699940026113,
                "cdate": 1699940026113,
                "tmdate": 1699940026113,
                "mdate": 1699940026113,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "03gJTOOKEV",
                "forum": "e2YOVTenU9",
                "replyto": "VKtwvyoThI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8804/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Any remaining concerns?"
                    },
                    "comment": {
                        "value": "We are looking forward to your feedback and wondering if any concerns remain."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8804/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362440735,
                "cdate": 1700362440735,
                "tmdate": 1700362440735,
                "mdate": 1700362440735,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ipdn958t0I",
                "forum": "e2YOVTenU9",
                "replyto": "03gJTOOKEV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8804/Reviewer_ySE1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8804/Reviewer_ySE1"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' reponse partially addressed my concerns.  The questions about S/TU/TK, GraphEncoder, Task Embeddings now are clear to me. However, I am still holding concerns about transferability and general patterns for less-transferable architecture.\n\nI will keep my current reviews."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8804/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572040637,
                "cdate": 1700572040637,
                "tmdate": 1700572040637,
                "mdate": 1700572040637,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]