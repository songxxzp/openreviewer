[
    {
        "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"
    },
    {
        "review": {
            "id": "GTHpbnPEfO",
            "forum": "N8N0hgNDRt",
            "replyto": "N8N0hgNDRt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4520/Reviewer_mqn7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4520/Reviewer_mqn7"
            ],
            "content": {
                "summary": {
                    "value": "The authors aim to bridge the noticeable performance gap of open-access LLMs in solving complex mathematical problems. The paper introduces a framework that includes (i) a diverse dataset of math problems generated through transformations such as forward-backward reasoning and self-verification (MetaMathQA) and (ii) open-access LLMs (llama series) fine-tuned on MetaMathQA. Experiments on benchmark datasets demonstrate clear and impressive gains with MetaMath over other open LLMs. Additionally, the authors conduct insightful analyses, highlighting the role of question diversity in enhancing LLM performance."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- **Novel Approach:** The paper introduces a unique data augmentation strategy for mathematical reasoning. The MetaMath framework is generic and can be easily extended to other numerical reasoning datasets.\n\n- **Rich and Comprehensive Analysis:** The analysis is rich and comprehensive, offering numerous insights into data augmentation and the fine-tuning of LLMs for reasoning tasks."
                },
                "weaknesses": {
                    "value": "- **Potential for Benchmark Hacking:** Given the experimental setup, there is a slight risk that the proposed approach could lead to benchmark hacking.\n\n- **Dependence on High-Quality Initial Questions:** Given that both datasets used have extensive training data available, the performance of the proposed method in the absence of high-quality initial questions available for mutation remains uncertain.\n\nTo some extent, both the weaknesses can be addressed by doing 0-shot evaluation on some other datasets like DROP (https://allenai.org/data/drop)"
                },
                "questions": {
                    "value": "In Table 3, MetaMath finetuning always begins with the AnsAug split, right? Do the authors have any thoughts on what would happen if we start training from (say) SV or FOBAR?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4520/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4520/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4520/Reviewer_mqn7"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4520/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789148173,
            "cdate": 1698789148173,
            "tmdate": 1699636428876,
            "mdate": 1699636428876,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rEnAIqxFZC",
                "forum": "N8N0hgNDRt",
                "replyto": "GTHpbnPEfO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4520/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4520/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mqn7"
                    },
                    "comment": {
                        "value": "We would like to sincerely thank the reviewer for the useful comments on our work. We take every comment seriously and hope our response can address the reviewer\u2019s concerns. If there are any remaining questions, we are more than happy to address them.\n\n> Q1. **Potential for Benchmark Hacking & Dependence on High-Quality Initial Questions. To some extent, both the weaknesses can be addressed by doing 0-shot evaluation on some other datasets like DROP (https://allenai.org/data/drop)**\n\n**A1.** As suggested, we perform a zero-shot evaluation on the DROP dataset to compare MetaMath with baseline models. Since we focus on mathematical reasoning, we only consider the DROP questions with numerical answers. The table below shows the testing accuracy. As can be seen, MetaMath-7B and MetaMath-13B still outperform the baseline models by a large margin, which shows MetaMath does not suffer benchmark hacking on GSM8K and MATH datasets. We have added the results in the updated paper (Table 10 in Appendix A.8).\n\n| Method | #params | Accuracy (Exact Match) |\n|  :---- |  :----: | :----: |\n|SFT | 7B | 25.8 |\n|RFT| 7B | 26.7 |\n|WizardMath| 7B | 31.5|\n|MetaMath|7B| **37.1**|\n|||\n|WizardMath|13B| 46.4|\n| MetaMath |13B| **49.5** |\n\n---\n\n\n> Q2. **In Table 3, MetaMath finetuning always begins with the AnsAug split, right? Do the authors have any thoughts on what would happen if we start training from (say) SV or FOBAR?**\n\n**A2.** We apologize for the confusion caused by Table 3. When using data from multiple bootstrapping methods for finetuning, we mix all augmented data instead of using them sequentially (AnsAug -> Rephrasing -> SV -> FOBAR). In other words, at each training step, we randomly take a batch of samples from the mixed augmented data for updating the model parameters."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4520/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700346057667,
                "cdate": 1700346057667,
                "tmdate": 1700346057667,
                "mdate": 1700346057667,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n2nEMwQsbO",
                "forum": "N8N0hgNDRt",
                "replyto": "GTHpbnPEfO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4520/Reviewer_mqn7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4520/Reviewer_mqn7"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for including the results on DROP and for the clarification. The gains are impressive and convincing. Did you get a chance to try the 70B model? I understand if running inference on 70b is not possible due to resource constraints."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4520/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598100605,
                "cdate": 1700598100605,
                "tmdate": 1700598299807,
                "mdate": 1700598299807,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "elmaSv3gZx",
            "forum": "N8N0hgNDRt",
            "replyto": "N8N0hgNDRt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4520/Reviewer_AmB6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4520/Reviewer_AmB6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to fine-tune smaller open-source LLMs (LIama) based on data augmentation from large closed-source LLMs (GPT-3.5). A set of data augmentation techniques are employed: answer augmentation, question bootstrapping by rephrasing, and backward reasoning, including self-verification and FOBAR. The data augmentation is applied to the GSM8K and MATH datasets. The augmented MetaMathQA dataset is then used to fine-tune the LIama model series. \n\nExperiments on the fine-tuned 7B, 13B, and 70B LIama models demonstrate significant improvements over various baselines. The authors also made insightful analyses regarding how the perplexity and diversity of the training data affect performance, the reversal mathematical ability, reasoning paths with incorrect answers, as well as data quantity."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed MetaMathQA dataset will be a very valuable contribution to the community.\n2. The proposed data augmentation techniques achieve good performances compared to various baselines.\n3. The authors made insightful analyses regarding different factors affecting the performance of such small LM fine-tuning. This analysis will not only contribute to the specific topic of mathematical reasoning but also will help the general direction of small LM fine-tuning as well."
                },
                "weaknesses": {
                    "value": "1. Some baseline approaches to compare are missing, e.g., [1, 2] and code-based LLMs like [3]\n2. The ablation study is not comprehensive enough. Only the 7B model is tested. Table 3 is confusing - should add a line breaker between SFT and MetaMath. \n\n[1] MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning, Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, 2023\n\n[2] Platypus: Quick, Cheap, and Powerful Refinement of LLMs, Ariel N. Lee, Cole J. Hunter, Nataniel Ruiz, 2023\n\n[3] Code Llama: Open Foundation Models for Code, Rozi\u00e8re et al., 2023"
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4520/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816549753,
            "cdate": 1698816549753,
            "tmdate": 1699636428807,
            "mdate": 1699636428807,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jVO8MHA0fO",
                "forum": "N8N0hgNDRt",
                "replyto": "elmaSv3gZx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4520/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4520/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AmB6"
                    },
                    "comment": {
                        "value": "We would like to sincerely thank the reviewer for the useful comments on our work. We take every comment seriously and hope our response can address the reviewer\u2019s concerns. If there are any remaining questions, we are more than happy to address them.\n\n> Q1. **Some baseline approaches to compare are missing, e.g., MAmmoTH, Platypus and Code LLaMA.**\n\n**A1.** As suggested, we added the mentioned baselines in Table 2 of the updated paper and the additional comparisons are shown in the table below. As can be seen, MetaMath outperforms other CoT-based models and Code-LLaMA with the same number of parameters.\n\n|     | \\#params |  GSM8K | MATH |\n|  :---- |  :----: |  :----: | :----: | \n| Code-LLaMA | 7B | 25.2 | 13.0  |\n| MAmooTH-CoT | 7B | 50.5  | 10.4   | \n| MetaMath |7B|   66.5 | 19.8    |\n|  ||    |      |\n| Code-LLaMA |13B | 36.1 | 16.4 |\n| Platypus | 13B |  25.7 |  2.5   | \n| MAmooTH-CoT | 13B|  56.3  |  12.9  | \n| MetaMath | 13B | 72.3  |  22.4  |\n|  ||    |      |\n| Platypus | 70B | 70.6 | 15.6  | \n| MAmooTH-CoT | 70B |   72.4  | 21.1  | \n| MetaMath | 70B | 82.3 |  26.6    |\n\n\n---\n\n> Q2. **The ablation study is not comprehensive enough. Only the 7B model is tested.**\n\n**A2.** As suggested, we conducted an ablation study on LLaMA-2-13B and the results are shown below. The observation is consistent with our ablation study on the 7B model in Section 4.3: Combing answer augmentation and rephrasing augmentation data for finetuning leads to a slightly higher accuracy. The accuracy can be further improved by merging the FOBAR and SV augmentation data. We added the results in the updated paper (Table 9 in Appendix A.7).\n\n| Method    | AnsAug | Rephrasing |  SV | FOBAR | GSM8K | MATH|\n|  :---- |  :----: | :----: | :----: | :----: |   :----: | :----: | \n| SFT       |  &#10005; |  &#10005;  |  &#10005;  |  &#10005;   |50.9 |4.5  | \n| MetaMath  |  &#10003; |  &#10005;  |   &#10005;   |  &#10005; |66.0 |5.5  | \n| MetaMath  |  &#10005; |  &#10003;  |   &#10005;   |  &#10005; |67.5 |5.9  |\n| MetaMath  |  &#10003; |  &#10003;  |   &#10005;   |  &#10005; |68.1 |5.8  | \n| MetaMath  |  &#10003; |  &#10003;  |   &#10003;   |  &#10003; |72.3 |7.2  | \n\n\n---\n\n> Q3. **Table 3 is confusing - should add a line breaker between SFT and MetaMath**\n\n**A3.** Thanks for your suggestion and we have fixed it accordingly in the updated version."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4520/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700345939769,
                "cdate": 1700345939769,
                "tmdate": 1700345939769,
                "mdate": 1700345939769,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yqffZEG15C",
            "forum": "N8N0hgNDRt",
            "replyto": "N8N0hgNDRt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4520/Reviewer_pSoC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4520/Reviewer_pSoC"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method for data augmentation to train LLMs for improving mathematical reasoning.\nThe authors combine several existing techniques such as question re-writing, self-verification, forward-backward reasoning, and answer augmentation to create a larger dataset called MetaMathQA.\nThe paper shows that this dataset can be distilled back into the model resulting in a fine-tuned model that outperforms several baselines on two benchmarks of mathematical reasoning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed approach for bootstrapping seems sound and also results in better mathematical reasoning performance through thorough experimentation\n- The authors also perform ablations that show that all of the bootstrapping techniques help improve performance\n- The paper is well presented and easy to follow"
                },
                "weaknesses": {
                    "value": "- The major weakness I see is the lack of novelty. The paper in essence combines existing methods for bootsrapping. \n\nNevertheless, I feel that the empirical findings of the paper would be interesting to the community and therefore vote for acceptance"
                },
                "questions": {
                    "value": "- It is interesting that even reasoning paths with incorrect answers can be useful. Do you try to train using both correct and incorrect reasoning paths? Does this perform better than just correct?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4520/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4520/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4520/Reviewer_pSoC"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4520/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699024698471,
            "cdate": 1699024698471,
            "tmdate": 1700598664203,
            "mdate": 1700598664203,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5NxQHMC7pU",
                "forum": "N8N0hgNDRt",
                "replyto": "yqffZEG15C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4520/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4520/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pSoC -- Part 1"
                    },
                    "comment": {
                        "value": "We would like to sincerely thank the reviewer for the useful comments on our work. We take every comment seriously and hope our response can address the reviewer\u2019s concerns. If there are any remaining questions, we are more than happy to address them.\n\n> Q1. **The major weakness I see is the lack of novelty. The paper in essence combines existing methods for bootsrapping.**\n\n**A1.** This paper proposes a novel question bootstrapping idea to augment the training dataset. Compared with previous methods (e.g., SFT, RFT, WizardMath), our MetaMath introduces two novel augmentation methods:\n\n(i) **Rephrasing questions.** Existing methods (e.g., RFT and WizardMath) focus on enlarging the answer data, which can be achieved by sampling more answers from LLMs. MetaMath proposes to create more questions, which is more challenging. To the best of our knowledge, **we are the first to use a rephrasing prompting method for augmenting questions**.\n\n(ii) **Question bootstrapping by backward reasoning.**  Backward reasoning ability is crucial to solving many mathematical questions. However, the training set of mathematical tasks (e.g., GSM8K) lacks such data to improve backward reasoning. To deal with this problem, we introduce the templates (i.e., masking a number in the question and asking the LLM to predict the masked number) proposed in Self-Verification and FOBAR to create backward questions. **Note that both Self-Verification and FOBAR use backward reasoning for verification rather than data augmentation.** \n\nIn summary, our paper proposes rephrasing questions and bootstrapping by backward reasoning to augment a diverse dataset --- MetaMathQA. MetaMath, which finetunes from state-of-the-art open-source LLMs on our MetaMathQA dataset, demonstrating excellent elementary mathematical problem-solving capability. In addition, we have released the MetaMathQA dataset for public use to improve the forward and backward reasoning capabilities of the LLM."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4520/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700345794411,
                "cdate": 1700345794411,
                "tmdate": 1700345794411,
                "mdate": 1700345794411,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dz2ufxWtbd",
                "forum": "N8N0hgNDRt",
                "replyto": "mAul2EAOz8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4520/Reviewer_pSoC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4520/Reviewer_pSoC"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the additional experiments"
                    },
                    "comment": {
                        "value": "Thanks for your response and additional experiments. I am more confident in my assessment now. \n\nDo you have any intuitions into why the incorrect paths improve accuracy but when used in tandem with the correct paths are more detrimental?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4520/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598642734,
                "cdate": 1700598642734,
                "tmdate": 1700598642734,
                "mdate": 1700598642734,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4n1pZRnYQX",
            "forum": "N8N0hgNDRt",
            "replyto": "N8N0hgNDRt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4520/Reviewer_nyhB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4520/Reviewer_nyhB"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes MetaMath, a fine-tuned language model specializing in mathematical reasoning. The proposed method includes bootstrapping mathematical questions by rewriting them from multiple perspectives to create the new dataset MetaMathQA. The LLaMA-2 models are then fine-tuned on the MetaMathQA dataset. Experimental results on two popular benchmarks, GSM8K and MATH, show that MetaMath significantly outperforms other open-source large language models. The authors also introduce the concept of question diversity when creating the MetaMathQA dataset, which is important in reasoning directions, and highlight that backward reasoning questions are very helpful for large language models in understanding mathematical knowledge without memorization."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method of bootstrapping mathematical questions by rewriting them from multiple perspectives is novel.\n2. The authors construct a new dataset, MetaMathQA, by combining forward and backward mathematical questions with augmented answers. This dataset could help the community with advancing progress in mathematical reasoning.\n3. The experiments are pretty extensive in that they have compared to a lot of models/approaches. (Although there are clear weaknesses in the experiments, will discuss in the weaknesses.)\n4. The paper is well-organized and clearly written, making it easy to understand the motivation behind the proposal, the method, the dataset construction, and the experiments conducted."
                },
                "weaknesses": {
                    "value": "1. It is unclear how the proposed bootstrapping approach generalizes to other types of multi-hop reasoning problems.\n2. The ablation of the method is not rigorously done.  It is unclear if we keep increasing the number of AnsAug, we can get similar improvement."
                },
                "questions": {
                    "value": "I think it is necessary to show that increasing AnsAug to 395K cannot further increase the performance in order to prove the point made in the paper. I understand that this experiment can be costly, so doing this in a small scale to show the trend is good enough. I would love to see a curve on the accuracy vs. # of AnsAug and a curve on the accuracy vs # of a mixed of different augmentations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4520/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699202394932,
            "cdate": 1699202394932,
            "tmdate": 1699636428579,
            "mdate": 1699636428579,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yVz8kaJ05e",
                "forum": "N8N0hgNDRt",
                "replyto": "4n1pZRnYQX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4520/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4520/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nyhB -- Part 1"
                    },
                    "comment": {
                        "value": "We would like to sincerely thank the reviewer for the valuable comments on our work. We take every comment seriously and hope our response can address the reviewer\u2019s concerns. If there are any remaining questions, we are more than happy to address them.\n\n> Q1: **It is unclear how the proposed bootstrapping approach generalizes to other types of multi-hop reasoning problems.**\n\n**A1.** The core idea of the proposed bootstrapping approach is to diversify the questions in both forward and backward reasoning directions. Our approach can be extended to other reasoning tasks. We conducted an additional experiment to show a successful application of our bootstrapping method to the [Game of 24](https://en.wikipedia.org/wiki/24_(puzzle)), which involves multi-hop reasoning steps to reach 24 given 4 numbers. \nGiven an original question with 4 numbers (2,3,4,12), its answer (2\\*3 - 4) \\* 12 is an expression that can reach 24.\nWe can apply answer augmentation and question bootstrapping to generate more question-answer pairs.\n\n**Answer augmentation.** The solutions of obtaining 24 given 4 numbers may not be unique, e.g., 2\\*12\\*(4-3) = 24 is another solution to the given numbers (2,3,4,12). Hence, for a question with 4 numbers, we enumerate all the correct solutions and obtain the answer augmentation data.\n\n**Question bootstrapping.** Game of 24 can be extended to **Game of $n$**, i.e., given 4 numbers (one number is 24), the goal is to obtain $n$ using basic arithmetic operations (+, -, \\*, /). We use Game of n for question bootstrapping. We replace a number in the original question with 24 and the question is to obtain the number. This idea is similar to creating backward questions in our paper, i.e., masking a number in the question and asking the LLM to predict the number. For a Game of 24 question, we can bootstrap it and obtain 4 Game of $n$ questions, as an example shown in the table below.\n\n|          | Bootstrapping1 | Bootstrapping2 | Bootstrapping3|Bootstrapping4| \n|  :----   | :----: | :----: | :----: | :----: | \n| Input (4 numbers)     | 24, 3, 4, 12   |  2, 24, 4, 12 | 2, 3, 24, 12  | 2, 3, 4, 24  |\n| Target (n)  | 2 | 3 | 4 | 12|\n| Solution (Valid Expression)      |     (4-3)/(12/24) = 2   | (24/12+4)/2 = 3| 24/12\\*3-2 = 4 | (24/4-2)\\*3 = 12 | \n\n**Game of 24 Setup.** We randomly select 1362 Game of 24 questions from 4num.com,\nwhere 681 questions are for training and the remaining 681 questions are held-out for testing.\nWe apply the above augmentation methods to generate more training data from the 681 questions. We apply answer augmentation by enumerating all the correct forward solutions and obtain an AnsAug dataset consisting of 6052 question-answer pairs. We apply question bootstrapping to obtain a bootstrapping dataset (consisting of 2724 Game of n question-answer pairs). To verify the effectiveness of the bootstrapping approach, we randomly sample 4000 question-answer pairs (Game of 24) from the AnsAug datasets, and 2052 backward question-answer pairs (Game of n) from the bootstrapping dataset. We finetune LLaMA-2-7B on AnsAug and the mixed data separately for comparison.\n\n**Results on Game of 24.** Table below shows the testing accuracy. As can be seen, our proposed augmentation approaches (AnsAug and AnsAug+Bootstrapping) have higher accuracy than SFT, which trains on the original 681 question-answer pairs. Furthermore, using question bootstrapping for augmentation can boost the performance of AnsAug. Hence, the proposed bootstrapping method is useful for Game of 24.\n\n|            |  #Samples      | Accuracy  |\n|  :----     |:----:     |  :----:   |\n|SFT | 681 | 1.8 |\n|AnsAug | 6052 | 10.2 |\n|AnsAug + Bootstrapping | 6052 | **12.0** | \n\n**Results on Game of n.** For each question-answer pair in the testing set of **Game of 24**, we create 4 more testing questions of **Game of n** using the above question bootstrapping method. In total, we obtain 3405 testing questions. Table below shows the testing accuracy. Again, using our augmentation methods performs better than SFT by a large margin. Furthermore, AnsAug + Bootstrapping performs the best, demonstrating our proposed method is also useful for Game of n.\n\n|            |  #Samples      | Accuracy  |\n|  :----     |:----:     |  :----:   |\n|SFT | 681 | 0.8 |\n|AnsAug | 6052 | 3.0 |\n|AnsAug + Bootstrapping | 6052 | **8.1** | \n\nWe have included all the above experiments in Appendix A.4 of the updated paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4520/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700345576922,
                "cdate": 1700345576922,
                "tmdate": 1700345576922,
                "mdate": 1700345576922,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]