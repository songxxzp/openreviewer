[
    {
        "title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources"
    },
    {
        "review": {
            "id": "nIjM7qFODb",
            "forum": "cPgh4gWZlz",
            "replyto": "cPgh4gWZlz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8874/Reviewer_TzUb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8874/Reviewer_TzUb"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed chain-of-knowledge (CoK), a novel framework that augments LLMs by dynamically incorporating grounding information from heterogeneous sources. The CoK framework consists of three stages, reasoning preparation, dynamic knowledge adapting, and answer consolidation. This paper provided rich experiments and analysis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1. This paper proposed chain-of-knowledge (CoK), a novel framework to enhance the factual correctness of LLMs with heterogeneous knowledge sources. CoK can dynamically select one or multiple knowledge sources based on the types of questions.\n\nS2. Cok participates in Cot which modifies incorrect answers in each reasoning step by introducing external knowledge, thereby generating the correct answer.\n\nS3. This paper performs extensive experiments on knowledge-intensive tasks spanning a range of domains, including factual, medical, physical, and biological."
                },
                "weaknesses": {
                    "value": "This paper bears a high resemblance to VE with the main distinction being the inclusion of multiple knowledge sources, which might be considered a relatively ordinary contribution. The AQG retrieves relevant knowledge from multiple knowledge sources by converting questions into SPARQL, SQL, and natural language queries. The SPARQL query generator is fine-tuned based on question-SPARQL pairs. I think this training process may make it challenging for AQG to generate SPARQL queries for complex and compositional questions. Because it cannot guarantee that the sub-questions derived from the CoT decomposition are all simple."
                },
                "questions": {
                    "value": "Q1. The authors specifically evaluated the proposed framework in experiments, with a focus on the biology, medical, and physics domains, showing better results compared to the VE method. The knowledge sources used by the author include Wikidata, medical Flashcard, UpToDate, ScienceQA Physics, and ScienceQA Biology. I'm wondering if the authors also incorporated these knowledge sources into the VE method. If not, I believe such a comparison would be unfair.\n\nQ2. LLMs generate multiple results based on self-consistency, if the consistency falls below a threshold, the proposed method is initiated to correct the answer. I'm curious about how often such a process needs to be introduced in practical experiments. Which of the three extraction methods (natural language, SPARQL, and SQL) do the authors consider to be more reliable for LLMs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8874/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8874/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8874/Reviewer_TzUb"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8874/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698457615483,
            "cdate": 1698457615483,
            "tmdate": 1699637117157,
            "mdate": 1699637117157,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bXtITagDX6",
                "forum": "cPgh4gWZlz",
                "replyto": "nIjM7qFODb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8874/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8874/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TzUb"
                    },
                    "comment": {
                        "value": "Dear reviewer TzUb,\n\nThank you for your recognition of our extensive experiments and analysis. We would like to address your queries and concerns here.\n\n>This paper bears a high resemblance to VE with the main distinction being the inclusion of multiple knowledge sources, which might be considered a relatively ordinary contribution.\n\nThe key contributions of our work are the following: \n1. Querying diverse heterogeneous knowledge sources with appropriate queries and aggregating answers from those in an effective manner posits unique scientific challenges and were not addressed in prior work. To the best of our knowledge, we are the first to enhance the factual correctness of LLMs with heterogeneous knowledge sources. \n2. To the best of our knowledge, we are the first to progressively correct the rationales, ensuring that inaccuracies from preceding rationales do not propagate into the subsequent steps. Prior studies such as VE and ReAct leave errors from prior steps in the prompt, causing potential noise. \n3. We propose AQG, which is versatile and can seamlessly transition between fine-tuned models and black-box LLMs, enabling scalability of knowledge sources. \n4. Extensive experiments prove the effectiveness of CoK across ranges of domains, including factual, medical, physical, and biological. \n\n>I think this training process may make it challenging for AQG to generate SPARQL queries for complex and compositional questions. Because it cannot guarantee that the sub-questions derived from the CoT decomposition are all simple.\n\nWe used an AQG trained with data whose logical granularity is on par with the CoT rationales. For example, for WikiData, both training data and CoT rationales only contain one entity and one relation in each sentence. This enables the AQG to formulate more precise queries.\n\n>I'm wondering if the authors also incorporated these knowledge sources into the VE method. If not, I believe such a comparison would be unfair.\n\nYes. In our baseline results for VE, the retrieval system used is Google Search, which includes all the knowledge that CoK can access. \n\n>I'm curious about how often such a process needs to be introduced in practical experiments.\n\nAs mentioned in Appendix I, 86 out of 308 instances for HotpotQA 6-shot, and 127 out of 1000 instances for FEVER 3-shot pass through the knowledge adapting stage.\n\n>Which of the three extraction methods (natural language, SPARQL, and SQL) do the authors consider to be more reliable for LLMs?\n\nCould you please clarify your question? If you are inquiring about which query format is most reliable for LLMs to produce, then the query format would depend on which knowledge source we are retrieving from. For example, for knowledge graphs such as wikidata, SPARQL is used. For natural sentence knowledge sources such as Medical Flashcard, natural language is used. Using different query languages that do not match the format of the knowledge source will likely result in suboptimal performances."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700195111364,
                "cdate": 1700195111364,
                "tmdate": 1700195111364,
                "mdate": 1700195111364,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ki5Ln1BZwB",
                "forum": "cPgh4gWZlz",
                "replyto": "bXtITagDX6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8874/Reviewer_TzUb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8874/Reviewer_TzUb"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the author's rebuttal. My concerns about this paper have been resolved. In short, this paper proposed a novel framework to enhance the factual correctness of LLMs with heterogeneous knowledge sources. I would like to keep my current scores."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535979502,
                "cdate": 1700535979502,
                "tmdate": 1700535979502,
                "mdate": 1700535979502,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "y9A2B0Ilt7",
            "forum": "cPgh4gWZlz",
            "replyto": "cPgh4gWZlz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8874/Reviewer_SrPL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8874/Reviewer_SrPL"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the chain-of-knowledge (CoK) framework designed to augment the performance of LLMs by reducing instances of information hallucination and improving factual correctness. The CoK operates in three stages: reasoning preparation, dynamic knowledge adaptation, and answer consolidation. A distinctive feature of CoK is its ability to dynamically tap into diverse knowledge sources, including structured databases like Wikidata, using an adaptive query generator (AQG) capable of generating varied query languages, such as SPARQL, SQL, and natural language queries. The framework emphasizes progressive correction of the generated rationales, minimizing the risk of error propagation across reasoning steps. Experimental evaluations indicate that CoK enhances the performance of LLMs on knowledge-intensive tasks across different domains, surpassing the chain-of-thought (CoT) baseline by an average of 4.3%."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- CoK addresses the inherent limitations of existing methods by leveraging diverse knowledge sources, both structured and unstructured, improving the accuracy and factual correctness of LLM outputs.\n- The adaptive query generator (AQG) showcases versatility, enabling seamless transition between specialized models and generic LLMs, allowing effective querying across different knowledge source formats.\n- Progressive rationale correction reduces the risk of error propagation, ensuring more reliable generation of answers.\n- Comprehensive experiments covering various knowledge domains, demonstrating a consistent performance boost over the CoT baseline."
                },
                "weaknesses": {
                    "value": "- While the paper emphasizes the use of diverse knowledge sources, it might benefit from a more explicit discussion on the scalability and efficiency of the framework as the volume and variety of sources grow.\n- The framework's dependence on the AQG's capability to generate effective queries for all types of knowledge sources might be a potential bottleneck, especially for highly specialized domains.\n- Although the paper mentions improvement over the CoT baseline, deeper insights into the limitations and areas where CoK might not perform optimally would be beneficial. Except for the limitations of \u201cKnowledge Sources\u201d and \u201cKnowledge Retrieval\u201d discussed in Appendix G."
                },
                "questions": {
                    "value": "- How does the CoK framework handle situations where knowledge sources provide conflicting information? How are you going to solve it?\n- What are the computational overheads introduced by the AQG and the dynamic knowledge adaptation process, especially when querying multiple heterogeneous sources?\n- Have there been considerations or plans to extend the CoK framework to accommodate real-time or streaming knowledge sources?\n- Can you provide insights into the training and fine-tuning process of the AQG, especially its adaptability across different query languages and knowledge sources?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8874/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698586520324,
            "cdate": 1698586520324,
            "tmdate": 1699637117036,
            "mdate": 1699637117036,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "la0OWVLQJ0",
                "forum": "cPgh4gWZlz",
                "replyto": "y9A2B0Ilt7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8874/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8874/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SrPL (1/2)"
                    },
                    "comment": {
                        "value": "Dear reviewer SrPL,\n\nThank you for your recognition of our comprehensive experiments and contributions! We would like to address your queries and concerns here.\n\n>While the paper emphasizes the use of diverse knowledge sources, it might benefit from a more explicit discussion on the scalability and efficiency of the framework as the volume and variety of sources grow.\n>What are the computational overheads introduced by the AQG and the dynamic knowledge adaptation process, especially when querying multiple heterogeneous sources?\n\nAs sources grow, CoK will remain computationally efficient due to the following reasons:\n1. The knowledge domain selection step enhances the efficiency of knowledge retrieval by ensuring only the most pertinent knowledge sources are selected for retrieval.\n2. As highlighted in Section 1, AQG is versatile and can either be a fine-tuned model or an off-the-shelf LLM. The fine-tuning process is a one-time effort. And using off-the-shelf LLMs only requires prompt engineering. This simplicity facilitates the scaling of both the volume and variety of knowledge sources.\n\nThe computational overheads of CoK are on-par with or less than existing retrieval augmentation methods:\nThe inference of CoK is on par with Verify-and-Edit (VE) in terms of time and cost. Firstly, the inference time of CoK is similar to VE, with the only additional time required being for query generation. VE utilizes ChatGPT for this purpose. AQG can use either a fine-tuned LLaMA-2 or ChatGPT. The average inference time per instance for LLaMA-2 is 4 seconds, while for ChatGPT is 2 seconds. During inference, data retrieval from various knowledge sources is executed concurrently. As such, the maximum inference time overhead of CoK compared with VE per instance is only 2 seconds. Secondly, as shown in Table 12, the cost per instance of CoK is on par with VE as well. The extra costs are incurred by the dynamic knowledge editing stage, which is shown to boost performance in the main results. \n\nCoK also costs much less than ReAct, incurring only around 40% of ReAct\u2019s costs. A detailed cost analysis can be found in Appendix I and Table 12.\n\n>The framework's dependence on the AQG's capability to generate effective queries for all types of knowledge sources might be a potential bottleneck, especially for highly specialized domains.\n\nAs mentioned in the CoK framework (Section 3), AQG is a versatile component and can always be replaced by the SOTA model depending on the querying language. Furthermore, AQG can be either a fine-tuned model such as LLaMA-2 or a black-box LLM. For instance, as indicated in Table 11, when generating commonly-used querying languages like SQL, ChatGPT produces better results. However, for specialized domains that require specific querying languages, such as SPARQL, we find that a fine-tuned LLaMA-2 proves to be a more performant option.\n\n>Although the paper mentions improvement over the CoT baseline, deeper insights into the limitations and areas where CoK might not perform optimally would be beneficial. Except for the limitations of \u201cKnowledge Sources\u201d and \u201cKnowledge Retrieval\u201d discussed in Appendix G.\n\nThank you for the suggestion! One potential limitation is that CoK relies on the reasoning capability of LLMs. Thus, failure cases may stem from reasoning failures of LLMs. Please refer to the Appendix G in our revised submission for more information. We also include three examples of these failure cases in Appendix H.2.\n\n>How does the CoK framework handle situations where knowledge sources provide conflicting information? How are you going to solve it?\n\nAs highlighted in Appendix G, we only select highly authoritative knowledge sources which are unlikely to contain inaccurate or conflicting information. Additionally, we manually examined a random selection of 100 samples from HotpotQA and found no instances of conflicting information.\n\n>Have there been considerations or plans to extend the CoK framework to accommodate real-time or streaming knowledge sources?\n\nCoK supports real-time and streaming knowledge sources. For knowledge sources such as Wikidata, Wikipedia, UpToDate, PhysicsClassroom, and CK-12, knowledge is retrieved from their corresponding online websites. These websites are consistently maintained and updated in real-time. Furthermore, CoK supports static data, such as local tables, Flashcard, ScienceQA Physics, and ScienceQA Biology. We believe that this further demonstrates the versatility of CoK."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700194881324,
                "cdate": 1700194881324,
                "tmdate": 1700194881324,
                "mdate": 1700194881324,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R3EU3ux8Pr",
                "forum": "cPgh4gWZlz",
                "replyto": "y9A2B0Ilt7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8874/Reviewer_SrPL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8874/Reviewer_SrPL"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors and Chairs after Reviewing the Rebuttal"
                    },
                    "comment": {
                        "value": "Dear Authors and Chairs,\n\nI have reviewed the rebuttal from authors, and really appreciate the authors' effort and further explanation on this work. \n\nTheir response have addressed my concerns, and I recognize the versatility of robustness and generalizability of CoK. The main reason that I choose to keep my score is that, the additional contribution of this work compared with Verify-and-Edit (VE) [1] is anyhow limited. \n\nIf this work can be further improved, for example, to propose a more general framework of refining reasoning process during training and after generation with adaptive knowledge retrieval, and provide more theoretical analysis, I think it will make this work much more solid and can better meet the ICLR acceptance criteria. \n\n[1] Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework (ACL 2023)\n\nBest Wishes,\n\nReviewer SrPL"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621081130,
                "cdate": 1700621081130,
                "tmdate": 1700621142931,
                "mdate": 1700621142931,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dCrQqzah2R",
            "forum": "cPgh4gWZlz",
            "replyto": "cPgh4gWZlz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8874/Reviewer_bxPp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8874/Reviewer_bxPp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a chain-of-knowledge framework to augmenting LLM in question answering by dynamically retrieving the grounding information from multiple sources. The proposed method first gives the rationale in steps. Then the search query is generated by determining the domain involved in each rationale and the final query. Eventually, the rationale is corrected by the information obtained from the query to give a more plausible answer. The authors conducted experiments on multiple datasets and defeated the compared few-shot methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This article proposes a novel idea to reduce the hallucination of LLM by correcting each rationale in the chain of thought.\n2. The proposed method considers structured and unstructured queries for heterogeneous data sources.\n3. The authors conducted detailed experiments and analysis on multiple datasets from different domains."
                },
                "weaknesses": {
                    "value": "1. The general outline of the suggested approach is somewhat understandable, but the crucial stages lack the necessary level of specificity. In my opinion, a key point in the effectiveness of the proposed method is how to construct accurate queries to retrieve grounding information, but the description in section 3 is still not clear enough. For example, how to build a high-quality question-sparql dataset to ensure the accuracy of this step. While the prompt used with the training loss function is provided in the appendix by the authors, I believe it is important for the main body of the paper to be self-contained, and critical steps like this should not solely rely on the information provided in the appendix.\n2. How does the author determine the table or tables used when generating a SQL query for a rationale, and how can they ensure that the query generated by ChatGPT is valid and answerable if table schema information is not provided. Ditto for SPARQL.\n3. Baseline performance on the hotpotQA dataset seems too poor. Existing work [1] has achieved an accuracy of 0.37 when reviewing ChatGPT.\n\n[1] Zheng, Shen, Jie Huang, and Kevin Chen-Chuan Chang. \"Why does chatgpt fall short in providing truthful answers.\" ArXiv preprint, abs/2304.10513 (2023)."
                },
                "questions": {
                    "value": "1. How does the author ensure that LLM can give faithful answers (query) when performing knowledge adaptation?\n2. The proposed method leaves the decomposition of the question (i.e., the generation of each rationale) to ChatGPT. I am interested in whether this method is only applicable to chain-style question? For more complex questions, where the corresponding SPARQL contains more than one inference path, can the proposed method handle it?\n3. When generating SQL (SPARQL) queries, how do the authors guarantee that the produced queries are valid?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8874/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8874/Reviewer_bxPp",
                        "ICLR.cc/2024/Conference/Submission8874/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8874/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698659969922,
            "cdate": 1698659969922,
            "tmdate": 1700635024720,
            "mdate": 1700635024720,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UrIisXNV5U",
                "forum": "cPgh4gWZlz",
                "replyto": "dCrQqzah2R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8874/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8874/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bxPp (1/2)"
                    },
                    "comment": {
                        "value": "Dear reviewer bxPp,\n\nThank you for your time and suggestions. We would like to address your queries and concerns here.\n\n>In my opinion, a key point in the effectiveness of the proposed method is how to construct accurate queries to retrieve grounding information, but the description in section 3 is still not clear enough.\n\nThank you for the suggestion! We have added more details about the adaptive query generator in Section 3. This includes task-specific details such as providing the table schema for SQL, and how the domain of the training data corresponds to the respective knowledge source.\n\n>How does the author determine the table or tables used when generating a SQL query for a rationale, and how can they ensure that the query generated by ChatGPT is valid and answerable if table schema information is not provided. Ditto for SPARQL.\n\nFollowing the existing formulation for the FeTaQA task, each question is provided with a specific table and the table schema. To ensure a successful query generation, the AQG should be aware of the structure and schema of the knowledge source. Therefore we always feed in the table schema. \n\nFor SPARQL, no table schema is required. Instead, we use an AQG trained with data whose logical granularity is on par with the CoT rationales, i.e, both training data and CoT rationales only contain a single entity and relation in each sentence. This enables the AQG to formulate more precise queries.\n\n>Baseline performance on the hotpotQA dataset seems too poor. Existing work [1] has achieved an accuracy of 0.37 when reviewing ChatGPT.\n\nFollowing hotpotQA [2] and other previous work [3,4], we adopt the exact match metric for evaluating hotpotQA. However in [1], the evaluation for hotpotQA is partial match, a less stringent standard that typically yields higher scores. Below, we include results using partial match for hotpotQA for your consideration. In this case, CoK still outperforms over baseline methods.\n\n|       Method      | HotpotQA (E.M.) | HotpotQA (P.M.) |\n|:-----------------:|:---------------:|:---------------:|\n| Standard (3-shot) |      22.7%      |      32.5%      |\n|    CoT (3-shot)   |      29.9%      |      39.0%      |\n|  CoT-SC (3-shot)  |      30.8%      |      40.6%      |\n|    VE (3-shot)    |      31.8%      |      43.8%      |\n|  **CoK (3-shot)** |    **34.1%**    |    **47.1%**    |\n| Standard (6-shot) |      24.0%      |      33.4%      |\n|    CoT (6-shot)   |      34.4%      |      39.9%      |\n|  CoT-SC (6-shot)  |      33.4%      |      41.2%      |\n|    VE (6-shot)    |      34.4%      |      48.4%      |\n|  **CoK (6-shot)** |    **35.4%**    |    **49.7%**    |\n\n\n\n[1] Zheng, Shen, Jie Huang, and Kevin Chen-Chuan Chang. \"Why does chatgpt fall short in providing truthful answers.\" ArXiv preprint, abs/2304.10513 (2023).\\\n[2] Yang, Zhilin, et al. \"HotpotQA: A dataset for diverse, explainable multi-hop question answering.\" In Proceedings of EMNLP, 2018.\\\n[3] Zhao, Ruochen, et al. \"Verify-and-edit: A knowledge-enhanced chain-of-thought framework.\" In Proceedings of ACL, 2023.\\\n[4] Yao, Shunyu, et al. \"React: Synergizing reasoning and acting in language models.\" In Proceedings of ICLR, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700194205037,
                "cdate": 1700194205037,
                "tmdate": 1700194616972,
                "mdate": 1700194616972,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vnVhvUDDlc",
                "forum": "cPgh4gWZlz",
                "replyto": "dCrQqzah2R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8874/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8874/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bxPp (2/2)"
                    },
                    "comment": {
                        "value": ">How does the author ensure that LLM can give faithful answers (query) when performing knowledge adaptation?\\\n>When generating SQL (SPARQL) queries, how do the authors guarantee that the produced queries are valid?\n\nWe acknowledge that there is a possibility of AQG generating erroneous queries. However, we believe this is a common problem for all generative models. To mitigate this, we employ diverse techniques for query generation and  knowledge retrieval:\n1. For query generation, AQG utilizes either black-box LLM or fine-tuned Llama-2. When using black-box LLM such as ChatGPT, we employ few-shot prompting and incorporate all necessary information in the prompt. For example, when generating SQL queries, the table schema along with data snippets are included in the prompt. When utilizing fine-tuned Llama-2, we ensure the logical granularity of AQG training data is on par with the decomposed CoT rationales. For example for WikiData, both training data and CoT rationales only contain one entity and one relation in one sentence. This enables the AQG to formulate more precise queries.\n2. Our knowledge retrieval relies on highly authoritative knowledge sources. Therefore, even if the generated queries contain inaccuracies, the retrieved knowledge is faithful. As outlined in Appendix A.1.3, during knowledge editing, we also include a demonstration in the training prompt where the original rationale is unchanged. Thus, if no usable knowledge is retrieved, the baseline scenario would be where the original CoT rationale remains unchanged. \n\nAs highlighted in Section 5.3, we provide both quantitative and human evaluations to demonstrate the effectiveness of our techniques. Results demonstrate that CoK enhances the factual accuracy of rationales, leading to more precise final answers.\n\n\n>I am interested in whether this method is only applicable to chain-style question? For more complex questions, where the corresponding SPARQL contains more than one inference path, can the proposed method handle it?\n\nThe chain-of-knowledge framework utilizes chain-of-thought decomposition, which is not only beneficial for multi-hop QA (i.e., chain-style questions), but also questions requiring parallel reasoning (i.e., questions containing more than one inference path). Experiments on FEVER dataset support its effectiveness as FEVER is not multi-hop (chain-style), but contains claims that require composition of independent evidence from multiple sentences [5]. \\\nFurthermore, we ensure that each rationale after CoT decomposition only contains a single entity and relation via prompt engineering as shown in Appendix A.1.1. As shown in Appendix D.1.2, the natural sentence input of the training data for AQG also contains a single entity and relation. This alignment in the complexity level of rationales and training data allows AQG to generate more accurate queries.\n\n[5] Thorne, James, et al. \"FEVER: a large-scale dataset for fact extraction and VERification.\" In Proceedings of NAACL, 2018."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700194290352,
                "cdate": 1700194290352,
                "tmdate": 1700194632330,
                "mdate": 1700194632330,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o4PypgtAd2",
                "forum": "cPgh4gWZlz",
                "replyto": "dCrQqzah2R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8874/Reviewer_bxPp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8874/Reviewer_bxPp"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the author for the reply. Some of my concerns are addressed. I expect that the authors will be able to explain the issues mentioned in more detail in the main body of the paper, including the validation of the query, and the choice of evaluation metric. I have updated the rating."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635109855,
                "cdate": 1700635109855,
                "tmdate": 1700635109855,
                "mdate": 1700635109855,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bfmlrjvVlf",
            "forum": "cPgh4gWZlz",
            "replyto": "cPgh4gWZlz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8874/Reviewer_84zL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8874/Reviewer_84zL"
            ],
            "content": {
                "summary": {
                    "value": "This is a very interesting paper on Chain of Knowledge and integrating the question answering from LLM with traditional source of knowledge such as KG to improve the overall accuracy of question answering in domain-specific text.\nMain contributions:\n1. Chain of Knowledge for factual correctness\n2. Adaptive query generator for identifying knowledge source and converting queries in respective form\n3. Progressive correction of rationale using CoK\n4. Following domains have been chose for the work factual, medical, physical, and biological."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is very well written.\nThe authors have considered good number of scenarios for knowledge, and carefully selected wikidata as the knowledge source, provided the limitation on retrieval of knowledge in general.\nI enjoyed reading this paper."
                },
                "weaknesses": {
                    "value": "Please see the questions section for more."
                },
                "questions": {
                    "value": "* For the given SPARQL query SELECT ?answer WHERE { wd:/Souleymane Sane/ wdt:/child/ ?answer . \u00b4 }, do you use some library to convert the entities/relation in their specific identifier form (entity linking), how do you know which is the correct identifier, in specific cases where similar natural language entities have two identifiers.\n* Did you try domain specific questions with ChatGPT? Upon trying the example given in Figure 1 on ChatGPT, I could see the result as George Gamow. Maybe you should come up with an example where ChatGPT would hallucinate. Or if the idea here is to show open source LLM, then this example makes complete sense. Although it should be mentioned in this case which model is used (Llama2-7b-chat? or instruction tuned by yourself)\n* It is possible that the knowledge injected through Wikidata doesn't answer the question asked, what would be done to answer the question in that case?\n* Can you please explain how do you find specific relation for querying with an entity? There is a possibility of multiple entity attached with one relation. It is also possible that multiple relations connected with multiple entities, and it may be difficult to identify the relation depending on the question. What would be done in that case?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8874/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828162076,
            "cdate": 1698828162076,
            "tmdate": 1699637116814,
            "mdate": 1699637116814,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OrMoHr192W",
                "forum": "cPgh4gWZlz",
                "replyto": "bfmlrjvVlf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8874/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8874/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer 84zL,\n\nThank you for your recognition of our contributions! We would like to address your queries and concerns here.\n\n>do you use some library to convert the entities/relation in their specific identifier form (entity linking), how do you know which is the correct identifier, in specific cases where similar natural language entities have two identifiers.\n\nWe employ the commonly used GENRE model [1] for entity linking, which effectively addresses the cases of ambiguous entities by generating unique entity identifiers. We have also included the details in Appendix C.1 in our revised submission.\n\n>Maybe you should come up with an example where ChatGPT would hallucinate.\n\nThank you for your suggestion! In general, we observe that ChatGPT is more likely to hallucinate for domain-specific questions. Our chosen example highlights that even for questions needing common factual knowledge, ChatGPT can generate erroneous responses. We utilized gpt-3.5-turbo-0613  for our experiments, the latest version available at the time of experiments. \n\nIn the revised submission, we have updated the example where the model failed to provide a correct answer. The CoT result in the paper can be reproduced using gpt-3.5-turbo-0613 with a temperature of 0, following the below prompt. Furthermore, even the latest gpt-3.5-turbo model generates erroneous responses.\n\n\"\"\"\\\nStrictly follow the format of the below examples, provide two rationales before answering the question.\\\nQ: This British racing driver came in third at the 2014 Bahrain GP2 Series round and was born in what year?\\\nA: First, at the 2014 Bahrain GP2 Series round, DAMS driver Jolyon Palmer came in third. Second, Jolyon Palmer (born 20 January 1991) is a British racing driver. The answer is 1991.\n\nQ: What band did Antony King work with that formed in 1985 in Manchester?\\\nA: First, Antony King worked as house engineer for Simply Red. Second, Simply Red formed in 1985 in Manchester. The answer is Simply Red.\n\nQ: How many inhabitants were in the city close to where Alberta Ferretti\u2019s studios was located?\\\nA: First, Alberta Ferretti\u2019s studio is near Rimini. Second, Rimini is a city of 146,606 inhabitants. The answer is 146,606.\n\nQ: What year was the Argentine actor who directed El Tio Disparate born?\\\nA: First,\\\n\"\"\"\n\n\n>It is possible that the knowledge injected through Wikidata doesn't answer the question asked, what would be done to answer the question in that case?\n\nAs illustrated in Appendix A.1.3, we include a demonstration in the training prompt where the original rationale is unchanged. Thus, if no usable knowledge is retrieved, the baseline scenario would be where the original CoT rationale remains unchanged (a fall-back strategy).\n\nWe acknowledge that retrieval of irrelevant information (i.e., doesn\u2019t answer the question asked) is a possibility. To mitigate this, we employed two strategies in this paper throughout the framework, leading to SOTA results compared to existing methods, such as ReAct and VE, as shown in Table 2. \n1. We used an AQG trained with data whose logical granularity (number of entities and relations involved)  is on par with the CoT rationales. For example for WikiData, both training data and CoT rationales only contain one entity and one relation in one sentence. This enables the AQG to formulate more precise queries. \n2. We incorporated multiple knowledge sources for each domain to get more validating evidence. The results in Table 4 confirm that using various sources enhances knowledge coverage. \n\n\n\n>Can you please explain how do you find specific relation for querying with an entity? There is a possibility of multiple entity attached with one relation. It is also possible that multiple relations connected with multiple entities, and it may be difficult to identify the relation depending on the question. What would be done in that case?\n\nAs illustrated in Appendix A.1.1, every rationale after the reasoning preparation stage should comprise a single entity and relation. Similarly, Appendix D.1.2 indicates that each training data sample for SPARQL contains only one entity and one relation. This consistency ensures that the training data aligns with the rationale. Consequently, AQG can effectively generate more precise and disambiguous queries.\n\n\n[1] Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. Autoregressive entity retrieval. In Proceedings of ICLR, 2021."
                    },
                    "title": {
                        "value": "Response to Reviewer 84zL"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8874/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700193410965,
                "cdate": 1700193410965,
                "tmdate": 1700193711023,
                "mdate": 1700193711023,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]