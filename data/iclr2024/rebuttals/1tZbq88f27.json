[
    {
        "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"
    },
    {
        "review": {
            "id": "q2mOyEQoWo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6224/Reviewer_auSX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6224/Reviewer_auSX"
            ],
            "forum": "1tZbq88f27",
            "replyto": "1tZbq88f27",
            "content": {
                "summary": {
                    "value": "This paper presents a recipe for reproducing the GPT-4 vision model. The approach involves aligning the pre-trained vision encoder of BLIP-2 (ViT + Qformer) with the LLM Vicuna using a noisy image-caption dataset. The model is then fine-tuned with a curated detailed image description dataset, which is generated by refining the initial model predictions using ChatGPT and manual refinement. Experimental results demonstrate the capabilities of the proposed MiniGPT-4 model in challenging vision-language tasks like Poem Generation and Meme understanding. While qualitative results are mostly presented, quantitative findings indicate that MiniGPT-4 outperforms BLIP-2 on image captioning when evaluated with ChatGPT, although it tends to generate more hallucinations than BLIP-2."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The exploration to achieve a GPT-4 level VLMs by combining LLMs & vision encoders is greatly appreciated. Additionally, the provision of open-sourced model checkpoints and dataset enhances the reproducibility and transparency of the research.\n- The qualitative results presented in the study demonstrate the potential effectiveness of the proposed recipe. The process of aligning the model through training on a noisy dataset and subsequently fine-tuning it with a high-quality dataset shows promise in improving the model's performance and response quality."
                },
                "weaknesses": {
                    "value": "- The quantitative evaluation setup lacks convincing elements:\n  - The paper only compares the proposed model with BLIP-2 as the baseline, which utilizes a FLAN-T5 model as the base LLM. To enhance the credibility of the results, it would be beneficial to compare the proposed model with more recent open-sourced components that utilize similar base LLMs, such as LLaVA[1], InstructBLIP[2], and mPlug-Owl[3].\n  - The selection of the four Advanced Abilities tasks appears arbitrary, and the evaluation sets for these tasks are relatively small (25 samples for each task). To provide a more comprehensive evaluation, it is recommended to include evaluation on recent multimedia benchmarks such as TouchStone[4] and MMBench[5].\n  - In Table 2, the coco caption evaluation with ChatGPT seems questionable. The approach of randomly selecting a ground-truth caption and asking ChatGPT if the generated text covers the main visual concepts in the label text may lead to instances where the selected \"ground-truth\" caption and the generated text depict the image in different aspects, yet both are considered correct. Reporting standard metrics such as BLEU and Rouge scores, which consider multiple candidate ground-truth captions, could mitigate the issues.\n  - Table 5 lacks the CHAIR_S score for different methods. Additionally, it indicates that MiniGPT-4 fails to follow the instruction's requirement of 20 words, as evidenced by the average length of 27 words for MiniGPT-4 (short). To me, instead of showcasing the promising results on so-called adavanced tasks, satisfying the user requirement of a basical instruction should be of higher priority.\n- Some claims made in the paper are misleading:\n  - In Section 3.2, the paper claims that no equivalent datasets exist (for multi-modal instruction tuning) in the vision-language domain. However, there have been recent studies addressing this problem, such as LLaVA[1], InstructBLIP[2], M3IT[6], and MultiInstruct[7]. To accurately reflect the current state of the field, the author should properly cite these studies and consider exploring further fine-tuning with these datasets.\n  - In Section 4.4, the paper claims that MiniGPT-4 achieves similar results with Qformer, implying that Qformer does not play a critical role in advanced skills. However, this claim may not be fully supported by the AOK-VQA and GQA results, along with additional case studies. Additionally, if the advanced skills require inference between multiple images, the downsampling effect of Qformer (i.e., downsampling visual patches from 256 to 32) could be crucial and should be considered.\n\n## References\n\nIt is not a curated list, and most of them appeared  more than 3 months before the submission deadline.\n\n[1] Visual Instruction Tuning, https://arxiv.org/abs/2304.08485\n\n[2] InstructBLIP: Towards general-purpose vision-language models with instruction tuning, https://arxiv.org/abs/2305.06500 \n\n[3] mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality, https://arxiv.org/abs/2304.14178\n\n[4] TorchStone: https://github.com/QwenLM/Qwen-VL/blob/master/touchstone/README.md\n\n[5] MMBench: https://opencompass.org.cn/leaderboard-multimodal\n\n[6] M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning, https://arxiv.org/abs/2306.04387\n\n[7] MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning, https://arxiv.org/abs/2212.10773"
                },
                "questions": {
                    "value": "Q1: What criteria did you use to select the Advanced Abilities tasks? Additionally, can you clarify if the images used for evaluation in these tasks overlap with the dataset used in stage 1 pre-training and stage 2 fine-tuning?\n\nQ2: Could you specify the version of ChatGPT that you adopted for answer revision in your experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6224/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6224/Reviewer_auSX",
                        "ICLR.cc/2024/Conference/Submission6224/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6224/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697388127741,
            "cdate": 1697388127741,
            "tmdate": 1700625799927,
            "mdate": 1700625799927,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U4Fg3Qmq4P",
                "forum": "1tZbq88f27",
                "replyto": "q2mOyEQoWo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6224/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6224/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback! We response to the questions as follows:\n\n**Q1 It would be beneficial to compare the proposed model with more recent open-sourced components that utilize similar base LLMs, such as LLaVA, InstructBLIP, and mPlug-Owl. It is recommended to include evaluation on recent multimedia benchmarks such as MMBench.**\n\nAs mentioned by Reviewer 7oDs, MiniGPT4 has been evaluated in the MMBench benchmark paper, and also in other benchmarks like MME and VisIT-Bench, together with similar baseline methods like LLaVa and InstructBLIP.  Please refer to Q0 in the general response for a detailed discussion.\n\n**Q2 What criteria did you use to select the Advanced Abilities tasks? Additionally, can you clarify if the images used for evaluation in these tasks overlap with the dataset used in stage 1 pre-training and stage 2 fine-tuning?**\n\nFor our study, we chose advanced abilities that necessitate both an understanding of visual content and complex language generation. We gathered the images for our dataset using Google Image Search, employing related keywords such as \"funny memes\" and \"photos of delicious food.\" \n\nFurthermore, it's important to note that the training set consists solely of image description texts and does not include any text data pertaining to advanced abilities. MiniGPT4 was not trained with image-text paired data to learn these skills. However, it gains this skill compositionally by efficiently mapping images to the embedding space that the LLM (frozen by default) can operate on. \n\n**Q3 In Table 2, the approach of randomly selecting a ground-truth caption and asking ChatGPT if the generated text covers the main visual concepts in the label text may lead to instances where the selected \"ground-truth\" caption and the generated text depict the image in different aspects, yet both are considered correct. Reporting standard metrics such as BLEU and Rouge scores, which consider multiple candidate ground-truth captions, could mitigate the issues. Could you specify the version of ChatGPT that you adopted for answer revision in your experiments?**\n\nIn Table 2, we focus not on the correctness of the generated captions but on their information coverage. A high score indicates that the generated descriptions encompass more information from the image, rather than \u201cmore correct\u201d. For this evaluation, we employed the API gpt-3.5-turbo.\n\nTraditional metrics such as BLEU and Rouge, which rely on sentence similarity, fall short when the format of evaluated image descriptions diverges from the ground truth. This discrepancy is particularly noticeable when the generated descriptions are significantly longer than the original ground truth captions. We designed a new metric based on GPT-4 that takes all the ground truth captions into account. Please refer to Q2 in the general response section for details.\n\n**Q4 Table 5 lacks the CHAIR score for different methods.**\n\nKindly refer to Q5 in the general response where we include results from more methods.\n\n**Q5 MiniGPT-4 fails to follow the instruction's requirement of 20 words, as evidenced by the average length of 27 words for MiniGPT-4 (short). To me, satisfying the user requirement of a basic instruction should be of higher priority.**\n\nThe ability to generate a sentence with exactly 20 words is constrained by the LLM's capabilities. While SOTA closed-sourced models like GPT-4 perform well in this task, open-source models like Llama2 13B struggle. We conducted a small experiment with Llama2 13B, instructing it to \"generate a sentence with 20 words\". The average number of words over the 50 generations is only 14.8 words. This seemingly simple instruction proves challenging for a 13B open-source LLM. Since we freeze the open-source LLM on MiniGPT-4, it's unsurprising that MiniGPT-4 can't precisely generate a 20-word caption. Nonetheless, this prompt still encourages MiniGPT-4 to significantly reduce the image description length from 175 to 28.8 words\n\n**Q6 In Section 3.2, the paper claims that no equivalent datasets exist (for multi-modal instruction tuning) in the vision-language domain. However, there have been recent studies addressing this problem, such as LLaVA, InstructBLIP, M3IT, and MultiInstruct. To accurately reflect the current state of the field, the author should properly cite these studies and consider exploring further fine-tuning with these datasets.**\n\nAt the time when this project was developed, there was no other multi-modal instruction tuning dataset that existed. We updated section 3.2 (highlighted in blue) in the updated paper to better reflect this background. In addition, we updated the related work section to better represent the current landscape. \nFurthermore, we carried out a new experiment, substituting our second-stage dataset with a newly-released image description dataset from LAION. This dataset features descriptions generated by GPT-4v. For detailed information, please refer to Q3 in the general response section."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603052417,
                "cdate": 1700603052417,
                "tmdate": 1700607338128,
                "mdate": 1700607338128,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uYz50cSGjo",
                "forum": "1tZbq88f27",
                "replyto": "cXi3PBVEYi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6224/Reviewer_auSX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6224/Reviewer_auSX"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response, and most of my concerns are addressed (e.g., more baseline comparison on more benchmarks and the state of current progress). Accordingly, I will raise my score to 6.\n\nFurthermore, I would like to discuss more about the instruction following ability. I know the difficulty of asking the model to generate a response with the **exact word number**, such as 20 words in the short instruction. Nevertheless, LLaMa-13B could understand the key idea is a short response within 20 words (e.g., 14.8 words), why does the frozen LLM in MiniGPT-4 still generate longer responses (e.g., 28.8 words on average)? Is there a tendency to produce a longer response after fine-tuning and what are the potential side effects?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625778823,
                "cdate": 1700625778823,
                "tmdate": 1700625778823,
                "mdate": 1700625778823,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gecKKWle79",
                "forum": "1tZbq88f27",
                "replyto": "q2mOyEQoWo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6224/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6224/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comment! To verify more directly, we conducted a simple experiment with Vicuna-13B, the LLM used by MiniGPT-4. We employed the prompt \"Introduce me an animal in 20 words.\" Across 20 samples, the average length of the responses generated was 29.6 words, similar to the average length (28.8 words) in MiniGPT-4's generation from our ablation study. These results suggest that the over generating issue is due to the limitations of MiniGPT-4's LLM."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685991189,
                "cdate": 1700685991189,
                "tmdate": 1700686198635,
                "mdate": 1700686198635,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ygbOM4ceck",
            "forum": "1tZbq88f27",
            "replyto": "1tZbq88f27",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6224/Reviewer_A8a5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6224/Reviewer_A8a5"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces MiniGPT-4, which combines a frozen visual encoder with a frozen large language model, Vicuna, using a single projection layer. The research revealed that aligning visual features with a sophisticated large language model can replicate many of GPT-4's advanced multi-modal functions. A two-step training process is proposed, and the second step plays a vital role in generating long and meaningful responses. Additionally, MiniGPT-4 showcased unique capabilities like writing stories or poems inspired by images and teaching users cooking techniques based on food photos."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Easy Approach: The paper presents an easy two-step approach to combining a visual encoder with a large language model, providing insights into GPT-4's enhanced multi-modal capabilities.\n\nDiverse Capabilities: MiniGPT-4 replicates many of GPT-4's features, such as writing stories based on images and culinary guidance from food photos.\n\nInteresting Experiments: The authors provide qualitative and quantitative experiments to show the various abilities of the proposed method, including the human study of advanced abilities."
                },
                "weaknesses": {
                    "value": "* The necessity of using a large number of image and text pairs to train a projection layer is not addressed sufficiently. \n\n* The comparisons between BLIP-2 and MiniGPT are unfair because BLIP-2 uses a weaker LLM and is not designed for the advanced abilities mentioned in the paper. Furthermore, the designed new evaluation method using ChatGPT to check whether the generation covers all the objects and visual relations is not compelling. It is possible because many hallucinations are generated, as Table 5 illustrates."
                },
                "questions": {
                    "value": "* Is it possible to use less data from the combined dataset to train the projection layer in the first pertaining stage? It is better to provide insights on the size data necessary to align the vision and text space.\n\n* Surprisingly, the performance drops a lot when three projection layers are used instead of one. Could you try different designs of the projection module, e.g., two or more than three projection layers, or adding activation layers between projection layers to provide insights on why one projection layer is better and why the drop is so huge?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6224/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698185880461,
            "cdate": 1698185880461,
            "tmdate": 1699636679655,
            "mdate": 1699636679655,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NzmZAQ4erE",
                "forum": "1tZbq88f27",
                "replyto": "ygbOM4ceck",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6224/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6224/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback! We response to the questions as follows:\n\n\n**Q1 The comparisons between BLIP-2 and MiniGPT are unfair because BLIP-2 uses a weaker LLM and is not designed for the advanced abilities mentioned in the paper.**\n\nThe essential message our paper conveys to the community is as follows: advanced multimodal capabilities can be attained by utilizing a stronger LLM with advanced pure language skills, even if the LLM is not specifically designed for advanced **multimodal** capabilities. The particularity of our design choices and the details of making it work in a two-stage manner are at the heart of our work/contributions.  In addition, we provide a discussion of MiniGPT-4 in benchmarks like MMBench and MME in our general response. Kindly refer to Q0 of the general response for more details.\n\n**Q2 The designed new evaluation method using ChatGPT to check whether the generation covers all the objects and visual relations is not compelling. It is possible because many hallucinations are generated, as Table 5 illustrates.**\n\n\nFrom Q5 in the general response, we can see that the hallucination issue of MiniGPT-4 is less obvious compared to other baseline models like LLava or mPLUG-Owl.\nIn addition, we introduce a more robust metric that better considers all the ground truth caption into account with a stronger judger GPT-4-turbo. Please refer to Q2 in the general response.\n\n\n**Q3 The necessity of using a large number of image and text pairs to train a projection layer is not addressed sufficiently.**\n\nKindly refer to Q4 in the general response section, where we respond experimentally to this question. \n\n\n**Q4. The performance drops a lot when three projection layers are used instead of one. Why the drop is so huge?**\n\nThe three-layer variant almost triples the learning capacity (the number of learnable parameters). This increase may necessitate more training data compared to the original one-layer version. Consequently, training this variant on the same amount of data as the original version might not suffice to achieve comparable performance."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602710445,
                "cdate": 1700602710445,
                "tmdate": 1700602710445,
                "mdate": 1700602710445,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OQIuMgM39R",
                "forum": "1tZbq88f27",
                "replyto": "NzmZAQ4erE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6224/Reviewer_A8a5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6224/Reviewer_A8a5"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response, which has partially addressed my concerns. Hence, I have decided to maintain my original scores."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632158530,
                "cdate": 1700632158530,
                "tmdate": 1700632158530,
                "mdate": 1700632158530,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PNfOvIZFmJ",
                "forum": "1tZbq88f27",
                "replyto": "ygbOM4ceck",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6224/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6224/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your comment. Regarding Q4, our design with a single linear projection layer aligns with previous studies like [1][2]. These works already demonstrated that training only a single linear projection is sufficient for transferring image representations to a frozen LM space. Similarly, another concurrent work, LLaVa, uses a single projection layer to align visual and language features. The significant performance decline observed in AOK-VQA may be attributed to increased training capacities, suggesting the need for more training data than our original single-layer version required. As the reviewer suggests, we will expand our discussion in the final version to include ablations on alternative projection designs and the use of additional alignment data.\n\n[1 ]Merullo, Jack, et al. Linearly Mapping from Image to Text Space. ICLR 2023.\n\n[2] Scialom, Thomas, et al. What BERT sees: Cross-modal transfer for visual question generation. INLG 2020."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685565810,
                "cdate": 1700685565810,
                "tmdate": 1700686086282,
                "mdate": 1700686086282,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BnhmXKLIro",
            "forum": "1tZbq88f27",
            "replyto": "1tZbq88f27",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6224/Reviewer_TFFX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6224/Reviewer_TFFX"
            ],
            "content": {
                "summary": {
                    "value": "This paper attempts to create Multimodal LLMs that can understand the visual inputs. To do this, MiniGPT-4 uses a frozen visual encoder along with a frozen vicuna model and only trains the single projection layer. Various emerging properties of the M-LLMs are discussed and a novel detailed image description-based fine-tuning stage is proposed to further improve the image understanding.\n\nOverall this paper has contributed significantly. However, it\u2019s not yet ready for publication. Other rounds of revisions are needed with extensive benchmarking."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* One of the first works attempts to introduce visual modalities in pre-trained LLMs in a parameter-efficient manner.\n* MiniGPT-4 requires 40 A100 GPU hours for training and is able to outperform the BLIP-2. \n* Various qualitative examples are shown along with ablation studies to measure the effectiveness of the two-stage training procedure. \n* Limitations of the MiniGPT-4 (in terms of hallucinations and spatial relations) are discussed."
                },
                "weaknesses": {
                    "value": "* Benchmark is very limited. Rigorous benchmarking on different downstream tasks is needed. For reference, MMBench, TextVQA, etc.\n* Only one baseline is reported. However, for a holistic understanding of the approach, more baselines are needed. \n\nSome of the missing References:\n* Liu, Haotian, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. \"Visual instruction tuning.\" arXiv preprint arXiv:2304.08485 (2023).\n* Mou, Chong, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. \"T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.\" arXiv preprint arXiv:2302.08453 (2023)."
                },
                "questions": {
                    "value": "* Instead of having the data post-processing steps for stage 2 training, why not use GPT-4 itself to get the image captions and perform knowledge distillation as stage 2? Getting ~3000 such image-caption pairs should not be that costly.\n* Does MiniGPT-4 apply a unique projection layer to all output vectors corresponding to the learned queries? Or some other processing steps are involved?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6224/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6224/Reviewer_TFFX",
                        "ICLR.cc/2024/Conference/Submission6224/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6224/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731289659,
            "cdate": 1698731289659,
            "tmdate": 1700626977400,
            "mdate": 1700626977400,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NhgLvlsRQ3",
                "forum": "1tZbq88f27",
                "replyto": "BnhmXKLIro",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6224/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6224/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback! We response to the questions as follows:\n\n**Q1 Rigorous benchmarking on different downstream tasks is needed. For reference, MMBench. More baselines are needed.**\n\n\nAs mentioned by Reviewer 7oDs, this project has been evaluated in the MMBench benchmark paper, and also in other benchmarks like MME and VisIT-Bench, together with similar baseline methods like LLaVa and InstructBLIP.  Kindly refer to Q0 in the general response for a detailed discussion.\n\n\n**Q2 Why not use GPT-4 itself to get the image captions and perform knowledge distillation as stage 2?**\n\nKindly refer to Q3 in the general response for the new experiment.\n\n\n**Q3 Does MiniGPT-4 apply a unique projection layer to all output vectors corresponding to the learned queries? Or some other processing steps are involved?**\n\nThe projection layer is applied to Q-former\u2019s output vectors (tokens), which correspond to the Q-former\u2019s internal learnable queries. This projection layer converts each of the output tokens individually to match the input size required by the LLM model."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602589779,
                "cdate": 1700602589779,
                "tmdate": 1700602589779,
                "mdate": 1700602589779,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vpEn9E2vVL",
                "forum": "1tZbq88f27",
                "replyto": "NhgLvlsRQ3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6224/Reviewer_TFFX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6224/Reviewer_TFFX"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response"
                    },
                    "comment": {
                        "value": "Thank you for the response and for performing new experiments (especially, gpt-4v). I recognize that gpt4-v might not have been available to everyone at this work time and I appreciate the quick experimental comparison.  \n\nThat said, the presentation still needs some improvements. I would appreciate it if the authors could include the above-mentioned benchmarking results in the main paper as this is a must-have at this stage of the submission. \n\nMost of my concerns are resolved. I'm increasing the score to 5 (would be happy to increase it to 6 if the presentation is improved)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627428282,
                "cdate": 1700627428282,
                "tmdate": 1700627428282,
                "mdate": 1700627428282,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ygupOwUBFq",
            "forum": "1tZbq88f27",
            "replyto": "1tZbq88f27",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6224/Reviewer_7oDs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6224/Reviewer_7oDs"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces MiniGPT-4, which combines a frozen visual encoder with a frozen advanced LLM, Vicuna, through a single projection layer. Remarkably, MiniGPT-4 exhibits advanced multi-modal abilities similar to GPT-4, extending to generating detailed image descriptions, creating websites from hand-drawn drafts, writing stories and poems inspired by images, and providing cooking instructions based on food photos. However, initial training on short image captions led to issues like repetition and fragmentation in the generated text. To remedy this, the authors curated a detailed image description dataset for a second stage of fine-tuning, significantly enhancing the model\u2019s reliability and overall performance. They have made their code available for further research and validation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This submission is one of the earliest attempts at open-source reproduction of large vision-and-language models. It provides a parameter-efficient solution to this difficult problem, and makes use of the existing open-source models as much as possible, which inspires many follow-up works."
                },
                "weaknesses": {
                    "value": "There are two main weaknesses with regard to the model structure and the evaluation setup. My evaluation of this submission will be greatly improved if the authors properly address these two main issues. \n\n\n**(1) Model Structure:**\nThe introduction of MiniGPT-4 in the submission presents a novel approach in the domain of vision-and-language models. However, a noticeable limitation lies in its capacity to process only a single visual input per run. This design contrasts with other large-scale models such as Flamingo and GPT-4, which have demonstrated capabilities in handling multiple visual inputs simultaneously, as acknowledged in the related work section of the submission.\n\nAdditionally, the literature review appears to omit significant works [1,2,3,4] in the realm of large vision-and-language models that facilitate in-context learning with multiple visual inputs. The inclusion of these works could provide a more comprehensive backdrop for the MiniGPT-4, situating it within the broader context of ongoing research in the field.\n\nThe MiniGPT-4\u2019s visual encoder is founded on the Q-Former architecture, converting each visual input into a fixed-length learned visual query. With reference to the BLIP-2 paper, this length is specified as 32 dimensions. Considering that this is considerably shorter than the maximum input length supported by the LLM backbone, there seems to be an opportunity to extend the model\u2019s capacity to accommodate multiple visual inputs. An exploration into why the MiniGPT-4 is constrained to single image inputs, alongside potential avenues for extending its flexibility and application range, would strengthen the submission.\n\n**(2) Evaluation Setup:**\nThe submission primarily focuses on showcasing qualitative results derived from the MiniGPT-4, complemented by various ablation studies. However, there is an apparent gap in benchmark comparisons with other established large vision-and-language models. A number of benchmarks [5,6,7], encompassing diverse aspects and task setups related to vision-and-language evaluation, have included results from MiniGPT-4 alongside comparisons with other models. The incorporation of such direct comparisons would offer a more definitive assessment of MiniGPT-4\u2019s performance, enhancing the overall credibility of the evaluation presented.\n\n\n\n**Missing references**:\n\n[1] OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models\n\n[2] Otter: A Multi-Modal Model with In-Context Instruction Tuning\n\n[3] mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality\n\n[4] MIMIC-IT: Multi-Modal In-Context Instruction Tuning\n\n[5] MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models\n\n[6] MMBench: Is Your Multi-modal Model an All-around Player?\n\n[7] VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use"
                },
                "questions": {
                    "value": "(1) What is the maximum input and output length for the training and inference stage? \n\n(2) In Section 3.2, the authors mention they would examine if the generated sentence exceeds 80 tokens or not \u2013 how is the length of 80 determined? Is it through empirical observations?\n\n(3) What\u2019s the length of the learned visual queries from the Q-Former in Mini-GPT4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6224/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698732170343,
            "cdate": 1698732170343,
            "tmdate": 1699636679402,
            "mdate": 1699636679402,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6tjPSa6nyo",
                "forum": "1tZbq88f27",
                "replyto": "ygupOwUBFq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6224/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6224/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback! We response to the questions as follows:\n\n**Q1 Extend the model\u2019s capacity to accommodate multiple visual inputs.**\n\nKindly refer to Q1 in the general response section.\n\n\n**Q2 Evaluation Setup**\n\n\nKindly refer to Q0 in the general response section.\n\n\n\n**Q3 What is the maximum input and output length for the training and inference stage?**\n\nThe context length is limited by the LLM itself. For the vicuna version we use which is based on Llama 1, the context length is 2048 tokens (input + output). To ensure the training process does not exhaust GPU memory, we truncate the output to a maximum of 160 tokens if it exceeds this limit during training.\n\n\n\n**Q4 In Section 3.2, the authors mention they would examine if the generated sentence exceeds 80 tokens or not \u2013 how is the length of 80 determined? Is it through empirical observations?**\n\n\nThe choice of this threshold is grounded in empirical observations. During our initial explorations, we manually examined the sentences generated and noticed that descriptions shorter than 80 tokens tended to be incomplete. Consequently, we established the threshold at 80 tokens.\n\n\n\n**Q5 What\u2019s the length of the learned visual queries from the Q-Former in Mini-GPT4?**\n\nIt is 32.\n\n\n**Q6 Missing reference.**\n\nWe have updated the related work section in the paper (highlighted in blue) to include more related references."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6224/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602422174,
                "cdate": 1700602422174,
                "tmdate": 1700602422174,
                "mdate": 1700602422174,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]