[
    {
        "title": "Masked Completion via Structured Diffusion with White-Box Transformers"
    },
    {
        "review": {
            "id": "qr20GIAU2o",
            "forum": "PvyOYleymy",
            "replyto": "PvyOYleymy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2759/Reviewer_2DR1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2759/Reviewer_2DR1"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces CRATE-MAE, a novel white-box deep network architecture designed for large-scale unsupervised representation learning. Unlike traditional networks, CRATE-MAE is rooted in the mathematical connection between diffusion, compression, and masked completion. Each layer of this architecture has a clear, interpretable role, transforming data into structured representations and vice versa. The study's key contribution is adapting the white-box design for unsupervised learning, a notable departure from its typical supervised applications. Empirically, CRATE-MAE outperforms traditional models on large imagery datasets with 30% fewer parameters, while offering structured and semantically meaningful representations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. **Theoretical Depth and Scientific Rigor**: The research stands out for its robust theoretical foundation, seamlessly intertwining denoising-diffusion models and information theory with white-box models. The model's design is intricately tied to its theoretical underpinnings, exemplifying the paper's scientific precision and thoroughness.\n\n2. **Problem Significance**: By addressing representation learning in high-dimensional data and delving into the untapped potential of white-box models in unsupervised settings, the paper carves a significant niche in the contemporary machine learning domain. Indeed, it paves the way for new avenues of exploration for the broader ML community."
                },
                "weaknesses": {
                    "value": "Firstly, I'd like to clarify that my emphasis is not solely on state-of-the-art results. My questions regarding the experiments stem from the belief that robust ideas and arguments deserve to be bolstered by thorough experiments.\n\n1. **Evaluation Concerns**: The introduction of the CRATE-MAE architecture in the paper falls short in offering a comprehensive quantitative analysis when compared to established benchmarks like MAE or contrastive methods. The results presented in Table 2 seem somewhat restrictive, making it challenging for readers to gauge the model's efficacy in relation to others.\n\n2. **Local Self-Supervised Learning Comparison**: From a broader perspective on self-supervised learning (SSL), this paper could be classified under local SSL, emphasizing layer-specific objectives and training. Although this area might be less traversed, incorporating findings from related works, such as [A], would enhance the paper's credibility.\n[A] Siddiqui, Shoaib Ahmed, et al. \"Blockwise self-supervised learning at scale.\" arXiv preprint arXiv:2302.01647 (2023).\n\n3. **Absence of Linear Probing Results**: Omitting linear probing results restricts the paper from showcasing the practicality and caliber of the representations derived using CRATE-MAE.\n\n4. **Dataset Limitations**: The study's dependence on a confined dataset for classification raises concerns about the breadth of its applicability and potential generalization to diverse scenarios."
                },
                "questions": {
                    "value": "I have no questions about the methodology part. Just please add more quantitative results to paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2759/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2759/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2759/Reviewer_2DR1"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2759/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697959178168,
            "cdate": 1697959178168,
            "tmdate": 1700487667645,
            "mdate": 1700487667645,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EZrhkkYYcE",
                "forum": "PvyOYleymy",
                "replyto": "qr20GIAU2o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2759/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2759/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments. We especially appreciate your remarks regarding the \u201ctheoretical depth and scientific rigor\u201d of our work, as well as the significance of the problem. We would like to respectfully point out a minor error in your review: instead of using 30% fewer parameters than base MAE (that is, using 70% of the parameters of MAE), we use 30% of the total parameters of MAE. We hope that this further clarifies the value of our empirical evaluations.\n\nBelow, we attempt to resolve your remaining comments and concerns.\n\n> Evaluation Concerns: The introduction of the CRATE-MAE architecture in the paper falls short in offering a comprehensive quantitative analysis when compared to established benchmarks like MAE or contrastive methods. The results presented in Table 2 seem somewhat restrictive, making it challenging for readers to gauge the model's efficacy in relation to others.\n\n> Absence of Linear Probing Results: Omitting linear probing results restricts the paper from showcasing the practicality and caliber of the representations derived using CRATE-MAE.\n\nThank you for pointing it out. We summarize some recent experimental results, which we will put in the camera-ready version, which evaluate the linear probing and fine-tuning performance of CRATE-MAE-Base (44.6M parameters) versus MAE-Small (47.5M parameters) [1] over different mask ratios. This experimental result shows that while CRATE-MAE-Base slightly underperforms its black-box counterpart, the performance is still quite reasonable.\n\n|Model (mask ratio)|CRATE-MAE-Base (25%)|CRATE-MAE-Base (50%)|CRATE-MAE-Base (75%)|CRATE-MAE-Base (90%)|*MAE-Small (25%)*|*MAE-Small (50%)*|*MAE-Small (75%)*|*MAE-Small (90%)*|\n|-|-|-|-|-|-|-|-|-|\nFinetuning|89.87|92.83|96.4|96.58|95.79|96.8|97.61|97.34|\nLinear Probing|58.41|62.72|71.12|64.15|54.22|68.99|79.43|75.13|\n\n> Local Self-Supervised Learning Comparison: From a broader perspective on self-supervised learning (SSL), this paper could be classified under local SSL, emphasizing layer-specific objectives and training. Although this area might be less traversed, incorporating findings from related works, such as [A], would enhance the paper's credibility. [A] Siddiqui, Shoaib Ahmed, et al. \"Blockwise self-supervised learning at scale.\" arXiv preprint arXiv:2302.01647 (2023).\n\nThank you for pointing this out. We will add a comparison to these methods in the camera-ready version, since we were not aware of them previously. We note that there is a significant conceptual difference between our work and \u201clocal SSL\u201d; instead of training layer-wise, we train our model end-to-end and the architecture (derived through discretized structured denoising-diffusion) ensures that the layer-wise characteristics (e.g. compression/sparsity improvements) are preserved. In contrast to the layer-wise training in [A], where each layer has an interpretable goal but black-box mechanisms, each layer and operator in our white-box model is interpretable by construction, and the interpretations hold throughout training.\n\n> Dataset Limitations: The study's dependence on a confined dataset for classification raises concerns about the breadth of its applicability and potential generalization to diverse scenarios.\n\nWe pre-train on ImageNet-1K, a standard large 2D image classification dataset commonly used for pre-training large image models. When fine-tuning for classification, we use many datasets such as CIFAR10/CIFAR100, Oxford Flowers, and Oxford Pets. We believe that this resolves any concern about fine-tuned classification performance overfitting to one dataset.\n\nAgain, thank you for your helpful comments. Please let us know if you have any more concerns or questions. We look forward to a fruitful discussion period.\n\n[1]: Gandelsman, Y., Sun, Y., Chen, X., & Efros, A. (2022). Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 35, 29374-29385."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700354032200,
                "cdate": 1700354032200,
                "tmdate": 1700354032200,
                "mdate": 1700354032200,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RWRdYWnbdj",
                "forum": "PvyOYleymy",
                "replyto": "EZrhkkYYcE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2759/Reviewer_2DR1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2759/Reviewer_2DR1"
                ],
                "content": {
                    "title": {
                        "value": "Further question"
                    },
                    "comment": {
                        "value": "Thank you for the clarification on the results and comparison. However, I'm curious as to why the model does not utilize true layer-wise training. The paper outlines explicit objectives for each layer, suggesting that they can be conceptually disentangled. From my own experiments, I understand that end-to-end training often outperforms layer-wise approaches. However, in your case, it seems like adopting a layer-wise manner would be a natural extension. Could you elaborate on this choice?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450747068,
                "cdate": 1700450747068,
                "tmdate": 1700450747068,
                "mdate": 1700450747068,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h7LjPosl36",
                "forum": "PvyOYleymy",
                "replyto": "NnVfZEuwzy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2759/Reviewer_2DR1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2759/Reviewer_2DR1"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I have no more technical questions any more. My suggestion is that, whenever you submit a paper, please include as much experiments as possible in the initial manuscript, instead of adding them in the rebuttal phase. Scalability is crucial for SSL, and this paper still leaves questions about its application to different tasks and data volumes.\n\nI have raised my score to 8. Good Luck."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635109610,
                "cdate": 1700635109610,
                "tmdate": 1700635109610,
                "mdate": 1700635109610,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1Ck4xXZxno",
            "forum": "PvyOYleymy",
            "replyto": "PvyOYleymy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2759/Reviewer_86oH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2759/Reviewer_86oH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a white-box diffusion model and unifies it with data compression under a single framework. Based on this, the method proposed by the author has achieved results comparable to the state-of-the-art, which validates their proposed theory."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Novelty in Approach: This paper is the first attempt to turn the diffusion model into a white-box network, and it has achieved good results. \n\nVersatility: Its methodology and conclusions can be used as good reference for subsequent research on white-box neural networks."
                },
                "weaknesses": {
                    "value": "1. In Sec 2.2, the authors intend to learn representations Z, and hope that the results learned are low-dimensional, sparse, bijective, etc. \nIf the method proposed by the authors is a white box, then these properties of Z should be verifiable through experiments. \nTherefore, the authors should provide experimental results of representations Z to support their theory. \n\n2. I checked the provided Pytorch code and find that MSSA and ISTA are composed of Linear layer, which implies large GPU Memory consumption.\nSo I'm wondering whether this method can be extended to large images, just like stable diffusion. \nIn addition, can the authors provide results from larger datasets? The images from CIFAR and ImageNet-1k are too small. \n\n3. How effective is this network at unconstrained image generation? \nIn other words, if the learning target is pure noise, which may not be viewed as an image compression task, would this method still work?"
                },
                "questions": {
                    "value": "Please refer to my comments in the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2759/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699057485743,
            "cdate": 1699057485743,
            "tmdate": 1699636218576,
            "mdate": 1699636218576,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iEmaoGEKJ9",
                "forum": "PvyOYleymy",
                "replyto": "1Ck4xXZxno",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2759/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2759/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments. We would like to respectfully reiterate that the methodology we introduce is not a diffusion model _per se_ \u2013 when one discusses \u201cdiffusion models\u201d they usually refer to generative models trained through score matching [1]. On the other hand, in our work we have introduced a framework for  invertible representation-learning models, whose representations are achieved through discretizations of probability flow equations, and trained via masked autoencoding (among many possible tasks). Thus the mechanism is analogous, not identical, to diffusion models.\n\nBelow, we attempt to resolve the remaining questions and concerns.\n\n> In Sec 2.2, the authors intend to learn representations Z, and hope that the results learned are low-dimensional, sparse, bijective, etc. If the method proposed by the authors is a white box, then these properties of Z should be verifiable through experiments. Therefore, the authors should provide experimental results of representations Z to support their theory.\n\nWe measure the low-dimensionality (indeed, the _compression_) of $Z$ by the rate distortion $R^{c}(Z \\mid U_{[K]})$. The network is shown to iteratively optimize this quantity at each layer, as demonstrated in Figure 4 of the paper. In this figure, not only are the final representations shown to be compressed, (almost) every intermediate representation in the network is shown to become progressively more compressed, showing that each layer implements compression, which aligns with our conceptual framework. Figure 4 also demonstrates a similar trend with the sparsity. We ensure that the representation is invertible by using a decoder, and training the encoder and decoder together on the masked autoencoding task. Figure 5 and Table 3 demonstrate invertibility of the representations.\n\n> I checked the provided Pytorch code and find that MSSA and ISTA are composed of Linear layer, which implies large GPU Memory consumption. So I'm wondering whether this method can be extended to large images, just like stable diffusion. In addition, can the authors provide results from larger datasets? The images from CIFAR and ImageNet-1k are too small.\n\nWe are not sure what you mean; linear layers are present in the vast majority of machine learning systems, such as Stable Diffusion [2]. We resize our images to 224 x 224 pixels before processing in order to standardize the input processing and tokenization, so the dataset would not matter. Increasing this size would make further pre-training much more computationally expensive, and infeasible for us to complete during this rebuttal period. If you think it would be better, we can increase the size of the figures in the camera-ready version, so as to make the fine details easier to see. \n\n> How effective is this network at unconstrained image generation? In other words, if the learning target is pure noise, which may not be viewed as an image compression task, would this method still work?\n\nUnconditional image generation is at odds with masked autoencoders, which are empirically used to learn representations instead of perform generation, and mathematically attempt to output $\\mathbb{E}[X \\mid \\mathtt{Mask}(X)]$ by minimizing the training loss. The best that masked autoencoders (even black-box MAE) can do with 100% mask ratio is to always output the constant image $\\mathbb{E}[X]$, which would be a blurring of all possible images, and so would not be desirable. Our framework does not have to use masked autoencoding as a pretext task, however, and there are several such tasks (e.g. denoising) which may enable generation. Concrete extensions of our framework to generation are out of scope of this paper.\n\nAgain, thank you for your helpful comments. Please let us know if you have any more concerns or questions. We look forward to a fruitful discussion period.\n\n[1]: Song, Y., & Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32.\n\n[2]: Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2021). High-resolution image synthesis with latent diffusion models. 2022 IEEE. In CVF Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 10674-10685)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700353272324,
                "cdate": 1700353272324,
                "tmdate": 1700353272324,
                "mdate": 1700353272324,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1MbjRpYaQD",
                "forum": "PvyOYleymy",
                "replyto": "1Ck4xXZxno",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2759/Reviewer_86oH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2759/Reviewer_86oH"
                ],
                "content": {
                    "title": {
                        "value": "Response to author's comment"
                    },
                    "comment": {
                        "value": "Thank you for your detailed response and the effort put into addressing the concerns raised. After careful consideration of your responses and a re-evaluation of the manuscript, I have decided to maintain my original score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558846943,
                "cdate": 1700558846943,
                "tmdate": 1700558874831,
                "mdate": 1700558874831,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fC6uNTQyzB",
            "forum": "PvyOYleymy",
            "replyto": "PvyOYleymy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2759/Reviewer_vMuz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2759/Reviewer_vMuz"
            ],
            "content": {
                "summary": {
                    "value": "The paper extends the white-box models from supervised learning to unsupervised (or self-supervised) representation learning. And in particular, it trains a Masked Autoencoder (MAE) to learn transferrable representations to downstream classification tasks. Besides, it shows interesting visualizations to demonstrate learned representations are of emerging semantic properties."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ Learning unsupervised representations for white-box models is a natural topic to study after white-box on supervised models, and could be of interest to members of the community.\n+ The paper did a good job in presentation, making both the approach and the experiments easy to follow.\n+ I like the visualizations in the end, which is a more intuitive addition to the numerical comparisons in the table."
                },
                "weaknesses": {
                    "value": "- I see a lot of similarities to the main white-box paper (White-Box Transformers via Sparse Rate Reduction) which is already out there for supervised learning. I want more justifications for the meaningfulness of the current work apart from doing unsupervised learning. \n- I am not convinced what's the advantage of models being white-box here, especially whether it has synergies with unsupervised learning. The paper explains a lot about what's done, but I don't see a strong motivation of why it is done (especially since the introduction is less of a story but more of a break-down of context and contributions).\n- While the explorations are interesting, I don't think the claims are backed up well by experiments. This is my biggest concern and would like to raise them by asking questions. So please see below."
                },
                "questions": {
                    "value": "* The paper lacks a fair comparison with MAE, especially on downstream tasks. Table 1 lists that MAE-base has more parameters and it could partially explain why Crate-MAE has a higher reconstruction loss. So what would a fair comparison (in terms of model parameters) look like? I think it is very easy to train MAE with a smaller encoder/decoder pair given the open-sourced code.\n* How are the evaluations done in Table 2? Are they using the encoder only? Are they with fully-visible inputs?\n* Table 4 again lacks a comparison of a similarly-sized MAE (how it performs on the same dataset as the mask ratio changes). It is not clear a conclusion from ImageNet classification can be transferred to CIFAR.\n* Figure 4: how does it compare with a supervised encoder/decoder trained on ImageNet? I want to know the compression and sparsity behavior is a result of unsupervised learning, or a result of the architecture design. The same applies to Figure 6 and 7."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2759/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699147639209,
            "cdate": 1699147639209,
            "tmdate": 1699636218519,
            "mdate": 1699636218519,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K4lwqPbxJh",
                "forum": "PvyOYleymy",
                "replyto": "fC6uNTQyzB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2759/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2759/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response, Part 1"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments. We especially appreciate your remarks regarding the work\u2019s \u201cgood job in presentation\u201d and the quality of the various visualizations. Below, we respond to the raised questions and comments.\n\n> I see a lot of similarities to the main white-box paper (White-Box Transformers via Sparse Rate Reduction) which is already out there for supervised learning. I want more justifications for the meaningfulness of the current work apart from doing unsupervised learning.\n\nWhile the main point of the work is to extend the white-box framework in [1] to unsupervised learning, we believe that this extension is in itself meaningful. To begin with, one cannot use the unrolled optimization framework in [1] to understand or construct white-box decoder models; this is because they map from structured feature spaces to the original data distribution, so they cannot optimize any representation learning objective at all. This means that one needs to develop a new conceptual toolkit to extend the white-box approach in [1] to unsupervised learning. In this work, we propose the novel \u201cstructured denoising-diffusion\u201d framework towards this end. We recover the unrolled optimization framework from [1] as a special case, but this time the framework is able to handle inversion and conditional generation (e.g., able to construct a white-box decoder network) via time-reversal. Note that this structured denoising-diffusion framework could be of independent interest in building practical and principled models for various tasks (such as unsupervised generation and self-supervised learning) beyond what we explored in this work. What\u2019s more, it relies on certain quantitative connections, derived in the paper (e.g. Theorem 1), between statistical notions of denoising and information-theoretic notions of lossy compression, which would be of independent technical and conceptual interest. \n\nWhile remarks to the above effect are interspersed through the paper, we will consolidate and expand on them in the camera-ready version.\n\n> I am not convinced what's the advantage of models being white-box here, especially whether it has synergies with unsupervised learning. The paper explains a lot about what's done, but I don't see a strong motivation of why it is done (especially since the introduction is less of a story but more of a break-down of context and contributions).\n\nThank you for the comment on desiring more motivation; we will certainly include more exposition towards this end in the camera-ready version. At a high level, white-box or theoretically principled models serve to make models more understandable or interpretable at a high granularity. This line of work thus attempts to bridge the divide between theory and practice; such a divide exists in both supervised and unsupervised deep learning. For theorists, simpler and principled models are easier to analyze and build meaningful theory off of. For practitioners, white-box networks retain empirical benefits over black-box models. To wit, we demonstrate in the paper that we obtain comparable autoencoding results to the MAE-ViT with only 30% of the parameters. (Below, we will clarify several of your remarks about to what degree the results are \u201ccomparable\u201d.) We also observe that attention maps in the encoder are interpretable.\n\n> The paper lacks a fair comparison with MAE, especially on downstream tasks. Table 1 lists that MAE-base has more parameters and it could partially explain why Crate-MAE has a higher reconstruction loss. So what would a fair comparison (in terms of model parameters) look like? I think it is very easy to train MAE with a smaller encoder/decoder pair given the open-sourced code.\n\nThank you for pointing it out. We ran a comparison against MAE-Small [2], which uses a similar number of parameters (47.5M) as CRATE-MAE-Base (44.6M). Since you ask for reconstruction loss, we evaluate the reconstruction loss on the training and testing datasets of ImageNet-1K. It shows that CRATE-MAE slightly underperforms its black-box counterpart, but the performance remains reasonable.\n\n||Train Reconstruction Loss|Test Reconstruction Loss|\n|-|-|-|\n|CRATE-MAE-Base|0.266|0.303|\n|MAE-Small|0.250|0.283|\n\n\n> How are the evaluations done in Table 2? Are they using the encoder only? Are they with fully-visible inputs?\n\nThe precise fine-tuning methodology is discussed in Appendix D due to space constraints. In short: the fine-tuning networks use the encoder only with an attached classification head, and use fully-visible inputs (i.e., no masking).\n\n**(Remaining responses and references posted in a followup.)**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700353080376,
                "cdate": 1700353080376,
                "tmdate": 1700353080376,
                "mdate": 1700353080376,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZG64vPz3t3",
                "forum": "PvyOYleymy",
                "replyto": "fC6uNTQyzB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2759/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2759/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response, Part 2"
                    },
                    "comment": {
                        "value": "> Table 4 again lacks a comparison of a similarly-sized MAE (how it performs on the same dataset as the mask ratio changes). It is not clear a conclusion from ImageNet classification can be transferred to CIFAR.\n\nThank you for pointing it out. We compared CRATE-MAE-Base against MAE-Small when pre-trained on different mask ratios, and conducted fine-tuning and linear probing on the training sets of CIFAR10. Below, we show the resulting models\u2019 classification accuracies on the test sets of CIFAR10.\n\n|Model (mask ratio)|CRATE-MAE-Base (25%)|CRATE-MAE-Base (50%)|CRATE-MAE-Base (75%)|CRATE-MAE-Base (90%)|*MAE-Small (25%)*|*MAE-Small (50%)*|*MAE-Small (75%)*|*MAE-Small (90%)*|\n|-|-|-|-|-|-|-|-|-|\nFinetuning|89.87|92.83|96.4|96.58|95.79|96.8|97.61|97.34|\nLinear Probing|58.41|62.72|71.12|64.15|54.22|68.99|79.43|75.13|\n\n\nWe are unfortunately not sure what you mean by \u201cIt is not clear a conclusion from ImageNet classification can be transferred to CIFAR.\u201d The pre-training is on ImageNet, and the fine-tuning in the referenced table is on CIFAR10, so we are not extrapolating classification performance anywhere.\n\n> Figure 4: how does it compare with a supervised encoder/decoder trained on ImageNet? I want to know the compression and sparsity behavior is a result of unsupervised learning, or a result of the architecture design. The same applies to Figure 6 and 7.\n\nThank you for the question. Actually, we observe from [1] that such a compression and sparsification effect holds for supervised classification training of the encoder network. Figures 6 and 7 are replicated in the supervised classification context in [3, Figures 1 and 3], which also demonstrates that black-box networks generally do not have this interpretability unless specifically trained for it [3, Figure 14]. To answer your question directly: [1] and [3] demonstrate that the main effect of the interpretability properties we observe is due to the _architecture design_, and _not_ due to training procedure. We will briefly clarify this in the camera-ready version.\n\nAgain, thank you for your helpful comments. Please let us know if you have any more concerns or questions. We look forward to a fruitful discussion period.\n\n[1]: Yu, Y., Buchanan, S., Pai, D., Chu, T., Wu, Z., Tong, S., ... & Ma, Y. (2023). White-Box Transformers via Sparse Rate Reduction\n\n[2]: Gandelsman, Y., Sun, Y., Chen, X., & Efros, A. (2022). Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 35, 29374-29385.\n\n[3]: Yu, Y., Chu, T., Tong, S., Wu, Z., Pai, D., Buchanan, S., & Ma, Y. (2023). Emergence of segmentation with minimalistic white-box transformers."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700353180499,
                "cdate": 1700353180499,
                "tmdate": 1700353180499,
                "mdate": 1700353180499,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O1CsRi3pNf",
                "forum": "PvyOYleymy",
                "replyto": "ZG64vPz3t3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2759/Reviewer_vMuz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2759/Reviewer_vMuz"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks, I acknowledge I have read the responses (and I should have said \"It is not clear a conclusion *of* ImageNet classification can be transferred *from* CIFAR\". I think they are detailed and helpful. However, since it is allowed to update the draft as well, I did expect more changes on the draft to reflect it would be \"ready\" before publication (sorry I checked late and there is not much time left for detailed updates now). Nonetheless, I think the rebuttal has largely addressed my concerns."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719317141,
                "cdate": 1700719317141,
                "tmdate": 1700719317141,
                "mdate": 1700719317141,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zwkNJKCTti",
            "forum": "PvyOYleymy",
            "replyto": "PvyOYleymy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2759/Reviewer_pN5Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2759/Reviewer_pN5Y"
            ],
            "content": {
                "summary": {
                    "value": "This paper generalizes the white-box design of transformer, i.e., CRATE, to the unsupervised representation learning context. The author finds out that the gradient on the rate distortions term $R^c(Z | U_[K])$ plays a similar role as the gradient for the score function with the noised input $\\tilde Z$, which points towards the closest point to $\\tilde Z$ on the data distribution support. Thus they construct a masked auto-encoder using CRATE backbone and achieves fine results on the representation learning tasks.\n\n===============\nThe author address most of my concerns and I will increase my score"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. This work generalizes the white-box transformer model to the unsupervised representation learning task, which is a novel attempt to both the theory and empirical community.\n2. The visualization results are quite impressive and show that CRATE-MAE can reconstruct the original data well, and Fig. 4 roughly shows that the compression measure and sparsity measure match the theory setting."
                },
                "weaknesses": {
                    "value": "1. I think the comparisons between CRATE-MAE-Base amd MAE-Base are not fair. I understand that the empirical evaluations are not optimally engineered and actually the visualization results are every good. However, I still think that the author should compare CRATE-MAE and MAE with (almost) the same amount of parameters and different performance, or alternatively, (almost) the same performance and different amount of parameters, and then to compare these two models.\n2. What's the choice of LASSO coefficient hyperparameter $\\lambda$, and the step size of discretization $\\kappa, \\eta$? Are they chose carefully and is the model sensitive to them?"
                },
                "questions": {
                    "value": "1. Which dataset does Fig.4 belongs to? Does the model have the similar patterns as Fig.4 on other datasets evaluated in this paper?\n2. Empirically, people think that the attention map $Q/K$ plays a different role as the mapping matrix $V$ and they'd better not be set to be the same. However, in this papers' theoretical framework, they can be set to the same parameter $U$. If change the mapping matrix from $U$ to $V\\neq U$, will the performance of CRATE-MAE change a lot? If no, what's the main reason why CRATE-MAE has such property?\n3. Will the layer normalization influence the rate deduction process, or it's just for making the training process more stable or other reasons?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2759/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2759/Reviewer_pN5Y",
                        "ICLR.cc/2024/Conference/Submission2759/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2759/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699836976289,
            "cdate": 1699836976289,
            "tmdate": 1700543455279,
            "mdate": 1700543455279,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tMGHjdvU0B",
                "forum": "PvyOYleymy",
                "replyto": "zwkNJKCTti",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2759/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2759/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response, Part 1"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments. We especially appreciate your remarks that our work is a \u201cnovel attempt to both the theory and empirical community\u201d and that \u201c[t]he visualization results are quite impressive.\u201d Below, we respond to the raised questions and comments.\n\n> I think the comparisons between CRATE-MAE-Base amd MAE-Base are not fair. I understand that the empirical evaluations are not optimally engineered and actually the visualization results are every good. However, I still think that the author should compare CRATE-MAE and MAE with (almost) the same amount of parameters and different performance, or alternatively, (almost) the same performance and different amount of parameters, and then to compare these two models.\n\nThank you for the comment. The CRATE-MAE-Base has similar parameters to MAE-Small [1], which uses a similar number of parameters (47.5M) as CRATE-MAE-Base (44.6M). In the following table, we demonstrate a comparison of linear probing (LP) and finetuning (FT) accuracy on the test data on CIFAR10 between these two models (pretrained on ImageNet-1K), showing that CRATE-MAE-Base is slightly outperformed by its black-box counterpart, but the performance remains reasonable.\n\n\n||CIFAR10 FT|CIFAR10 LP|CIFAR100 FT|CIFAR100 LP|Oxford Flowers FT|Oxford Flowers LP|Oxford Pets FT|Oxford Pets LP|\n|-|-|-|-|-|-|-|-|-|\n|CRATE-MAE-Base|96.4|71.12|80.24|48.64|70.29|59.23|76.18|38.38|\n|MAE-Small|97.61|79.43|82.96|54.36|84.22|68.45|81.68|51.21|\n\n\n> What's the choice of LASSO coefficient hyperparameter $\\lambda$, and the step size of discretization $\\kappa$, $\\eta$? Are they chose carefully and is the model sensitive to them?\n\nWe follow the hyperparameter choices in [2], that is, $\\lambda = 0.1$, $\\eta = 0.1$, and $\\kappa$ and $\\epsilon^{2}$ tuned so that $\\kappa p/(N\\epsilon^{2}) = 1$. We do _not_ choose hyperparameter values carefully (e.g., through ablation or other robust tuning). Indeed, as you already noted the experiments in this work are intended as a proof of concept to illustrate the promise of our white-box approach and framework, though they already demonstrate some empirical benefits such as parameter saving and interpretability. If you would like a more detailed quantitative analysis, let us know and we can upload it to the camera-ready version, but performing a detailed robustness analysis on ImageNet would be computationally infeasible during the review period.\n\n> Which dataset does Fig.4 belongs to? Does the model have the similar patterns as Fig.4 on other datasets evaluated in this paper?\n\nThank you for pointing out that the paper is missing this information; we will include it in the camera-ready version. Figure 4 demonstrates the compression and sparsity of tokens within an image from ImageNet-1K, averaged over 10,000 random samples with error bars displayed. All models in this work were pre-trained on ImageNet-1K, and this task is evaluated before fine-tuning on any datasets. Thus we are not sure if you suggest that we look at models that are pre-trained from scratch on the other datasets using masked autoencoding, or look at models that are pre-trained on ImageNet-1K and fine-tuned on other datasets. We have tried both on the CIFAR10 dataset; we generally observe the same trends as in the paper Figure 4 for both settings.\n- Here are the results for the model pre-trained on CIFAR10: https://imgur.com/a/KdTzZDU\n- Here are the results for the model pre-trained on ImageNet-1K and fine-tuned on CIFAR10: https://imgur.com/a/GZ2iYbK\n\n**(Remaining responses and references posted in a followup.)**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700351794287,
                "cdate": 1700351794287,
                "tmdate": 1700544390294,
                "mdate": 1700544390294,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xg3ABDvRS2",
                "forum": "PvyOYleymy",
                "replyto": "zwkNJKCTti",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2759/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2759/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response, Part 2"
                    },
                    "comment": {
                        "value": "> Empirically, people think that the attention map $Q$/$K$ plays a different role as the mapping matrix $V$ and they'd better not be set to be the same. However, in this papers' theoretical framework, they can be set to the same parameter $U$. If change the mapping matrix from $U$ to $V \\neq U$, will the performance of CRATE-MAE change a lot? If no, what's the main reason why CRATE-MAE has such property?\n\nThank you for pointing this out! We were not aware that people suggest that the $Q$ and $K$ matrices have distinct conceptual roles from the $V$ matrices, and would be interested in any references(s) which explain(s) this, so that we could incorporate them into our paper. We believe that in fact the $Q$, $K$, and $V$ matrices could be considered conceptually in the same role. Our justification in this paper, as well as other works on white-box transformers [2, 3], yield this interpretation from both empirical and theoretical perspectives, as well as a proposal of Hinton: [4]. White-box transformers are simpler [2] and have different, more interpretable [3] operational characteristics than the ViT. Thus, the suggestion that $V \\neq U$, obtained through some subtle post-hoc analysis in the ViT context, may not be straightforwardly carried over to CRATE.\n\n\n> Will the layer normalization influence the rate deduction process, or it's just for making the training process more stable or other reasons?\n\nThe approximations made to derive the MSSA and ISTA blocks require the features to have constant-order magnitude, so some normalization is required in order to make the implementation match the underlying theoretical principles. Using layer-normalization as opposed to \u201ctrue\u201d normalization (i.e., manually normalizing each token feature to the unit sphere) is motivated by improved empirical performance and easier training. Our framework does not attempt to give a principled reason why we should use one over the other. We will incorporate this discussion into the camera-ready version.\n\nOnce again, we thank you for your insightful comments. Please let us know if you have any other questions or concerns. We look forward to a fruitful discussion period.\n\n[1]: Gandelsman, Y., Sun, Y., Chen, X., & Efros, A. (2022). Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 35, 29374-29385.\n\n[2]: Yu, Y., Buchanan, S., Pai, D., Chu, T., Wu, Z., Tong, S., ... & Ma, Y. (2023). White-Box Transformers via Sparse Rate Reduction\n\n[3]: Yu, Y., Chu, T., Tong, S., Wu, Z., Pai, D., Buchanan, S., & Ma, Y. (2023). Emergence of segmentation with minimalistic white-box transformers.\n\n[4]: Hinton, G. (2023). How to represent part-whole hierarchies in a neural network. Neural Computation, 35(3), 413-452."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700351874392,
                "cdate": 1700351874392,
                "tmdate": 1700351874392,
                "mdate": 1700351874392,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Kd8ES2YiTl",
                "forum": "PvyOYleymy",
                "replyto": "zwkNJKCTti",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2759/Reviewer_pN5Y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2759/Reviewer_pN5Y"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response. Your additional experiments and illustration address most of my concerns and I will increase my score.\n\n As for the sensitive analysis, the ablation study for parameters like $ \\lambda $ may be helpful but not necessary since as you mentioned, the main focus of this study is to construct the theoretical framework of a white-box transformer.\n\nFor the roles of $Q/K$ and $V$, I\u2019m very sorry that my claim may be confusing. Some previous works like reformer[5] may claim that shared-QK parameters will not significantly influence the performance of model, but people may heuristically think that the attention matrix $W_Q$/$W_K$ and the value matrix $W_V$ will project the input to different spaces. Actually, I can\u2019t find other reference papers for this common choice, and the materials you provide address my concern, I think it may be beneficial to add the discussion to the paper. Maybe the MSSA block and the update process (2.4~2.6) which is different from the heuristically designed transformer result in the effectiveness of using the single matrix $U$. And I think this is really an interesting point of this approach\n\n[5]  Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. ICLR, 2020."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543401397,
                "cdate": 1700543401397,
                "tmdate": 1700543638536,
                "mdate": 1700543638536,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qwtfeRDqwC",
                "forum": "PvyOYleymy",
                "replyto": "zwkNJKCTti",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2759/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2759/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for the reply, and for raising the score!\n\nWe totally agree with your point \u2013 we believe that the white-box MSSA block is an efficient and useful operator (in part) because of its derivation from first principles as compressing the representations towards the desired statistical model. In particular, such derivations from first principles could potentially inform useful architecture modifications, even if not all architectures currently have such interpretations.\n\nWe will add this discussion on parameter-sharing (including some discussion of related work, such as [5], which considers sharing $Q$/$K$ parameters), and an ablation on the sparsity parameter $\\lambda$ and learning rate $\\eta$, to the camera-ready version. \n\nThank you again! Please let us know if you have other questions or comments during the discussion period."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2759/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546505374,
                "cdate": 1700546505374,
                "tmdate": 1700547175813,
                "mdate": 1700547175813,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]