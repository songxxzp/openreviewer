[
    {
        "title": "Towards 3D Molecule-Text Interpretation in Language Models"
    },
    {
        "review": {
            "id": "cYMdK2OGfT",
            "forum": "xI4yNlkaqh",
            "replyto": "xI4yNlkaqh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5052/Reviewer_3SWU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5052/Reviewer_3SWU"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to learn 3D molecule representations that can be used with pretrained large language models. Using pretrained 3D molecule encoder and large language models, the proposed method fine-tunes Q-Former to get molecule representation in language model space, and fne-tunes large language model to predict text (i.e., description) given at molecule embeddings and SMILES representations. The proposed method shows improvements compared with using large language model (e.g., LLaMA) naively."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- I'm not an expert in this area, but it seems the proposed approach is quite novel to leverage pretrained language model for downsteam tasks related to 3D molecules.\n- The paper is generally well-written and easy to follow.\n- The paper is well-motivated. \n- The paper includes code for reproducibility."
                },
                "weaknesses": {
                    "value": "- The paper misses to include some failure cases (e.g., hallucination) of the proposed method. For instance, in Table 3(b), it seems the proposed 3D-MoLM wrongly interprets Globostellatic acid B (C34H48O6 while the ground truth is C33H48O7); the paper needs to discuss when the model tends to predict the wrong results. \n- I think the comparison in Table 3 might be somewhat unfair due to the different model sizes used for evaluation. Specifically, MolT5-Large has 800M parameters, while 3D-MoLM mainly uses LLaMA-7B models. In this respect, I suspect the improvements might simply come from the simple usage of language models with a large number of parameters and large training data rather than considering the 3D geometry of molecules. A good supporting experiment can be conducted by using a smaller LM for experiments (e.g., FLAN-T5-large).\n- I have a similar concern in Table 2 as well since the model used in the proposed method is much larger and uses much more data for pretraining compared with other baselines."
                },
                "questions": {
                    "value": "- Have the authors tried using larger model (e.g., LLaMA-13B)? If the authors tried, does it show the better performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5052/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5052/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5052/Reviewer_3SWU"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5052/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698648896836,
            "cdate": 1698648896836,
            "tmdate": 1700633804350,
            "mdate": 1700633804350,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L5epI4bn20",
                "forum": "xI4yNlkaqh",
                "replyto": "cYMdK2OGfT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5052/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5052/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3SWU -- Part 1"
                    },
                    "comment": {
                        "value": "Thank you for your time and constructive feedback! To address your concerns, we present the point-to-point responses as follows. We have carefully revised our paper, taking all your feedback into account.\n\n> **Q1. Failure case study.** The paper misses to include some failure cases (e.g., hallucination) of the proposed method. For instance, in Table 3(b), it seems the proposed 3D-MoLM wrongly interprets Globostellatic acid B (C34H48O6 while the ground truth is C33H48O7); the paper needs to discuss when the model tends to predict the wrong results.\n\n**Response:** Thank you for your insightful suggestion. We agree that discussing failure cases is crucial for a comprehensive understanding of our model's performance. In response, we have revised our submission to include two failure case studies in Appendix D. Specifically, we show two mistakes of 3D-MoLM: 1) it confuses between Globostellatic acid C and Globostellatic acid B; 2) it fails to generate a molecule's correct chemical formula. We attribute these two mistakes to the following limitations:\n\n1. The discernment of 3D-MoLM at a more refined granularity requires further enhancement. \n2. Existing language models, including GPT-4, are not yet fully capable of accurately counting the number of atoms and providing the correct chemical formula.\n\nThese cases underscore areas where 3D-MoLM can be improved. Thank you again for your constructive critique, which has significantly contributed to enhancing the quality of our paper.\n\n> **Q2: Performance of 3D-MoLM using smaller base LMs.** I think the comparison in Table 3 might be somewhat unfair due to the different model sizes used for evaluation. Specifically, MolT5-Large has 800M parameters, while 3D-MoLM mainly uses LLaMA-7B models. In this respect, I suspect the improvements might simply come from the simple usage of language models with a large number of parameters and large training data rather than considering the 3D geometry of molecules. **A good supporting experiment can be conducted by using a smaller LM for experiments** (e.g., FLAN-T5-large).\n\n**Response:** Thank you for your insightful comments. We understand your concern about the potential bias in the comparison due to different model sizes. To address this, we have included additional experiments by replacing Llama2 with MolT5 in Appendix F.\n\nSpecifically, we have transplanted the 3D molecule encoder and the projector onto MolT5. This allows us to make a fair comparison with the same base LM. As shown in the table below, MolT5s with 3D perception consistently outperforms those without 3D perception across all scales. This result substantiates our claim that the incorporation of 3D geometry perception enhances the molecular understanding of LM.\n\n**Table 1: Comparison between 3D-MoLM and MolT5 for molecule captioning.** **X: without 3D perception. Y: with 3D perception.**  \n\n| Base LM     | 3D Perception | BLEU-2    | BLEU-4    | ROUGE-1   | ROUGE-2   | ROUGE-L   | METEOR    |\n| ----------- | :-----------: | --------- | --------- | --------- | --------- | --------- | --------- |\n| MolT5-small |       X       | 22.53     | 15.23     | 30.44     | 13.45     | 20.30     | 23.98     |\n| MolT5-small |       Y       | **23.73** | **16.57** | **31.57** | **14.34** | **21.63** | **24.81** |\n| MolT5-base  |       X       | 24.51     | 16.61     | 32.19     | 14.04     | 21.35     | 26.10     |\n| MolT5-base  |       Y       | **25.55** | **17.31** | **33.36** | **15.49** | **22.62** | **27.54** |\n| MolT5-large |       X       | 25.87     | 17.28     | 34.07     | 16.42     | 23.41     | 28.04     |\n| MolT5-large |       Y       | **26.91** | **18.50** | **35.25** | **17.37** | **25.50** | **29.45** |\n\nWe hope this additional experiment adequately addresses your concern and further underscores the value of considering 3D geometry of molecules in our model."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579940478,
                "cdate": 1700579940478,
                "tmdate": 1700579940478,
                "mdate": 1700579940478,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GzyTyAbsCF",
                "forum": "xI4yNlkaqh",
                "replyto": "GExL64eHOq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5052/Reviewer_3SWU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5052/Reviewer_3SWU"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. It addressed my concern well and I raise my score from 5 to 6."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633791137,
                "cdate": 1700633791137,
                "tmdate": 1700633791137,
                "mdate": 1700633791137,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cgiIrK9Un3",
            "forum": "xI4yNlkaqh",
            "replyto": "xI4yNlkaqh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5052/Reviewer_Zm2K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5052/Reviewer_Zm2K"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework to interpret and analyze 3D molecules by equipping the LM with a 3D molecular encoder. The framework integrates a 3D unimol and a LLama language model, the framework is the same as BLIP2. Through a pre-train and fine-tune strategy, the authors demonstrate the effectiveness of the molecular caption, retrieval, and open QA tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper proposes a new framework for 3D molecule-text pertaining and fine-tuning to conduct multiple 3D molecular tasks.\n2. The framework is clear and reasonable.\n3. The experiments are conducted different tasks and the effectiveness is demonstrated."
                },
                "weaknesses": {
                    "value": "1. The novelty of this work is actually hard to say enough. It is clear that the framework is almost the same as BLIP2, the pre-training tasks and the fine-tuning stages are also the same. \n2. Besides, there are multiple different small places where I would like to ask questions. Most of them are unclear descriptions. Please look in the questions."
                },
                "questions": {
                    "value": "1. The first question is about the Q-former (this may be the same as the Q-former in BLIP2, but the process is not clear here).  From Figure 2, the caption task and the contrastive task will use the text branch for text processing. These tasks are only used in pre-training, hence the question is that in the inference stage, the text branch in Q-former will be ignored (if I understand correctly). Therefore, the pre-training and the fine-tuning are mismatched, since we will only use llama2 for text generation, instead of the text branch in Q-former. What's the performance if we remove these two pertaining tasks and also the text branch in Q-former?\n2. In Figure 3, stage 1 is molecule-text retrieval, and stage 2 is the molecular caption, but in section 2.2.1, stage 1 describes multitask training with the whole three tasks. See \"Following BLIP,m we perform multi-task training, including....\". Please clarify the correct one. \n3. For the downstream tasks, the model is fine-tuned after different stages, which causes multiple different models, each task for a model. I am wondering about the multitasking fine-tuning and the performance of one model. Or at least, after three/two stage trainings, then finetune on the downstream tasks. \n4. At the beginning of section 3.1, the authors mentioned \"we assess the stage-1 checkpoint of ...\", but the last sentence on page 6 is \"Q-former benefits from the multi-task pre-training\", if the stage only contains retrieval task, the claim is wrong.\n5. From Table 4, it seems the performance gain is not as large as expected on open-text QA, do the authors have some analysis? 2D is similar to 3D.\n6. In Appendix C, did the retrieval task use LORA for fine-tuning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5052/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5052/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5052/Reviewer_Zm2K"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5052/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698746251597,
            "cdate": 1698746251597,
            "tmdate": 1700795262886,
            "mdate": 1700795262886,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MldgPQbQhr",
                "forum": "xI4yNlkaqh",
                "replyto": "cgiIrK9Un3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5052/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5052/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Zm2K -- Part 1"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your constructive and thorough comments. Your suggestions, especially the ones about including a generalist model and clarifying training stages, have significantly improved our presentation. To address your concerns, we provide responses as follows. If you have additional concerns, we would be pleased to discuss them with you.\n\n> **Q1: Novelties and contributions.** The novelty of this work is actually hard to say enough. It is clear that the framework is almost the same as BLIP2, the pre-training tasks and the fine-tuning stages are also the same.\n\n**Response:** We appreciate your comments, but wish to respectfully emphasize our novelties and contributions. Our adaptation of Q-Former to 3D molecule-to-text generation tasks is nontrivial, offering a new dimension to 3D molecule understanding. \n\nFurther, we have made significant modifications to BLIP-2 for adaptation to the molecule domain, and devise a dataset for 3D-molecule centric instruction tuning. Our novelties and contributions, compared to BLIP-2, are summarized below:\n\n* **First 3D molecule-text modeling method.** 3D-MoLM is, to our knowledge, the first method to explore 3D molecule-text modeling. We believe 3D-MoLM advances the field by enabling text-based understanding of 3D molecules.\n* **Multi-task Generalist Model through Instruction-tuning.** Unlike BLIP-2, 3D-MoLM includes a multi-modal instruction-tuning stage. This instruction tuning phase aims to align 3D-MoLM to human preference, and develop a generalist model that can perform various downsteam tasks. The performance of the generalist 3D-MoLM are reported in our updated Table 3 and Table 4. See our response to your Q4 for more details. \n* **Instruction-tuning Dataset.** We present 3D-MoIT for 3D molecule-text instruction tuning. 3D-MoIT is, to our knowledge, the first 3D molecule-centric instruction tuning dataset. We also propose the GPT-3.5 enriched version of PubChem for 3D molecule-text alignment. We believe these datasets can facilitate further research and development in this area. \n\nWe hope this clarifies the unique contributions of our work and differentiate it from existing models like BLIP2."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579654195,
                "cdate": 1700579654195,
                "tmdate": 1700579654195,
                "mdate": 1700579654195,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pdyeUQJsRy",
                "forum": "xI4yNlkaqh",
                "replyto": "cgiIrK9Un3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5052/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5052/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Zm2K -- Part 2"
                    },
                    "comment": {
                        "value": "> **Q2.** The first question is about the Q-former (this may be the same as the Q-former in BLIP2, but the process is not clear here). From Figure 2, the caption task and the contrastive task will use the text branch for text processing. These tasks are only used in pre-training, hence the question is that in the inference stage, **the text branch in Q-former will be ignored (if I understand correctly)**. Therefore, **the pre-training and the fine-tuning are mismatched**, since we will only use llama2 for text generation, instead of the text branch in Q-former. **What's the performance if we remove these two pertaining tasks and also the text branch in Q-former?**\n\nThank you for the valuable question and comment. We have divided this question into three sub-questions and prepared answers separately:\n\n**Q2.1. Is** **Q-Former's text-branch ignored in inference?**\n\n**Response:** **Q-Former's text-branch is crucial in the inference of molecule-text retrieval, alghough it is not used in molecule-to-text generation.** Specifically, Q-Former's text branch is used in both molecule-text contrasting (MTC) and molecule-text matching (MTM) -- the two essential components for molecule-text retrieval [1]. In MTC, the text branch generates text embeddings to calculate cosine similarities for contrasting; In MTM, the text branch processes text tokens, from which the query tokens obtain text information. **Therefore, removing the text-branch will impede the molecule-text retrieval functionality.** \n\nOn the other hand, removing the text branch will not affect molecule-to-text generation inference. However, the performance of molecule-to-text generation can be hurted, which is explained in the response to Q2.3.\n\n**Q2.2. The text generation task uses Q-Former's text branch, which is ignored during the inference of text generation. Therefore,** **the pre-training and the fine-tuning are mismatched**, **since we will only use Llama2 for text generation, instead of Q-Former's text branch.**\n\n**Response:** Thank you for kindly identifying the ambiguity! We would like to clarify that the pretrainnig (using Llama2 for text generation in stage 2 and 3) and the inference (using Llama2 for generation after stage 3) is aligned, instead of mismatched. Our original Figure 2 may give the wrong impression that only the Q-Former's text branch is used for text generation. This issue is fixed in our revised Figure 2. The correct training strategy is: Q-Former's text branch is trained for text generation in stage 1, and Llama2 is trained for text generation in stage 2 and 3. Therefore, Llama2 is fully optimzed for generation before inference, and the pretraining and inference is aligned.\n\n**Q2.3. How will 3D-MoLM perform if we remove the text-branch and the objectives of molecule-text contrasting and molecule captioning in training stage 1?** \n\n**Response:** It is important to note that removing the text-branch eliminates all three objectives in training stage 1: the molecule-text mathcing objective is eliminated because it relies on the text-branch to process text tokens [1]. Therefore, removing the text-branch means to remove training stage 1 and use randomly initialized Q-Former for stage 2. Then, the question boils down to stage 1's influence to text generation performance, which is shown below:\n\n**Table 1: Molecule captionging performance on PubChem dataset.** \n\n| Training stages     | BLEU-2    | BLEU-4    | ROUGE-1   | ROUGE-2   | ROUGE-L   | METEOR    |\n| ------------------- | --------- | --------- | --------- | --------- | --------- | --------- |\n| Only Stage 2        | 27.84     | 20.35     | 34.19     | 19.42     | 28.90     | 30.81     |\n| Original, stage 1&2 | **30.32** | **22.52** | **36.84** | **22.32** | **31.23** | **33.06** |\n\nWe observe that including stage 1 pretraining significantly improves molecule captioning performances. This is because stage 1 pretraining acts as a \"warmup\" training before stage 2 and 3 [1]. It prepares the Q-Former's ability of extracting molecule features that are the most relevant to the texts, which is achieved by the three multi-modal objectives in stage 1."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579695521,
                "cdate": 1700579695521,
                "tmdate": 1700644221195,
                "mdate": 1700644221195,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Gv5jmLxf2c",
                "forum": "xI4yNlkaqh",
                "replyto": "cgiIrK9Un3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5052/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5052/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Discussion"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments and suggestions on our submission. Your suggestions to 1) clarify the novelties and contributions of this work, 2) illustrate and evaluate the functionality of Q-Former's text branch, 3) clarify the training objective functions of the three training stages, and 4) include a generalist model for generations tasks of molecule captioning and open-text QA, have helped us to substantially improve the clarity, significance, and comprehensiveness of our submission. We sincerely hope that these improvements can be taken into consideration.  \n\n**Now we are approaching the end of the discussion period on November 22nd.** If our response has resolved your concerns on our paper, we will greatly appreciate it if you could re-evaluate our paper. Should you have any further questions or need additional clarification, please know that we are eager and prepared to continue our discussions."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726410455,
                "cdate": 1700726410455,
                "tmdate": 1700726444493,
                "mdate": 1700726444493,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wb0c3rNtPe",
            "forum": "xI4yNlkaqh",
            "replyto": "xI4yNlkaqh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5052/Reviewer_mT4V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5052/Reviewer_mT4V"
            ],
            "content": {
                "summary": {
                    "value": "This paper presented the 3D-MoLM, a novel molecule-text multimodal language model. This model is a combination of two pretrained foundation models: Uni-Mol and Llama2 for molecular and language understanding. The key contribution of this paper lies in its three-staged training process that integrates these two pretrained models.\n\nIn the first stage of training, the authors adopted the idea of Q-former and multi-task training approach, a method that has been previously proposed in vision-language multimodal model, to align the representation spaces of molecules and languages. Afterwards, the Q-former and Llama2 models are finetuned by solving the molecular captioning task. In the final stage, instruction tuning was performed to the model. To this end, the authors constructed 3D-MoIT, a 3D molecule-centric instruction tuning dataset.\n\nThe proposed method was tested on several molecule-language multimodal tasks, including molecule-text retrieval, molecule captioning, and molecular question answering. The experimental results demonstrated that the superiority of the proposed models by comparing them with several baseline models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "As a machine learning researcher, multimodal learning between molecule and language domain was expected to appear sooner or later. In this light, I think this study was well presented at the right time. The authors effectively adopted several techniques of existing visual-language multimodal learning to combine a molecular domain model and large language model. In addition, initial results of instruction tuning via 3D-MoIT dataset demonstrated the potential usage of large language model in molecular domain."
                },
                "weaknesses": {
                    "value": "The main criticism I have is that the motivation and needs for molecule-language multimodal model remains unclear. Of course, the multimodality between molecule and language domain is a promising research direction for machine learning researchers and practitioners. However, let us consider the situation where scientists actually conduct research using the proposed model. In what scenarios can the proposed model be utilized and how can it lead to the synthesis or retargeting of novel/existing compounds? The molecular domain is the task for domain experts (i.e., chemists and biologists), not the general users. Thus, discussion on the use case of the molecule-language multimodality for domain experts is crucial for setting the ultimate goal for this line of research. Unfortunately, however, this paper and other work in this field, seems to avoid answering this pivotal question. I hope that the authors address this question in the rebuttal or the revised manuscript."
                },
                "questions": {
                    "value": "First of all, as mentioned in the weakness section, I would like to ask the authors about the motivation and need for molecule-language multimodal learning. What are the specific use cases of the proposed model molecule-language model? How would scientists or practitioners (i.e., domain experts) be able to harness the proposed method in their research or business?\n\nRegarding the question-answering task, the authors performed molecular properties prediction. Basically, properties including HOMO/LUMO are typically derived from the density functional theory calculations or molecular representation models such as Uni-Mol. Is there any specific rationale or chance that the language model enhances the prediction accuracy of these properties? Additionally, this work only compared the prediction accuracy with language models, and the ability to predict the properties comes from the Uni-Mol as aforementioned; therefore, I think the Uni-Mol should also be the baseline of the experiment, which will demonstrate the effect of the multimodal learning.\n\nFinally, PubChem database provides the 3D conformation of each molecule. While I am not sure what method were employed to obtain the 3D structure in PubChem, it might be worthwhile to consider leveraging these data instead of MMFF-relaxed structures."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5052/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816172620,
            "cdate": 1698816172620,
            "tmdate": 1699636494935,
            "mdate": 1699636494935,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7T2QSU1v5c",
                "forum": "xI4yNlkaqh",
                "replyto": "wb0c3rNtPe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5052/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5052/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mT4V -- Part 1"
                    },
                    "comment": {
                        "value": "Thank you for the thorough review and positive feedback! Your meticulous suggestions have helped us to improve the clarity and enhance the comparison to baselines. To address your concerns, we present the point-to-point responses as follows.\n\n> **Q1. The motivation and practical application of studying molecule-text modeling.** The main criticism I have is that the **motivation and needs for molecule-language multimodal model remains unclear.** Of course, the multimodality between molecule and language domain is a promising research direction for machine learning researchers and practitioners. However, let us consider the situation where scientists actually conduct research using the proposed model. In what scenarios can the proposed model be utilized and **how can it lead to the synthesis or retargeting of novel/existing compounds?** The molecular domain is the task for domain experts (i.e., chemists and biologists), not the general users. Thus, discussion on the use case of the molecule-language multimodality for domain experts is crucial for setting the ultimate goal for this line of research. Unfortunately, however, this paper and other work in this field, seems to avoid answering this pivotal question. **I hope that the authors address this question in the rebuttal or the revised manuscript.** \n>\n> ...\n>\n> First of all, as mentioned in the weakness section, I would like to ask the authors about the motivation and need for molecule-language multimodal learning. What are the specific use cases of the proposed model molecule-language model? How would scientists or practitioners (i.e., domain experts) be able to harness the proposed method in their research or business?\n\n**Response:** Thank you for the insightful suggestion. We believe studying molecule-text multi-modal learning have values for scientists in (but not limited to) two aspects: 1) solving scientific tasks that are multi-modal in nature; 2) improving uni-modal scientific tasks by leveraging the science knowledge or reasoning ability of LMs. We now elaborate:\n\n* **Solving multi-modal scientific tasks.** These tasks are multi-modal in nature and cannot be solved without multi-modal methods. For example, experimental procedure prediction [1] requires reading chemical reactions (molecule modality) and output experimental instructions (text modality). **This task has significant practical values for automating chemical synthesis experiment design**, and 3D-MoLM can be a foundation model for fine-tuning on this task. Another example is molecule-text retrieval, where chemists can specify desired chemical properties (e.g., biological activity and toxicity) using natural language, and retrieve molecules from database that includes unannotated molecules. **This task can help retargeting existing molecules.** \n\n* **Improving uni-modal scientific tasks.** Uni-modal tasks (e.g., molecule property prediction) can potentially be improved by using the chemistry knowledge in LMs. [2] shows an example where joint multi-modal molecule-text learning improves molecule property prediction performance. [3] shows an example where joint multi-modal learning between an LM and a protein encoder improves the protein encoder's protein classification performance."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579500026,
                "cdate": 1700579500026,
                "tmdate": 1700579500026,
                "mdate": 1700579500026,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "evDHNoihaG",
                "forum": "xI4yNlkaqh",
                "replyto": "39F8bhkfzc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5052/Reviewer_mT4V"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5052/Reviewer_mT4V"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "After carefully reading the response from the authors, I generally agree with their perspective.\n\nNonetheless, the proposed method and dataset do not seem to be capable of dealing with the scientific tasks mentioned in the response at the moment.\nWhile I acknowledge that this study is a first step towards the development of a multi-modal scientific language model, I think it is important that the authors specify the limitations of their work.\nThis will not only clarify the contribution of this work, but also provide a guide for future research directions in this area.\n\nConsidering these points, I will maintain my initial rating."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661211629,
                "cdate": 1700661211629,
                "tmdate": 1700661211629,
                "mdate": 1700661211629,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JPubk6TZif",
                "forum": "xI4yNlkaqh",
                "replyto": "wb0c3rNtPe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5052/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5052/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mT4V"
                    },
                    "comment": {
                        "value": "> **Q4:** After carefully reading the response from the authors, I generally agree with their perspective.\n>\n> Nonetheless, the proposed method and dataset do not seem to be capable of dealing with the scientific tasks mentioned in the response at the moment. While I acknowledge that this study is a first step towards the development of a multi-modal scientific language model, I think it is important that the authors specify the limitations of their work. This will not only clarify the contribution of this work, but also provide a guide for future research directions in this area.\n>\n> Considering these points, I will maintain my initial rating.\n\n**Response:** Thanks for your meticulous review and insightful feedback. In response, we have revised our submission and prepared responses below: \n\n\n\n**Regarding the capability of solving multi-modal scientific tasks.** \n\n* First, we want to highlight that 3D-MoLM have shown promising results for some (although not all) multi-modal scientific tasks mentioned in our previous response. Specifically, 3D-MoLM achieves significant and consistent improvements for molecule-text retrieval (Table 3) and predicting descriptive molecular properties (Table 4a); it achieves comparable performances for predicting computed molecular properties (Table 4c), when contrasted with Uni-Mol. \n* Second, we acknowledge the limitations of the existing model and dataset, and want to propose the directions for future improvement:\n\n    * **More diverse and high-quality datasets.** As you might have noticed, while some tasks are interesting and valuable (e.g., experimental procedure prediction [1]), they lack publicly available datasets (as per our submission time) to assist open-source studies. Additionally, existing tasks like molecular captioning and molecule-text retrieval can be further developed by introducing new datasets that focus on more complex and novel chemical properties (e.g., molecular spectra). \n\n    * **More fine-grained alignemnt between 3D molecules and texts.** In Appendix D, we have included failure case studies showing 3D-MoLM's limitation in discerning fine-grained small molecular structures. Overcoming this issue is crucial for enhancing performance in tasks that demand a precise understanding of 3D molecular structures. One potential direction is to curate a new 3D molecule-text dataset with explicit 3D coordinate references within textual descriptions. Another direction is to explore more powerful 3D molecular encoders and 3D molecule-text projectors.\n\n\n\n**Regarding the limitation section.** In response to your suggestion, **we have revised and expanded the limitation section in Appendix G (page 18).** This revised section now includes discussion about `expanding to more molecule-text modeling tasks` and `fine-grained 3D molecule-text alignment`. We also have a section discussing 3D-MoLM's failure cases in Appendix D, suggesting the exploration of better architectures and datasets for a more fine-grained alignment between 3D molecular geometries and textual concepts. \n\n\n\nYour insights have significantly contributed to enhancing the completeness of our work. We hope our explanations and revisions can resolve your concerns about our submission. Should you have any other concerns, we are more than happy for discussions on openreview.\n\n**Reference:**\n\n[1] Inferring experimental procedures from text-based representations of chemical reactions. In Nature Communications 2021."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719771266,
                "cdate": 1700719771266,
                "tmdate": 1700719870530,
                "mdate": 1700719870530,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kuNMOhO24U",
            "forum": "xI4yNlkaqh",
            "replyto": "xI4yNlkaqh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5052/Reviewer_HgTH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5052/Reviewer_HgTH"
            ],
            "content": {
                "summary": {
                    "value": "This paper focus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular Language Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze 3D molecules by equipping the LM with a 3D molecular encoder. This integration is achieved by a 3D molecule-text projector, bridging the 3D molecular encoder\u2019s representation space and the LM\u2019s input space. Overall, this is an interesting work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is well-organized and written. The proposed 3D-MoLM enables an LM to interpret and analyze 3D molecules by equipping the LM with a 3D molecular encoder which is interesting and motivating."
                },
                "weaknesses": {
                    "value": "The analysis of the proposed 3D-MoLM is not enough. The proposed method is interesting and straight-forward, any insight or analysis that could be offered to read to have a good understanding?"
                },
                "questions": {
                    "value": "Refer to Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5052/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834278153,
            "cdate": 1698834278153,
            "tmdate": 1699636494862,
            "mdate": 1699636494862,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OZHZDPnqvu",
                "forum": "xI4yNlkaqh",
                "replyto": "kuNMOhO24U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5052/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5052/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HgTH"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your valuable comments and positive feedback! They have helped us to enrich the analysis of 3D-MoLM for a better understanding. To address your concern, we present the following response.\n\n> **Q1: More insight and analysis.** The analysis of the proposed 3D-MoLM is not enough. The proposed method is interesting and straightforward, any insight or analysis that could be offered to read to have a good understanding?\n\n**Response:** Thank you for your constructive feedback. We agree that an in-depth analysis can help readers better understand 3D-MoLM's performances, and identify potential directions for future research and improvement.\n\n**Firstly,** we have updated our paper to include a comparison between the performance of specialist 3D-MoLMs, which are fine-tuned for each task individually, and the generalist 3D-MoLM, which is a unified model trained on all tasks and evaluated using a consistent checkpoint. We also summarize the performance of them in the table below:\n\n**Table1: One checkpoint for captioning, open-text QA and computed properties.****\n\n**Table 1a: Molecule captioning performance.**\n\n|            | BLEU-2 | BLEU-4 | ROUGE-1 | ROUGE-2 | ROUGE-L | METEOR |\n| ---------- | ------ | ------ | ------- | ------- | ------- | ------ |\n| Specialist | 30.32  | 22.52  | 36.84   | 22.32   | 31.23   | 33.06  |\n| Generalist | 29.25  | 22.07  | 36.48   | 21.80   | 30.95   | 33.12  |\n\n**Table 1b: Descrptive open-text QA performance.**\n\n|            | BLEU-2 | BLEU-4 | ROUGE-1 | ROUGE-2 | ROUGE-L | METEOR |\n| ---------- | ------ | ------ | ------- | ------- | ------- | ------ |\n| Specialist | 32.00  | 26.13  | 40.13   | 25.55   | 34.64   | 52.15  |\n| Generalist | 31.61  | 25.88  | 39.93   | 25.68   | 34.78   | 51.74  |\n\n**Table 1c: Computed properties QA performance.**\n\n|            | Weight | LogP | TPSA  | Complexity | HOMO | LUMO | H-L Gap | SCF  |\n| ---------- | ------ | ---- | ----- | ---------- | ---- | ---- | ------- | ---- |\n| Specialist | 14.79  | 0.66 | 9.71  | 44.85      | 0.26 | 0.25 | 0.28    | 0.35 |\n| Generalist | 16.58  | 0.78 | 10.90 | 45.49      | 0.35 | 0.36 | 0.32    | 0.38 |\n\nThe results show that while the generalist model slightly underperforms in comparison to the specialist models, it still exhibits a performance gain over other baselines. This underscores its versatility and capability to effectively handle multiple tasks. For more detailed information, please refer to the performance tables in the revised paper (Table 3 and Table 4 in the revised paper) and the training details in Appendix C.\n\n**Secondly,** we have revised our draft to include failure case studies in Appendix D, serving as preliminary examinations of 3D-MoLM's limitations. We identify two main limitations:\n\n* 3D-MoLM's ability to discern fine-grained small molecular structures can be improved. In the revised Table 6, we show an example that 3D-MoLM confuses between Globostellatic acid C and Globostellatic acid B, where the only difference is the position of a methoxy group.\n\n* We show that 3D-MoLM, and also GPT-4, cannot accurately count the number of atoms and generate the chemical formulas.\n\nWe are committed to continuously refining 3D-MoLM to address these issues. For a more detailed analysis and failure case study, please refer to Appendix D in the revised paper. We hope this analysis provides a more nuanced understanding of 3D-MoLM's capabilities and prompts future enhancement."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5052/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579375904,
                "cdate": 1700579375904,
                "tmdate": 1700579375904,
                "mdate": 1700579375904,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]