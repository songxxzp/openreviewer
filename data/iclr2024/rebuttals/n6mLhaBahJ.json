[
    {
        "title": "HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments"
    },
    {
        "review": {
            "id": "ApEyG0psge",
            "forum": "n6mLhaBahJ",
            "replyto": "n6mLhaBahJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4703/Reviewer_qCJU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4703/Reviewer_qCJU"
            ],
            "content": {
                "summary": {
                    "value": "This paper builds a novel embodied AI environment, addressing the dynamically changing environments. They also provide an API to employ large language models (LLMs) for action selection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The ability to detect changes and adapt to changes in dynamically changing environments is key to intelligent agents. It is good to see people start to develop environments to address such challenges. The authors also provide support for LLMs to perform action selection."
                },
                "weaknesses": {
                    "value": "see questions"
                },
                "questions": {
                    "value": "I am curious about how hard are the three environments. Also, which decision-making approaches do you think will dominate in the HAZARD challenge?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4703/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698588752302,
            "cdate": 1698588752302,
            "tmdate": 1699636451982,
            "mdate": 1699636451982,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kWoHY1YTHj",
                "forum": "n6mLhaBahJ",
                "replyto": "ApEyG0psge",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4703/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4703/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer qCJU"
                    },
                    "comment": {
                        "value": "> How hard are the three environments\n\nThe HAZARD challenge's difficulty primarily comes from its constantly changing environments. We believe it is challenging enough to effectively differentiate the performance of various methods. However, it is not excessively difficult, as all agents can complete the task to some degree. Specifically, the challenge involves planning in dynamic environments, comprehending the effects and dynamics of these changes, and accurately perceiving the evolving surroundings.\n\n> Which decision-making approaches do you think will dominate in the HAZARD challenge?\n\nThank you for your question. Based on current results, LLM-based agents demonstrate strong decision-making capabilities, while MCTS-based agents excel among other baselines. The success of MCTS may be attributed to its natural ability to simulate environments, leading to a deeper understanding of potential future states. Therefore, we identify two key factors for superior performance in HAZARD:\n* Agents capable of simulating environmental changes and imagining future states accurately during decision-making, then basing their decisions on these internal simulations.\n* Agents powered by more robust foundational models. Our research reveals that LLM-based agents with a GPT-4 backbone outperform those using GPT-3.5, and GPT-3.5 surpasses Llama-13B. This trend indicates that LLM-based agents benefit from stronger foundational models."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4703/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649561400,
                "cdate": 1700649561400,
                "tmdate": 1700649561400,
                "mdate": 1700649561400,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "J0H2iuXiwB",
            "forum": "n6mLhaBahJ",
            "replyto": "n6mLhaBahJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4703/Reviewer_BqkX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4703/Reviewer_BqkX"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new simulated benchmark for interactive agents that focuses on dynamic changes in the environment, caused by fire, flooding or strong winds, which the agent needs to react to dynamically. It provides a simulator, a set of 100 scenes for each of the three tasks (split between train and test) and a language interface that allows LLM-based agents to interact with the environment. Evaluations of LLM and non-LLM agents show that the common-sense reasoning in LLMs is beneficial, but they lack ability to react to dynamic environment changes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The simulator is an interesting step towards testing agents' abilities in scenarios where the environment is dynamically changing and these changes are uncontrollable by the agent. This seems like a scenario of practical relevance for search-and-rescue applications.\n\nThe physics of the fire / flood / wind, albeit simplistic, seem sufficient for meaningful yet systematic changes to occur in the environments. The visual rendering of the effects is sufficiently realistic judging from the photos provided.\n\nIt is nice that the benchmark supports language-based descriptions and interaction out of the box to facilitate research on higher level reasoning systems while removing the burden of low-level control and perception if not desired.\n\nFurther, the procedural environment generation allows for the creation and evaluation of diverse scenes, so that overfitting to a few environments is prevented."
                },
                "weaknesses": {
                    "value": "While I like the text-based interface, it also seems like a weakness of the benchmark that it seems primarily designed to evaluate the high-level reasoning capabilities of agents, rather than the potentially more challenging low-level manipulation aspects of these tasks. To put differently, would there be any difference if the benchmark wasn't implemented with a nice graphics pipeline but instead as a fully text-based game a la nethack? My understanding is that at least the main LLM planner method would work without change.\n\nI do acknowledge that the authors try to provide versions of the benchmark that require visual perception of the environment. It would be nice though if a similar array of different options was provided on the action representation side. The current benchmark only supports very high-level actions. It is further unclear whether the RL policy that the authors compare to has all the same privileged observation and action primitive access that the LLM planner has. A fair comparison would be good here.\n\nThere is some information that is missing from the paper (see questions below). The paper does provide some videos on their website, but the videos \"with agent\" did not load for me, which makes it hard to judge how fast the environment changes with respect to the speed of the agent, and thus how challenging the tasks are.\n\nWhile the current split of train:test is reasonable, it would be nice to investigate certain axis of generalization for the different agents more systematically. E.g. one could specifically test generalization to larger rooms or to new objects etc.\n\nFinally, as acknowledged by the authors, the agent currently has no way of *influencing* the dynamics of the environment, e.g. by putting out the fires instead of working around them. It would be good to add such capabilities to more holistically evaluate disaster responses, but I acknowledge that this is challenging and the lack thereof does not invalidate the contributions of this submission."
                },
                "questions": {
                    "value": "- is the robot agent itself incurring damages as it moves eg through fire / water?\n\n- is there a way to generate training demonstrations, e.g. for an imitation learning pipeline?\n\n- what simulation speed does the current simulator support? can it be run on headless servers & with multiple workers in parallel? this is important for understanding whether it can support RL workflows.\n\n\n# Post-Rebuttal Comments\n\nThank you for answering my review. I appreciate the adaptations to the benchmark, in particular taking environment effects on the agent into account and supporting generalization evaluations.\nOverall, I support acceptance of the paper. I also skimmed through the other reviews and the rebuttal seems to at least in part address the concerns of the reviewer that voted \"marginally below acceptance\", so I will increase my score to accept."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "--"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4703/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4703/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4703/Reviewer_BqkX"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4703/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698631502549,
            "cdate": 1698631502549,
            "tmdate": 1700934977621,
            "mdate": 1700934977621,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VAhpnaDPqJ",
                "forum": "n6mLhaBahJ",
                "replyto": "J0H2iuXiwB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4703/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4703/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer BqkX"
                    },
                    "comment": {
                        "value": "> Would there be any difference if the benchmark wasn't implemented with a nice graphics pipeline but instead as a fully text-based game a la nethack? My understanding is that at least the main LLM planner method would work without change\u2026It would be nice though if a similar array of different options was provided on the action representation side. The current benchmark only supports very high-level actions.\n\nThank you for your comment. Unlike text-based games, our challenge supports the simulation of continuous observations and low-level actions, such as moving a specific distance or turning by a precise degree.\n\nFollowing your suggestion, we have provided interfaces for using low-level action APIs for agents, along with relevant documentation. However, when limited to low-level actions alone, current agents struggle with task completion. For example, when we replace the action set of wind scenes with low level actions {turn, move forward, pick up, drop} instead of {walk to nearest, pickup nearest, drop}, the RL agent almost fails to retrieve even one single object.\n\nWe preliminarily test the RL agent equipped with low-level actions in wind scenes. However, as the following results demonstrate, the agent almost fails in this setting, indicating that this new setting poses significant challenges to the agents.\n\n| Without Perception     | Value | Step   |\n|------------------------|-------|--------|\n| RL                     | 8.5   | 1044.9 |\n| RL (low-level actions) | 3.7   | 1144.0 |\n\n> It is further unclear whether the RL policy that the authors compare to has all the same privileged observation and action primitive access that the LLM planner has. A fair comparison would be good here.\n\nYes, the observations are the same. We modify the observation to fit a 2D-array for RL and text description for other methods, but they all have access to full observation. We modified the actions of RL a bit as described in the paper, as it\u2019s infeasible for RL to output exact object indices.\n\n> It would be nice to investigate certain axis of generalization for the different agents more systematically. E.g. one could specifically test generalization to larger rooms or to new objects etc.\n\nIn response to your valuable suggestion, we have created two new test sets: one with new objects and another with larger rooms. We've also provided documentation to facilitate the addition of new objects.\n\nIn addition, we propose a method to tackle the test set with new objects in the \u2018with perception' version of HAZARD. We have replaced the R-CNN perception module with Grounded-SAM to generate segmentations for scenes containing new objects. We examine this new perception module on the test set with new objects, and results are summarized as follows:\n\n| With Perception | Fire  |       |        | Flood |       |        |\n|-----------------|-------|-------|--------|-------|-------|--------|\n|                 | Value | Step  | Damage | Value | Step  | Damage |\n| Rule            | 35.8  | 334.3 | 28.0   | 21.9  | 332.3 | 84.9   |\n| Greedy          | 31.3  | 326.5 | 32.3   | 23.6  | 318.8 | 80.0   |\n| Random          | 42.3  | 311.9 | 24.5   | 32.5  | 291.6 | 76.9   |\n| RL              | 45.7  | 291.9 | 25.5   | 36.0  | 233.0 | 77.2   |\n| MCTS            | 69.6  | 164.5 | 16.5   | 32.2  | 181.1 | 71.2   |\n\nAccording to the results, with the help of the strong segmentation model, all baseline models are able to retain most of their effectiveness when encountering new objects. The results are included in Appendix C.\n\n> The agent currently has no way of influencing the dynamics of the environment\n\nThank you for your insightful comments. We are developing more diverse actions to enhance agents' interaction with the environment. For instance, we've introduced a 'put-out-fire' action, enabling an agent to extinguish fire at a specific location (please refer to the \u2018put out fire\u2019 gif on our website). We are also developing actions for water pumping and wind blocking. Such additions will not only increase mission complexity but also make the challenge more engaging."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4703/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649429892,
                "cdate": 1700649429892,
                "tmdate": 1700649429892,
                "mdate": 1700649429892,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SHUrsnfyg9",
                "forum": "n6mLhaBahJ",
                "replyto": "J0H2iuXiwB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4703/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4703/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer BqkX - part 2"
                    },
                    "comment": {
                        "value": "> Is the robot agent itself incurring damages as it moves eg through fire / water?\n\nCurrently not. We value your suggestion and have accordingly integrated the impact of hazards on agents into the HAZARD challenge. To achieve this, we've developed a new setting where agents are directly affected by environmental effects:\n* In fire scenes, the environment now causes damage to agents from fire by influencing the agent's temperature. This adaptation requires modifications to all baseline methods, particularly for fire scenes, as fire often blocks paths, demanding a different approach of planning algorithms.\n* For flood and wind scenes, we've introduced a setting where agents are influenced by drag forces. Detailed descriptions and results of baseline methods in this new setting can be found in Appendix B. Given that most baseline methods show only a slight performance decrease, it suggests that environmental factors slightly increase the challenge in both flood and wind scenes.\n\n> Is there a way to generate training demonstrations, e.g. for an imitation learning pipeline?\n\nWe have produced demonstrations on a training set using an oracle planner with access to complete ground truth information. After obtaining the ground truth information for all time steps, the oracle planner traverses all possible rescue plans and identifies the one with the highest value. If multiple plans have the same value, the plan with the fewest time steps is selected. However, this planner assumes successful action execution (e.g., unobstructed navigation), so it may not be perfect. This part of discussion is included in Append D.\n\n> What simulation speed does the current simulator support? can it be run on headless servers & with multiple workers in parallel? This is important for understanding whether it can support RL workflows. \n\nThe HAZARD benchmark, developed on the ThreeDWorld simulator, can accommodate multiple workers on a single machine and is compatible with headless servers equipped with an X11 server. For further details, please refer to the ThreeDWorld documentation (https://github.com/threedworld-mit/tdw/tree/master/Documentation). Slow simulation is a common challenge with many simulators, but it can be mitigated through a large pool of parallel processes or by setting a lower resolution."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4703/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649509787,
                "cdate": 1700649509787,
                "tmdate": 1700649509787,
                "mdate": 1700649509787,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "J9rLwDcBAX",
            "forum": "n6mLhaBahJ",
            "replyto": "n6mLhaBahJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4703/Reviewer_oxZh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4703/Reviewer_oxZh"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new virtual environment for embodied agents from the perspective of dynamic environmental change in the real world, focusing on unexpected disaster scenarios, including fires, floods, and winds. A benchmark HAZARD is developed to evaluate embodied agents making decisions in dynamically changing environments. Further qualitative results on HAZARD the challenges of dynamic environments to existing baseline agents. Notably, the limitations of reasoning and responding to dynamically changing environments for LLM-based agents are analyzed."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- This work simulates new scenarios of dynamic unexpected disasters for embodied agents. Based on the simplified physical model, the scenes achieve a good balance between realistic physical properties and real-time simulation\n- This work provides a new simulated embodied benchmark HAZARD to save valuable items in unexpected disaster scenarios to assess the performance of agents in dynamic simulations. The new benchmark will promote the study and enhancement of the performance of embodied agents in dynamic scenarios\n- Under the HAZARD benchmark, the quantitative results on a range of baselines, including LLM-based agents, provide a base performance of all baseline methods in dynamic environments.\n- The paper proposes to focus on the ability of embodied agents to respond quickly to environmentally driven changes, and reveal the strengths and limitations of LLMs-based agents based on the results, where the performance of the LLMs-based approach is limited by the dynamic environment.\n- The paper is well written and easy to follow with its detailed documentation and open source code."
                },
                "weaknesses": {
                    "value": "- The rendering quality and motion effects of the simulated scene are still somewhat different from reality, which may result in some perceptually based agents using camera images that will lead to domain gaps.\n- The current scenario exhibits a limited scale. Is there a method available to efficiently expand the current scenario, such as loading assets and assigning attributes to different disaster scenes?\n- The proposed LLM pipeline has to input the same task description at each step to perform sequential decisions, which is not efficient.\n- The mission objectives of the benchmarks are relatively simple, focusing on the task of object rescue, and the benchmarks can be expanded further in terms of mission complexity."
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4703/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698678422596,
            "cdate": 1698678422596,
            "tmdate": 1699636451814,
            "mdate": 1699636451814,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fP3velq8YF",
                "forum": "n6mLhaBahJ",
                "replyto": "J9rLwDcBAX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4703/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4703/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer oxZh"
                    },
                    "comment": {
                        "value": "> The rendering quality and motion effects of the simulated scene are still somewhat different from reality, which may result in some perceptually based agents using camera images that will lead to domain gaps.\n\nThank you for your comments. \n\nWe view rendering quality as a balance with simulation speed. Given the large scale of fire and flood in the HAZARD challenge, we abandon pursuit for fine-grained simulations, such as detailed fire particle effects and fluid simulation, to evaluate embodied agents more efficiently. Additionally, our work utilizes the ThreeDWorld simulator, which comparatively offers realistic visual effects among current simulators.\n\n> The current scenario exhibits a limited scale. Is there a method available to efficiently expand the current scenario, such as loading assets and assigning attributes to different disaster scenes?\n\nYes. Our codebase includes documentation on efficiently adding new assets and assigning attributes within HAZARD. We have updated the documentation with more detailed instructions for loading assets and assigning attributes.\n\n> The proposed LLM pipeline has to input the same task description at each step to perform sequential decisions, which is not efficient.\n\nWe apologize for any confusion. In the chat model, the chain-of-thought reasoning's second step only requires previous conversations and an added prompt stating, \u201cAnswer with only one best next action. So the answer is option\". For decisions at different time steps, it is necessary to input the task description each time, as LLM-based agents struggle to make decisions without the description in their context.\n\n> The mission objectives of the benchmarks are relatively simple, focusing on the task of object rescue, and the benchmarks can be expanded further in terms of mission complexity.\n\nThank you for your insightful comments. We are developing more diverse actions to enhance agents' interaction with the environment. For instance, we've introduced a 'put-out-fire' action, enabling an agent to extinguish fire at a specific location (please refer to the current \u2018put out fire\u2019 gif on our website). We are also developing actions for water pumping and wind blocking. Such additions will not only increase mission complexity but also make the challenge more engaging.\n\nMoreover, we wish to highlight that HAZARD's main difficulty lies in its dynamic environmental changes, which is our primary focus. Considering the baseline methods' performance on HAZARD, we believe the challenge already presents sufficient difficulty."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4703/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648967003,
                "cdate": 1700648967003,
                "tmdate": 1700648967003,
                "mdate": 1700648967003,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YlEjUwgNCd",
            "forum": "n6mLhaBahJ",
            "replyto": "n6mLhaBahJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4703/Reviewer_p2Ka"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4703/Reviewer_p2Ka"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new benchmark, \u2018HAZARD\u2019, to evaluate an agent's ability to complete a task in environments with changing dynamics. Specifically, the proposed environments simulate three different unexpected disaster scenarios:  fire, flood, and wind. The three disaster scenarios are simulated based on the ThreeDWorld platform. In addition, the authors evaluate several baselines, including an LLM-based approach, on the newly proposed benchmark. The experimental results show that the LLM-based approach outperforms other methods in most of the tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality and Significance:    \nThe reviewer found the proposed environments interesting. The environment could be valuable in two folds: (1) It evaluates an agent\u2019s capability to adapt to changing dynamics, which is an important capability of an embodied agent. (2) The simulated three hazard scenarios could foster future research on rescuing embodied agents. \n \n\nQuality:   \nThe paper is technically sound. The proposed environments and LLM-based policy are thoroughly evaluated. \n\n\nClarity:    \nThe paper is generally well-organized and easy to follow."
                },
                "weaknesses": {
                    "value": "1. It seems that the hazard\u2019s effect on the functionality of the embodied agents is not simulated. For instance, how does the fire and high temperature affect the functionality of the agent? Similarly, in the flood scenario, is the agent affected by the buoyancy force and drag force? The reviewer found the simulation of damage to the agents is an important piece of a realistic simulator.   \n\n2. How would the proposed approach address a partially broken embodied agent? Would approaches similar to Zeng [1] work?  \n\n3. The reviewer found the baselines in the experimental section somewhat weak. Comparing existing works, such as Landi [2] for embodied AI on changing environments could make the experimental section more convincing.   \n\n4. Could you elaborate why the MCTS-based method outperforms other baselines in terms of fire step and flood step?  \n\n\n[1] Moving Forward by Moving Backward: Embedding Action Impact over Action Semantics, Zeng et al., ICLR 23.  \n\n[2] Spot the Difference: A Novel Task for Embodied Agents in Changing Environments, Landi et al., 2022"
                },
                "questions": {
                    "value": "Please see the above section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4703/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699233138407,
            "cdate": 1699233138407,
            "tmdate": 1699636451737,
            "mdate": 1699636451737,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IF55NSr6BE",
                "forum": "n6mLhaBahJ",
                "replyto": "YlEjUwgNCd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4703/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4703/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer p2Ka"
                    },
                    "comment": {
                        "value": "> It seems that the hazard\u2019s effect on the functionality of the embodied agents is not simulated.\n\nWe value your suggestion and have accordingly integrated the impact of hazards on agents into the HAZARD challenge. To achieve this, we've developed a new setting where agents are directly affected by environmental effects:\n\n* In fire scenes, the environment now causes damage to agents from fire by influencing the agent's temperature. This adaptation requires modifications to all baseline methods, particularly for fire scenes, as fire often blocks paths, demanding a different approach of planning algorithms.\n* For flood and wind scenes, we've introduced a setting where agents are influenced by drag forces. Detailed descriptions and results of baseline methods in this new setting can be found in Appendix B. Given that most baseline methods show only a slight performance decrease, it suggests that environmental factors slightly increase the challenge in both flood and wind scenes.\n\n> How would the proposed approach address a partially broken embodied agent? Would approaches similar to Zeng [1] work?\n\nCurrently, our approach to hazards can involve blocking paths or applying additional forces on agents, which do not result in partially broken embodied agents. Zeng [1]'s work examines different agent action impacts, distinct from our study of environmental change impacts on agent decisions. We have incorporated this discussion into our related work section.\n\n> The reviewer found the baselines in the experimental section somewhat weak. Comparing existing works, such as Landi [2] for embodied AI on changing environments could make the experimental section more convincing.\n\nFollowing your advice, we have introduced a new baseline, MCTS-diff, building upon the previously strongest baseline, MCTS. MCTS-diff adopts the concept from Landi's work, encouraging agents to move towards areas where environmental changes are more probable. This is implemented by assigning additional value to MCTS nodes with significant temperature or flood signal changes, guiding agents to areas with rapid environmental changes.\n\nHowever, Landi's work primarily focuses on identifying differences between two scenes, leading their method to motivate agents towards areas with probable environmental changes. In contrast, our environment has continuous changes. Our objective is to evaluate agent decisions under these ongoing environmental changes, rather than merely identifying the changes. As the following results indicate, MCTS-diff performs worse than MCTS. This suggests that exploring areas with environmental changes may not be an effective strategy in the HAZARD challenge.\n\n|           | Fire  |       |        | Flood |       |        |\n|-----------|-------|-------|--------|-------|-------|--------|\n|           | Value | Step  | Damage | Value | Step  | Damage |\n| MCTS      | 75.9  | 150.1 | 19.7   | 43.7  | 146.6 | 69.9   |\n| MCTS-diff | 72.2  | 156.6 | 20.9   | 40.6  | 150.0 | 73.2   |\n\nThe summarization of this discussion has been included in our related work section.\n\n> Could you elaborate why the MCTS-based method outperforms other baselines in terms of fire step and flood step?\n\nThe MCTS method outperforms other baselines in both Step and Value metrics. Compared to LLM-based agents, MCTS-based methods consistently pursue the target with the lowest cost without altering their target during navigation or replan chances. On the other hand, LLM-based agents might choose a different target upon reaching the maximum navigation steps and having the opportunity to replan. Consequently, MCTS agents typically achieve a lower Step count than LLM-based agents."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4703/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648821274,
                "cdate": 1700648821274,
                "tmdate": 1700648821274,
                "mdate": 1700648821274,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]