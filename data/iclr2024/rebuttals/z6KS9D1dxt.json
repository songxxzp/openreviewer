[
    {
        "title": "Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game"
    },
    {
        "review": {
            "id": "sTB1o0eY5X",
            "forum": "z6KS9D1dxt",
            "replyto": "z6KS9D1dxt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission872/Reviewer_UzHb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission872/Reviewer_UzHb"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses Byzantine failures in multi-agent reinforcement learning (MARL), where agents can adopt malicious behavior due to malfunction caused by hardware/software faults. First, a new problem setting, called Bayesian Adversarial Robust Dec-POMDP (BARDec-POMDP), is proposed to model adversarial intervention as a transition, where originally cooperative joint actions are modified before being executed in the actual environment. The setting is inspired by Bayesian games, where agents have types that need to be inferred by other agents through their beliefs. The goal is to learn a more fine-grained solution than prior work, where policies are able to collaborate with functional agents, while being robust against adversaries therefore finding a better cooperation-robustness trade-off. Ex post mixed-strategy robust Bayesian Markov perfect equilibrium is proposed as a solution concept for BARDec-POMDPs, which is claimed to be weakly dominant over equilibria that were pursued in previous works.\n\nBased on this setting, a two-timescale actor-critic is proposed based on the robust Harsanyi-Bellman equation, defining the maximin value according to the ideal case (before any adversarial modification) and the adversarial case. The policy gradient uses the value of the adversarial case as critic. The approach is evaluated in a variety of games, including a matrix game, a gridworld game, and a map of SMAC. It is also evaluated against different attacker types."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper addresses a very important problem, which is often neglected in the MARL community.\n\nThe proposed setting (BARDec-POMDP) can model realistic scenarios, where agents can behave adversarially due to malfunctioning.\n\nWhile most works on robust MARL focus on defenses against any kind of adversarial attack, which leads to conservative policies, the paper aims to find a trade-off between collaborating with actual functional agents and robustness against actual adversaries, which is a more desirable goal for real-world applications. Bayesian games provide a neat framework to discern between these types of agents, which is a convincing proposal.\n\nThe solution concept seems valid.\n\nI like the evaluation, which first starts with a small toy problem before scaling up to larger benchmarks like SMAC. I also appreciate the evaluation with different adversarial strategies."
                },
                "weaknesses": {
                    "value": "My main concern with the paper is the lack of self-containment: The paper excessively references the appendix, which hurts readability (it is not convenient to peek into the appendix for almost every paragraph of the main paper).\n\nThe paper needs a clear prioritization of what is really important and what can be safely left in the appendix. For example, important proofs can be sketched informally with little space, but delegating them to the appendix completely is not acceptable.\n\nBefore checking, if I can raise my score, I need a revised version of the paper that is more self-contained, e.g., where the number of appendix references is significantly reduced and proofs are at least sketched such that the reader is not forced to switch documents all the time.\n\n**Minor Comments**\n- In Dec-POMDPs (Section 2.1), policies select their actions by the history of past observations and actions. Conditioning only on observations is insufficient since they are not Markovian. The definition in Section 2.2 is correct, though.\n- Line 119: \u201cwas replaced\u201d - \u201c**is** replaced\u201d\n- Line 277: \u201cfour type of threats\u201d - \u201cfour type**s** of threats\u201d\n- The text size in Figure 2 is too small for printed versions."
                },
                "questions": {
                    "value": "1. Why do adversaries condition on observations despite them not being Markovian (in contrast to the non-adversarial policies)? Is this intentional or a mistake?\n2. Do you have any intuition on why M3DDPG performs so poorly in the presence of an adversary compared to the standard MAPPO without any robustness mechanism in Figure 4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission872/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission872/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission872/Reviewer_UzHb"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission872/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698041520105,
            "cdate": 1698041520105,
            "tmdate": 1700544258549,
            "mdate": 1700544258549,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i3EmSOpulG",
                "forum": "z6KS9D1dxt",
                "replyto": "sTB1o0eY5X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission872/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission872/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response of Submission 872 to Reviewer UzHb"
                    },
                    "comment": {
                        "value": "We thank Reviewer UzHb for recognizing the significance, problem setting, theoretical concept, solution concept and evaluation of our paper. A detailed response is provided below.\n\n> Q1: Self-containment and conciseness.\n\nWe appreciate this valuable feedback. To address this, we have:\n\n* Removed unnecessary discussions, reducing references to the appendix from 17 times to 8 times and decreasing the appendix length from 21 pages to 12 pages. We have meticulously revised the manuscript to ensure it remains focused on the core message. For completeness, we will host a GitHub page for additional experiment results upon acceptance.\n\n* Included a concise proof sketch for each theorem to enhance self-containment.\n\n* Reduced the use of mathematical notations and added more textual explanations.\n\nPlease refer to our global response and the revised manuscript for more details.\n\n> Q2: Why do adversaries condition on observations despite them not being Markovian (in contrast to the non-adversarial policies)? Is this intentional or a mistake?\n\nThank you for pointing out the concern. Please allow us to clarify that in our original draft, for both defender and adversary, their policies are conditioned on histories $H$, not observations $o$. For example:\n\n* Section. 2.1, paragraph 2, \"...each agent $i$ selects its action $a_t^i \\in \\mathcal A^i$ using its policy $\\pi^i(\\cdot|H_i^t)$\"; \"the goal of all agents is to learn a joint policy ... that maximize the long-term return \"$J(\\mathbf{\\pi}) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t|s_0, \\mathbf{a}_t \\sim \\pi(\\cdot|H_t)\\right]$\".\n* Section. 2.2, paragraph 2, \"replaced by action $\\hat{a}^i_t$ sampled from an adversary with policy $\\hat{\\pi}^i(\\cdot|H_t^i, \\theta)$\"; \"the value function can be defined as $V_\\theta(s) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t|s_0=s, \\mathbf{a}_t \\sim \\pi(\\cdot|H_t), \\hat{\\mathbf{a}}_t \\sim \\hat{\\pi}(\\cdot|H_t, \\theta)\\right]$\".\n\nA thorough review of our paper confirms that the policy of both defenders and adversaries rely on histories, not observations.\n\n> Q3: Do you have any intuition on why M3DDPG performs so poorly in the presence of an adversary compared to the standard MAPPO without any robustness mechanism in Figure 4?\n\nThere are two key reasons for M3DDPG\u2019s underperformance in adversarial contexts, as compared to standard MAPPO without robustness mechanisms.\n\nFirst, as discussed in Proposition 2.2, the existence of a pure-strategy equilibrium (RMPBE) isn't guaranteed. M3DDPG, which derives from MADDPG, inherently generates deterministic policies. Such policies may not be optimal in robust cooperative MARL contexts where a pure-strategy equilibrium is not assured. Despite this, M3DDPG was included as a baseline due to its recognition in the field.\n\nSecond, as a part of the max entropy RL family, MAPPO employs stochastic policies that encourage diverse solution strategies, thereby enhancing resilience to perturbations. This advantage of max entropy policies is well-documented in several studies [1, 2, 3, 4].\n\n[1] Ziebart, Brian D. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. Ph.D. Thesis, Carnegie Mellon University, 2010.\n\n[2] Haarnoja, Tuomas, et al. \"Reinforcement learning with deep energy-based policies.\" ICML 2017.\n\n[3] Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" ICML 2018.\n\n[4] Eysenbach, Benjamin, and Sergey Levine. \"Maximum entropy rl (provably) solves some robust rl problems.\" ICLR 2022.\n\n> Minors:\n\nThank you for your suggestions. We have meticulously revised the manuscript to correct grammatical errors and enhanced the clarity of Figure 2 in our revised submission."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185991197,
                "cdate": 1700185991197,
                "tmdate": 1700185991197,
                "mdate": 1700185991197,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DnsZExC2PB",
                "forum": "z6KS9D1dxt",
                "replyto": "sTB1o0eY5X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission872/Reviewer_UzHb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission872/Reviewer_UzHb"
                ],
                "content": {
                    "title": {
                        "value": "Follow-Up"
                    },
                    "comment": {
                        "value": "Thank you for the rebuttal and the revised version. Since I asked for a revision that affected the whole paper, I needed to completely re-read the new version to check.\n\nThe revision reads well to me now. Therefore, I am happy to increase my score since all my concerns have been sufficiently addressed."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544237097,
                "cdate": 1700544237097,
                "tmdate": 1700544237097,
                "mdate": 1700544237097,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rz0BidXag7",
            "forum": "z6KS9D1dxt",
            "replyto": "z6KS9D1dxt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission872/Reviewer_EssV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission872/Reviewer_EssV"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the robustness of multi-agent reinforcement learning against Byzantine failures. A Bayesian Adversarial Robust\nDec-POMDP (BARDec-POMDP) framework is proposed.  The theoretical formulation of the problem is one of the contributions of the\npaper and an actor-critic algorithm that produces convergence under some conditions.  Convergence is not ensured but experimentally\nthe approach shows great resilience against a spectrum of adversaries.\n\nThe paper makes some strong assumptions, like the fact that in each episode there is only one attacker. Other assumptions are consistency and sequential rationality, which are used to form an ex interim robust Markov perfect Bayesian equilibrium (RMPBE). With some additional assumptions, the existence of ex ante and ex interim RMPBE are guaranteed. For the ex interim equilibrium, the Q function can be written using the Bellman-type equation for the two Q functions, which the paper calls \"the robust Harsanyi-Bellman equation.\" With a few more assumptions, updating the value function via the Bayes rule guarantees convergence to the optimal value of Q.\n\nThe algorithm proposed is applied to different problems, a toy metrics game and some map problems. For the attacks, four types of threats are considered: non-oblivious adversaries, random agents, noisy observations, and transferred adversaries. The experimental results cover robustness over non-oblivious attacks and various types of attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The novelty of the paper is in the theoretical formulation of the problem and in the actor-critic algorithm that produces convergence\nunder some conditions.  Convergence is not ensured but experimentally the approach shows great resilience against a spectrum of adversaries."
                },
                "weaknesses": {
                    "value": "- The fact that convergence cannot be guaranteed despite the assumptions made is a weakness of the paper. The method proposed is complex, and the assumptions are strong, yet there is no guarantee of convergence\n- The proofs are all in the Appendix, which makes the paper quite long, too long for a conference paper, and that makes it appear incomplete.  Even the analysis of two of the environments used for the experiments is in the Appendix. Expecting the reviewers to read a paper of 36 pages is too much for a conference.  Without the Appendices the paper is incomplete.\n- The paper is hard to read, it assumes significant knowledge of the field and is full of acronyms and citations that break the flow of the text.\n\nAfter the discussion and the changes made to the paper, I reduced my criticism about convergence and increased the rating I gave for contribution. However, the presentation needs more work.  Many of the corrections made to the paper have grammar errors, like mismatches of singular/plural. I understand the rush of making the changes, but the writing is not where it should be.  I also still object to the length of the paper."
                },
                "questions": {
                    "value": "If you were to write a paper that fits in the page limits, would you include some of the material from the Appendices and drop some of the material currently in the paper or leave the paper unchanged?  I am wondering what you think are the most important parts needed to present the work without exceeding the page limits."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission872/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission872/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission872/Reviewer_EssV"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission872/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699329370678,
            "cdate": 1699329370678,
            "tmdate": 1700457812183,
            "mdate": 1700457812183,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kV2EzYS8hl",
                "forum": "z6KS9D1dxt",
                "replyto": "rz0BidXag7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission872/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission872/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response of Submission 872 to Reviewer EssV"
                    },
                    "comment": {
                        "value": "We thank Reviewer EssV for appreciating the theoretical formulation and empirical evaluation for our paper. A detailed response is provided below.\n\n> Q1: Convergence cannot be guaranteed.\n\nThank you for pointing out the concern. Firstly, please allow us to clarify that we have established convergence in our original draft. As stated on page 6, Section 3.2, \"Using common assumptions in stochastic approximation proposed by Borkar (Borkar, 1997; Borkar & Meyn, 2000; Borkar, 2009), we show that Theorem 3.1 converges almost surely to ex interim robust Markov perfect Bayesian equilibrium, with detailed assumptions and proof deferred to Appendix. A.8\". This proof leverages stochastic approximation theory and is underscored in both our abstract and introduction.\n\nSecondly, the term \"almost sure\" convergence implies that the sequence of defender and attacker policies converge with probability 1 as the number of iterations goes to infinity.\n\nThirdly, for greater clarity, we have added a bolded subtitle \"Convergence\" in Section 3.2 and restructured the relevant paragraph.\n\nFourthly, while our approach and methodology indeed rely on specific assumptions and mathematical techniques, the implementation of our algorithm is practical and straightforward. It involves conditioning the policy and value function on the belief of the current type, combined with a two-timescale update strategy (i.e., faster policy updates for the adversary and slower for the defender).\n\n> Q2: The paper is too long, and reference appendix for too many times. Concentrate on the most important parts.\n\nWe appreciate this valuable feedback. To address this, we have:\n\n* Removed unnecessary discussions, reducing references to the appendix from 17 times to 8 times and decreasing the appendix length from 21 pages to 12 pages. We have meticulously revised the manuscript to ensure it remains focused on the core message. For completeness, we will host a GitHub page for additional experiment results upon acceptance.\n\n* Included a concise proof sketch for each theorem to enhance self-containment.\n\n* Reduced the use of mathematical notations and added more textual explanations.\n\nPlease refer to our global response and the revised manuscript for more details.\n\n> Q3: The paper is hard to read, it assumes significant knowledge of the field and is full of acronyms and citations that break the flow of the text.\n\nThank you for this suggestion. In our revised version, we have carefully go through the whole paper and reduced the use of mathematical notations and added more textual explanations. However, it's important to note that certain expressions equations cannot be overly simplified for correctness."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185870898,
                "cdate": 1700185870898,
                "tmdate": 1700185870898,
                "mdate": 1700185870898,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9SgU2WaYSf",
                "forum": "z6KS9D1dxt",
                "replyto": "rz0BidXag7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission872/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission872/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-Up: Reviewer-Author Discussion Period Ends Tomorrow"
                    },
                    "comment": {
                        "value": "Dear Reviewer EssV,\n\nWe sincerely thank you for raising the score of our paper, the paper's quality has been greatly enhanced by your tremendous efforts. As the reviewer-author discussion period ends tomorrow, we hope that our recent updates and responses have adequately addressed your queries and concerns. Should you have any further questions, we are committed to promptly addressing them within the remaining timeframe.\n\nBest regards,\nAuthors of Submission 872"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580043119,
                "cdate": 1700580043119,
                "tmdate": 1700580043119,
                "mdate": 1700580043119,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8Z5SZbUvjd",
            "forum": "z6KS9D1dxt",
            "replyto": "z6KS9D1dxt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission872/Reviewer_FinH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission872/Reviewer_FinH"
            ],
            "content": {
                "summary": {
                    "value": "The research explores the stability of cooperative multi-agent reinforcement learning when faced with adversarial strategies and disruptions. It introduces the Bayesian Adversarial Robust Decentralized Partially Observable Markov Decision Process (BARDec-POMDP) as a framework for addressing these challenges. The study demonstrates that conventional robust cooperative MARL approaches only achieve an ex interim equilibrium and advances the concept of an ex post equilibrium for BARDec-POMDP, which it argues is superior under worst-case scenarios. To attain this ex post equilibrium, the study presents a novel actor-critic method named EPR-MAPPO. Testing on simple games, the LBF environment, and the StarCraft Multi-Agent Challenge (SMAC) illustrates that EPR-MAPPO outperforms standard models, showcasing resilience against four distinct threat categories."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The manuscript is easy to follow. Addressing the robustness of Multi-Agent Reinforcement Learning (MARL) is crucial, particularly for its practical deployment in real-life situations. The introduction of the ex post equilibrium within the BARDec-POMDP framework by the authors offers a thoughtful contribution to the field. The paper includes an in-depth examination of the algorithms it introduces. Furthermore, the empirical findings presented are coherent and compelling."
                },
                "weaknesses": {
                    "value": "The document frequently refers to its appendix, suggesting that it does not stand alone as effectively as it could. To enhance the feeling of a complete and self-contained work, the authors should further take simplification, such as using simpler terms within the equations for greater clarity."
                },
                "questions": {
                    "value": "How can the proposed method benefit real-world applications? Any concrete examples?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Non"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission872/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699378313670,
            "cdate": 1699378313670,
            "tmdate": 1699636013565,
            "mdate": 1699636013565,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XX2Mcla46i",
                "forum": "z6KS9D1dxt",
                "replyto": "8Z5SZbUvjd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission872/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission872/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response of Submission 872 to Reviewer FinH"
                    },
                    "comment": {
                        "value": "We thank Reviewer FinH for recognizing the significance, clarity, theoretical contribution and empirical findings of our paper. A detailed response is provided below.\n\n> Q1: The document frequently refers to its appendix, suggesting that it does not stand alone as effectively as it could. The authors should further take simplification, such as using simpler terms within the equations for greater clarity.\n\nWe appreciate this valuable feedback. To address this, we have:\n\n* Removed unnecessary discussions, reducing references to the appendix from 17 times to 8 times and decreasing the appendix length from 21 pages to 12 pages. We have meticulously revised the manuscript to ensure it remains focused on the core message. For completeness, we will host a GitHub page for additional experiment results upon acceptance.\n\n* Included a concise proof sketch for each theorem to enhance self-containment.\n\n* Reduced the use of mathematical notations and added more textual explanations.\n\nPlease refer to our global response and the revised manuscript for more details.\n\n> Q2: How can the proposed method benefit real-world applications? Any concrete examples?\n\nLet's illustrate this by examples. In robot swarm control [1, 2], individual robots may behave unpredictably due to malfunctions or external interference [3, 4]. Our approach studies the lower bound for these scenarios where a swarm faces a worst-case, non-oblivious adversary, as discussed in our introduction.\n\nApart from robot swarms, in traffic light control [5], individual lights might malfunction or be compromised, leading to a non-cooperative system. Similarly, in power grid control [6], nodes might fail or transmit erroneous signals. Our cooperative MARL approach is designed to adaptively respond to these challenges, ensuring efficient operation of these systems during deployment. We plan to apply our algorithm in these domains as future work, as discussed in our conclusion section.\n\n[1] H\u00fcttenrauch, Maximilian, Sosic Adrian, and Gerhard Neumann. \"Deep reinforcement learning for swarm systems.\" JMLR 2019.\n\n[2] Batra, Sumeet, et al. \"Decentralized control of quadrotor swarms with end-to-end deep reinforcement learning.\" CoRL 2022.\n\n[3] Giray, Sait Murat. \"Anatomy of unmanned aerial vehicle hijacking with signal spoofing.\" RAST 2013.\n\n[4] Ly, Bora, and Romny Ly. \"Cybersecurity in unmanned aerial vehicles (UAVs).\" Journal of Cyber Security Technology 2021.\n\n[5] Chu, Tianshu, et al. \"Multi-agent deep reinforcement learning for large-scale traffic signal control.\" IEEE TITS 2019.\n\n[6] Xi, Lei, et al. \"Smart generation control based on multi-agent reinforcement learning with the idea of the time tunnel.\" Energy 2018."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185808707,
                "cdate": 1700185808707,
                "tmdate": 1700185808707,
                "mdate": 1700185808707,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Sxh4sD9iVo",
                "forum": "z6KS9D1dxt",
                "replyto": "lTUgswkecb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission872/Reviewer_FinH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission872/Reviewer_FinH"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the efforts to revise the paper and answering my questions. It looks better now. However, I agree with Reviewer Essa that the paper still needs more works on its presentation. Therefore, I would like to maintain the current rating."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission872/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670026124,
                "cdate": 1700670026124,
                "tmdate": 1700670026124,
                "mdate": 1700670026124,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]