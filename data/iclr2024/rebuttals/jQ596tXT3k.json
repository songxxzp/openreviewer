[
    {
        "title": "Explaining the Out-of-Distribution Detection Paradox through Likelihood Peaks"
    },
    {
        "review": {
            "id": "aYLEsy1R73",
            "forum": "jQ596tXT3k",
            "replyto": "jQ596tXT3k",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8543/Reviewer_i5V7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8543/Reviewer_i5V7"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new method for out-of-distribution detection using a normalizing flow. The method is motivated by the Local Intrinsic Dimension estimation using approximate Likelihood (LIDL) and at a high level rejects a datapoint as OOD if either its density or local intrinsic dimension is low. The paper presents an approximation to LIDL that allows it to be applicable given just a single normalizing flow. The authors choose hyperparameters for their approach by considering SVHN vs. CIFAR and the reverse as validation tasks and show good OOD detection performance across eight different image dataset pairs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Originality: The paper is the first to my knowledge to use an estimate of local intrinsic dimension to detection OOD datapoints. The authors come up with an approximation to a recent estimation technique for LID that allows it to be utilized with a single normalizing flow. \n2. Quality: The authors present great visualizations and and good supplemental analyses to the main results.\n3. Clarity: The method and experiments are clearly written.\n4. Significance: The authors address an interesting problem and propose a new angle."
                },
                "weaknesses": {
                    "value": "Soundness:\n1. The approximation of $\\log \\rho_r(\\mathbf{x})$ comes about by using a first-order Taylor approximation of the invertible function $f_\\theta$ defining the flow. It's not clear to me how this upstream approximation affects the downstream approximation of the actual quantity of interest, $\\rho_r(\\mathbf{x})$. In addition, the fact that $\\hat{\\rho}_r(\\mathbf{x})$ is shown to be a poor approximation of $\\rho_r(\\mathbf{x})$ in \"Identifying Sharp Likelihood Peaks\" (in that the order for ID and OOD data is the opposite of what is intended) makes it a bit tenuous why it's ok for the $\\hat{\\rho}_r(\\mathbf{x})$ to be used to approximate $\\rho_r(\\mathbf{x})$ in $\\frac{\\partial}{\\partial r}\\log \\rho_r(\\mathbf{x})$.\n2. The proposed method is sensitive to choice of $r$, yielding almost no separation between datasets under some values of $r$. The authors instead set hyperparameters using two of the dataset pairs, CIFAR-10 vs. SVHN and the reverse, yet these two benchmarks are also two of the eight benchmarks presented in the main results, and at least one of the datasets is in four of the eight benchmarks. \n\nPresentation:\n1. It would benefit the authors to be a bit more careful in their discussion about manifolds and sub-manifolds. Namely, it is a bit strange to hypothesize that OOD data lies on a lower-dimensional submanifold than ID data and call these \"lower probability mass\" regions, since a submanifold constitutes a measure-zero set with respect to the Lebesgue measure of the ambient dimension of the data. At one point the authors use the phrase \"concentrate around a lower-dimensional submanifold\" which might be better, but it would be helpful to formalize what the authors mean so that the hypothesis makes mathematical sense.\n2. The first contribution claimed by the paper (to explain how DGMs can assign higher likelihood to OOD datapoints yet only generate in-distribution samples) has already been offered by previous work. See section 5.1 in [1], which offers the same explanation as the one given in this current submission, i.e. \"OOD datapoints can be assigned higher likelihood while not being generated if they belong to regions of low probability mass.\" Moreover, in the related work, the authors repeat arguments made in [1] as if they were their own, e.g. \"...we challenge this assumption...\"\n3. In Sec 4.1, the authors start by considering a uniform distribution over the volume of a given R-radius ball but then switch mid-paragraph to considering a uniform distribution on just the surface of said ball. It would help to explicitly mention the phenomenon that in high dimensions nearly all the volume is at the surface to justify the jump.\n\n[1] Zhang et al. 2021. Understanding Failures in Out-of-Distribution Detection with Deep Generative Models. ICML 2021."
                },
                "questions": {
                    "value": "1. Do the authors have any analysis to suggest that $\\hat{\\rho}_r(\\mathbf{x})$ should be a good approximation of $\\rho_r(\\mathbf{x})$?\n2. Can the authors provide evidence that this method works better than, say, an arbitrary statistic of the generative model that is validated to perform well on both SVHN vs. CIFAR-10 and the reverse? How about that the proposed method is robust to the validation strategy (e.g. alternative dataset pairs or different strategy altogether)?\n\nI think the paper has the potential to yield an interesting contribution if at least one of the above questions is addressed. In such a scenario, provided the above weaknesses are also addressed, I would be happy to raise my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8543/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8543/Reviewer_i5V7",
                        "ICLR.cc/2024/Conference/Submission8543/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8543/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698639839830,
            "cdate": 1698639839830,
            "tmdate": 1700616963907,
            "mdate": 1700616963907,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gfXJ05lUKh",
                "forum": "jQ596tXT3k",
                "replyto": "aYLEsy1R73",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8543/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8543/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer i5V7"
                    },
                    "comment": {
                        "value": "Thank you for the insightful review and praise for our ideas and presentation. We address your concerns point-by-point below:\n\nSoundness\n1. You expressed some concerns about the validity of our Taylor approximation of $\\log \\rho_r(\\mathbf{x})$. Part of this was based on how $\\hat{\\rho}_r$ ranks in-distribution and OOD data in Figure 3 in the manuscript. In fact, Figure 3 shows them behaving precisely as would be expected. For very negative $r$, the OOD data has higher mass because it is assigned higher likelihood under the model. For values closer to zero (here above $r \\approx -7$), the Taylor approximation loses its accuracy since the mass is estimated over too large of a region. The slopes of the curves are also appropriate, as the more negative slope for OOD data indicates it has lower LID. To further alleviate these concerns, we are conducting experiments to verify the suitability of our LID estimator on benchmark datasets, and we will include them before the end of the discussion period.\n2. You shared a concern about how we set $r$ with reviewer **urvk**; please find our response to this in the general rebuttal. \n\nPresentation\n1. You asked us to be more precise in the work about manifolds, submanifolds, and probability mass. Thank you for the suggestion; we will update the language around manifolds in the manuscript accordingly. To summarize,\n   - The in-distribution data sits on a submanifold (of zero Lebesgue measure).\n   - The likelihood-based DGM, which represents a full-dimensional density, concentrates mass around the (complex) in-distribution submanifold in such a way as to encode the submanifold\u2019s dimensionality [A].\n   - Due to its inductive biases, the DGM density *also* inadvertently concentrates a negligible amount of mass around out-of-distribution submanifolds when these are \u201csimpler\u201d (i.e. of lower intrinsic dimension).\n  \n    Since there is a distinction between the probability mass of the DGM and the underlying (measure zero) submanifolds being modelled, there is no underlying error in the way we use these concepts, though we certainly take your point that they can be made clearer.\n2. We thank you for pointing this out. You are correct that the notion that the OOD paradox must be due to lower probability mass was not originally proposed by us, although we do highlight that the paper you point out does not propose a way to empirically verify this. We will reword the manuscript to make this clear. Our main contributions are linking this explanation to intrinsic dimension, and exploiting this newfound connection to build a highly-performant OOD detection algorithm.\n3. You suggested a rewording to make the jump from a uniform distribution on a high-dimensional ball to a Gaussian a bit clearer. Thank you for this; we will clarify this paragraph in the manuscript.\n\nYou also had two additional questions:\n\n1. Thank you for bringing this point up. We are currently conducting a set of experiments where the ground truth values of $\\rho_r$ and $\\frac{\\partial}{\\partial r} \\rho_r$ are either known or can be accurately computed. We will include these results in the manuscript before the end of the discussion period.\n2. You also asked us about whether choosing hyperparameters for $r$ based on SVHN-CIFAR-10 does better than choosing another arbitrary statistic from a generative model, and whether our method is robust to this choice. In the general rebuttal we propose a new intuitive way to select $r$ without hyperparameter tuning; we hope this resolves your concerns on this front. Otherwise, could you please clarify what kind of arbitrary statistic you have in mind? We do point out that the \u201ccomplexity correction\u201d and \u201clikelihood ratios\u201d baselines we compare against could be loosely interpreted as leveraging arbitrary statistics, namely the compression length of images and the likelihood of another model, respectively.\n\n[A] Tempczyk et al., \u201cLIDL: Local Intrinsic Dimension Estimation Using Approximate Likelihood\u201d, ICML 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700264488339,
                "cdate": 1700264488339,
                "tmdate": 1700597651807,
                "mdate": 1700597651807,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6G6hkDPvwg",
                "forum": "jQ596tXT3k",
                "replyto": "gfXJ05lUKh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8543/Reviewer_i5V7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8543/Reviewer_i5V7"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the reply"
                    },
                    "comment": {
                        "value": "Thank you for the response. For the points related to soundness, I appreciate that the authors have added an error bound for the estimator $\\hat{\\rho}$ and updated the hyperparameter selection of $r$. One follow-up question to the latter: how is $k$ selected for LPCA? A quick note on the former: it looks like there's a small typo in the Lemma 1.1 proof, that the domain of integration hasn't been updated with the change of variables.\n\nOn presentation, I appreciate the author's responses and look forward to the updated main paper; should I expect this by the end of the discussion period? \n\nOn the questions, thank you for addressing the first with the synthetic experiment and error bound. The second is related to the concerns I had about the original selection of $r$ using the CIFAR-10 vs. SVHN and SVHN vs. CIFAR-10 OOD detection tasks, which could lead to the scenario of fitting the method to the detection tasks themselves. With the updated selection of $r$, I am less concerned about this issue, and with evidence that the estimator $\\hat{\\rho}$ can be a good approximation of $\\rho$, I view the updated experimental results more favorably as support for the paper's main hypothesis. As such, I have raised my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616951246,
                "cdate": 1700616951246,
                "tmdate": 1700616951246,
                "mdate": 1700616951246,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jkRTmTbIGQ",
                "forum": "jQ596tXT3k",
                "replyto": "RFup2tPSA4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8543/Reviewer_i5V7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8543/Reviewer_i5V7"
                ],
                "content": {
                    "title": {
                        "value": "Re: Thank you!"
                    },
                    "comment": {
                        "value": "Thanks for the reply. Re: point 2, you're right; sorry, I mistakenly read the domain of integration of the two lines as the same expression and did indeed miss the dropped $\\sigma$."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704148404,
                "cdate": 1700704148404,
                "tmdate": 1700704148404,
                "mdate": 1700704148404,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SMnaDwuTmr",
            "forum": "jQ596tXT3k",
            "replyto": "jQ596tXT3k",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8543/Reviewer_c1to"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8543/Reviewer_c1to"
            ],
            "content": {
                "summary": {
                    "value": "This paper tried to explain the paradox \"OOD samples are never generated by these DGMs despite having\nhigh likelihoods\" and propose using the probability mass that the model assigns to a small neighborhood of a data sample as the OOD detection method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper analyzes the availability and efficiency of using LID estimators for OOD detection and proposes to use density-based methods instead of the previous data-based ones. Specifically, it is inspired by a previous observation of adding Gaussian noise to corrupt the density, which is easy to understand but low-efficiency in computing as multiple NFs are needed, the authors propose a much more efficient method that only needs one pre-trained NF."
                },
                "weaknesses": {
                    "value": "**W1:** The claim **\"This observation becomes even more puzzling in light of the fact that said DGMs generate\nonly in-distribution samples...\"**, which is also the major motivation of this work, could be wrong. Though existing research always takes a different dataset like SVHN as an OOD dataset to quantitatively evaluate the model's OOD detection performance when trained on IID dataset like CIFAR-10, we should not only focus on these real-world datasets. For example, when drawing multiple generations out of a deep generative model, some of the generation samples could be very different and low-quality compared to the IID training set, where these samples should also be seen as OOD samples. When considering these OOD samples, the claim could be wrong. Overall, if the generative model cannot exactly model $p(x_{IID})$, there is no guarantee that a deep generative model would never generate OOD samples, otherwise there would be no such \"puzzling behavior\".\n\n**W2:** There are already some theoretical explanations for the flow-based generative model's over-confidence in OOD samples, such as the location and variances of the data and the model curvature [1]. Thus, what is the difference between their interpretation and the explanation in this paper? \n\n[1] Do Deep Generative Models Know What They Don't Know? ICLR 2019.\n\n**W3:** The proposed method highly depends on a strong assumption that the probability mass around an OOD sample is negligible. However, the authors seem not to provide sufficient support for this assumption, which may weaken the convincing of the proposed methods. And the experiments can also not support this assumption well, as the tested datasets are limited. Actually, not all OOD datasets would be assigned higher likelihoods by flow models [2], like detecting random noise images as OOD, what would happen when applying this method to these datasets? Besides, these OOD datasets especially random noise may not be \"sharply peaked\" as they are distributed widely in the input space.\n\n[2] Input Complexity and Out-of-distribution Detection with Likelihood-based Generative Models. ICLR 2020.\n\n**W4:** \"The explanation for the OOD paradox\" is a little bit weak and empirical."
                },
                "questions": {
                    "value": "Here are some minor questions.\n\n**Q1:** As claimed \"While our main ideas are widely applicable to likelihood-based DGMs\" in Section 4, I am wondering how this could be applicable to DGMS like diffusion models or VAEs since these two kinds of DGMs are optimizing a lower bound of the marginal data likelihood."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8543/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8543/Reviewer_c1to",
                        "ICLR.cc/2024/Conference/Submission8543/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8543/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698747209816,
            "cdate": 1698747209816,
            "tmdate": 1700733613722,
            "mdate": 1700733613722,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "neXKhbEqpH",
                "forum": "jQ596tXT3k",
                "replyto": "SMnaDwuTmr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8543/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8543/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer c1to"
                    },
                    "comment": {
                        "value": "We thank you for your time, effort, and feedback, and we appreciate the detailed review. Here we discuss the concerns you listed. You also asked about the applicability of this method to VAEs and diffusion models; for this, please see the general rebuttal.\n\nBefore addressing your concerns, we think it relevant to clarify at a high level some of the overarching themes and methodology in our work as follows.\n\nThe core paradox of the study is that, while a trained DGM generates samples that look much more like its training data (e.g., CIFAR 10), it can assign higher likelihoods to data that is very clearly outside its training data distribution (eg. SVHN). Our work shares its motivation with the rich body of literature initiated by [A] and [B] which we contour in our Background and Related Work sections. Though there may exist subtle differences between generated samples and training data, the much starker dissimilitude between in-distribution/OOD pairs such as CIFAR-10/SVHN make the aforementioned paradox remarkably surprising. \n\nThe notion that in-distribution points must belong to regions of higher probability mass than OOD points is broadly accepted in the literature (e.g. [A] section 2.1 and [C]) and follows naturally from first principles: if a CIFAR10 model never generates SVHN data, it must assign negligible probability to OOD regions (where OOD here is defined as SVHN). As a result, we respectfully disagree that it is a \u201cstrong assumption\u201d without \u201csufficient support\u201d.\n\nBut we do not take this notion as an assumption; instead, we take it as a hypothesis (we will clarify this in the manuscript). Recall the following pair of facts outlined in our Methods section:\n- If a sample has low likelihood, it is in a region of low probability mass.\n- If a sample has high likelihood and low intrinsic dimension (i.e. the density is highly peaked), then it is also in a region of low probability mass. \n\nIn our experiments (e.g. Figure 4), we see that OOD data tends to have low probability mass according to one of the above criteria, which verifies the hypothesis. We use this information to develop an OOD detection method, which as reviewer **urvk** noted, produced significant improvements over competing approaches.\n\nHaving clarified our perspective, here we respond to your individual concerns:\n\n\n- (**W1**): As mentioned above, we emphasize that the stark dissimilarity between OOD and in-distribution data does make the behaviour extremely puzzling, even when models are imperfect, as evidenced by the large existing literature on the topic. We also point out that all models are imperfect (some more so than others of course), and so OOD detection with imperfect likelihood models remains a relevant problem. \n\n    We agree with you that there is \u201cno guarantee that a deep generative model would never generate OOD samples\u201d, and indeed since the model assigns high likelihoods to OOD data there must be some (vanishingly small) probability of sampling them. However, for all practical purposes, the well-trained models used in our examples do not generate OOD data.\n\n    Finally, we also point out that if we redefine in-distribution samples as generated samples from the (imperfect) model, OOD samples *still* achieve higher likelihoods. See for example Figure 2 (we note that this is a novel observation of ours). Because in this hypothetical situation our model is equal to $p(x_\\text{IID})$, these results show that \u201cpuzzling behaviour\u201d persists regardless of model fit.\n\n- (**W2**): Indeed several explanations have been proposed in the literature, as we summarized in the Background and Related Work sections, including [A]. In particular, the one you bring up [A] has strong assumptions, namely that the in- and out-of-distribution densities have overlapping support and that they are both Gaussian. When, as in our case, in- and out-of-distributions are semantically different, the support overlap assumption is unrealistic [C]. Our explanation and resulting method do not require either of these assumptions (note our local Taylor approximation results in a local Gaussian approximation, not a global one)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700264188936,
                "cdate": 1700264188936,
                "tmdate": 1700264188936,
                "mdate": 1700264188936,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "j0qy6wI0Gd",
                "forum": "jQ596tXT3k",
                "replyto": "SMnaDwuTmr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8543/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8543/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewr c1to (cont'd)"
                    },
                    "comment": {
                        "value": "- (**W3**): Please see our discussion above as to why OOD samples being assigned negligible probability mass is indeed well established. We also point out that we do **not** claim OOD data is **always** assigned higher density, nor that it **always** has lower intrinsic dimension. In our work we often discus non-pathological cases like training on SVHN and evaluating on CIFAR10, where CIFAR10 data is not assigned higher likelihoods (e.g. Figure 14 for this pair). This is completely consistent with previous literature, which has found OOD data is only assigned higher likelihoods when OOD data is \u201csimpler\u201d than in-distribution data, which we quantify through intrinsic dimension. You also ask what would happen if we used our method in non-pathological settings. First, note that Table 1 contains several such examples, e.g. models trained on MNIST and tested on FMNIST, or trained on SVHN and tested on CIFAR10, are not subject to the standard paradox (and the intrinsic dimension of OOD data is not lower in this case), yet our method nonetheless succeeds at OOD detection. If random noise was used as OOD data instead, despite not having a low-intrinsic dimension, it would achieve a lower likelihood and still be detected by the other threshold in our dual threshold method (Algorithm 1). We will include such a result by the end of the discussion period.\n- (**W4**): We respectfully disagree with this characterization of our work. We hope that all the points we made above elucidate why, and that you will consider reevaluating our work at the end of the discussion period once we have made the promised manuscript updates. Otherwise, please let us know aspects you believe are \u201cweak\u201d or \u201cempirical\u201d and we will engage in discussion around them.\n\n[A] Nalisnick et al., \u201cDo Deep Generative Models Know What They Don\u2019t Know?\u201d, ICLR 2019.\n\n[B] Choi et al., \u201cWAIC, but Why? Generative ensembles for robust anomaly detection\u201d, 2018.\n\n[C] Zhang et al., \u201cUnderstanding Failures of Out-of-Distribution Detection with Deep Generative Models\u201d, ICML 2021."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700264242967,
                "cdate": 1700264242967,
                "tmdate": 1700264623903,
                "mdate": 1700264623903,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lYQzDlhF3N",
                "forum": "jQ596tXT3k",
                "replyto": "SMnaDwuTmr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8543/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8543/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Experiments with Random Noise Images as OOD"
                    },
                    "comment": {
                        "value": "We have added an extra set of experiments in response to your question in (W3) about whether our algorithm is capable of detecting random noise as OOD data because it would have high LID. In these experiments, we consider a model trained on one of the datasets in the paper and then feed it random images; each pixel intensity is chosen uniformly at random from the values 0 to 255. As you pointed out, the densities for these random images would not be sharply peaked, so they would have no low-dimensional structure and hence would have high LID.\n\nWe validated our method for all datasets, and in every case the OOD datapoints are completely separated from in-distribution points by the likelihood threshold. Hence, in every case the AUC-ROC is a perfect 1.00:\n\n| In-distribution                | Out-of-distribution      | AUC - ROC |\n|------------------------|-----------------|------------------|\n| FMNIST-gen             | Random Noise    | 1.000            |\n| FMNIST                | Random Noise    | 1.000            |\n| MNIST-gen              | Random Noise    | 1.000            |\n| MNIST                 | Random Noise    | 1.000            |\n| EMNIST-gen             | Random Noise    | 1.000            |\n| EMNIST                | Random Noise    | 1.000            |\n| Omniglot-gen           | Random Noise    | 1.000            |\n| Omniglot              | Random Noise    | 1.000            |\n| CIFAR10-gen            | Random Noise    | 1.000            |\n| CIFAR10               | Random Noise    | 1.000            |\n| CelebA-gen             | Random Noise    | 1.000            |\n| CelebA                | Random Noise    | 1.000            |\n| TinyImageNet-gen       | Random Noise    | 1.000            |\n| TinyImageNet          | Random Noise    | 1.000            |\n| SVHN-gen               | Random Noise    | 1.000            |\n| SVHN                 | Random Noise    | 1.000            |\n| CIFAR100-gen           | Random Noise    | 1.000            |\n| CIFAR100              | Random Noise    | 1.000            |\n\nFor a visualization of why our method is perfectly effective here, please see Figure 1 of the newly-uploaded supplementary material, which depicts estimated LID values and likelihoods for four of our models. As in the paper, each point represents a datapoint with its colour indicating whether it is in-distribution, generated from the model, or OOD. In red, we have visualized both the LID and likelihood thresholds chosen by our dual-threshold OOD detector. As illustrated, the likelihood threshold easily divides in-distribution from out-of-distribution. This is a \u201cnon-pathological\u201d case, where the OOD datapoints lie on a flat region of the density obtaining high LID estimates while simultaneously obtaining extremely low likelihoods. The density does not peak around random noise."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700435941677,
                "cdate": 1700435941677,
                "tmdate": 1700439380401,
                "mdate": 1700439380401,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Uidj8Za5l5",
                "forum": "jQ596tXT3k",
                "replyto": "lYQzDlhF3N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8543/Reviewer_c1to"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8543/Reviewer_c1to"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your huge efforts in addressing my concerns"
                    },
                    "comment": {
                        "value": "Thanks for your comprehensive response to my concerns!\nAnd I appreciate the tremendous efforts in revising the paper and adding experiments to support the ideas.\n\nHowever, there are still some big concerns of mine that need further discussion.\n\n----\n\n**For W1, sorry for my previous comment that may cause you to misunderstand my point** about \"DGMs generate only in-distribution samples\".\nI do not mean that OOD samples like SVHN would be generated if you train a generative model on CIFAR-10. Actually, I agree that it is even impossible to generate data similar to SVHN though it is assigned a higher likelihood.\nWhat I really care is the \"samples could be very different and low-quality compared to the IID training set\", an example of this can be seen in Figure 10 of your Appendix.\n\nTake Figure 10 (b) for example, should these generated samples be seen as \"in-distribution\"? If so, as you claim \"in- and out-of-distributions are semantically different\", what is their semantic information like category? If not, you claim \"the fact that said DGMs generate only in-distribution samples\" is too strong. You need to be very careful to modify this claim, otherwise, it will be abused by the following papers after acceptance.\n\n---\n\n**Aspects I believe are \u201cweak\u201d or \u201cempirical\u201d:** I may missed something. Please allow me to check one thing, is Figure 1 (b) a real experimental result or just an illustration? If so, where are the experimental details? If not, I think that is why I think \"empirical\". And you may need to support your claim on such a toy example. Here is an example you may consider, see Figure 2 of this paper [1].\nActually, when we use a generative model to estimate the data likelihood in the high-dimension input space, where the data is actually supported on a lower dimension, the learned likelihood may be very strange to the true data likelihood, indicating that the hypothesis about \"peak\" maybe not the truth.\n\n[1] Loaiza-Ganem, Gabriel, et al. \"Diagnosing and fixing manifold overfitting in deep generative models.\" arXiv preprint arXiv:2204.07172 (2022).\n\n---\n\nOverall, I appreciate the authors' effort in rebuttal and I would raise my score if the above two concerns could be well addressed."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677010751,
                "cdate": 1700677010751,
                "tmdate": 1700677010751,
                "mdate": 1700677010751,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BqjEqOJckH",
                "forum": "jQ596tXT3k",
                "replyto": "Z89cT8RyUQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8543/Reviewer_c1to"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8543/Reviewer_c1to"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for addressing my concerns!"
                    },
                    "comment": {
                        "value": "**For Q1:** \n>\"We point out that Figure 10 (b) depicts samples from Glow, which we do not use in our experiments, but as mentioned above, we take your point that samples from an underfit model could be considered OOD in and of themselves.\"\n\nIt seems that we achieved an agreement that the images in Figure 10 (b) should be considered OOD data as they have no obvious semantic information compared to the training set. with this agreement, there should be some weaknesses of this paper, firstly, you should change the overclaim \"the fact\" as you promised \"generating samples which are much closer to the former\"; secondly, the images in Figure 10 (b) seems not to lie in some OOD \"peak\", where your developed method may be seen as a specially designed OOD detector targeted on the \"peak\" OOD data, i.e., the typically used benchmark realistic image datasets.\nThe latter weakness exhibits a potential issue that, one can easily attack your OOD detector by an image like in Figure 10 (b).\nHowever, as the images in Figure 10 (b) have no semantic information (at least for a human it is hard to identify any semantic information), a good OOD detector focused on semantic information should successfully detect them.\n\n**For Q2:**\n> Figure 1 is just an illustration.\n\nGiven Figure 1 (left) is also an illustration, I do not know why you need to put \"1D\" (left) plus a \"2D\"(right) for illustration, as they are showing the same hypotheses. Actually, Figure 1(left) could be very misleading in that one may think the OOD \"peak\" phenomenon actually exists in a 1-D setting. As you also claim \"We do not believe the effect illustrated in Figure 1 can necessarily be shown in a toy example\",\nthere is highly possible no such OOD peak phenomenon in a low-dimensional. So just delete Figure 1(left) may be better in the next revision.\n\n> if the setting is too simple, the likelihoods are unlikely to exhibit pathological OOD peaks. One common observation throughout the literature is that pathologies of OOD detection are related to the relative complexity of in- and out-of-distribution data - the required complexities are hard to replicate in toy examples.\n\n This could also be added to the limitation of your method, as your method's effectiveness may be highly related to the relative complexity of in- and out-of-distribution data. As the complexity may be hard to explicitly measure though you may use some image compressor to measure it, it could be difficult to make a decision when adopting your method to achieve promising results, especially in real-world cases where we may encounter different complexity-level OOD samples.\n\nOverall, although there are some limitations, given the extensive analysis, the novelty of the method, and comprehensive experimental results, I believe the proposed method is a promising method to detect certain types of OOD data.\n\nThus, I will hold a \"borderline\" attitude to this paper that I will raise my score from 3 to 5 and lower my confidence to 1."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733575612,
                "cdate": 1700733575612,
                "tmdate": 1700733575612,
                "mdate": 1700733575612,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wvEDPdsS8B",
                "forum": "jQ596tXT3k",
                "replyto": "SMnaDwuTmr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8543/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8543/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "We are grateful for your insightful comments and your decision to adjust your score favourably. Your feedback is invaluable, and in light of your suggestions, we will take extra care to articulate our paper more clearly, ensuring that it accurately conveys our methodology and addresses potential ambiguities."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740678310,
                "cdate": 1700740678310,
                "tmdate": 1700740678310,
                "mdate": 1700740678310,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CjCPY8JcGY",
            "forum": "jQ596tXT3k",
            "replyto": "jQ596tXT3k",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8543/Reviewer_urvk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8543/Reviewer_urvk"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the well known issue of probabilistic generative model assigning higher likelihood to simple OOD data. It proposes a feasible way to estimate the probability mass around a datapoint, instead of just the value of the density function. Doing so allows filtering sample with high density but low probability mass around it (i.e., sharp density function), and hence can be a reliable OOD detector. Empirical results verify the effectiveness of this method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper tackles a well known and important issue of using DGM for OOD detection. Multiple methods has been proposed to correct the likelihood misalignment of DGM, but this paper tackles the problem in a principled way, by estimating the probability mass. Although it is intuitively clear that OOD samples with high density must have low probability mass, it is highly non-trivial to actually compute the mass. This paper shed a light on that.\n\n2. The idea of estimating the probability mass is interesting. It smartly uses a linearization to obtained tractable Gaussian distribution, which, when doing convolution with another Gaussian distribution, results in tractable form. This is a clever design, and it's a significant technical contribution.\n\n3. The experimental results are promising, showing significant improvements over competing methods."
                },
                "weaknesses": {
                    "value": "1. The current analysis is built exclusively on normalizing flows, while it does not have discussion of other models. In particular, for models like VAEs, where the exact likelihood is not available, is it still applicable?\n\n2. The way to determine the radius $r$ seems to be ad hoc. It depends on the dataset and the flow model as well. Is there a more principled way of choosing it? Can we just set r to be extremely small?"
                },
                "questions": {
                    "value": "My questions are stated above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8543/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792232238,
            "cdate": 1698792232238,
            "tmdate": 1699637068412,
            "mdate": 1699637068412,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qAOGVHUcrv",
                "forum": "jQ596tXT3k",
                "replyto": "CjCPY8JcGY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8543/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8543/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer urvk"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback on our ideas, methodology, and experimental results.\n\n1. You asked whether our analysis is generalizable to models outside of normalizing flows; please see the general rebuttal for more discussion on this point.\n2. You also asked a couple of questions about setting $r$.\n     - You asked about a more principled way of setting it; please see the general rebuttal, where we propose an improvement over our original approach.\n     - You also asked why we cannot simply set $r$ to a very small value; we agree this is a relevant question and we will further elaborate on it when updating the paper. An intuitive way to think about this is that the hyperparameter $r$ effectively chooses a scale at which we probe the density model, and that it determines how sensitive the estimate is to slight perturbations in the density. This is completely analogous to how LIDL [A] sets the various amounts of noise it uses to estimate LID, and to how commonly used estimators of global intrinsic dimension set the number of nearest neighbours as a hyperparameter [B]. Please refer to section 2.4 of [A] for a discussion on how they heuristically choose the scale variable according to how sensitive they require their estimator to be.\n\n[A] Tempczyk et al., \u201cLIDL: Local Intrinsic Dimension Estimation Using Approximate Likelihood\u201d, ICML 2022.\n\n[B] Levina and Bickel, \u201cMaximum Likelihood Estimation of Intrinsic Dimension\u201d, NeurIPS 2004."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263606571,
                "cdate": 1700263606571,
                "tmdate": 1700264356570,
                "mdate": 1700264356570,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U4Czk8gyTD",
                "forum": "jQ596tXT3k",
                "replyto": "qAOGVHUcrv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8543/Reviewer_urvk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8543/Reviewer_urvk"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for addressing my concerns"
                    },
                    "comment": {
                        "value": "My concerns on the generalization to other models as well as the way of choosing radius are addressed. However, I do think it would be better to conduct studies on at lease one more type of models (e.g., VAE), to really show the generalizability. I will maintain my score, as I think it is indeed a good contribution to OOD detection."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716317367,
                "cdate": 1700716317367,
                "tmdate": 1700716317367,
                "mdate": 1700716317367,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aeXkwY6Ix6",
                "forum": "jQ596tXT3k",
                "replyto": "CjCPY8JcGY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8543/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8543/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "We deeply appreciate your valuable feedback. Your suggestion to illustrate the generalizability of our approach using additional model types, such as diffusions or VAEs, is well-received."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8543/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740550001,
                "cdate": 1700740550001,
                "tmdate": 1700740770706,
                "mdate": 1700740770706,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]