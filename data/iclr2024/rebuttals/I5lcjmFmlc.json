[
    {
        "title": "Robust Classification via a Single Diffusion Model"
    },
    {
        "review": {
            "id": "H2AuadFKIh",
            "forum": "I5lcjmFmlc",
            "replyto": "I5lcjmFmlc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission42/Reviewer_noA3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission42/Reviewer_noA3"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a diffusion-based classifier that surpasses the SOTA level of robust accuracy against lp-bound adversarial attacks without relying on adversarial training. The method is advantageous to previous methods because it does not require inference for every class in the dataset by leveraging a \"multi-head diffusion\" block. Further, to improve the density estimation of real-world models the authors propose a \"Likelihood Maximization\" technique."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Clear and quality writing.\n2. The method is carefully designed with theoretical justifications. In particular, the authors go to great lengths to address gradient obfuscation.\n3. The method is benchmarked against modern adversarial attacks (AA, StAdv, BPDA) and beats SOTA by a large margin while showing generalization to more threats than previous methods.\n4. Thorough review of related work and comparison to previous methods."
                },
                "weaknesses": {
                    "value": "1. The manuscript mentions multiple times the expensive inference in both time and memory - however, a quantitative analysis is missing. I would like to see a table with inference time and memory for this method in comparison to other generative classifiers and regular ones.\n2. The method appears to work well on \"unseen\" threats. However, all threats are limited to adversarial attacks. Is there any evidence for an increased robustness to other robustness aspects such as common corruptions (e.g., CIFAR10-C)?\n3. The experimental evaluation is performed on a clearly separated and limited number of classes. Are there any results or theoretical insights on how this method would scale to more and potentially more fine-grained classes?"
                },
                "questions": {
                    "value": "1. In Appendix B the authors state \"However, this architecture only achieves 60% accuracy on the CIFAR10 dataset\". How does that relate to the 90-ish % in Tab. 1? I.e. what is different in this section?\n2. \u201c... [the mthod] ... leverages an off-the-shelf diffusion model\u201d - how is this possible when the last layer is modified? Which diffusion model is this exactly? How does it compare to the diffusion models previous works have used?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission42/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission42/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission42/Reviewer_noA3"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission42/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698701082923,
            "cdate": 1698701082923,
            "tmdate": 1699635928513,
            "mdate": 1699635928513,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "h49rp2rUUo",
                "forum": "I5lcjmFmlc",
                "replyto": "H2AuadFKIh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission42/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission42/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable review (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for the supportive review. We are encouraged by the appreciation of the clarity, soundness, and effectiveness of this work. We have uploaded a revision of our paper. Below we address the detailed comments, and hope that you may find our response satisfactory.\n\n\n\n**Question 1: Qualitative analysis of time complexity in terms of real-time cost.**\n\nThank you for your advice. We test the memory and real-time cost on a single 3090 GPU. The results are shown below:\n\n\n| Method     | NFEs | Real Time | Memory Cost (MB) | \n|:-----------|:----:|:---------:|:----------------:|\n| WRN-70-2   |  1   |   0.01    |       973        |\n| AT-DDPM    |  1   |   0.01    |       973        |\n| AT-EDM     |  1   |   0.01    |       973        |\n| JEM        |  1   |   0.01    |       1655       |\n| HybViT     |  1   |   0.01    |       2502       |\n| SBGC       | 30TK |   15.78   |       3557       |\n| DiffPure   | 101  |   0.60    |       3089       |\n| LM (ours)  |  T   |   0.10    |       3401       | \n| DC (ours)  |  TK  |   9.76    |       2664       | \n| RDC (ours) | N+TK |   9.86    |       3661       | \n| RDC (ours) | N+T  |   1.43    |       3661       | \n\n\nWe have also supplemented this analysis in the Table 3 in Appendix. As shown, our multi-head diffusion greatly improves the efficiency while maintains the accuracy and robustness.\n\n\n\n**Question 2: Experiments on CIFAR10-C.**\n\nThank you for your advice. The results on CIFAR10-C are shown below.\n\n\n|                   | WRN70-2  | AT-DDPM | AT-EDM | DiffPure| DC   |\n| :---------------- | :-------: | :-------------------: | :----------------: | :------: | :-------: |\n| Glass blur        | 41\\.2     | 52\\.2                 | **55\\.4**          | 54\\.0    | 36\\.4     |\n| Gaussian noise    | 45\\.8     | 69\\.6                 | **73\\.0**          | 72\\.6    | 54\\.0     |\n| Shot noise        | 65\\.4     | 87\\.0                 | **90\\.0**          | 89\\.0    | 69\\.0     |\n| Speckle noise     | 69\\.0     | 87\\.2                 | **90\\.8**          | 87\\.8    | 67\\.0     |\n| Impulse noise     | 50\\.4     | 75\\.0                 | **77\\.4**          | 81\\.4    | 33\\.4     |\n| Defocus blur      | 59\\.0     | 55\\.6                 | **59\\.6**          | 56\\.0    | 58\\.8     |\n| Gaussian blur     | 71\\.6     | 83\\.4                 | **89\\.0**          | 84\\.4    | 88\\.2     |\n| Motion blur       | 84\\.8     | 83\\.2                 | **88\\.0**          | 83\\.4    | 85\\.8     |\n| Zoom blur         | 81\\.8     | 81\\.8                 | **90\\.2**          | 84\\.2    | 88\\.6     |\n| Snow              | 91\\.0     | 85\\.4                 | **91\\.2**          | 87\\.8    | 86\\.2     |\n| Fog               | **61\\.6**     | 47\\.8                 | 51\\.6              | 47\\.0    | 58\\.4 |\n| Brightness        | **62\\.6**     | 57\\.2                 | 60\\.2              | 58\\.4    | 60\\.6 |\n| Contrast          | 57\\.9     | 37\\.2                 | 42\\.0              | 42\\.8    | **58\\.0** |\n| Elastic transform | 58\\.0     | 54\\.4                 | **59\\.2**          | 54\\.6    | 56\\.2     |\n| Pixelate          | 82\\.2     | 89\\.4                 | **92\\.2**          | 87\\.4    | 83\\.0     |\n| JPEG compression  | 83\\.8     | 88\\.8                 | **92\\.6**          | 88\\.0    | 87\\.2     |\n| Spatter           | 90\\.8     | 86\\.8                 | **91\\.6**          | 86\\.6    | 86\\.8     |\n| Saturate          | **95\\.0** | 86\\.8                 | 91\\.0              | 88\\.2    | 91\\.4     |\n| Frost             | 57\\.8     | 52\\.8                 | 57\\.8              | 56\\.6    | **58\\.2** |\n| Average           | 69\\.0     | 71\\.2                 | 75\\.9              | 73\\.1    | 69\\.4     |\n\n\n\nThe performance of our method does not exceed the baselines. We observe that our diffusion classifier assigns extremely low likelihood to out-of-distribution (OOD) samples. This may be because our diffusion classifier accurately captures the distribution of the training set but does not generalize well to data that is outside of this distribution. We intend to investigate this issue further and aim to enhance the OOD accuracy in future work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission42/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232212343,
                "cdate": 1700232212343,
                "tmdate": 1700390969065,
                "mdate": 1700390969065,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bHZ5r6BsA0",
                "forum": "I5lcjmFmlc",
                "replyto": "H2AuadFKIh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission42/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission42/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable review (2/2)"
                    },
                    "comment": {
                        "value": "**Question 3: Scalability to dataset with more fine-grained classes.**\n\n\nThank you for the suggestion. To verify the scalability of our method to fine-grained dataset, We test the robustness of different methods against $\\ell_\\infty$-norm threat model with $\\epsilon_\\infty=8/255$ on the CIFAR-100 dataset, following the same experimental settings as CIFAR-10. Due to the time limit, we only random sample 128 images for evaluation. The results are shown below.\n\n\n| Method | Clean Acc | Robust Acc|\n|---------|:-------:|:--------:|\n| WRN-40-2 |   78.13     | 0.00 |   \n| AT-DDPM |    63.56     |   34.64   |\n| AT-EDM|  75.22        |   42.67 |\n| DiffPure  |         39.06         |     7.81     |\n| DC  |    79.69    |  39.06|\n|RDC |      80.47       |  53.12|\n\n\n\n\nWe find that RDC still achieves superior result compared with the state-of-the-art adversarially trained models and DiffPure.\nMore surprisingly, we discover that DiffPure does not work well on CIFAR-100. We guess this is because CIFAR-100 has more fine-grained classes, and thus a small amount of noise will make the image lose its semantic information of a specific class. Hence, DiffPure is not suitable for datasets with more fine-grained classes and small resolution. This experiment indicates that our methods could be scaled to datasets with more fine-grained classes.\n\n\n\n\n\n\n**Question 4: Training of multi-head diffusion.**\n\nAs detailed in Appendix B.2, training a multi-head diffusion model directly results in an accuracy of 60\\%. To address this, we experimented with introducing negative examples and knowledge distillation. We discovered that initializing the model from an unconditional diffusion model and then refining it through distillation with a conditional diffusion model (as outlined in Algorithm 2) effectively resolves the problem.\n\n\n\n**Question 5: Clarification of usage of the off-the-shelf diffusion model.**\n\n\nWe apologize for the ambiguity. In this paper, we initially explore the robustness of a generative classifier using a single off-the-shelf model. Subsequently, we propose the multi-head diffusion, which requires training and is not an off-the-shelf model. We have revised this in the revision."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission42/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232245024,
                "cdate": 1700232245024,
                "tmdate": 1700232512560,
                "mdate": 1700232512560,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8LRvnFUCKZ",
                "forum": "I5lcjmFmlc",
                "replyto": "h49rp2rUUo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission42/Reviewer_noA3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission42/Reviewer_noA3"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your rebuttal.\n\n1. Could you clarify why there are 2 AT-DDPM columns? Is it $\\ell_\\infty/_2$?\n2. Could you clarify what you mean by [low-likelihood on]\u00a0\"OOD\" samples? Does that include adversarial samples or only CC samples?\n3. If your method assigns a low likelihood for OOD, it could be used as a detector. On ImageNet this could be tested on ImageNet-O [1]. This is out-of-scope for this paper and just a suggestion for future work. However, having a plot of the likelihood distribution for ID and OOD might enhance.\n\n[1] Hendrycks et al., \"Natural Adversarial Examples\", CVPR 2021."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission42/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700348267700,
                "cdate": 1700348267700,
                "tmdate": 1700348267700,
                "mdate": 1700348267700,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mijN7uvi18",
            "forum": "I5lcjmFmlc",
            "replyto": "I5lcjmFmlc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission42/Reviewer_ScPt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission42/Reviewer_ScPt"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents the Robust Diffusion Classifier (RDC), a generative classifier designed for robust classification tasks. RDC operates by first optimizing the data likelihood of an input and then estimating class probabilities for this optimized input. This is achieved using the diffusion model with transforms using the Bayes' rule. Recognizing the need for classification efficiency, the authors introduce a novel diffusion backbone termed \"multi-head diffusion\" and for a sampling strategy with fewer NFE. Notably, the method requires no specific training against particular adversarial attacks, showcasing its adaptability in defending against a spectrum of previously unseen threats."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The proposed methodology stands out as both technically sound and effective. Its capability to achieve robust classification without specific knowledge of adversarial attacks is admirable. The paper has both theoretical and empirical contributions, which are beneficial to both the theoretical-favored researchers and practitioners.\n\n- The paper is overall well-written and provides a smooth reading experience. The incorporation of model overviews and illustrative diagrams further clarifies the proposed methodology, enhancing comprehension.\n\n- The experimental results well validate the approach. Notably, the paper also provides an ablation study to interpret the effects of the hyperparameters. \n\n - The authors have provided a thorough study that motivates the design of the multi-head classifier in the appendix. This study provides additional insight for researchers to figure out problems in related domains.\n\n- I appreciate the authors' transparency in addressing potential limitations. The practical side of the research is solid. The authors have been very detailed in their implementation and provided their experiment code, which ensures reproducibility."
                },
                "weaknesses": {
                    "value": "Although the content of the current version is satisfactory, some points listed below can further enhance the depth and completeness of this work:\n\n- Some details regarding the diffusion model need clarification. For example, the sampling strategy is not very clear to me. It seems the authors deploy a VP sampling with uniform sliding timesteps similar to the one used in Nichol and Dhariwal, (2021). In the appendix and the code the authors also seem to leverage some implementation from Karras et al. (2022). As we note the sampler in Karras et al. (2022) involves additional correction steps, the NFE of the diffusion backbone would be more than T. \n\n- The theoretical results are based on the hypothesis that the evidence lower bound is tight. It would be intriguing to explore the implications of the gap between the likelihood and this lower bound, especially when viewed through the lens of the Bayesian framework for uncertainty quantification. Exploring how this gap influences robust classification performance could enrich the paper's depth and utility.\n\n\n- While the current experiments are limited to relatively small-scale datasets, there is inherent value in examining the method's scalability. It would be beneficial if the authors could present results or potential methodologies to apply their approach to larger datasets, drawing inspiration from other generative classifiers in the Bayesian paradigm, such as those highlighted by Heek and Nal (2019) and Han et al. (2022).\n\n\n- This paper also has close relation to other generative classifiers not specified for robust classification. Although the authors have discussed some concurrent works, some prior works may need to include and discuss potential connections. \n\n- The current form of the paper draws parallels to several generative classifiers, though not specifically designed for robust classification. While some works have been discussed as concurrent works, it might be helpful in enhancing the completeness to integrate and discuss other prior works, emphasizing their relevance and potential connections to the proposed methodology.\n\n----------\nReference:\n\nHeek, Jonathan, and Nal Kalchbrenner. \"Bayesian inference for large scale image classification.\" arXiv preprint arXiv:1908.03491 (2019).\n\nMackowiak, R., Ardizzone, L., Kothe, U., & Rother, C. (2021). Generative classifiers as a basis for trustworthy image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2971-2981).\n\nHoogeboom, E., Nielsen, D., Jaini, P., Forr\u00e9, P., & Welling, M. (2021). Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34, 12454-12465.\n\nHan, X., Zheng, H., & Zhou, M. (2022). Card: Classification and regression diffusion models. Advances in Neural Information Processing Systems, 35, 18100-18115."
                },
                "questions": {
                    "value": "Please see the first point of the weakness regarding the details in diffusion settings."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission42/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission42/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission42/Reviewer_ScPt"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission42/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698707241392,
            "cdate": 1698707241392,
            "tmdate": 1700638641885,
            "mdate": 1700638641885,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x6ry5U2mm9",
                "forum": "I5lcjmFmlc",
                "replyto": "mijN7uvi18",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission42/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission42/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable review"
                    },
                    "comment": {
                        "value": "Thank you for the supportive review. We are encouraged by the appreciation of the soundness, clarity, and thoroughness of this work.  We have uploaded a revision of our paper. Below we address the detailed comments, and hope that you may find our response satisfactory.\n\n**Question 1: Clarification of the diffusion classifier.**\n\nWe would like to clarify that the time complexity of our diffusion classifier is not related to the sampling complexity. Our diffusion classifier initially calculates the Evidence Lower Bound (ELBO) to approximate the log likelihood, and then computes the classification probability using $p(y|\\textbf{x}) \\propto p(\\textbf{x}|y)$. Therefore, the time complexity of our method is determined by the time required to calculate the ELBO. For each class, we need $T$ NFEs to calculate its ELBO, hence we need $K\\times T$ NFEs to calculate $p(y|\\textbf{x})$.\n\n**Question 2: The gap between the ELBO and log likelihood.**\n\nWe agree that exploring the gap between ELBO and log likelihood on the robustness of diffusion classifier is quite interesting. However, calculating the exact likelihood of  the diffusion model is extremely hard. Song et al. (2021) suggest using the instantaneous change of variable theorem (Chen et al., 2018) to compute the likelihood that a diffusion ODE generates certain images. However, this likelihood is not equivalent to the one in the evidence lower bound (ELBO), which actually represents the likelihood of the diffusion SDE generating such an image. These two likelihoods are only equal when the score function is optimal. In practice, we train a UNet to approximate the score of the real data distribution, which is certainly not optimal. Consequently, these two scores are not identical, and we have not found any method to calculate the exact likelihood of the diffusion SDE. We will strive to explore new ways to calculate the exact likelihood and the gap between ELBO and log likelihood in  future work.\n\n**Question 3: Potential solution to scale to large datasets.**\n\nThank you for the suggestion. In Appendix B.3, we have provided an experiment on Restricted ImageNet, which shows the effectiveness of our approach to larger datasets. During the rebuttal phase, as suggested by Reviewer Xkzq, we supplement an experiment on CIFAR-100. The results of different methods are shown below.\n\n| Method | Clean Acc | Robust Acc|\n|---------|:-------:|:--------:|\n| WRN-40-2 |78.13| 0.00 |   \n| AT-DDPM|63.56| 34.64|\n| AT-EDM  | 75.22 |42.67|\n| DiffPure | 39.06|7.81|\n| DC  |79.69|39.06|\n|RDC |80.47|53.12|\n\nOur proposed RDC achieves superior result compared with state-of-the-art adversarially trained models and DiffPure.\nThese two experiments imply the scalability of our method to larger datasets with more fine-grained classes.\n\n\nWe apologize for the confusion regarding the application of the methods proposed by Heek and Nal (2019) and Han et al. (2022) to enhance the scalability of our approach. Our method, being a generative classifier, utilizes the ELBO to approximate the log likelihood. We argue that the primary challenge in adapting our method to larger datasets is finding a more efficient way to compute the ELBO. The ATMC sampler proposed by Heek and Nal (2019) is designed to accelerate the training of Bayesian networks, while Han et al. (2022) focus on constructing a diffusion bridge for converting logits from discriminative classifiers into more accurate and calibrated probabilities. We reckon that these methods are quite different from our paradigm since they are discriminative approaches and hence their method could not be directly used to improve the scalability of our diffusion classifier.\n\n**Question 4,5: Lack of discussion of some popular generative classifiers.**\n\nThank you for your suggestion. We have incorporated a discussion of more popular generative classifiers (Mackowiak et al., 2021; Hoogeboom et al., 2021; Han et al., 2022) into Section 2 of the revision.\n\n***\n\n**References:**\n\nSong, Yang, et al. \"Score-based generative modeling through stochastic differential equations.\" International Conference on Learning Representation, 2021.\n\nChen, Ricky TQ, et al. \"Neural ordinary differential equations.\" Advances in Neural Information Processing Systems, 2018.\n\nHeek, Jonathan, and Nal Kalchbrenner. \"Bayesian inference for large scale image classification.\" arXiv preprint arXiv:1908.03491 (2019).\n\nMackowiak, R., et al. \"Generative classifiers as a basis for trustworthy image classification.\" IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\n\nHoogeboom, E., et al. \"Argmax flows and multinomial diffusion: Learning categorical distributions.\" Advances in Neural Information Processing Systems, 2021.\n\nHan, X., et al. \"Card: Classification and regression diffusion models.\" Advances in Neural Information Processing Systems, 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission42/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232017385,
                "cdate": 1700232017385,
                "tmdate": 1700232544052,
                "mdate": 1700232544052,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KbXDvPrvaL",
                "forum": "I5lcjmFmlc",
                "replyto": "x6ry5U2mm9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission42/Reviewer_ScPt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission42/Reviewer_ScPt"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "I appreciate the author's response in addressing my questions and concerns. After reading the revision and all reviews, my concerns are addressed. \n\nI am also interested in the observation that the robust accuracy does not saturate at $T=2000$, as mentioned by my co-reviewer and the authors. It would be good if the authors could add the results in the final manuscript and involve relative analysis in exploring the limit of robust accuracy in terms of T (if possible).  \n\nA minor point revision: \n\nThe real-time column in Table 3 may need a unit (s, min, or h). \n\nI believe the paper can be valuable to the community and thus increase my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission42/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638610751,
                "cdate": 1700638610751,
                "tmdate": 1700638610751,
                "mdate": 1700638610751,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "APg5VqtViB",
            "forum": "I5lcjmFmlc",
            "replyto": "I5lcjmFmlc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission42/Reviewer_Xkzq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission42/Reviewer_Xkzq"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to build a robust classifier using a single diffusion model by calculating $p (y|x)$ via $p (x|y)$. The authors identify likelihood maximization as a key ingredient for ensuring the adversarial robustness of such models. To address the high computational complexity associated with this type of classifiers, the authors further introduce a multi-head U-Net and ablate on efficient sampling methods. Experiment results on a subset of CIFAR-10 using BPDA-AutoAttack show that the proposed method achieves SOTA clean and adversarial accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper proposes an interesting and relevant framework for robust classification. Given how fast diffusion models are improving in comparison to traditional discriminative robust classifiers, this work opens a new method of building robust models. It thus has a lot of potential for inspiring future research that builds even better robust models. Furthermore, the authors are careful with evaluating the proposed method with strong adaptive attacks, providing justifications for the proposed robustness estimation methods."
                },
                "weaknesses": {
                    "value": "The main weaknesses are twofold: computational complexity and paper presentation.\n\n### Computational Complexity\n\nEven with the proposed multi-head U-Net and other complexity reduction measures, the computational complexity still seems to be high. While the likelihood maximization step only requires $N=5$ forward and backward passes, approximating $p(x|y)$ requires $T$ U-Net queries. That being said, I agree that this drawback can be left for future work.\n\n### Paper Presentation\n\nMany important details are missing from the discussion. Some of them can be found in the appendix, but they really should be in the main text. This is especially the case for Section 3.3.\n- Theorem 3.2 discusses \"optimal diffusion models\". In what sense is the diffusion model optimal? The proof to this theorem clarifies that such a model minimizes the noise estimation error, but this should be in the main text.\n- There is a softmax operation in Theorem 3.2, but the quantity on which it operates is a scalar (a norm square divided by some variance). What does softmax exactly mean here? Same for Corollary 3.3.\n- \"We find that the optimal diffusion classifier achieves 100% robust accuracy in both cases, validating our hypothesize that accurate density estimation of diffusion models facilitates robust classification.\" How was this found? Also, it should be \"hypothesis\", not \"hypothesize\". What is the main gap between an optimal diffusion classifier and an empirical diffusion classifier? Is it the limited amount of training data? Or is it how well the U-Net is optimized?\n- Section 3.5 says, \"instead of calculating the diffusion loss using all timesteps like Eq. (9), we only sample a single timestep\". How is this time step sampled? Uniformly randomly?\n- Section 3.5 also says, \"(BPDA) approximates the gradient with an identity mapping\". How exactly is the identity mapping applied? A pseudo-code or Python code explanation would be appreciated.\n- It would be nice to have the experiment results from the CIFAR-100 dataset to have some diversity in the evaluation."
                },
                "questions": {
                    "value": "- Since AutoAttack with BPDA is used for evaluation, is the Square Attack component of AutoAttack also included in the evaluation? Does Square find additional examples on top of the BPDA gradient-based attacks?\n- In Figure 2a, why is the robust accuracy barely over 30%? What is the value of $T$ and $T'$ for the main results (Table 1)? Does the result get even better if $T'$ is larger than 1000? How large is $T$ during the training of the diffusion model? If we train a diffusion model with fewer time steps (i.e., discretize the trajectory into less than 1000 steps during training), should we expect the resulting classifier obtained via the proposed method to work well with a smaller $T'$?\n- With multi-head diffusion, is it true that only the last layer receives the class condition signal? How does this affect the performance compared with injecting class conditioning into various locations in the U-Net? It would also be nice to see some generations from this diffusion model."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission42/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission42/Reviewer_Xkzq",
                        "ICLR.cc/2024/Conference/Submission42/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission42/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728120270,
            "cdate": 1698728120270,
            "tmdate": 1700352725578,
            "mdate": 1700352725578,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CVSoUaabwz",
                "forum": "I5lcjmFmlc",
                "replyto": "APg5VqtViB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission42/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission42/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable review  (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for appreciating our new contributions as well as providing the valuable feedback. We have uploaded a revision of our paper. Below we address the detailed comments, and hope that you may find our response satisfactory.\n\n\n\n***Question 1: Computational complexity.***\n\nWe agree that the high computational complexity is a drawback of diffusion classifiers, and more generally generative classifiers. In our paper, we have made significant efforts to reduce the computational complexity by proposing multi-head UNet and various accelerated sampling strategies, as detailed in Section 3.5. Recently, we also made some progress for further reducing the time complexity of diffusion classifier. We found that a significant drop in robust accuracy, primarily caused by reducing $T'$ (i.e., number of sampling timesteps), is due to the large variance in diffusion loss. Owing to this substantial variance, more instances of diffusion loss must be calculated to achieve stable predictions. However, if we manage to reduce the variance of the diffusion loss (for example, by using the same $\\textbf{x}_t$ for different classes), it becomes feasible to decrease $T'$ and thus achieves faster inference speeds. By implementing this approach, **we successfully reduce the time complexity of the diffusion classifier by 200 times while maintaining the accuracy of clean data.** In the future, we will continue our work with the diffusion classifier, striving to further reduce its time complexity without compromising either robust or clean accuracy.\n\n\n\n***Question 2: Unclear definition of optimal diffusion model.***\n\nThank you for pointing this out. Yes, the optimal diffusion model  has minimal noise estimation error. We have revised our paper to make this clearer.\n\n\n\n***Question 3: Unclear definition of softmax operator.***\n\nSorry for the ambiguity. The softmax function is operated over all data samples. We have revised our paper to make this clearer.\n\n\n\n\n\n**Question 4: The gap between optimal diffusion classifier and empirical diffusion classifier.**\n\n\nThe primary difference between an optimal diffusion classifier and an empirical diffusion classifier is that the empirical diffusion classifier does not generalize well to the test data. In essence, the diffusion model fails to provide accurate score estimations for the noised versions of the test data. Conversely, an optimal diffusion model can yield accurate score estimations for test data. We empirically test the optimal diffusion classifier against AutoAttack, which achieves 100\\% robustness. This result indicates that if a diffusion model can generalize  well to the test set, it would attain 100\\% adversarial robustness.\nThe gap between the optimal diffusion model and the empirical one could be reduced by using more training data or designing advanced training techniques, which would further improve the performance of diffusion classifier.\nIn this paper, we focus on an alternative strategy (i.e., likelihood maximization) that moves the input $\\mathbf{x}$ to $\\hat{\\mathbf{x}}$ that diffusion model could give a more accurate score estimation.\n\n\n\n**Question 5: Lack of sampling details in likelihood maximization.**\n\nThank you for pointing this out. As you mentioned, we uniformly sample a timestep and then calculate the diffusion loss at that specific timestep. This process is used to approximate the expected diffusion loss across all timesteps. We have made this clearer in the revision.\n\n\n\n\n**Question 6: Specific details of BPDA**\n\nThank you for pointing this out. Here is the python code of BPDA:\n\n```\nclass LikelihoodMaximization(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x: Tensor):\n        x = likelihood_maximization(x.detach().clone())\n        return x\n\n    @staticmethod\n    def backward(ctx, upstream_grad):\n        return upstream_grad\n```\n\nIn the code, we do the forward pass normally, but in the backward pass, we directly return the upstream gradient. In other words, we view the Jacobian of likelihood maximization as an identity matrix."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission42/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231602121,
                "cdate": 1700231602121,
                "tmdate": 1700231602121,
                "mdate": 1700231602121,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dMIUjKRuMY",
                "forum": "I5lcjmFmlc",
                "replyto": "APg5VqtViB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission42/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission42/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the valuable review (2/2)"
                    },
                    "comment": {
                        "value": "**Question 7: Result on CIFAR-100.**\n\nThank you for the suggestion. We further conduct experiments on CIFAR-100 and test the robustness of different methods against $\\ell_\\infty$-norm threat model with $\\epsilon_\\infty=8/255$, following the same experimental settings as CIFAR-10. Due to the time limit, we only random sample 128 images for evaluation. The results are shown below.\n\n\n| Method | Clean Acc | Robust Acc|\n|---------|:-------:|:--------:|\n| WRN-40-2 |   78.13     | 0.00 |   \n| AT-DDPM |    63.56     |   34.64   |\n| AT-EDM  |  75.22        |   42.67 |\n| DiffPure   |         39.06         |    7.81    |\n| DC  |    79.69    |  39.06|\n|RDC |      80.47       |  53.12|\n\n\n\n\n\nWe find that RDC still achieves superior result compared with the state-of-the-art adversarially trained models and DiffPure.\nMore surprisingly, we discover that DiffPure does not work well on CIFAR-100. We guess this is because CIFAR-100 has more fine-grained classes, and thus a small amount of noise will make the image lose its semantic information of a specific class. Hence, DiffPure is not suitable for datasets with more fine-grained classes and small resolution.\n\n\n\n\n\n**Question 8: Square attacks can not find more adversarial examples.**\n\nYes, we used Square Attack in AutoAttack for robustness evaluation. We found that Square Attack can not find additional examples on top of the BPDA adaptive attack. \n\n\n\n\n**Question 9: Experiments about $T'$.**\n\nIn Figure 2a, we show the robustness of Diffusion Classifier (DC), but not the Robust Diffusion Classifier (RDC). DC can only achieve 35.94\\% robustness against the $\\ell_\\infty$ threat model with $\\epsilon=8/255$, as shown in Table 1.\n\nDuring training, $T$ is set as 1000 in DDPM (Ho et al., 2020). EDM (Karras et al., 2022) extends the discrete diffusion model into continuous diffusion model, where $t$ represents the standard deviation of isotropic Gaussian noises added to images. Consequently, in the context of EDM, there is no equivalent concept of $T$. Instead, during training, a $t$ value is randomly sampled from the range $[\\sigma_{min}, \\sigma_{max}]$.\n$T'$ is set as 1000 in Table 1.\n\n\nAs argued in Karras et al. (2022), the training and inference of diffusion model can be decoupled. During training, a continuous $t$ could provide more accurate score estimation for different $t$, or even play a role of regularization, making the diffusion model generalize better. Therefore, $T'$ during inference is not related with $T$ during training. \nTherefore, we assert that reducing $T$ during training does not contribute positively to the performance of the diffusion classifier. We believe that a significant drop in robust accuracy, primarily caused by reducing $T'$, is due to the large variance in diffusion loss (See Question 1 for a solution). \n\n\n\n\nWe agree that adopting a large $T'$ may further improve the robustness of the diffusion classifier. We conduct another experiment where we set $T'=2000$. As shown below, the robustness is further improved. \n\n\n| $T'$ |   Robustness  |\n|---------|:-------:|\n|  1000   |    35.94    |\n| 2000    |    39.06       |\n\n\n\n\n**Question 10: Details about multi-head diffusion.**\n\nWe did not add any class condition signal to the multi-head diffusion. The last layer of multi-head diffusion directly maps the feature into $3\\times K \\times H \\times D$ dimension to output predictions for $K$ classes. If we inject the class condition signal into some intermediate locations in UNet, then we have to calculate $K$ times for the rest part of the UNet, because the activations of each class are not same after the injection.\n\n\nThank you for your advice on including the generation of multi-head diffusion. We were surprised to find that our multi-head diffusion achieves a 1.96 FID on the CIFAR-10 dataset, demonstrating its generative ability, which is comparable to the state-of-the-art generative models.\n\n\n\n****\n\n\n\n\n\n**References:**\n\nHo, Jonathan, Ajay Jain, and Pieter Abbeel. \"Denoising diffusion probabilistic models.\" Advances in Neural Information Processing Systems, 2020.\n\n\nKarras, Tero, et al. \"Elucidating the design space of diffusion-based generative models.\" Advances in Neural Information Processing Systems 35 (2022): 26565-26577.\n\n\nNie, Weili, et al. \"Diffusion models for adversarial purification.\" International Conference on Machine Learning, 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission42/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231645549,
                "cdate": 1700231645549,
                "tmdate": 1700232625378,
                "mdate": 1700232625378,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RqjYzsOdfX",
                "forum": "I5lcjmFmlc",
                "replyto": "dMIUjKRuMY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission42/Reviewer_Xkzq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission42/Reviewer_Xkzq"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response."
                    },
                    "comment": {
                        "value": "Thank you for the response. The response addressed most of my concerns, so I have increased the rating from 6 to 8.\n\nJust a couple of minor things:\n- Is the softmax operator in Corollary 3.3 defined as the softmax across all classes?\n- There are some occasions where the equations exceed the page margin. Please fix them.\n- Please explain the four legend entries of Figure 2a in its caption, since I don't think they are used elsewhere. It should also be clarified in the caption that the figure was made on DC, not RDC.\n- It would be nice to add $T' = 2000$ (and potentially even higher if computation allows) to Figure 2a to show whether the robust accuracy \"saturates\", which seems to be the case based on the number for $T' = 2000$.\n- It's odd that Figure 2c is mentioned first in the text, then 2b, and finally 2a. It might make sense to change the order.\n- It would be interesting to show some of the generated images from the diffusion models as figures in the paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission42/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700352868368,
                "cdate": 1700352868368,
                "tmdate": 1700352868368,
                "mdate": 1700352868368,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]