[
    {
        "title": "A Theoretical Explanation of Deep RL Performance in Stochastic Environments"
    },
    {
        "review": {
            "id": "dgfg2QZeLa",
            "forum": "5ES5Hdlbxw",
            "replyto": "5ES5Hdlbxw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8925/Reviewer_UBNV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8925/Reviewer_UBNV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new explanation of the success of deep RL algorithms that use random exploration strategies for stochastic environments. The key observation is that many environments can be solved by a few steps of value iteration starting with the uniformly random policy, meaning that sophisticated exploration is unnecessary. Based on the observation, this paper designs a novel provable RL algorithm called SQIRL that runs a few steps of fitted-Q iteration, and the sample complexity of SQIRL is only exponential in (roughly) the number of value iteration steps needed to solve the environment. This paper also shows empirically that the sample complexity of SQIRL correlates with the sample complexity of standard deep RL algorithms such as DQN and PPO."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tThis paper extends the effective horizon definition in Laidlaw et al. (2023) to stochastic environments, and consequently designs a novel provable algorithm SQIRL that depends exponentially only on the effective horizon. This paper also empirically verifies that the effective horizon is small for many realistic environments including Atari games. Hence, the SQIRL algorithm is the first algorithm that (a) has provable sample complexity upper bound on realistic environments, and (b) achieves non-trivial performance empirically.\n-\tAlthough the algorithm is very similar to the classic FQI algorithm, the sample complexity upper bound in Theorem 3.6 is novel. In addition, the assumptions on the oracle are mild and potentially can be satisfied by realistic neural networks.  \n-\tThis paper is well-written and easy-to-follow."
                },
                "weaknesses": {
                    "value": "-\tGiven that SQIRL has a provable and also computable sample complexity bound, this paper could benefit from more analysis on the comparison between the theoretical and empirical performance of SQIRL. For example, how does the percentage of environments that SQIRL can solve change with k? Is there a strong correlation between the actual sample complexity SQIRL and the sample complexity upper bound in Theorem 3.6?\n-\tFor an unknown environment, computing the effective horizon or the sample complexity bound requires running a few steps of value iteration, which is not much easier than running the algorithm since we must iterate through the entire state space. This makes the instance-dependent complexity upper bound less helpful in determining the hardness of an environment / predicting the performance of deep RL algorithms on new environments. The bound could be more impactful if there is an efficient algorithm to estimate the effective horizon.\n-\tIt is unclear whether the small effective horizon is an artifact of the sticky actions modification to the environment. A 25% chance of sticky action could potentially make long-term planning impossible, hence decreasing the effective horizon. Hence the conclusion of Figure 2 needs further justification."
                },
                "questions": {
                    "value": "-\tHow does the performance of SQIRL change with the hyperparameter k? Is the best performance always achieved by choosing the smallest k such that the environment is k-QVI-solvable?\n-\tIs there a typo in the legend of Figure 2? Currently the figure shows that PPO succeeds less often when k=1.\n-\tIt would be intriguing to see whether the sample complexity of the empirical version of SQIRL is still upper bounded by Theorem 3.6. For example, for all the environments with k=1, does SQIRL always solve the environment after $N^{SQIRL}$ steps?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698193483463,
            "cdate": 1698193483463,
            "tmdate": 1699637124087,
            "mdate": 1699637124087,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0luOABh34C",
                "forum": "5ES5Hdlbxw",
                "replyto": "dgfg2QZeLa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8925/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments. We appreciate they found our results to be a significant contribution and our paper to be \"well-written and easy-to-follow.\" The weaknesses and questions in the review are addressed below:\n * **Comparing the theoretical and empirical performance of SQIRL:** we agree that it would be ideal to compare our theoretical bounds on SQIRL's sample complexity to its empirical performance. However, this is actually quite difficult because our practical implementation of SQIRL uses neural networks and neural network generalization is not well understood. In the bound on SQIRL's sample complexity in equation (1), we can easily calculate the effective horizon $\\bar{H}_k$, but the $D$ term depends on how well the regression oracle\u2014neural networks in our practical implementation\u2014can generalize given a certain number of training samples. Since there are few or no bounds on neural network generalization error that reflect their empirical performance, we cannot provide a tight bound on SQIRL's sample complexity using neural networks. However, an advantage of our work is that any advances in understanding neural network generalization can immediately result in better bounds on SQIRL's sample complexity. Furthermore, we believe that the fact that SQIRL performs well empirically suggests our assumptions are met in practice and that our theory is a valuable tool for analyzing model-free RL with random exploration.\n * **Computing the effective horizon is not much easier than solving the environment:** this is a valid point, and we agree that it is intractable to compute the effective horizon for many problems. However, we still think that it can provide useful insights to theorists and practitioners. There may be ways of bounding the effective horizon using simpler quantities. For instance, Theorem B.4 of Laidlaw et al. (2023) shows that for any deterministic goal-based environment, the effective horizon can be bounded based on the probability of reaching the goal under the random policy from various states; their analysis translates easily to the stochastic setting. It is often easy to reason about the probability of reaching a goal, making the effective horizon a useful tool for analysis in goal-based environments. Besides precise bounds like this theorem, the effective horizon also has an intuitive interpretation\u2014when the Q-values of optimal actions under the random policy are easier to distinguish from those of suboptimal actions (i.e., the $k$-gap is larger), then it will be easier to learn an optimal policy. This type of intuitive insight can help practitioners design better environments and reward functions such that RL will easily succeed.\n * **How does the performance of SQIRL change based on $k$?** In the table below, we show how many environments can be solved by SQIRL as we allow just $k = 1$, $k \\leq 2$, or $k \\leq 3$. We find that even with $k = 1$, SQIRL can solve many of the sticky-action BRIDGE environments.\n\n   |            | Number of environments solved |\n   |-|-|\n   | $k = 1$    | 48 |\n   | $k \\leq 2$ | 64 |\n\t | $k \\leq 3$ | 68 |\n\n   We also found that the ideal value of $k$ for SQIRL varied depending on the environment (see the table below), and was not always the same as the minimum value of $k$ for which the MDP is approximately $k$-QVI-solvable (which for most environments was $k = 1$\u2014see Figure 2). The reason for this is that while increasing $k$ increases the first term in the definition of the stochastic effective horizon (Definition 3.3), it might decrease the second term by making the $k$-gap larger, thus making the stochastic effective horizon $\\bar{H}_k$ smaller as $k$ increases.\n\n   | Optimal value of $k$ for SQIRL | Number of environments |\n   |-|-|\n   | 1 | 19 |\n   | 2 | 22 |\n   | 3 | 27 |\n\n   We will include these tables and analysis in the final paper if accepted.\n\n * **Is the small effective horizon just a product of sticky actions?** We believe that the low effective horizon in our environments is more of an intrinsic property that does not just result from using sticky actions, since Laidlaw et al. (2023) also found that the (deterministic) effective horizon was low in the BRIDGE environments even without sticky actions. Furthermore, sticky actions are a standard method for making environments stochastic and widely used as a way of evaluating RL algorithms [1]. Thus, the fact that the stochastic effective horizon is low in these environments suggests that it underlies the success of many deep RL algorithms in this common evaluation paradigm of sticky actions.\n * **Is there a typo in the legend of Figure 2?** Yes, this is a typo, and we have fixed it in an updated revision of the paper.\n\n[1] Machado et al. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 2018."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093296365,
                "cdate": 1700093296365,
                "tmdate": 1700093296365,
                "mdate": 1700093296365,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uSLoo3YQ5d",
                "forum": "5ES5Hdlbxw",
                "replyto": "0luOABh34C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8925/Reviewer_UBNV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8925/Reviewer_UBNV"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I will keep my score as it is."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589483682,
                "cdate": 1700589483682,
                "tmdate": 1700589483682,
                "mdate": 1700589483682,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jJcDNXB0sz",
            "forum": "5ES5Hdlbxw",
            "replyto": "5ES5Hdlbxw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8925/Reviewer_1dWJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8925/Reviewer_1dWJ"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors aim to provide theoretical explanations, supported by an empirical analysis, of the practical success of Deep-RL algorithms. In recent work, Laidlaw et al., 2023, \"Bridging RL Theory and Practice with the Effective Horizon\", the authors have shown that deep-RL methods succeed in (deterministic) MDPs, especially in cases in which it is sufficient to take few \"greedyfication\" steps from the Q-function of the random policy (i.e., the effective horizon). In this work, the authors show that these claims can be extended to stochastic MDPs by introducing the \"stochastic effective horizon\". More specifically, the paper is articulated in the following points:\n1. The authors provide a formal definition of the \"stochastic effective horizon\" notion that presents itself as the natural extension of the effective horizon introduced by Laidlaw et al., 2023. \n2. Secondly, the authors propose a simple algorithm (SQIRL) whose sample complexity scales exponentially with the stochastic effective horizon (which is typically much smaller than the optimization horizon of the underlying problem).  \n3. On a stochastic extension of the benchmark Bridge (Laidlaw et al., 2023), the authors show that modern deep-RL algorithms performance significantly correlates with the empirical success of SQIRL. This empirical phenomenon suggests that the concept of stochastic effective horizon can explain some of the reasons behind the success/failure of modern deep-RL methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Comprehending the factors contributing to the successes and shortcomings of deep reinforcement learning (Deep-RL) algorithms is of utmost importance. Analyzing the environments in which current methods excel provides insight into both their capabilities and constraints. This understanding serves as a foundation for inspiring and creating innovative approaches that address the limitations of current technologies. In this sense, the work done by the authors goes in this direction, as the problem deserves attention from the community (both practitioners and theoreticians).\n2. The paper takes large inspiration from the recent work of Laidlaw et al., 2023, \"Bridging RL Theory and Practice with the Effective Horizon\". Nevertheless, in Laidlaw et al., 2023, the authors consider a deterministic setting. In this work, instead, the authors propose an extension to the more general (and challenging) stochastic environments. Although, in this sense, the novelty of the underlying idea is incremental, the challenges that are introduced from stochastic environments do not allow for a direct extension. \n3. The main text is overall well-written and easy to understand."
                },
                "weaknesses": {
                    "value": "**1. Novelty**\n\nFirst, I remark that the main contribution done by the authors goes into the direction of providing explanations behind the success of deep-RL algorithms. Although these explanations are highly appreciated, it has to be remarked that the main idea on which the work is built has been already proposed in Laidlaw et al., 2023. Indeed, in Laidlaw et al., 2023, the authors have shown that deep-RL succeed in (deterministic) MDPs especially in cases in which it is sufficient to take few \"greedyfication\" steps from the Q-function of the random policy. In this sense, the novelty, in terms of new explanations that are given to the success of deep-RL algorithms is somehow limited. The contributions of the authors, in this sense, is limited to to the extension to stochastic environments.\n\n**2. Theoretical claims and analysis**\n\nI have concerns regarding the theoretical claims done by the authors, especially regarding the main Theorem (i.e., Theorem 3.6). Specifically, I checked the proofs behind Theorem 3.6, and there are some steps that are unclear/unprecise/uncorrect (p.s., I haven't had a look in details to the other sections of the appendix, so I am unaware if there mistakes in those parts). \n\nFirst, I begin with the result on the sample complexity (Eq. 1). Using standard tools from the bandit community, it is possible to show that, for $\\epsilon$-best arm identification with 2 arms, the Lower bound is given by $\\widetilde{\\Omega}[ \\max \\left( \\Delta^{-2}, \\epsilon^{-2} \\right) ]$ (RL with 1 state, depth 1, and 2 actions). In Eq. 1, instead, the authors claim a complexity of $\\widetilde{\\mathcal{O}}(\\epsilon^{-1})$, which is clearly impossible. It seems to me that the main problem is that the authors masked inside the $\\widetilde{\\mathcal{O}}$ instant dependent quantities such as $\\Delta^{-2}$. \n\nSecondly, I invite the authors to provide details on the last step behind the proof of Lemma B.1. To me, it seems that they upper-bound $P_\\pi(\\mathcal{E}) \\le \\epsilon$, but, then it is unclear how they upper bound the cumulative sum of rewards to $1$.\n\n(minor) some assumptions in Lemma B.1 are not used. For instance, k-solvability seems to be not used within the proof.\n\nOverall, I currently believe that all these issues could be solved, leading to results comparable (or maybe, slightly worse) to the one presented in Theorem 3.6. Nevertheless, the paper, at its current status, seems to be lacking in formal correctness.\n\n**3. Weaknesses of the proposed algorithm (minor)**.\n\nIt has to highlighted that the algorithm proposed by the authors needs to be aware of the k parameter of K-QVI-solvability property. This parameter is often unknown in practice. I consider this to be a minor weakness, as the purpose of this work is not to propose an algorithm (with theoretical guarantees) that can be applied in practice, but rather it focuses on explaining why deep RL algorithms succed in practice. \n\n**4. Minor comments:**\n- Colors in Figure 2 seems to be swapped. I currently read the Figure as follows: PPO fails most likely with small values of k (however, I guess the opposite claim should be the correct one)."
                },
                "questions": {
                    "value": "See weakness section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8925/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8925/Reviewer_1dWJ",
                        "ICLR.cc/2024/Conference/Submission8925/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698397466132,
            "cdate": 1698397466132,
            "tmdate": 1700559761938,
            "mdate": 1700559761938,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o18AHu77oO",
                "forum": "5ES5Hdlbxw",
                "replyto": "jJcDNXB0sz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8925/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed comments. We appreciate that they found our submission addresses an important problem and that it is \"overall well-written and easy to understand.\" Below, we have responded to the weaknesses raised by the reviewer:\n\n * **Novelty:** while our paper builds on Laidlaw et al. (2023), we argue that it is a significant contribution in its own right. Since the GORP algorithm introduced in the prior paper relies heavily on the environment being deterministic, it was not clear to us at first how to extend it to stochastic environments. While the novelty of our submission may seem \"limited\" in hindsight, we found that in discussions with other researchers and among ourselves, it was quite difficult to think of a stochastic analog of GORP. Furthermore, our theoretical analysis requires quite different tools than Laidlaw et al. They need to only analyze mean estimation-type problems for the deterministic case, which is quite simple. In contrast, we analyze the statistical generalization of complex function approximators across distributions of state-action pairs. Thus, we believe that our submission represents a significant advancement beyond prior work.\n * **Correctness of theoretical claims:** we address the specific concerns of the reviewer regarding the theoretical claims of the submission below.\n    * *Sample complexity bound (Equation 1):* the instance-dependent $1/\\Delta_k^2$ term is contained within the $A^{\\bar{H}_k}$ term of the sample complexity bound presented in Equation 1, since $\\bar{H}_k = k+ \\log_A ( 1 / \\Delta_k^2 )$ according to Definition 3.3. Thus, since the sample complexity bound is always higher than $1/\\Delta_k^2$, it does not contradict lower bounds for the bandit case.\n    * *Last step of proof of Lemma B.1:* we do assume that the total rewards over an episode are bounded almost surely in $[0, 1]$, as we note in the second paragraph of Section 2 (page 3): \"We assume that the total reward $\\sum_{t=1}^T R_t(s_t, a_t)$\nis bounded almost surely in $[0, 1]$; any bounded reward function can be normalized to satisfy this assumption.\"\n    * *Assumptions in Lemma B.1:* we should have been more explicit about how $k$-QVI-solvability is used in the proof of Lemma B.1. It is used to show that $\\pi^* \\in \\Pi(Q^k)$ is an optimal policy (by the definition of $k$-QVI-solvability), and thus that $J(\\pi^*) = \\max_\\pi J(\\pi)$. Thank you for pointing this out\u2014we will make it explicit in the final version of the paper.\n   \n   Overall, we do believe that any of these concerns affect the formal correctness of the paper. If the reviewer still believes there are issues with any of our theoretical claims, we invite them to let us know in a follow-up comment.\n * **Weaknesses of the proposed algorithm:** we agree with the reviewer's assessment that, while SQIRL has hyperparameters that must be carefully tuned for a particular environment, the algorithm is still useful for theoretically understanding when and why deep RL works in practice. As we wrote at the top of page 9, \"we do not claim that SQIRL is as practical as PPO or DQN, since it requires much more hyperparameter tuning; instead, we mainly see SQIRL as a tool for understanding deep RL.\"\n * **Figure 2 colors:** the reviewer is correct that we accidentally swapped the colors in Figure 2. We have corrected the problem in an updated submission."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093113996,
                "cdate": 1700093113996,
                "tmdate": 1700093113996,
                "mdate": 1700093113996,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NS5OSzhG40",
                "forum": "5ES5Hdlbxw",
                "replyto": "o18AHu77oO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8925/Reviewer_1dWJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8925/Reviewer_1dWJ"
                ],
                "content": {
                    "title": {
                        "value": "Ack"
                    },
                    "comment": {
                        "value": "I thank the authors for their in-depth rebuttals. \n\nConcerning the novelty, what I meant is the following aspect: the \"explanation\" of what is happening in deep RL methods takes large inspiration from an already existing work. In this sense, the novelty is limited. Notice that I ack. novelty in terms of extension to stochastic settings among the strength points.\n\nThat being said, most of my theoretical concerns have been addressed. I have one final question for the authors. I see that the authors are operating under the assumption that cumulative sums are bounded in [0,1]. This assumption, however, is somehow different w.r.t. the one that is commonly adopted in the literature (i.e., rewards are bounded in [0,1] and cumulative returns in [0,T]). Can the authors modify their assumption to the standard setting? Is this possible? What kind of result would we obtain? In this way, the comparison in Table 1 can be more appreciated (as the other works mentioned, at least for the tabular setting, operate under this assumption). \n\nMinor comment:\n- The authors refer to the work of Azar et al., 2017 in Table 1. It seems to me, however, that Azar 2017 builds algorithms and presents results for the regret metric."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469505315,
                "cdate": 1700469505315,
                "tmdate": 1700469505315,
                "mdate": 1700469505315,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S2f4Cg9dPE",
                "forum": "5ES5Hdlbxw",
                "replyto": "A2BpywUt8K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8925/Reviewer_1dWJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8925/Reviewer_1dWJ"
                ],
                "content": {
                    "title": {
                        "value": "Ack"
                    },
                    "comment": {
                        "value": "I thank the authors for their in-depth response. I have updated my score accordingly. \nI have no further questions for the authors."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559736903,
                "cdate": 1700559736903,
                "tmdate": 1700559736903,
                "mdate": 1700559736903,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5zeWMB8Qhz",
            "forum": "5ES5Hdlbxw",
            "replyto": "5ES5Hdlbxw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8925/Reviewer_yWoc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8925/Reviewer_yWoc"
            ],
            "content": {
                "summary": {
                    "value": "The paper extends the previous work GORP which studied the explanation of RL in deterministic environments by proposing and studying a new method under stochastic environments under function approximation. GORP shows that the success of both PPO and DQN can simply be explained by a simple procedure of greedily improving a few steps over a value function of random policy. SQIRL extends the algorithm and analysis for stochastic environments. The results demonstrate that the performance of the proposed method SQIRL correlates with the performance of PPO and DQN in stochastic environments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper addresses the key limitation of prior work: GORP which showed that performance of deepRL can be explained by just acting greedily with respect to value function of a random policy in deterministic environments. GORP does not extend to stochastic environments directly and the authors propose one way to extend the algorithm to stochastic environments by using function approximation.\n2. The authors theoretically explain the sample complexity of their proposed algorithm in terms of psuedo-dimension of the function class along with a combination of concentration and FQI analysis.  The authors also demonstrate that this sample complexity is closely related to sample complexity of Deep RL algorithms used in practice.\n3. The method SQIRL is tested extensively over a set of 150 environments from previous work GORP. The environments are made stochastic by using sticky actions and show that whenever PPO or DQN does well, SQIRL does well 78% of the time too."
                },
                "weaknesses": {
                    "value": "1. My major concern is around the novelty of the proposed approach:\na. To address stochasticity an open loop trajectory optimization is replaced by FQI. This is not new in my opinion:\nPrior works:\n[1] Considers a tree search with the FQI procedure along with analysis to account for stochastic environments.\n[2] Considers a FQI analysis of H-step lookahead under empirical distributions for a more general setting of learned models (A.3) \n[3] Considers a similar FQI setting with learned models where exploratory policy is the dataset policy\nA proper discussion on the novelty of using FQI to replace open-loop trajectory optimization along with a comparison with these prior works is warranted.\n2. The strong claim of in-distribution generalization: On page 6 the authors claim that if we can properly regress for approximate-Q then we can also estimate the maximum action properly. The claim seems to be too strong without evidence: With finite data, the Q will almost always have errors. These errors will lead to perhaps suggesting actions OOD and lead to overestimation bias commonly observed in deep RL. This analysis seems to be missing in analysis of FQI for SQIRL.\n3. A minor nitpick: Unlike GORP, the whole set of stochastic environments are not explained by SQIRL - I think the title and introduction implying something more stronger that it should?\n4. Sticky action is a particular kind of stochasticity - More ways of inducing stochasticity and varying the noise std can make the empirical experiments stronger.\n[1]: https://arxiv.org/pdf/2107.01715.pdf\n[2]: https://arxiv.org/pdf/2107.01715.pdf\n[3]: https://arxiv.org/pdf/2008.05556.pdf"
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8925/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8925/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8925/Reviewer_yWoc"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698720814783,
            "cdate": 1698720814783,
            "tmdate": 1700703065692,
            "mdate": 1700703065692,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XnBzrggeWE",
                "forum": "5ES5Hdlbxw",
                "replyto": "5zeWMB8Qhz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8925/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments. We are glad they found that our paper \u201caddresses the key limitation of prior work\u201d and that our experiments are \u201cextensive.\u201d Below we have responded to the weaknesses raised in their review.\n\n * **Novelty compared to previous work:** we acknowledge that tree search and FQI have been extensively used and analyzed in prior works, including those the reviewer cited. However, the papers mentioned in the review use a model for planning. Unlike these works, our focus is on learning an optimal policy within a model-free setting, where no access to a pre-existing model is available. This introduces a higher level of complexity as we are constrained to sample episodes from a fixed initial state distribution and cannot reset to arbitrary states for further planning. This means that it is not possible to apply standard tree search procedures\u2014instead, one of our key contributions is constructing an RL algorithm, SQIRL, which behaves in some ways like tree search but is actually completely model-free. We will clarify this point and compare to the work cited by the reviewer in the final version of the paper if accepted.\n * **Claim of in-distribution generalization:** the reviewer notes that in many Q-learning-like RL algorithms, the Q values of states which are rarely seen or actions which are rarely taken or may be significantly mis-estimated. However, our theoretical analysis shows that this is *provably* not the case for SQIRL. Since SQIRL explores by taking actions uniformly at random before running FQI, no actions are sampled more often than others in expectation, meaning that no actions are any more \"out-of-distribution\" than others. According to Lemma B.2, this means that the mean-squared error of the Q-function can blow up by a factor of $A$ (the number of actions) at each step of FQI, but no more. We then combine this error bound with Markov's inequality in Lemma B.3 to show that the probability of any action's Q-value being off by too much is bounded. This allows us to give the bounds on sample complexity of SQIRL in Theorem 3.6.\n * **Whole set of stochastic environments is not explained by SQIRL:** we will revise our language in the introduction to indicate that we do not provide a complete explanation for deep RL performance in all possible environments. Instead, our analysis is aimed at generally understanding when and why deep RL works across a broad range of environments. We note that in Laidlaw et al. (2023), GORP also does not solve all the deterministic BRIDGE environments\u2014is there another way in which you are claiming that GORP \"explains\" all the environments?\n * **Other types of stochasticity:** we agree that there are other ways of adding stochasticity to environments besides sticky actions. We primarily focus on sticky actions based on the evidence provided by Machado et al. [1], who explore a number of ways of inducing stochasticity in Atari games. They conclude that sticky actions are the best way to introduce stochastic transitions:\n\n   > Sticky actions [leverages] some of the main benefits of other [stochasticity] approaches without most of their drawbacks. It is free from researcher bias, it does not interfere with agent action selection, and it discourages agents from relying on memorization. The new environment is stochastic for the whole episode, generated results are reproducible, and our approach interacts naturally with frame skipping and discounting.\n\n   Thus, following their conclusions, we decided to focus on creating stochastic environments using sticky actions.\n\n[1] Machado et al. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 2018."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700092991611,
                "cdate": 1700092991611,
                "tmdate": 1700092991611,
                "mdate": 1700092991611,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3ONXxYTsz5",
                "forum": "5ES5Hdlbxw",
                "replyto": "XnBzrggeWE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8925/Reviewer_yWoc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8925/Reviewer_yWoc"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer response"
                    },
                    "comment": {
                        "value": "Thank you for the rebuttal. \n\nMy concerns still remain about novelty despite the model-free/model-based argument since any H-step trajectory optimization can be replaced by H-step QVI. This has been used in prior works before including Laidlaw et al. (2023). I think my concerns about in-distribution generalization are addressed. \n\n**is there another way in which you are claiming that GORP \"explains\" all the environments?**\n\nCiting this line from their work:\n>  This proportion rises to four-fifths among environments that PPO [9], a popular deep RL algorithm, can solve efficiently (Table 1). Conversely, when this property does not hold, PPO is more likely to fail than succeed\u2014and when it does succeed, so does simply applying a few steps of lookahead on the Q-function of the random policy\n\nFinally, I think ICLR allows paper revisions. It would also help to see the promised changes updated in the paper. eg \"we will revise our language in the introduction to indicate that we do not provide a complete explanation for deep RL performance in all possible environments.\" and \"Novelty compared to previous work:\""
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323292425,
                "cdate": 1700323292425,
                "tmdate": 1700323292425,
                "mdate": 1700323292425,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cQOXYhgjYK",
                "forum": "5ES5Hdlbxw",
                "replyto": "8OmWQcDGsx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8925/Reviewer_yWoc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8925/Reviewer_yWoc"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed clarifications. While I still have reservations about the novelty of QVI to replace TrajOpt, I find the rebuttal satisfactory. I have raised my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703103515,
                "cdate": 1700703103515,
                "tmdate": 1700703103515,
                "mdate": 1700703103515,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1LMmEcM4Nt",
            "forum": "5ES5Hdlbxw",
            "replyto": "5ES5Hdlbxw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8925/Reviewer_kbuB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8925/Reviewer_kbuB"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers a setting where exploration is not needed. They formalize the setting in Def 3.1. They also design an algorithm and show that the algorithm is sample-efficient in the setting they consider. They also conduct experiment to validate their idea."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The setting they consider is closely related to the tasks in the application."
                },
                "weaknesses": {
                    "value": "1. The theory is simple and straightforward. In fact, in the setting they considered, exploration is not needed. \n\n2. It is unclear whether their algorithm can be adapted to the scenario where exploration is needed."
                },
                "questions": {
                    "value": "1. See the 'Weakness' section.\n\n2. Figure 2 shows that PPO fails in most tasks with k=1, which contradicts the claim 'Furthermore, these are the environments where deep RL algorithms like PPO are most likely to find an optimal policy' on Page 5. Can you provide an explanation?\n\n3. Apart from the tasks in Bridge, can you verify Def 3.1 for other tasks, including go and robotic?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698957025135,
            "cdate": 1698957025135,
            "tmdate": 1699637123728,
            "mdate": 1699637123728,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b5DqyBPTiy",
                "forum": "5ES5Hdlbxw",
                "replyto": "1LMmEcM4Nt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8925/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments. We have addressed the weaknesses and questions they mentioned below:\n\n * **\u201dThe theory is simple and straightforward\u2026 [since] exploration is not needed.\u201d** Just to clarify, in *any* RL problem exploration is needed. However, in the setting we consider, random exploration seems to be enough, i.e., strategic exploration is not necessary. One of our central points in the paper is that many common RL benchmarks *can* be solved with random exploration, as shown by the success of deep RL algorithms that explore randomly. Our contribution is to explain\u2014both theoretically and empirically\u2014how random exploration works well in practice with complex function approximators like neural networks, since existing theory suggests that random exploration can result in exponentially bad sample complexity.\n\n   Furthermore, we argue that our theory is not simple or straightforward, since very few previous works in the RL theory literature have been able to explain why random exploration is sample efficient in practice despite being exponentially inefficient in the worst case. The few papers that have focused on random exploration either rely on assumptions that do not seem to hold in realistic environments or they do not consider generalization with complex function approximators. In contrast, our theory both rests on assumptions that are verified in common RL benchmarks and is very general, as it can be applied to even quite complex function approximators like neural networks.\n * **\u201dFigure 2 shows that PPO fails in most tasks with k=1.\u201d** This was actually a typo on our part; the labels should have been reversed. We have updated the figure accordingly and it now correctly shows that PPO succeeds in most of the MDPs with k = 1.\n\n * **\u201dApart from the tasks in Bridge, can you verify Def 3.1 for other tasks, including go and robotic?\u201d** In this paper, our focus is on model-free RL in environments with discrete action spaces. Model-free RL does not work very well for Chess or Go\u2014the best RL algorithms for solving these games have consistently been model-based, like AlphaZero and MuZero. Furthermore, they are two-player games, while we consider single-agent environments. Most robotics tasks are also outside of the scope of our analysis since they have continuous action spaces. While we hope to extend our work from discrete actions to continuous action spaces in the future, our current submission already covers a large number of common RL benchmarks, like Atari games, Procgen, and MiniGrid. Thus, we believe that verifying Definition 3.1 ($k$-QVI-solvability) in these environments is already enough to show that this property holds in a large proportion of the types of environments where model-free RL works in practice."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700092800747,
                "cdate": 1700092800747,
                "tmdate": 1700092800747,
                "mdate": 1700092800747,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]