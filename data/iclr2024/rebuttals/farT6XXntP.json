[
    {
        "title": "A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models"
    },
    {
        "review": {
            "id": "VezMUEs1o4",
            "forum": "farT6XXntP",
            "replyto": "farT6XXntP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4491/Reviewer_TMW7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4491/Reviewer_TMW7"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed an innovative two-stage fine-tuning method: initially fine-tuning on non-English monolingual data to enhance comprehension, followed by further fine-tuning on a small amount of high-quality human-translated parallel text. This approach enabled even smaller LLMs to achieve state-of-the-art translation performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The results of the paper indicated that smaller models could achieve SOTA translation levels through specialized fine-tuning, suggesting that there might not be a continuous need to expand datasets and models for better performance.\n\nThrough compact, specialized fine-tuning, smaller LLMs could achieve SOTA translation quality without billions of parameters. The focus of this research was on tailored fine-tuning methods that unleashed the potential of LLM's multilingual capabilities on a broader scale.\n\nThe paper demonstrated that instead of increasing data scale, intentional fine-tuning targeting key language capabilities might be the key to maximizing LLM performance.\n\nBy revealing the potential of smaller LLMs for efficient and accurate machine translation, this work laid the foundation for developing more user-friendly and scalable machine translation systems. This training approach offered more possibilities for deploying capable multilingual LLMs in real-world applications."
                },
                "weaknesses": {
                    "value": "There were certain flaws in the method, and prompts affected the results. \n\nThe evaluation methods had its limitations. \n\nThe stability of the proposed method was not verified."
                },
                "questions": {
                    "value": "1. Typically, if an LLM was fine-tuned with specific bilingual corpora to enhance translation capabilities between those two languages, it might impair other NLP capabilities of the LLM, such as document summarization and logical question answering. Did this issue not arise in this study?\n\n2. If possible, please consider using the 'Instruct Score' from EMNLP 2023 (Instructscore: Towards Explainable Text Generation Evaluation with Automatic Feedback) as a metric. I believe it's a better benchmark for evaluating LLM-MT.\n\n3. The outputs of large models were uncertain. Even a minor change in a prompt could lead to variations in the output. During the two-stage fine-tuning process, was the specific impact of the prompt considered?\n\n3. Given the powerful In-Context Learning capabilities of large language models, it would be worth exploring whether adding relevant knowledge to the prompt could further enhance translation capabilities.\n\n4. The section \"Small Training Data Is Enough\" contained many uncertain descriptions, which should be rigorous in a paper and supported by convincing data. Moreover, the best-performing commercial LLM, GPT-4, remained proprietary, leaving us in the dark about the amount of data used, the number of parameters trained, and potential data leakage issues during translation metric testing.\n\n5. The sentence in the \"Large Parallel Data Wash Out the Knowledge\" section: \"As expected, it tends up with a similar performance in both BLEU and COMET evaluations (triangle in Figure 4),\" was hard to understand.\n\n6. The proposed method significantly improved translation metric scores, highlighting ALMA's effectiveness. Based on this, how did the authors believe LLM generalized translation capabilities? Did the proposed method fundamentally assist LLMs in learning deep bilingual text alignment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4491/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4491/Reviewer_TMW7",
                        "ICLR.cc/2024/Conference/Submission4491/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4491/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685896361,
            "cdate": 1698685896361,
            "tmdate": 1700659876439,
            "mdate": 1700659876439,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D4CfuU9mjy",
                "forum": "farT6XXntP",
                "replyto": "VezMUEs1o4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4491/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4491/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Rebuttal 1 by Authors"
                    },
                    "comment": {
                        "value": "We genuinely appreciate the valuable feedback provided by the reviewer and have addressed them in a point-by-point manner below. We are more than willing to engage in further discussions with the reviewers should any follow-up questions arise.\n\n**Regarding your concern about prompt & stability :**\n>There were certain flaws in the method, and prompts affected the results.\n\n>The stability of the proposed method was not verified.\n\nThank you for your insightful feedback!  We wholeheartedly acknowledge the idea that different prompts can influence translation performance, and that enriching these prompts could further improve results. Our primary focus, however, is to demonstrate that a properly formulated prompt enables moderate-sized large language models (LLMs) to achieve performance comparable to state-of-the-art (SOTA) encoder-decoder translation models or GPT3.5/4. The prompts we have employed are widely recognized and validated in the field of machine translation, as evidenced by previous research ([1], [2], [3]). Nonetheless, we agree that exploring alternative prompts could potentially yield even better translation outcomes! Hence, we re-train ALMA-7B-LoRA model with **two new prompts** as suggested in [4]:\n\nRecap that:\n- Our prompt is: Translate this from <src> to <tgt>:\\n<src>: <input>\\n <tgt>:\n- New Prompt 1: <src>: <input>\\n<tgt>:\n- New Prompt 2: <input> Translate this from <src> to <tgt>:\n\n|                | Avg. en>xx |       | Avg. xx>en |       |\n|----------------|------------|-------|------------|-------|\n| Prompt (for ALMA-7B-LoRA) | BLEU  | Comet22 | BLEU  | Comet22 |\n| Our prompt               | **29.78** | **86.80**   | **34.34** | **84.24**   |\n| New prompt 1             | 27.63 | 85.44   | 33.434| 83.60   |\n| New prompt 2             | 27.44 | 85.31   | 33.314| 83.85   |\n\nWe conducted evaluations of the two new prompts using the WMT\u201922 test set and report the average scores for both translation directions. The performance of both new prompts is comparable, though they slightly underperform compared to our originally proposed prompt.\n\nReference: \n\n[1] Hendy A, Abdelrehim M, Sharaf A, Raunak V, Gabr M, Matsushita H, Kim YJ, Afify M, Awadalla HH. How good are gpt models at machine translation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210. 2023 Feb 18.\n\n[2] Chen Y, Liu Y, Meng F, Chen Y, Xu J, Zhou J. Improving Translation Faithfulness of Large Language Models via Augmenting Instructions. arXiv preprint arXiv:2308.12674. 2023 Aug 24.\n\n[3] Zeng J, Meng F, Yin Y, Zhou J. Tim: Teaching large language models to translate with comparison. arXiv preprint arXiv:2307.04408. 2023 Jul 10.\n\n[4] Zhang B, Haddow B, Birch A. Prompting large language model for machine translation: A case study. arXiv preprint arXiv:2301.07069. 2023 Jan 17.\n\n**Regarding your Concern about evaluation metric:**\nThank you for your constructive feedback. We acknowledge that finding reliable evaluation metrics remains a challenging aspect of machine translation research. During our experiments, we reported results using COMET-22 and BLEU, which were the predominant metrics at that time. These metrics are not only widely accepted but also facilitate easy comparison within the research community. To address your concerns, we have also evaluated our ALMA models using three additional automatic metrics:\n\nThank you for recommending InstructScore, which is an intriguing automatic evaluation metric. We observed that the prompt for this metric (found at https://github.com/xu1998hz/SEScore3/blob/586f7f22ced93b136cbde818e86fbddcdd8580ce/InstructScore.py#L67C19-L67C19) is specifically tailored for translations from Chinese to English. Therefore, we limited our evaluation of the Instructscore to this language pair. The results, as detailed in the table below, demonstrate that our method surpasses GPT-3.5-D in performance, with higher scores indicating better results.\n\n|             | zh->en (Instructscore) |\n|-------------|------------------------|\n| GPT-3.5-D   | -5.2944                |\n| ALMA-13B-LoRA | **-4.5856**              |\n\nTo provide a more thorough assessment using recently released evaluation metrics, we conducted evaluations using wmt23-cometkiwi-da-xxl, and Unbabel/XCOMET-XXL. Both of these are 10 billion parameter models that support all languages. The results, as presented in the table below, indicate that our method continues to outperform GPT-3.5-D when evaluated with these latest metrics.\n\n|              | Avg. xx->en |       |            |\n|-|---|--|---|\n|              | Comet22     | wmt23-cometkiwi-da-xxl | XCOMET-XXL |\n| GPT-3.5-D    | 83.90       | 80.06 | 84.59      |\n| ALMA-13B-LoRA| **84.59**       | **81.50** | **86.74**      |\n\n\n|              | Avg. en->xx |       |            |\n|------|---|-------|------|\n|              | Comet22     | wmt23-cometkiwi-da-xxl | XCOMET-XXL |\n| GPT-3.5-D    | 84.59       | 75.99 | 88.10      |\n| ALMA-13B-LoRA| **87.00**       | **82.66** | **92.76**      |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4491/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263327226,
                "cdate": 1700263327226,
                "tmdate": 1700263327226,
                "mdate": 1700263327226,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hhs5xwysYw",
                "forum": "farT6XXntP",
                "replyto": "acjNM2Jq3j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4491/Reviewer_TMW7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4491/Reviewer_TMW7"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for their patient responses and the extensive experimental validations provided for the questions raised. Although the responses are somewhat cautious, they adequately address the issues raised.\n\nI have revised my rating to an 8, and I believe the paper is worthy of acceptance."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4491/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661009990,
                "cdate": 1700661009990,
                "tmdate": 1700661009990,
                "mdate": 1700661009990,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RzDrUF628F",
            "forum": "farT6XXntP",
            "replyto": "farT6XXntP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4491/Reviewer_AH62"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4491/Reviewer_AH62"
            ],
            "content": {
                "summary": {
                    "value": "This paper examines ways of improving the translation performance of large language models. There is no modelling or training innovation in the paper, but they are the first to show that you can take a smaller English focussed language model (LLaMa 7B,13B) and make it into a translation model with equivalent performance to a large LLM (GPT 3.5 &4) using relatively small amounts of monolingual (20B?) and parallel data (1B) in a fine-tuning step. The paper is generally clearly written but some important details are missing, some claims are not entirely supported, and there are some typos. \n\nIn more detail:\nThey first show that large models (GPT3.5) do extremely well translating into and out of  5 diverse languages and English, mostly high resource except for Icelandic. They then select the best performing smaller model (7B parameters) and experiment on fine-tuning to the translation task using instructions. They show that LLama27B quickly maxes out after 10k examples, whereas MPT-7B continues improving until the final datapoint at 20M. \nThey then experiment with using monolingual data in pretraining, and finetuning with instructions, and call the resulting model ALMA. They show that with a relatively small multilingual (but not parallel) pretraining dataset 1B, they get significant gains in translation quality. Also they used high quality parallel data for finetuning and experimented with full fine-tuning and LORA fine-tuning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "They show large improvements in the translation capabilities of the most useful size of models (7B,13B) with very affordable limited fine-tuning and data. \nThis is a useful paper for people working in machine translation to see what works in fine-tuning large language models for the translation task."
                },
                "weaknesses": {
                    "value": "There is not a lot of novelty in the approach - either in training or modelling. I am not sure that the \"New paradigm\" title is justified. \nI have not learned much from reading the paper - it is still not clear what the contribution of the monolingual vs parallel training data is. It is also not clear whether the good performance of the trained models is due to the reduced number of non English languages (5) vs other models (NLLB, GPT3.5,4). \nI am also not sure these results (improvement over LLaMa7B with fine-tuning) would hold if you used few-shot - and it would have been a very easy experiment to conduct. \nThe paper writing is not particularly clear (see questions for details)."
                },
                "questions": {
                    "value": "They claim/state things which are either not entirely correct or are overstated: \n\"Both monolingual data fine-tuning and human-written data\" - I think they mean parallel data here - both monolingual and translated data are human-written. \nThis claim is not correct: \"We demonstrate that LLMs, such as LLaMA-2-7B, do not voraciously consume parallel data. \" What they demonstrate is that for fine-tuning LlaMa-2-7B does not improve much beyond 10k examples. However, their other model MPT-7B does keep improving and does not max out even at the 20M mark.\nThe citation for this claim in the intro: \"they still fall short in translation for low-resource languages\" Zhang et al. is wrong. They do not look at low resource languages, only experiments with English, Chinese and German. \nThey conclude: \"From our observations, LLMs should not adopt the same training approach as earlier models\u2014whether randomly initialized or pre-trained\u2014that rely heavily on vast amounts of training data\" but do not specify that this is just for the fine-tuning LLaMa - it is a too broad claim to make.\n\nSome things are not explained or described:\nThey do not explain why they selected MPT for experiments in 3.2, and more importantly they do not discuss why it performs contrary to their claimed results - that LLMs to not voraciously consume parallel data. \nAlso for Section 5 they do not say how much parallel data is used for fine-tuning and what ratios of parallel data are we using - same as the monolingual data? This really should be detailed in the main paper. \nThis caption is confusing \"Figure 5: The average performance of ALMA-7B at the completion of each 1B-token fine-tuning\". Is this without the instruction fine-tuning? How do these numbers compare to Tables 1 and 2? I can't figure this out due to the differences in the data - but it seems like the instruction fine-tuning makes little/no difference here. (Fig5) 85.28 COMET vs. (84.12 + 86.27) / 2 (Table 1 and 2) for the ALMA7B?\n\nThere are a number of typos and inconsistencies that need to be polished for final submission: \nThe ALMA models are called AMLA,\nTypo:  \"As expected, it tends up with a similar performance \". \nThe graphics are not very consistent in the paper and don't look clean or very legible. Figure 5 is particularly  hard to read.  \nResults are not very structured. For some (2.2) use NLLB and other not (3.1) .\nMany different result formats vertical/horizontal bar, line, table - it is harder to read and looks messy."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4491/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771177041,
            "cdate": 1698771177041,
            "tmdate": 1699636424990,
            "mdate": 1699636424990,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ovepv9PfTt",
                "forum": "farT6XXntP",
                "replyto": "RzDrUF628F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4491/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4491/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Rebuttal 1 by Authors"
                    },
                    "comment": {
                        "value": "We genuinely appreciate the valuable feedback provided by the reviewer and have addressed them in a point-by-point manner below. We are more than willing to engage in further discussions with the reviewers should any follow-up questions arise.\n\nIn responding to your concerns, we wish to first address a potential misunderstanding. The ALMA models underwent a two-stage fine-tuning process. Initially, they were fine-tuned using monolingual data, with ALMA-7B being trained on 20 billion tokens and ALMA-13B on 12 billion tokens. Next, a fine-tuning was conducted on a smaller set of human-written parallel data. This parallel dataset comprised **58,000 parallel sentences spanning 10 languages**. We believe there is a misalignment between our description of this process and the reviewer's interpretation. Our intention here is to clarify this point to prevent any further misunderstanding.\n\n**Regarding unclear parts in *Weakness***\n>There is not a lot of novelty in the approach - either in training or modelling. I am not sure that the \"New paradigm\" title is justified\n\nWe are grateful for your feedback and would like to provide further clarification. \u201cThe new paradigm\u201d specifically addresses machine translation in a new method, rather than proposing a general methodology. We acknowledge that the concepts of 'modeling' (using a decoder-only model) and 'training' (through causal language modeling) are not novel in themselves. However, applying these techniques to machine translation is a significant innovation (Note that we discuss three \u2018training\u2019 method for translation: casual language modeling, prefix language modeling, and mixture of denoisers in Appendix A, but the simplest CLM performs the best).  Traditionally, machine translation has primarily relied on processing **vast quantities (millions)** of parallel sentences using an **encoder-decoder** architecture. The application of decoder-only models to this field is basically unexplored and has rarely outperformed the conventional encoder-decoder approach.\n\nThe remarkable performance of models like ChatGPT has sparked interest in decoder-only large language models (LLMs) for various applications. There must be potential for machine translation. However, most recent study on machine translation for LLMs fails to achieve comparative performance to conventional encoder-decoder models. This area remains largely untapped, posing critical questions:\n\n- How should we train decoder-only models for translation tasks which were traditionally training on encoder-decoder architecture?\n- Given that LLMs are already extensively trained, is there still a need for large volumes of parallel data for additional training? Could this potentially impair the model's effectiveness?\n- Considering that LLMs are predominantly trained in English, how can we enhance their proficiency in other languages to improve translation quality?\n\nOur paper seeks to address these questions, proposing a novel and efficient training methodology for machine translation. **We are the first to utilize 7B/13B LLMs for machine translation, and achieve performance comparable to state-of-the-art encoder-decoder models.** This breakthrough lays the groundwork for future research in integrating LLMs with machine translation, marking an exciting new direction in the field.\n\n> it is still not clear what the contribution of the monolingual vs parallel training data is\n\nThank you for the opportunity to further elucidate the roles of monolingual and parallel data in our training process. Although these aspects are presented in Table 3 of our ablation study, we recognize the need for additional clarity:\n\n- Contribution of Monolingual Data: As indicated in Table 3, when the same parallel data fine-tuning is applied (or when no parallel data training is used), models fine-tuned with monolingual data demonstrate notable improvements. This is particularly evident in translations from English to other languages (e.g., in Table 3, compare row 4 with row 8, where the COMET score for **en\u2192xx translation jumps from 76.52 to 86.49)**.\n\n- Contribution of Parallel Data: Also detailed in Table 3, models exhibit enhanced performance when trained with higher-quality parallel data,  under the constraint that the monolingual data training remains constant (or is absent). This trend is observable as we move from row 2 to row 4 in the table. For instance, the **COMET score for en\u2192xx translation progressively increases (73.89 in row 2, 74.35 in row 3, and 76.52 in row 4).**\n\nWe are grateful for the feedback highlighting this ambiguity. In our revised manuscript, we will ensure a more comprehensive and explicit discussion of these findings."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4491/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700262175740,
                "cdate": 1700262175740,
                "tmdate": 1700262175740,
                "mdate": 1700262175740,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XozJtG57x1",
            "forum": "farT6XXntP",
            "replyto": "farT6XXntP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4491/Reviewer_vUYo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4491/Reviewer_vUYo"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on improving the translation capability of large language models (LLMs). The authors find that LLMs do not require a large amount of parallel data as traditional models do to achieve decent translation quality. Accordingly, they propose a new training recipe including two stages: firstly finetune LLM on monolingual data and then finetune the model on a small set of high-quality parallel data. Experiments with LLAMA-2 show promising performance even on par with GPT-3.5."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) Propose a simple training recipe for LLMs for translation tasks: finetuning first on monolingual data and then on small high-quality parallel data.\n2) Demonstrate impressive performance across 5 language pairs with LLAMA-2."
                },
                "weaknesses": {
                    "value": "1) The statement of \"paradigm shift\" is somehow overestimated.\n2) The few-shot prompting results are highly undervalued.\n3) The proposed recipe might not apply to other LLMs and languages."
                },
                "questions": {
                    "value": "Firstly, the authors claim the finding/proposal is a \"paradigm shift\" as highlighted in the title, which might be inadequate. The recipe generally follows the pretraining-finetuning paradigm which has already been well-established since BERT and mBart/T5. In addition, similar solutions have already been used in prior studies, such as BigTranslate.\n\nSecondly, the few-shot results are largely undervalued. As shown in Table 11, the quality gap between the HW 5-shot and the proposal is only 0.2 COMET on XX->En, although the finetuning used a large amount of monolingual corpus and more parallel data. Few-shot performance should be able to be further enhanced via beam search and optimized prompt construction, and it should be used as the fair baseline rather than 0-shot prompting. It's also misleading to state in Appendix I that \"ICL substantially underperforms our stage-2 finetuning\". \n\nLastly, finetuning performance is highly dependent on pretraining conditions and the downstream task. Intuitively, when the downstream tasks highly correlate with the pretraining, the demand for a large volume of the supervised corpus is reduced as highlighted in this paper. However, as the MPT-7B performance indicates in Figure 4, when the correlation is low, adding more supervised data is almost always helpful. In other words, the findings in this paper might not be generalization. Do you also have the finetuning results for MPT-7B? Does the recipe also apply to low-resource languages like Gu?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4491/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816064065,
            "cdate": 1698816064065,
            "tmdate": 1699636424914,
            "mdate": 1699636424914,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0Sy37eid3K",
                "forum": "farT6XXntP",
                "replyto": "XozJtG57x1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4491/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4491/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Rebuttal 1 by Authors"
                    },
                    "comment": {
                        "value": "We genuinely appreciate the valuable feedback provided by the reviewer and have addressed them in a point-by-point manner below. We are more than willing to engage in further discussions with the reviewers should any follow-up questions arise.\n\n**Regarding your concern about statement:**\n>The statement of \"paradigm shift\" is somehow overestimated.\n\nWe are grateful for your feedback and would like to provide further clarification. \u201cThe new paradigm\u201d specifically addresses machine translation in a new method, rather than proposing a general methodology. While the general concept of \"pre-training + fine-tuning\" is indeed widely used in NLP tasks, it represents a broad strategy rather than a specific methodology. We acknowledge that the concepts of 'modeling' (using a decoder-only model) and 'training' (through causal language modeling) are not novel in themselves. However, applying these techniques to machine translation is a significant innovation (Note that we discuss three \u2018training\u2019 method for translation: casual language modeling, prefix language modeling, and mixture of denoisers in Appendix A, but the simplest CLM performs the best).  Traditionally, machine translation has primarily relied on processing vast quantities (millions) of parallel sentences using an encoder-decoder architecture. The application of decoder-only models to this field is basically unexplored and has rarely outperformed the conventional encoder-decoder approach.\n\nThe remarkable performance of models like ChatGPT has sparked interest in decoder-only large language models (LLMs) for various applications. There must be potential for machine translation. However, most recent study on machine translation for LLMs fails to achieve comparative performance to conventional encoder-decoder models. This area remains largely untapped, posing critical questions:\n\n- How should we train decoder-only models for translation tasks which were traditionally training on encoder-decoder architecture?\n- Given that LLMs are already extensively trained, is there still a need for large volumes of parallel data for additional training? Could this potentially impair the model's effectiveness?\n- Considering that LLMs are predominantly trained in English, how can we enhance their proficiency in other languages to improve translation quality?\n\nOur paper seeks to address these questions, proposing a novel and efficient training methodology for machine translation. We are the first to utilize 7B/13B LLMs for machine translation, and achieve performance comparable to state-of-the-art encoder-decoder models. This breakthrough lays the groundwork for future research in integrating LLMs with machine translation, marking an exciting new direction in the field.\n\nWe recognize that previous studies, such as BigTranslate, have implemented similar fine-tuning approaches. However, there are significant differences between their methods and ours, particularly in acknowledging the negative impacts of excessive parallel data and the importance of data quality. Their approach resulted in performances that did not meet our expectations. Our goal is to provide an accurate and effective training \"recipe\" for decoder-only LLMs in machine translation, ensuring high performance. In our revised manuscript, we commit to clearly delineating our method's relationship to the broader \"pre-training and fine-tuning\" approach, emphasizing the unique aspects of our strategy and avoiding any unintentional overstatements."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4491/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261467056,
                "cdate": 1700261467056,
                "tmdate": 1700261467056,
                "mdate": 1700261467056,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ls28hhc4xO",
            "forum": "farT6XXntP",
            "replyto": "farT6XXntP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4491/Reviewer_TXCQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4491/Reviewer_TXCQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper is motivated by that the translation performance of LLMs are not as good as other tasks compared to the task-specific methods. To improve the translation capabilities of the moderate LLMs, it proposes a fine-tuning paradigm which is firstly fine-tuning on monolingual data followed by subsequent fine-tuning on a small set of high-quality parallel data. It turns out a huge gain in translation quality compared to the zero-short performance across 10 translation directions from the WMT21 and WMT22."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper is clearly written and provides many insights. Using LLM to boost the translation quality is an interesting and important topic. It proposes a novel fine-tuning paradigm to let the moderate size LLMs better at translation. Many analyses should be very helpful to the NLP and ML community."
                },
                "weaknesses": {
                    "value": "The paper is mainly focusing on improve the translation quality of LLMs. It'd be better to compare more with the encoder-decoder translation models and shed light on the best practice of translation itself."
                },
                "questions": {
                    "value": "How's your fine-tuned models compared with the dedicated encoder-decoder based translation models in similar size? Please discuss in both high and low resource settings.\n\nIf fine-tuning with large amount of parallel data is not optimal, how about using the during pre-training phase? If targeting at the highest translation performance, what's your take? Please include the dedicated translation model in the discussion.\n\nWhat would be the results of evaluating on some out-of-domain test sets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4491/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816641336,
            "cdate": 1698816641336,
            "tmdate": 1699636424817,
            "mdate": 1699636424817,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z6XQrx9LBK",
                "forum": "farT6XXntP",
                "replyto": "Ls28hhc4xO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4491/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4491/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Rebuttal 1 by Authors"
                    },
                    "comment": {
                        "value": "We genuinely appreciate the valuable feedback provided by the reviewer and have addressed them in a point-by-point manner below. We are more than willing to engage in further discussions with the reviewers should any follow-up questions arise.\n\n**Regarding your question about dedicated encoder-decoder model**:\n>How's your fine-tuned models compared with the dedicated encoder-decoder based translation models in similar size?\n\nThank you for your insightful question! Most online translation models are generally smaller, often adhering to the transformer-big architecture [1], which results in around 300 million parameters. We'd like to clarify that in our paper, the NLLB-200 model serves as a benchmark for one of the strongest encoder-decoder based translation models available that is comparable in size to our ALMA models. The NLLB-54B's base model has 3 billion parameters, with additional parameters derived from a Mixture of Experts (MoE) approach. Our research demonstrates that for high-resource languages such as Chinese, German, Russian, Czech, and the low-resource language Icelandic, our ALMA models either match or outperform the dedicated encoder-decoder model NLLB-54B. However, we acknowledge that for very low-resource languages, where even monolingual data is limited, further research is required to optimize positive cross-lingual transfer. We plan to explore these avenues in future studies.\n\nReference:\n[1] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser \u0141, Polosukhin I. Attention is all you need. Advances in neural information processing systems. 2017;30.\n\n**Regarding your question about using parallel data for pre-training:**\n>If fine-tuning with large amount of parallel data is not optimal, how about using them during the pre-training phase?\n\nThank you for your intriguing question. In fact, at an early stage in our research, we experimented with treating parallel data as monolingual data during the pre-training phase. Specifically, we used 20 million English-Russian parallel sentences, amounting to a total of 1.8 billion tokens. Our initial approach involved fine-tuning the LLaMA-2-7B model on these 1.8 billion tokens, mimicking the pre-training process, followed by further fine-tuning on English to Russian translation using the Flores development and test datasets (comprising approximately 2,000 sentences in total). To ensure a fair comparison, we also conducted an experiment where we fine-tuned LLaMA-2-7B on 1.8 billion tokens from the Oscar dataset, consisting of both English and Russian data, and this was also followed by fine-tuning on the same English-Russian parallel data. The outcomes of these comparative tests are presented below:\n\n|                              | BLEU  | Comet22 |\n|------------------------------|-------|---------|\n| Parallel data as monolingual data | 25.26 |  85.29  |\n| Oscar as monolingual data    | **26.43** |  **86.36**  |\n\nThe results indicate that treating parallel data as monolingual data is beneficial, but it does not yield as favorable outcomes as using authentic monolingual data, such as that from the Oscar dataset. We hypothesize that the advantage of using Oscar's monolingual data lies in its document-level context, which enables ALMA models to learn more natural expressions in Russian. In contrast, treating parallel data as monolingual data limits learning to sentence-level information, which may not provide the same depth of linguistic understanding and fluency."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4491/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261171026,
                "cdate": 1700261171026,
                "tmdate": 1700261171026,
                "mdate": 1700261171026,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]