[
    {
        "title": "Ensemble Distillation for Unsupervised Constituency Parsing"
    },
    {
        "review": {
            "id": "DqVUerbaeF",
            "forum": "RR8y0WKrFv",
            "replyto": "RR8y0WKrFv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6909/Reviewer_BQVw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6909/Reviewer_BQVw"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers ensembling unsupervised (unlabled binary) constituency parsers. The main technical piece is an MBR decoding algorithm that finds the highest-F1 tree with respect to a set of candidate trees. Experiments show that this ensembling method is more effective than simply selecting a max-intra-F1 candidate tree or training a student model on the union of candidate trees (union distillation)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Simple but insightful observation that it is possible to \"generate\" a tree from candidate trees\n- Derivation of the hit-count maximization algorithm \n- Strong results"
                },
                "weaknesses": {
                    "value": "- Somewhat narrow scope (ensembling constituency trees)\n- Some details missing (see the questions)"
                },
                "questions": {
                    "value": "Would \"Our ensemble (X teacher across runs)\" in Table 2 use candidate trees from different models, or do they all end up from the same model (e.g., ConTest for X=\"best\")? It's good to know the answer to this question because one of the main claims of the paper is that it's important to exploit the large qualitative differences between different methods (Table 1). The fact that we get the best result by just using outputs from the same model seems to refute that claim (i.e., it's all variance reduction)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6909/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698611548357,
            "cdate": 1698611548357,
            "tmdate": 1699636803837,
            "mdate": 1699636803837,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S6ehQNNenE",
                "forum": "RR8y0WKrFv",
                "replyto": "DqVUerbaeF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6909/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the review.\n\n\n> Weakness 1: Somewhat narrow scope (ensembling constituency trees)\n\nUnsupervised constituency parsing, in fact, fits ICLR themes well, as it concerns **learning the representations** of human language. In the literature, a number of impactful papers were published in previous editions of ICLR, such as Shen et al., 2018, 2019.  \n\nIn addition, our study has general machine learning implications, especially for multi-teacher distillation. Our study shows that a straightforward union distillation from multiple teachers (as was done in previous work) may not yield improved performance, whereas our ensemble-then-distill approach is able to alleviate the over-smoothing issue in traditional methods. We\u2019re happy to explore multi-teacher distillation for different data types (such as sequences and graphs), mentioned in Future Work. \n\n\n> Weakness 2: Some details missing (see the questions)\n>\n> Questions: Would \"Our ensemble (X teacher across runs)\" in Table 2 use candidate trees from different models, or do they all end up from the same model (e.g., ConTest for X=\"best\")? It's good to know the answer to this question because one of the main claims of the paper is that it's important to exploit the large qualitative differences between different methods (Table 1). The fact that we get the best result by just using outputs from the same model seems to refute that claim (i.e., it's all variance reduction).\n\nThanks for the detailed question. By \u201cBest/Worst teacher across runs\u201d, we mean that we must still choose exactly one run for a model, but which run may not correspond among models. A simple example:\n\nModel1: Run1=85, Run2=90, Run3=95\n\nModel2: Run1=65, Run2=60, Run3=55\n\nThen, the best teachers across runs will be Model1-Run3 and Model2-Run1. The worst teachers across runs will be Model1-Run1 and Model2-Run3. \n\nWe\u2019ve clarified this in the revision (third paragraph in Sec 3.3)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244978038,
                "cdate": 1700244978038,
                "tmdate": 1700271670003,
                "mdate": 1700271670003,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1cn0NA9fev",
            "forum": "RR8y0WKrFv",
            "replyto": "RR8y0WKrFv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6909/Reviewer_mYw3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6909/Reviewer_mYw3"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a consistency-based decoding method for unsupervised constituency parsing, which can also be formulated as minimum Bayes risk decoding.\nExperiments demonstrate significant improvement over existing methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Significant contribution to unsupervised constituency parsing, including a generative MBR process and consistently improved results.\n\n- Comprehensive analyses are carefully conducted and presented.\n\n- The paper is very well written and easy to follow.\n\nI also wanted to note the statements and conclusions in this paper are remarkably honest. \nMost of the claims are very well supported by related work and experimental results. \nWhile this is generally not considered as a strength, such a presentation style should be commended and encouraged in recent years."
                },
                "weaknesses": {
                    "value": "- MBR-style or consistency-based methods have also been applied to parsing for decoding or model selection, but the authors failed to recognize and discuss them. To name a few, [Smith and Smith, 2007](https://aclanthology.org/D07-1014.pdf) and [Zhang et al., 2020](https://aclanthology.org/2020.acl-main.302.pdf) used MBR-decoding to improve dependency parsing; [Shi et al., 2019](https://aclanthology.org/P19-1180.pdf) adapted an agreement-based model selection process for distantly supervised constituency parsing.\n\n- The motivation is not completely convincing. Recent trends in NLP demonstrate that explicit parses might not be crucial or even necessary to many user-facing applications (e.g., GPT models do not really use explicit language structures), which contradicts the first sentence in this paper (*Constituency parsing is a core task in natural language processing*). Traditionally, such structures served as a backbone for many NLP models, and the prediction of them was therefore referred to as *core NLP*. I am not sure if the parsing is as important as what I receive from this paper. Please consider revising or including more justification. \n\n\n**Minor points on weaknesses**\n\n- The references in this paper, especially to conventional linguistic literature, need some work:\n    - Section 1: Carnie (2007) and Fromkin et al. (2003) are introductory books. Both should be changed to Chomsky (1957). Syntactic Structures.\n    - [Spitkovsky et al. (2013)](https://aclanthology.org/D13-1204.pdf) is worth a mention in the related work section.\n\n- Page 1: low correlation among different unsupervised parsers: [Williams et al. (2018)](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00019/43445/Do-latent-tree-learning-models-identify-meaningful) discovered a similar issue within the same model architectures (Table 1, right settings).\nThis is worth a discussion.\n\n- Not really a weakness for a machine learning conference submission: since the topic of this paper is highly linguistic, I am willing to see a detailed analysis of what patterns are fixed. For example, do NPs/PPs with rare words receive more fixes than those with frequent words, or the opposite, or not significant? Do VPs with transitive verb heads receive more fixes than those with intransitive verb heads, or the opposite, or insignificant? Does the student extract any constituent that does not receive any vote from teacher models, due to fixes on shorter spans and CYK?\n\nI am being conservative in my initial evaluation and am happy to increase my rating if most of the above issues are fixed."
                },
                "questions": {
                    "value": "- Which split of PTB did you use to generate the statistics in Table 1?\n- Table 2: why are the +RNNG/+URNNG oracle performance different from the basic one (83.3)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6909/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698713820157,
            "cdate": 1698713820157,
            "tmdate": 1699636803725,
            "mdate": 1699636803725,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7YFrZlzhbK",
                "forum": "RR8y0WKrFv",
                "replyto": "1cn0NA9fev",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6909/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the support, especially for recognizing the scientific rigor of our study by mentioning \u201cthe statements and conclusions in this paper are remarkably honest. Most of the claims are very well supported by related work and experimental results.\u201d\n\n\n> Weakness 1 (MBR-style or consistency-based literature review)\n\nThanks for sharing the literature. We have included the suggested references, as well as some of their references, in the revision. \n\n\n> Weakness 2 (Motivation of core NLP)\n\nWe\u2019d clarify that, by saying \u201ca core task,\u201d we mean that parsing has been traditionally referred to as a core NLP task. In the revision, we call parsing \u201ca well-established task.\u201d\n\nNevertheless, we would like to point out the significance of unsupervised parsing as a curious task of language structure discovery. It verifies linguistic theory, showing that linguistically defined constituents can naturally emerge in an unsupervised way. In our related work, we also discussed how unsupervised parsing may inspire the structure discovery of motion-sensor data (Peng et al., 2011).\n\n> Minor points on weaknesses 1 (conventional linguistic literature references)\n\nWe\u2019re grateful for the suggestions and have included them in the revision. \n\n> Minor points on weaknesses 2 (Related work about the low correlation discovery)\n\nThanks for the suggestion. The mentioned work is discussed and cited in the revision. Williams et al., 2018 showed low correlations among early latent-tree models that were claimed to induce task-specific tree structures, whereas our paper shows low correlation among unsupervised parsing models that are aimed at discovering linguistically plausible tree structures. Further, our paper shows that such low correlation suggests different expertise of unsupervised parsers, which can be utilized by model ensembles for performance improvement.  \n\n> Minor points on weaknesses 3: Detailed linguistic analysis of what patterns are fixed\n\nIn the performance-by-type analysis, we\u2019ve shown that our approach achieves consistently high performance across all constituency types. We\u2019re further presenting a case study in the appendix, suggesting that our ensemble indeed performs voting for local structures and fixes teachers\u2019 predictions.\n\n> Question 1: Which split of PTB did you use to generate the statistics in Table 1?\n\nWe used the standard split, namely, Section 23 of PTB for tests.\n\n> Question 2: why are the +RNNG/+URNNG oracle performance different from the basic one (83.3)?\n\nIn Table 2, the +RNNG column for Oracle row (Row 15) is, in fact, a supervised RNNG trained on the binarized PTB-training set (binarized ground-truth). +URNNG is further tuning the RNNG in an unsupervised manner. The performance is lower than Oracle, which is the performance upper bound."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244733153,
                "cdate": 1700244733153,
                "tmdate": 1700244733153,
                "mdate": 1700244733153,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QWoDiESOd1",
            "forum": "RR8y0WKrFv",
            "replyto": "RR8y0WKrFv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6909/Reviewer_ZbQV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6909/Reviewer_ZbQV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an ensembling approach for the task of unsupervised constituency parsing. Having trained a group of various prior models, they use a dynamic program to find the \u201caverage tree\u201d of their predictions. While this approach works well, it can then be further used for distillation into an RNNG which works more efficiently and in some settings more accurately. They also analyze to what extent the improvements in performance from ensembling are down to smoothing vs. combining expertise."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The insight that different models are weakly correlated despite similar F1 is interesting and well motivates the approach\n- The proposed dynamic program is intuitive and well explained\n- Experiments are strong and thorough\n- The analysis of gains from denoising vs. difference in expertise is well conducted"
                },
                "weaknesses": {
                    "value": "- The fact that the F1 gains from distillation do not carry over to the out of domain setting is a drawback and somewhat underexplored\n- There is a lack of qualitative analysis of the types of behaviors that different model types exhibit, and how ensembling actually combines those. Some of this is done in the Appendix but it would be nice to see specific examples in the main paper, especially since that analysis is wrt constituency labels which the model isn\u2019t actually being evaluated on."
                },
                "questions": {
                    "value": "- How does regular RNNG perform, and why not use it as a teacher?\n- Regarding the experiment in Figure 1, do you see similar results if you measure the gains from the *distilled* ensemble? That would be useful to see alongside"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6909/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6909/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6909/Reviewer_ZbQV"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6909/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817023315,
            "cdate": 1698817023315,
            "tmdate": 1699636803541,
            "mdate": 1699636803541,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HQgkzlVUnh",
                "forum": "RR8y0WKrFv",
                "replyto": "QWoDiESOd1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6909/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for recognizing the contributions of our work and strongly supporting our paper.\n\n> Weakness 1. not carrying distillation boost under domain shift\n\nThanks for the insight! We acknowledge that distillation does not help our ensemble, or even the supervised model, in the SUSANNE experiment. To be honest, this is also a little bit unexpected to us as well, but we have honestly stated that in the paper (left to the caption of Table 3). We\u2019re happy to explore domain adaptation of unsupervised parsing more in future work. \n\n\n> Weakness 2: There is a lack of qualitative analysis of the types of behaviors that different model types exhibit, and how ensembling actually combines those. Some of this is done in the Appendix but it would be nice to see specific examples in the main paper, especially since that analysis is wrt constituency labels which the model isn\u2019t actually being evaluated on.\n\nThanks for the suggestion. We have included a case study in the revision (which inevitably overflows to the appendix due to the volume and substance of our paper).\n\n\n> Question 1: How does regular RNNG perform, and why not use it as a teacher?\n\nRNNG does not get trained well from scratch in an unsupervised manner, which is also reported in previous work [Kim et al., 2019a; Cao et al., 2020] and mentioned in Footnote 4 on Page 6. As a result, we did not use RNNG/URNNG as a teacher.\n\n\n> Question 2: Regarding the experiment in Figure 1, do you see similar results if you measure the gains from the distilled ensemble? That would be useful to see alongside\n\nThanks for the suggestion. Distillation analysis is expensive, as it requires performing inference for all teachers on the training data, training RNNG, and then refining with URNNG. For Figure 1, it requires 21 teachers' inference over the training set and training 14 RNNG/URNNG models, which may be unaffordable to us. \n\nNevertheless, we expect the phenomenon to hold for RNNG/URNNG, because during our development, we observed similar patterns among RNNG, URNNG, and their ensemble teacher. The F1 scores are also similar, with RNNG slightly lower and URNNG slightly higher, consistent with all PTB experiments."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244097243,
                "cdate": 1700244097243,
                "tmdate": 1700244097243,
                "mdate": 1700244097243,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3yr4D5LZl1",
                "forum": "RR8y0WKrFv",
                "replyto": "HQgkzlVUnh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6909/Reviewer_ZbQV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6909/Reviewer_ZbQV"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed clarifications! Considering the overall scope of the paper and its contributions I don't feel that I can raise my score any higher, but I appreciate your response."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691404722,
                "cdate": 1700691404722,
                "tmdate": 1700691404722,
                "mdate": 1700691404722,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7O8N18WngL",
            "forum": "RR8y0WKrFv",
            "replyto": "RR8y0WKrFv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6909/Reviewer_QZms"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6909/Reviewer_QZms"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel strategy following an ensemble-then-distill paradigm to deal with the unsupervised constituency parsing task that aims to hierarchically structure sentences without relying on linguistically annotated data. The proposed approach firstly ensembles existing unsupervised parsers based on the notion of \u201ctree averaging\u201d and then conducts distillation to create a student model. This technique efficiently alleviates the over-smoothing issue that frequently arises in multi-teacher distillation. Experimental results indicate that such an ensemble-then-distill method outperforms existing approaches with superior effectiveness and robustness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The major contributions include:\n\n1.      A new notion of tree averaging and the corresponding search algorithm: CYK variant\n\n2.      Ensemble-then-distill approach that trains a student parser from an ensemble of teachers.\n\n3.      The inference time of the student model is 18x faster than the ensemble method.\n\n4.      A hypothesis that different unsupervised parsers capture different aspects of the language structures and the verification with experiments."
                },
                "weaknesses": {
                    "value": "1. Lack of clarification regarding the methodology design.\n\n- The averaging tree is derived with the highest total F1 score compared with different teachers. Have the authors tried other methods of calculating the similarities between trees? Perhaps a fair comparison is needed to further indicate the effectiveness of the proposed tree averaging method.\n- The authors did not provide a detailed explanation for choosing the seven unsupervised parsers introduced in Section 3.2 as teacher models. For instance, why did the authors select ContextDistort as one of the teachers despite its relatively inferior performance and inference efficiency?\n\n2. The authors state in Introduction that combining the different parsers may leverage their different expertise. The authors attempt to verify this statement in Section 3.4 by comparing two settings: the ensemble of three runs of the same model and that of three heterogeneous models. I wonder if it is more appropriate to choose the model with highest performance in the former setting so that it can be further validated there exists additional boost due to different expertise (e.g., Neural PCFG for Group 2).\n3. The writing can be improved. There are some typos and unclear descriptions. Please refer to comments for detail.\n\n\nComments\n\n1. Minor comments on writing:\n(1)\tParagraph #1 in Introduction: ...to explore unsupervised methods as it eliminates... -> ...to explore unsupervised methods as they eliminate...\n(2)\tParagraph #1 in Section 3.1: ... on the widely used the Penn Treebank -> ... on the widely-used Penn Treebank"
                },
                "questions": {
                    "value": "Despite these merits, there are some points which need further clarification, and some suggestions.\n\n1.      The ensemble method demonstrates its effectiveness on PTB. However, in 2020, the F1 score of CRF parser on PTB variants had already been above 90 (Zhang et al., 2020). There is indeed a performance boost in comparison to oracle score (the highest possible F1 score of binarize groundtruth trees). But it is not appropriate to claim that \u201clargly bridging the gap between supervised and unsupervised constituency parsing\u201d on Page 6.\n\n2.      In Results on SUSANNE, the authors claim that \u201cThis is a realistic experiment to examine the models\u2019 performance in an unseen low-resource domain.\u201d However, SUSANNE is an English dataset. In CoNLL Shared task, there are tree-banks on low-resource languages (Zeman et al., 2017). It\u2019s better that the authors can demonstrate the effectiveness of the approach on some of these low-resource language datasets.\n\nReferences:\n\nZeman, D., Popel, M., Straka, M., Hajic, J., Nivre, J., Ginter, F., Luotolahti, J., Pyysalo, S., Petrov, S., Potthast, M., Tyers, F., Badmaeva, E., Gokirmak, M., Nedoluzhko, A., Cinkova, S., Hajic Jr., J., Hlavacova, J., Kettnerov\u00e1, V., Uresova, Z., \u2026 Li, J. (2017). CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from           Raw Text to Universal Dependencies, 1\u201319. https://doi.org/10.18653/v1/K17-3001\n\nZhang, Y., Zhou, H., & Li, Z. (2020). Fast and Accurate Neural CRF Constituency Parsing. Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, 4046\u20134053. https://doi.org/10.24963/ijcai.2020/560\n\n3.      In Table 3 on Page 7, PTB-supervised model is used for comparison. Which PTB-supervised model is used?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6909/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821743905,
            "cdate": 1698821743905,
            "tmdate": 1699636803428,
            "mdate": 1699636803428,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "r7naiCrOyM",
                "forum": "RR8y0WKrFv",
                "replyto": "7O8N18WngL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6909/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for recognizing a number of our key contributions, including a new notion of tree averaging, the ensemble-then-distill approach, the efficient inference, and an intriguing phenomenon. \n\n\n> Weakness 1.  Lack of clarification regarding the methodology design\n\n* (F1 score) We chose the F1 score because F1 is the standard measure for constituency parsing, explicitly mentioned in the second paragraph of Sec 2.2. Therefore, our similarity score is well motivated. We would be grateful if the reviewer can recommend alternatives that are more justified. \n\n* (Choice of teacher models) We chose the seven teachers because they are classic or state-of-the-art unsupervised parsers (mentioned in Sec. 3.2, first line) with publicly available code. Specifically, we chose ContextDistort because it is a very recent study (ACL2023) and largely differs from previous approaches in its methodology. Note that in the analysis of the number of teachers (Fig. 2), we have experimented with various combinations of the teachers, where the results are consistent. \n\n\n> Weakness 2: \u201cI wonder if it is more appropriate to choose the model with highest performance in the former setting so that it can be further validated there exists additional boost due to different expertise (e.g., Neural PCFG for Group 2).\u201d\n\nIn Figure 1, four of the seven groups (namely, Groups 1, 4, 6, and 7) meet the criterion that the reviewer mentioned. Hence, it's possible to draw the conclusion that the reviewer is seeking from a subset of our experimentation. \nMoreover, we do not believe it is essential to use the best-performing model of the latter setting for multiple runs in the former, control setting. As we can see from the figure, the aforementioned four groups are not different from the rest.\n\n\n> Weakness 3 (two typos in a paper)\n \nThanks for catching two typos in our paper! We\u2019ve fixed them in the revision. \n\n\n> Question 1: The ensemble method demonstrates its effectiveness on PTB. However, in 2020, the F1 score of CRF parser on PTB variants had already been above 90 (Zhang et al., 2020). There is indeed a performance boost in comparison to oracle score (the highest possible F1 score of binarize groundtruth trees). But it is not appropriate to claim that \u201clargly bridging the gap between supervised and unsupervised constituency parsing\u201d on Page 6.\n\nThanks for pointing this out. We have revised our claim as \u201clargely bridging the gap between supervised and unsupervised *binary* constituency parsing\u201d. \n\n\n> Question 2: In Results on SUSANNE, the authors claim that \u201cThis is a realistic experiment to examine the models\u2019 performance in an unseen low-resource domain.\u201d However, SUSANNE is an English dataset. In CoNLL Shared task, there are tree-banks on low-resource languages (Zeman et al., 2017). It\u2019s better that the authors can demonstrate the effectiveness of the approach on some of these low-resource language datasets.\n\nHere, our claim about the SUSANNE experiment is domain shift, instead of low-resource language. As shown in Table 3, models trained on PTB underperform in other domains in the same language, showing that unsupervised parsing remains challenging in low-resource domains, even for English.\n\nAs mentioned in the response to Reviewer yYrJ, a key difficulty of a non-English ensemble is to find teacher unsupervised parsers, which are less addressed in the literature. We\u2019re happy to address this direction as future work.\n\n\n> Question 3: In Table 3 on Page 7, PTB-supervised model is used for comparison. Which PTB-supervised model is used?\n\nThe PTB-supervised models are the RNNG and URNNG, shown by the column headers. \n\n---\n\nOverall, the reviewer identified a number of important contributions in our work, but gave an outrageous score of 3 by missing the key rationales of our model design and key experimental results that are already mentioned in the paper, as well as overly emphasizing two typos in our paper as a weakness of the writing. On the contrary, our contributions, thorough experimentation, and clear writing are well recognized by all the other reviewers. \n\nIn our author response, we have answered all the questions raised by this reviewer. We urge the reviewer to revisit our paper and adjust the score accordingly."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243745563,
                "cdate": 1700243745563,
                "tmdate": 1700243745563,
                "mdate": 1700243745563,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3OYlDg4HCP",
            "forum": "RR8y0WKrFv",
            "replyto": "RR8y0WKrFv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6909/Reviewer_yYrJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6909/Reviewer_yYrJ"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a method for combining the outputs from unsupervised parsers in the manner similar to MBR decoding, but different in considering all possible trees. The proposed method simply assigns score to every span, a hit count that is the number of constituency appearing in outputs from multiple unsupervised parser. Then, it runs CKY to derive the maximum scored tree using the hit count score. Experiments on PTB and SUSANNE presents gains over SOTA baselines."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method is very simple to combine multiple outputs from unsupervised parsing, and the method might have an impact to other system combination method, e.g., NER with CRF. The ensemble by hit-count is sound and the merit is proved effective in the experiments especially when comparing MBR which can consider only spans in the multiple system outputs.\n\n- Experiments are well designed and the effect of the proposed method is proved empirically. This work also presents knowledge distillation using RNNG and URNNG so that it might have a potential for a practical application. Analysis is also convincing by comparing multiple diverse systems."
                },
                "weaknesses": {
                    "value": "- It is comparing only for English, and it would be better to compare the model with other languages, e.g., Chinese, for further strengthening this submission."
                },
                "questions": {
                    "value": "- I'd like to know the impact of length of inputs, e.g., whether the proposed method is better in lengthy input or not."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6909/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698882022871,
            "cdate": 1698882022871,
            "tmdate": 1699636803317,
            "mdate": 1699636803317,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZQtl0aQpdB",
                "forum": "RR8y0WKrFv",
                "replyto": "3OYlDg4HCP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6909/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6909/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the great insight and strong support. \n\n\n> Weakness: It is comparing only for English, and it would be better to compare the model with other languages, e.g., Chinese, for further strengthening this submission.\n\nThanks for pointing out that our approach currently works for the English language. A key difficulty of non-English ensemble is to find teacher unsupervised parsers, which are less addressed in the literature. Nevertheless, our ensemble approach brings new opportunities for unsupervised multilingual and non-English parsing (such as transferring and building ensembles with the structural knowledge of different languages). We\u2019re happy to explore this direction in the future, as has been mentioned in our future work section.\n\n\n> Question: I'd like to know the impact of length of inputs, e.g., whether the proposed method is better in lengthy input or not.\n\nThanks for the suggestion! We actually had the length analysis in our development but didn\u2019t include it in the submission because our paper had already overflowed to the appendix. The finding is similar to the analysis of constituency labels: our ensemble maintains high performance across different lengths, compared with teacher models. \n\nWe now show the results in Appendix B.2."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6909/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243230364,
                "cdate": 1700243230364,
                "tmdate": 1700243230364,
                "mdate": 1700243230364,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]