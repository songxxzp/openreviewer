[
    {
        "title": "SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation"
    },
    {
        "review": {
            "id": "VrWc5irLKw",
            "forum": "y8dHnNEcJu",
            "replyto": "y8dHnNEcJu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission925/Reviewer_LBiv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission925/Reviewer_LBiv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a novel method for weakly-supervised semantic segmentation (WSSS) by exploiting the large vision-language model, CLIP. By adopting a pre-trained WSSS model as a mask generator, they update the parameters of the generator using contrastive learning between the image-text triplet. After that, the learnable prompts are trained by contrastive prompt learning. Then, the mask generator is further updated using the proposed class-associated semantic refinement. By leveraging the CLIP-based image-text aligning, they improve the strong baseline, WeakTr, and achieve state-of-the-art performances on VOC 2012 and COCO 2014 benchmarks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written, and the description of the proposed methods is clear with well-illustrated figures.\n\n2. The proposed method (segment-label matching, contrastive prompt learning, class-associated semantic refinement) is intuitive and convincing.\n\n3. The experiment is somewhat complete and the proposed methods are well-ablated."
                },
                "weaknesses": {
                    "value": "## 1. complex training pipeline.\n\nAs I understand, this method is a refinement method for existing WSSS methods using knowledge of CLIP.\nNamely, this paper adopts WeakTr as a strong baseline WSSS method and refines it using CLIP-based contrastive learning.\nFrom a from-scratch training perspective, WeakTr requires a two-step training pipeline (CAM generation and online retraining) and SemPLeS requires a three-step training pipeline (segment-label matching, contrastive prompt learning, and class-associated semantic refinement).\n\nI wonder if the mask generator could be replaced by attention maps of CLIP or activation maps from CLIP (as done in CLIP-ES).\n\n## 2. dependency of the mask generator.\nI have a concern that the performance of the proposed method may largely depend on the mask generator. If the mask generator fails to generate proper segmentation masks, the WSSS performance is largely dropped.\nI guess the proposed (refinement) method can be adapted to other WSSS methods, but there is only one experiment using WeakTr.\n\n## 3. more meaningful comparison.\nThe methods in Table 3 can be categorized into two groups, CLIP-based methods (CLIMS and CLIP-ES) and CLIP-free methods.\nConsidering that CLIMS achieved 68.7% and CLIP-ES achieved 71.4% in VOC 2012 testset, the baseline method, WeakTr, already shows a much higher performance of 74.0%.\nAlthough SemPLeS achieved a 74.8% performance, I think this outstanding performance is mainly from WeakTr.\nIt would be great if there were any attempts to compare with CLIP-based methods more meaningfully (e.g., using the same seg model or the same baseline model).\n\nAlso, even though the SemPLeS used the large-scale vision-language model (CLIP) with the complex three-step refinement training, it is questionable that the 0.5% improvement in COCO 2014 validation set (Table 3) is promising."
                },
                "questions": {
                    "value": "Overall, I think the proposed method is interesting and convincing.\nHowever, I have some concerns related to the complex training pipeline and dependency of the mask generator model.\n\nTherefore, my initial rating is weak reject (4), and I finalize the rating after rebuttals."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I conclude that there are no ethics concerns in this paper."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission925/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission925/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission925/Reviewer_LBiv"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698586261309,
            "cdate": 1698586261309,
            "tmdate": 1699636019360,
            "mdate": 1699636019360,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EtOADWLxSk",
                "forum": "y8dHnNEcJu",
                "replyto": "VrWc5irLKw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission925/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for all the constructive comments. Here are our replies:\n\n**1. Complex training pipeline:**\n\nThe whole WeakTr pipeline has two stages: 1) CAM generation for pseudo-label, and 2) online retraining for final segmentation. The whole CLIP-ES pipeline also has two stages: 1) CAM generation with refinement for pseudo-label, and 2) segmentation model training with their proposed confidence-guided loss.\nTherefore, replacing the mask generator with CLIP-ES will not reduce the training pipeline complexity.\n\n**2. Dependency of the mask generator:** \nWe have also built our method upon CLIMS (we use the better version provided by the official repo). The results are as follows:\n\n|Methods\t\t\t|Val|\tTest|\n|---|---|---|\n|CLIMS (CVPR\u201922)|\t\t70.4| \t70.0|\n|CLIP-ES (CVPR\u201923)|\t\t71.1|\t71.4|\n|WeakTr|\t\t\t73.2|\t74.0|\n|SemPLeS w/ CLIMS (Ours)|\t**71.0**|\t**71.5**|\n|SemPLeS w/ WeakTr (Ours)|\t**74.2**|\t**74.8**|\n \nThe above table demonstrates that our proposed SemPLeS exhibits effectiveness with both a CNN-based mask generator, such as CLIMS, and a Transformer-based mask generator, like WeakTr, and improves both CLIMS and WeakTr, respectively.\n\n**3. More meaningful comparison:**\n\nAs shown in the above table, our proposed SemPLeS can work different segmentation models (e.g., CLIMS and WeakTr), and improve both performances, respectively.\n\nCOCO 2014 is a challenging dataset for the WSSS problem. The average mIoU for WSSS papers published at top conferences (e.g., CVPR, ECCV, NeurIPS, etc.) was 43.11 in 2022 and increased to 43.58 in 2023. Notably, the overall improvement in WSSS within the community over the past year is estimated at around 0.47. Hence, the achieved 0.5 improvement is a non-negligible advancement in the context of WSSS performance."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151651533,
                "cdate": 1700151651533,
                "tmdate": 1700151651533,
                "mdate": 1700151651533,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fOyt0IhPon",
            "forum": "y8dHnNEcJu",
            "replyto": "y8dHnNEcJu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission925/Reviewer_ZuKn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission925/Reviewer_ZuKn"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose a prompt-learning method to enhance weakly-supervised semantic segmentation prediction.  Existing works often utilize pre-trained CLIP models to guide the class-specific foreground mask predictions, however, they often fail to separate co-occurring background categories from foreground (e.g. train vs. railroad, horse vs. grass).  Prior works address such issues by manually design background prompts for each category, hoping to refine the predicted pseudo mask.  Such methods require human efforts to manually annotate the background prompts.  This paper propose a stage-learning technique, by first training object mask predictors and then background prompts with image-text contrastive learning.  This paper demonstrates SOTA performance on standard benchmakrs, such as VOC and MSCOCO."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of separating co-occurring background from foreground for each class makes a lot of sense.  In particular, the examples of the train and co-occurring railroad is convincing.\n\n2. The results are SOTA on both benchmarks.\n\n3. The paper is well written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. There are some citations missing:\n    * ***intra-class foreground-background discrimination***: Learning Integral Objects With Intra-Class Discriminator for Weakly-Supervised Semantic Segmentation.  Fan et al. CVPR 2020.\n    * ***pixel-wise contrastive learning for WSSS***: Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive Learning. Ke et al. ICLR 2021.\n\n2. The efficacy of class-specific background prompt is not clear.  In Table 4, training with $L_{prompt}^f$ seems to be more effective than $L_{prompt}^b$ and $L_{refine}$.\n\n3. Segment Anything (SAM) is a strong framework for mask proposals.  Without fine-tuning on VOC / MSCOCO, I believe SAM can still provide high-quality segmentation on out-of-distribution imagery.  Reasonable baselines are: 1) classifying mask proposals with SAM using OVSeg[1]/CLIP/MaskCLIP+[2] features, and 2) instead of learning background prompts, one can learn to refine per-category mask proposals using dense segmentations derived from SAM.  We can vote the foreground confidence within each SAM segment (very like refine binary masks with CRF).  However, this paper does not provide any comparison to SAM (see questions for details).\n\n\n[1] Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP. Liang et al.  CVPR 2023.\n[2] Extract Free Dense Labels from CLIP. Zhou et al. ECCV 2022."
                },
                "questions": {
                    "value": "1. My first concern is the idea of using CLIP to guide mask predictions.  CLIP is notoriously know to perform poorly on segmentation.  May works have been proposed to address such issues (e.g. OVSeg[1] and MaskCLILP+[2]).  My question is why the authors choose to use CLIP but not MaskClip+ features?  In fact, probably we don't even need the proposes background prompt learning when using mask-sensitive CLIP features. The authors should provide analysis on OVSeg/MaskCLIP+ features.\n\n2. I'm not sure how necessary it is to train the Mask Predictor. SAM is an existing strong segmentation framework.  Why not apply OVSeg[2] (with intra-class background prompt tuning) on mask proposals from SAM?  Or even refine the mask predictions with SAM.\n\n3. In Table 4, what's the performance if you train with only $L_{match}$ and $L_{prompt}^f$?  It seems to me that training with $L_{prompt}^f$ brings the most performance gain, meaning the background prompts might not be as effective as the paper claims."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771853777,
            "cdate": 1698771853777,
            "tmdate": 1699636019290,
            "mdate": 1699636019290,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rpoHvLfFKc",
                "forum": "y8dHnNEcJu",
                "replyto": "fOyt0IhPon",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission925/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for all the constructive comments. Here are our replies:\n\n**1. Citation missing:**\n\nThanks for sharing. We will cite both ICD [1] and SPML [2] in our revised paper.\nOur difference from the above two papers:\n* ICD [1] and our method both aim to separate the foreground and the background, but they achieve this by learning intra-class boundaries while we learn the class-associated prompts embedded with semantic knowledge.\n* SPML [2] and our method both conduct contrastive learning, but they focus on pixel-segment contrastive learning while we focus on image-text and text-text contrastive learning.\n\n**2. Confusion about class-specific background prompts:**\nThe whole $L_{prompt}$ in Eq. (2) is used to learn class-associated background prompts, including $L^b_{prompt}$ and $L^f_{prompt}$:\n* $L^b_{prompt}$ aims to attract class-associated background prompts to predicted background images\n* $L^f_{prompt}$ aims to repel class-associated background prompts from foreground class labels\n\nAfter the class-associated background prompts are learned, we use the learned prompts to refine the mask with $L_{refine}$. Therefore, without $L_{refine}$, we cannot know how $L^b_{prompt}$ and $L^f_{prompt}$ affect the final mIoU performance. That is to say, training with only $L_{match}$ and $L^f_{prompt}$ will provide the same results as training with $L_{match}$.\n\n**3. Comparison w/ SAM & Why not SAM:** \n\nThanks for the insightful comments. We acknowledge the effectiveness of SAM [5] as an established segmentation model and we agree that combining SAM with open-vocabulary semantic segmentation methods, such as OVSeg, leads to enhanced performance. However, our paper focuses on the challenging WSSS problem, specifically in the scenario where only class labels are available, without the use of mask annotations during training. SAM's training on the SA-1B dataset including over 1 billion mask annotations, places it beyond the scope of the WSSS problem. Nonetheless, we find it intriguing to explore how our SemPLeS framework could enhance SAM's semantic understanding capabilities, offering valuable insights into WSSS under different constraints. We consider this as a potential direction for our future work.\n\n**4. Why CLIP instead of OVSeg/MaskCLIP+:** \n\nThis paper tackles the WSSS problem under the constraint of using only class labels during training, excluding the use of mask annotations. In light of this restriction, CLIP emerges as a favorable option for external knowledge incorporation due to its training without reliance on mask annotations. Notably, CLIP is designed for general purposes rather than being tailored solely for segmentation tasks. A recent trend involves leveraging knowledge within the CLIP space to enhance performance in dense prediction tasks. We aim to harness the semantic knowledge ingrained in the CLIP space, contributing to the improvement of WSSS training.\n\nThe reason why OVSeg [4] and MaskCLIP+ [3] are not better choices for this paper:\n* OVSeg (CVPR\u201923) [4]: it is trained w/ COCO-Stuff segmentation annotations, which violates the WSSS protocol.\n* MaskCLIP+ (ECCV\u201922) [3]: it uses the transductive setting, so the testing data is seen during training, which also violates the WSSS protocol.\n\nNonetheless, there is an intriguing avenue for exploration in integrating certain concepts from OVSeg (e.g., learnable mask prompt) or MaskCLIP (e.g., prompt denoising) into our SemPLeS framework. We consider this as a potential direction for our future work.\n\n---\nReferences:\n* [1] (ICD) Junsong Fan, et al. \u201cLearning Integral Objects with Intra-Class Discriminator for Weakly-Supervised Semantic Segmentation\u201d, CVPR, 2020.\n* [2] (SPML) Tsung-Wei Ke, et al. \u201cUniversal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive Learning\u201d, ICLR, 2021.\n* [3] (MaskCLIP) Chong Zhou, et al. \u201cExtract Free Dense Labels from CLIP\u201d, ECCV, 2022.\n* [4] (OVSeg) Feng Liang, et al. \u201cOpen-Vocabulary Semantic Segmentation with Mask-adapted CLIP\u201d, CVPR, 2023.\n* [5] (SAM) Alexander Kirillov, et al. \u201cSegment Anything\u201d, ICCV, 2023.\n\nWe will add missing citations in the revised paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151404232,
                "cdate": 1700151404232,
                "tmdate": 1700151404232,
                "mdate": 1700151404232,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dRbjwIHhTE",
                "forum": "y8dHnNEcJu",
                "replyto": "rpoHvLfFKc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission925/Reviewer_ZuKn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission925/Reviewer_ZuKn"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response.\n\n---\n\n**Re: 2. Confusion about class-specific background prompts**\n\nThanks for the explanation.  I would encourage the authors to refactor the loss notations.  It is somewhat difficult to follow how each loss term differ from others without multiple rounds of re-reading.\n\n---\n\n**Re: 3. Comparison w/ SAM & Why not SAM**\n\nI disagree with the statement *\"SAM's training on the SA-1B dataset including over 1 billion mask annotations, places it beyond the scope of the WSSS problem\"*.  The main reason is that WSSS is intrinsically a pixel-classification task, while SAM is not trained with semantic segmentation, but general image segmentation.  In addition, the segmentation quality of SAM is much higher than most of the existing mask proposal methods, which could facilitate the research topic of WSSS.\n\n---\n\n**Re: 4. Why CLIP instead of OVSeg/MaskCLIP+**\n\nThanks for the clarification.  The response makes sense to me."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700530164885,
                "cdate": 1700530164885,
                "tmdate": 1700530164885,
                "mdate": 1700530164885,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pgcKoi2L07",
                "forum": "y8dHnNEcJu",
                "replyto": "fOyt0IhPon",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Re: Re: 2. Confusion about class-specific background prompts**\n\nThanks for the valuable feedback. We will change the notation as follows:\n* $L^b_{prompt}$ $\\rightarrow$ $L^I_{prompt}$: the loss is computed using class-associated background prompts and predicted background *images*.\n* $L^f_{prompt}$ $\\rightarrow$  $L^T_{prompt}$: the loss is computed using class-associated background prompts and foreground class *labels*.\n\nWe will revise the whole paper accordingly.\n\n**Re: Re: 3. Comparison w/ SAM & Why not SAM**\n\nThank you for the comments. \n\nThe annotation for semantic segmentation includes two aspects: *localization* and *classification*.\nFully-supervised semantic segmentation (FSSS) relies on full mask annotation for training, with pixel-level localization and class labels. Methods without full supervision can be categorized as WSSS. However, the current convention of the WSSS community defines **Weak** only in terms of *localization*, which means that the ground truth does not contain precise pixel-level location information, but with (sparse) point-level, box-level, or image-level supervision instead. Our focus is exclusively on image-level supervision, lacking any location information.\n\nSAM, although falling under the WSSS umbrella according to the aforementioned definition due to the absence of class annotation, surpasses the focus of this study by incorporating foreground/background annotations. We are sorry for the confusion caused by our previous comments. While SAM aligns with the broader WSSS definition, it extends beyond the scope of *WSSS with image-level supervision* in this work. This work focuses on the problem with only image-level supervision without any location information.\n\n\nWe appreciate the discussion prompted by your inquiries. *WSSS with SAM* is an under-explored research avenue, involving class-agnostic mask annotation and image-level labels. \nWe find it intriguing to explore how our SemPLeS framework could benefit this direction, offering valuable insights into WSSS under different constraints. We see this as a prospective direction for our future work. Thank you for initiating this valuable discussion."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557843481,
                "cdate": 1700557843481,
                "tmdate": 1700580075097,
                "mdate": 1700580075097,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6Zq2K5VwYg",
                "forum": "y8dHnNEcJu",
                "replyto": "pgcKoi2L07",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission925/Reviewer_ZuKn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission925/Reviewer_ZuKn"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response.\n\n---\n\n**Re: Re: 2. Confusion about class-specific background prompts**\n\nMy concern is addressed\n\n---\n\n**Re: Re: 3. Comparison w/ SAM & Why not SAM**\n\nYes! I do believe the WSSS task could be mostly solved using SAM.  In fact, there're exisitng works applying similar ideas.  For example, a recent NeuRIPS paper \"Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping\".\n\nMy personal perspective on WSSS is that working merely on COCO/VOC dataset is obsolete, especially after the invention of SAM.  The community could instead explores what type of weakly-supervised segmentation tasks that SAM fail to solve.  I can name few of which: hierarchical segmentation and concealed object segmentation.  It'd be interesting to study weakly-supervised learning on both topics."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583680101,
                "cdate": 1700583680101,
                "tmdate": 1700583680101,
                "mdate": 1700583680101,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5bdYdmxrZy",
                "forum": "y8dHnNEcJu",
                "replyto": "fOyt0IhPon",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful feedback.\n\nWe agree that SAM reshapes the research directions in the community, especially for segmentation-related tasks, and SAM can solve various types of WSSS problems. However, *WSSS with image-level supervision* remains a formidable challenge. Recent investigations [2-5] have explored the potential of leveraging SAM for this specific problem.\nNotably, most of them integrate SAM with additional foundation models (e.g., BLIP-2 [6], Grounding-DINO [7], RAM [8]) to enhance SAM\u2019s ability to incorporate semantic information. The results in the below table demonstrate that even with SAM, the *WSSS with image-level supervision* problem is still challenging, and our proposed SemPLeS has already outperformed several SAM-based methods.\n\n|Methods\t\t\t|Val|\tTest|\n|---|---|---|\n|CLIMS + SEPL (with SAM) [2]\t|71.1|\t      -|\n|BLIP-2 + SAM [3]\t\t\t|71.1|\t72.2|\n|Grounding-DINO + SAM [4]\t\t|72.7|\t72.8|\n|RAM + Grounding-DINO + SAM [4]\t|74.0|\t73.8|\n|Grounding-DINO(*) + SAM [5]\t|77.2|\t77.1|\n|SemPLeS (Ours)\t\t\t|74.2|\t74.8|\n(*with advanced text prompt design)\n\n\nWe chose VOC and COCO since they are widely recognized benchmark datasets for this problem at the time of submission to ICLR\u201924. We acknowledge your suggestion to focus future research directions on tasks where SAM faces challenges or is inapplicable. \n\nIn accordance with the feedback you provided, *data domain* emerges as a prominent challenge, particularly when data distributions deviate significantly from SAM's original image training data, encompassing scenarios such as concealed objects [1] and glass objects [11]. Addressing this issue requires the adoption of additional methodologies. Moreover, *model efficiency* is also another issue. SAM's application in resource-limited or time-sensitive scenarios, such as mobile applications, is hindered by its large model size. Exploring strategies to enhance SAM's efficiency without compromising accuracy represents a compelling and practical avenue for future research. Furthermore, similar to most deep models, *robustness* is another issue that SAM is facing [9-10]. We consider these directions as our future work.\n\nThank you for continuing this valuable discussion. We hope our responses address your concern. We would also be appreciative if you could consider re-evaluating the rating of this work. Please let us know if there are any concerns or questions, and we are eager to provide further clarification or discussion.\n\n---\nReferences:\n* [1] Chunming He, et al. \u201cWeakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping\u201d, NeurIPS, 2023.\n* [2] Tianle Chen, et al. \u201cSegment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation\u201d, NeurIPS Workshop, 2023.\n* [3] Peng-Tao Jiang, et al. \u201cSegment Anything is A Good Pseudo-label Generator for Weakly Supervised Semantic Segmentation\u201d, arXiv, 2023.\n* [4] Zhaozheng Chen, et al. \u201cWeakly-Supervised Semantic Segmentation with Image-Level Labels: from Traditional Models to Foundation Models\u201d, arXiv, 2023.\n* [5] Weixuan Sun, et al. \u201cAn Alternative to WSSS? An Empirical Study of the Segment Anything Model (SAM) on Weakly-Supervised Semantic Segmentation Problems\u201d, arXiv, 2023.\n* [6] Junnan Li, et al. \u201cBLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\u201d, ICML, 2023.\n* [7] Shilong Liu, et al. \u201cGrounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection\u201d, arXiv, 2023.\n* [8] Youcai Zhang, et al. \u201cRecognize Anything: A Strong Image Tagging Model\u201d, arXiv, 2023.\n* [9] Yuqing Wang, et al. \u201cAn Empirical Study on the Robustness of the Segment Anything Model (SAM)\u201d, arXiv, 2023.\n* [10] Yu Qiao, et al. \u201cRobustness of SAM: Segment Anything Under Corruptions and Beyond\u201d, arXiv, 2023.\n* [11] Dongsheng Han, et al. \u201cSegment Anything Model (SAM) Meets Glass: Mirror and Transparent Objects Cannot Be Easily Detected\u201d, arXiv, 2023."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673004554,
                "cdate": 1700673004554,
                "tmdate": 1700721404692,
                "mdate": 1700721404692,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8u19XhPj13",
            "forum": "y8dHnNEcJu",
            "replyto": "y8dHnNEcJu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission925/Reviewer_WM2o"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission925/Reviewer_WM2o"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a Semantic Prompt Learning for WSSS (SemPLeS) framework. The author proposes contrastive prompt learning to acquire class-associated background prompts and further proposes a Class-associated Semantic Refinement module to suppress erroneous associations of co-occurring backgrounds. This method shows better performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors propose a novel CLIP prompt method that was automatically learned rather than manually designed, which effectively promotes the alignment of the semantic space.\n\n2. The authors provide a detailed explanation of the modulation process, demonstrating the effectiveness of learnable prompts.\n\n3. The logic of this paper is clear and easy to read."
                },
                "weaknesses": {
                    "value": "1. In Sec. 2.1, the author briefly introduces the current research status of WSSS three-stage learning. However, this method is only a research branch of WSSS, and the end-to-end method should be supplemented. Furthermore, as far as we know, the recent WSSS research is basically based on CLIP. The author's innovation lies in the non-manual design of prompts rather than establishing vision-language associations, thus there is no need to emphasize the contribution of using CLIP.\n\n2. In Figure 2 (a), symbol abbreviations X_k^f and X_k^b are given for the foreground and background of the image, and the text prompt t_k should also be indicated in this figure.\n\n3. In Sec. 3.2.2, it writes \u201clearnable prompts p_k as the input of the text encoder E_t\u201d. It is necessary to illustrate how to initialize it and its shape. The motivation for learning prompts is not new, there exist many works that learn prompts in the CLIP community, such as COOP. More differences between these methods should be discussed.\n\n4. Missed comparison with other methods, such as the work titled \"MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation \" from ICCV 2023. To ensure a comprehensive research evaluation and establish a fair assessment of the proposed method's performance, the authors should include extensive analysis and comparison with more SOTAs.\n\n5. Figures 3 and 4 can be augmented by incorporating a dedicated column on the left-hand side to present image labels, rather than embedding them within the figure itself. This graphical modification not only improves visual clarity but also aligns with the model's separate processing of images and labels.\n\n6. What does \"All BG Prompts\" in picture 4 mean?\n\n7. Result of L_match + L_prompt^b + L_prompt^f  should be added in Table 4."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698926122615,
            "cdate": 1698926122615,
            "tmdate": 1699636019214,
            "mdate": 1699636019214,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2SRzm7FMlb",
                "forum": "y8dHnNEcJu",
                "replyto": "8u19XhPj13",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission925/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for all the constructive comments. Here are our replies:\n\n**1. (i) End-to-end WSSS:**\n\nMost WSSS papers follow the standard 3-stage pipeline. There are a few end-to-end WSSS papers, but their performances are worse than standard 3-stage methods. The quantitative comparison in the below table shows that our method significantly outperforms current SOTA end-to-end methods, including ToCo [4] and MCC [14]. \nWe will introduce more end-to-end methods in Related Works. \n\n| Methods\t| Seg. Model | Backbone | Val | Test |\n|-----------------|--------------|-------------|------|-----|\n| ToCo (CVPR\u201923) [4] | DeiT-B | DeiT-B | 69.8 | 70.5 |\n| MCC (WACV\u201924) [14] | DeiT-B | DeiT-B | 70.3 | 71.2 |\n| SemPLeS (Ours) | DeiT-S | DeiT-S | **74.2** | **74.8** |\n\n**(ii) Contribution of using CLIP:**\n\nThere are not many WSSS papers based on CLIP. Most CLIP-based papers are related to the open-vocabulary setting which requires mask annotations for training. We summarize recent CLIP-based WSSS papers and explain how they exploit CLIP as follows:\n* CLIMS (CVPR. 2022) [3]: refine masks by manually prompting the pretrained CLIP model.\n* CLIP-ES (CVPR, 2023) [5]: extract attention maps from the CLIP-pretrained ViT model\n* MMCST (CVPR, 2023) [7]: use text features from the CLIP-pretrained ViT model to perform their proposed contrastive loss.\n\nDifferent from the above, we learn the class-associated prompts embedded with semantic knowledge, which can be used to refine masks.\nThe quantitative comparison in the below table shows that our method outperforms current CLIP-based WSSS methods. \n\n|Methods\t\t\t|Val|\tTest|\n|---|---|---|\n|CLIMS (CVPR\u201922) [3]|\t\t69.3 |\t68.7|\n|CLIP-ES (CVPR\u201923) [5]\t|71.1|\t71.4|\n|MMCST (CVPR\u201923) [7]\t|72.2|\t72.2|\n|SemPLeS (Ours)\t\t|**74.2**|\t**74.8**|\n\n**2. Symbols in Figure 2 (a):**\n\nThanks for the suggestion, and we will revise the figure as suggested.\n\n**3. (i) details of text prompt t_k (how to initialize & shape):**\n\nThe learnable prompts are randomly initialized with the shape K by D, where K is prompt length, and D is the prompt embedding dimension of CLIP.\n\n**(ii) difference from current prompt learning methods:**\n\nAs mentioned in Related Works, our Contrastive Prompt Learning aims to capture class-associated semantic knowledge for segmentation purposes rather than replacing the manual-defined text template like \u201ca photo of {}\u201d for classification purposes as in current prompt learning methods.\n\n**4. More SOTA comparison (including MARS):**\n\nWe are not sure if it\u2019s reasonable to ask for a comparison with publications where the conference dates are after the ICLR\u201924 submission deadline, such as ICCV 2023.\n\nNevertheless, we list all WSSS papers published in CVPR and ICCV 2023 in the below table with the heatmap and segmentation backbones. Our method outperforms almost all other methods in the table except for MARS (ICCV\u201923) [10]. \nMARS [10] builds upon a strong baseline RS-EPM (arXiv\u2019 22) [1] with a strong backbone DeepLabv3+ (ResNet101), and it also proposes to exploit an additional strong Unsupervised Semantic Segmentation model, STEGO (ICLR\u2019 22) [2] with ViT-B, to remove biased objects and achieve the current SOTA for WSSS. This method is interesting and we envision its potential integration with our automatic prompt learning method for further improvement. We leave this prospect as part of our future work.\n\t\t\t\t\t\t\t\t\t\n|Methods\t|\t\tSeg. Model|\tBackbone|\tVal|\tTest|\n|-----------------|--------------|-------------|------|-----|\n|ToCo (CVPR\u201923) [4]|\tDeiT-B\t|\tDeiT-B\t|\t69.3 |\t68.7|\n|CLIP-ES (CVPR\u201923) [5]|\tDL2-Res101|\tViT-B|\t\t71.1|\t71.4|\n|OCR (CVPR\u201923) [6]|\tDL1-WRes38|\tDeiT-S|\t\t72.7\t|72.0|\n|MMCST (CVPR\u201923) [7]|\tDL1-WRes38|\tViT-B|\t\t72.2|\t72.2|\n|ACR (CVPR\u201923) [8]\t|\tDL1-WRes38|\tWRes38|\t71.9|\t71.9|\n|BECO (CVPR\u201923) [9]\t|\tDL3+-Res101|\tRes101|\t72.1|\t71.8|\n|MARS (ICCV\u201923) [10]\t|\tDL3+-Res101|\tRes101|\t***77.7***|\t***77.2***|\n|USAGE (ICCV\u201923) [11]|\tDL1-WRes38|\tDeiT-S|\t\t71.9|\t72.8|\n|FPR (ICCV\u201923) [12]|\t\tDL2-Res101|\tRes50|\t\t70.3\t|70.1|\n|Mat-Label (ICCV\u201923) [13]|\tDL2-Res101|\tRes50|\t\t73.0|\t72.7|\n|SemPLeS (Ours)|\t\tDeiT-S|\t\tDeiT-S|\t\t**74.2**\t|**74.8**|\t\n\n\n**5. Labels presented in Figures 3 and 4:**\n\nThanks for the suggestion, and we will revise the figure as suggested.\n\n**6. \u201cAll BG Prompts\u201d in Figure 4:**\n\n\u201cAll BG Prompts\u201d means the visualization is generated from ALL background prompts manually defined in CLIMS [3].\n\n**7. Table 4 clarification:**\n\n$L_{prompt}^b$ and $L_{prompt}^f$ are used to learn the background prompts, and then we use the learned prompts to refine the mask with $L_{refine}$. Therefore, without $L_{refine}$, we cannot know how $L_{prompt}^b$ and $L_{prompt}^f$ affect the final mIoU performance. Training with $L_{match}$ + $L_{prompt}^b$ + $L_{prompt}^f$ will be the same as training with $L_{match}$."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700150107824,
                "cdate": 1700150107824,
                "tmdate": 1700151550613,
                "mdate": 1700151550613,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WIWUXKn7gI",
            "forum": "y8dHnNEcJu",
            "replyto": "y8dHnNEcJu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission925/Reviewer_BYUr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission925/Reviewer_BYUr"
            ],
            "content": {
                "summary": {
                    "value": "A new Weakly-Supervised Semantic Segmentation (WSSS)  method, called SemPLeS, is proposed in this paper. In SemPLeS, Contrastive Prompt Learning and Class-associated Semantic Refinement are used to learn the prompts that adequately describe and suppress the image backgrounds associated with each target object category. The authors tested SemPLeS, and it outperformed the existing SOTA on popular benchmarks like PASCAL VOC and MS COCO."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Using contrastive learning and CLIP text/visual encoders is an interesting idea. Especially optimizing learnable negative prompts and applying it to contrastive learning with positive image regions is very interesting. The novelty of the proposed method seems to be high.\n\nBy the experiments, the effectiveness of the proposed method was clearly shown. The proposed method seems to outperform the current SOTA method, WeakTr."
                },
                "weaknesses": {
                    "value": "According to Paper-with-code, SOTAs for val and test are 78.4 and 79.0 by WaekTr, respectively. Why the results of the baselines shown in Table 2 are less than that ? The authors should present results of the proposed method under the same settings as Paper-with-code SOTA.\nhttps://paperswithcode.com/sota/weakly-supervised-semantic-segmentation-on-1\nhttps://paperswithcode.com/sota/weakly-supervised-semantic-segmentation-on"
                },
                "questions": {
                    "value": "I'm wondering if the proposed method is also effective for panatomic segmentation tasks such as Pascal Panatomic and CityScapes."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698952286669,
            "cdate": 1698952286669,
            "tmdate": 1699636019122,
            "mdate": 1699636019122,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ogENfT4AIz",
                "forum": "y8dHnNEcJu",
                "replyto": "WIWUXKn7gI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for all the constructive comments. Here are our replies:\n\n**1. SOTA comparison:** \n\nThe better SOTA by WeakTr uses different settings, including a better ViT backbone pre-trained on ImageNet-21K [1] and a larger resolution (384) for fine-tuning. For the classification performance on the ImageNet-1K dataset, DeiT-S (our backbone) has 79.9 accuracy while ViT-S (SOTA WeakTr\u2019s backbone) has 83.7 accuracy. \nWe choose not to use the better WeakTr as the backbone because we would like to follow the standard protocol of the WSSS benchmark for fair comparison.\n\n**2. Panoptic segmentation:**\n\nTo our best understanding, although there are some weakly-supervised panoptic segmentation papers like WSSPS [2] (box-supervised), PSPS [3] (point-supervised), and Panoptic-FCN [4] (point-supervised), there is no panoptic segmentation work with class supervision. It would be an interesting research direction to explore, and we see it as a potential future work.\n\n---\nReferences:\n* [1] Andreas Steiner, et al. \u201cHow to train your ViT? Data, Augmentation, and Regularization in Vision Transformers\u201d, TMLR, 2022.\n* [2] (WSSPS) Qizhu Li, et al. \u201cWeakly- and Semi-Supervised Panoptic Segmentation\u201d, ECCV, 2018.\n* [3] (PSPS) Junsong Fan, et al. \u201cPointly-Supervised Panoptic Segmentation\u201d, ECCV, 2022.\n* [4] (Panoptic-FCN) Yanwei Li, et al. \u201cFully Convolutional Networks for Panoptic Segmentation with Point-based Supervision\u201d, TPAMI, 2022.\n\nWe will add missing citations in the revised paper."
                    },
                    "title": {
                        "value": "Rebuttal"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148682875,
                "cdate": 1700148682875,
                "tmdate": 1700579640894,
                "mdate": 1700579640894,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]