[
    {
        "title": "RealChat-1M: A Large-Scale Real-World LLM Conversation Dataset"
    },
    {
        "review": {
            "id": "Fz1gr7tyL1",
            "forum": "BOfDKxfwt0",
            "replyto": "BOfDKxfwt0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3046/Reviewer_M7pJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3046/Reviewer_M7pJ"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces RealChat-1M, a large-scale dataset containing 1M human-LLM conversations.  The authors presented a comprehensive exploration of the dataset through four distinct use cases: the development of content moderation models, establishment of a robust safety benchmark, training of instruction-following models, and the creation of challenging benchmark questions, highlighting the multifaceted utility of the dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- A large-scale real-world conversational dataset with diverse range of applications, including the development of content moderation models, the creation of a robust safety benchmark (jailbreak conversations), the training of instruction-following models, and the collection of challenging benchmark questions (Arena-Hard-200). \n- This work offers a comparative analysis across all use cases, leveraging both open-source LLMs like Vicuna and proprietary models such as GPT-4. The experimental results reveals 1) large performance gaps between open and proprietary models, 2) safety concerns of all the models especially the models developed by open source community (e.g., Vicuna, Alpaca)."
                },
                "weaknesses": {
                    "value": "While the presented dataset significantly surpasses the scale of other datasets, I find myself questioning the unique value it brings. OpenAssistant, SharedGPT, Anthropic HH, and Chatbot Arena have already proven their worth in fine-tuning LLMs, assessing LLM safety and performance. What compelling reasons can be put forth to encourage researchers to opt for a subset of RealChat-1M for these very purposes?"
                },
                "questions": {
                    "value": "See Weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3046/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698616039370,
            "cdate": 1698616039370,
            "tmdate": 1699636249891,
            "mdate": 1699636249891,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RzFJZhT5nB",
                "forum": "BOfDKxfwt0",
                "replyto": "Fz1gr7tyL1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3046/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3046/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank reviewer M7pJ for their feedback and for recognizing the scale of our dataset. To address the weakness pointed out by the reviewer, we wish to highlight the unique aspects of our dataset and emphasize its importance in terms of scale:\n\n1. Our dataset is up-to-date and will be continually updated. We actively maintain our data collection website and plan to release more data dumps in the future. Over the past month, we received approximately 25K user queries per day for the latest models, including GPT-4 Turbo, Mistra, and Zephyr. We also intend to release more data on human preferences (votes).\n2. Our dataset is highly diverse. Unlike the datasets mentioned, which typically include only one model or a very limited number of models, ours encompasses a much wider range of models and user distributions. \n3. The scale of the dataset is crucial. In an era where there is a growing interest in scraping the entire internet to train LLMs, we believe that scale is one of the most critical metrics for a dataset. The advancements in machine learning have often been driven by scaling up datasets, as evidenced by projects like ImageNet and GPT-3/4."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3046/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631504117,
                "cdate": 1700631504117,
                "tmdate": 1700631504117,
                "mdate": 1700631504117,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VsEqEyMPab",
            "forum": "BOfDKxfwt0",
            "replyto": "BOfDKxfwt0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3046/Reviewer_kFoy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3046/Reviewer_kFoy"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces RealChat-1M, a dataset contains 1 million real-world conversations with 25 large language models collected from 210,000 unique IP addresses in the wild. The authors demonstrate the usefulness of the dataset on multiple use cases like content moderation models, safety benchmarks, or training instruction-following models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The dataset introduced by this paper is a large-scale dataset containing interactive logs of 210,000 unique IP addresses with 25 large language models, which is both extremely valuable and meaningful for future LLM development, given most datasets that were used to train LLMs are not publicly available.\n- Part of the data that can jailbreak the safeguards of leading LLMs is repurposed by the authors to be a benchmark for safety and robustness study.\n- The authors also curate a benchmark that contains challenging and high-quality user prompts for identifying the gap between open-sourced and proprietary models.\n- Demonstration for the use cases of the dataset were nicely done."
                },
                "weaknesses": {
                    "value": "- Figure 1 and 2 could be redrawn by leaving vicuna/English out because the distribution is left-skewed and therefore the number for the rest of the models/languages are hard to interpret.\n- Although in the limitation section there is a paragraph about the data quality, deeper analysis could be done. For example, how many of them is from MMLU and MT-Bench, is there some human annotation done for duplicate data?"
                },
                "questions": {
                    "value": "How many prompts are there after filtering for section 3.2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3046/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3046/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3046/Reviewer_kFoy"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3046/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698638791163,
            "cdate": 1698638791163,
            "tmdate": 1699636249812,
            "mdate": 1699636249812,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AQYq6rdY8S",
                "forum": "BOfDKxfwt0",
                "replyto": "VsEqEyMPab",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3046/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3046/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank reviewer kFoy for their feedback.\n\n> Figure 1 and 2 could be redrawn by leaving vicuna/English out\n\nWe will consider redrawing Figures 1 and 2, either by using a log scale or by splitting them into two separate figures.\n\n> Although in the limitation section there is a paragraph about the data quality, deeper analysis could be done. For example, how many of them is from MMLU and MT-Bench, is there some human annotation done for duplicate data?\n\nWe ran a string match to find the overlap between RealChat-1M and MT-bench questions. We found that 12 out of 160 questions from MT-bench are present in RealChat-1M with an exact string match. When performing topic clustering, we also applied duplication checks based on string matching.\n\n> How many prompts are there after filtering for section 3.2?\n\nWe began with 100K conversations, each averaging two turns, resulting in 200K prompts. After filtering, half of the prompts were removed, leaving us with 100K prompts."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3046/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631466716,
                "cdate": 1700631466716,
                "tmdate": 1700631466716,
                "mdate": 1700631466716,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DsbywKamiz",
            "forum": "BOfDKxfwt0",
            "replyto": "BOfDKxfwt0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3046/Reviewer_hTkb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3046/Reviewer_hTkb"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a dataset of 1 million real user chats with 25 popular LLMs. The main contribution of the paper is the data itself, which was collected over 5 months and includes a diverse array of user interactions. The authors present some analysis of the contents of the dataset, then show 4 use cases: building a moderation model, a safety benchmark, instruction tuning, and a challenging prompt benchmark."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The data itself is quite valuable. As the authors note, much of the data actually used in training models is proprietary and private\n- The 4 use cases are all creative and well designed. They demonstrate the potential of the dataset as a strong resource \n- The analysis is quite interesting, for example the cluster analysis in figure 3. It also supports/is validated by past work, e.g. the observation that many users now are interested in LLMs for help with coding\n- The size and diversity of the dataset promises many future uses"
                },
                "weaknesses": {
                    "value": "- The dataset does not seem quite as diverse as the abstract suggests. Although 25 LLMs are used, Vicuna-13B is by far the most frequently used one. Similarly, while there may be a few instances of many languages, the vast majority is still in English\n- As the authors point out, there are no human preference values which is one weakness compared to related datasets (see table 1)\n- It is not clear whether the use format (single model vs side-by-side) and IP address have similar balance issues to model and language above. Although there are 210K unique IP addresses, is some large portion of the dataset covered by only a few of these? And do most users access single models rather than the perhaps more interesting side-by-side mode? Having the answers to these questions would help evaluate this paper further."
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "It would be useful to ensure that the real user inputs do not put any of the users at risk of deanonymization or harm. The authors make a best effort to prevent this, but having some review of this may be necessary."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3046/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788787352,
            "cdate": 1698788787352,
            "tmdate": 1699636249737,
            "mdate": 1699636249737,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uuMcHiVmsR",
                "forum": "BOfDKxfwt0",
                "replyto": "DsbywKamiz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3046/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3046/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your questions and comments!\n\n> The dataset does not seem quite as diverse as the abstract suggests. Although 25 LLMs are used, Vicuna-13B is by far the most frequently used one. Similarly, while there may be a few instances of many languages, the vast majority is still in English\n> As the authors point out, there are no human preference values which is one weakness compared to related datasets (see table 1)\n\nIt is true that Vicuna-13B and English comprise a large fraction of our dataset. This is because Vicuna-13B is the first model we hosted on the platform and the fact that most of our users speak English. Meanwhile, our dataset will continue to be updated. This is expected to mitigate the bias present in earlier models and keep pace with the latest data from new models. We are actively maintaining our data collection website and plan to release more data dumps in the future. In the month following our paper submission, we received approximately 25K user queries per day for the latest models, including GPT-4 Turbo, Mistra, and Zephyr. We also plan to release more data on human preferences, such as votes.\n\n> It is not clear whether the use format (single model vs side-by-side) and IP address have similar balance issues to model and language above. Although there are 210K unique IP addresses, is some large portion of the dataset covered by only a few of these? And do most users access single models rather than the perhaps more interesting side-by-side mode? Having the answers to these questions would help evaluate this paper further.\n\nThe IP addresses are actually very sparse, so there is no small set of IP addresses that can cover a large portion of the dataset. It is true that most users access single models rather than side-by-side. But the distribution is shifting towards side-by-side more recently because we are promoting more on the side-by-side mode and making it the landing page."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3046/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631406918,
                "cdate": 1700631406918,
                "tmdate": 1700631406918,
                "mdate": 1700631406918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aElLi86tuZ",
                "forum": "BOfDKxfwt0",
                "replyto": "uuMcHiVmsR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3046/Reviewer_hTkb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3046/Reviewer_hTkb"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for a detailed response, this answers my questions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3046/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714582492,
                "cdate": 1700714582492,
                "tmdate": 1700714582492,
                "mdate": 1700714582492,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Fr4883U4KC",
            "forum": "BOfDKxfwt0",
            "replyto": "BOfDKxfwt0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3046/Reviewer_D9AT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3046/Reviewer_D9AT"
            ],
            "content": {
                "summary": {
                    "value": "Through the introduction of the RealChat-1M dataset, this paper provides a comprehensive resource for studying user interactions with LLMs in real-world settings. They leverage RealChat-1M for 4 use cases: developing content moderation models, building a safety benchmark, training instruction-following models, and creating challenging benchmark questions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. With a size of 1 million, this dataset stands out as unparalleled among its peers.\n2. This dataset offers authentic user prompts and highlights potential safety concerns, paving the way for future research.\n3. This paper presents studies that highlight the four distinct applications of this novel dataset.\n4. The authors have a plan to regularly update the dataset in the future, which is a crucial step given the frequent release of newer and more advanced LLMs from the community."
                },
                "weaknesses": {
                    "value": "While the user prompts in this dataset are genuine, the responses are synthetic and do not have quality ratings. Thus, additional processing is necessary before use."
                },
                "questions": {
                    "value": "Are there plans to enhance the website's interface design to increase its accessibility for a broader audience? Such improvements could lead to a more diverse user base and a richer dataset distribution."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3046/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3046/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3046/Reviewer_D9AT"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3046/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836656916,
            "cdate": 1698836656916,
            "tmdate": 1699636249649,
            "mdate": 1699636249649,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9MIctOGGfu",
                "forum": "BOfDKxfwt0",
                "replyto": "Fr4883U4KC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3046/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3046/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the feedback.\n\n> While the user prompts in this dataset are genuine, the responses are synthetic and do not have quality ratings. Thus, additional processing is necessary before use.\n\nAlthough limited, our data does include some user ratings (e.g., upvote/downvote) on model responses, in which the upvoted responses were used to train the model in Section 4.3. We will soon release the user feedback dataset after finishing the in-depth study and data cleaning.\n\n> Are there plans to enhance the website's interface design to increase its accessibility for a broader audience? Such improvements could lead to a more diverse user base and a richer dataset distribution.\n\nYes, we\u2019re actively enhancing the website\u2019s interface and committed to growing this platform to a broader user base. In the past few weeks, our traffic has increased significantly due to active community engagement and now we\u2019ve collected over five million user chat queries. Those new data will be included in our next dataset release. Please kindly let us know if you have any suggestions on UI/UX, thanks!"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3046/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631216351,
                "cdate": 1700631216351,
                "tmdate": 1700631216351,
                "mdate": 1700631216351,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qqtd8TlLwF",
                "forum": "BOfDKxfwt0",
                "replyto": "9MIctOGGfu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3046/Reviewer_D9AT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3046/Reviewer_D9AT"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's response, I have carefully read them."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3046/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711031057,
                "cdate": 1700711031057,
                "tmdate": 1700711031057,
                "mdate": 1700711031057,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]