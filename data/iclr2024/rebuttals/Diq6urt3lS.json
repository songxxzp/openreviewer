[
    {
        "title": "Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform"
    },
    {
        "review": {
            "id": "O5exoBYVwy",
            "forum": "Diq6urt3lS",
            "replyto": "Diq6urt3lS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1394/Reviewer_eKtC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1394/Reviewer_eKtC"
            ],
            "content": {
                "summary": {
                    "value": "The authors present, Cleanba, a new platform for distributed DRL that addresses reproducibility issues under different hardware settings. One of the biggest challenges in applying deep reinforcement learning to real world problems is reproducibility. In general, the algorithms are not robust to implementation details which are often crucial for successful real-world applications. The authors introduce a new distributed architecture that is reproducible if random seeds are controlled and leverages interleaving the actor and learner\u2019s computations. The main contribution of the paper is that the authors propose a new mechanism to synchronize the actor and learner which reduces the non-determinism and improves reproducibility. While the synchronization mechanism is simple, the experiments show that Cleanba\u2019s variants can obtain equivalent or higher scores than moolib\u2019s IMPALA, but with 1) less training wall time under the 8 GPU setting and 2) more reproducible learning curves in different hardware settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality & Significance. The proposed synchronization mechanism is simple yet results in significant improvement in training time and reproducibility. Both reduced training time and reproducibility are among the key challenges in applying deep reinforcement learning to real works problems. Therefore despite the simplicity of the proposed mechanism, I believe that the performance improvement is significant. \n\nQuality & Clarity. The authors describe the current state-of-the-art and its pitfalls clearly using clear code samples. They provide empirical results to further their hypothesis on what contribute to the current reproducibility issues in popular distributed reinforcement learning architectures. The experiments are extensive in comparing the proposed architecture to other benchmarks in the literature."
                },
                "weaknesses": {
                    "value": "Originality & Significance. The authors propose to mechanisms to overcome the reproducibility issues in distributed reinforcement learning: 1) de-coupling hyper-parameters, 2) a new synchronization mechanism to better align actors and learners. The de-coupling mechanism is not novel and already exists in prior literature. The synchronization mechanism is simple yet novel. The performance of the new approach is tested only in Atari which is not a real-world problem where reproducibility is a key challenge. The paper can greatly benefit from a real-world application.\n\n\nQuality & Clarity. The language is a bit off in several sections, specifically in the first paragraph, which makes it difficult to read at times. There are several grammatical errors as well."
                },
                "questions": {
                    "value": "At first, I was really confused about the novelty and contribution of the paper. In the abstract and introduction, the paper talks about variants of already existing algorithms but does not talk about the novelty of the proposed variant specifically until past mid-way though the paper. It will be better to talk about the contributions first even when they may be simple yet have far reaching implications to reduce training time and improve reproducibility at scale.\n\nIt is important to consider a real-world example when reproducibility is one of the main challenges. Atari is a great way to benchmark but does not present all the variability such as in robotics applications for example to truly highlight issues related to the reproducibility.\n\nIn Section 5.1, the authors state that Cleanba\u2019s IMPALA is 6.8x faster than monobeast\u2019s IMPALA, mostly because Cleanba actors\nrun on GPUs, whereas monobeast\u2019s actors run on CPUs. I think the performance metrics should be based on the same compute hardware CPU only or GPU only."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1394/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1394/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1394/Reviewer_eKtC"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1394/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782249762,
            "cdate": 1698782249762,
            "tmdate": 1699636067264,
            "mdate": 1699636067264,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MjpFDoktNd",
                "forum": "Diq6urt3lS",
                "replyto": "O5exoBYVwy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1394/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1394/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer\u2019s helpful comments. We are glad they found our synchronization mechanism results in significant improvements in training time and reproducibility.  \n\n> Q: The paper can greatly benefit from a real-world application. Atari is a great way to benchmark but does not present all the variability such as in robotics applications for example to truly highlight issues related to the reproducibility.\n\nThe reproducibility issues related to working with physical robots seem to be in a completely different realm of reproducibility issues dealt with in this paper. Our work focuses on reproducibility issues related to scaling caused by distributed data structures and implementation.\n\n> Q: The language is a bit off in several sections, specifically in the first paragraph, which makes it difficult to read at times. There are several grammatical errors as well.\n\nWe thank this constructive feedback and will get the paper proofread by a native speaker should the paper get accepted. \n\n> Q: In Section 5.1, the authors state that Cleanba\u2019s IMPALA is 6.8x faster than monobeast\u2019s IMPALA, mostly because Cleanba actors run on GPUs, whereas monobeast\u2019s actors run on CPUs. I think the performance metrics should be based on the same compute hardware CPU only or GPU only.\n\nThis is a great observation! GPU only is difficult because both monobeast\u2019s IMPALA and the original IMPALA does not support GPU actors by default. Moolib however is a more fair comparison though, since it runs the actor on GPUs."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1394/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152593356,
                "cdate": 1700152593356,
                "tmdate": 1700152593356,
                "mdate": 1700152593356,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CqQC3R6tDO",
                "forum": "Diq6urt3lS",
                "replyto": "MjpFDoktNd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1394/Reviewer_eKtC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1394/Reviewer_eKtC"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their response to my questions. My rating stays the same."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1394/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686751833,
                "cdate": 1700686751833,
                "tmdate": 1700686751833,
                "mdate": 1700686751833,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yviA01iky9",
            "forum": "Diq6urt3lS",
            "replyto": "Diq6urt3lS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1394/Reviewer_Ui8X"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1394/Reviewer_Ui8X"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents cleanba, a jax based software framework for reproducible distributed deep reinforcement learning (DDRL). The authors present errors that arise from existing non-deterministic DDRL frameworks that could harm reproducibility efforts, then propose a deterministic solution (which comes at the cost of stale data). The authors show the stability and reproducibility of cleanba, and evaluate it on a suite of standard Atari benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Reproducibility is a notorious issue in all of machine learning, but RL especially, and it is good to see more work addressing it\n- The paper is generally clear and understandable\n- Figure/text colouring is generally pretty useful"
                },
                "weaknesses": {
                    "value": "- The abstract\u2019s first sentences feel a bit choppy\n- There are other Sebulba implementations that might be worth comparing to (e.g. https://github.com/instadeepai/sebulba) \n- Fig 2 could motivate the problem better, given that in the current figure both versions end up at very similar scores. If there was a different environment just highlighting that this 1 second lag has a meaningful detriment to final performance that could be really compelling.\n- Although IMPALA is a common algorithm, devoting a short paragraph to it in the background could be helpful (since although many are familiar with it, not as many are with the intricacies as it relates to distributed computing that get mentioned in the next section)\n- In Figure 5, the y axis could have their tickers removed. The absolute numeric scales are not particularly meaningful (I suspect even the most experienced RL researcher will not know if a score of 200,000 on UpNDown is good) and add a lot of clutter to the figure\n- The majority of Fig 1 is highlighted. Maybe it\u2019s better to just highlight the part that is the same? Or remove the highlighting? It just doesn\u2019t add much when the points you want to draw attention to are basically the whole code block.\n- In figure 3, the optimality gap x tickers are overlapping\n- Moolib is a good point of comparison, but I see that this repo is now archived. Are there other DDRL libraries worth comparing to that are more actively maintained?\n- One concern I have is that this paper tries too hard to be an algorithm paper. It presents cleanba, which is a software package that the authors want people to use, but not a lot of time is dedicated to that. More time is dedicated to the analysis of results from cleanba. While these results are somewhat interesting, I think they could be consolidated more (e.g. move the individual atari games to the appendix, and just present HNS, just an idea). Then use that space to actually describe the package of cleanba more (e.g. how does a user interact with it? Extend it? UML diagram perhaps? Etc.). The paper is called \u201ccleanba\u201d for the package, but I can\u2019t say I really know programmatically much about cleanba after reading the paper."
                },
                "questions": {
                    "value": "- I can\u2019t evaluate the code, since there is no anonymized access to it, but I feel given that this introduces a package, that code quality is relevant. Is the code documented well and complete with type hints and docstrings and the like?\n- How easy is it to extend the framework? What plans are there for other algorithms? What do the development goals look like from here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1394/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1394/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1394/Reviewer_Ui8X"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1394/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817762375,
            "cdate": 1698817762375,
            "tmdate": 1700377715661,
            "mdate": 1700377715661,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nTyprAYGBM",
                "forum": "Diq6urt3lS",
                "replyto": "yviA01iky9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1394/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1394/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful feedback. We are glad the reviewer found the text/color helpful. \n\n> Q: The abstract\u2019s first sentences feel a bit choppy\n\nWe changed it to \"Distributed Deep Reinforcement Learning (DRL) aims to train autonomous agents in less wall-clock time by leveraging more computational resources\". This puts the parallel construction closer so less difficult to read.\n\n> Q: There are other Sebulba implementations that might be worth comparing to (e.g. https://github.com/instadeepai/sebulba)\n\n`instadeepai/sebulba` looks interesting and we are glad to see more researchers doing open source distributed DRL. Our work differs in several ways:\n\n* **instadeep/sebulba and cleanba gets similar end-to-end performance**, both using about 11 minutes to achieve 800 scores in Breakout when using 128 accelerator cores (see our Appendix F and Figure 5 at their [blog post](https://cloud.google.com/blog/products/compute/instadeep-performs-reinforcement-learning-on-cloud-tpus)).\n* instadeep/sebulba\u2019s PPO has higher FPS but lower data efficiency: instadeep/sebulba\u2019s PPO can reach a 2.25x higher FPS than cleanba\u2019s PPO. Cleanba gets 1.6M FPS with 128 A100s, whereas instadeep/sebulba get 3.6M FPS with 128 TPUv4 cores, but since the end-to-end performance is similar, we can see that instadeep/sebulba\u2019s PPO\u2019s speed up is offset by the reduction in data efficiency. \n* instadeep/sebulba mainly focus on the FPS scaling and end-to-end experiments in Breakout, while our work is more comprehensive (e.g., 57 Atari games, more random seeds, support both IMPALA and PPO, comparison w/ moolib and torchbeast, reproducibility under different hardware settings)\n\n> Fig 2 could motivate the problem better, given that in the current figure both versions end up at very similar scores. If there was a different environment just highlighting that this 1 second lag has a meaningful detriment to the final performance could be really compelling.\n\nThank you for this suggestion. Our cluster is under heavy usage at the moment but happy to explore this later (unfortunately after the discussion period). We may just run these experiments on Breakout since we expect to see a much larger difference. \n\n> In figure 3, the optimality gap x tickers are overlapping\n\nThank you. This is fixed.\n\n> Moolib is a good point of comparison, but I see that this repo is now archived. Are there other DDRL libraries worth comparing to that are more actively maintained?\n\nThere are some DDRL libraries that we thought about comparing but chose not to do it for the following reasons. \n\n1. Ray\u2019s RLlib: we are not aware of its distributed IMPALA reproducing the level of performance in Atari. For example, the following table shows RLlib\u2019s official benchmark at https://github.com/ray-project/rl-experiments#impala-and-a2c ran for 4 (out of 57) games and significantly underperformed IMPALA Shallow under an hour of training\n\n| | RLlib IMPALA 32-workers | IMPALA Shallow (Table C.1, Espeholt et al 2018) |\n| --- | --- | --- |\n| BeamRider | 3,181 | 32,463.47 |\n| Breakout | 538 | 640.43 |\n| QBert | 10,850 | 351,200.12 |\n| Spacelnvaders | 843 | 43,595.78 |\n\n2. DeepMind/acme: we noted that its technical report [V2](https://arxiv.org/pdf/2006.00979.pdf) contained only Atari evaluations on 5 (out of 57) games and revealed no runtime information. Its [v1](https://arxiv.org/pdf/2006.00979v1.pdf) revealed runtime information IMPALA but it seems quite different from the original IMPALA results\n  * For example, ACME\u2019s IMPALA takes 12 hours to get 100k in Asteroids, 24 hours to get 750 in Breakout, but original IMPALA (deep) reported 100k in Asteroids and 783 in Breakout in ~2 hours (the runtime is interpolated based on Figure 2 in https://openreview.net/pdf?id=r1lyTjAqYX)\n  * We also tried running ACME\u2019s IMPALA and got very low FPS (like 1200 FPS) both in single GPU mode and distributed node for some reason\u2026 We suspect a higher performance of ACME might be coupled with DeepMind\u2019s internal infrastructure. \n\nWe chose to evaluate against moolib and tourchbeast because they, to our knowledge, are the **only** open-source DDRL infrastructures that have evaluated their performance on 50+ Atari games using the 200M frames. 200M frames also make the computational resources reasonable for us (about 2 hours per game with a single GPU or 20-30 minutes per game with 8 GPUs), as opposed to Seed RL which is evaluated on 40B frames (about 40 hours using their settings \u2014 8xTPUv3, 213 CPUs)\n\nThe lack of highly reproducible and easy-to-understand DDRL infrastructure is what partly motivated this work \u2014 we aim to make DDRL more accessible and transparent for the research community."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1394/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152273903,
                "cdate": 1700152273903,
                "tmdate": 1700452146328,
                "mdate": 1700452146328,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1wlKwRKiZq",
                "forum": "Diq6urt3lS",
                "replyto": "yviA01iky9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1394/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1394/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (cont'd)"
                    },
                    "comment": {
                        "value": "> Q: I can\u2019t evaluate the code, since there is no anonymized access to it, [...] The paper is called \u201ccleanba\u201d for the package, but I can\u2019t say I really know programmatically much about cleanba after reading the paper.\n\nWe made the anonymized version of Cleanba available at https://anonymous.4open.science/r/cleanba-test-9868/README.md. It\u2019s basically two files: cleanba_ppo.py and cleanba_impala.py, both of which are self-contained and within 800 lines of code. \n\nWe just want to provide simple-to-understand DDRL infrastructure which is battle-tested and scalable in various scenarios, so we ran extensive evaluation on 57 Atari games and reproducibility under different hardware scenarios.\n\nOn the other end of the spectrum, we have libraries like RLlib / ACME, which are monolithic. Have tons of abstraction and modules (ACME has 500 python files, and Ray (including) has 2.8k Python files). A motivating (dummy) question for us is \u201cDo we really need to read hundreds of interconnected files to understand how DDRL works from end to end?\u201d\n\nThe main target audience is students and researchers, so they can learn DDRL with a principled approach and hack it for their respective research, similar to CleanRL\u2019s approach to DRL infrastructure (https://www.jmlr.org/papers/v23/21-1342.html). \n\n> Is the code documented well and complete with type hints and docstrings and the like?\n\nCleanba\u2019s implementations are extremely lightweight: they have minimal lines of code and some necessary comments but come with extensive documentation, evaluation, and documentation in the repo/paper. We note that since each Cleanba variant is only 800 lines of code end-to-end without extra modules, adding extensive docstring may not be necessary. \n\nThe users can easily trace the flow of the program all happening in a single file (e.g., cleanba_ppo.py), as opposed to DRL libraries like SB3, where it flows through dozens of files like below. In SB3, since there are hundreds of classes and methods involved, docstrings are necessary. In cleanba_ppo.py, there are only 7 classes and 16 functions, all of which are self-explanatory.\n\nThe list of involved files in SB3 (taken from Appendix B in https://www.jmlr.org/papers/volume23/21-1342/21-1342.pdf)\n1. stable baselines3/ppo/ppo.py \u2014 315 lines of code (LOC), 51 lines of docstring (LOD)\n2. stable baselines3/common/on policy algorithm.py \u2014 280 LOC, 49 LOD\n3. stable baselines3/common/base class.py \u2014 819 LOC, 231 LOD\n4. stable baselines3/common/utils.py \u2014 506 LOC, 195 LOD\n5. stable baselines3/common/env util.py \u2014 157 LOC, 43 LOD\n6. stable baselines3/common/atari wrappers.py \u2014 249 LOC, 84 LOD\n7. stable baselines3/common/vec env/ init .py \u2014 73 LOC, 24 LOD\n8. stable baselines3/common/vec env/dummy vec env.py \u2014 126 LOC, 25 LOD\n9. stable baselines3/common/vec env/base vec env.py \u2014 375 LOC, 112 LOD\n10. stable baselines3/common/vec env/util.py \u2014 77 LOC, 31 LOD\n11. stable baselines3/common/vec env/vec frame stack.py \u2014 65 LOC, 14 LOD\n12. stable baselines3/common/vec env/stacked observations.py \u2014 267 LOC, 74 LOD\n13. stable baselines3/common/preprocessing.py \u2014 217 LOC, 68 LOD\n14. stable baselines3/common/buffers.py \u2014 770 LOC, 183 LOD\n15. stable baselines3/common/policies.py \u2014 962 LOC, 336 LOD\n16. stable baselines3/common/torch layers.py \u2014 318 LOC, 97 LOD\n17. stable baselines3/common/distributions.py \u2014 700 LOC, 228 LOD\n18. stable baselines3/common/monitor.py \u2014 240 LOC, 76 LOD\n19. stable baselines3/common/logger.py \u2014 640 LOC, 201 LOD\n20. stable baselines3/common/callbacks.py \u2014 603 LOC, 150 LOD\n\n> How easy is it to extend the framework? What plans are there for other algorithms? What do the development goals look like from here?\n\nExtending Cleanba is pretty straightforward: the researcher can just clone the Cleanba variants and make whatever modifications they desire. Cleanba variants have less than 800 lines of code and make minimal abstractions/modularities, so it opens up a lot of possibilities for developing customized features.\n\nWe do not plan to add other algorithms at this time. The purpose of Cleanba may be similar to https://github.com/karpathy/nanoGPT: small, clean, and highly hackable, and has strong empirical performance."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1394/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152289108,
                "cdate": 1700152289108,
                "tmdate": 1700152604006,
                "mdate": 1700152604006,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Q8qFyVkjuH",
                "forum": "Diq6urt3lS",
                "replyto": "1wlKwRKiZq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1394/Reviewer_Ui8X"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1394/Reviewer_Ui8X"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors responses, and they have addressed many of my concerns. As such I am upgrading my score (5->6)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1394/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377707207,
                "cdate": 1700377707207,
                "tmdate": 1700377707207,
                "mdate": 1700377707207,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6aGIlYjizE",
            "forum": "Diq6urt3lS",
            "replyto": "Diq6urt3lS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1394/Reviewer_Z35M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1394/Reviewer_Z35M"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Cleanba, a new reproducible distributed RL training platform. It verifies the reproducibility on PPO and IMPALA in different hardware settings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It implements a new reproducible and fast distributed RL training platform."
                },
                "weaknesses": {
                    "value": "1) The paper does not touch the real problems that cause the reproducibility issues. It not only occurs in distributed RL training but also occurs in the single machine training even using the same hyperparameters. Reverb and [1] have reported that replay ratio (the number of gradient updates per environment transition) greatly affects the performance. [1] Revisiting Fundamentals of Experience Replay\n2) The Cleanba shown in Figure 1 does not match the \u201clearner always learns from the rollout data obtained from the second latest policy\u201d (line 7).\n3) \u201censuring the learner performs gradient updates with rollout data of second latest policy\u201d assumption is too strong. It may slow down the IMPALA training.\n4) It would be beneficial to provide more reproducibility results in relation to different RL algorithms such as Ape-X, SeedRL, etc."
                },
                "questions": {
                    "value": "Please address the above weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1394/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1394/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1394/Reviewer_Z35M"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1394/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839028146,
            "cdate": 1698839028146,
            "tmdate": 1699636067093,
            "mdate": 1699636067093,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5Bcrydn1Hd",
                "forum": "Diq6urt3lS",
                "replyto": "6aGIlYjizE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1394/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1394/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "> Q: The paper does not touch the real problems that cause the reproducibility issues. It not only occurs in distributed RL training but also occurs in the single machine training even using the same hyperparameters.\n\nYes, we absolutely agree that reproducibility issues can occur in single-machine settings even using the same hyperparameters since we are dealing with stochastic environments. Some environments can be more brittle than others. For example, in SpaceInvaders-v5, two of the Cleanba PPO (Sync) runs get ~20,000 scores, but one run got ~2,000 scores, whereas all three runs of Cleanba PPO (Sync) get ~450 scores.\nIn this paper, we are focusing on reproducibility issues related to scaling caused by distributed data structures and implementation.\n\n>Q: Reverb and [1] have reported that replay ratio (the number of gradient updates per environment transition) greatly affects the performance. [1] Revisiting Fundamentals of Experience Replay\n\nThanks for referencing this very interesting work on *off-policy* DRL algorithms, however, it may not be related to our work which focuses on *on-policy* algorithms like PPO and IMPALA. In particular, PPO and IMPALA do not have a replay buffer like in reverb; if we tried to use [1]\u2019s terminology, then the *age of a transition* is always 1, and the *age of the oldest policy* is always 2 under the Cleanba architecture. \n\n> Q: The Cleanba shown in Figure 1 does not match the \u201clearner always learns from the rollout data obtained from the second latest policy\u201d (line 7).\n\nThere is a pre-condition \u201c**starting from iteration 3** the learner always learns from the rollout data obtained from the second latest policy\u201d; before iteration 3, the learner does not necessarily learn from the second latest policy.\n\n> Q: \u201censuring the learner performs gradient updates with rollout data of second latest policy\u201d assumption is too strong. It may slow down the IMPALA training.\n\nThough this may be a possibility, our experiments show otherwise: Cleanba IMPALA has obtained equivalent scores and faster training time than monobeast and moolib\u2019s IMPALA. Cleanba\u2019s IMPALA is faster than monobeast\u2019s IMPALA because Cleanba IMPALA run actors on GPU while monobeast\u2019s IMPALA run actors on CPU. Cleanba\u2019s IMPALA is faster than moolib\u2019s IMPALA likely because Cleanba\u2019s JAX + EnvPool is faster than Moolib\u2019s PyTorch + various C++ async operations (e.g., async accumulation of gradients, async environment stepping, async network evaluation). \n\n> Q: It would be beneficial to provide more reproducibility results in relation to different RL algorithms such as Ape-X, SeedRL, etc.\n\nTo the best of our knowledge, there is no open-source Ape-X replication that has public results showing that they could reproduce 350% median HNS in 20 hours. Without reliable Ape-X infrastructure, it\u2019s not viable to provide reproducibility results. \n\nReproducibility results with Seed RL is challenging for several reasons:\n* Seed RL is already unmaintained and running its code can be a challenge. \n  * We tried to follow its setup instructions \u201c./run_local.sh atari r2d2 4 4\" and it immediately errored out with some GPG errors `W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease: The following  signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC`\n  * We then tried fixing it with https://github.com/NVIDIA/nvidia-docker/issues/1632#issuecomment-1112667716 and got \n      ```ERROR: Could not find a version that satisfies the requirement ale-py~=0.8.0; extra == \"atari\" (from gym[atari]) (from versions: 0.6.0.dev20200207, 0.7rc0, 0.7rc1, 0.7rc2, 0.7rc3, 0.7rc4, 0.7, 0.7.1)```\n  * We then kept trying to fix it for 30 minutes to no avail and it was just an error every step of the way (e.g., cmake errors)\n\nIts repository contains many reproducibility-related issues: \n* unable to reproduce FPS (https://github.com/google-research/seed_rl/issues/80) \n* unable to reproduce Pong results (https://github.com/google-research/seed_rl/issues/75, https://github.com/google-research/seed_rl/issues/51) \n* does not work as expected with TPU / never tested end-to-end results with GPUs (https://github.com/google-research/seed_rl/issues/78#issuecomment-1258515133)\n* error with multiple GPUs (https://github.com/google-research/seed_rl/issues/16)\n\nWe chose to evaluate against moolib and tourchbeast because they, to our knowledge, are the only open-source DDRL infrastructures which have evaluated their performance on 50+ Atari games using the 200M frames. 200M frames also make the computational resources reasonable for us (about 2 hours per game with a single GPU or 20-30 minutes per game with 8 GPUs), as opposed to Seed RL which is evaluated on 40B frames (about 40 hours using their settings \u2014 8xTPUv3, 213 CPUs)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1394/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151946990,
                "cdate": 1700151946990,
                "tmdate": 1700452028491,
                "mdate": 1700452028491,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "euz10Cu5ij",
                "forum": "Diq6urt3lS",
                "replyto": "5Bcrydn1Hd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1394/Reviewer_Z35M"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1394/Reviewer_Z35M"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the author's response. It addresses some of my questions. However, my main concern regarding the generalization of the proposed methods to different types of RL algorithms remains unaddressed. Additionally, learning from rollout data of second latest policy cannot provide a mathematical guarantee for exact result reproduction of original on-policy RL algorithms."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1394/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656597495,
                "cdate": 1700656597495,
                "tmdate": 1700656597495,
                "mdate": 1700656597495,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cZl5pOZ4lL",
                "forum": "Diq6urt3lS",
                "replyto": "6aGIlYjizE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1394/Reviewer_Z35M"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1394/Reviewer_Z35M"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the author's response. My question is that Cleanba cannot exact reproduce the original PPO algorithm althrough Cleanba can reproduce itself without environment randomness and GPU non-determinism."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1394/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738044047,
                "cdate": 1700738044047,
                "tmdate": 1700738446141,
                "mdate": 1700738446141,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "53hhzQ90ce",
            "forum": "Diq6urt3lS",
            "replyto": "Diq6urt3lS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1394/Reviewer_fstX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1394/Reviewer_fstX"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce CleanBa, a distributed RL library containing distributed implementations of IMPALA and PPO. They demonstrate that IMPALA's actor-learner architecture causes issues with reproducibility \nbecause the learner can do a different number of updates depending on its speed. \n\nTo solve this, they then propose a new architecture, where the policy updates based on the rollouts of the second latest policy. This ensures reproducibility at the cost of more synchronisation and the introduction of stale data.\n\nThey demonstrate that their framework performs well compared to relevant baselines and runs much more quickly."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper is well written and clearly explained.\n* The reproducibility problems identified in IMPALA are interesting and well addressed by their new framework.\n* The empirical results demonstrate the framework is faster than prior methods on comparable hardware."
                },
                "weaknesses": {
                    "value": "* The authors claim to have built a distributed learning framework, but have only evaluated their method on a single machine. This is a major flaw in their evaluations, and the reason I recommend rejection for this paper. If this is included I will increase my score. \n\n\nTypos: \nAt the end of section 4.2 you write `jax.distibuted` -- I think this should be `jax.distributed` (extra r)."
                },
                "questions": {
                    "value": "* How does the extra param queue impact the distributed performance (i.e. speed)? i.e. how much cost is paid in speed for fixing the reproducibility issues?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1394/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1394/Reviewer_fstX",
                        "ICLR.cc/2024/Conference/Submission1394/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1394/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699375096928,
            "cdate": 1699375096928,
            "tmdate": 1700670204401,
            "mdate": 1700670204401,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MSv3Y1y1KI",
                "forum": "Diq6urt3lS",
                "replyto": "53hhzQ90ce",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1394/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1394/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the helpful feedback, and we are glad they found the reproducibility problem identified in IMPALA interesting.\n\n\n> Q: The authors claim to have built a distributed learning framework, but have only evaluated their method on a single machine. This is a major flaw in their evaluations, and the reason I recommend rejection for this paper. If this is included I will increase my score.\n\nWe thank the reviewer for this constructive feedback. Cleanba **can** scale to multi-node and we have included evaluations of multi-node training in Appendix F. Multi-node training is computationally expensive, so we focus evaluation specifically on a representative Atari game Breakout instead of the full Atari suite. We have also mentioned the multi-node scaling results briefly in the main text.\n\n\n> Q: Typos: At the end of section 4.2 you write jax.distibuted -- I think this should be jax.distributed (extra r).\n\nThanks for the correction on our typo.\n\n> Q: How does the extra param queue impact the distributed performance (i.e., speed)? i.e. how much cost is paid in speed for fixing the reproducibility issues?\n\nThere are some idle times associated with ensuring the actor always learns from the second latest policy. We measure the estimated training time and rollout time, estimate how much percentage of the actor and learner computations can overlap (`min(training time, rollout time) / max(training time, rollout time)`), and as a result, how much time either the learner or actor is idling `( max(training time, rollout time) - min(training time, rollout time)) * the number of updates`. \n\n|                                    | training time   | rollout time    | learner / actor computation overlap percentage | actor / learner idle time (mins) |\n|------------------------------------|-----------------|-----------------|------------------------------------------------|------------------------------------|\n| Cleanba IMPALA, 1 A100, 10 CPU     | 0.1130 \u00b1 0.0003 | 0.3864 \u00b1 0.0045 | 29.24%                                         | 88.99625667                        |\n| Cleanba IMPALA, 8 A100, 46 CPU     | 0.0453 \u00b1 0.0004 | 0.0585 \u00b1 0.0002 | 77.44%                                         | 4.29682                            |\n| Cleanba PPO (Sync), 1 A100, 10 CPU | 2.0839 \u00b1 0.0024 | 2.2496 \u00b1 0.0264 | -                                              | -                                  |\n| Cleanba PPO (Sync), 8 A100, 46 CPU | 0.3257 \u00b1 0.0004 | 0.4945 \u00b1 0.0010 | -                                              | -                                  |\n| Cleanba PPO, 8 A100, 46 CPU        | 0.5720 \u00b1 0.0058 | 0.4198 \u00b1 0.0016 | 73.39%                                         | 7.73937                            |\n| Cleanba PPO, 1 A100, 10 CPU        | 2.1270 \u00b1 0.0005 | 3.6205 \u00b1 0.3244 | 58.75%                                         | 75.944475                          |\n\nTake the second row as an example, the training time and rollout time of Cleanba IMPALA, 8 A100, 46 CPU is roughly similar, so they can overlap 77.44% of learner and actor\u2019s computation. We then get 0.0585 * 19531 = 1142 seconds ~= 19 minutes of the estimated total runtime (roughly similar to the empirical total runtime of 22 minutes which included initial compilation time and other overhead), during which the learner idled for (0.0585 - 0.0453) * 19531 ~= 257 seconds ~= 4 minutes\n\nWhile the idle time may seem like a lot, it\u2019s unclear whether removing these idle times can result in faster end-to-end training. Without the restriction that the agent can only learn from the second latest policy, the agent may learn from more stale data. Take the recent result from instadeep/sebulba (https://cloud.google.com/blog/products/compute/instadeep-performs-reinforcement-learning-on-cloud-tpus), which removes the aforementioned restriction. As a result, their PPO runs seemingly faster and get ~2.2x higher FPS than Cleanba\u2019s PPO. However, the end-to-end training results are almost identical \u2014 both achieved about ~800 scores in Breakout-v5 in 10 minutes."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1394/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151670843,
                "cdate": 1700151670843,
                "tmdate": 1700152637062,
                "mdate": 1700152637062,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v1WqDX1CSd",
                "forum": "Diq6urt3lS",
                "replyto": "wFBKzcupO3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1394/Reviewer_fstX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1394/Reviewer_fstX"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks very much for your thorough reply to my comment -- it was interesting to read.\n\n> Cleanba can scale to multi-node and we have included evaluations of multi-node training in Appendix F\n\nMy apologies! I completely missed this section. I am usually careful to check appendices for results I think are missing but obviously somehow missed them on this occasion.\n\nI will update my score to vote for acceptance."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1394/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670172094,
                "cdate": 1700670172094,
                "tmdate": 1700670172094,
                "mdate": 1700670172094,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]