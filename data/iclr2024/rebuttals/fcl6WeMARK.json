[
    {
        "title": "Improved Regret Bounds in Stochastic Contextual Bandits with Graph Feedback"
    },
    {
        "review": {
            "id": "7EiV7vibw2",
            "forum": "fcl6WeMARK",
            "replyto": "fcl6WeMARK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1984/Reviewer_Z4bM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1984/Reviewer_Z4bM"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of (stochastic) contextual bandits under feedback graphs, which generalizes the classic contextual bandit problem. Specifically, the authors consider the realizable setting [Foster et al., 2018, Foster et al., 2020] where the true expected loss function $f^\\star$ lies in a function class $\\mathcal{F}$, which is known to the learner. The authors proposed an algorithm based on FALCON [Simchi-Levi & Xu 2022] but with a different choice of exploration set $S_t$ and claimed to achieve $O(\\sqrt{\\delta T})$ regret where $\\delta$ is the averaged expected domination number of feedback graph. This is achieved by selecting $S_t$ to be the dominating set of the graph. The author also shows that with an adaptive tuning technique proposed in [Foster et al., 2020], their proposed algorithm achieves the gap dependent bound $\\tilde{O}(\\delta/\\Delta)$. Moreover, the authors also prove that the problem-independent lower bound is $\\Omega(\\sqrt{\\delta T})$."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The problem considered in this paper is important and the motivation is clearly stated."
                },
                "weaknesses": {
                    "value": "- This paper is not written clearly and does not provide clear proofs for the claimed theorems. Specifically, I do not find proofs about the $O(\\sqrt{\\delta T})$ regret upper bound in the appendix. In fact, this $O(\\sqrt{\\delta T})$ result just should not hold since it breaks the lower bound proven in [Alon et al., 2015, 2017] even in the non-contextual setting. Consider the star graph (or K-tree graph in the context of this paper). If the center node has 1 loss and the remaining nodes have $Ber(1/2)$ loss except for one node having $Ber(1/2-\\epsilon)$ loss, we can not do better than $\\min(\\sqrt{KT}, T^{2/3})$ regret bound but what claimed in this paper is a $O(\\sqrt{T})$ regret bound. Although the authors do not show the exact analysis in the appendix for the $O(\\sqrt{\\delta T})$ result, technically, the error is that their Lemma B.2, Lemma B.3 both consider the policy on $\\Psi(S_t)$ but the regret benchmark may not be within this set, making the analysis break.\n\n- For the upper bound result achieving $O(\\sqrt{\\alpha T})$ regret, option 1 for ConstructExplorationSet is actually also not new and is shown in Proposition 4 of [Zhang et al., 2023] for self-aware undirected graphs. Therefore, I feel that the result with respect to the independence number is also not hard to obtain based on FALCON and this exploration set construction."
                },
                "questions": {
                    "value": "Can authors provide detailed proofs for the results claimed in the paper, especially for the upper bound results that is related to the dominating number?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1984/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697916979613,
            "cdate": 1697916979613,
            "tmdate": 1699636130145,
            "mdate": 1699636130145,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "qmqmJc4cS0",
            "forum": "fcl6WeMARK",
            "replyto": "fcl6WeMARK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1984/Reviewer_7kWt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1984/Reviewer_7kWt"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors consider a contextual bandits framework with a general reward function space and side-observations given to the learner in form of (informed) feedback graphs. The authors propose a new algorithm that achieves an improved upper-bound (instance-independent) regret upper-bound than previously known algorithms.  The authors proceed to prove a lower-bound to show the near-optimality of this algorithm (ignoring logarithmic terms and constants). Finally, some modifications are introduced in order to derive a gap-dependent upper-bound. Several experiments are conducted to highlight the improvements of proposed algorithms (and further analyses are realized to show the limitations of previously known algorithms)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "As I understand, the paper poses a quite interesting question on the extension of contextual bandit framework with general reward function space (and the associated inverse gap weighting technique) into scenarios with graphical feedbacks. In general, the writing is quite good (although the structure is less so). The authors attempt to provide both concrete proofs and intuition explanation, this is applaudable.\n \nMoreover, the authors have done a quite comprehensive comparison with three previous works that facilitates the review tasks.  The authors also keep both options in building the exploration set (which seems to be the key ingredient of the proposed algorithm) and analyse the involved exploration-exploitation trade-off (on top of the traditional trade-off of bandit); which is rather quite interesting although this make the presentation more cumbersome (see below)."
                },
                "weaknesses": {
                    "value": "- The paper is not very well structured; it is not easy to understand the flow of the paper. In particular, although Table 1 and 2 capture quite well the main contributions, the involved notation are not introduced there but scattering across the paper (hence, readers need to revise these tables after finishing the paper to understand the notation). Similarly, while it is good to have a comparison with previous works in Section 1.2, it is hard to understand as no proper description of the contribution is introduced yet (for example, the beginning of page 4 discusses \u201cfirst and second options\u201d that has never been mentioned before). A simplified version of main theorem before this section, for example, could facilitate the comprehension here. \n- Another major critic is that it does not seem necessary to present ConstructExplorationSet with two options (or at least it lacks a major justification for the necessity of this). As I understand from page 6, \u201cOption 2\u201d is recommended to be used with an integration of empirical best arms from Option 1. Moreover, from Table 1, the major improvement (switching from bounds with independence numbers to bounds with dominating numbers) comes from Option 2. The experiments also show the speriority of this Option 2. I do not see why the authors cannot simply combine these two so-call options into one procedure. \n- The notion delta_f of fractional dominating number is important to this paper, but it is never defined properly. As mentioned above, the ability to obtain a method having bounds with this delta_f instead of alpha is a major point; however, the proof is written only for the alpha case (so-called option 1) and the detailed proof for delta_f is omitted in appendices. Only a small explanation is presented in page 6 that is not sufficient. \n- The idea of the main algorithm is quite natural and obvious (applying IGW to a well-selected exploration set). Can the authors highlight further any novelty of this algorithm or the contribution comes more in the proof aspects? \n- Another point that should be mentioned is that the main result is of the high-probability bound flavour that differs from most of previous works that are directed compared."
                },
                "questions": {
                    "value": "- Technically, in Tables 1 and 2, results of Wang et al.2021 can be presented with upper-bound of alpha(G_t) and hence, only require to know this upper-bound instead of the real independence number. This mitigate the \"critics\" in page 4. \n\n- Do the authors choose to run experiements only in comparison with FALCON and not the one of Zhang et al. 2023 because the latter is instable? \n\n- Why do the authors prefer high-probability bound? Is it posible/easy to derive an expected regret result from Theorem 3.1?\n\n- The presented framework uses undirected feedback graph, can this be extended with directed ones? (note that Zhang et al. consider directed graph). \n\n- Can we have a definition of the set S_t?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethical issues with this paper to signify."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1984/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1984/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1984/Reviewer_7kWt"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1984/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698690820044,
            "cdate": 1698690820044,
            "tmdate": 1699636130057,
            "mdate": 1699636130057,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZhKPp4y9th",
                "forum": "fcl6WeMARK",
                "replyto": "qmqmJc4cS0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1984/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1984/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q: The paper is not very well structured\n\nA: Thanks for suggestions. To help readers understand our work better, we add the name of graph parameters in the caption of tables. \nWe also show the order sequence of this graph parameters to highlight our contributions. \nAs for ``a simplified version of main theorem'', we will try to put it in our work if there exists additional space.\nAfter all, the limited page requirement of ICLR forces us to put formal descriptions in the following sections. \n\n\nQ: it does not seem necessary to present ConstructExplorationSet with two options\n\nA: Our main contribution stems from the second option. \nHowever, the first option is not always useless, as in certain types of graphs like perfect graphs, \nit can outperform the second option. \nThe two options also capture the trade-off of the exploration between the gaps and number of actions. \nAdditionally, the first option can be served as a rough realization of the method in [1] as their regret order scales with the independence number. \n\n\n\nQ: the contribution comes more in the proof aspects\n\nA: It requires non-trivial modifications of IGW technique. \nThe time-varying hyperparameter $\\gamma_t$ and the adaptive set $S_t$ are the crucial difference of original works. \nThe original works focus on the upper bound so they only need an upper bound $|\\mathcal{A}|$. \nIn order to capture the influence of time-varying action sets, \nwe have to bound the size of action sets in a more refined way. \nIt requires us to define the policy space and prove lemmas in a more refined way. \n\nHere is the high-level intuition of our proof. \nFor any policy $\\pi \\in \\Psi(S_t)$, the regret of $\\pi$ with respect to the greedy policy can be controlled by our lemma B.1. \nSecondly, since the sample size will increase as the learning processes, the regret of the greedy policy with respect to the optimal policy will become smaller, \nas shown in lemma B.2. \nDue to these two lemmas, \nthe policy $\\pi \\in \\Psi(S_t)$ can be controlled.\n\n\n\nQ: the high-probability bound to the expected bound\n\nA: It can be converted to expected bounds by setting $\\delta=1/T$.\n\n\n\nQ: Technically, in Tables 1 and 2, results of Wang et al.2021 can be presented with upper-bound of $\\alpha(G_t)$ and hence, only require to know this upper-bound instead of the real independence number. This mitigate the \"critics\" in page 4.\n\nA: You are correct, but the original content in their work write in the form of $\\alpha(G_t)$.\n\n\n\nQ: Do the authors choose to run experiments only in comparison with FALCON and not the one of Zhang et al. 2023 because the latter is instable? \n\nA: We appreciate the Zhang's work[1], which firstly deals with the general function space, \nbut their method need online regression oracle and works in semi-adversarial setting.\nHowever, our method only requires offline regression oracle and works in stochastic setting. \nFor one thing, the adversarial setting is more difficult than the stochastic setting.\nIt usually happens that the algorithms for adversarial settings will have worse regrets than those in the stochastic setting.\nFor another, the online oracle is more difficult to design than the offline oracle. \nOnline oracle is sensitive to the order of input data sequence while offline oracle not.\nEven for a fixed dataset, the input sequence will also affect the output of online oracles.\nAs an example, the OLS estimator is a valid offline oracle but not a valid online oracle.\n\nTo see our efforts, we refer the reviewers to option 1.\nRoughly speaking, the option 1 can be viewed as the realization of this method,\nthough not fully identical. \nWe try to find effective baselines and compare them in theory and simulations.\n\n\nQ: The presented framework uses undirected feedback graph, can this be extended with directed ones? (note that Zhang et al. consider directed graph).\n\nA: This is an interesting work to extend our method to general graphs. \nHowever, as we admit in the conclusion section in Appendix D, \nwe haven't find a way to deal with general graphs. \n\n\nQ: Can we have a definition of the set $S_t$?\n\nA: The name of $S_t$ is called the exploration set. \n\n\n[1] Practical contextual bandits with feedback graphs"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1984/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664396035,
                "cdate": 1700664396035,
                "tmdate": 1700664396035,
                "mdate": 1700664396035,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TBEmrzmK6K",
            "forum": "fcl6WeMARK",
            "replyto": "fcl6WeMARK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1984/Reviewer_nWZ2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1984/Reviewer_nWZ2"
            ],
            "content": {
                "summary": {
                    "value": "The paper tackles the challenging problem of multi-armed bandits with arm rewards in general function spaces and time-varying graph feedback.  The central challenge is the quantification of the graph feedback influence on the regret. To make things harder, the graph changes with time and the reward function doesn't have a closed-form structure that can be exploited. \n\nThe authors propose the algorithm FALCON.G to tackle the MAB with a probabilistic time-varying graph feedback problem.  The authors provide both gap-dependent and gap-independent regret bounds and provide matching lower bounds to showcase the optimality of the same. This is supplemented by simulation evidence showcasing the prowess of the proposed methods"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper exhibits several commendable strengths. \n1. The authors have done an exceptional job in comparing themselves with other closely related works, ensuring that they improve on the subject matter with this paper. This meticulous attention to detail provides readers with a comprehensive understanding of the existing literature in the field. \n2. The proposed method FALCON.G is theoretically proven to be optimal by showing matching upper and lower bounds. Showcasing the dependence on $\\delta_f(\\cdot)$ for both bounds adds strength to the tightness argument. \n3. Both routine of \"Option 1\" and \"Option 2\" setups not only demonstrate practicality but also significantly enhances the complexity of problem-solving as compared to previous works.\n4. Overall the clarity and coherence of the writing make the paper accessible and easy to follow."
                },
                "weaknesses": {
                    "value": "I would really appreciate the author's comments on the following:\n\n1. **UCB-like approach**: The necessity for forced exploration is not clearly justified. The possibility of employing a UCB (Upper Confidence Bound) type scheme is not explored in depth. It would be beneficial if the authors could provide an explanation of the challenges or limitations associated with the UCB approach.\n2. **Offline Oracle**: The usage of regression oracle in FALCON.G resembles (in my opinion) batch regression rather than offline regression.  Especially when considering that FALCON.G utilizes it in the inner loop, albeit not at every cycle. Would you strengthen your argument on the usage of the \"offline regression\"? Also, what would be the typical complexity for solving the regression problem or would it be an artifact of the functional form of rewards?\n3. **Fundamental importance of $\\delta_f(\\cdot)$**: Would really appreciate a section on the discussion as to whether this graph parameter $\\delta_f(\\cdot)$ is fundamental to the problem or is it just an artifact of the design of FALCON.G and proof methodology. \n4. **Real-world dataset**: Would you see any impending issues for running the simulations on a real-world dataset or dataset with much higher dimensions? \n\nI am willing to change the score after responses from the authors on the above concerns."
                },
                "questions": {
                    "value": "The paper, while detailed, leaves a few questions unanswered:\n\n1. Theorem 3.1's phrasing presents a contradiction. On one hand, it mentions that the \"expectation of regret is taken w.r.t. all kinds of randomness,\" but then goes on to state the result is \"with high probability.\" What is the specific randomness associated with the high probability argument, and how does it differ from the randomness tied to the expectation? Could you clarify this split?\n\n2. It would be beneficial to have real-world examples that align with the setup described in the paper. Specifically, are there tangible instances where the function class and changing graphs over time can be observed?\n\n3. Regarding Theorem 3.2, how expansive is the function class? Are there any practical applications or examples that fall within these classes that can provide a clearer context?\n\n4. In the simulations section, is it feasible to use the baseline of algorithms from related works for comparison? This would offer a more holistic view of how the proposed methods stack up against existing solutions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1984/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1984/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1984/Reviewer_nWZ2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1984/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699120912175,
            "cdate": 1699120912175,
            "tmdate": 1699636129980,
            "mdate": 1699636129980,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x4vPywJedv",
                "forum": "fcl6WeMARK",
                "replyto": "TBEmrzmK6K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1984/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1984/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Q: the forced exploration and the construction of the exploration set $S_t$\n\nA: The forced exploration is necessary to avoid suboptimal greedy policy.\nWe need to control the size of the exploration set to reduce regret order.\n\n\n\n\nQ: UCB-like approach\n\nA: It is widely known that UCB can achieve comparable performance with sampling method. \nHowever, it is unclear how one can build valid confidence upper bounds on general function space. \nIt is also unknown whether one can truncate the action set as we do.\nTo point out the challenges, the previous works [1,2,3] about general function space all focus on IGW rather than UCB. \n\n\n[1] Practical contextual bandit with regression oracle\n[2] Bypassing the monster: A faster and simpler optimal algorithm for contextual bandits under realizability \n[3] Instance-dependent complexity of contextual bandits and reinforcement learning: A disagreement-based perspective\n\n\n\nQ: Fundamental importance of $\\delta_f(G)$\n\nA: This is a crucial factor in stochastic graph feedback setting, which dates back to [1]. \nIt control the minimal size of arms which should be explored. \nSuch factor can be obtained by solving LP \n\\begin{equation}\n    \\begin{aligned}\n    & \\min \\mathbf{1}^\\top \\mathbf{z} \\\\\n    \\text{s.t.} \\ \\ \\ & G_t \\mathbf{z} \\geq \\mathbf{1} \\\\\n    & \\mathbf{z} \\geq \\mathbf{0}.\n    \\end{aligned}\n\\end{equation}\nThe LP can be interpreted as following. \nIf one want to explore all arms at least once, how many arms should one pull given the graph $G$.\nThe LP can help control the number of exploration in a most efficient way. \nThe appearance of this factor in upper and lower bounds[1,2] also show its importance in analysis.\n\n\n\n[1] Reward maximization under uncertainty: leveraging side-observations on networks\n[2] Contextual bandit with side-observations\n\n\n\n\n\nQ: It would be beneficial to have real-world examples that align with the setup described in the paper. Specifically, are there tangible instances where the function class and changing graphs over time can be observed?\n\nA: We haven't found any open dataset which fully fit our setting. \nThough we can conduct experiments in real-world datasets, what we obtain is a regret curve. \nTo illustrate the basic property, we believe that the randomly generalized contexts and graphs are enough.\n\n\n\nQ: Theorem 3.1's phrasing presents a contradiction. On one hand, it mentions that the \"expectation of regret is taken w.r.t. all kinds of randomness,\" but then goes on to state the result is \"with high probability.\" What is the specific randomness associated with the high probability argument, and how does it differ from the randomness tied to the expectation? Could you clarify this split?\n\nA: Thanks to point out our typos. The expectation is taken with respect to the randomness of algorithms.\nIf one wants to get the expected regret bounds, one can simply set $\\delta=1/T$ and it only leads to additional $\\delta T$ regrets, which is a constant.\n\n\nQ: Regarding Theorem 3.2, how expansive is the function class? Are there any practical applications or examples that fall within these classes that can provide a clearer context?\n\nA: Theorem 3.2 provides a lower bound. Typically, such bound is proved by construction to show the difficulty level of this problem.\nIf one cares about practical applications, the gap-dependent upper bound in Theorem 3.3 might be more meaningful, \nas it holds for general gap $\\Delta$ and general function space $\\mathcal{F}$.\n\n\nQ: In the simulations section, is it feasible to use the baseline of algorithms from related works for comparison? This would offer a more holistic view of how the proposed methods stack up against existing solutions.\n\nA: For our mentioned works, their settings are not fully identical to us. \n[1] works in MAB with fixed graphs. \n[2] works for linear contextual graphs with fixed graphs. \n[3] deals with the general function space, but their method need online regression oracle and works in semi-adversarial setting.\nHowever, our method only requires offline regression oracle and works in stochastic setting. \nRoughly speaking, the option 1 can be viewed as the realization of this method,\nthough not fully identical. \nTo compare them effectively, we have to illustrate the theoretical properties of this work in Table 1.\n\n\n\n\n[1] Reward maximization under uncertainty: leveraging side-observations on networks\n[2] Contextual bandit with side-observations\n[3] Practical contextual bandits with feedback graphs"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1984/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663994049,
                "cdate": 1700663994049,
                "tmdate": 1700663994049,
                "mdate": 1700663994049,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]