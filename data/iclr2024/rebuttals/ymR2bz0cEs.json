[
    {
        "title": "Interaction-centric Hypersphere Reasoning for Multi-person Video HOI Recognition"
    },
    {
        "review": {
            "id": "QBmaxRe5oW",
            "forum": "ymR2bz0cEs",
            "replyto": "ymR2bz0cEs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3816/Reviewer_dGtW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3816/Reviewer_dGtW"
            ],
            "content": {
                "summary": {
                    "value": "This work focused on video HOI recognition and proposed a hypersphere-based method to learn the interdependency between humans, objects, and interactions. The authors proposed several modules like CF, ISR, and BiGRU to build a new pipeline to learn complex spatio-temporal video HOIs. On three benchmarks, the proposed method was evaluated and compared with previous works and showed improvements."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The complex relations within video HOIs are a meaningful problem for intelligent visual understanding, using hypersphere is an interesting attempt.\n\n+ The whole paper is written well and easy to follow."
                },
                "weaknesses": {
                    "value": "- Some design choices were not well illustrated and verified, which will be detailed in the questions.\n\n- Some claims are ambiguous, please give more explanations:\n\nusually lack of ability to capture global context information: which works and why?\n\nultimately compromising their representational accuracy: what is representational accuracy? Why Euclidean cannot?"
                },
                "questions": {
                    "value": "1. Why choose the hypersphere? Its pros upon Euclidean? Maybe discussions and experiments for support.\n\n2. Each class has its own hypersphere, then how to embed the relations between classes, e.g., holding and grasping? Is the current setting reasonable?\n\n3. How to handle the multi-label classifications given hyperspheres, and the long-tailed bias?\n\n4. Discussion about the temporal action localization or segmentation? And some possible comparison between this line of works?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3816/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698330175598,
            "cdate": 1698330175598,
            "tmdate": 1699636339075,
            "mdate": 1699636339075,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Wtx1bAG8s7",
                "forum": "ymR2bz0cEs",
                "replyto": "QBmaxRe5oW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3816/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3816/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the constructive comments. All the questions are addressed as follows."
                    },
                    "comment": {
                        "value": "* ***Capturing global context information.*** All of the CNN-based works listed  Section 2 \"Related Works-HOI detection\" lacks the ability to capture global context information due to the inherent local nature of the CNN structure.\n\n* ***Representational accuracy.*** Representational accuracy indicate the quality of learned representations of entities in the scence. Euclidean distance do not introduce structure bias of HOI into the model as hypersphere does (illustrated in the next question), ignoring valuable structure information of HOI for guiding predictions.\n\n* ***The reason for choosing hyperspher.*** \n\n  **(1)** The interaction-centric hypersphere is crafted to introduce a **structure bias** of HOI into our model. Specifically, this bias asserts that **human-object pairs adhere to their respective interaction classes**. Consequently, the model is compelled to make predictions within the confines of this structure bias. It is essential to clarify that the hypersphere serves merely as a means of visualizing this bias. Alternately, various other techniques may be employed for visualizing the structure bias. Hence, our paper's fundamental proposition is to instill the desired structure bias, emphasizing that hypersphere is just one illustrative manifestation of this broader concept. \n\n  **(2)** The advantage of hypersphere over Euclidean distance is that the latter does not introduce such priors, ignoring valuable structure information of HOI for guiding predictions. In the revised paper, we conduct experiments of utilizing Euclidean distance by setting $\\lambda=0$ in Equation (6). Results in Table 1, 2 and 3 (highlighed in red in the revised paper) show that utilizing Euclidean distance results in at least 7%, 10% and 3% $F_1$ score drop in MPHOI dataset, CAD-120 dataset and Bimanual Actions dataset, respectively. Thus, the effectiveness of the structure bias introduced by hypersphere have been validated. We also show the results below for better understanding:\n\n    |***MPHOI*** | $F_1 @10$  |  $F_1 @25$  |  $F_1 @50$  |  \n    | ------------- | ------------- | --------------  | -------------- |\n    |**Ours**       | **91.6**$\\pm$**0.9** |  **84.5**$\\pm$**2.6** |  **67.7**$\\pm$**2.2** | \n    |Ours ($\\lambda=0$)  |  81.2$\\pm$0.7  |   74.8$\\pm$4.2  |   53.2$\\pm$0.3  | \n\n    |***CAD-120*** | $F_1 @10$               |  $F_1 @25$  |  $F_1 @50$  |  \n    | -------------    | -------------              | --------------  | -------------- |\n    |**Ours**          | **90.7**$\\pm$**2.9** |  **88.1**$\\pm$**2.8** |  **77.6**$\\pm$**4.7** | \n    |Ours ($\\lambda=0$)  |  72.0$\\pm$4.4  |   65.0$\\pm$6.9  |   48.6$\\pm$6.3  | \n\n    |***Bimanual Actions*** | $F_1 @10$  |  $F_1 @25$  |  $F_1 @50$  |  \n    | ------------- | ------------- | --------------  | -------------- |\n    |**Ours**       | **85.0**$\\pm$**2.5** |  **82.9**$\\pm$**2.9** |  **74.2**$\\pm$**4.3** | \n    |Ours ($\\lambda=0$)  |  76.7$\\pm$5.2  |   74.3$\\pm$6.0  |   65.2$\\pm$6.3  |\n\n* ***Relations between classes.*** The relations between interaction classes are modeled by Interaction State Reasoner (ISR) module. ISR explicitly empowers the model to determine whether the current interaction should persist or transit to another interaction with a state interpolation module and a reasoner block shown in Figure 3. \n\n* ***Multi-label and long-tail.*** \n\n     (1) At each time step, there is only one sub-activity for each person. Therefore there is no multi-label problem in HOI recognition task. \n\n     (2) We deal with long-tail problem with focal loss $\\mathcal{L}_{cls}$, which is illustrated in Section 4.3. \n\n* ***Temporal action segmentation.*** Temporal action localization or segmentation works usually focus on a single action of interest happening in the video, which is a different task compared to video HOI recognition, where multiple actions could happen in the input video."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3816/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533610854,
                "cdate": 1700533610854,
                "tmdate": 1700616301606,
                "mdate": 1700616301606,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ymbBJVltkv",
                "forum": "ymR2bz0cEs",
                "replyto": "Wtx1bAG8s7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3816/Reviewer_dGtW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3816/Reviewer_dGtW"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the responses. After reading the reviews and the responses from the authors, I tend to retain my initial rating."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3816/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641587147,
                "cdate": 1700641587147,
                "tmdate": 1700641587147,
                "mdate": 1700641587147,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XjuK7LjXPo",
                "forum": "ymR2bz0cEs",
                "replyto": "QBmaxRe5oW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3816/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3816/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your comments. I would be glad to answer any additional questions if you have."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3816/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705001747,
                "cdate": 1700705001747,
                "tmdate": 1700705116757,
                "mdate": 1700705116757,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IxYqXb5b9l",
            "forum": "ymR2bz0cEs",
            "replyto": "ymR2bz0cEs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3816/Reviewer_NfZw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3816/Reviewer_NfZw"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an interaction-centric hypersphere reasoning model for multi-person video HOI recognition. The design of interaction-centric hypersphere explicitly directs the learning process towards comprehending the HOI manifold structures governed by interaction classes, a hitherto unexplored domain."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The method proposed in the paper is interesting."
                },
                "weaknesses": {
                    "value": "The experimental evaluation is not comprehensive. \nThe presentation for some key concepts and ideas is unclear, which needs extensive improvement."
                },
                "questions": {
                    "value": "Presentation:\n1. The definition of the task is unclear. What is multi-person video HOI recognition? How to understand multi-person interaction and what is the specific expression? If it is multi-person interaction, what is the difference from the research direction of group action recognition? This work is anchored to explore multi-person interactive action recognition, so please clearly describe the task content and specific input and output.\n\nIn addition, regarding the definition of the task of this article, I have some idea until the fourth section. The previous sections do not elaborate on it, which is very good for understanding.\n\n2. Why it is called a hypersphere? What is the meaning of hypersphere and does it have any theoretical implications? Hypergraphs and graphs are different theories. In this paper, what is the difference between the concept of a hypersphere and a sphere?\n\n\u201cThe design of interaction-centric hypersphere explicitly directs the learning process towards comprehending the HOI manifold structures governed by interaction classes, a hitherto unexplored domain.\u201d This hypersphere appears to be used to predict interaction probabilities. Please explain how it differs from traditional classifiers? This explanation is necessary since this hypersphere is the key idea. In addition, there is not much comparison, description, and argumentation between manifold structures and hypersphere theories in this paper. On the contrary, other modules explain more, which makes me wonder what is the core of the paper.\n\n3. What are the HOI manifold structure, which has been mentioned several times in this paper? It is hard to understand. Is it related to Riemannian geometry? Please elaborate it. It is better to have a clear explanation.\n\n4. \u201dTo enhance the awareness of complex HOI structures in our representations, we introduce the Context Fuser (CF)...\u201d Is there any connection between complex HOI and multi-person HOI?\n\n5. \u201cTo facilitate interaction reasoning, we place the ISR module on top of the context fuser module, yielding entity representations capable of capturing interaction transition dynamics.\u201d Does entity representation represent the characteristics of human and object entities? Or does it represent the transition characteristics of the same person or object between different states?\nThis sentence confuses me a lot about what exactly it represents.\n\n6. \u201cHowever, current video HOI recognition methods do not fully explore such inherent structural nature of HOI components. Instead, they often opt for disentangled representations for each component, which may have suboptimal representation capabilities.\u201d It is recommended to visualize the problem to be solved, so that readers can understand it clearly.\n\n7. In Figures 2 and 3, it is better to replace the letters with specific features. Using a large number of letters is too unintuitive and makes it difficult for readers to understand.\n\n8. \u201cWe follow 2G-GCN to extract feature of humans and objects from backbone network\u201d.\nYou used 2G-GCN to capture features, but the input {vt}t=1T seems to be a clip. Is the input of 2G-GCN a video? The output is the characteristics of people and objects in each frame of the video? Do ZH and ZO represent the characteristics of people and objects in each frame, or the characteristics of the entire video? I'm totally confused. \n\nExperiments:\n1. Although three datasets are compared, the algorithm is not fully verified. Why the VidHOI dataset is not used for comparison? This is a well-known video-based human-object interaction dataset.\n\n2. There are no comparisons for this hypersphere module in the ablation experiments. It's the key component that needs comparative validation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3816/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767537771,
            "cdate": 1698767537771,
            "tmdate": 1699636338985,
            "mdate": 1699636338985,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bA1Sz9qsPe",
                "forum": "ymR2bz0cEs",
                "replyto": "IxYqXb5b9l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3816/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3816/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the detailed comments. We answer all the questions as follows."
                    },
                    "comment": {
                        "value": "* ***Task definition.*** \n\n    **(1)** As we mentioned in Section 4.1 ''Problem Formulation'' part in the paper, multi-person video HOI recognition means there are multiple persons in the scene and we aim to predict the temporal segmentation of each peron's sub-activity in that scene, indicating the person's interaction with the object. \n\n    **(2)** Note that we do not aim to predict *multi-person interaction*. Rather, we predict ***human-object inteaction*** represented by human sub-activity classes. \n\n   **(3)** It is important to underscore that multi-person HOI interaction stands apart from group action recognition [1][2]. Unlike the latter, which yields a singular action class for all individuals in the scene, multi-person HOI interaction goes a step further by prognosticating each person's distinct sub-activity in the given scene. This added granularity introduces a heightened level of complexity, surpassing the challenges inherent in group action recognition.\n\n   *[1] Azar, Sina Mokhtarzadeh, et al. \"Convolutional relational machine for group activity recognition.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.*\n\n   *[2] Gavrilyuk, Kirill, et al. \"Actor-transformers for group activity recognition.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.*\n\n* ***Hypersphere.*** \n\n    **(1)** Hypersphere indicates a high-dimensional sphere, indicating the boundary of a high-dimensional ball. We call \"hypersphere\" in this work because the feature dimensions in Equation (6) are high-dimensional vectors (larger than 3). \n\n    **(2)** The interaction-centric hypersphere is crafted to introduce a **structured bias** of HOI into our model. Specifically, this bias asserts that **human-object pairs adhere to their respective interaction classes**. Consequently, the model is compelled to make predictions within the confines of this structured bias. It is essential to clarify that the hypersphere serves merely as a means of visualizing this bias. Alternately, various other techniques may be employed for visualizing the structure bias. Hence, our paper's fundamental proposition is to instill the desired structure bias, emphasizing that hypersphere is just one illustrative manifestation of this broader concept.\n\n* ***Differnece of hypersphere from traditional classifier.*** As we explained in the above section, hypersphere is a way to visualize the introduced structured bias underlying HOI. This structure bias forces the model to make predictions within the confines of this structured bias. Compared to tranditional classifiers which usually employ multi-layer perceptron (MLP) to generate prediction logits, hypersphere introduce a strong prior into model prediction, which is abscent in traditional classifiers. Moreover, hypersphere do not introduce any training parameters like traditional classifiers, featuring higher computational efficiency. \n\n* ***HOI manifold structure.*** In mathematics and machine learning, a \"manifold structure\" refers to a topological space that locally resembles Euclidean space. In other words, in the vicinity of any point on the manifold, the space behaves like a flat, n-dimensional Euclidean space. Manifolds can have various dimensions and shapes, and they are often used to represent complex data structures or patterns in high-dimensional spaces. In this work, *HOI manifold structure* specifically describes the topological relationship between **human-object paris** and **interaction class**, which is a structure bias that we introduce into our model. \n\n* ***Complex HOI.*** Complex HOI incorporates multi-person HOI, meaning there would be interdependencies between human and object in single-person cases, or interdependencies between human and human under multi-person scenarios. \n\n* ***Entities representations.*** Human and object are called \"entity\", so entity representations denote representation of human-object pair."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3816/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533520164,
                "cdate": 1700533520164,
                "tmdate": 1700615728743,
                "mdate": 1700615728743,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XPvLqq4GAk",
                "forum": "ymR2bz0cEs",
                "replyto": "IxYqXb5b9l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3816/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3816/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "* ***Figure 2 and 3.*** We have added annotations  for the letters in the figures.\n\n* ***Follow 2G-GCN to extract feature.*** {${v_t}$}$_{t=1}^T$ denote video frames, whose features are extracted from 2G-GCN model. We use CLIP model to extract text features, not video frame features. The input of 2G-GCN is the same with our model, which is a video. The output is the human sub-activity class at each time step. $z_H$ and $z_O$ are extracted features of human and object from 2G-GCN, respectively. \n\n* ***Experiment on VidHOI dataset.*** VidHOI is used for video HOI detection task, while our paper works on video HOI recognition, which is a different task. Consequently, the VidHOI dataset is not suitable for comparison in our study.\n\n* ***Ablating hypersphere module.*** In the revised paper, we conduct an ablative analysis on the hypersphere module, substituting it with a conventional classifier implemented through a Multilayer Perceptron (MLP). The outcomes, as presented in Table 1, 2, and 3 (highlighted in red in the revised paper), reveal a notable decline of over 7% in the $F_1$ score within the MPHOI dataset when employing the traditional classifier. Likewise, in the CAD-120 dataset, the traditional classifier results in a substantial decrease of more than 11% in the $F_1$ score. Additionally, within the Bimanual Actions dataset, the traditional classifier induces a decline exceeding 2% in the $F_1$ score. These findings unanimously underscore the efficacy of the structural bias introduced by the hypersphere module. We also show the results below for better understanding: \n\n  |***MPHOI*** | $F_1 @10$  |  $F_1 @25$  |  $F_1 @50$  |  \n  | ------------- | ------------- | --------------  | -------------- |\n  |**Ours**       | **91.6**$\\pm$**0.9** |  **84.5**$\\pm$**2.6** |  **67.7**$\\pm$**2.2** | \n  |Ours (Traditional Classifier)  |  84.6$\\pm$7.2  |   74.7$\\pm$10.5  |   54.6$\\pm$13.7  | \n\n  |***CAD-120*** | $F_1 @10$               |  $F_1 @25$  |  $F_1 @50$  |  \n  | -------------    | -------------              | --------------  | -------------- |\n  |**Ours**          | **90.7**$\\pm$**2.9** |  **88.1**$\\pm$**2.8** |  **77.6**$\\pm$**4.7** | \n  |Ours (Traditional Classifier)  |  79.5$\\pm$11.0  |   73.9$\\pm$11.4  |   56.6$\\pm$12.5  | \n\n  |***Bimanual Actions*** | $F_1 @10$  |  $F_1 @25$  |  $F_1 @50$  |  \n  | ------------- | ------------- | --------------  | -------------- |\n  |**Ours**       | **85.0**$\\pm$**2.5** |  **82.9**$\\pm$**2.9** |  **74.2**$\\pm$**4.3** | \n  |Ours (Traditional Classifier)  |  82.0$\\pm$3.6  |   79.8$\\pm$4.1  |   71.0$\\pm$5.6  |"
                    },
                    "title": {
                        "value": "(Continued)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3816/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533531458,
                "cdate": 1700533531458,
                "tmdate": 1700615838178,
                "mdate": 1700615838178,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jPMQmgCAPi",
                "forum": "ymR2bz0cEs",
                "replyto": "IxYqXb5b9l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3816/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3816/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your comments. I would be glad to answer any additional questions if you have."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3816/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705105451,
                "cdate": 1700705105451,
                "tmdate": 1700705105451,
                "mdate": 1700705105451,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CplKqN2hCd",
            "forum": "ymR2bz0cEs",
            "replyto": "ymR2bz0cEs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3816/Reviewer_dB2v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3816/Reviewer_dB2v"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an interaction-centric hypersphere reasoning model for multi-person video HOI recognition. To do this, a context fuser is designed to learn the interdependencies among humans, objects, and interactions; a state reasoner model on top of context fuser is used for temporal reasoning; an interaction-centric hypersphere is used to represent the manifold structure of HOIs. \n\nThe model is flexible for multi-person or single-person videos. Experiments show the method outperforms the previous method by 22% F1 score on multi-person dataset, MPHOI-72 and the method performs similarly with existing methods on single-person dataset, Bimanual Actions and CAD-120."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper proposes an interaction-centric hypersphere representation scheme for HOI recognition learning.\n- The method achieves SOTA performance with\na huge improvement of more than 22% F1 score over existing methods."
                },
                "weaknesses": {
                    "value": "- The main focus of the paper is HOI recognition for multi-person videos. In the experiment, there is only 1 multi-person dataset used for evaluation but 2 single-person datasets. Showing model performance on different multi-person datasets will help strength the claims in the paper.\n- After reading the paper, there is still a lack of proof or explanation about why an interaction-centric hypersphere will help in the task theoretically. The ablation study does not show an ablation study on it."
                },
                "questions": {
                    "value": "- In the method, the context fuser and interaction state reasoner extract CLIP features for the representation. Ablation on the features is necessary to test if the feature is important or over-complex where a simple binary feature is enough. For example, in the interaction state reasoner, the two possible states \u201ccontinue\u201d and \u201cstop\u201d can be represented by binary labels or simpler features of lower dimensions compared with CLIP.\n- In 4.3.1 Model inference, during model inference, the interaction probability is predicted on each frame. It is not clear who is in interaction if there are multiple people in the video. Interaction prediction for each person is more detailed and straightforward.\n- Based on the question above, ablation studies with methods of HOI detection on images are necessary. HOI detection can detect interaction for each person. If there are multiple people, the results for comparison from the HOI detection is whether there is any interaction from all people or not.\n- In the Conclusion Sec, it mentions that the method outperforms SOTA on the multiple-people dataset but is on par with the single-person dataset. Is it because in the single-person video, there is only one person so the interaction prediction is determined by that single person? But for multi-person videos, the model does not need to predict correctly for each person to get the correct answer for the image."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3816/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698987435810,
            "cdate": 1698987435810,
            "tmdate": 1699636338918,
            "mdate": 1699636338918,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "h9Hpj7Ezug",
                "forum": "ymR2bz0cEs",
                "replyto": "CplKqN2hCd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3816/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3816/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the constructive comments. The questions are addressed in the following sections."
                    },
                    "comment": {
                        "value": "* ***Experiment datasets.*** In our multi-person video HOI recognition experiment, we exclusively leverage the MPHOI dataset, as it stands as the sole dataset tailored for multi-person video HOI recognition at present. Aligning with the SOTA methodology, specifically 2G-GCN, which also centers on multi-person video HOI recognition, our experimental design mirrors theirs by utilizing the MPHOI dataset. Furthermore, 2G-GCN augments its investigation with two single-person datasets. Hence, our experimental settings maintain congruence with 2G-GCN to facilitate a meaningful comparative analysis. \n\n* ***Why interaction-centric hypersphere helps?*** The interaction-centric hypersphere is crafted to introduce a **structure bias** of HOI into our model. Specifically, this bias asserts that **human-object pairs adhere to their respective interaction classes**. Consequently, the model is compelled to make predictions within the confines of this structured bias. It is essential to clarify that the hypersphere serves merely as a means of visualizing this bias. Alternately, various other techniques may be employed for visualizing the structure bias. Hence, our paper's fundamental proposition is to instill the desired structure bias, emphasizing that hypersphere is just one illustrative manifestation of this broader concept.\n\n* ***Ablating CLIP features.*** We conduct ablation studies in the revised peper by utilizing random initialization for interaction features and context features, instead of using CLIP representations. Ablation study results in Table 1 (highlighted in red in the revised paper) show that random initialization results in around 10% in the $F_1$ scores in MPHOI dataset, while still higher than SOTA method 2G-GCN around 15%. Furthermore, Tables 2 and 3 showcase ablation study outcomes on the CAD-120 and Bimanual Actions datasets, indicating a drop of less than 3% in $F_1$ scores with random initialization. Notably, these scores remain competitive, if not superior, to the 2G-GCN model. Thus, our findings suggest that extracting CLIP features is not imperative for achieving strong performance. We also show the results below for better understanding:\n\n  |***MPHOI*** | $F_1 @10$  |  $F_1 @25$  |  $F_1 @50$  |  \n  | ------------- | ------------- | --------------  | -------------- |\n  | 2G-GCN     |  68.6$\\pm$10.4 |  60.8$\\pm$10.3 |  45.2$\\pm$6.5 | \n  |**Ours**       | **91.6**$\\pm$**0.9** |  **84.5**$\\pm$**2.6** |  **67.7**$\\pm$**2.2** | \n  |Ours ($\\Delta$ CLIP+BLIP)  |  80.0$\\pm$6.7  |   73.0$\\pm$9.9  |   55.6$\\pm$7.4  | \n\n  |***CAD-120*** | $F_1 @10$               |  $F_1 @25$  |  $F_1 @50$  |  \n  | -------------    | -------------              | --------------  | -------------- |\n  | 2G-GCN     |  89.5$\\pm$1.6 |  87.1$\\pm$1.8 |  76.2$\\pm$2.8 | \n  |**Ours**          | **90.7**$\\pm$**2.9** |  **88.1**$\\pm$**2.8** |  **77.6**$\\pm$**4.7** | \n  |Ours ($\\Delta$ CLIP+BLIP)  |  89.4$\\pm$2.3  |   85.5$\\pm$3.9  |   74.9$\\pm$5.7  | \n\n  |***Bimanual Actions*** | $F_1 @10$  |  $F_1 @25$  |  $F_1 @50$  |  \n  | ------------- | ------------- | --------------  | -------------- |\n  | 2G-GCN     |  **85.0**$\\pm$**2.2** |  82.0$\\pm$2.6 |  69.2$\\pm$3.1 | \n  |**Ours**       | 85.0$\\pm$2.5 |  **82.9**$\\pm$**2.9** |  **74.2**$\\pm$**4.3** | \n  |Ours ($\\Delta$ CLIP+BLIP)  |  84.3$\\pm$1.4  |   81.8$\\pm$1.8  |   73.2$\\pm$2.7  | \n\n\n* ***Interaction prediction for each person.***  In the case of multi-person video HOI recognition, our model is able to predict interaction class for each person, as qualitative examples shown in Figure 5 and Figure 7 in the paper. \n\n* ***Ablation studies with methods of HOI detecion.*** Our emphasis lies specifically on the **HOI recognition** task rather than HOI detection. These two tasks differs in both problem formulation and technical challenges. As articulated in Section 4.1 \"Problem Formulation,\" our focus is on predicting the HOI class present in a given video frame. It is essential to clarify that our paper does not address HOI detection, which involves generating bounding box predictions for each individual and object in the scene, a distinct task from the scope of our work.\n\n* ***Performacne on single-person videos.*** The relatively modest improvement of our model's performance on single-person videos compared to the SOTA method in contrast to its notable gains in multi-person videos can likely be attributed to the absence of a dedicated context fuser module for single-person scenarios, as illustrated in Figure 2. As evident from the outcomes presented in Table 1, the contextual fusion module plays a pivotal role in enhancing performance in multi-person settings. Consequently, we hypothesize that the less remarkable results observed in single-person instances are attributable to the lack of a context fuser module."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3816/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533253551,
                "cdate": 1700533253551,
                "tmdate": 1700614395504,
                "mdate": 1700614395504,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2AdiwQmnai",
                "forum": "ymR2bz0cEs",
                "replyto": "CplKqN2hCd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3816/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3816/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your comments. I would be glad to answer any additional questions if you have."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3816/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705094478,
                "cdate": 1700705094478,
                "tmdate": 1700705094478,
                "mdate": 1700705094478,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "w1x5bQF7HH",
            "forum": "ymR2bz0cEs",
            "replyto": "ymR2bz0cEs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3816/Reviewer_3Vxe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3816/Reviewer_3Vxe"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel *Context Fuser* module leveraging the strengths of the CLIP and BLIP models, incorporates an *Interaction State Reasoner* module, and introduces *Interaction Feature Loss* to address the video Human-Object Interaction (HOI) problem."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The experimental results demonstrate superior performance over other methods on MPHOI-72 and CAD-120 benchmarks."
                },
                "weaknesses": {
                    "value": "1. **Unfair Comparisons and Insufficient Ablation Studies:**\nThe primary weakness of the proposed method is the unfair comparisons with other video HOI methods and the lack of thorough ablation studies. The Context Fuser employs large Vision-Language Models (VLMs), CLIP and BLIP, pre-trained on big data. While ASSIGAN and 2G-GCN do not. In Table 1, removing Context Fuser results in a notable decline in $F_1@10$. This raises suspicions that the significant performance enhancement could be largely attributed to the text-image alignment capabilities inherent in large VLMs rather than the proposed Context Fuser. The performance is below the benchmark set by Qiao et al., 2020, in the absence of the Context Fuser. The paper lacks critical ablation studies to disentangle the contributions of the VLMs and the proposed method.\n2. **Missing References:**\nThere is a relevant ICLR\u201923 paper you should refer to, Gao et al., ICLR\u201923. Gao et al, which also uses large VLMs for the HOI problem. Unlike fixed prompt used in Context Fuser, while Gao et al. delve into learnable prompts.\n>Gao, K., Chen, L., Zhang, H., Xiao, J. & Sun, Q. Compositional Prompt Tuning with Motion Cues for Open-vocabulary Video Relation Detection. _ICLR_ (2023)."
                },
                "questions": {
                    "value": "A deeper ablation study focusing on the efficacy of the CLIP and BLIP parts within the Context Fuser is advisable. This would ascertain whether the observed improvements stem from the newly proposed module or merely from the integration of CLIP and BLIP."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3816/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699179287396,
            "cdate": 1699179287396,
            "tmdate": 1699636338826,
            "mdate": 1699636338826,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V7pm7ukZAK",
                "forum": "ymR2bz0cEs",
                "replyto": "w1x5bQF7HH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3816/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3816/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the constructive comments. We address the questions as follows."
                    },
                    "comment": {
                        "value": "* ***Comparison with other methods and ablation studies.***: In our revised paper, we employ ablation studies to elucidate the impact of excluding prominent Vision-Language Models (VLMs) CLIP and BLIP (highlighted in red in the revised paper). In the ablation studies, we adopt random initialization for the interaction feature $z_I$ and context feature $z_C$. The results, detailed in Table 1, indicate approximately 10% in $F_1$ scores within the MPHOI dataset upon removing CLIP and BLIP. Despite this reduction, our scores remain superior to the SOTA method 2G-GCN by approximately 15% $F_1$ score. Moreover, Tables 2 and 3 reveal that in the CAD-120 and Bimanual Actions datasets, the omission of CLIP and BLIP yields a drop of less than 3% in the $F_1$ scores, still comparable to or surpassing the performance of the 2G-GCN model. These findings underscore that the comparison between our model and SOTA methods is equitable, as the reliance on large VLMs proves non-essential for achieving commendable performance. Rather, it is the intricately designed structure of our model that substantiates the substantial enhancement in performance. We also show the results below for better understanding:\n\n  |***MPHOI*** | $F_1 @10$  |  $F_1 @25$  |  $F_1 @50$  |  \n  | ------------- | ------------- | --------------  | -------------- |\n  | 2G-GCN     |  68.6$\\pm$10.4 |  60.8$\\pm$10.3 |  45.2$\\pm$6.5 | \n  |**Ours**       | **91.6**$\\pm$**0.9** |  **84.5**$\\pm$**2.6** |  **67.7**$\\pm$**2.2** | \n  |Ours ($\\Delta$ CLIP+BLIP)  |  80.0$\\pm$6.7  |   73.0$\\pm$9.9  |   55.6$\\pm$7.4  | \n\n  |***CAD-120*** | $F_1 @10$               |  $F_1 @25$  |  $F_1 @50$  |  \n  | -------------    | -------------              | --------------  | -------------- |\n  | 2G-GCN     |  89.5$\\pm$1.6 |  87.1$\\pm$1.8 |  76.2$\\pm$2.8 | \n  |**Ours**          | **90.7**$\\pm$**2.9** |  **88.1**$\\pm$**2.8** |  **77.6**$\\pm$**4.7** | \n  |Ours ($\\Delta$ CLIP+BLIP)  |  89.4$\\pm$2.3  |   85.5$\\pm$3.9  |   74.9$\\pm$5.7  | \n\n  |***Bimanual Actions*** | $F_1 @10$  |  $F_1 @25$  |  $F_1 @50$  |  \n  | ------------- | ------------- | --------------  | -------------- |\n  | 2G-GCN     |  **85.0**$\\pm$**2.2** |  82.0$\\pm$2.6 |  69.2$\\pm$3.1 | \n  |**Ours**       | 85.0$\\pm$2.5 |  **82.9**$\\pm$**2.9** |  **74.2**$\\pm$**4.3** | \n  |Ours ($\\Delta$ CLIP+BLIP)  |  84.3$\\pm$1.4  |   81.8$\\pm$1.8  |   73.2$\\pm$2.7  | \n\n* ***Reference.*** Thanks for the suggestion, we have added the reference in the revised version."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3816/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533152526,
                "cdate": 1700533152526,
                "tmdate": 1700613853699,
                "mdate": 1700613853699,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4MUHv9lhZa",
                "forum": "ymR2bz0cEs",
                "replyto": "w1x5bQF7HH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3816/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3816/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your comments. I would be glad to answer any additional questions if you have."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3816/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705079816,
                "cdate": 1700705079816,
                "tmdate": 1700705079816,
                "mdate": 1700705079816,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]