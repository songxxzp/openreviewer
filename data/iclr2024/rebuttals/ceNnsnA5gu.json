[
    {
        "title": "WL-Tree: a New Tool for Analyzing Graph Neural Networks"
    },
    {
        "review": {
            "id": "QU7uHKhoTz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2059/Reviewer_py5L"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2059/Reviewer_py5L"
            ],
            "forum": "ceNnsnA5gu",
            "replyto": "ceNnsnA5gu",
            "content": {
                "summary": {
                    "value": "The paper presents WL-tree, a tool for analyzing graphs based on the multiset of walks of a given length that leave a node. An algorithm that identifies whether a certain node satisfies a given WL tree is also presented. Finally, a working implementation of two graph neural networks (GNNs) that enhance the expressiveness of message-passing GNNs with node id representations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The presentation is good."
                },
                "weaknesses": {
                    "value": "The tool presented in the paper, WL tree, essentially corresponds to the well-notion of tree unravelling from a node in a graph. That this notion is equivalent with WL coloring is absolutely folklore and has been used in many papers for decades. As such, the paper does not bring any new conceptual contribution into the picture. Theoretically speaking, all results in the paper are simple exercises. \n\nThe authors also show a poor understanding of the related literature. A concrete example is when they mention that WL has the same expressive power than *guarded* FO_2^\\cnt. This is simply not true, and it is not what Cai et al have proved. They have shown that the distinguishing expressive power of WL is exactly the same as FO_2^\\cnt (the guarded version is, in fact, weaker). The results by Barcel\u00f3 et al do not concern this notion of expressive power, but a different one. They show that each guarded FO_2^\\cnt unary formula can be turned into an equivalent GNN over the set of all graphs. That is, the result by Barcel\u00f3 et al is *uniform*, while the one by Cai et al. is not (and neither is the result of Morris et al.)"
                },
                "questions": {
                    "value": "I have no concrete questions. The paper is below the bar in my view."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2059/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697215385411,
            "cdate": 1697215385411,
            "tmdate": 1699636137822,
            "mdate": 1699636137822,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fhGlX4IW1Y",
                "forum": "ceNnsnA5gu",
                "replyto": "QU7uHKhoTz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2059/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2059/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A quick reponse"
                    },
                    "comment": {
                        "value": "If you state \"That this notion is equivalent with WL coloring is absolutely folklore and has been used in many papers for decades.\" Could you just point out one paper to support your statement? Thank you!\n\nWe will post a longer response later."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2059/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699707867884,
                "cdate": 1699707867884,
                "tmdate": 1699707867884,
                "mdate": 1699707867884,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SQuHcR0Zls",
                "forum": "ceNnsnA5gu",
                "replyto": "fhGlX4IW1Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2059/Reviewer_py5L"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2059/Reviewer_py5L"
                ],
                "content": {
                    "comment": {
                        "value": "Sure, the notion of unravelling in relationship with WL has been used, for instance, in the following two papers: \n\n- https://arxiv.org/pdf/2204.04661.pdf (See Theorem C.2) \n- https://openreview.net/pdf?id=r1lZ7AEKvB (see Definition C.2)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2059/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699896129544,
                "cdate": 1699896129544,
                "tmdate": 1699896129544,
                "mdate": 1699896129544,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "shiPOOhGY0",
            "forum": "ceNnsnA5gu",
            "replyto": "ceNnsnA5gu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2059/Reviewer_UyCd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2059/Reviewer_UyCd"
            ],
            "content": {
                "summary": {
                    "value": "In the paper, the authors propose a new tree model for GNNs, which they call WL-trees. The key idea of constructing WL-trees is based on a variant of breadth first search that allows to revisit non-parent nodes. Then the authors show that WL trees are equivalent to the 1-WL algorithm in terms of node coloring. They also propose an algorithm to identify subgraphs anchored at nodes which have the same node representations corresponding to a given WL tree. The contributions claimed by the authors are that the proposed WL trees can provide a more intuitive understanding of graph structures learned by message-passing GNNs."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "[1] The paper proposes a different perspective to analyse graph structures underlying message-passing graph neural networks. \n\n[2] The connections between their proposed WL trees and anchored graphs are discussed, along with an algorithm that can identify anchored subgraphs corresponding to a given WL tree.\n\n[3] Two existing GNN models are considered and analyzed in the experiments."
                },
                "weaknesses": {
                    "value": "[W1] The proposed method is not well defined. Below are some specific comments:\n\n - Page 3: The formulations in Equations 3-5 are not consistent. In Equation 3, an anchored subgraph is defined in terms of a set of walks but Equation (5) defines an anchored subgraph as a set of pairs of nodes and walks. Further, the definition of $\\dot{\\cup}$ is not clearly presented. Also, why $dist(i,j)$ is only less than $\\ell$ but $dist(i,k)$ is less than or equal to $\\ell$?\n\n- Page 4: For the function id(\u00b7) that maps a tree node to a node in an anchored graph, since a node in an anchored graph may appear multiple times in the tree, is it still a function?\n\n[W2] The proposed WL-trees differ from the computational tree structures of message-passing GNNs mainly in disallowing the revisit of the parent nodes. The authors claim that the proposed WL-trees are equivalent to the 1-WL algorithm in terms of node coloring. This does not seem correct. Consider a counter-example, where G is a graph consisting of two triangles and H is a cycle of length 6. These two graphs cannot be distinguished by 1-WL, but would have different WL-trees proposed in the paper.\n\n[W3] The tree structures underlying message-passing GNNs and their connection to 1-WL have been well studied in the literature. It is unclear why the proposed WL-trees can provide a more fine-level analysis of the expressiveness of node representations learned by message-passing GNNs. In particular, the proposed WL-trees are not equivalent to 1-WL (see the above point [2]).\n\n[W4] In what kinds of scenarios will the proposed algorithm 1 be useful?\n\n[W5] For the section 6, what are the justifications for selecting CLIP and Nested GNN? There are a large number of GNN models developed in the literature. I don't see why these two particular GNN models are selected for analysis. \n\n[W6] Theorem 12 and Theorem 13 look confusing. Why is max used in Equation 8? Is the notation $G(j,h)$ defined? Also, the expressive power of Nested GNN goes beyond 1-WL, but the WL-trees proposed in the paper are claimed to be equivalent to 1-WL. So why does Theorem 13 state that there is a bijective mapping between WL-trees and their node embeddings calculated by Equation 10?\n\n[W7] For the statement \"A smaller count or conditional entropy means that the WL-tree can better identify a node\u2019s surround structure\", is any theoretical justification? Analysing GNN models using average counts of anchored subgraphs and the conditional entropy of anchored graphs look ad hoc."
                },
                "questions": {
                    "value": "W1 - W7"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2059/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2059/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2059/Reviewer_UyCd"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2059/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698218200470,
            "cdate": 1698218200470,
            "tmdate": 1699636137749,
            "mdate": 1699636137749,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xASsEDs2lr",
                "forum": "ceNnsnA5gu",
                "replyto": "shiPOOhGY0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2059/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2059/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments"
                    },
                    "comment": {
                        "value": "Thank you for your comments. We appreciate that you have seen our effort in getting a new perspective of graph structures behind GNNs.\n\n--- [W1] Equations 3 and 5. \n\nEquation 3 is the definition, and equation 5 is derived from equation 3. The idea of equation 5 is as follows: if i is the anchor of an anchored subgraph, then the anchored subgraph can be viewed as the union of 1) i and i's neighbors and 2) anchored subgraphs surrounding i's neighbors. \n\nIn the edge set of equation (4), we have $\\\\{(j, k) \\in E: \\mathrm{dist}(i, j) < \\ell, \\mathrm{dist}(i, k) \\le \\ell \\\\}$. A better writing should be $\\\\{(j, k) \\in E: \\mathrm{dist}(i, j) \\le \\ell, \\mathrm{dist}(i, k) < \\ell \\\\}$, which is consistent with our usage of $j$ in the node set. It means that: if $(j, k)$ is included in the edge set of $S_i^\\ell$, then it must in a length-$\\ell$ walk starting from $i$, then the distance between $i$ and one end needs to be less than $\\ell$. \n\n--- Page 4: For the function id(\u00b7) that maps a tree node to a node in an anchored graph, since a node in an anchored graph may appear multiple times in the tree, is it still a function?\n\nIt is a function since it may map multiple tree nodes (multiple appearances of a graph node) to a graph node. \n\n--- [W2] Consider a counter-example, where G is a graph consisting of two triangles and H is a cycle of length 6. These two graphs cannot be distinguished by 1-WL, but would have different WL-trees proposed in the paper.\n\nWe will have the same WL tree, which has the following pattern: each node has two children. \n\n--- [W3] It is unclear why the proposed WL-trees can provide a more fine-level analysis of the expressiveness of node representations learned by message-passing GNNs. In particular, the proposed WL-trees are not equivalent to 1-WL (see the above point [2]).\n\nAs we have shown in our work, a WL-tree is equivalent to a WL-color. We have clarified W2 as well. In this regard, our WL-trees provide a new way of analyzing graph structures behind node representations. It is easier to use because 1) it connects well with the original graph structure, and 2) it is easy to visualize. \n\n--- [W4] In what kinds of scenarios will the proposed algorithm 1 be useful?\n\nThe algorithm is able to enumerate graph structures that give the same WL tree/color. When we want to analyze these structures, e.g. checking which structures a GNN cannot distinguish, then we can use the algorithm to do so. In fact, we use the algorithm to check how many graph structures can possibly be confused by GNNs in section 6. \n\n--- [W5] For the section 6, what are the justifications for selecting CLIP and Nested GNN? There are a large number of GNN models developed in the literature. I don't see why these two particular GNN models are selected for analysis.\n\nBecause these two GNNs can be best explained by our WL-trees. We plan to examine the expressiveness of more GNNs in our future work. \n\n--- [W6] Theorem 12 and Theorem 13 look confusing. Why is max used in Equation 8? Is the notation \n defined? \n\nThe max operation is taken over all possible colorings $\\bar{c}$. It is from the CLIP-2 method -- we just formalize the method. \n\n--- Also, the expressive power of Nested GNN goes beyond 1-WL, but the WL-trees proposed in the paper are claimed to be equivalent to 1-WL. So why does Theorem 13 state that there is a bijective mapping between WL-trees and their node embeddings calculated by Equation 10?\n\nBoth the two GNNs first color graph nodes with extra information (random colors or local message-passing) and then run a vanilla GNN to get node representations. So we can use our WL trees to quantify the abilities of their vanilla GNNs using extra node colors. \n\n--- [W7] For the statement \"A smaller count or conditional entropy means that the WL-tree can better identify a node\u2019s surround structure\", is any theoretical justification? Analysing GNN models using average counts of anchored subgraphs and the conditional entropy of anchored graphs look ad hoc.\n\nThe conditional entropy is computed from possible graph structures in the dataset that give the same WL-tree/color. If the conditional entropy is 0, it means that there is only one graph structure giving the WL tree/color. In our view, the analysis with an information quantity is reasonably because it tights to a GNN's ability in terms of distinguishing graph structures."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2059/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709711313,
                "cdate": 1700709711313,
                "tmdate": 1700709749548,
                "mdate": 1700709749548,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u0knGTn77f",
            "forum": "ceNnsnA5gu",
            "replyto": "ceNnsnA5gu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2059/Reviewer_wEKA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2059/Reviewer_wEKA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new concept, WL-tree, as a new perspective for analyzing GNNs. It theoretically proves that WL-trees are bijective mapping with the colors given by the 1-WL algorithm. It claims that such a new perspective could bring new understandings of the encoded structural information in GNN node representations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation of analyzing what structural informative is encoded in node representations is important.\n\n2. The formulations of the theorems in the paper are formal, which could be potentially useful for the community."
                },
                "weaknesses": {
                    "value": "1. Although WL-tree seems to be a new concept, I did not quite get what perspective from which it is important compared to existing 1-WL algorithm results. It is known that message passing GNN, such as GIN, are capturing the rooted subtree around each node, which is exactly the structure captured by 1-WL, as shown in Figure 1 of [1]. As defined in Section 4, the WL-tree proposed in this paper is also such a rooted subtree. The only difference is that the rooted tree in this paper does not include the parent of a node as its child. It is not clear to me why this difference is important and how it brings significant differences compared to existing understanding. (Please correct me if I am understanding wrongly or incompletely.)\n\n2. Also, it is unclear what advantages or new understandings can be inspired by this new concept. Could you summarize what findings we can get from this new analysis tool?\n\n3. The experiments only show a simple analysis of two existing GNN models. What new model designs this new concept can lead to? This is not obvious from the reading. I think the experimental section can include deeper analyses or include a model inspired by the introduced WL-tree tool.\n\n\n\n\n[1] Xu, Keyulu, et al. \"How Powerful are Graph Neural Networks?.\" International Conference on Learning Representations. 2018."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2059/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698444464030,
            "cdate": 1698444464030,
            "tmdate": 1699636137682,
            "mdate": 1699636137682,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MaOWJDgYgA",
                "forum": "ceNnsnA5gu",
                "replyto": "u0knGTn77f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2059/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2059/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments"
                    },
                    "comment": {
                        "value": "Thank you for seeing our effort in a formal analysis of GNN's expressive power. Below are answers to your questions. \n\n1. \"It is not clear to me why this difference is important and how it brings significant differences compared to existing understanding.\" \n\nThere are at least two benefits if we do not include the parent in the child. First, it is easier to discuss a node's WL color in a tree -- our formulation directly uses the original tree structure without adding new nodes. Second, it is easier to enumerate graph structures behind a WL color. With our formulation, we only need to merge some nodes to get a graph structure. If a node's parent is also included in the child list, then the situation is much more complex. \n\n2. \" Could you summarize what findings we can get from this new analysis tool?\"\n\nFirst, with the new tool, it is easier to discuss which graph nodes have the same color/node representations. For example, showing that a GNN with a fixed depth cannot distinguish a graph node from a particular tree node. Second, it is easier to explain how previous GNNs enhance node representations. Random node colors give a particular tree pattern (some nodes much take the same color), so that the graph structure can be separated from others that have the same WL color. \n\nAs we mentioned in our submission, this new tool is fundamentally no different from WL colors but it makes some analysis clearer, particularly when we can visualize examples with WL trees. As a comparison, it is more abstract to discuss WL colors and logic to reach the same conclusion. \n\n    \n3. Findings from our experiments\n\nIn our experiment, we use our new method to enumerate graph structures corresponding to a WL tree/color. We show that advanced GNN methods are able to eliminate a large number of possible graph structures by including extra node colors. But they do not do much better in terms of distinguishing graph structures **in the dataset**: a WL tree/color may correspond to many graph structures, but if only one of these structures is in the dataset, then it still can be distinguished by a vanilla GNN. Our experiment explains why we don't see a significant increase in performance when we use stronger GNNs on some datasets."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2059/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707087253,
                "cdate": 1700707087253,
                "tmdate": 1700707087253,
                "mdate": 1700707087253,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]