[
    {
        "title": "UGSL: A Unified Framework for Benchmarking Graph Structure Learning"
    },
    {
        "review": {
            "id": "JyS87O7oHF",
            "forum": "DLfdJEuXkR",
            "replyto": "DLfdJEuXkR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6088/Reviewer_RW4a"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6088/Reviewer_RW4a"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a framework to unify and experimentally investigate existing graph structure learning methods. The unified framework consists of EdgeScorer, Sparsifier, Processor, Encoder and Output components. With experimental comparisons, it provides some interesting insights without serious proof."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "It is interesting to provide a review and comparison to graph structure learning."
                },
                "weaknesses": {
                    "value": "Although this may be the first work to present an experimental comparison of graph structure learning, my main concern is the contribution to the community of graphs. This paper seems like a review of this field. However, I don't think it is novel since most of the insights are without serious proof. Some conclusions may be correct with the limited methods."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6088/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6088/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6088/Reviewer_RW4a"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6088/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698487857929,
            "cdate": 1698487857929,
            "tmdate": 1699636656492,
            "mdate": 1699636656492,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "ZsbT8jNbFP",
            "forum": "DLfdJEuXkR",
            "replyto": "DLfdJEuXkR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6088/Reviewer_WpgZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6088/Reviewer_WpgZ"
            ],
            "content": {
                "summary": {
                    "value": "To tackle the inconsistent setups of various methods in the GSL domain, this paper presents a unified framework for evaluating GSL methods. This framework encompasses over 10 existing methods and four thousand diverse architectures. By conducting experiments on six datasets, valuable insights are provided regarding the efficacy of these components."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* A unified GSL framework is devised, which splits the GSL framework into four components, allowing for seamless substitution of different components.\n* The author conducted over 30,000 comparisons on each dataset and offered numerous valuable insights."
                },
                "weaknesses": {
                    "value": "* The scope of the work is limited. The authors only utilize node features on all datasets, without using their original graph structures. This may limit the scope of the benchmark, especially when some GSL methods are specifically designed for refining graphs[2, 3, 5]. Moreover, only node classification is included. Other tasks such as graph-level tasks should also be taken into consideration. It would also be better to compare the time complexity and memory consumption of different components.\n* I'm concerned about whether UGSL framework can cover all the methods in Table 15. It seems that certain key designs of typical methods, such as the Bilevel Programming of LDS[1] and the Iteration Mechanism of IDGL[2], may not be adequately represented in UGSL. Additionally, common postprocessing techniques in GSL work, such as residual connection[2, 3, 4], are missing in the paper.\n* Regarding the experiments,  variance is not given in many results, which is necessary to draw statistically significant conclusions regarding the relative merits of different components. In Table 7, 8, 10, it would be better to use the average ranking rather than average accuracy due to the potential large numerical variations of accuracy across different datasets. Lastly, while this may be beyond the scope, it would be interesting to look into the preferences of different dataset characteristics for specific components.\n\n[1] Franceschi, Luca, et al. \"Learning discrete structures for graph neural networks.\" *International conference on machine learning*. ICML, 2019.\n\n[2] Chen, Yu, Lingfei Wu, and Mohammed Zaki. \"Iterative deep graph learning for graph neural networks: Better and robust node embeddings.\" NeurIPS, 2020\n\n[3] Yu, Donghan, et al. \"Graph-revised convolutional network.\" ECML PKDD, 2020\n\n[4] Zhao, Jianan, et al. \"Heterogeneous graph structure learning for graph neural networks.\" AAAI, 2021.\n\n[5] Liu, Yixin, et al. \"Towards unsupervised deep graph structure learning.\" WWW, 2022."
                },
                "questions": {
                    "value": "This paper seems easy to reproduce, though the data splits are not given. Besides, the results supporting Insight 3,4,5 are missing. Please make sure that all insights are supported by corresponding tables or figures. The experimental settings, results and insights should be more detailed and organized."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6088/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698744144189,
            "cdate": 1698744144189,
            "tmdate": 1699636656384,
            "mdate": 1699636656384,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "6nCkYcdyzD",
            "forum": "DLfdJEuXkR",
            "replyto": "DLfdJEuXkR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6088/Reviewer_k3xx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6088/Reviewer_k3xx"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors focus on the growing area of graph neural networks (GNNs) and their applications. While traditional GNN approaches assume a predefined graph structure, recent methods have expanded GNN applicability by demonstrating their effectiveness even in the absence of an explicitly provided graph structure. Instead, these methods learn both the GNN parameters and the graph structure simultaneously. The challenge arises from the diverse experimentation setups used in previous studies, making it hard to compare their effectiveness directly. To address this issue, the authors introduce a benchmarking strategy called Unified Graph Structure Learning (UGSL). UGSL reformulates existing models into a unified framework, enabling the implementation of a wide range of methods. Through extensive analyses, the authors evaluate the effectiveness of different components within the framework. Their results offer a comprehensive understanding of various methods in this domain, elucidating their respective strengths and weaknesses. This research provides valuable insights into the complex landscape of graph structure learning and offers a standardized approach for comparing different techniques in the field of graph neural networks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper systematically studies the problem of graph structure learning and compares different types of methods on different benchmark graphs.\n\n- Several insights have been provided based on the benchmarking studies which are potentially useful when selecting graph structure learning models\n."
                },
                "weaknesses": {
                    "value": "- These insights provided in this paper rely on limited graph benchmark datasets and these datasets have limitations in 1) they are in relatively small sizes, e.g., 10k nodes. 2) Datasets with graphs reflect more homophily but heterophily. However, in practice, many graphs show the pattern of a mixture of both homophily and heterophily. 3) The number of datasets is relatively small, e.g., there are only 3 datasets w/ and w/o graphs respectively. 4) All the tasks are node classification (although this has been discussed in future directions). It is necessary to consider different tasks in order to make more convincing conclusions.\n\n- In addition to graph datasets, the tested base models are limited to GCN and GIN. More types of GNNs should be compared in order to make more convincing conclusions and/or more general insights.\n\n- To show the effectiveness of structure learning, more insights from connecting the learned structures and downstream tasks should be discussed."
                },
                "questions": {
                    "value": "To make more convincing conclusions and/or more general insights, the benchmarking study should be more extensive including more types of GNNs, e.g., base models, and more graph data (with different sizes, characteristics, and tasks). Therefore, my questions include:\n\n- Will the insights/conclusions be consistent with different GNNs and different datasets?\n- Will the insights/conclusions be consistent with different downstream tasks?\n\nBesides, it will be interesting to have a more detailed discussion on the relationships between the learned graph structures and downstream tasks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6088/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752759281,
            "cdate": 1698752759281,
            "tmdate": 1699636656280,
            "mdate": 1699636656280,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "ap9WxVy7vQ",
            "forum": "DLfdJEuXkR",
            "replyto": "DLfdJEuXkR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6088/Reviewer_B447"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6088/Reviewer_B447"
            ],
            "content": {
                "summary": {
                    "value": "This paper talks about a new way to test graph structure learning (GSL) methods. The authors look at different parts and designs in this method, see how they work, and tell us what's good and bad about current GSL methods. They also talk about UGSL layers and show how other models can fit into their new method. The paper has many tests on different data and looks at more than 4,000 designs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors compare many structures, and their new method is clear.\n2. Some ideas in the paper are helpful and can make people think more about this topic."
                },
                "weaknesses": {
                    "value": "1. Lack of the baselines and large graph datasets: though the author did great jobs in searching on various architectures, the work itself is not enough for supporting a comprehensive paper. The used graph is relatively small.\n2. Lack of the novelty: divide the training process of GSL is trivial, and and some tests are hard to understand because they are too similar."
                },
                "questions": {
                    "value": "1. In Insight 16, you said \"GIN is better than GCN for self-loop information.\" Where did you get this idea? Please show where you found this.  \n2. The paper is hard to read, especially the tests. Maybe you can use pictures or charts to show the differences in each test section.  \n3. Can you add more basic examples and use bigger graphs in your tables?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6088/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752852214,
            "cdate": 1698752852214,
            "tmdate": 1699636656179,
            "mdate": 1699636656179,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]