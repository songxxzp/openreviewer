[
    {
        "title": "Risk-Sensitive Variational Model-Based Policy Optimization"
    },
    {
        "review": {
            "id": "D7VxefXKBP",
            "forum": "4FUa5dxiiA",
            "replyto": "4FUa5dxiiA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6236/Reviewer_dkQy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6236/Reviewer_dkQy"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the issue of risk-seeking policies in model-based RL-as-inference which arises due to learning optimistic dynamics.\nThe paper highlights how optimistic dynamics lead to risk-seeking policies in Figure 1.\nThe authors then introduce a risk parameter ($\\beta$) to the RL-as-inference objective that enables (nonlinear) interpolation\nbetween optimistic dynamics (for low $\\beta$) and the true dynamics (for large $\\beta$).\nThey then highlight connections between their ELBO (objective) and the risk-sensitive exponentiated utility for SafeRL.\nHence the paper's name, I guess.\nDrawing on connections between this objective and constrained optimization with Lagrange multipliers,\nthey propose a method to automatically adapt $\\beta$.\nIntuitively, their method restricts learning such that the KL divergence between the variational and prior dynamics remains\nbelow a threshold $\\epsilon$.\nThat is, the variational dynamics can be optimistic as long as they're close to the real dynamics.\nThey provide a theoretical analysis of their risk-sensitive method and\ndemonstrate that it overcomes issues of risk-seeking behaviour in a stochastic tabular environment.\nThey also show that it fixes issues with VMBPO in deterministic continuous environments (Mountain Car), which I found to be a surprising (and interesting) result.\nFinally, they show that it scales to high(ish) dimensional benchmarks such as Hopper/Walker2D/HalfCheetah."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, I think this is a good paper.\nIt highlights a known (yet understudied) problem in the RL-as-inference framework (that of optimistic dynamics)\nand presents a novel solution to combat it.\nI found the paper well-written, I particularly liked Sec. 2 and Figure 1 as it was helpful for providing intuition for\nthe issue that the paper is trying to solve.\nThe theoretical analysis seems correct, although I have not checked it in detail.\nThe experiments are also well-structured and easy to follow."
                },
                "weaknesses": {
                    "value": "I do not have any major weaknesses with this paper.\nNevertheless, I will provide some comments with the aim of helping the authors further improve their manuscript.\n\nThe dynamics models are learned using an ensemble of MLPs which parameterize Gaussian densities.\nThat is, each ensemble member learns a heteroscedastic noise model which captures the MDP's transition noise.\nAlthough not detailed in the paper, I assume the ensemble is a uniformly weighted mixture\n$q_{\\phi}(s_{t+1}\\mid s_t,a_t) = \\frac{1}{B} \\sum_{b} q_{\\phi_{b}}(s_{t+1}\\mid s_t,a_t)$\nwhere each member is given by,\n$q_{\\phi_{b}}(s_{t+1}\\mid s_t,a_t) = \\mathcal{N}(\\mu_{\\phi_{b}}(s_{t},a_{t}), \\Sigma^{2}(s_{t}, a_{t}))$.\nThe ensemble then captures the epistemic uncertainty arising from limited data.\nHow do you calculate the KL divergence between the variational and prior dynamics given that they are Gaussian mixtures?\nDo you assume that the output is unimodal and fit a single Gaussian to the mixture?\nSubtleties like this could impact the performance of VMBPO and $\\beta$-VMBPO so they should be detailed in the paper.\n\nAlso, how do you train the ensemble?\nIs each member's parameters initialised differently? Or is each member trained on different data?\n\nIn my mind, the notion of risk applies in uncertain environments.\nIn Fig. 4b, how is this dynamics model risky?\nIsn't it just optimistic and stuck in a local optimum induced by the optimism?\nI'm happy to be corrected here.\n\nI'm also wondering if Fig. 4 used an ensemble of MLPs without probabilistic heads?\nThe paper states that all experiments use probabilistic heads.\nBut this environment is deterministic so this feels like an odd thing to do.\n\nHow do you populate $\\mathcal{D}\\_{\\text{model}}$? I mainly wanted to know how the initial state is sampled.\nDo you sample randomly from the state space or do you sample a state from the replay buffer and then perform a rollout?\nI later found this in Alg. 1 in the appendix. Perhaps reference this algorithm from paragraph 2 of Sec. 3.3 so that it's clear at this point how $\\mathcal{D}_{\\text{model}}$ is populated.\n\nMinor things:\n- There is an abuse of notation which should be made clear. That is, $Q(s_t,a_t)$ in Eq. 2/3 is different to Eq. 5. It could be made $Q(s_t, a_t;\\beta)$ in Eq. 5 or there could be a sentence to explain that the notation is being abused.\n- The first paragraph of Sec. 3 should reference Appendix D and not just the appendix.\n- How is $V_{\\psi}'(s_t)$ different to $V_{\\psi}(s_t)$? I couldn't find this anywhere in the text.\n- Third paragraph of Sec. 3.3 should have $\\pi_{\\kappa}(a_t\\mid s_t)$ instead of $\\pi(a_t\\mid s_t)$?\n- Theorem 4.3 uses \"Thm 4.1\" and \"Thm 4.2\" but the rest of the paper uses \"Theorem 4.1\". Be consistent.\n- This sentence in the related work feels out of place. \"Variational approximate RL-as-inference by maximizing a lower bound on the marginal likelihood.\". Is part of the sentence missing?\n- Fig. 2 caption. \"Learn\" should be \"Learned\" or \"Learnt\".\n- In Fig. 2 it's not clear the grey area is a cliff. I'd suggest overlaying text saying \"CLIFF\" on the grey area.\n- Fig. 3 specifies $\\beta$ as a parameter. It might be clearer to use $\\beta^0$ to indicate that this is the initial value of $\\beta$. At the moment it gives the impression that $\\beta$ is fixed.\n- Fig. 3 uses \"Beta-VMBPO\" and Fig. 5 uses \"Beta_VMBPO\". Be consistent.\n- Appendix C.1 \"Pytorch\" should be \"PyTorch\".\n- Fig 2/3/4/5 axis numbers and labels are too small.\n- Fig 3/5 legend is too small.\n- In Fig. 5 the left/middle plots don't show the curves converging. Do they converge to the same value as SAC? It feels suspect not to show $\\beta$-VMBPO not converging to the same value as SAC."
                },
                "questions": {
                    "value": "- Have you added more details on the ensemble of probabilistic MLPS dynamics models?\n- Have you addressed my issue with Fig. 5? That you don't show $\\beta$-VMBPO converging to the same value as SAC."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6236/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6236/Reviewer_dkQy",
                        "ICLR.cc/2024/Conference/Submission6236/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6236/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685771136,
            "cdate": 1698685771136,
            "tmdate": 1700646117530,
            "mdate": 1700646117530,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rszal1SR0P",
                "forum": "4FUa5dxiiA",
                "replyto": "D7VxefXKBP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6236/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6236/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**KL divergence calculation**\n\nOur approach computes the KL divergence between two probabilistic networks --- the network with best validation loss for each ensemble model. We then estimate KL divergence using Monte Carlo integration where the samples are generated using the variational probabilistic network. We agree with the reviewer that this choice should be detailed in the paper and we will include a brief discussion.\n\n**Ensemble optimization**\n\nOur ensemble is composed of seven networks, each with a different random initialization. Component networks are optimized on the same data and rollouts are generated by randomly selecting a network out of the five networks with smallest the validation loss.  \n\n**Optimistic vs risk-seeking behavior**\n\nWe agree with the reviewer that in this example the dynamics are acting optimistic rather than risk-seeking. This distinction applies in general to the framework even when the environment is uncertain --- the policies are optimistic (risk-ignoring) rather than risk-seeking. Nonetheless \"risk-seeking\" is the more common parlance in the literature. \n\n**Probabilistic model for deterministic environment**\n\nA probabilistic model offers some benefits even when the environment dynamics are deterministic. We find that a probabilistic approach provides robustness to poor NN function approximation during early stages of learning.  In our setting, it also plays an important role in smoothing the KL constraint --- under deterministic dynamics the KL is undefined everywhere except on the value $q=p$.\n\n**Population of the model buffer $\\mathcal{D}_{model}$**\n\nYes, we sample a state from $\\mathcal{D}_{env}$ and then produce a roll-out using our variational policy and dynamics. We will include this clarification in the main text.\n\n**Performance convergence**\n\nWe do not provide guarantees that our method will converge to the same value as SAC. Model-based RL methods are known to perform worse asymptotically than model-free methods. We did not train the models to convergence as it becomes computationally prohibitive. Instead, we use the standard number of training iterations in the model-based RL literature for these environments that is matched to comparisons in the existing literature (e.g. MBPO)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6236/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497934380,
                "cdate": 1700497934380,
                "tmdate": 1700497934380,
                "mdate": 1700497934380,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qHuxQEe6H3",
                "forum": "4FUa5dxiiA",
                "replyto": "rszal1SR0P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6236/Reviewer_dkQy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6236/Reviewer_dkQy"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I have read the author's comments, the other reviewers' comments and the public comments.\n\nWhilst I found the paper clear, with regards to the definitions of optimism/risk used in the paper, I have two issues that have made me decrease my score (from 8 to 6), and my confidence (from 4 to 3). \n\nFirst (as highlighted by Reviewer wKFul), the inconsistency of the MBPO experiments relative to the original MBPO paper should be addressed. I recommend the authors rename the current MBPO results to make it clear how they differ from MBPO. Then I'd suggest adding results for a good implementation of MBPO. Whilst both results have value, it is misleading to report MBPO results for a modified version of MBPO.\n\nSecond, I was not aware of papers highlighted by the public comment. These seem very relevant and should at least be cited in the related work."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6236/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646091308,
                "cdate": 1700646091308,
                "tmdate": 1700646091308,
                "mdate": 1700646091308,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IEYHVVRgpU",
                "forum": "4FUa5dxiiA",
                "replyto": "D7VxefXKBP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6236/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6236/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We agree with the reviewer that this difference should be made more explicit in the experiment section. We have uploaded a revised version that addresses this modification in the introduction of the baselines algorithms and renames the MBPO performance curve to MBPO (Fixed iterations). \n\nWe agree that these references are very relevant, and we will include them in the Related Work section."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6236/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691249570,
                "cdate": 1700691249570,
                "tmdate": 1700695095475,
                "mdate": 1700695095475,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LJojG61v6B",
            "forum": "4FUa5dxiiA",
            "replyto": "4FUa5dxiiA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6236/Reviewer_9Tpq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6236/Reviewer_9Tpq"
            ],
            "content": {
                "summary": {
                    "value": "The study introduces a risk-sensitive approach to reinforcement learning (RL) by framing RL as Bayesian inference within a probabilistic graphical model, termed \"control as probabilistic inference.\" Conventional model-based control-as-inference does not consider risk-sensitive policy learning. The authors introduce the \u03b2-VMBPO algorithm, a risk-sensitive variant of the variational model-based policy optimization (VMBPO) algorithm. The novelty of this method lies in its dual descent optimization, which incorporates the risk parameter \u03b2 as an adaptive parameter. This approach is bolstered by a thorough theoretical analysis. Empirical evaluations validate the efficacy of this risk-sensitive methodology, demonstrating its advantage over traditional methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper establishes a clear connection between dual-descent and risk-sensitive variational inference, enabling the optimization of \\beta in a logical manner.\n1. Comprehensive theoretical analyses are provided."
                },
                "weaknesses": {
                    "value": "1. The research's novelty appears limited. Previous studies have already introduced \\beta. If the paper's main contribution centers around a reinterpretation of \\beta through the lens of dual-descent, there's a need for a clearer distinction between this work and earlier research.\n1. Although the theoretical approach is interestingly anchored in the RL-as-inference framework, the paper falls short in its discussion regarding the interpretation of the parameter \\beta within the context of variational inference. A more in-depth exploration and dialogue from the variational inference standpoint would significantly enrich the paper."
                },
                "questions": {
                    "value": "1. In the \"PRELIMINARIES\" section, Equation (5) appears to divide the reward by \\beta directly. While I recognize that this might not be the primary focus of the authors, readers could wonder how such a minimal change can imbue the method with risk sensitivity. A brief and intuitive explanation addressing this would be valuable.\n1. To my understanding, the pioneering work introducing variational inference to model-based reinforcement learning is \"VI-MPC\" by Okada et al. (2020). While the goals of the two studies might differ, I believe it's pertinent for the authors to reference this foundational work.\nReference:\nOkada, Masashi, and Tadahiro Taniguchi. \"Variational inference MPC for Bayesian model-based reinforcement learning.\" Conference on Robot Learning. PMLR, 2020.\n1. In Figure 1, an overlaid directed graph makes the illustration less intuitive. The representation seems muddled, especially when juxtaposed with the legends. It's evident that this figure could benefit from some revisions for clarity.\n1. Dual optimization emerges as a crucial component of this research. Given that several potential readers may be unfamiliar with this concept, adding a reference, such as a standard textbook on convex optimization or a pertinent tutorial paper, would be of great assistance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6236/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6236/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6236/Reviewer_9Tpq"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6236/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822740832,
            "cdate": 1698822740832,
            "tmdate": 1700605686277,
            "mdate": 1700605686277,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vG71iBwjTx",
                "forum": "4FUa5dxiiA",
                "replyto": "LJojG61v6B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6236/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6236/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Risk parameter novelty**\n\nAlthough the risk parameter $\\beta$ has been considered in previous work, its role has been understudied---as reviewer dkQy correctly points out. The need for, and impact of, practical tuning of the risk parameter is further supported by the public comment on this paper and associated work (see Brendan O'Donoghue's comment above).  In the safe RL literature (Garcia \\& Fernandez, 2015; Shen et al.~2013; Noorani \\& Baras, 2022) it has been shown that $\\beta > 0$ exhibits risk seeking behavior and recovers the risk-neutral objective when $\\beta \\to 0$, but no method has been proposed to adequately tune this parameter. In VMBPO (Chow et al, 2021), it was demonstrated that learning degradation might occur due to variance amplification, and this parameter can alleviate this issue, but tuning it was not addressed. Our work, studies how $\\beta$ affects the posterior dynamics and the optimal policy, and demonstrates that it is a key component to obtain policies that can be applied to the real environment. \n\n**$\\beta$ within the context of variational inference**\n\nIn the variational lower bound, the parameter $\\beta$ limits the disagreement between the variational dynamics and the true dynamics through the KL penalty. Fig. 1b shows this behavior where we found the optimal posterior dynamics for different $\\beta$ values. $\\beta$-VAE (Higgins et al, 2017) studied a similar trade-off, albeit in a different context where $\\beta$ limits the capacity of the variational distribution thus permitting the method to learn disentangled representations. In our work, the constraint is w.r.t.~the environment dynamics producing less risk seeking variational distributions.  We will include this discussion in the Related Work section.\n\nReference:\n\nHiggins, Irina, et al. \"beta-vae: Learning basic visual concepts with a constrained variational framework.\" International conference on learning representations. 2016.\n\n**Shared reference VI-MPC**\n\nWe appreciate the shared reference by the reviewer. We agree that it is relevant and we will include it accordingly in our work. But we want to differentiate this method from variational model-based RL algorithms. The variational inference for VI-MPC only affects the policy --- similarly to SAC (Haarnoja et al, 2018) and MPO (Abdolmaleki et al, 2018). In VI-MPC model dynamics do not appear as part of the optimization objective, contrary to VMBPO and our work where the model is part of the variational optimization.\n\n**Dual optimization reference**\n\nWe agree with the reviewer that the reader may benefit from including a reference for dual optimization. We will include it in our work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6236/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498315331,
                "cdate": 1700498315331,
                "tmdate": 1700498357528,
                "mdate": 1700498357528,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LcTgGDte4O",
                "forum": "4FUa5dxiiA",
                "replyto": "LJojG61v6B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6236/Reviewer_9Tpq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6236/Reviewer_9Tpq"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for your clarification.\n\n>In Figure 1, an overlaid directed graph makes the illustration less intuitive. \n\nI hope the authors deal with this problem in their revised version as well.\n\n> In VI-MPC model dynamics do not appear as part of the optimization objective, contrary to VMBPO and our work where the model is part of the variational optimization.\n\nAgree and understood. \nThis comment reminded me the following paper as well. This is just for your information.\n\nOkada et al. \"Planet of the Bayesians: Reconsidering and improving deep planning network by incorporating bayesian inference.\" 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2020.\n\nThe score has been updated based on your feedback."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6236/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605635729,
                "cdate": 1700605635729,
                "tmdate": 1700605971709,
                "mdate": 1700605971709,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XDFyqYh5Kf",
            "forum": "4FUa5dxiiA",
            "replyto": "4FUa5dxiiA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6236/Reviewer_wKFu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6236/Reviewer_wKFu"
            ],
            "content": {
                "summary": {
                    "value": "Briefly, the original VMBPO algorithm learns a dynamics model\nwhere the dynamics transitions are weighted according to the\nexponential of the value of the transitions, in accordance\nwith the control as inference approach.\nThis paper modifies the VMBPO algorithm to include an inverse\ntemperature parameter $\\beta$ so that the risk seeking behavior of\nVMBPO could be modulated. Another change is the way how the likelihood\nratio $q/p$ of the variational model to the true dynamics is computed:\nin the original VMBPO, they use a direct estimator for the ratio,\nwhereas in the current work, they learn two models, one for $q$ and\none for $p$. Moreover, they suggest to tune the $\\beta$ parameter by\nsetting a target KL divergence, and optimizing $\\beta$, to achieve the\ntarget in a similar way how the SAC algorithm optimizes their inverse\ntemperature parameter to achieve a target entropy.\n\nThe experiments included two simple tabular domains to show how\nmodulating $\\beta$ controls the risk seeking behavior of the method,\nand simple control tasks: Mountain Car, Half Cheetah, Walker2D, Hopper\n(note that the abstract says they tried DeepMind Control Suite, but this\nseems inaccurate, it seems the work only contains OpenAI Gym experiments)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The necessity of $\\beta$ in the VMBPO algorithm is clear, so the\napproach is well-motivated from this point of view.\n2. There was a study looking at how the tuning of the $\\beta$ works\nin an MDP domain that showed that the method is robust to initializations\nin this domain suggesting reliability of the method."
                },
                "weaknesses": {
                    "value": "1. While the necessity of the $\\beta$ term is clear, why one\nshould use a VMBPO-based approach to begin with was not clear to me.\nThe majority of the discussion was about how VMBPO can be risk seeking,\nand how modulating the $\\beta$ term can prevent the risk seeking\nbehavior, but one can also avoid the risk seeking behavior by simply\nnot using VMBPO. I think the advantage of using the control as inference\napproach should also be explained and demonstrated.\n\n2. There were only 3 OpenAI Gym benchmark environments. It would have been\ngood to include more, e.g., Ant and Humanoid.\n\n3. The experimental results in HalfCheetah and Walker2d do not match the\npublished results in the original MBPO paper\n(https://arxiv.org/pdf/1906.08253.pdf, figure 2). In particular, HalfCheetah\nseems to go clearly over 10000 (reaching around 12000), whereas it barely\nreaches 10000 in this paper, also in Walker, the results go clearly\nabove 4000, but barely reach 4000 in this paper. The results on Hopper\nwere indistinguishable between MBPO and Beta_VMBPO. Also the improvement\nis quite marginal. I am not confident the method reliably improves over\nMBPO. Also, the number of random trials was 5, which is low.\n\n4. While there were experiments on smaller tasks aimed at explaining how the\nmethod works, the only statistics provided in the Gym tasks were the reward\ncurves. It would have been better to provide some other statistics that\ndemonstrate that the risk sensitivity control is working as intended.\n\n5. The computational time was not discussed. In particular, as you compute\ntwo models, $p$ and $q$ does that affect the computational time?\n\n6. I think the risk seeking behavior of VMBPO and that $\\beta$ modulates\nthis are obvious, and I think too much space was used for explaining\nthese points. I think the simple experiments on the tabular\ndomains and on the mountain car were redundant, as they do not show any non-obvious\nresult. For example, in the mountain car task, if the $\\beta$ parameter is set\nsufficiently large, then clearly the variational model should try to match the\ntrue dynamics, and the method should work. I would have liked to see a result\nthat requires tuning $\\beta$ and cannot be achieved without simply trying to\nlearn the true model.\n\n7. Equation 8 suggests an inequality constraint for the KL, yet in\nEquation 9, you are optimizing for an equality constraint (as is done\nin SAC, also there was no reference to SAC). I was not completely\nconvinced with this.\n\n8. The works lists the necessity to tune the KL constraint target\n$\\epsilon$ parameter as a limitation. However, it is also necessary to\nset an initial $\\beta$ value, as well as a learning rate for\n$\\beta$. While the experiments looked at the sensitivity to these\nparameters in a simple tabular task, the senstivity was not examined\nin the other tasks, so it is not completely clear how reliable the\nmethod is.\n\n9. VMBPO and the suggested Beta_VMBPO have more differences than simply adding in a beta value. I think there should have been ablation studies on the different components of Beta_VMBPO, e.g., the tuning of the beta value, etc. (performed in the OpenAI Gym tasks)"
                },
                "questions": {
                    "value": "In this work, the ratio q/p is estimated by learning two models:\none for q and one for p, then taking the ratio. In the origianl VMBPO,\nthey use a direct estimator, v, for q/p. Vapnik's principle suggests\nthat learning a direct estimator is typically better than solving\nmore general intermediate steps. Have you compared with this method,\nand why did you choose to learn two models?\n\nPlease also feel free to respond to any of the listed weaknesses.\n\nBottom of page 12, there's a typo: \"p(.|s_t, s_t)\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6236/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698931899924,
            "cdate": 1698931899924,
            "tmdate": 1699636681450,
            "mdate": 1699636681450,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xlAkEoBrfg",
                "forum": "4FUa5dxiiA",
                "replyto": "XDFyqYh5Kf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6236/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6236/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the multiple points raised by the reviewer. We have incorporated an extra comparison for OpenAI benchmarks (Ant) and an ablation study for the initialization and learning rate of beta in the appendix. We now address some points made by the reviewer.\n\n**Motivation for control as inference**\n\nThe risk seeking behavior is not just an issue with VMBPO, but a fundamental problem with the control-as-inference framework that merits further investigation. Despite this behavior the methodology has inspired many novel techniques and algorithms with good performance in high dimensional spaces (see Related Work). These methods, such as MaxEnt RL, avoid problematic risk-seeking behavior by imposing additional constraints on the variational approximation that lead to weak bounds on the marginal likelihood. Our work demonstrates that these constraints are unnecessary, allowing for more flexible variational approximations.  By further emphasizing the connection to the exponential utility we establish $\\beta$-VMBPO as an instance of Risk-Sensitive RL for which there is extensive benefit in practical applications (Garcia and Fernandez, 2015). This work opens a path for unconstrained variational methods in the control-as-inference framework.\n\n**Experimental results gap**\n\nImplementation details are documented in Appendix C.1.  To reduce computation and obtain a fair comparison we impose consistent optimization of the dynamics architecture across all methods.  The original MBPO optimization stops when validation loss sees no improvement for five consecutive epochs, leading to long computation times. We instead optimize the dynamics network for a fixed number of epochs (See appendix C.1). All methods use the exact same number of iterations and same network architectures making the comparison consistent across algorithms. The number of random trials (five) is the same as was used in the original MBPO paper. \n\n**\"Inequality constraint for the KL...\"**\n\nOur optimization corresponds to an inequality constraint $KL(q||p) \\leq \\epsilon$.  This results in the Langrangian $\\mathbb{E}_q[\\sum r(s_t, a_t)] -\\beta[KL(q||p)-\\epsilon]$. The dual optimization results in the loss from Eq. (9) which is correct by complementary slackness and the necessary KKT conditions.  SAC does not optimize an equality constraint, but an inequality constraint just as our method. We make reference to SAC after Eq. (8) to compare the model-free approach where this term the variational dynamics is removed.  We also discuss the relationship with SAC in the related work section where we compare the similarities between our dual optimization and SAC's optimization, with the major difference being risk modulation instead of controlling the policy's entropy.\n\n**\"The ratio q/p is estimated by learning two models...\"**\n\nThrough extensive experimentation we found that directly estimating the log ratio using the convex conjugate of Chow et al. (2021) was unstable for environments with deterministic dynamics, such as the OpenAI Gym benchmarks. Our estimator solves this problem by relaxing the assumption of determinism with two dynamics models represented by network ensembles."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6236/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498666914,
                "cdate": 1700498666914,
                "tmdate": 1700498666914,
                "mdate": 1700498666914,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RbBCsugtMz",
                "forum": "4FUa5dxiiA",
                "replyto": "XDFyqYh5Kf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6236/Reviewer_wKFu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6236/Reviewer_wKFu"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the comment, but unfortunately I have not changed my mind.\n\nRegarding the constraint: (EDIT: removed comment on constraint as I may have misunderstood something). However, I am still of the opinion that the discussion of SAC is not sufficiently clear. The method for tuning the parameter is basically exactly the same. I believe this should be made clear, e.g., by equation 9. To me, the discussion of SAC was too obfuscated.\n\nThe performance you showed on Ant also does not match previous results.\n\nMy impression is that any gains you are seeing in the experimental results are due to implementation changes rather than a fundamental advantage of your method compared to MBPO (i.e., the settings you chose may leave an impression that your method performs better, but with other settings, MBPO would be better or the same). For me to change my assessment, I would request that the method is compared against a well-performing implementation of MBPO, or that you show a clear win on some task. Computation times and statistics other than reward curves were still not discussed. Regarding only using 5 seeds, this was still accepted at the time of the MBPO paper, but it has received a lot of criticism since then, e.g., see \"Deep Reinforcement Learning at the Edge of the Statistical Precipice\" (https://arxiv.org/abs/2108.13264) for a discussion, and better assessment methods.\n\nCurrently, the main clear result in the paper seems to be the necessity for the $\\beta$ parameter, but I believe that this alone is not sufficiently significant. The public commenter has also kindly pointed out many related works that also use a $\\beta$ parameter. Also, the use of an ensemble for aleatoric uncertainty does not seem well founded. So, I'm afraid I will not be changing my assessment based on the comments in the rebuttal. I would encourage the authors to revise the paper, and try running experiments when comparing against a well-performing version of MBPO."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6236/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620630580,
                "cdate": 1700620630580,
                "tmdate": 1700621709961,
                "mdate": 1700621709961,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9OidubbU7E",
                "forum": "4FUa5dxiiA",
                "replyto": "blqr9eYO7j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6236/Reviewer_wKFu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6236/Reviewer_wKFu"
                ],
                "content": {
                    "comment": {
                        "value": "**SAC discussion:**\n\nThe method for tuning the dual parameter is the same. The only difference is that in SAC they take the difference between the entropy and target entropy, whereas in your paper, you use the difference between the KL and target KL. The method for tuning the $\\beta$ parameter was claimed as a main contribution of the paper, but unfortunately I do not consider it very novel (moreover, I would request that the relationship to the method in SAC is made clear). This is not a major issue by itself, but I would want to be convinced by the experiments that the method is working well. Moreover, did you have any experiments with fixed $\\beta$ values? It would be necessary for me to show that the tuning clearly beats using a fixed $\\beta$ value.\n\n**Remaining perceived issues in the paper:**\n\nThe biggest issue for me is that the experiments do not reproduce previous performance (and the results you show for your algorithm are also not competitive with current methods). I understand that MBPO is slow in its original formulation, but if your method really works, then it should not be too difficult to run your algorithm in a similar formulation. If the method is effective, I would expect it to give better performance with the parameters that you have already tuned. For me to change my assessment of the work, running these experiments would be necessary.\n\nSome additional concerns I have are:\nFor the new ablation study on the learning rate and initial $\\beta$ parameter, why did you choose Hopper? This was the only environment where there was clearly no advantage to your method compared to other algorithms, so perhaps this environment was not very sensitive to tuning. Also, the method was not completely robust to the setting of the initial $\\beta$ value: the performance drops considerably for $\\beta=10$ (it is particularly worrying that 1, and 100 seem to give reasonable performance, so we can't identify a clear trend of when the performance drops). Considering that you originally only had 3 environments, it may be better to show the results for all of the environments. Also, how does the performance differ when the $\\epsilon$ parameter is changed? I didn't find a study on this. The setting of $\\epsilon$ was also not clear to me. In Table 1, it is written that $\\epsilon=0.1$, but in section C.4, it is written that $\\epsilon=10$.\n\nYou wrote: \"Through extensive experimentation we found that directly estimating the log ratio using the convex conjugate of Chow et al. (2021) was unstable for environments with deterministic dynamics, such as the OpenAI Gym benchmarks.\"\nPlease provide the evidence for this in the article, as it could be an additional contribution of the work.\n\nRegarding the use of ensemble models, I guess your explanation is that you used an ensemble to improve the performance, but considered aleatoric uncertainty for the KL computation. In your response to reviewer dkQy you stated that you will include an explanation of the KL computation, but I didn't find this in the paper yet. How exactly you did this was also not completely clear to me.\n\nThere are also many points in my review that were never addressed in the rebuttal, e.g., what is the computation time? Do you have any additional statistics (e.g., how does the KL change during learning, how does $\\beta$ change during learning in the MuJoCo tasks)? I would request more extensive experimental results in the paper for me to change my assessment. Currently, the results are not convincing to me to show that the method is indeed working well and providing an advantage over a regular MBPO based approach. I do not expect myself to change my score during this submission phase, but if the authors wish to continue the rebuttal, then please address all of the points in my reviews, either agreeing or disagreeing."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6236/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706297141,
                "cdate": 1700706297141,
                "tmdate": 1700706297141,
                "mdate": 1700706297141,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]