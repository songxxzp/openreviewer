[
    {
        "title": "Learning without Forgetting for Vision-Language Models"
    },
    {
        "review": {
            "id": "Z9RgAlBa7G",
            "forum": "k9NYnsC4Mq",
            "replyto": "k9NYnsC4Mq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1667/Reviewer_s6fA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1667/Reviewer_s6fA"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes one of the first mechanism to do continual learning with Vision-Language Models (VLM) such as CLIP. Through a system of projectors and a revised definition of context, the authors tested their model, PROOF, on a variety of datasets for continual learning obtaining state-of-the-art performances."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors tested for the first time a VLM model for continual learning. \n- The authors tested their PROOF on a variety of datasets testing the effectiveness of the model.\n- The authors proved the effectiveness of the model with very interesting and detailed ablation studies."
                },
                "weaknesses": {
                    "value": "- The paper lacks motivation and innovation: The authors suggest using CLIP for class-incremental continual learning, but it would be more interesting to see its performance on tasks like incremental captioning or retrieval. Unlike L2P, where a large pretrained model was used, CIL could have been just one application.\n- Furthermore, the PROOF mechanism, while innovative, lacks depth. Projection networks are common in continual learning, and the new context definition isn't explored.\n- The main paper lacks standard deviation in results and doesn't consider multiple runs with different class orders. \n- There's no analysis of time and memory usage, except for a basic mention of memory requirements in supplementary materials. \n- The paper's narration could also be improved"
                },
                "questions": {
                    "value": "- It looks like the supplementary materials are more informative and present more interesting results w.r.t. the main paper. Why did the authors exclude them from the main paper?\n- The definition of W is not reported in the paper. How W is defined in the context?\n- Can the authors provide an analysis of the accuracies of the model varying the number of exemplars?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1667/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1667/Reviewer_s6fA",
                        "ICLR.cc/2024/Conference/Submission1667/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698658440497,
            "cdate": 1698658440497,
            "tmdate": 1700662948739,
            "mdate": 1700662948739,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l66Zi8NZCD",
                "forum": "k9NYnsC4Mq",
                "replyto": "Z9RgAlBa7G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1667/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1667/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1667/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1667/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1667/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1667/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1667/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer s6fA (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback and comments. We have carefully revised the manuscript following the reviewer\u2019s comments, and the equation/section numbers in the rebuttal refer to the latest version. We respond to the concerns below:\n\n**Q1** The paper lacks motivation and innovation: The authors suggest using CLIP for class-incremental continual learning, but it would be more interesting to see its performance on tasks like incremental captioning or retrieval. Unlike L2P, where a large pretrained model was used, CIL could have been just one application.\n\n**A1** We thank the reviewer for the suggestion. Just as the reviewer pointed out, we not only investigate PROOF\u2019s strong performance in class-incremental learning in this paper. Additionally, **we have compared PROOF to other competitors in the continual cross-modal retrieval tasks in Section 5.4 and Figure 4 (a) in the initial submission**. Correspondingly, we find PROOF is a concise and unified framework that solves catastrophic forgetting in various continual learning scenarios (e.g., class-incremental learning and continual cross-modal retrieval).\n\n**Q2** Furthermore, the PROOF mechanism, while innovative, lacks depth. Projection networks are common in continual learning, and the new context definition isn't explored.\n\n**A2** We thank the reviewer for the suggestion. Since the reviewer did not give any related papers on \u201cProjection networks are common in continual learning\u201d, we have searched for related topics and found some, e.g., [1-3] However, among these related papers, the projection is designed on the parameter/gradient space, i.e., these works are designed to tackle forgetting by finding optimization paths through projection. By contrast, the projection in this paper is designed on the embedding space, which aims to encode task-specific information on pre-trained embeddings. Our method creates and learns parallel subspaces based on the pre-trained vision-language model, while others do not. **Hence, it seems the \u201cprojections\u201d are obviously two different concepts.** We are happy to give further clarifications if the reviewer has other related papers for supplementary.\n\n**Furthermore, we need to highlight that the core contribution of our paper is not how to design the projection since they are only linear layers. By contrast, our contributions lie in enabling a pre-trained vision-language model to adapt to downstream tasks continually. Such a task requires the projection expansion strategy, cross-modal fusion, and context joint adaptation. We believe these parts are not common in other papers.**\n\n\n**Q3** The main paper lacks standard deviation in results and doesn't consider multiple runs with different class orders.\n\n**A3** We thank the reviewer for the suggestion. In the main paper, we follow the benchmark class-incremental learning setting [4] to shuffle classes with random seed 1993. Just as the reviewer pointed out, we not only run the paper one time in our experiments. By contrast, **we have compared PROOF to other competitors with five class orders (1993, 1994, 1995, 1996, 1997) and reported the performance curve in Supplementary Section D.1 in the initial submission**. These performance curves contain the average and standard deviation information, and results on CIFAR and ImageNet-R with multiple runs verify that our PROOF consistently outperforms other competitors."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700380136578,
                "cdate": 1700380136578,
                "tmdate": 1700380136578,
                "mdate": 1700380136578,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C10Bpjf91l",
                "forum": "k9NYnsC4Mq",
                "replyto": "Z9RgAlBa7G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1667/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1667/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking Forward to Further Discussions"
                    },
                    "comment": {
                        "value": "Dear Reviwer s6fA,\n\nWe sincerely appreciate your great efforts in reviewing this paper. Your constructive advice and valuable comments really help improve our paper. Considering the approaching deadline, please let us know if you have follow-up concerns. We sincerely hope you can consider our reply in your assessment, and we can further address unclear explanations and remaining concerns, if any.\n\nOnce more, we appreciate the time and effort you've dedicated to our paper.\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567269116,
                "cdate": 1700567269116,
                "tmdate": 1700567269116,
                "mdate": 1700567269116,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dyb2kQlxjd",
                "forum": "k9NYnsC4Mq",
                "replyto": "Z9RgAlBa7G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1667/Reviewer_s6fA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1667/Reviewer_s6fA"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the answers, however, I have still some concerns about the innovation and applicative scenarios."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639252774,
                "cdate": 1700639252774,
                "tmdate": 1700639252774,
                "mdate": 1700639252774,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gfGfhZXi5U",
                "forum": "k9NYnsC4Mq",
                "replyto": "FkSCUMGIy2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1667/Reviewer_s6fA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1667/Reviewer_s6fA"
                ],
                "content": {
                    "comment": {
                        "value": "I carefully revised the new version of the paper and raised the score to 5."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714523601,
                "cdate": 1700714523601,
                "tmdate": 1700714523601,
                "mdate": 1700714523601,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3yMp8grbnX",
            "forum": "k9NYnsC4Mq",
            "replyto": "k9NYnsC4Mq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1667/Reviewer_VzW8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1667/Reviewer_VzW8"
            ],
            "content": {
                "summary": {
                    "value": "Prior works only focus on the visual branch of CLIP for incremental learning. This paper argues both modalities are important.\n- PROOF freezes the image and text encoders of the pre-trained VLM (e.g. CLIP). These contain the generalizable representations learned during pre-training.\n- For each new incremental task, it adds new projection layers (P_i, P_t) on top of the frozen encoders. These projections are task-specific.\n- When a new task arrives, only the parameters of the projections for the new task are trained. The old projections remain frozen.\n- Cross-modal attention fusion is used to adjust the query embedding using context like prototypes and prompts. This allows utilizing both visual and textual information to obtain comprehensive embeddings.\n- At inference time, the projections are aggregated to obtain a unified classification. But the old projections remain unchanged."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Technically a novel idea to incorporate both the visual and the text encoders. \nImproves upon SOTA."
                },
                "weaknesses": {
                    "value": "- Inference Mismatch - Projections are combined at inference time which may not fully match the training conditions for a specific task projection. \n\n- Representation Drift - The post-attention module representations learned by the frozen projections may drift or shift slightly during new task training due to weight updates elsewhere. Small drifts can accumulate.\n\n- Section 3 is really long and has a lot of redundant information, it should be made much shorter. That space should be given to increase the length of section 4 to give a better understanding of the fusion module."
                },
                "questions": {
                    "value": "- Any comments on the issues pointed out in the weaknesses will be appreciated.\n\n- Also please make it more clear how you are using attention."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789113732,
            "cdate": 1698789113732,
            "tmdate": 1699636094720,
            "mdate": 1699636094720,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hnDoaUDrvs",
                "forum": "k9NYnsC4Mq",
                "replyto": "3yMp8grbnX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1667/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1667/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1667/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1667/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1667/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1667/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1667/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VzW8 (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback and comments. We have carefully revised the manuscript following the reviewer\u2019s comments, and the equation/section numbers in the rebuttal refer to the latest version. We respond to the concerns below:\n\n**Q1** Inference Mismatch - Projections are combined at inference time which may not fully match the training conditions for a specific task projection.\n\n**A1** We thank the reviewer for the question. We first review the training process of PROOF to illustrate why inference mismatch will not happen. Given the pre-trained vision-language model, we freeze the pre-trained weights and learn expandable projections to map image/text embeddings into task-specific space. Since we learn expandable projections as data evolves, we utilize the additive result as the final image/text embedding (i.e., $P_i(z)$ and $P_t(w)$ in Eq. 4). During training, we freeze former tasks\u2019 projections and only train the current one. The training target is to match the additive features of image-text pair $P_i(z)$ and $P_t(w)$, and the latest projection (the current task\u2019s projection) is optimized to serve as **residual terms** that can adjust the embedding for a holistic prediction. **Since the loss term in Eq. 7 is optimized with exemplars (i.e., a few old class instances) and the current dataset, optimizing it will make the aggregated features holistic to consider all classes.** In other words, since the target is to learn new classes and meanwhile maintain the performance of old classes, being \u2018ideal\u2019 projections for some specific task is not the best solution for all seen classes, and our learning protocol strikes a balance between learning new and remembering old. Even if additional projection terms are added after some specific tasks, optimizing Eq. 7 with exemplars and the current dataset still leads to a well-calibrated prediction considering all seen classes. \n\nTo empirically show the forgetting degree of different tasks, we also measure the relative forgetting degree using the \u201cForgetting Measure\u201d in [1][2]. Considering each task\u2019s forgetting degree, it measures the gap between the best and final performance and averages among all tasks. Hence, methods with a lower forgetting measure show better performance in resisting catastrophic forgetting. In this revision, we have supplied the results in **Supplementary Section E.4** and report the results here. We report the results compared to three competitors, i.e., CoOp, iCaRL, and MEMO. \n\n\n| Method | Exemplar | Aircraft B0 Inc10| Cars B0 Inc10| CIFAR100 B0 Inc10| UCF B0 Inc10|\n|:----|:----:|:----:|:----:|:----:|:----:| \n|CoOp| &#10003;| 28.20|\t7.43\t|\t20.74|\t10.81|\n|iCaRL | &#10003;| 13.44|\t4.18\t|\t31.45|\t6.66|\n|MEMO| &#10003;| 17.34|\t4.77|\t\t12.40|\t9.78|\n|PROOF| &#10003;| **9.41**| **3.51**| **8.02**| **4.07**|\n\nAs shown in the table, **PROOF has the lowest forgetting measure, indicating it suffers the least forgetting (less than 10%) compared to other competitors.** It verifies that the final performance and best performance are almost the same even after long training stages for PROOF. In other words, \nthe optimization target makes the latest projection a residual term to **learn a unified embedding considering all seen classes, and inference mismatch can be alleviated**."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700380045655,
                "cdate": 1700380045655,
                "tmdate": 1700380045655,
                "mdate": 1700380045655,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GqSH4W4AxX",
                "forum": "k9NYnsC4Mq",
                "replyto": "3yMp8grbnX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1667/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1667/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking Forward to Further Discussions"
                    },
                    "comment": {
                        "value": "Dear Reviwer VzW8,\n\nWe sincerely appreciate your great efforts in reviewing this paper. Your constructive advice and valuable comments really help improve our paper. Considering the approaching deadline, please let us know if you have follow-up concerns. We sincerely hope you can consider our reply in your assessment, and we can further address unclear explanations and remaining concerns, if any.\n\nOnce more, we appreciate the time and effort you've dedicated to our paper.\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567225801,
                "cdate": 1700567225801,
                "tmdate": 1700567225801,
                "mdate": 1700567225801,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O5SQAeKZmn",
                "forum": "k9NYnsC4Mq",
                "replyto": "GqSH4W4AxX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1667/Reviewer_VzW8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1667/Reviewer_VzW8"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the Authors for the extended explanations.\n\nThey are certainly helpful. Although they improve my view of the paper, I am confirming my initial rating."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616156004,
                "cdate": 1700616156004,
                "tmdate": 1700616156004,
                "mdate": 1700616156004,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aMEheRX2i2",
            "forum": "k9NYnsC4Mq",
            "replyto": "k9NYnsC4Mq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1667/Reviewer_asYX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1667/Reviewer_asYX"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a class-incremental learning (CIL) method based on vision-language models. Specifically, this paper mainly focuses on two key challenges to CIL, named how to adapt the model without forgetting and how to make full use of the multi-modal information. To deal with the first challenge, a task-specific projections are proposed based on the frozen image/text encoders. To deal with the second challenge, a fusion module is proposed for better exploit the cross-modality information. Experiments have shown the state-of-the-art performance of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- In general, the proposed method is well motivated and clearly presented.\n- The paper turns a VLM into a continual learner that is both retentive and comprehensive.\n- Good performance is achieved."
                },
                "weaknesses": {
                    "value": "- The effectiveness of alleviating forgetting is uncertain. The process involves incrementally learning image projection heads and text projection heads, which are then combined for various tasks. When new tasks are learned, the projections of previous tasks are fixed and not updated. However, during inference, the projections of all tasks are merged, which might not be ideal for test data from older tasks due to potential side effects caused by the projections from the new tasks.\n- The extent to which contextual information is effective has not been extensively studied. The projection fusion method proposes to contextualize and merge embeddings and contextual information using self-attention. However, in the experiments, only the results of Projection & Fusion are compared with Projection & Fusion & Context Prompt, without explicitly evaluating the effectiveness of the concatenated context information in Q, K, V as [P_i(z), Context] in self-attention, or the effectiveness of the context prompt. In other words, the final context information is defined as Context = [P, W, C], but the specific contributions of W and C to the final results need further analysis.\n- The evaluation metric used may not provide a comprehensive measure of the extent of forgetting."
                },
                "questions": {
                    "value": "- To what extent the proposed method could alleviate forgetting?\n- How does each component of the contextual information contribute to the final results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1667/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1667/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1667/Reviewer_asYX"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1667/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819925330,
            "cdate": 1698819925330,
            "tmdate": 1700635408475,
            "mdate": 1700635408475,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "drM3wuEn4f",
                "forum": "k9NYnsC4Mq",
                "replyto": "aMEheRX2i2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1667/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1667/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1667/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1667/Reviewers",
                    "ICLR.cc/2024/Conference/Submission1667/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission1667/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1667/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer asYX (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback and comments. We have carefully revised the manuscript following the reviewer\u2019s comments, and the equation/section numbers in the rebuttal refer to the latest version. We respond to the concerns below:\n\n**Q1** The effectiveness of alleviating forgetting is uncertain. The process involves incrementally learning image projection heads and text projection heads, which are then combined for various tasks. When new tasks are learned, the projections of previous tasks are fixed and not updated. However, during inference, the projections of all tasks are merged, which might not be ideal for test data from older tasks due to potential side effects caused by the projections from the new tasks.\n\n**A1** We thank the reviewer for the question and answer it from two aspects. We first review the training process of PROOF to illustrate its rationale for overcoming forgetting. Given the pre-trained vision-language model, we freeze the pre-trained weights and learn expandable projections to map image/text embeddings into task-specific space. Since we learn expandable projections as data evolves, we utilize the additive result as the final image/text embedding (i.e., $P_i(z)$ and $P_t(w)$ in Eq. 4). During training, we freeze former tasks\u2019 projections and only train the current one. The training target is to match the additive features of image-text pair $P_i(z)$ and $P_t(w)$, and the latest projection (the current task\u2019s projection) is optimized to serve as **residual terms** that can adjust the embedding for a holistic prediction. **Since the loss term in Eq. 7 is optimized with exemplars (i.e., a few old class instances) and the current dataset, optimizing it will make the aggregated features holistic to consider all classes.** In other words, since the target is to learn new classes and meanwhile maintain the performance of old classes, being \u2018ideal\u2019 projections for old classes is not the best solution for all seen classes, and our learning protocol strikes a balance between learning new and remembering old. \n\nSecondly, we also measure the relative forgetting degree using the \u201cForgetting Measure\u201d in [1][2]. Considering each task\u2019s forgetting degree, it measures the gap between the best and final performance and averages among all tasks. Hence, methods with a lower forgetting measure show better performance in resisting catastrophic forgetting. In this revision, we have supplied the results in **Supplementary Section E.4** and report the results here. We report the results compared to three competitors, i.e., CoOp, iCaRL, and MEMO. \n\n\n| Method | Exemplar | Aircraft B0 Inc10| Cars B0 Inc10| CIFAR100 B0 Inc10| UCF B0 Inc10|\n|:----|:----:|:----:|:----:|:----:|:----:| \n|CoOp| &#10003;| 28.20|\t7.43\t|\t20.74|\t10.81|\n|iCaRL | &#10003;| 13.44|\t4.18\t|\t31.45|\t6.66|\n|MEMO| &#10003;| 17.34|\t4.77|\t\t12.40|\t9.78|\n|PROOF| &#10003;| **9.41**| **3.51**| **8.02**| **4.07**|\n\nAs shown in the table, **PROOF has the lowest forgetting measure, indicating it suffers the least forgetting (less than 10%) compared to other competitors.** It verifies that the final performance and best performance are almost the same even after long training stages for PROOF.\n\nIn summary, the optimization target makes the latest projection a residual term to **learn a unified embedding considering all seen classes**. Besides, experimental results also verify that PROOF shows the lowest forgetting among all competitors."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379992813,
                "cdate": 1700379992813,
                "tmdate": 1700379992813,
                "mdate": 1700379992813,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RggtEGGmEr",
                "forum": "k9NYnsC4Mq",
                "replyto": "aMEheRX2i2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1667/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1667/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking Forward to Further Discussions"
                    },
                    "comment": {
                        "value": "Dear Reviwer asYX,\n\nWe sincerely appreciate your great efforts in reviewing this paper. Your constructive advice and valuable comments really help improve our paper. Considering the approaching deadline, please let us know if you have follow-up concerns. We sincerely hope you can consider our reply in your assessment, and we can further address unclear explanations and remaining concerns, if any.\n\nOnce more, we appreciate the time and effort you've dedicated to our paper.\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567184499,
                "cdate": 1700567184499,
                "tmdate": 1700567184499,
                "mdate": 1700567184499,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZloQpTaN3c",
                "forum": "k9NYnsC4Mq",
                "replyto": "RggtEGGmEr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1667/Reviewer_asYX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1667/Reviewer_asYX"
                ],
                "content": {
                    "title": {
                        "value": "Post rebuttal"
                    },
                    "comment": {
                        "value": "The authors have addressed most of my concerns. I would like to raise my score to 6."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1667/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635388218,
                "cdate": 1700635388218,
                "tmdate": 1700635388218,
                "mdate": 1700635388218,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]