[
    {
        "title": "TADA: Timestep-Aware Data Augmentation for Diffusion Models"
    },
    {
        "review": {
            "id": "vURGF9NfgW",
            "forum": "U6Mb3CRuj8",
            "replyto": "U6Mb3CRuj8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1139/Reviewer_L1D2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1139/Reviewer_L1D2"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed TADA, which uses SNR to control the strength of augmentation functions applied to the images in different timesteps when training diffusion models. The experimental results show the effectiveness of TADA on FFHQ and AHFQ-v2 datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Although the idea of learning the augmentation strength is studied in image classification, using SNR as a measure to control the augmentation strength in generative diffusion models seems novel to me.\n\nThe paper is easy to follow."
                },
                "weaknesses": {
                    "value": "(1) As TADA uses multiple types of augmentation, including h-flip, the improvement of TADA over the h-flip baseline may come from the use of other augmentations but not the proposed weighting scheme. The author should include more experiments to show the effectiveness of the time-aware weighting scheme, for example, comparing with a baseline with a fixed augmentation strength while keeping other configurations the same as TADA.\n\n(2) TADA underperforms the AR baseline in various settings. The author should repeat the experiments multiple times and report the average and standard deviation of the results.\n\n(3) The proposed timestep-aware weighting scheme is motivated based on the observation of two toy examples on 200 images from the FFHQ-5k dataset. Many of the parameters are selected manually: the augmentation probability, the choice of the augmentation functions, the SNR-sensitive region, and the maximum augmentation strength $\\delta$. As observed from Table 9, the FID and KID increase if we use a sensitive region of [-3, 1] instead of [-3, 0]. Although the current setting is found to be effective on FFHQ and AHFA, it is unclear whether these pre-defined parameters need to be fine-tuned on any new datasets.\n\nWhile it is acceptable that there are no solid theories supporting the timestep-aware mechanism, the method should be validated on sufficiently many scenarios, showing that the method is effective in general. I believe evaluating the method only on human faces (FFHQ) and animal faces (AHFQ) is not enough."
                },
                "questions": {
                    "value": "The proposed TADA can only apply a timestep-aware global strength for all augmentations. The method cannot assign strength and probability values for individual augmentations, which may result in limited augmentation flexibility. In Table 8, applying only the color transformation leads to the best result. However, TADA cannot discover such an augmentation policy."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697530991764,
            "cdate": 1697530991764,
            "tmdate": 1699636040361,
            "mdate": 1699636040361,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2ZbAakeZPR",
                "forum": "U6Mb3CRuj8",
                "replyto": "vURGF9NfgW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer L1D2 (1)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your time in reading the paper and constructive feedback to improve our paper. \n\nPlease find below our detailed responses to your comments: \n\n**The proposed TADA can only apply a timestep-aware global strength for all augmentations. The method cannot assign strength and probability values for individual augmentations, which may result in limited augmentation flexibility.**\n\nWe can assign different strengths and probability to individual augmentation. This can be checked on the provided source code. Specifically, a user can set the values of probability and adjust the strength as a hyperparameter. Furthermore, we will make our code publicly available and user-friendly.\n\n**In Table 8, applying only the color transformation leads to the best result. However, TADA cannot discover such an augmentation policy.**\n\nOur primary focus is to show the effectiveness of our method with a standard augmentation set, by adopting an augmentation set that is widely used in GANs literature. As mentioned in the limitations of our paper, the augmentation set we chose may not be optimal for diffusion models. Although not every augmentation has a positive effect, as shown in table 8, the augmentation overall gave benefits on the performance compared to the baseline. The optimal set can lead to additional improvement in performance. Thus, we believe that further research on finding the best set for diffusion models can be an interesting avenue.\n\n**The improvement may come from the use of other augmentations rather the proposed weighting scheme (comparison with fixed strength should be done)**\n\nWe agree on your point that the benefit may come from the use of various augmentation sets. However, when we apply these various augmentations with fixed strength, the problem of distribution shift arises severely, as shown in the mean face of \u2018naive\u2019 augmentation in Figure 3. That is the reason we did not put the naive augmentation (the same set of augmentations with fixed strength) into the performance comparison table. Still, to provide clear results, we will add the FID scores of naive augmentation to our paper. \n\n**TADA underperforms the AR baseline. should repeat the experiments multiple times**\n\nWe agree on your point that the experiment should be done multiple times with mean and std. We tested whether the results of our method vary with noticeable differences on one of the subsets where TADA underperforms AR. We measured the results three times and obtained a mean of 12.30 and a standard deviation of 0.09, which does not change any trend (lower than h-flip but higher than AR). Please note that AR used an augmentation probability of 0.12 or 0.15 depending on the dataset and we reported their best performance in comparison to our method. \n\nThroughout our experiments, our primary focus was not to assert TADA outperforms AR in all scenarios by manually setting parameters to surpass that of AR, but rather to emphasize its adaptability and ease of integration into various diffusion model settings while maintaining comparable performance to AR, with minimal tuning. We have tried to emphasize this point in our paper, but this point was not explicitly emphasized in our paper. We will revise our paper to make this clear."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700036154756,
                "cdate": 1700036154756,
                "tmdate": 1700037412994,
                "mdate": 1700037412994,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yDHwmnSNxm",
                "forum": "U6Mb3CRuj8",
                "replyto": "vURGF9NfgW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer L1D2 (2)"
                    },
                    "comment": {
                        "value": "**Many of the parameters are selected manually. It is unclear whether these pre-defined parameters need to be fine-tuned on any new datasets.**\n\nFor toy experiments, we reported the observations with 200 images by following the setting of P2 weighting [b]. During the design of this experiment, we confirmed that the number of chosen images does not affect the trend of this experiment. As mentioned in the paper, we mainly adopted augmentation sets from ADA [a], which is widely used in GANs literature. Therefore, we conducted our main experiments with a probability of 0.8 (that has been used as the maximum probability in ADA [a]) to demonstrate the feasibility of applying high augmentation probability with our proposed method. Since higher augmentation has been shown to be more effective in alleviating overfitting, we applied the highest probability possible. However, higher augmentation probability can lead to distribution shift, as reported in [a]. Therefore, being able to apply p=0.8 implies that our method allows usage of high probability and takes advantage of improved performance as well as alleviation of overfitting, without resulting in distribution shift. To further evaluate the effect of different probability parameters, we are currently performing probability sweeps with p=0.2, 0.4, and 0.6. We will report the results as soon as the training ends.\n\nOur proposed method has a primary focus on data augmentation techniques that do not cause distribution shifts. As pointed out, in Tables 8 and 9 of the appendix, performance can vary depending on the sensitive region. While the region boundaries that yield the best FID scores can be fluid, we consistently set the boundary to [-3, 0] across all datasets to prioritize robustness over maximizing performance. Indeed, performance could be further improved by fine-tuning these parameters. However, we emphasize that we avoided manually tuning the parameters to maximize performance and instead conducted our experiments using safe and moderate parameters.\n\n**Evaluations on only human faces are not enough**\n\nAs we mainly focused on a data augmentation technique that does not lead to distribution shift, we utilized aligned datasets (e.g., subsets of FFHQ) that allow explicit evaluation of distribution shift in a qualitative manner, following ADA [a]. As reported in Tables 8 and 9 of the appendix, although the region boundaries that result in the best FID scores can be fluid, we consistently fixed the boundary to [-3, 0] across all datasets to prioritize robustness over performance maximization.\n\nHowever, we agree that showing effectiveness on an unaligned dataset can imply better generalizability, we are currently training on CIFAR-10. In addition, if time allows, we will also report on the LSUN dataset.\nWe will make the results on these datasets available as soon as possible.\n\nOnce again, we deeply appreciate the time you dedicated to reviewing our paper. We will incorporate your constructive feedback into revised versions of our paper. \nWe respectfully ask if you consider raising the score. If you have any additional feedback that could further enhance our work, please share it with us at your convenience.\n\n[a] Karras, Tero, et al. \"Training generative adversarial networks with limited data.\" Advances in Neural Information Processing Systems 33 (2020): 12104-12114.\n\n[b] Choi, Jooyoung, et al. \"Perception prioritized training of diffusion models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[c] Karras, Tero, et al. \"Elucidating the design space of diffusion-based generative models.\" Advances in Neural Information Processing Systems 35 (2022): 26565-26577."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700036251860,
                "cdate": 1700036251860,
                "tmdate": 1700036251860,
                "mdate": 1700036251860,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Iy5YjW5X0Q",
                "forum": "U6Mb3CRuj8",
                "replyto": "vURGF9NfgW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1139/Reviewer_L1D2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1139/Reviewer_L1D2"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. The authors provided more justifications for the experiment setup, e.g., the comparison with naive augmentations and the choices of the hyperparameters. The authors also emphasized their focus on the adaptability and the ease of integration. While these are favorable features of an augmentation scheme, it is essential to show the effectiveness of the method empirically and theoretically. Empirically, the current method sometimes underperforms the AR baseline and is evaluated on a limited choice of datasets. Theoretically, I agree with Reviewer xARD that the work lacks theoretical background explaining why the strength of the transformation at different timesteps helps suppress distribution shifts. Therefore I would like to keep my current score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642359853,
                "cdate": 1700642359853,
                "tmdate": 1700642359853,
                "mdate": 1700642359853,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pmkHfkvVp5",
            "forum": "U6Mb3CRuj8",
            "replyto": "U6Mb3CRuj8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1139/Reviewer_xARD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1139/Reviewer_xARD"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies data augmentation in the training of diffusion models. Through two pre-experiments, the paper found that the level of impact of the data augmentation transformation on the output differs at each time step, especially in the interval called \"sensitive time-steps,\" where the inverse diffusion process of the diffusion model significantly alters the input image. Based on this finding, the paper proposes a method to adjust the strengths of the data augmentation at each time step. Experiments show that the proposed method partially outperforms the existing baseline on FFHQ and AFHQ-v2. On the other hand, there is a lack of discussion on how the change in transformation strengths at each time step affects learning data distribution. In addition, the experiments are not extensive in evaluation datasets and diffusion model formulations, raising questions about the generality of the proposed method."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper is the first to investigate the impact of data augmentation at each time step in training a diffusion model.\n+ Based on experimental observations, the paper proposes a heuristic data augmentation method that adjusts the strengths of the transformation at each time step."
                },
                "weaknesses": {
                    "value": "- While the observations by Toy Examples 1 and 2 provided by the paper are interesting, the paper fails to explain why the distribution shifts arise in the diffusion model from these results (-> **Q1** and **Q2**).\n- The theoretical foundation for the proposed method is weak. It is not clear why the proposed method solves the problems of distribution shift and overfitting (-> **Q3**).\n- The experiments are limited in terms of the variations of datasets and the diffusion models. This limits the impact of the paper's claim and is inconsistent with the claims made in the Introduction (-> **Q4** and **Q5**).\n- The experimental setup is unfair. The proposed method introduces color transformations that are not used in the baseline (AR), making it impossible to evaluate the proposed method with the experimental results. Also, in some of the experimental evaluations (Table 4 and Fig. 4), there is no comparison with the AR and the presentation is selective (-> **Q6** and **Q7**).\n- The evaluation of the distribution shift in Fig. 3 is qualitative and subjective, and not convincing.\n- The quality of the presentation with figures and tables is poor. Fig. 1(a), 4, and 5 cannot be accurately understood by a colorblind person like me."
                },
                "questions": {
                    "value": "**Q1.** Why do the results in Fig. 1 indicate a distribution shift at sensitive time steps? To begin with, the definition of \"distribution shift\" here is vague and difficult to understand intuitively. According to the protocol in Sec. 3.2, the experiment in Toy Example 1 was performed by applying a data augmentation $T$ to an input image $x_0$ to generate $T(x_0)$, generating $x_t$ by adding noise, and applying an inverse diffusion step. Finally, the LPIPS between $\\hat{x}_0$ and the original image $x_0$ are measured. This can evaluate the denoising performance of each step of the diffusion model, but it is unclear what LPIPS($x_0$, $\\hat{x}_0$) means. Since $T(x_0)$ differs from $x_0$, we do not expect the inverse diffusion process to reconstruct $x_t$ into $x_0$. Therefore, it is unlikely that it makes sense to compare them. Also, the paper should provide more explanations about what it means theoretically to observe this LPIPS gap across data augmentations.\n\n**Q2.** What does Toy Example 2 imply? The paper should specify what it wants to investigate by replacing $\\hat{\\epsilon}$. At least, I did not understand how the findings from this experiment are connected to the proposed method.\n\n**Q3.** What is the theoretical background of the proposed method and what is the expected semantic behavior as a data augmentation? Why does the strength of the transformation at each time step help to suppress distribution shifts and overfitting?\n\n**Q4.** Why does the paper experiment only with the facial datasets, i.e., FFHQ and AFHQ-v2? How about the performance of the proposed method on CIFAR-10 and ImageNet, which are used in EDM [a]? In the introduction, it is claimed that the proposed method is \"applicable across various diffusion model settings\", but since it is evaluated on fewer datasets than previous studies like EDM, the current results are not sufficient to claim the applicability.\n\n**Q5.** Is the proposed method effective for formulations other than ADM? For example, how about using latent diffusion [b]? The paper should show that $r_\\text{rough}$ and $r_\\text{fine}$ are robust across the model formulations.\n\n**Q6.** Why does the proposed method add transformations that are not used in AR? The paper states in section 4.1 that \"we incorporate color transformations, which were not used in EDM\". We could not compare the proposed method and AR without using shared transformations for the data augmentation because the effects of adding transformations cannot be separated from the effects of the proposed method.\n\n**Q7.** Why are the AR results not listed in Table 4 and Fig. 4? Similarly, na\u00efve data augmentation baselines that do not use the proposed method but use the same data augmentation transformations should be added to all experiments. The lack of these evaluations damages the significance of the paper.\n\n[a] Karras, Tero, et al. \"Elucidating the design space of diffusion-based generative models.\" Advances in Neural Information Processing Systems 35 (2022): 26565-26577.\n\n[b] Rombach, Robin, et al. \"High-resolution image synthesis with latent diffusion models.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698135117344,
            "cdate": 1698135117344,
            "tmdate": 1699636040278,
            "mdate": 1699636040278,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3uhpf6zh2w",
                "forum": "U6Mb3CRuj8",
                "replyto": "pmkHfkvVp5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xARD (1)"
                    },
                    "comment": {
                        "value": "We appreciate your constructive comments and suggestions, and they are really helpful for us to improve our paper. We will carefully incorporate them into our paper.\nFurthermore, we apologize for inconvenience due to some figures that were not carefully made with appropriate colors. We have revised the corresponding figures (Fig.1(a), 4 and 5).  Please find below our detailed responses to your comments:\n\n**Q1. Why do the results in Fig. 1 indicate a distribution shift at sensitive timesteps?**\n\nFor each model, we measured the perceptual similarity between the predicted $\\hat{x}_0$ and the original image $x_0$ at each timestep of the reverse process. We then analyzed the two models' difference in the perceptual similarity.\n\nThe baseline model, which was trained on the original dataset without data augmentation, is highly likely to produce in-distribution samples. Therefore, the perceptual similarity between the predicted $\\hat{x}_0$ and the original image $x_0$ from this model can be thought of as the lower limit at each timestep. We then compare this with the augmented model. Since this model was trained with an augmented dataset, the generated samples will encompass both the in-distribution and out-of-distribution samples. This can be confirmed when the perceptual similarity between $\\hat{x}_0$ and $x_0$ shows a noticeably higher perceptual difference if the predicted sample is an out-of-distribution sample.\n\nIn summary, the difference in perceptual similarity between the two models is an indication of the degree to which the augmented model is producing out-of-distribution samples. The larger the difference, the more likely it is that the augmented model is producing samples that are not representative of the original dataset. The results in Fig. 1 align with this explanation and suggest that data augmentation can cause diffusion models to produce out-of-distribution samples, especially at sensitive timesteps.\n\n**Q2. What does Toy Example 2 imply?**\n\nToy experiment 2 illustrates that exchanging the reverse process between the baseline and augmented models during the intermediate timesteps can modify the sampling trajectory, ultimately determining whether the generated samples end up within the augmented data distribution, even when they initially originated from the original data distribution, and vice versa. This highlights the influence of sensitive timestep on the sample distribution.\n\nThe observation that exchanging the trajectory at other timesteps had no impact on the generated data distribution further strengthens the result of toy experiment 1 that the middle timesteps are the most susceptible to data augmentation (Figures 8 and 9). This implies that the model's ultimate generation to adhere to the original or augmented data distribution is predominantly made during these middle timesteps."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700035084893,
                "cdate": 1700035084893,
                "tmdate": 1700035084893,
                "mdate": 1700035084893,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tNziabHx6H",
                "forum": "U6Mb3CRuj8",
                "replyto": "pmkHfkvVp5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xARD (2)"
                    },
                    "comment": {
                        "value": "**Q3. What is the theoretical background of the proposed method and what is the expected semantic behavior as a data augmentation? Why does the strength of the transformation at each time step help to suppress distribution shifts and overfitting?**\n\nWe acknowledge that our data augmentation technique does not have a solid theoretical basis, and we view this as a limitation of our paper. Meanwhile, our paper is built upon our empirical findings that distribution shift occurs due to data augmentation and it appears in specific timestep intervals. To address this, we devised a method that adjusts the augmentation strength based on the timesteps. The reason that we view that the weak strength helps to suppress distribution shifts is that, we keep x_t of each timestep to not deviate too far and let the model learn how to construct the actual contents of an image, similar to findings from P2 weighting [a]. Effect on overfitting is not exclusively attributed to our proposed method (suppressing the augmentation strength) but can also result from the use of data augmentation itself [b]. The point we wanted to convey through our effectiveness on overfitting is to show that although we weaken the augmentation strength along specific timesteps, we still maintain the effectiveness on alleviating overfitting.\n\nAs our method is built upon empirical evidence, we apologize that at this point, we cannot exactly provide answers with profound theoretical proofs. We will  endeavor to perform a deeper theoretical analysis on our future research. Still, we would like to emphasize the importance of this empirical study since we believe that empirical research can provide valuable insights and practical benefits, regardless of formal theoretical understanding. Indeed, a lot of theoretical studies are inspired and informed by empirical observations. One of the widely-used studies is CutMix [c], despite lacking rigorous theoretical investigations, has been utilized by many practitioners and following studies. CutMix has provided motivation for many theoretical studies such as [d, e]. We hope that empirical study is regarded as a valuable and essential form of research.\n\n[a] Choi, Jooyoung, et al. \"Perception prioritized training of diffusion models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[b] Karras, Tero, et al. \"Training generative adversarial networks with limited data.\" Advances in Neural Information Processing Systems 33 (2020): 12104-12114.\n\n[c] Yun, Sangdoo, et al. \"CutMix: Regularization strategy to train strong classifiers with localizable features.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.\n\n[d] Park, Chanwoo, Sangdoo Yun, and Sanghyuk Chun. \"A unified analysis of mixed sample data augmentation: A loss function perspective.\" Advances in Neural Information Processing Systems 35 (2022): 35504-35518.\n\n[e] Oh, Junsoo, and Chulhee Yun. \"Provable benefit of mixup for finding optimal decision boundaries.\" International Conference on Machine Learning. PMLR, 2023.\n\n**Q4. Why does the paper experiment only with the facial datasets, i.e., FFHQ and AFHQ-v2?**\n\nAs we mainly focused on a data augmentation technique that does not lead to a distribution shift, we utilized aligned datasets (e.g., subsets of FFHQ) that allow explicit evaluation of distribution shift in a qualitative manner, following ADA [b]. As reported in Tables 8 and 9 of the appendix, although the region boundaries that result in the best FID scores can be fluid, we consistently fixed the boundary to [-3, 0] across all datasets to prioritize robustness over performance maximization.\n\nHowever, we agree that showing effectiveness on an unaligned dataset can imply better generalizability, we are currently training on CIFAR-10. In addition, if time allows, we will also report on the LSUN dataset.\nWe will make the results on these datasets available as soon as possible."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700035185620,
                "cdate": 1700035185620,
                "tmdate": 1700035185620,
                "mdate": 1700035185620,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "igXyJJNOua",
                "forum": "U6Mb3CRuj8",
                "replyto": "pmkHfkvVp5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xARD (3)"
                    },
                    "comment": {
                        "value": "**Q5. Is the proposed method effective for formulations other than ADM? For example, how about using latent diffusion?**\n\nA recent study [d] found out that latent diffusion has different timestep-wise task affinity compared to ADM (that we mainly used). Therefore, to apply our TADA to latent diffusion models, the boundary regions (r_rough and r_fine) should be adjusted. \n\nHowever, the majority of diffusion models that are currently used and studied are based on the same objective and architecture with variations on hyperparameters [a, b, c]. Therefore, we believe that our method can be seamlessly integrated into various diffusion models. To confirm this, we are currently training under VP [b]  and will provide the corresponding results.\n\n[a] Song, Yang, et al. \"Score-based generative modeling through stochastic differential equations.\" arXiv preprint arXiv:2011.13456 (2020).\n\n[b] Karras, Tero, et al. \"Elucidating the design space of diffusion-based generative models.\" Advances in Neural Information Processing Systems 35 (2022): 26565-26577.\n\n[c] Kingma, Diederik, et al. \"Variational diffusion models.\" Advances in neural information processing systems 34 (2021): 21696-21707.\n\n**Q6. Why does the proposed method add transformations that are not used in AR?**\n\nWe agree on your point that we employed color transformations that were not used in AR. This was because we tried to perform a fair comparison by presenting the best performance of AR methods, adhering to the specific augmentations that were found to be effective for EDM (color transformations were not used as authors of EDM mentioned that they found out color corruptions were rather harmful (Section F.2 of EDM [a])). \n\nStill, we agree on your point that the exact augmentation sets used for AR and TADA differ. Therefore, we are currently training TADA on the EDM augmentation set and training EDM on the augmentations that we used for TADA. We will report the results as soon as we obtain the corresponding results.\n\n[a] Karras, Tero, et al. \"Elucidating the design space of diffusion-based generative models.\" Advances in Neural Information Processing Systems 35 (2022): 26565-26577.\n\n**Q7. Why are the AR results not listed in Table 4 and Fig. 4?**\n\nThe purpose of Table 4 was to demonstrate the generalizability of TADA to various settings. Therefore, we performed a simple comparison with the most frequently used data augmentation for diffusion models (h-flip). Similarly, Figure 4 aimed to illustrate the effectiveness of TADA in addressing overfitting (albeit potentially trivial due to the well-known ability of data augmentations to mitigate overfitting), rather than directly comparing it to other data augmentation methods.\n\nThroughout our experiments, our primary focus was not to assert that TADA outperforms AR in all scenarios, but rather to emphasize its adaptability and ease of integration into various diffusion model settings, while maintaining comparable performance to AR. We have tried to emphasize this point in our paper, but this point was not explicitly emphasized in our paper. We will revise our paper to make this clear.\n\nOnce again, we deeply appreciate the time you dedicated to reviewing our paper. We will incorporate your constructive feedback into revised versions of our paper. We respectfully ask if you consider raising the score. If you have any additional feedback that could further enhance our work, please share it with us at your convenience."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700035330895,
                "cdate": 1700035330895,
                "tmdate": 1700035345650,
                "mdate": 1700035345650,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NzEX1ay8oC",
                "forum": "U6Mb3CRuj8",
                "replyto": "pmkHfkvVp5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1139/Reviewer_xARD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1139/Reviewer_xARD"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response.\n\n**For the response to Q1**\n\n> In summary, the difference in perceptual similarity between the two models is an indication of the degree to which the augmented model is producing out-of-distribution samples.\n\nI'm less convinced by this interpretation because the meaning of the noise per time step for two diffusion models trained on different datasets is not necessarily the same. But, I could understand the argument of the paper with this reply. Thank you.\n\n**For the response to Q3**\n\n> Still, we would like to emphasize the importance of this empirical study since we believe that empirical research can provide valuable insights and practical benefits, regardless of formal theoretical understanding. \n\nMy comment was not meant to say that there must always be a formal theoretical discussion. I agree that empirical research based on intuitive insights has practical importance and can motivate theoretical research. However, even with such an empirical approach, I believe that the presentation of a hypothesis and its empirical evidence is essential to a scientific paper. In that sense, I acknowledge that the paper did indeed present the hypothesis that there is a time step that is sensitive to domain shifts and attempted to demonstrate the validity of this hypothesis through empirical experiments. However, I still believe that the evidence and the influence of the hypothesis are not adequately discussed in the current experimental evaluation.\n\n**For the response to Q4 and Q6**\n\nI look forward to additional experimental results. Thank you for your efforts."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646272950,
                "cdate": 1700646272950,
                "tmdate": 1700646393722,
                "mdate": 1700646393722,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g5lwpY6sWY",
            "forum": "U6Mb3CRuj8",
            "replyto": "U6Mb3CRuj8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1139/Reviewer_zkiV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1139/Reviewer_zkiV"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new data augmentation technique for the generative models using diffusion models. Data augmentation of generative models causes a distribution shift, which leads to the generation of unintended out-of-distribution samples. To avoid the distribution shift, the proposed method uses augmented data in early and last time steps in the reverse process. To justify this approach, this paper empirically reveals that only the middle time steps, which are called sensitive time steps, cause a distribution shift. \nExperiments demonstrate that the proposed augmentation method outperforms the augmentation regularization method and 50% horizontal flip."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper is well-written, and experiments are well-designed to present the claim of this paper.\n- The empirical analyses about sensitive time steps are interesting and important. I think the existence of sensitive time steps will have an impact on future data enhancement techniques for diffusion models. I am not an expert in diffusion models, and it might not be very surprising that the process in the middle time step is sensitive. Even so, two toy experiments carefully confirm this phenomenon. \n- This paper evaluates the proposed method from various perspectives.\nFor example, this paper evaluates the proposed method on different scales of the model, with/without transfer learning, and on high-resolution data. An ablation study is also contained, and I think the proposed method is sufficiently evaluated."
                },
                "weaknesses": {
                    "value": "- It is difficult for the reader to reproduce the experiments in this paper alone. The proposed method is not very clearly presented because this paper does not contain pseudo codes or an explanation of the procedure of the whole proposed method. The specific data augmentation methods used in the proposed method are left to [Karras et al. (2020)] and not explained.\nThus, the paper by itself does not tell the readers how the strength of data augmentation $w_t$ works.\n- This paper seems to evaluate the proposed method on only a limited data setting. Since data augmentation for discriminative models tends to improve performance when using full datasets besides limited data size settings, I would like to see the performance of the proposed method when a dataset has a sufficiently large size. If the proposed method does not improve the performance in this situation, it is the limitation of the proposed method."
                },
                "questions": {
                    "value": "- Does the proposed method work well if a dataset has a sufficiently large size?\n- Where can the readers understand the whole procedure of the proposed method? I think the readers have to carefully read several points e.g., Augmentation pipeline in Section 4.1, besides Section 3 to understand the proposed method, which is a bit burdensome."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698323737755,
            "cdate": 1698323737755,
            "tmdate": 1699636040166,
            "mdate": 1699636040166,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dpLco6Iedg",
                "forum": "U6Mb3CRuj8",
                "replyto": "g5lwpY6sWY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zkiV"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your time in reading the paper, and the strengths that you found in our work. Please find below our detailed responses to your comments: \n\n**Does the proposed method work well if a dataset has a sufficiently large size?**\n\nAs data augmentation is primarily employed in scenarios with limited data, our focus has been on evaluating its effectiveness in this context. Consistent with prior studies that demonstrate the benefits of data augmentation for small datasets, we presented results for datasets up to a size of 30k, following DiffAug [a] and ADA [b].\nHowever, we acknowledge the value of evaluating performance on the full FFHQ dataset to assess effectiveness and generalizability even under a large dataset. Accordingly, we are currently training TADA on the full FFHQ dataset and will provide the results as soon as the training is complete.\n\n**Where can the readers understand the whole procedure of the proposed method?**\n\nWe sincerely apologize for any confusion regarding the proposed method\u2019s procedure. To enhance clarity and reproducibility, we will provide more detailed information about the augmentation policy. Additionally, we have added the pseudo-code in the Appendix D.3 section to further explain our method\u2019s implementation.\n\nOnce again, we deeply appreciate the time you dedicated to reviewing our paper. We will incorporate your constructive feedback into revised versions of our paper. \n\n[a] Zhao, Shengyu, et al. \"Differentiable augmentation for data-efficient gan training.\" Advances in neural information processing systems 33 (2020): 7559-7570.\n\n[b] Karras, Tero, et al. \"Training generative adversarial networks with limited data.\" Advances in neural information processing systems 33 (2020): 12104-12114."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700034788639,
                "cdate": 1700034788639,
                "tmdate": 1700034788639,
                "mdate": 1700034788639,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HrWE7fdVEP",
                "forum": "U6Mb3CRuj8",
                "replyto": "dpLco6Iedg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1139/Reviewer_zkiV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1139/Reviewer_zkiV"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the feedback"
                    },
                    "comment": {
                        "value": "Thank you for the feedback. I think the claims of the paper and the proposed method are important results. However, I am not very familiar with the diffusion models and do not know the impact of the experimental results. Therefore, I keep my score and wait for discussion with other reviewers.\n\n> As data augmentation is primarily employed in scenarios with limited data, our focus has been on evaluating its effectiveness in this context. \n\nI understand the motivation, and the aim of my question was to clarify the limitation. I look forward to seeing the results of the full data.\n\n> Additionally, we have added the pseudo-code in the Appendix D.3 section to further explain our method\u2019s implementation.\n\nThe code in Appendix D.3 is unclear, and it is difficult to understand the role of each line; it relies on python and numpy code, and the relationship between the transforms and w_t is unclear. The detail explanation about transforms would be more reader-friendly even though they seen to be explained in ADA (Karras et al., 2020)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632628010,
                "cdate": 1700632628010,
                "tmdate": 1700632628010,
                "mdate": 1700632628010,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9GSN3QQlKF",
            "forum": "U6Mb3CRuj8",
            "replyto": "U6Mb3CRuj8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1139/Reviewer_o4Pj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1139/Reviewer_o4Pj"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an interesting and novel approach to data augmentation in diffusion models by introducing a timestep-aware augmentation strategy. The authors identify a significant issue in the standard application of data augmentation to diffusion models, specifically the distribution shifts that occur and are tied to certain timesteps within the generative process. The proposed TADA approach is a logical and promising solution to this issue."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originality:** The paper addresses a gap in the literature by focusing on diffusion models where timestep-related distribution shifts during data augmentation have been understudied.\n\n**Relevance:** Given the increasing importance of diffusion models in various generative tasks, the paper's focus is timely and relevant to the community.\n\n**Technical Depth:** The paper appears to offer a comprehensive study and a well-thought-out augmentation strategy that is sensitive to the unique requirements of diffusion models.\n\n**Empirical Evidence:** The authors provide experimental results that suggest their method is robust across different diffusion model settings and datasets, which is commendable."
                },
                "weaknesses": {
                    "value": "**Clarity and Depth of Experiments:** While the experiments are diverse, the review is based on a limited preview, and a more thorough assessment of the experimental design and results is necessary.\n\n**Comparison with Baselines:** The paper would benefit from a more detailed comparison with existing data augmentation methods, particularly in how the results translate to qualitative improvements in the generated samples.\n\n**Impact and Significance:** The practical impact of the proposed method, while promising, is not fully established. The paper could strengthen its argument by demonstrating clear use cases where its method significantly outperforms existing techniques."
                },
                "questions": {
                    "value": "**Methodology Clarification:**\n\nCould you provide more detail on how the augmentation strength is adjusted during different timesteps? Is there a theoretical framework or empirical evidence that supports the chosen method?\n\n**Experimental Design:**\n\nIn your experiments, did you explore the impact of the timestep-aware augmentation on the convergence speed and stability of the training process for diffusion models?\nHow does the proposed augmentation method affect the computational resources required for training and inference compared to traditional augmentation techniques?\n\n**Baseline Comparisons:**\n\nHow does TADA compare with other state-of-the-art data augmentation methods in terms of qualitative and quantitative results?\nCould you provide insights or metrics on how the proposed method improves the robustness and generalization of diffusion models in scenarios with limited data?\n\n**Limitations and Challenges:**\n\nWhat are the potential limitations or drawbacks of your proposed method?\nAre there specific types of diffusion models or data domains where TADA might be less effective?\n\n**Scalability and Generalization:**\n\nHow scalable is the TADA method with respect to model size and dataset complexity?\nHave you tested the method on out-of-distribution samples or on datasets with significant class imbalances?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1139/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699432775464,
            "cdate": 1699432775464,
            "tmdate": 1699636040091,
            "mdate": 1699636040091,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fc9fxFdh3b",
                "forum": "U6Mb3CRuj8",
                "replyto": "9GSN3QQlKF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer o4Pj (1)"
                    },
                    "comment": {
                        "value": "Thank you for your time and helpful feedback! We are really glad that you found our paper intriguing, comprehensive, and original. Please find below our detailed responses to your comments:\n\n**Methodology Clarification**\n\nAs illustrated in Figure 2, augmentation strength (w_t) can be expressed as a quadratic curve across all timesteps. Specifically, since SNR can be determined as a function of timestep, we put the SNR value into Equation 3 to compute the augmentation strength w_t. Consequently, we adjust the augmentation strength by multiplying w_t to the maximum augmentation strength applied to training data. We newly provided an additional pseudo code describing the whole procedure in Appendix D.3.\n\nWhile the strength of the data augmentation can be kept constant across all timesteps, toy experiments suggest that diffusion models trained with augmented training data become more susceptible to producing unintended transformed samples during the intermediate timesteps. Based on this empirical evidence, we designed an augmentation strength schedule that suppresses the augmentation during the intermediate timesteps.\nThe rationale behind our choice of a quadratic function is twofold:\n\n* Simplicity: The quadratic function provides a straightforward and intuitive approach to controlling the augmentation strength across timesteps. This simplicity makes it easy for users to adjust the augmentation intensity of a sensitive region without the need for complex hyperparameter tuning.\n\n* Smooth Transition: The quadratic function facilitates a seamless transition between strong and weak augmentation depending on the timestep axis. Toy experiments have demonstrated that data augmentation applied to sensitive timestep can significantly impact the generated samples. However, precisely identifying these sensitive regions is challenging. Therefore, we opted for a smooth function that ensures weak augmentation around sensitive timesteps while still enabling data augmentation to be applied to a substantial portion of the timesteps during training.\n\nThis allows us to attain a balance between simplicity and effectiveness, making it a reasonable choice for the proposed method. \n\nWe apologize for any ambiguity in the paper. We will revise the manuscript to improve the clarity of the methodology.\n\n**Experimental Design**\n\nOur proposed framework exhibits improved convergence speed and stability compared to the baseline (h-flip) and alleviates overfitting. As shown in Figure 4, FID scores computed every 10k training iterations indicate that our framework converges slightly faster than the baseline models. Additionally, our framework effectively alleviates overfitting, as evidenced by the lower FID scores at later stages of training.\n\nEmploying our framework does not introduce any additional computational overhead during the training stage. Modern deep learning frameworks, such as PyTorch, execute data processing operations on the CPU, while the actual training process (forward and backward propagation) is performed on the GPU. Since our modifications are focused on the data processing stage, which is typically CPU-bound, they do not impact the training time."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700034594182,
                "cdate": 1700034594182,
                "tmdate": 1700034594182,
                "mdate": 1700034594182,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eGzfvyewgk",
                "forum": "U6Mb3CRuj8",
                "replyto": "9GSN3QQlKF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1139/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1139/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer o4Pj (2)"
                    },
                    "comment": {
                        "value": "**Baseline Comparisons**\n\nWhile extensive research has been conducted on data augmentation for generative adversarial networks (GANs), limited research has explored this for auto-regressive and diffusion models. Specifically, no prior studies have investigated data augmentation for diffusion models, with only a rudimentary method introduced in EDM, leaving no other model for comparison. We believe this gap in research presents a significant opportunity for further exploration and advancement of data augmentation techniques for diffusion models.\n\nThe reason that our method improves the robustness and generalization of diffusion models is not solely attributed to our proposed method but can also result from the use of data augmentation itself. Our method's primary impact lies in alleviating distribution shifts by adaptively adjusting the augmentation strength during specific timesteps. This strategy involves suppressing augmentation steps in regions where the diffusion model is susceptible to distribution shift, effectively mitigating this issue. Simultaneously, it allows for the application of diverse augmentation techniques with high augmentation probability, leveraging the benefits of data augmentation to promote better generalizability.\n\n**Limitations and Challenges**\n\nAs we mentioned in the discussion section of the paper, limitation lies in the lack of a profound theoretical basis. The majority of diffusion models that are currently used and studied are based on the same objective and architecture with variations on hyperparameters [a, b, c]. Therefore, we believe TADA can be seamlessly integrated into various diffusion models. To confirm this, we are currently training under VP [a]  and will provide the results under this setting.\n\nMeanwhile, a recent study [d] found that latent diffusion has different timestep-wise task affinity compared to ADM (that we mainly used). Therefore, to apply our TADA to latent diffusion models, the boundary regions should be adjusted.   \n\nFurthermore, we believe that our method is applicable across wide datasets with limited data settings. However, as we mentioned in the discussion of our paper, it can be less effective when an abundant amount of data is available.\n\n**Scalability and Generalization**\n\nWe varied the model size (i.e., the number of parameters), and found that our method showed noticeable performance improvement in smaller model sizes (Table 4). This implies that TADA offers huge benefits when large computational resources and abundant training data are not available. \nIn addition to the results on various sizes of the FFHQ dataset, we demonstrated our scalability on the AFHQ dataset. To further show the scalability of our method, we are currently training on CIFAR10. We will report the results as soon as the training is completed. \nWe have not yet tested our method on out-of-distribution samples or class imbalance datasets. We appreciate your feedback on this point and hope that we can incorporate this aspect into our further research. \n\nOnce again, we deeply appreciate the time you dedicated to reviewing our paper. We will incorporate your constructive feedback into revised versions of our paper. \n\n[a] Song, Yang, et al. \"Score-based generative modeling through stochastic differential equations.\" arXiv preprint arXiv:2011.13456 (2020).\n\n[b] Karras, Tero, et al. \"Elucidating the design space of diffusion-based generative models.\" Advances in Neural Information Processing Systems 35 (2022): 26565-26577.\n\n[c] Kingma, Diederik, et al. \"Variational diffusion models.\" Advances in neural information processing systems 34 (2021): 21696-21707.\n\n[d] Go, Hyojun, et al. \"Addressing Negative Transfer in Diffusion Models.\" arXiv preprint arXiv:2306.00354 (2023)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1139/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700034701724,
                "cdate": 1700034701724,
                "tmdate": 1700034701724,
                "mdate": 1700034701724,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]