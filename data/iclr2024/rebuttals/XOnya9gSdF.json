[
    {
        "title": "Consistent algorithms for multi-label classification with macro-at-$k$ metrics"
    },
    {
        "review": {
            "id": "cjj3jIPiDb",
            "forum": "XOnya9gSdF",
            "replyto": "XOnya9gSdF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7700/Reviewer_s4jL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7700/Reviewer_s4jL"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript studies the problem of multi-label classification with budgeted (top-k) prediction metrics. The problem is very well established by now, and of much practical relevance. The paper is overall well motivated, well-written, and the contributions are par for the venue.\n\nI see two key contributions: 1) formalizing the empirical uitility maximation notion of '@k' metrics as that of optimizing over certain classes of confusion matrices, which decomposes over individual labels, but the optimization problem itself doesn't decompose easily because of the constraint that any classifier in the hypothesis space must output exactly k labels per instance; 2) deriving a neat form for the Bayes optimal, which yields interpretable closed form solutions for well-known metrics like recall and balanced accuracy; the most-general closed form solution is not very useful because it depends on the optimal values --- this observation by itself is not novel, as pretty much every paper that talks about non-decomposable losses for binary/multi-label classification problems (several of which are clearly cited in the paper) have developed similar results.  \n\nGiven the form of the Bayes optimal, Frank-Wolfe based algorithm for estimating the classifiers can be written down, following the work of Narasimhan et al. (2015). The empirical results demonstrate the effectiveness of the algorithm, and the consistency between optimal Bayes rule and the algorithmic convergence for certain measures, compared to several natural baselines.\n\nOverall, I like the work and I am inclined to accept. I've some minor concerns which I outline under 'weaknesses' section. It would be good to hear from the authors on these questions in their rebuttal.\n\n-- Post rebuttal --\nMost of my concerns are addressed. I'm more positive about the paper now."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Clearly formulated problem -- it's non-trivial to set up the problem, and I really like the simplicity of the formulation in terms of confusion matrices/tensors, and the authors have done a great job of presenting notation-heavy material with careful development of ideas. \n2. Technical rigor -- the theoretical results are well established, and the supporting key lemmas are included in the main paper, which is very helpful."
                },
                "weaknesses": {
                    "value": "The intro neatly positions the problem in the context of several closely-related work in this space, but I felt those connections didn't surface as much as I'd have loved to see in the main paper. \n\nFor instance, the first main result in Theorem 4.1. Just looking at the constants a_j and b_j, one can see, not surprisingly, that these are the same constants one would see for weighted binary classification problems, see Lemma 2 of https://www.jmlr.org/papers/volume18/15-226/15-226.pdf (of course, there's no top-k notion here in binary classification). It would be good to draw these connections, position the main results in the context of known results, and tease apart new observations and insights.\n\nSimilarly, with respect to, Theorem 4.4, it would be good to see some form of the result, say special cases, that can be dotted lined to known results for multi-label problems."
                },
                "questions": {
                    "value": "It would be good to address the points raised in the weaknesses section. And a note on the challenges or novelty in the proofs for Thm 4.4 over and above known work.\n\nIt's nice to see the consistency between Macro-R_prior and Macro-R_FW in the experiments for the recall metric. Did the authors also experiment with balanced accuracy in the experiments, where the closed form solution is also easy to compute?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "A lot of the formulation developed in the paper reminded me of this result I came across a few years ago https://arxiv.org/pdf/1908.09057.pdf. It turns out there is a lot in common (Corollary 3 and 4 in this arXiv I believe are analogous to Theorems 4.1 and 4.4), but the current submission also some algorithmic contributions. Perhaps the authors are overlapping, but I just wanted to make this note here."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7700/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7700/Reviewer_s4jL",
                        "ICLR.cc/2024/Conference/Submission7700/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7700/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698378461638,
            "cdate": 1698378461638,
            "tmdate": 1700642039192,
            "mdate": 1700642039192,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Nn5aaGhT0j",
                "forum": "XOnya9gSdF",
                "replyto": "cjj3jIPiDb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7700/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7700/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer s4jL (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the insightful review. We appreciate the hard work. Below, we address the main questions and concerns:\n\n---\n\n> For instance, the first main result in Theorem 4.1. Just looking at the constants a_j and b_j, one can see, not surprisingly, that these are the same constants one would see for weighted binary classification problems, see Lemma 2 of https://www.jmlr.org/papers/volume18/15-226/15-226.pdf (of course, there's no top-k notion here in binary classification). It would be good to draw these connections, position the main results in the context of known results, and tease apart new observations and insights.\n\nThe cost-sensitive binary classification is indeed a special case of our linear metric setup with a single label, which is why these constants turn out to be the same. In our work, we choose top-$k$ labels with respect to a linear transformation of marginals, while in binary classification a single label is selected whenever that linear transformation of class conditional probability is non-negative. Having said that, we agree that we should make a better connection between the results presented in our paper and known results.\n\n---\n\n> Similarly, with respect to, Theorem 4.4, it would be good to see some form of the result, say special cases, that can be dotted lined to known results for multi-label problems.\n\nThe easiest special case are linear/weighted measures, i.e., $\\Psi(\\mathbf{C}) = \\mathbf{L} \\cdot \\mathbf{C} + d$.  In that case, $\\nabla \\Psi = \\mathbf{L}$, so the statement reduces to the trivial claim that any $\\mathbf{C}$ maximizing $\\mathbf{L} \\cdot \\mathbf{C}$ also maximizes $\\mathbf{L} \\cdot \\mathbf{C} + d$.\n\nFor a more interesting case, consider macro-recall: Here, \n$$\n\\Psi(\\mathbf{C}) = \\sum_j C^j_{11} / (C^j_{11} + C^j_{10}),\n$$ \nand \n$$\n\\nabla_{\\boldsymbol{C}^j} \\Psi = \\begin{pmatrix}\n0 & 0 \\\\\\\\\nC^j_{11} / (C^j_{11} + C^j_{10})^2 & -C^j_{10} / (C^j_{11} + C^j_{10})^2\n\\end{pmatrix}.\n$$\n\nThus, the theorem claims that one needs to optimize the linear measure\n$$\\sum_j (C^{\\star j}\\_{11} + C^{\\star j}\\_{10})^{-2} (C^{\\star j}\\_{11} C^j\\_{10}\n-C^{\\star j}\\_{10} C^j\\_{11}).\n$$ \n\nNote that, in fact, for any given problem, $C^j_{10} + C^j_{11} = \\pi_j$ is a constant, so we can simplify the expression:\n$$\n\\sum_j \\pi_j^{-2} C^{\\star j}\\_{11} (\\pi_j - C^j\\_{11}) - (\\pi_j - C^{\\star j}\\_{11}) C^j\\_{11} = \\sum_j \\pi_j^{-1} (C^{\\star j}\\_{11} - C^j\\_{11}).\n$$ \nAs macro-recall is formulated as a loss function, our goal is to minimize the expression above. As $\\pi_j^{-1} C^{\\star j}\\_{11}$ is a constant, the optimization objective becomes\n$$\n\\min \\sum_j - \\pi_j^{-1} C^j\\_{11} = \\max \\sum_j \\pi_j^{-1} C^j\\_{11}.\n$$ \n\nThus, this recovers the solution for optimizing macro-recall. \n\n---\n\n> It's nice to see the consistency between Macro-R_prior and Macro-R_FW in the experiments for the recall metric. Did the authors also experiment with balanced accuracy in the experiments, where the closed form solution is also easy to compute?\n\nWe have performed such an experiment, and we can confirm that the proposed Frank-Wolfe algorithm (Macro-BA$\\_{FW}$) also recovers the optimal analytical solution for Balanced Accuracy obtained directly from label priors (Macro-BA$_{PRIOR}$). We present the results below, showing that both approaches get almost the same performance.\n\n**Mediamill:**\n| method                         | mBA@3      | mBA@5      | mBA@10     |\n|:-------------------------------|-----------:|-----------:|-----------:|\n| Macro-BA$_{PRIOR}$             | *58.51*    | **60.93**  | **65.03**  |\n| Macro-BA$_{FW}$                | **58.51**  | *60.93*    | *65.03*    |\n\n**Flickr:**\n| method                         | mBA@3      | mBA@5      | mBA@10     |\n|:-------------------------------|-----------:|-----------:|-----------:|\n| Macro-BA$_{PRIOR}$             | **72.02**  | **75.70**  | **79.86**  |\n| Macro-BA$_{FW}$                |*71.93*     | *75.63*    | *79.73*    |\n\n**RCV1x:**\n| method                         | mBA@3      | mBA@5      | mBA@10     |\n|:-------------------------------|-----------:|-----------:|-----------:|\n| Macro-BA$_{PRIOR}$             | **58.58**  | *61.54*    | *66.02*    |\n| Macro-BA$_{FW}$                | *58.58*    | **61.55**  | **66.03**  |\n\n**AmazonCat:**\n| method                         | mBA@3      | mBA@5      | mBA@10     |\n|:-------------------------------|-----------:|-----------:|-----------:|\n| Macro-BA$_{PRIOR}$             | *71.62*    | **74.04**  | **76.28**  |\n| Macro-BA$_{FW}$                | **71.63**  | *74.01*    | *76.26*    |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7700/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258481105,
                "cdate": 1700258481105,
                "tmdate": 1700259355402,
                "mdate": 1700259355402,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "37hGFAZ4Gt",
                "forum": "XOnya9gSdF",
                "replyto": "cjj3jIPiDb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7700/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7700/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer s4jL (2/2)"
                    },
                    "comment": {
                        "value": "---\n\n### **Regarding ethical concerns (comparison to [Wang et al](https://arxiv.org/pdf/1908.09057.pdf))**\n\nWe thank the reviewer for bringing this work to our attention. We were not aware of it (if not wrong, it was not published in a peer-reviewed venue or journal). After reading it, we see substantial differences from our submission. \n\nThe reviewer is correct that this paper does look similar to ours. However, this is not surprising; both papers are different extensions of earlier works, such as [Narasimhan et al](https://proceedings.mlr.press/v37/narasimhanb15.html). We tried to clearly indicate in our submission that it is based on this line of work. Below we discuss the differences between our paper and [Wang et al](https://arxiv.org/pdf/1908.09057.pdf).\n\n\nThat paper considers _multi-output_ classifiers with $K$ classes in each output, and makes $M$ such outputs, which is conceptually quite different from at-$k$ prediction considered in our paper. This can already be seen in the respective definitions of the confusion tensor: In our case, it is an $m \\times 2 \\times 2$ object, and in the other paper, it is an $M \\times K \\times K$ object. \n\nIn case of multi-output classification, each of the outputs chooses a class _independently_ of the other. Of course, an $m$-label multilabel problem can be considered as a multi-output problem, with $K=2$ and $M=m$. In that case, though, the setting of Wang et al. does not contain the at-$k$ constraint, which is what makes this problem really challenging.\n\nOn a technical side, our formulation of the theoretical results and our proof techniques are different (as a side note, their proof of the compactness of the feasible confusion set is incorrect as it falsely assumes the space of bounded functions to be compact).\n\nOn the basis of the above arguments, we hope that we have assured the reviewer that we do not plagiarize any other work. Nevertheless, we admit that paper of Weng et al. is related to our submission and we should cite it."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7700/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258596117,
                "cdate": 1700258596117,
                "tmdate": 1700259408988,
                "mdate": 1700259408988,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ApPzSJXir2",
                "forum": "XOnya9gSdF",
                "replyto": "Nn5aaGhT0j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7700/Reviewer_s4jL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7700/Reviewer_s4jL"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the responses"
                    },
                    "comment": {
                        "value": "Thank you, it's good to see that the method is competitive to optimal strategy in case of balanced error as well. It would be great if you could add some of these observations to the main text."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7700/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642134663,
                "cdate": 1700642134663,
                "tmdate": 1700642134663,
                "mdate": 1700642134663,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SYyi2ehk03",
                "forum": "XOnya9gSdF",
                "replyto": "37hGFAZ4Gt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7700/Reviewer_s4jL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7700/Reviewer_s4jL"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thank you, I see that the notation and set up do draw commonly from Narasimhan et al. I wanted to register my concern because the overlap seemed particularly striking. Thanks for calling out the differences -- I do realize that the submission makes enough technical advances and is sufficiently different compared to the arXiv work.\n\nI'm raising the score to clear accept."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7700/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642279658,
                "cdate": 1700642279658,
                "tmdate": 1700642279658,
                "mdate": 1700642279658,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GYPSsyfHOp",
            "forum": "XOnya9gSdF",
            "replyto": "XOnya9gSdF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7700/Reviewer_gGLD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7700/Reviewer_gGLD"
            ],
            "content": {
                "summary": {
                    "value": "In the paper \"Consistent algorithms for multi-label classification with macro-at-k metrics\", the authors propose a framework of consistent multi-label learning algorithms for targeting macro-averaged metrics that are however budgeted for a k-subset of labels. The presented approach is based on the Frank-Wolfe algorithm and represents a principled extension towards multi-label classification with corresponding theoretical guarantees. An empirical study confirms the consistency with the targeted metric."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Theoretical sound and underpinned approach for targeting macro-averaged at k metrics for multi-label classification.\n- An empirical study confirms the theoretical findings for four datasets and various metrics.\n- The overall presentation and language of the paper are very good."
                },
                "weaknesses": {
                    "value": "- The empirical evaluation is relatively limited as only four datasets are considered. However, the main focus should remain on the theoretical findings here.\n- The theoretical results could be accompanied by an intuition to ease understanding of the results.\n- A comparison to something like binary relevance learning treating each label independently for demonstrating also empirically that these measures cannot be sufficiently tackled by such an approach would be desirable.\n\nminor:\np. 4 \"define in Table 1\"\np. 7 \"tensor measure measure\""
                },
                "questions": {
                    "value": "- In https://doi.org/10.1007/s10994-021-06107-2 an @k metric interpolating between  Hamming and subset 0/1 loss is presented. To what extent would this relate to the considered @k-metrics in this paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7700/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758018435,
            "cdate": 1698758018435,
            "tmdate": 1699636938001,
            "mdate": 1699636938001,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0TQDKJK9cP",
                "forum": "XOnya9gSdF",
                "replyto": "GYPSsyfHOp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7700/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7700/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gGLD"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the review. We appreciate the hard work. Below, we address the question and concern:\n\n---\n\n> The theoretical results could be accompanied by an intuition to ease understanding of the results.\n\nWe thank the reviewer for the suggestion; we will try to incorporate more intuitions in the next revision. If the reviewer has specific suggestions regarding which results would benefit the most from them, we would be happy to focus on these parts.\n\n---\n\n>A comparison to something like binary relevance learning treating each label independently for demonstrating also empirically that these measures cannot be sufficiently tackled by such an approach would be desirable.\n\nFor metrics for which macro averages can be written in a linear form (see page 5), the optimal solution can also be obtained by binary relevance-like approaches. In the experiments, we explicitly include methods (e.g., Top-K, Macro-R$_{\\text{PRIOR}}$) of this type. In the general case, binary relevance methods do not lead to the optimal solution, as discussed in Appendix E.\n\n---\n\n> In https://doi.org/10.1007/s10994-021-06107-2 an @k metric interpolating between Hamming and subset 0/1 loss is presented. To what extent would this relate to the considered @k-metrics in this paper?\n\nWhile the Hamming loss is a part of the framework we consider in our work, the 0/1 loss is not, as it cannot be represented as a function of the confusion tensor we use. That also applies to the family of losses introduced by [H\u00fcllermeier et al. 2022](https://doi.org/10.1007/s10994-021-06107-2), which are a generalization of these two losses. Therefore, we cannot consider them in our framework.\n\n---\n\n> minor: p. 4 \"define in Table 1\" p. 7 \"tensor measure measure\"\n\nWe thank the reviewer for noticing and letting us know."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7700/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260449084,
                "cdate": 1700260449084,
                "tmdate": 1700260449084,
                "mdate": 1700260449084,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wxBUf7mpta",
                "forum": "XOnya9gSdF",
                "replyto": "0TQDKJK9cP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7700/Reviewer_gGLD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7700/Reviewer_gGLD"
                ],
                "content": {
                    "title": {
                        "value": "Re: Response to Reviewer gGLD"
                    },
                    "comment": {
                        "value": "Thank you for your brief and to-the-point response.\n\n> While the Hamming loss is a part of the framework we consider in our work, the 0/1 loss is not, as it cannot be represented as a function of the confusion tensor we use. That also applies to the family of losses introduced by H\u00fcllermeier et al. 2022, which are a generalization of these two losses. Therefore, we cannot consider them in our framework.\n\nMaybe I get it wrong but your work and the work by H\u00fcllermeier et al. both focus on predicing a subset of labels correctly. While in your work this is fixed for a specific K, in H\u00fcllermeier et al. the K is a parameter of choice for the user ranging from 1 resembling Hamming loss and the number of labels to obtain subset 0/1 loss. Of course, this work here looks at macro-averaged metrics but still assigns a degree of importance to a subset of labels, namely the top-K labels. So I do believe that there is a connection although it might be subtle and that you cannot consider subset 0/1 loss in your framework."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7700/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477632939,
                "cdate": 1700477632939,
                "tmdate": 1700477632939,
                "mdate": 1700477632939,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tWwP4P6Cck",
                "forum": "XOnya9gSdF",
                "replyto": "GYPSsyfHOp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7700/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7700/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "The key difference between the approach of H\u00fcllermeier et al. and ours is that \nthe former measures performance *jointly on each instance*, while we focus on macro-averages. \n\nIn our work, $k$ is a strict requirement on the number of predicted positive labels per instance. In case of the other work, $k$ is a parameter of a loss function that indicates the minimal size of correctly predicted subsets of labels for an instance that will be rewarded by the loss function. It does not limit in any way the number of predicted positive labels.\n\nTo see the difference, consider $k = 1$. In our case, we will predict only one label per instance. The Bayes classifier will be a label with the highest value of the linear transformation of the label marginal (with parameters $\\mathbf{a}$ and $\\mathbf{b}$ depending on the base loss function and data distribution). In case of H\u00fcllermeier et al., $k=1$ leads to the standard Hamming loss without any restriction on the number of predicted labels. The optimal prediction is then a set of labels with marginal probabilities being greater than (or equal) 0.5."
                    },
                    "title": {
                        "value": "The difference between the approach of H\u00fcllermeier et al. and ours"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7700/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666513466,
                "cdate": 1700666513466,
                "tmdate": 1700666556878,
                "mdate": 1700666556878,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "72ttiyu1rP",
            "forum": "XOnya9gSdF",
            "replyto": "XOnya9gSdF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7700/Reviewer_XNZA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7700/Reviewer_XNZA"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to find consistent algorithms for macro-at-k metrics, which is widely-used in many long-tailed multi-label classification problems. For the multi-label problem, the author shows such optimal classifiers can be derived by selecting top-k scoring labels based on an affine transformation of the marginal label probabilities (which is unknown in practice). They further presents a Frank-Wolfe algorithm to empirically find the optimal classifiers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The technical derivation of this paper seems solid"
                },
                "weaknesses": {
                    "value": "- The writing of this paper is not easy to follow\n- Some baseline methods are missing in experiments"
                },
                "questions": {
                    "value": "- Q1: From the results of Table 2, the proposed method seems to greatly sacrifice instance-wise metrics to trade for gains in macro-average metrics. Is it possible for the proposed method to optimize a interpolated version of the objective that flexibly control the performance tradeoff between instance-wise metrics and macro-averaged metrics?\n\n- Q2: Some baseline methods that claim to also perform good on tail-labels are not discussed in related work, or compared in the experiment section. For example [1] and [2], to name just a few.\n\n- Q3: To improve the clarity of the proposed methods, the author may consider a toy synthetic dataset where data distributions are known, and show derivations of the proposed method, and verify the consistency property through simulations.\n\n- Q4: This submission also seems highly related to [3]. The author should discuss what's the difference, and compare it empirically.\n\n\n### Reference\n- [1] Menon et al. Long-Tail Learning via Logit Adjustment. ICLR 2020.\n- [2] Zhang et al. Long-tailed Extreme Multi-label Text Classification by the Retrieval of Generated Pseudo Label Descriptions. EACL 2023.\n- [3] Schultheis et al. Generalized test utilities for long-tail performance in extreme multi-label classification. NeurIPS 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7700/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818629941,
            "cdate": 1698818629941,
            "tmdate": 1699636937903,
            "mdate": 1699636937903,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vDcrmF2Psh",
                "forum": "XOnya9gSdF",
                "replyto": "72ttiyu1rP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7700/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7700/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XNZA (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the review. We appreciate the hard work. Below, we address the questions:\n\n---\n\n> From the results of Table 2, the proposed method seems to greatly sacrifice instance-wise metrics to trade for gains in macro-average metrics. Is it possible for the proposed method to optimize a interpolated version of the objective that flexibly control the performance tradeoff between instance-wise metrics and macro-averaged metrics?\n\nYes, it is possible to optimize a measure that interpolates between instance-wise metric and macro-metric as long as the resulting objective is a function of confusion tensor $\\mathbf{C}$ and ideally meets all assumptions required by the Frank Wolfe procedure.\nSuch objective is, for example, a linear interpolation between instance-precision@k and macro-f1@k:\n$$\n\\Psi(\\mathbf{C}) = (1 - \\alpha) \\text{Precision}@k(\\mathbf{C}) + \\alpha \\text{Macro-F1}(\\mathbf{C}).\n$$\n\nBelow, we link to the image presenting plots for two datasets, Mediamill and Flickr. The results show that, in this case, the instance-vs-macro curve has a nice concave shape that dominates simple baselines. In particular, we can initially improve macro-measures significantly with only a small drop in instance-measures. We will include this discussion and more results in the appendix.\n\n\nPlots: https://i.imgur.com/LJTTxyq.png\n\n---\n\n> Some baseline methods that claim to also perform good on tail-labels are not discussed in related work, or compared in the experiment section. For example [1] and [2], to name just a few.\n\nIn the paper, we included a comparison with some popular techniques that are domain-agnostic. However, we will be happy to include a comparison with more methods of that type if the reviewer has some particular in mind. Regarding the ones already mentioned by the reviewer:\n\n- [1] considers multi-class ($y \\in [m]$) setting, while in this work we consider multi-label ($\\mathbf{y} \\in \\\\{0,1\\\\}^m$) setting. Post-hoc logit adjustment presented in Section 4 of [1] is the simple weighting by the inverse of priors, which we also use in experiments (denoted as $\\textrm{Macro-R}_{\\text{PRIOR}}$). \n\n- [2] heavily relies on the problem domain (textual data) and cannot be applied to all datasets we consider. Please also note that the approach proposed in our submission can be used on top of any method aiming at improving the probability estimates of tail labels. We will try to highlight it more in the revised version of the paper.\n\n---\n\n> To improve the clarity of the proposed methods, the author may consider a toy synthetic dataset where data distributions are known, and show derivations of the proposed method, and verify the consistency property through simulations.\n\nWe thank the reviewer for the suggestion! We will add an experiment on synthetic data to improve the presentation of our paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7700/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257583607,
                "cdate": 1700257583607,
                "tmdate": 1700257583607,
                "mdate": 1700257583607,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TnDIusmCRp",
                "forum": "XOnya9gSdF",
                "replyto": "72ttiyu1rP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7700/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7700/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XNZA (2/2)"
                    },
                    "comment": {
                        "value": "---\n\n> This submission also seems highly related to [3]. The author should discuss what's the difference, and compare it empirically.\n\nOur submission and paper [3] look at the same problem setup, but using two different optimization frameworks. The latter follows  the _Expected Test Utility_ (ETU) framework where one assumes a _given fixed_ test set $\\boldsymbol{X}$ and then tries to predict the corresponding labels $\\boldsymbol{Y}$. This has important consequences, both theoretical and practical.\n\nFrom a theoretical perspective, by having a fixed test set, the optimal classifier is always deterministic, and one essentially solves a discrete optimization problem over the space of all at-$k$ predictions. As this becomes computationally intractable except for trivial cases, [3] needs to make an approximation, actually optimizing $\\Psi(\\mathbb{E}[\\mathbf{C}(\\boldsymbol{Y}, \\hat{\\boldsymbol{Y}}) \\mid \\boldsymbol{X}])$ instead of the actual ETU objective, $\\mathbb{E}[\\Psi(\\mathbf{C}(\\boldsymbol{Y}, \\hat{\\boldsymbol{Y}})) \\mid \\boldsymbol{X}]$. This objective is then minimized using a block-coordinate-ascent algorithm.\n\nIn contrast, the _Population Utility_ (PU) framework adopted in this work means we are looking for (potentially stochastic) classifiers $\\boldsymbol{h}$ that work on the level of _a single instance_. Thus, we want to optimize $\\Psi(\\mathbb{E}[\\mathbf{C}(\\boldsymbol{y}, \\boldsymbol{h}(\\boldsymbol{x}))])$. While this resembles the approximation above, there are important differences. Firstly, the expectation is no longer conditioned on the test set $\\boldsymbol{X}$. Secondly, instead of matrices, $\\boldsymbol{Y}$ and $\\boldsymbol{X}$, we now only need vectors $\\boldsymbol{y}$ and $\\boldsymbol{x}$, as the expectation is taken over single instances.\n\nFrom a practical point of view, this means that the PU-classifier introduced in this paper can be employed in an online inference setting, where queries are presented one-by-one, instead of being in one large batch. In contrast, if you apply the ETU algorithm [3] in online inference, i.e., independently apply to test sets of size 1, it degenerates back to the top-k prediction.\nHowever, when evaluating on a given test set, the ETU solution can play its strengths, since it is possible to exploit the actual instances present in the test set, which PU cannot do. Thus, when using the same label probability estimator $\\boldsymbol{\\eta}$, we generally expect ETU to perform better than PU.\n\nTo illustrate our expectation empirically, we present below the results obtained for the Flickr data set (the method introduced in [3] is denoted in the table below with $_{BCA}$ subscript).\n\n**Flickr:**\n| method                         | iP@5       | iR@5       | mP@5       | mR@5       | mF@5       |\n|:-------------------------------|-----------:|-----------:|-----------:|-----------:|-----------:|\n| Top-K                          | **16.99**  | **65.90**  | 17.11      | 46.88      | 23.47      |\n| Top-K + $w^{\\text{POW}}$       | 16.10      | 62.69      | 13.76      | 52.22      | 20.67      |\n| Top-K + $w^{\\text{LOG}}$       | 16.76      | 65.10      | 15.05      | 49.60      | 21.99      |\n| Top-K + $\\ell_{\\text{FOCAL}}$  | *16.89*    | *65.50*    | 18.54      | 45.53      | 24.14      |\n| Top-K + $\\ell_{\\text{ASYM}}$   | 16.73      | 64.91      | 17.39      | 45.47      | 23.58      |\n| Macro-P$_{FW}$                 | 5.66       | 22.71      | *41.74*    | 9.67       | 10.55      |\n| Macro-R$_{BCA}$                | 0.39       | 1.63       | **65.80**  | 3.74       | 3.29       |\n| Macro-R$_{PRIOR}$              | 12.17      | 47.40      | 13.98      | *53.64*    | 19.71      |\n| Macro-R$_{FW}$                 | 12.17      | 47.40      | 13.98      | *53.64*    | 19.71      |\n| Macro-R$_{BCA}$                | 12.63      | 49.12      | 12.93      | **55.44**  | 18.86      |\n| Macro-F1$_{FW}$                | 11.78      | 45.83      | 34.68      | 30.77      | *29.40*    |\n| Macro-F1$_{BCA}$               | 11.23      | 43.53      | 32.36      | 33.21      | **30.70**  |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7700/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700257779420,
                "cdate": 1700257779420,
                "tmdate": 1700259523750,
                "mdate": 1700259523750,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OEpHjy5NMk",
                "forum": "XOnya9gSdF",
                "replyto": "TnDIusmCRp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7700/Reviewer_XNZA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7700/Reviewer_XNZA"
                ],
                "content": {
                    "comment": {
                        "value": "I am satisfied with the authors response. I will keep my score the same."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7700/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638522269,
                "cdate": 1700638522269,
                "tmdate": 1700638522269,
                "mdate": 1700638522269,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oFJtLAbRgM",
            "forum": "XOnya9gSdF",
            "replyto": "XOnya9gSdF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7700/Reviewer_gTuy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7700/Reviewer_gTuy"
            ],
            "content": {
                "summary": {
                    "value": "Paper presents an approach for multi-label classification with a budget using so called macro-at-k metrics which are linearly decomposable into a sum of binary classification utilities, and could be useful in the case of extreme classification with large number of imbalanced labels. This leads to challenging optimisation problem which is tackled using Frank-Wolfe method on label-wise confusion matrices. Theoretical underpinnings of producing consistent classifier are analysed, and performance improvement of macro-at-k against top-k strategies are shown on four different multi-label benchmark datasets with different number of labels and label distribution."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposal is theoretical sound, and detailed analysis of the properties of proposed metrics as well as how to build the consistent classifier based on confusion matrix measure and tensor representation, are shown. One practical solution for optimising the classifiers with proposed macro metrics, is derived. In empirical evaluations, four benchmark dataset with two base classifiers (MLP, Sparse linear model) are used to compare macro-at-k metrics with more straightforward top-k heuristics. In most cases, proposed approach could improve the precision and recall (and F1 score) in different k-budget levels, whereas on the largest dataset more simple baseline heuristics seems to work better. To my knowledge, the setting of considering budgeted macro-at-k in multi-label classification is novel, providing interesting approach and new knowledge to extreme multi-label problems."
                },
                "weaknesses": {
                    "value": "Although paper shows good theoretical background and promising results, it lacks some of the detailed analysis and discussion of the results, especially in a broader sense. For instance, the discussion which of proposed metrics and macro-at-k approach should be chosen for different problems from practitioners perspectives, would strengthen the presentation. Also, manuscript is missing the analysis of computational complexity and computational times (of optimising the classifiers) and how these relate to size of the budget and other chosen parameters, as well as how these compare between different heuristics."
                },
                "questions": {
                    "value": "- What would be the conclusions or \"rule of thumb\" of selecting particular heuristics from the practitioners' point of view for certain applications or multi-label classification problem?\n- What are the computational costs of proposed approach and how these relate  the size of k?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7700/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7700/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7700/Reviewer_gTuy"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7700/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823091139,
            "cdate": 1698823091139,
            "tmdate": 1700641308043,
            "mdate": 1700641308043,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xMxZRVzFeK",
                "forum": "XOnya9gSdF",
                "replyto": "oFJtLAbRgM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7700/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7700/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gTuy (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the insightful review. We appreciate the hard work. Below, we address the main questions and concerns:\n\n---\n\n> It lacks (...) the discussion which of proposed metrics and macro-at-k approach should be chosen for different problems from practitioners perspectives\n\n> What would be the conclusions or \"rule of thumb\" of selecting particular heuristics from the practitioners' point of view for certain applications or multi-label classification problem?\n\nThe basic heuristics might be more robust than the FW method in the case of datasets with very long tails and a very small number of positive instances. In this case, simplicity might be preferred over the sophisticated optimization procedure (which is the general rule of thumb in statistics and machine learning). Nevertheless, we believe that by improving the probability estimation for long tail labels (what we do not discuss in this paper), we should also get a more stable and better performance of the FW method. \n\nRegarding the metrics, macro-averaging treats all the labels equally important. This prevents ignoring tail labels (when used with an imbalance-sensitive binary loss). The budget of $k$ predictions requires the prediction algorithm to choose the labels \u201cwisely.\u201d The choice of the base metric (e.g., precision, recall, F-measure, etc.) depends on a given application, exactly in the same way as in the case of binary classification applications. For example, suppose a user wants to cover as many as possible relevant labels. In that case, they should use recall, but in our setting, the budget per each instance is precisely defined and recall for each label has the same importance."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7700/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700256987225,
                "cdate": 1700256987225,
                "tmdate": 1700256987225,
                "mdate": 1700256987225,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7UnYCFPZiu",
                "forum": "XOnya9gSdF",
                "replyto": "EWlV7xn5Z0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7700/Reviewer_gTuy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7700/Reviewer_gTuy"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the response. I have read all the rebuttals and reviews. I am satisfied with the clarifications and additional experiments to be added to revised version of the manuscript. The work gives nice theoretical contributions to at-$k$ constraint multi-label classification with practical optimisation strategy with competitive empirical results. I'll raise my score accordingly."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7700/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641274781,
                "cdate": 1700641274781,
                "tmdate": 1700641274781,
                "mdate": 1700641274781,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]