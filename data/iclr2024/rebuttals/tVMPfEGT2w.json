[
    {
        "title": "Provable Offline Preference-Based Reinforcement Learning"
    },
    {
        "review": {
            "id": "YXXAGqGmij",
            "forum": "tVMPfEGT2w",
            "replyto": "tVMPfEGT2w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4125/Reviewer_2Fsc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4125/Reviewer_2Fsc"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a PbRL algorithm via computing a return utility function, based on MLE. They then construct a confidence set, based on that estimate to solve a planning problem for deriving an approximate, optimal policy. In contrast to most PbRL work, they do not assume the existence of state-action utility. \nThis algorithm forms the base for analyzing the algorithms sample complexity, using a novel trajectory-based concentration coefficient. This analysis is also applied to a setting with learned transition dynamics and action preferences."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Most work in PbRL focuses on empirical evaluations, therefore additional work considering the theoretical background is important. This work is especially relevant, as it is not restricted to a specific class of function approximation and considers return-based utility. Reward-based utility is usually easier to handle, but can induce abnormalities in the preference setting, therefore the return-generalization is also highly interesting. Adapting the concentrability coefficient to this setting is interesting as it allows to separate (offline) optimization of the policy from the issues of exploration/exploitation."
                },
                "weaknesses": {
                    "value": "The most substantial weakness of the paper is, that the related work focuses on other, theoretical work, but not practical. E.g. the idea of return-based utility approximation is not novel (e.g. Preference-Based Policy Learning, Akrour, 2011 - see Wirth 2017 for more examples). This is relevant because of two reasons. First, it allows the reader to determine which algorithms fit under the proposed framework and second, it clarifies the differences between the proposed algorithm and other works. Considering the algorithm is mentioned as one of the main contributions, but likelihood-based estimation of the return utility is already known, the novelty of the algorithm is a bit limited.\n\nBesides that, there are several possibilities to improve the presentation:\n- Reward is defined wrt. trajectories, but this is commonly denoted return in the PbRL literature\n- Some relevant information is left to the appendix - mostly the feasible implementation and the comparison to Zu 2023. Especially some details concerning the feasible implementation are relevant for practioners.\n- Remark 3 is not explained\n\nFurthermore, to the reviewer, one of the statements is not obvious:\n- cannot be relaxed to the per-step concentrability without additional assumptions, such as on the reward structure - Why does it follow from the Theorems, that state-action rewards are not enabling this?\n\nAs small remarks:\n- Missing reference: \"..per-step concentrability coefficient Cst commonly used in the general offline RL literature\"\n- Missing reference: \"..It is well-known that when the reward is state-action-wise, the optimal policy \u03c0 is both Markovian and deterministic.\"\n- Definition 1 is a bit hard to understand, because g is never define.\n- The slack parameter in Sec. 4.1 has the wrong symbol\n- Typo in Alg 2: \"Distributionally robust plnanning\""
                },
                "questions": {
                    "value": "Please explain \"cannot be relaxed to the per-step concentrability without additional assumptions, such as on the reward structure\" (see above)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4125/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4125/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4125/Reviewer_2Fsc"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4125/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698309371546,
            "cdate": 1698309371546,
            "tmdate": 1700490719518,
            "mdate": 1700490719518,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NtLGYRfkZz",
                "forum": "tVMPfEGT2w",
                "replyto": "YXXAGqGmij",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4125/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for the valuable feedback! Here are our responses:\n\n- **Comparison against Practical Work**\n\nWe mainly discuss theoretical works in the related works because our paper also focuses on the theory of offline RLHF. That said, **our algorithm is still quite different from the practical algorithms presented in [1]**. Although they also use MLE to estimate the reward model, they typically run RL algorithms **with respect to the learned MLE rewards** (i.e., $\\hat{r}$ in Algorithm 1) without any pessimism. The lower bound in [Theorem 3.9, 2] has shown that this kind of algorithm can fail in the worst cases. \n\nIn contrast, we **use pessimism** to circumvent such failure. More specifically, we **construct a confidence set of the reward model and then evaluate each policy with the most pessimistic reward model inside the confidence set**. This is the key difference between our algorithm and existing commonly used algorithms in [1], and this modification is crucial to overcome their limit.\n\n- **Reward vs Return**\n\nThanks for pointing it out! We will revise this in the final version.\n\n- **Appendix**\n\nThanks for pointing it out! We will move the key points in Appendix to the main paper in the final version.\n\n- **Remark 3**\n\nWe will clarify it as follows. In line 4 of Algorithm 1, we need to compute $\\mathbb{E} _ {\\tau \\sim \\mu _ {ref}}[r (\\tau)]$, which might be computationally inefficient since the space of all trajectories is large. In this case, we can sample $N_0$ trajectories $(\\tau ^ i) _ {i = 1} ^ {N _ 0}$ from $\\mu _ {ref}$ and then simply use $ \\frac{1}{N _ 0}\\sum _ {i=1} ^ {N _ 0} r(\\tau ^ i)$ as a surrogate of $\\mathbb{E} _ {\\tau \\sim \\mu _ {ref}}[r (\\tau)]$. From Azuma-Hoeffding inequality, this will only incur an error scaling with $\\frac{1}{N _ 0}$. Therefore as long as we choose $N _ 0 = O(1 / \\epsilon ^2)$, this error can be neglected while line 4 becomes more computationally efficient.\n\n- **Cannot Relaxed to Per-step Concentrability**\n\nTheorem 2 shows that there exists **a set of MDPs whose rewards are defined per step (i.e., $r(\\tau) = \\sum _ {h=1} ^ H r _ h (s _ h, a _ h)$)** and **an offline dataset which satisfies the per-step concentrability** such that  **no offline algorithm** is able to learn a near-optimal policy. This implies that with only per-step concentrability, we are not able to find a high-quality policy, even if we assume the rewards of the MDPs are defined per step. In contrast, with our trajectory-wise concentrability coefficient in Definition 2, we are able to learn a near-optimal policy for any MDPs, no matter whether the reward is defined over trajectories or defined per step. We will add this explanation in the final version. \n\n- **Missing Reference**\n\nWe are sorry about the missing references. We list the references here and will add them in the main paper:\nPer-step concentrability coefficient: [3-7], Optimal policy: [8]\n\n- **Definition of $g(\\cdot|\\tau _ 1,\\tau _ 2)$**\n\nWe are sorry about the unclearness of the definition of $g$. $g$ is a function mapping from $(\\mathcal{T} \\times \\mathcal{T})$ (recall that $\\mathcal{T} = (\\mathcal{S} \\times \\mathcal{A}) ^ H$ is the set of all possible trajectories) to $\\mathbb{R}^2$ and thus $g(\\cdot|\\tau _ 1, \\tau _ 2)$ is a two-dimensional vector. It doesn\u2019t need to be a distribution, i.e., $g(\\cdot|\\tau _ 1, \\tau _ 2)$ is not necessarily positive and $\\Vert g(\\cdot|\\tau _ 1, \\tau _ 2) \\Vert _1$ doesn\u2019t need to be 1. Similar functions are elaborated in the definitions in Appendix E. We will revise this in the final version.\n\n- **Wrong Symbol and Typo**\n\nThanks for pointing them out! We will correct them in the final version.\n\n[1] Wirth, C., Akrour, R., Neumann, G., F\u00fcrnkranz, J., et al. (2017). A survey of preference-based reinforcement learning methods. Journal of Machine Learning Research, 18(136):1\u201346.\n\n[2] Zhu, B., Jiao, J., and Jordan, M. I. (2023). Principled reinforcement learning with human feedback from pairwise or k-wise comparisons. arXiv:2301.11270.\n\n[3] Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A. (2021). Bellman-consistent pessimism for offline reinforcement learning. arXiv:2106.06926\n\n\n[4] Uehara, M. and Sun, W. (2021). Pessimistic model-based offline rl: Pac bounds and posterior sampling under partial coverage. In arXiv:2107.06226.\n\n[5] Zhan, W., Huang, B., Huang, A., Jiang, N., and Lee, J. (2022a). Offline reinforcement learning with realizability and single-policy concentrability. In Conference on Learning Theory, pages 2730\u20132775. PMLR.\n\n[6] Shi, L., Li, G., Wei, Y., Chen, Y., and Chi, Y. (2022). Pessimistic q-learning for offline reinforcement learning: Towards optimal sample complexity. arXiv:2202.13890.\n\n[7] Li, G., Shi, L., Chen, Y., Chi, Y., andWei, Y. (2022b). Settling the sample complexity of model-based offline reinforcement learning. arXiv:2204.05275.\n\n[8] Bertsekas, D. P. (2017). Dynamic programming and optimal control. Athena Scientific."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700156103934,
                "cdate": 1700156103934,
                "tmdate": 1700156103934,
                "mdate": 1700156103934,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NO2pcAOzlP",
                "forum": "tVMPfEGT2w",
                "replyto": "NtLGYRfkZz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4125/Reviewer_2Fsc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4125/Reviewer_2Fsc"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifications. I slightly increased my score, mostly due to the \"Comparison against Practical Work\". I expected this to also be included into the CR version. (At least as a condensed version)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490830087,
                "cdate": 1700490830087,
                "tmdate": 1700490830087,
                "mdate": 1700490830087,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "00wY1hV612",
            "forum": "tVMPfEGT2w",
            "replyto": "tVMPfEGT2w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4125/Reviewer_1qtd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4125/Reviewer_1qtd"
            ],
            "content": {
                "summary": {
                    "value": "### Contributions\n* This paper proposed the first algorithm for offline trajectory-wise PbRL with general function approximation and under partial coverage.\n    * The reward function can be defined over the whole trajectory, or for each state action pair.\n    * Extending to the case where transition kernel is also learned, and the case where data is composed of action comparisons, rather than trajectory comparisons.\n* Setting: finite horizon MDP with trajectory-wise reward.\n* Preference model: known link function.\n* Goal: \u03f5-\u03b4 PAC offline RL: use offline dataset {(traj 1, traj 2, preference signal)} to learn the optimal policy, with unknown reward (extended to also unknown transition kernels).\n* Key innovation: function approximation\n    * Use a given realizable function class Gr to learn the reward function r.\n    * Measure the complexity as the \u03f5-bracketing number of the set of preferences induced by Gr.\n* If transition kernels is known:\n    * Algorithm\n        * (1) MLE the reward, r\\hat, using the data; (2) construct a confidence set around r\\hat; (3) distributionally robust planning to jointly max total reward and stay close to some reference trajectory, under the worst-case reward in the confidence set.\n    * Analysis\n        * This paper developed concentrability coefficient for preference-based feedback (Sec.4.2) and discuss the relation bw per-trajectory concentrability coefficient vs per-step concentrability coefficient (Sec.4.3). This result indicates that trajectory-wise feedback is intrinsically harder than step-wise feedback in offline PbRL.\n        * The resulting PAC bound (under Eq.2) depends on this coefficient and also the \u03f5-bracketing number of the function class.\n* If transition kernels is unknown:\n    * Algorithm: also do MLE and confidence set for transition kernel.\n    * Analysis: concentrability coefficient.\n* If action-based comparison:\n    * Assuming that the preference feedback is based on the Q value at the state with two actions.\n    * Algorithm: MLE the advantage function and output the greedy policy.\n    * Analysis: concentrability coefficient."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The algorithm and analysis are sound.\n* Considering multiple cases, e.g., unknown transition, action-based comparison, and fitting them within similar algorithmic framework.\n* Nice comparison with previous work in Sec.4.1, Remark 2, 4."
                },
                "weaknesses": {
                    "value": "* This paper is fully focusing on theory. It would be nice to also have some empirical results."
                },
                "questions": {
                    "value": "* It seems to me that the link function in Eq.1 is known? Would it work if the link function is unknown?\n* For unknown transition kernels, would the result extend to the case where the transition kernels' data are sampled separately from the preference data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4125/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803387779,
            "cdate": 1698803387779,
            "tmdate": 1699636377547,
            "mdate": 1699636377547,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3znE7dECaK",
                "forum": "tVMPfEGT2w",
                "replyto": "00wY1hV612",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4125/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you very much for the positive feedback! Here are our responses:\n\n- **Empirical Results**\n\nWe are currently considering adding an empirical case study in future work. That said, **we indeed have some discussions about the practical implementation of our algorithm in Appendix B**. In summary, we use the Lagrangian multiplier to convert our algorithm into a maximin optimization problem and demonstrate how to compute the gradient of the objective function so that we can use gradient ascent-descent to solve it. Given that gradient descent-ascent has shown superior performance in standard offline RL problems [1], we believe that the above approach can also work well in practice.\n\n\n- **Link Function**\n\nYes, we assume the link function is known. When the link function is unknown, we may consider a link function class $\\mathcal{F}$. Then we can generalize our analysis by implementing MLE over **the joint function class** $(\\mathcal{F}, \\mathcal{G} _ r)$, i.e., $(\\hat {r}, \\hat{\\Phi}) = \\arg\\max _ {r \\in \\mathcal{G} _ r, \\Phi\\in\\mathcal{F}} \\sum _ {n=1}^N \\log P _ {r,\\Phi}(o=o^{n}\\mid\\tau^{n,1},\\tau^{n,0})$ where $P_{r,\\Phi}(o= 1\\mid\\tau^1, \\tau ^ 0) := \\Phi( r(\\tau ^ 1) - r (\\tau ^ 0))$. We leave its formal analysis to our future work. \n\n\n- **Unknown Transition Kernel**\n\nYes, the analysis still holds. We reuse the dataset in order to reduce sample complexity, but you can definitely collect a different dataset for transition learning as long as the dataset has a bounded single-policy concentrability coefficient (Definition 3).\n\n[1] Rigter, M., Lacerda, B., and Hawes, N. (2022). Rambo-rl: Robust adversarial model-based offline reinforcement learning. Neurips 22."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700155685948,
                "cdate": 1700155685948,
                "tmdate": 1700155685948,
                "mdate": 1700155685948,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ah8H9ivx14",
                "forum": "tVMPfEGT2w",
                "replyto": "3znE7dECaK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4125/Reviewer_1qtd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4125/Reviewer_1qtd"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comment!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503035335,
                "cdate": 1700503035335,
                "tmdate": 1700503035335,
                "mdate": 1700503035335,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WAhTMPqgH4",
            "forum": "tVMPfEGT2w",
            "replyto": "tVMPfEGT2w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4125/Reviewer_Ekit"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4125/Reviewer_Ekit"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to investigate preferance based offline reinforcement learning. They consider a setting where the dataset consists of trajectory pairs with a preference. They assume a preference model over the trajectories and come up with an algorithm which has two parts, the first to obtain a confidence set of reward functions using maximum likelihood estimation and the second part is to obtain the policy over this confidence set. Their algorithm provides a guarantee that allows to learn the target policy with a polynomial number of samples."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The problem of reinforcement learning with human feedback, specifically offline preference learning is highly relevant. \n\n(2) Their algorithm provides a guarantee of being able to learn the target policy using polynomial number of samples.\n\n(3) They extend their algorithm to settings with unknown transition kernel and state-action preferences.\n\n(4) They introduce a new single-policy concentrability coefficient to measure the coverage of the target policy on the dataset."
                },
                "weaknesses": {
                    "value": "(1) There are no implementation details about the algorithms. These algorithms are theoretically sound but there is no intuition for the reader on how to implement them or construct a practical algorithm. \n\n(2) There are no experiments either. So there is no way to check how this method empirically scales. \n\n(3) Several terms in the paper are not well explained. For example, $\\epsilon$-bracketing: what do you mean by g(.|tau1, tau2)? Is g a probability measure, or a preference?\n\n(4) Proposition 1: For epsilon being arbitrarily small, and the bounds B and R being arbitrarily large, log N_G_r can be arbitrarily large making the sample bounds weak.\n\n(5) The sample bounds i.e. sample complexity N = O(..log (..1/N)). How can N be on both sides, something seems missing."
                },
                "questions": {
                    "value": "(1) In Section 4.1, it is mentioned the distance between r and r* is computed as total variation distance or l1 norm. Arent r and r* scalars? \n\n(2) Does C_tr (per trajectory concentration coefficient) not depend on \\mu_1? If so, why?\n\n(3) How does C_r(G_r, \\pi_tar, \\u_1) reduce to sqrt(Ctr)?\n\n(4) It looks like pi, mu and d are used interchangeably. I recommend correcting this."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4125/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4125/Reviewer_Ekit",
                        "ICLR.cc/2024/Conference/Submission4125/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4125/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809264108,
            "cdate": 1698809264108,
            "tmdate": 1700590801611,
            "mdate": 1700590801611,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AQYGFqLUXD",
                "forum": "tVMPfEGT2w",
                "replyto": "WAhTMPqgH4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4125/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for the valuable feedback. Here are our responses:\n\n- **Implementation Details and Empirical Experiments in Weaknesses**\n\nWe would like to clarify that this is mainly a **theoretical** work that strives to find sample efficient offline RLHF algorithms with **provable** guarantees. That said, we would like to point out that we indeed **have discussions about the practical implementation of our algorithm in Remark 1 in the main text and Appendix B**. In summary, we use the **Lagrangian multiplier** to convert our algorithm into a **maximin optimization problem** and demonstrate how to compute the gradient of the objective function so that we can use **gradient ascent-descent** to solve it. Given this type of gradient descent-ascent has shown superior performance in standard offline RL problems [1], we believe that the above approach can also work well in practice in our preference-based RL setting. We will emphasize this part more in the final version. \n\n- **Response to the claim: There are no experiments either. So there is no way to check how this method empirically scales.** \n\nWe would like to mention that **there are many practical and impactful works about offline RL in top machine learning conferences without any experiments**. First, for example, in the literature on offline model-free RL, [2] proposed the first provably efficient algorithm with general function approximation. While this paper does not have any experiments, it is later implemented in another follow-up paper [3]. It is reported that the proposed algorithm achieves SOTA and beats many offline RL baselines, such as CQL, COMBO ICL, and TD3+BC. \n\nSimilarly, in the literature on model-based RL, [4] proposed the first probably efficient algorithm with general function approximation. While this paper does not have any experiments again, later, it is implemented in another follow-up paper [1], and it is reported that it achieves SOTA and beats many baselines, such as again MOReL, CQL, IQL, and TD3+BC. **In this way, even with no experiments, theoretically backed algorithms have been empirically impactful.**\n\n- **Definition of $g(\\cdot|\\tau _ 1,\\tau _ 2)$  in Weaknesses**\n\nWe are sorry about the unclearness of the definition of $g$. $g$ is a function mapping from $(\\mathcal{T} \\times \\mathcal{T})$ (recall that $\\mathcal{T} = (\\mathcal{S} \\times \\mathcal{A}) ^ H$ is the set of all possible trajectories) to $\\mathbb{R}^2$ and thus $g(\\cdot|\\tau _ 1, \\tau _ 2)$ is a two-dimensional vector. It doesn\u2019t need to be a distribution, i.e., $g(\\cdot|\\tau _ 1, \\tau _ 2)$ is not necessarily positive and $\\Vert g(\\cdot|\\tau _ 1, \\tau _ 2) \\Vert _1$ doesn\u2019t need to be 1. Similar functions are elaborated in the definitions in Appendix E. We will revise this in the final version.\n\n- **Proposition 1 in Weaknesses**\n\n**We would like to state that our bounds are not weak compared to standard RL literature**. First, as you notice, the log bracket number increases **logarithmically** as the accuracy $\\epsilon$ decreases. However, as we will later see in Theorem 1, we just set $\\epsilon=1/N$, which results in an additional $\\log(N)$ term. This term is negligible compared to the leading term $1/N$.  Second, the log dependence on $B$ and $R$ is very standard in the literature on statistical machine learning in general, as seen in Section 4,5 in [5] and Definition 8.1 in a standard RL textbook [6].  \n\n\n- **Sample Bounds in Weaknesses**\n\nWe are sorry for the misleading expression. Here, we take $N$ so that the right-hand side of (2) in Theorem 1 is less than $\\epsilon$. So, this expression means if $N$ satisfies (2), we can get an $\\epsilon$-optimal policy.  To remove $N$ from the right-hand side, with some algebra, we can state that as long as $N=O(\\log(1/\\epsilon)/\\epsilon^2)$ (when we just focus on the dependence on $\\epsilon$), we can get an $\\epsilon$-optimal policy. We will clarify it in the final version. \n\n- **$r$ and $r ^ * $ in Questions**\n\nNo, $r$ and $r ^ * $ are reward **functions**. They map a trajectory $\\tau$ to a scalar. Thus, here $r$ ($r^*$) can be viewed as a vector whose $\\tau$-th entry corresponds to the value of $r(\\tau)$ ($r^*(\\tau)$) for each $\\tau \\in \\mathcal{T}$.\n\n- **$C _ {tr}$  in Questions**\n\nNo, it doesn\u2019t depend on $\\mu _ 1$ because when we reduce  $C _ r$ to $C _ {tr}$, we take $\\mu _ {ref} = \\mu _ 1$ in $C _ r(\\mathcal{G} _ r, \\pi _ {tar}, \\mu _ {ref})$ and then $\\mu _ 1$ is canceled out  by $\\mu _ {ref}$."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700155145382,
                "cdate": 1700155145382,
                "tmdate": 1700155145382,
                "mdate": 1700155145382,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cMtr7AJBmD",
                "forum": "tVMPfEGT2w",
                "replyto": "WAhTMPqgH4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4125/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (Part 2)"
                    },
                    "comment": {
                        "value": "- **from  $C_r(\\mathcal{G} _ r, \\pi _ {tar}, \\mu _ {ref})$ to $C _ {tr}$  in Questions**\n\nNote that in this reduction we take $\\mu _ {ref}$ to be $\\mu _ 1$. From Cauchy-Schwartz inequality, we have \n$$ \\sqrt{  \\mathbb{E} _ {\\tau ^ 0 \\sim \\pi _ {tar}, \\tau ^ 1 \\sim \\mu _ 1}\\big[|r ^ *(\\tau ^ 0) - r ^ *(\\tau ^ 1) - r(\\tau ^ 0) + r(\\tau ^ 1)| ^ 2 \\big] } \\geq  \\mathbb{E} _ {\\tau ^ 0 \\sim \\pi _ {tar}, \\tau ^ 1 \\sim \\mu _ 1}\\big[|r ^ *(\\tau ^ 0) - r ^ *(\\tau ^ 1) - r(\\tau ^ 0) + r(\\tau ^ 1)|\\big].$$\n\nThis implies that\n$$C^2_r(\\mathcal{G} _ r, \\pi _ {tar}, \\mu _ 1) \\leq \\sup _ {r} \\frac{ \\mathbb{E} _ {\\tau ^ 0 \\sim \\pi _ {tar}, \\tau ^ 1 \\sim \\mu _ 1}\\big[|r ^ *(\\tau ^ 0) - r ^ *(\\tau ^ 1) - r(\\tau ^ 0) + r(\\tau ^ 1)| ^ 2\\big]}{ \\mathbb{E} _ {\\tau ^ 0 \\sim \\mu _ 0,\\tau ^ 1 \\sim \\mu _ 1}\\big[|r ^ *(\\tau ^ 0) - r ^ *(\\tau ^ 1) - r(\\tau ^ 0) + r(\\tau ^ 1)| ^ 2\\big]}$$\n\nSince $\\tau _ 0$ and $\\tau _ 1$ are independent, we have\n$$C ^ 2 _ r(\\mathcal{G} _ r, \\pi _ {tar}, \\mu _ 1) \\leq \\sup _ {r} \\max _ {\\tau _ 0, \\tau _ 1 }\\frac{ d ^ {\\pi _ {tar}}(\\tau _ 0) \\mu _ 1(\\tau _ 1)}{ \\mu _ 0(\\tau _ 0) \\mu _ 1(\\tau _ 1)} =\\max _ {\\tau} \\frac{d ^ {\\pi _ {tar}}(\\tau)}{ \\mu _ 0(\\tau)}.$$\n\n- **$\\pi, \\mu$ and $d$ in Questions**\n\nWe are sorry for the unclearness. For $\\pi$, we use $\\tau \\sim \\pi$ to denote the distribution of $\\tau$ when executing $\\pi$ in the MDP. When $\\tau$ follows a more general distribution $\\mu$, i.e., when the distribution may not be induced by a policy $\\pi$, we use $ \\tau \\sim \\mu$. For $d$, we only use $d ^ {\\pi} (\\tau)$ to denote the probability of $\\tau$ under policy $\\pi$. We will further clarify these in the final version.  \n\n[1] Rigter, M., Lacerda, B., and Hawes, N. (2022). Rambo-rl: Robust adversarial model-based offline reinforcement learning. Neurips 2022. \n\n[2] Xie, T., Cheng, C. A., Jiang, N., Mineiro, P., & Agarwal, A. (2021). Bellman-consistent pessimism for offline reinforcement learning. Advances in neural information processing systems, 34, 6683-6694.\n\n[3] Cheng, C. A., Xie, T., Jiang, N., & Agarwal, A. (2022, June). Adversarially trained actor critic for offline reinforcement learning. In International Conference on Machine Learning (pp. 3852-3878). PMLR.\n\n[4] Uehara, M. and Sun, W. (2021). Pessimistic model-based offline rl: Pac bounds and posterior sampling under partial coverage. ICLR 22 \n\n[5] Wainwright, M. J. (2019). High-dimensional statistics: A non-asymptotic viewpoint (Vol. 48). Cambridge university press.\n\n[6] Agarwal, A., Jiang, N., Kakade, S. M., & Sun, W. (2019). Reinforcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, 32"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700155534971,
                "cdate": 1700155534971,
                "tmdate": 1700155534971,
                "mdate": 1700155534971,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ChtWzyuv0U",
                "forum": "tVMPfEGT2w",
                "replyto": "WAhTMPqgH4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4125/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Discussion?"
                    },
                    "comment": {
                        "value": "Dear reviewer, \n\nThere are only three days left for the discussion period. Do you have any other questions that we can help with?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505038270,
                "cdate": 1700505038270,
                "tmdate": 1700505038270,
                "mdate": 1700505038270,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T064Zqdg6y",
                "forum": "tVMPfEGT2w",
                "replyto": "ChtWzyuv0U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4125/Reviewer_Ekit"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4125/Reviewer_Ekit"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for your response. Thanks a lot for pointing out the discussion on the practical implementation. \nI do understand that the paper focuses on theoretical results and introduces several theoretical insights that are valuable for the community. On the other hand, I feel it is also important for works to introduce a practical version of their algorithms, and provide some empirical evaluations to support their theoretical claims. For instance, if you provide an algorithm with better sample efficiency, you could have come up with a very simple experiment showing how the algorithm has better sample efficiency than previous methods. \n\nNevertheless, I believe that the theoretical insights introduced in the paper are relevant to the community and have increased my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590771561,
                "cdate": 1700590771561,
                "tmdate": 1700590771561,
                "mdate": 1700590771561,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0b0xrcJA1K",
                "forum": "tVMPfEGT2w",
                "replyto": "WAhTMPqgH4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4125/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for increasing the score! We appreciate your feedback and will try adding some experiments in the future."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675988270,
                "cdate": 1700675988270,
                "tmdate": 1700676005638,
                "mdate": 1700676005638,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nWfAu8A0Ye",
            "forum": "tVMPfEGT2w",
            "replyto": "tVMPfEGT2w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4125/Reviewer_AQNJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4125/Reviewer_AQNJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes preference-based reinforcement learning in offline setting. Specifically, they propose three algorithms for Offline PbRL. One estimates the reward function using MLE, constructs confidence sets around the MLE and optimizes the policy against the worst reward model under the assumptions that the transition probabilities are known. The second algorithm is similar to the first except that it operates under the assumptions that the transition probabilities are unknown. The algorithm estimates the transition probabilities using MLE and constructs uncertainty sets around the MLE of the transition probabilities. It then optimizes the policy against the worst reward model and the transition probabilities in their respective uncertainty sets. The third algorithm considers the case where preferences are established over actions instead of trajectories. It estimates the advantage function and computes a greedy policy based on the advantage function.\nWhile the paper does not empirically evaluate these algorithms, they theoretically analyze the sample complexity and performance error of the algorithms and show that as long as the offline data covers the target policy,  it can compete with the target policy.\nThey also show that even if the reward is trajectory wise, you can still efficiently learn the policy if the transition dynamics are estimated per step.\nFinally, the paper establishes a partial coverage guarantees for the third algorithm and show that the sample complexity scales with a bound on the advantage function."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper is clearly written.\nThe paper provides several strong and novel theoretically results on preference-based RL in offline setting. Some of these results also generalize the results of Zhu et al for linear function approximators to general function approximators."
                },
                "weaknesses": {
                    "value": "Although the main contribution is theoretical, it would be nice to see an empirical case study or two  where the given algorithm works as expected."
                },
                "questions": {
                    "value": "No questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4125/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4125/Reviewer_AQNJ",
                        "ICLR.cc/2024/Conference/Submission4125/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4125/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699130189400,
            "cdate": 1699130189400,
            "tmdate": 1699668016928,
            "mdate": 1699668016928,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AJUZA4cvpj",
                "forum": "tVMPfEGT2w",
                "replyto": "nWfAu8A0Ye",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4125/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you very much for your positive feedback! We are currently considering adding an empirical case study in future work. That said, **we indeed have some discussions about the practical implementation of our algorithm in Appendix B**. In summary, we use the Lagrangian multiplier to convert our algorithm into a maximin optimization problem and demonstrate how to compute the gradient of the objective function so that we can use gradient ascent-descent to solve it. Given that gradient descent-ascent has shown superior performance in standard offline RL problems [1], we believe that the above approach can also work well in practice.\n\n[1] Rigter, M., Lacerda, B., and Hawes, N. (2022). Rambo-rl: Robust adversarial model-based offline reinforcement learning. arXiv preprint arXiv:2204.12581."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700154044278,
                "cdate": 1700154044278,
                "tmdate": 1700154044278,
                "mdate": 1700154044278,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5PhcD1azZr",
                "forum": "tVMPfEGT2w",
                "replyto": "AJUZA4cvpj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4125/Reviewer_AQNJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4125/Reviewer_AQNJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the comment!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503239788,
                "cdate": 1700503239788,
                "tmdate": 1700503239788,
                "mdate": 1700503239788,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]