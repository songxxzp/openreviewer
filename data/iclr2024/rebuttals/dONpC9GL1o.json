[
    {
        "title": "Closing the Curious Case of Neural Text Degeneration"
    },
    {
        "review": {
            "id": "U1ZdwK842i",
            "forum": "dONpC9GL1o",
            "replyto": "dONpC9GL1o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7923/Reviewer_4bep"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7923/Reviewer_4bep"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to develop a more precise sampling strategy for language models, specifically focusing on addressing errors arising from the softmax bottleneck. The authors establish two sufficient conditions, under certain assumptions, to ensure that sampled tokens belong to the true distribution's support. The first condition (Corollary 1) leads to the threshold sampling algorithm, providing a direct explanation for its success. Assuming the loss to be cross-entropy, the second condition imposes a linear constraint that can be solved with a linear programming optimizer.Combining both conditions, the proposed basis-aware threshold (BAT) sampling outperforms its threshold-based counterparts for low-entropy open-ended text generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed approach is innovative and theoretically-grounded. \n2. This paper brings new insights to the community. The theoretical concepts discussed in this paper are previously ignored but seems to be important."
                },
                "weaknesses": {
                    "value": "1. Basis-aware sampling is specifically designed for models trained using cross-entropy loss. However, not all language models meet this criterion. For instance, LLMs fine-tuned with RLHF do not adhere to this condition.\n2. Theorem 2 and Corollary 2 provide sufficient but unnecessary conditions for proving tokens are in the true support. Therefore, the induced sampling algorithm may also discard tokens in the support of true distribution, leading to biased sampling.\n3. In Corollary 1, the statement \"threshold-based truncation sampling correctly discards all tokens that are not in the support of p*\" is not precise, as it may also incorrectly discard tokens that are in the support of p*. A more precise phrasing would be: \"all tokens that are not in the support of p* will be discarded by threshold-based truncation sampling.\"\n4. The relationship between the softmax bottleneck and text degeneration phenomena has not been verified. It remains unclear whether text degeneration is directly caused by the softmax bottleneck, or if increasing the dimensionality (d) beyond the vocabulary size (v) would effectively resolve the issue."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7923/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7923/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7923/Reviewer_4bep"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7923/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697635914148,
            "cdate": 1697635914148,
            "tmdate": 1699636973105,
            "mdate": 1699636973105,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "afogs4K4WP",
                "forum": "dONpC9GL1o",
                "replyto": "U1ZdwK842i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7923/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7923/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your encouraging words! We are also excited by our approach and agree that this topic has been overlooked and underexplored, despite its centrality to generative language models. We hope we can address some of your concerns below.\n\n> Basis-aware sampling is specifically designed for models trained using cross-entropy loss. However, not all language models meet this criterion. For instance, LLMs fine-tuned with RLHF do not adhere to this condition.\n\nWe chose to focus on cross-entropy loss since it is the most popular method used to pretrain base language models. We agree that it would be worthwhile to consider the implications of alternative loss functions, though these are not guaranteed to give the \u201cnice\u201d linear constraints that we derive from cross entropy.\n\n> Theorem 2 and Corollary 2 provide sufficient but unnecessary conditions for proving tokens are in the true support. Therefore, the induced sampling algorithm may also discard tokens in the support of true distribution, leading to biased sampling.\n\nThis is true. By design, these methods prioritize precision over recall, though the degree of conservativeness can be refined by tuning the hyperparameter $\\tau$. This is actually a core contribution of our work: threshold-based methods can guarantee only in-support tokens are sampled, but at the cost of being overly conservative and rejecting good tokens. Our method allows us to lessen this issue by taking full advantage of our knowledge of a source of model error. We cannot recover the true distribution, but BAT allows us to get closer than vanilla thresholding.\n\n> In Corollary 1, the statement \"threshold-based truncation sampling correctly discards all tokens that are not in the support of p*\" is not precise, as it may also incorrectly discard tokens that are in the support of p*. A more precise phrasing would be: \"all tokens that are not in the support of p* will be discarded by threshold-based truncation sampling.\"\n\nThis is indeed what we meant. We believe the confusion may have come from the word \u201ccorrectly\u201d which we will remove.\n\n> The relationship between the softmax bottleneck and text degeneration phenomena has not been verified. It remains unclear whether text degeneration is directly caused by the softmax bottleneck, or if increasing the dimensionality (d) beyond the vocabulary size (v) would effectively resolve the issue.\n\nPrior work [1,2] offers evidence that the softmax bottleneck (SMB) indeed negatively impacts language model performance, both in theory and in practice, and it has been shown that \u201cbreaking\u201d the SMB can lead to better performance. A question that remains is whether (and when) this effect will have an impact on generation, since heuristics like threshold sampling may already mitigate the SMB effects to some degree. Our pilot experiments with low-entropy decoding seem to suggest that the SMB may still have a negative impact on low-entropy decoding that can be mitigated by BAT sampling.\n\n[1] Yang et al., Breaking the Softmax Bottleneck: A High-rank RNN Language Model.\n\n[2] Yang et al., Mixtape: Breaking the Softmax Bottleneck Efficiently."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7923/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700100267888,
                "cdate": 1700100267888,
                "tmdate": 1700100267888,
                "mdate": 1700100267888,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0EYFL0hNoA",
            "forum": "dONpC9GL1o",
            "replyto": "dONpC9GL1o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7923/Reviewer_8Qv3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7923/Reviewer_8Qv3"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a theoretical analysis of why decoding from language models with truncation sampling works well and provides a new decoding strategy called BAT-sampling. The gist is that if you truncate a sufficiently large portion of the distribution, then truncation sampling will avoid any tokens which are not in the support of the true language distribution. However, this may result in throwing out tokens which *are* in the support of the distribution. The paper then proposes BAT-sampling, which is an LP-based method that can determine which tokens are and are not in the support of the true distribution, even if they are \"out of order\" in terms of the probability assigned (i.e., the method can determine that lower-probability tokens are in the support of the distribution even when higher probability tokens are not). The paper concludes with a set of experiments, including an impressive discussion of speedups for BAT-sampling, as well as some (very slight) improvements over existing methods in certain conditions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is a great analysis paper, providing an interesting explanation for why truncation sampling works so well in language model decoding. The paper's motivation is clear and well-written. The fact that BAT can determine that some tokens have nonzero true support, even though they are assigned less probability than others which are not in the support of the true distribution, is a surprising and compelling result. Leveraging the softmax bottleneck is a clever trick here and one that will be unexpected to most readers in NLP. \n\nI expected BAT to be computationally infeasible to run in practice due to its dependence on an LP-solver at each tilmestep of decoding. However, the speedups in the \"Basis-aware threshold sampling in practice\" (namely, using a decomposition of the softmax matrix and only relying on BAT when a token under the threshold probability is chosen) seem reasonable and compelling, and the amortized cost of 0.1s/token, while slow, is not infeasible for certain classes of applications.\n\nThe experiments, although not particularly compelling as a reason to start using BAT sampling in practice, seem reasonable and sufficiently thorough. In particular, the analysis of performance as more constraints are added back (after the SVD) is very clear. In contrast, I did not find the \"BAT outperforms all other methods for GPT-2-Large\" paragraph very compelling given that BAT is not the best-performing model on any other model size."
                },
                "weaknesses": {
                    "value": "The primary weakness seems to be the performance of BAT compared to other methods. Despite its theoretical justification, it does not clearly outperform other sampling approaches (Figure 5). Although there is a preference for BAT to eta-sampling shown in Figure 6 and Table 1, this preference is very slight and the comparison is only between two sampling methods. However, I do not see this weakness as a legitimate reason to reject the paper, since its main contribution seems to be analysis and theoretical understanding of existing decoding algorithms."
                },
                "questions": {
                    "value": "1. Based on the figures (1,4), it seems like BAT is rejecting a lot of tokens corresponding to partial words. Out of curiosity: is this true, and do you have any insights into why this happens, or other qualitative insights into what tokens tend to get accepted/rejected?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7923/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810542102,
            "cdate": 1698810542102,
            "tmdate": 1699636972955,
            "mdate": 1699636972955,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rYBHZo7A4J",
                "forum": "dONpC9GL1o",
                "replyto": "0EYFL0hNoA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7923/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7923/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your perspective! We worked hard to make our paper clear, both in writing and mathematically. We were equally surprised and excited by the idea of recovering information about the true support using the softmax bottleneck. We put a lot of effort into making BAT computationally feasible and are pleased with the speedups we achieved. We agree that our pilot studies, while not fully convincing as a reason to immediately switch to BAT, offer indications that BAT could prove helpful in certain generations settings and offer some empirical and anecdotal evidence that our proposed method warrants further development and investigation.\n\n> The primary weakness seems to be the performance of BAT compared to other methods. Despite its theoretical justification, it does not clearly outperform other sampling approaches (Figure 5). Although there is a preference for BAT to eta-sampling shown in Figure 6 and Table 1, this preference is very slight and the comparison is only between two sampling methods. However, I do not see this weakness as a legitimate reason to reject the paper, since its main contribution seems to be analysis and theoretical understanding of existing decoding algorithms.\n\nWe agree that our pilot experiments show only preliminary evidence for the effectiveness of our methods, and we are glad that you share our view that these results are supplementary, rather than the primary contribution of our paper, which is the theoretical explanation and mathematically grounded algorithmic proposal.\n\nQuestions:\n\n> Based on the figures (1,4), it seems like BAT is rejecting a lot of tokens corresponding to partial words. Out of curiosity: is this true, and do you have any insights into why this happens, or other qualitative insights into what tokens tend to get accepted/rejected?\n\nThis is a great observation! Our leading hypothesis for why these partial words are rejected (e.g., `Swift` accepted, but `S` and `Sw` rejected) is that they have high embedding similarity with the full word. Tokens whose embeddings (in $W$) have high dot-product similarity with the model\u2019s hidden state receive the most probability mass. In our example, the model output a hidden state with high dot-product similarity with `Swift`, and consequently tokens like `Sw` that have similar embeddings to Swift also have high dot-product similarity with the hidden state, resulting in them getting probability mass.\n\nUsing our method, in order to identify a target token for truncation, it must be possible to transfer all the probability mass on the target token to other tokens while maintaining the constraints of the linear program. The $W^\\top\\hat{p}=W^\\top p$ constraint specifies that the probability mass must be transferred to tokens with similar embeddings to the target token, and the $p_j\\leq\\hat{p}_j\\exp(\\delta)$ constraint limits the amount of mass that can be transferred to any one token, where the higher probability a token is, the more probability mass can be added to it. Therefore, a token is most likely to be truncated if there is another token with high probability and high embedding similarity to it.\nIn our example, the probability mass on `Sw` embedding can be transferred to `Swift` which has high probability and (likely) high embedding similarity."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7923/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700100056078,
                "cdate": 1700100056078,
                "tmdate": 1700100056078,
                "mdate": 1700100056078,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rrgW7V8GfO",
            "forum": "dONpC9GL1o",
            "replyto": "dONpC9GL1o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7923/Reviewer_Hq1N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7923/Reviewer_Hq1N"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a theoretical understanding of truncation sampling methods. The authors proceed to devise a novel sampling strategy, building upon the approximation error incurred by the softmax output bottleneck. Essentially, the idea is to assume that a token has a non-zero probability under the true distribution, not only if it has a non-zero probability under the predicted distribution (which is a source of overestimation errors) but also if it is non-zero under all distributions that \"map back\" to the hidden state by taking the transpose of the output embedding matrix. Intuitively, this measures whether a token has a non-zero probability by chance, i.e. if its information can be conveyed by any combination of other tokens while mapping back to the hidden state. This is formalized in the paper by assuming that the hidden state is the minimizer of the cross-entropy loss with respect to the true distribution. This insight serves to devise a novel sampling strategy that can sample low-probability next tokens, which differs from current approaches that rely on thresholding."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Nice idea and analysis\n- Well written / clear\n- Shines new insights on a well-studied problem and could lead to more promising sampling methods"
                },
                "weaknesses": {
                    "value": "- Results are rather weak, efficacy of the method still remains to be demonstrated (minor)\n- An pseudo-code / algorithm box with the practical implementation of BA is needed in the main paper (minor)\n- Unclear whether the method will help for larger models or for models where the approximation errors (under-estimation / over-estimation) are small (kinda major)."
                },
                "questions": {
                    "value": "Thank you for the efforts in writing a clear and enjoyable paper.\n\nMain questions:\n- Can you include a pseudo-code of the final BA implementation in the main paper?\n\n- Regarding Eq. 3: what I am going to propose is a bit dirty but would it be possible at each step to minimize (W^T p - W^T \\hat p)^2, wrt to p with a sparsity constraint (e.g. l1) and the range constraints, and reject all tokens for which |p| = 0? It might not derive from the theory but it might capture the overall idea? just wondering.\n\n- Concerning the results: there isn't much of a pattern in the MAUVE results if I look at the improvements of BA across model scales. Isn't  BA expected to help more with smaller scales given that the approximation error might be bigger?\n\n- Main problem: what happens with bigger models? given that the approximation error will be smaller, would your method still help?\n\nNitpicks:\n- It might be clearer to re-introduce the \\epsilon and \\eta baselines in the experiments. I struggled a bit to remember given that they are just introduced in the background section.\n- Eq. 10 in the appendix is missing a parenthesis.\n\nI would love to give a 7, but I can't (I have to choose between 6 and 8 now). I will give a 6 for now, and wait for authors responses with the will to increase my scores if more details / addition to the papers are given :-)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7923/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698885764192,
            "cdate": 1698885764192,
            "tmdate": 1699636972859,
            "mdate": 1699636972859,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j281nWZNmC",
                "forum": "dONpC9GL1o",
                "replyto": "rrgW7V8GfO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7923/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7923/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are very grateful for your kind words, as we worked hard to make our writing and math clear, and are very excited about this new perspective and the resulting insights. We hope that we can sufficiently address your concerns.\n\n> Results, efficacy of the method still remains to be demonstrated (minor)\n\nWe agree that our pilot experiment results are largely preliminary and will benefit from further investigation in future work. We take the view that our experimental findings are secondary and meant to supplement our primary theoretical results and method proposal.\n\n> Unclear whether the method will help for larger models or for models where the approximation errors (under-estimation / over-estimation) are small.\n\nOne important implication of our results is that larger models do not automatically avoid under/over-estimation errors, since the errors caused by the softmax bottleneck (SMB) depend not on the model size, but on the discrepancy between the hidden size and the vocabulary size. A larger model with a proportionally larger vocabulary size will suffer the same bottleneck effects. By the same argument, a model with a small vocabulary (i.e., close to equal hidden and vocab sizes) could avoid the SMB altogether, no matter the size. Unfortunately, we do not see many models of this type in common use.\n\nOne size-related investigation we would like to pursue in future work is the hypothesis that our method will become **more** effective for larger model sizes. We hypothesize that some of our results were weaker than expected since our method primarily addresses SMB-related errors. If the model predicts the next token incorrectly, none of the considered truncation methods (including ours) will be able to correct the error. This issue should be reduced with larger models, since larger models should have fewer modeling errors, leaving only SMB errors. We see some anecdotal evidence for this where, for instance, BA-$\\eta$ becomes stronger compared to $\\eta$ sampling as model size increases (Figure 5).\n\n> Regarding Eq. 3: what I am going to propose is a bit dirty but would it be possible at each step to minimize (W^T p - W^T \\hat p)^2, wrt to p with a sparsity constraint (e.g. l1) and the range constraints, and reject all tokens for which |p| = 0? It might not derive from the theory but it might capture the overall idea? just wondering.\n\nThis idea is worth exploring! At first glance, the major difference with this approach is that it would require all truncated tokens to have 0 probability simultaneously, whereas our method only requires a solution with each truncated token having 0 probability independently. The advantage of your proposed method would be a faster runtime, with the drawback that the list of truncated tokens might be incomplete. Perhaps there is a middle ground, where we iteratively find solutions, each time finding multiple tokens to truncate, then removing the sparsity constraint from these tokens for the next iteration so we can find new potentially-0-probability tokens.\n\n> Concerning the results: there isn't much of a pattern in the MAUVE results if I look at the improvements of BA across model scales. Isn't BA expected to help more with smaller scales given that the approximation error might be bigger?\n\nSee our response to your concern above: there may be multiple effects here, such as the rate at which the model makes prediction mistakes independent of the SMB, which would confound the trend because these types of mistakes would become less frequent at larger model sizes.\n\n> What happens with bigger models? given that the approximation error will be smaller, would your method still help?\n\nAgain, we refer to our size-related responses above, with an additional perspective: we expect that small models will continue to play a role in language generation, given the ease of deployment in edge devices, as well as cheaper inference costs. Even if our method does not provide benefits for larger models, smaller models will continue to suffer from a SMB and our method (or some modification of it) will remain relevant.\n\n> Nitpicks: It might be clearer to re-introduce the \\epsilon and \\eta baselines in the experiments. I struggled a bit to remember given that they are just introduced in the background section. Eq. 10 in the appendix is missing a parenthesis. Can you include a pseudo-code of the final BA implementation in the main paper?\n\nThank you for pointing these out, we update accordingly in our revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7923/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700099929385,
                "cdate": 1700099929385,
                "tmdate": 1700099929385,
                "mdate": 1700099929385,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "K1uEotYN5O",
            "forum": "dONpC9GL1o",
            "replyto": "dONpC9GL1o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7923/Reviewer_pgkn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7923/Reviewer_pgkn"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to give a formal justification for why truncation based sampling approaches work well in language generation. They link this phenomenon to the softmax bottleneck\u2014the problem that the final linear layer in a neural network often bottlenecks the expressivity of the model. Explicitly, given the difference between the hidden (embedding) dimension and the vocabulary size, the final linear transformation before the softmax projection can only perform a low-rank projection. The authors claim that the resulting approximation to the target distribution is likely the source of model errors that leads to the \u201cunreliable tail\u201d probabilities observed by prior work. The authors develop an algorithm for uncovering which tokens are necessarily in the support of the target distribution, and propose to use this algorithm as the basis for a truncation sampling method. They provide empirical results (including human evaluations) when using this method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The work offers a theoretical explanation for why certain ad-hoc methods used during language generator decoding work well. This is a valuable insight to the NLG community\n* The work then develops a sampling algorithm based on this theoretical explanation"
                },
                "weaknesses": {
                    "value": "* The theoretical portion of the paper is at times difficult to understand due to notational choices and lack of specificity (for example, switching between individual token probabilities ). This is particularly important since the theoretical portion is the main contribution of the work \n* The method does not appear to have empirical performance benefits and is computationally expensive, making it impractical\n* There lacks robust empirical justification of the hypothesis. Figure 7, which is intended to show that including more of the original optimization problem constraints lead to better results, only consists of 3 points, which hardly feels like enough evidence to claim a \u201ctrend\u201d.\n* The terminology of the \u201ctrue\u201d distribution is perhaps misleading. I personally think that something like the \u201caggregate\u201d distribution or the \u201cdata-generating\u201d distribution would be more accurate\n* A small point: the intro of section 4 has some grammatical errors"
                },
                "questions": {
                    "value": "* Since the matrix W is static, can (3) not be solved for all elements of the vocabulary that meet the desired constraint ahead of time?\n* Why is typical sampling omitted from the discussion of truncation-based sampling methods? It is arguably the most similar to the proposed method since it likewise \u201cis able to discard higher-probability tokens while keeping\u2026 lower-probability tokens.\u201d On a similar note, I don\u2019t understand footnote 3; I don\u2019t think it actually describes what is done by locally typical sampling\n* The phrasing of footnote 1 is strange. Specifically, the statement \u201csetting p\u2217 to be the 1-hot vector indicating the gold token\u201d is underspecified. I imagine that this is referring to the conditional distribution for a particular prefix\n* In theorem 1, these factors are the collective probability over/underestimation across all tokens, right? This then implies that no individual token probability can exceed these bounds. This logic should be made more explicit (the current notation is vague) \n* I don\u2019t think that the softmax function satisfies the additivity property required of a \u201clinear map.\u201d Could you please elaborate on this claim at the top of page 6?\n* On page 5, what is the concrete distinction between low vs. high quality tokens? Is this another way of saying in vs. out of the true distribution? It would be helpful to change the language here to align with the other terminology used by the paper\n* The informal description about the practical implementation of basis-aware sampling is confusing. For example, what does \u201cdiscarding the majority of the constraints\u201d refer to?\n* Given the observation that BAT sampling performs more strongly in lower entropy settings, a logical next step would be to see how it performs in translation or summarization, where historically, sampling has not led to the best results.\n* It is perhaps more accurate to call the projection by W a linear layer instead of softmax layer, since the low-rank approximation is tied to this linear transformation, not the use of the softmax as a projection function. Further, are there insights into how the nature of these results will change with alternative projection functions, like the sparsemax?\n* Can these principles be used to explain the degeneracy that happens when selecting high probability tokens, i.e., during greedy decoding?\n* How do these results align with other work that has tried to explain why truncation methods work well in practice, such as [1]?\n\n[1] Meister et. al. 2023. On the Efficacy of Sampling Adapters."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7923/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7923/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7923/Reviewer_pgkn"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7923/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698999698905,
            "cdate": 1698999698905,
            "tmdate": 1700674001442,
            "mdate": 1700674001442,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0MS6NL5QQP",
                "forum": "dONpC9GL1o",
                "replyto": "K1uEotYN5O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7923/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7923/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your kind words and appreciating our contributions. We appreciate your feedback, insights, and overall engagement with our paper.\n\n> Notational choices \n\nWe strived to follow the ICLR style guidelines for notation, and also to be as consistent and specific as possible. We caught a few notational errors in our revision too. We would appreciate more feedback on which parts are confusing. \n\n> Performance benefits for computational expense\n\nWe address the overhead at the end of Sec. 4: the overall addition is $\\approx0.1$ seconds per token, which is acceptable for many applications. Additionally, this method can be further optimized in the future. \n\n> Figure 7 only 3 points\n\nEach point in Fig 7 is a mean over multiple seeds (fill region indicates standard deviation), which helps establish the trend. While our sampling method could benefit from refinement (see Section 5.2), we contend that our MAUVE experiments when taken together with our qualitative analyses (Figure 4) *do* indicate that the softmax bottleneck is a significant source of model errors, especially in low-entropy generation settings. While BAT does not particularly stand out in Fig 5, no other truncation method is consistently superior either. We believe that this is because high-entropy generations are generally similar across truncation methods.\n\nWe would like to highlight that these empirical analyses are only pilot studies, and that the main contributions of this paper are a theoretical explanation for threshold-based truncation sampling efficacy, and a mathematically-grounded method for mitigating distribution errors from the softmax bottleneck. \n\n> The \u201ctrue\u201d distribution\n\nWe address this in Footnote 1, where we refer to the distribution \u201cfrom which internet text is implicitly sampled\u201d, which we believe is equivalent to the \u201cdata-generating\u201d distribution. \n\n> Solve (3) ahead of time?\n\nThis would be desirable, but we cannot pre-solve since $\\hat{\\boldsymbol{p}}$ is different for each context. Pre-compilation of the program could speed up solving.\n\n> Typical sampling, footnote 3\n\nSee the new Footnote 3. We focus less on typical sampling since it is markedly different from the other truncation schemes in that it does not attempt to recover the support of true distribution.\n\n> The phrasing of footnote 1\n\nWe mean that the model is trained to minimize cross-entropy with a surrogate distribution that puts all probability mass on the gold token. Since this is confusing and not particularly relevant, we remove this.\n\n> Softmax function a \u201clinear map.\u201d\n\nThis is indeed unintuitive. See our newly added Appendix C for a short explanation. For more, see https://golem.ph.utexas.edu/category/2016/06/how_the_simplex_is_a_vector_sp.html.\n\n> Informal description of practical implementation confusing.\n\n\u201cDiscarding the majority of the constraints\u201d means that we use an approximation instead of the full $\\boldsymbol{W}$ matrix to make the linear program faster. See revision.\n\n> BAT for translation or summarization\n\nThis is a great future work direction, which we are pursuing! \n\n> Call the projection by W a linear layer\n\nYou are right about this: the bottleneck has nothing to do with the softmax function and everything to do with the linear projection parameterized by the \u201cembedding\u201d matrix $\\boldsymbol{W}$. Our terminology is for historical reasons (see new Footnote 5).\n\n> Insights into sparsemax?\n\nGreat question! We focus exclusively on the softmax function since it is the most commonly used, but the principles we use **may** be applicable to the sparsemax function, since it preserves some (though not all) the linear dependencies between token embeddings.\n\n> Can these principles explain degeneracy from selecting high probability tokens?\n\nWe suspect that degeneracy in greedy decoding stems from sampling from an unnatural distribution. One possible explanation for why our method does better in low-entropy (close-to-greedy) decoding may be that there are some non-greedy tokens that our method refuses to truncate ever. As an example, in the final frame of Figure 4 most threshold methods continue the repetition, but BAT allows other tokens to be sampled.\n\n> How do these results align with other work, such as (Meister et. al., 2023)?\n\nMeister et al. (2023) approach the question of \u201cwhy do truncation methods work\u201d by hypothesizing that threshold sampling reprioritizes precision over recall. In some respects, our explanation presupposes the Meister et al. hypothesis then proves guarantees on the mechanism by which threshold sampling achieves high precision. Other work [2] has also made the assumption that threshold sampling aims to avoid zero-probability tokens. We add Meister et al. to our revision.\n\n> Other minor comments\n\nSee revision.\n\n[1] Yang et al., Breaking the Softmax Bottleneck: A High-rank RNN Language Model.\n[2] Hewitt et al. Truncation sampling as language model desmoothing."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7923/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700099774513,
                "cdate": 1700099774513,
                "tmdate": 1700519798553,
                "mdate": 1700519798553,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wFBGA6Y2qk",
                "forum": "dONpC9GL1o",
                "replyto": "0MS6NL5QQP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7923/Reviewer_pgkn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7923/Reviewer_pgkn"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for the clarifications! Several of my concerns are addressed. I have raised my score to an 8 accordingly."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7923/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674091196,
                "cdate": 1700674091196,
                "tmdate": 1700674091196,
                "mdate": 1700674091196,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]