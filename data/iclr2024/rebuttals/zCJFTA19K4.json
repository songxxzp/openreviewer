[
    {
        "title": "Token Alignment via Character Matching for Subword Completion"
    },
    {
        "review": {
            "id": "d4qwD8BMVo",
            "forum": "zCJFTA19K4",
            "replyto": "zCJFTA19K4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3921/Reviewer_jqrQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3921/Reviewer_jqrQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper explored handling partially labeled complex tasks in generative models. Through token alignment, improvements in various scenarios were demonstrated."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed token alignment method could be combined with multiple techniques, such as subword regularization. \n\nThe method presented was meaningful for code completion and text output directions."
                },
                "weaknesses": {
                    "value": "In terms of prefix-indentation splitting, the results of the proposed method still lagged behind the baseline. \n\nThe method introduced a certain time delay. \n\nPrompts affected the results."
                },
                "questions": {
                    "value": "1. The outputs of LLMs were uncertain. Even a minor change in a prompt could lead to variations in the output. During the use of prompts in the paper, was the specific impact of the prompt considered?\n\n2. Given the powerful In-Context Learning capabilities of large language models, it would be worth exploring whether adding relevant knowledge to the prompt could further enhance the proposed method.\n\n3. Was there any consideration that an excessive amount of code data in the dataset might dilute the pre-trained knowledge in LLMs, impairing code generation capabilities?\n\n4. How did the proposed method perform in multilingual or cross-lingual scenarios?\n\n5. The paper mentioned \"token healing.\" Could there be a detailed comparison between \"token alignment\" and \"token healing\" in terms of performance across different datasets and application scenarios? What were the pros and cons of these two methods when dealing with partially labeled issues?\n\n6. In which areas did \"token alignment\" excel? Were there scenarios or applications where other methods might be more appropriate?\n\n7. For long texts or texts exceeding the model's maximum input length, did the \"token alignment\" method maintain its effectiveness? In such cases, was there a need to adjust or optimize the method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3921/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3921/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3921/Reviewer_jqrQ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3921/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698689110357,
            "cdate": 1698689110357,
            "tmdate": 1699636352385,
            "mdate": 1699636352385,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VSMz8rxWUZ",
                "forum": "zCJFTA19K4",
                "replyto": "d4qwD8BMVo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3921/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3921/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jqrQ"
                    },
                    "comment": {
                        "value": "Thank you for the reviewer for your comment and thoughtful questions.\n\n\nAddressing questions:\n1. This is a good point. We use the baseline datasets where we pre-backtrack so that the prompt does not contain partial token. Therefore, the difference between the baseline and non baseline is as low as possible, in order to control the effect of the prompt. Moreover, we average the results over many curated datasets to understand the aggregate behavior.\n\n2. Good point. One way to use in-context learning is to make the model repeat the entire input and generate full tokens that match the partial tokens instead. However, if the input is long, this will amount to high extra latency. Therefore, we do not explicitly consider this approach. \n\nIt is possible that we do in-context learning in such a way that makes the model output only a snippet shortly before the end of the prompt. Such method is likely quite complicated and requires processing to extract out the right portion. \n\nOur method presents a clean way to align tokens natively by guiding the token selection with the probabilities that the model already predicts, but masking out tokens that do not align. We believe that this is the cleanest and simplest approach.\n\n3. We would likely ask the reviewer to rephrase the question. The datasets that we used are only for evaluation. We take publicly pretrained models to do such evaluation.\n\n4. the effectiveness of token alignment itself is independent of the cross or multi-lingual setting, holding other factors constant. If cross lingual entails rare language, it would require models to have seen sufficient amount of such language to be able to generalize, even with full token scenario. Given such cases where models can output meaningful probability distribution over tokens in each step, our method helps when there is a presence of partial token.\n\nRare language may entail higher backtracking B. In such case, a tokenizer schema where we know which token corresponds to the beginning of pre-token can be helpful.\n\n5. Token healing is a similar concurrent approach that is a concurrent work in terms of blog post (not a full paper).\n\n6. This is a great question. The token alignment is very suitable for 'completion' mode. For instance, if we use language models to help write word document, or write code within the code editor. This is in contrast to the Question - Answer mode where the model does not have to output text that is a continuation of the prompt, but would perhaps start in a different line and start a new sentence / piece of code instead.\n\n7. Our method works for different input length. In short, if using the model without token alignment can support L input tokens, using token alignment also supports ~ L input tokens as well (give or take the number of backtrack tokens B where we use around 1-3, as demonstrated in the paper)\n\nThank you again for the thoughtful comments. We will incorporate it in the draft accordingly."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702126876,
                "cdate": 1700702126876,
                "tmdate": 1700704690514,
                "mdate": 1700704690514,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qAcCay87If",
            "forum": "zCJFTA19K4",
            "replyto": "zCJFTA19K4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3921/Reviewer_kAYh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3921/Reviewer_kAYh"
            ],
            "content": {
                "summary": {
                    "value": "This paper deals with problems arising from LLM prompts ending with incomplete subword tokens. This is an issue for use cases involving autocompletion, such as code generation. The authors propose a new decoding algorithm to handle partial tokens. The algorithm backtracks to a previous full token, then decodes the subsequent token while limited to only generating tokens that start with the partial token. This enables the model to use next-token predictions for complete tokens within its vocabulary, while ensuring that the resulting model output still aligns with the partially generated user input. The authors outline several classes of common partial token occurrences (e.g. natural language subwords, space prefixes, contiguous spaces in code) and construct evaluation data for testing generation from partial tokens for each class. They show that their backtracking algorithm improves performance across natural language and code generation datasets, with limited increased latency during decoding."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**-- 1. The problem of partial tokens is important and neglected \u2013**\n\nThe authors have highlighted an important issue with this work. The effect of tokenisation on recent LLMs has not been sufficiently studied, and partial tokens might well be an issue for common LLM use cases like code generation. \n\n**-- 2. Method is simple and effective \u2013**\n\nThe token alignment algorithm is an intuitive solution to the studied problem. It is easy to implement for existing LLMs, so it could realistically be used and adapted by practitioners. The reported results also look very good (large gains on some of the datasets), so I do see this work being useful for future research.\n\n**-- 3. Useful categorisation of partial token scenarios \u2013**\n\nThe authors outline a categorisation of the different types of common partial token errors. This in itself is a useful contribution, as it highlights the real-world use cases that cause problems."
                },
                "weaknesses": {
                    "value": "**-- 1. Uncertainty around backtracking steps --**\n\nThe paper is somewhat unclear about the method of backtracking multiple tokens (B in Algorithm 1), so I would ask the authors to clarify this. The introduction mentions \u201cOur approach involves backtracking to the last complete tokens\u201d, which is how I understood the method initially. But later it is suggested that the method backtracks multiple tokens (B = 3 tokens), and not just one token back.\n\nI am not sure why backtracking 3 tokens would work better than 1, as found in the ablation study. Wouldn\u2019t that give the model less context unnecessarily, given that 1 token back is all we need to get to a complete token? \n\n**-- 2. More details on increased latency --**\n\nThe paper would be improved by a more detailed explanation of the increased computational complexity introduced by token alignment. I appreciate that the authors have included a whole subsection on this topic, but I would suggest shifting the focus of the subsection to matters that are practically informative. What is the average added latency, in ms and percentage? This is mentioned, but the authors should include the relevant hardware details for reference as well. Increased latency would be a primary real world concern for practitioners.\n\n**-- 3. Framing of the problem --**\n\nThe problem of partial tokens could be presented more accurately in some instances. For example, at the end of page 3 the authors suggest that subword tokens of linguistic words like \u201cbanana\u201d (\u201cbanan\u201d or \u201cbana\u201d) could lead to issues. I think it would be best to cite work on this, or prove this experimentally? Linguistically unsound subword tokenisations are present in every LLM, yet they work well in most cases, presumably because there is enough data for models to learn how these subwords combine to form words (e.g. on https://platform.openai.com/tokenizer the word \u201cbananas\u201d is segmented \u201cban-anas\u201d.)\n\nThe reported results could be viewed as proof that this is an issue, but for that it would be useful to include more examples of where token alignment helps e.g. can models autocomplete \u201cbana\u201d as \u201cbananas\u201d given sufficient context? Does token alignment allow them to do this?\n\nThe same holds for the other partial token categories."
                },
                "questions": {
                    "value": "**-- Questions --**\n\n1. How does your work differ from this paper? https://aclanthology.org/D19-1507.pdf It seems like they are using the same backtracking algorithm (see Section 4), but testing it with different models and datasets.\n2. Will you release your evaluation datasets publicly?\n3. Section 2, end of paragraph 1. How does subword regularisation increase inference latency?\n4. What model\u2019s tokeniser was used for the examples in Figures 1-3? If these examples were constructed by hand, this should be mentioned.\n\n**-- Typos: --**\n\n1. Tables 1-3 have incorrectly boldfaced results that are lower than comparable results (e.g. Table 1, all 3 baselines results for LLaMA with token alignment are incorrectly bold).\n2. Section 1, paragraph 3: \u2019incomplete token \u201csys\u201d in Figure 2\u2019 -> \u2019incomplete token \u201cre\u201d in Figure 2\u2019 \n3. Section 1, paragraph 4 fix unclear wording: \u201cwith an average increase of only 3-7 ms for using token alignment, in addition to the number of backtracked tokens\u201d.\n4. Section 3, paragraph 2: fix notation \u201cwhere N is the number of tokens we need to backtrack\u201d -> \u201cwhere B is the number of tokens we need to backtrack\u201d.\n5. Section 3, paragraph 4: \u201cto avoid unnecessary the trie lookup\u201d -> \u201cby avoiding unnecessary trie lookups\u201d.\n6. Section 4, paragraph 3: \u201cit is general quite hard\u201d -> \u201cit is generally quite hard\u201d.\n7. Fix last sentence on page 4: \u201cthe model always obvious contiguous\u201d.\n8. Section 5, paragraph 1: fix \u201cfor each case described in Section 4 show\u201d\n9. Section 5, paragraph 1: \u201chandles the constrain due to all such cases\u201d -> \u201chandles the constraint\u2026\u201d\n10. Section 5, paragraph 2: fix unclear wording \u201cprocessing publicly available datasets to their corresponding variants\u201d.\n11. Section 5.2.1, paragraph 3: \u201ctoken alignment can be use in all cases\u201d -> \u201ct token alignment can be used in all cases\u201d.\n12. Section 6.2, paragraph 2: fix unclear wording \u201copportunities to mark some token as beginning of pretoken as building time\u201d."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3921/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3921/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3921/Reviewer_kAYh"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3921/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762684210,
            "cdate": 1698762684210,
            "tmdate": 1700726594837,
            "mdate": 1700726594837,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6lmOCttpMZ",
                "forum": "zCJFTA19K4",
                "replyto": "qAcCay87If",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3921/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3921/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kAYh"
                    },
                    "comment": {
                        "value": "Thank you for your detailed comments and feedback. It is clear that the reviewer understands the paper deeply and have spent time to read our work -- we truly appreciate it!!\n\nAddressing Strengths:\n- Thank you for acknowledging that the problem of partial token is important but largely neglected due to the artifact of most evaluation setup. \n- Thank you for recognizing the effort on categorizing the partial token scenarios. Some scenarios are highly not obvious and we make effort to select out these cases for surgical studies of the language models' behavior.\n\nAddressing Weaknesses:\n1. This is a very good question that we will emphasize further to make it clear! For different tokenizers, it can be the case that there can be no 'marker' if that token is the start of a pre token. In this case, B needs to be pre-set to be a certain number. However, in some tokenizer, such as SentencePiece, we can have such marker for pre-token, which allows us to backtrack with different B dynamically. (B can be 1 for some input and 2 or other inputs)\n\nBacktracking 3 tokens can be better in certain case. For example, if we have a very long token that gets broken up into partial tokens. For instance, if the full token is \"abcdefghijk\" (synthetic example here) but the prompt is \"abcdefgh\" -- instead of being a full token, this prompt likely gets broken up into something like 'abcd', 'ef', 'gh'. If we do not roll back for 3 steps in this case, then the prompt that the model sees still corresponds to partial tokens that got broken up. This would still constrain the model due to the artifact of tokenization.\n\nHowever, if we correctly rolls back to before \"abcdefgh\", then the model can see that the token \"abcdefghijk\" matches in prefix with the text \"abcdefgh\" -- so our token alignment method will do the job correctly. \n\nThat being said, B=3 typically is not required for most prompts, but it does not hurt to roll back in order to a full token that may be broken into up to 3 tokens as well. However, as the reviewer nicely pointed out, it gives the model less context. Therefore, it is a balance between giving the model context and precise alignment. The alignment process in itself may be seen as guiding the model in a way, however.\n\n2. This is a good point. We initially focus on the number of additional tokens since it will hold even though in the future the decoding process itself can be much faster, relative to the input processing latency etc. For instance, speculative decoding can make the latency per token much faster, which results different additional latency (lower) compared to without using speculative decoding. \n\nFor instance, on A100 GPU, for a 2000-token context input takes around ~400 ms for a moderate size model (7B LlaMA for instance). Generating 256 tokens without speculative decoding can take 256*30 ms. The overhead of token alignment with B=3 will be around 3*30 ms. In total, the overhead is 90/(400 + 256*30) = 1.1%\n\n3. Thank you for pointing this out. We will add additional examples to make it clear. For the current draft, examples given in Figure 1, 2, 3 and in Section 4 illustrates various scenarios. For example, in figure 3, \"_ _ _\" + \"_ return\" are the fully tokens corresponding to 4 spaces + \"return\". In the scenario where the model is presented to \" _ _ _ _\" as the end of a prompt, then it constitutes a partial token issue. \n\nIt is true that unsound subword tokenization are prevalent. However, once the prompt breaks from the patterns that the models observe during training, then problem arises. Such discrepancies happens when the end of the prompt is cut off before a full token. \nFor instance, 'banana' could be composed of two tokens 'ban' and 'ana'. If the prompt ends with \"monkeys eating yellow ban\", the the most likely next token is 'ana' and there is nothing preventing the model to predict such a token. \n\nHowever, if the prompt ends with \"monkeys eating yellow bana\" and the models have seen 'ban' + 'ana' for every semantic instance of 'banana' during training, then the model will most likely not predict 'na' after this prompt.\n\nAddressing questions:\n1. Thank you for pointing out this work.  The paper mentioned uses beam search together with approximate marginalization. For modern language models, beam search is not typically used but popular methods are top-p (nucleus sampling) and top-k. Our proposed approach is designed to be compatible with top-p and top-k and works seamlessly with such sampling approaches. We will add this to related work and contrast the difference, however. Thank you again!\n\n2. Yes.\n3. Subword regularization breaks up a piece of text into more tokens and show this to the model during training time. The model then has higher chance of using higher number of tokens to produce the same text, compared to without subword regularization.\n4. These are constructed by hands. We will point it out.\n\nThank you for pointing out the typos. We are extremely grateful for the detailed list!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701280145,
                "cdate": 1700701280145,
                "tmdate": 1700708250849,
                "mdate": 1700708250849,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RIlY9zGKry",
                "forum": "zCJFTA19K4",
                "replyto": "6lmOCttpMZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3921/Reviewer_kAYh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3921/Reviewer_kAYh"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for responding to my queries. Including your clarifications in the paper will certainly strengthen the work. Your comments clarifying the multi-step backtracking are especially useful. I have increased my overall rating of the paper, as well as the contribution score.\n\nA few comments on your response:\n\n**-- Weakness #3: framing the problem of partial tokens --**\n\nI understand your reasoning around this, but I still feel it would be worth further exploring the problem itself to a greater extent. \n\n> However, if the prompt ends with \"monkeys eating yellow bana\" and the models have seen 'ban' + 'ana' for every semantic instance of 'banana' during training, then the model will most likely not predict 'na' after this prompt.\n\nThis could be true, but LLMs trained on enough data are also exposed to a lot of noise in the training data (e.g. spelling variations, misspellings, incorrect spacings) that they could potentially learn to deal with something like a partial token. Previous work (https://arxiv.org/pdf/2206.02608.pdf) has shown that some models actually learn which characters their tokens consist of. Such knowledge could enable models to still work with partial tokens.\n\nThe contributions of the work would be stronger if it included an exploration of the problem before trying to fix it.\n\n**-- Subword regularisation latency --**\n\nDepending on the subword regulariser, they do not necessarily introduce more tokens. For example, sampling from ULM could lead to less/more segmentation, depending on the sample."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726574793,
                "cdate": 1700726574793,
                "tmdate": 1700726574793,
                "mdate": 1700726574793,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ROOCoIps1O",
            "forum": "zCJFTA19K4",
            "replyto": "zCJFTA19K4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3921/Reviewer_fPS1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3921/Reviewer_fPS1"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method for text completion in large language models (LLM) from incomplete tokens in prompts. The method uses an algorithm to backtrack generated tokens for the completion of sub-words.  The main contributions are: i) method for identification and processing of incomplete tokens, and ii) comparison of the proposed method with byte-pair-encoding (BPE) baselines on code and natural language processing (NLP) benchmarks. The proposed method shows competitive results compared to the baseline."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed method  tackles issues with the text completion in LLM with a small latency overhead.\n- Clear description of the proposed approach.  \n- The authors perform a comparison of the proposed approach on code and NLP benchmarks."
                },
                "weaknesses": {
                    "value": "- It is not clearly described the background knowledge needed to motivate and position the proposed method in the literature.\n- It is not clearly described the alignment task and the relation to code and NLP.\n- A possible extra contribution can be the addition of a statistical significant test of the results."
                },
                "questions": {
                    "value": "Please address the following questions during the rebuttal:\n\n- Please elaborate in the background used to develop the proposed method.\n- Is the alignment task based on fine-tuning a model or in-context-learning? Could you elaborate more on differences and benefits, compared to your method. \n- Could you elaborate on the selection and importance of hyper-parameters? such as backtracking B.\nIs the selected baseline a strong proposal compared to other related work on fine/instruction tuning or character-based models? \n\nExtra:\n\nPlease add related-work/literature context to the introduction and methodology sections"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I have no concerns."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3921/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3921/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3921/Reviewer_fPS1"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3921/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698774650129,
            "cdate": 1698774650129,
            "tmdate": 1700730790352,
            "mdate": 1700730790352,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9NYhPgsXfz",
                "forum": "zCJFTA19K4",
                "replyto": "ROOCoIps1O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3921/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3921/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fPS1"
                    },
                    "comment": {
                        "value": "We thank reviewer for your time and for the questions/comments.\n\n- Thank you for acknowledging the strengths of the paper. \nAddressing Weakness comments:\n- We describe the motivation in the introduction as well as accompanying figures where if prompt gets tokenized where the last token corresponds to a partial token, this corresponds to mismatch between training time versus inference time. The scores where we use token alignment versus without using demonstrates that partial token can hurt language model's generation abilities significantly.\n- The token alignment task is applicable to *any* text completion task -- we illustrate examples in Figure 1 and 3 and provide experiments for both text and code throughout the paper.\n- We make note on the statistical significant -- due to the scale of the evaluation we do not have them by default in this draft.\n\nAddressing questions:\n- The token alignment is applicable for both pretrained and finetuned model. In our case, we use pretrained model. We also experimented with few-shot prompting which is the 'in-context learning' scenario (Table 3).\n- We experimented with different values of B (number of backtrack tokens) -- Table 6 in appendix illustrates our extensive result. For the main paper, we use B=3. For B=1 or 2, we observe small differences.\n\nWe will aim to clarify the paper further based on the suggestions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698500724,
                "cdate": 1700698500724,
                "tmdate": 1700698500724,
                "mdate": 1700698500724,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2vuxtfKG8S",
                "forum": "zCJFTA19K4",
                "replyto": "9NYhPgsXfz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3921/Reviewer_fPS1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3921/Reviewer_fPS1"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my questions. I have no further comments."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730710002,
                "cdate": 1700730710002,
                "tmdate": 1700730710002,
                "mdate": 1700730710002,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IYzuf9di0M",
            "forum": "zCJFTA19K4",
            "replyto": "zCJFTA19K4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3921/Reviewer_dd8B"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3921/Reviewer_dd8B"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a token alignment model to apply in auto-completion task. The proposed method is described well and tested in the code generation task. There are some improvements recommended due to connection to literature and showing evidence that there is a problem that the paper addresses beyond the application of the LMs in code generation. The authors are referred to decoding literature in LM and sequence models in previous tasks for a fair comparison."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Interesting approach to improve efficiency in code generation."
                },
                "weaknesses": {
                    "value": "The main problem of this paper is that there is no evidence or support that this problem exists, and caused by subword misalignment. \nRelated work do not exactly align with the problem studied and there is no discussion why authors do not find evidence on the problem they choose to address.\nNovelty of the proposed model is not clear."
                },
                "questions": {
                    "value": "Can the authors find or add in their study evidence through actual experiments on the problem they propose to solve?\nThere are many variants of beam search that could do hierarchical subword/word generation. How do this approach differ from the existing methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3921/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698855586332,
            "cdate": 1698855586332,
            "tmdate": 1699636352118,
            "mdate": 1699636352118,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vfyvXPkdKL",
                "forum": "zCJFTA19K4",
                "replyto": "IYzuf9di0M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3921/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3921/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dd8B"
                    },
                    "comment": {
                        "value": "We thank you for your comment and questions.\n\nRegarding the necessity of token alignment, we have numerous results showing that without token alignment, when prompt ends with partial token in all the four scenarios we categorized (subword, punctuation, white spaces due to space prefix and contiguous spaces), the drops dropped drastically without a method such as token alignment to help alleviate the tokenization artifact. \n\nWe also have provided illustrations in Figure 2 and Figure 3. \n\nA concurrent work in terms of blog post also studies this exact topic, for instance, where our method is similar. (we cited this blog post in the paper as well under Related Work)\n\nOur work do not use beam search and is designed to work with the existing approach of incremental decoding in language model which is nucleus (top-p) sampling as well as top-k. \n\nPrompt boundaries and token healing, 2023. URL https://towardsdatascience.com/the-art-of-prompt-design-prompt-boundaries-and-token-healing-3b2448b0be38"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3921/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694353759,
                "cdate": 1700694353759,
                "tmdate": 1700705334647,
                "mdate": 1700705334647,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]