[
    {
        "title": "Catastrophic Negative Transfer: An Overlooked Problem in Continual Reinforcement Learning"
    },
    {
        "review": {
            "id": "65jeoNOeWh",
            "forum": "o7BwUyXz1f",
            "replyto": "o7BwUyXz1f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2524/Reviewer_p3gB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2524/Reviewer_p3gB"
            ],
            "content": {
                "summary": {
                    "value": "The submission studies the problem of negative transfer between tasks in a continual RL (CRL) setting. The authors posit that this issue is distinct from the plasticity/stability trade-off studied in the majority of the CRL literature. The manuscript contains an initial evaluation on pairs of Meta-World tasks that demonstrates that indeed learning a new task might be negatively affected because of the learning of a previous task. Then a new approach is proposed to simply re-initialize the network upon training each new task and then distill knowledge from the trained network into a shared network trained across all tasks. An evaluation on Meta-World tasks demonstrates that the approach avoids negative transfer."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "######## Strengths ########\n- The proposed method is clearly explained\n- The evaluation on Meta-World demonstrates that the method is capable of learning complex tasks\n- The empirical evaluation covers multiple baselines, sequences of tasks, and a few ablation studies"
                },
                "weaknesses": {
                    "value": "######## Weaknesses ########\n- The problem of negative transfer between RL tasks has been well known for a long time -- though not as extensively explored, indeed\n- The proposed solution is underwhelming -- in order to avoid negative transfer, the approach also rids itself of the possibility of achieving positive transfer\n- The most closely related approach, Progress & Compress (P&C) was apparently misunderstood by the authors, leading to a misleading evaluation\n- There is no distinct related work section\n\n######## Recommendation ########\n\nUnfortunately, I recommend that this manuscript is not accepted in its current form. While I agree with the authors that negative transfer has been understudied in continual RL, the phenomenon has been well known for a long time (e.g., [1]). A paper in this space should provide some novel insight or understanding of the problem, which I don't believe the current submission provides. The proposed solution, instead of seeking to avoid negative transfer while maintaining the possibility to achieve positive transfer, simply trains _without_ transfer, trivially sidestepping the problem (and the advantages of positive transfer). Critically, there is a fundamental flaw in the authors' understanding and implementation of P&C (and extensions to P&C), which I discuss in detail below. This error makes P&C unsuitable for avoiding negative transfer, which is in fact an issue that the P&C authors discussed in their original paper in 2018. \n\n######## Arguments ########\n- It is unclear what is the novelty of the setting studied in this work\n    - The problem of negative transfer in RL is well known. Indeed, in the continual setting it has been somewhat tied with plasticity/capacity (probably incorrectly)\n    - The authors' discussion of how it differs from these other issues is an interesting first step, but fails to provide any clear insight\n    - What is the reason for this negative transfer? When does it happen? How can we avoid it?\n    - There is a recent treatment of a very similar issue in [2] which the authors fail to reference in their work. In [2], the authors explicitly study one particular setting where negative transfer occurs: when two tasks require executing different actions in the same state. \n    - It's unclear why the results of Figure 2 cannot be explained via loss of capacity/plasticity. While I do agree that that's probably not the cause (because finetuning doesn't restrict capacity or plasticity), I don't believe there's a clear enough argument in the result to clearly establish that claim. \n- The solution doesn't really address the problem, but sidesteps it\n    - The problem the authors are trying to address is that, when attempting to transfer knowledge from one task to the next, sometimes that leads to negative transfer\n    - Instead of aiming to determine when this is the case or somehow prevent it from happening, the proposed solution simply _doesn't_ transfer any knowledge across tasks. Trivially, this solution avoids negative transfer -- but it also avoids positive transfer! \n    - I encourage the authors to continue down this path, and seek to develop a solution that maintains the benefits of positive transfer when available, and avoids negative transfer when it would occur.\n- There is a critical flaw in the authors' implementation of P&C\n    - The authors claim that P&C is restricted to \"one active column... without any re-initialization.\" Quoting from the original P&C paper: \"Note that one could make this phase similar to naive finetuning of a network trained on previous tasks by not resetting the active column or adaptors upon the introduction of a new task. Empirically, we found that this can improve positive transfer when tasks are very similar. For more diverse tasks however, we recommend re-initialising these parameters, which can make learning more successful.\"\n    - The first implication of this is that the authors' evaluation of P&C is incorrect.\n    - The second, and perhaps more important, is that P&C already explicitly addressed the issue of negative transfer 5 years ago. Of course, P&C is not an end-all solution, but it clearly is a mechanism that seeks to avoid negative transfer, and they do this while still maintaining positive forward transfer.\n    - Moreover, while indeed P&C originally didn't use replay, it is a trivial adaptation of P&C, which has already been successfully tried (Mendez et al., 2022).\n- There is no distinct related work section. While the authors do a somewhat complete treatment scattered throughout the paper of continual learning and CRL approaches, they miss any mention to existing studies of negative transfer (e.g., [1,2]).\n\nAs one additional point, the choice of tasks that can be trained within 3 million steps is especially well suited for an algorithm that achieves no forward transfer -- it's trained only on the easy tasks, which we can learn by definition with no forward transfer.\n\n\n[1] Taylor & Stone, \"Transfer Learning for Reinforcement Learning Domains: A Survey.\" JMLR, 2009.\n\n[2] Kessler et al., \"Same State, Different Task: Continual Reinforcement Learning without Interference.\" AAAI, 2022."
                },
                "questions": {
                    "value": "######## Additional feedback ########\n\nThe following points are provided as feedback to hopefully help better shape the submitted manuscript, but did not impact my recommendation in a major way.\n\n\nIntro\n- I question the claim toward the end of the intro that says that because the agent can still learn window-close, then it's not due to loss of capacity\n    - First, are we just doing finetuning? Then why would there be a loss of capacity?\n    - Second, maybe window close is more \"compatible\" with sweep-into and therefore doesn't require additional capacity -- it somehow \"fits in the same space\"\n- I still think this roughly boils down to the idea of Kessler et al., where it's not possible for a single model to perform well on two tasks that require \"opposite\" behaviors given the same state. I hoped to see some discussion about this. \n\nSec 2\n- The authors should clearly anticipate in the intro and in Sec 2.2 why behavior cloning is mentioned at all. When I got here I assumed it was to do something similar to Wolczyk et al., but was confused about why this wasn't mentioned before. \n\nSec 3\n- The experimental setting for Figure 2 is not very clear. \n    - Why are the first words supposed to denote whether tasks are similar? For example button press is a lot more similar to coffee button than to button press topdown wall\n    - Are the 8 groups all size 3 or are they different sizes? Do they all have at least 2 tasks?\n    - I think what confused me was that the authors stated that they used _all_ to refer to groups and not tasks, while I was expecting the evaluation to be over all pairs of _tasks_. Perhaps a bit of rephrasing could help make this clearer. \n- How should we interpret the results over 13 tasks? Does this mean training 13 tasks in sequence? How are they chosen, why...?\n\nSec 5\n- Figure 3/5 seems to only show the performance of the offline network. for R&D. If that's the case, then the curves should look like \"steps\" -- there's no reward improvement on during the training of each individual task, and then there's a jump at the end when the online model is distilled. \n- Do Figure 3/5 show the performance averaged across all tasks so far or only the current task?\n- The use of \"long task sequence\" throughout the paper to refer to sequences of 8 tasks is quite underwhelming.\n- \"To check that either CReLU or InFeR can be compatible to the CL baseline\" -- what does this mean?\n\nThere is no related work section \n\nTypos/style/grammar\n- Final paragraph of Sec 1 -- section --> Section\n- The citation format seems odd. Why is there a parenthesis within each parenthetical citation for the year? \n- Sec 3, paragraph 2 -- single task --> single-task"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2524/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698347949453,
            "cdate": 1698347949453,
            "tmdate": 1699636189081,
            "mdate": 1699636189081,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "M5MePe37SE",
                "forum": "o7BwUyXz1f",
                "replyto": "65jeoNOeWh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2524/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2524/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments for Reviewer p3gB (Part I)"
                    },
                    "comment": {
                        "value": "### Weakness 1: The novelty of the negative transfer problem in CRL\n\n- As we already mentioned in the Introduction section, we agree that the negative transfer has been studied in a similar way like loss of plasticity or capacity loss. However, our experiments have shown that a proper solution for resolving the negative transfer in CRL setting has not been proposed in other works. Either CReLU or InFeR cannot solve this problem, and we stress that this problem cannot be explained via plasticity or capacity loss. Furthermore, as reviewer p3gB mentioned, this problem is not extensively explored in previous methods, and we think that we extensively explored the negative transfer problem in Meta-World environment.\n- For the more in-depth analysis about the distinction between negative transfer and capacity/plasticity loss, please refer to the global comment.\n\n### Weakness 2: The proposed method just sidesteps the negative transfer problem\n\nAs mentioned in the conclusion, we agree that our method is not capable of positive transfer. However, addressing negative transfer is a prerequisite before contemplating positive transfer. Despite previous studies, even in the case of P&C, attempting to achieve positive transfer, our experiments revealed that such efforts did not effectively tackle negative transfer. We have demonstrated that our method outperforms existing baselines by solely addressing negative transfer, without explicitly incorporating positive transfer in a straightforward manner. Hence, we assert that our contribution lies in this aspect.\n\n\n### Weakness 3: There is a critical flaw in the implementation of P&C\n\nWe acknowledge that we were not aware of the reference to a reset in P&C. We have further experimented with the implementation of P&C. These results are available for review in the Supplementary Materials of the revised manuscript. Our conclusion is that we cannot agree that P&C solves the negative transfer issue. In our experimental setup, we implemented the algorithm proposed by P&C as it is, attaching an adaptor to the policy and receiving features from the knowledge base policy(`policy_kb`). When learning a new task, we reset the policy and the Q function (or the value function), and proceeded with learning again. Note that when learning second task, the adaptor is trained from randomly initialized network, and the third task is learned with pre-trained adaptor from the second task. In the result, we can see that the task is not learned at all. We think this is due to the part where the information from `policy_kb` is passed through the adaptor. It might seem that applying resetting to P&C would solve the negative transfer, but in our experiments, we found that it did not solve the negative transfer at all. Therefore, we can see that P&C is not able to maintain positive transfers and is also adversely affected by negative transfers.\n\n\n### Weakness 4: There is no distinct Related Work section\n\nOur intention was to have the Introduction section serve as an introduction to the related work, but this seems to have been confusing. We will add a new Related Work section to describe more about works such as transfer learning later on.\n\n\n### Weakness 5: The choice of tasks that can be trained within 3 million steps is well suited only for an algorithm that achieves no forward transfer\n\nWe focused our experiments on the occurrence and resolution of negative transfers. We excluded tasks that were not learned within 3 minutes of the CRL because they would not allow us to see the extent of negative transfer. Though it would be better to consider more hard and complex tasks to show the forward transfer in our experiment, we think that resolving the negative transfer is our major challenge, and achieving the forward transfer remained as future work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2524/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736681897,
                "cdate": 1700736681897,
                "tmdate": 1700736681897,
                "mdate": 1700736681897,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wq7bawA8u9",
            "forum": "o7BwUyXz1f",
            "replyto": "o7BwUyXz1f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2524/Reviewer_oDoz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2524/Reviewer_oDoz"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on continual reinforcement learning (CRL) and negative transfer. They show using Meta-World RL environments where the negative transfer occurs. For negative transfer, they delve into severe plasticity degradation depending on the learned task sequence. With this negative transfer phenomenon they propose an effective method denoted as Reset & Distill (R&D) thereby mitigating negative transfer and forgetting. For negative transfer they use MetaWorld and utilize SAC and PPO to show the negative transfer scenario in 8 task groups. In Table 1, they showcase how their method R&D has the lowest negative transfer and forgetting while comparing among fine-tuning, EWC, P&C, ClonEx, ClonEx+CReLU, and ClonEx+InFeR."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Great amount of experiments and a good amount of baselines to show why your method performs better than the others. In addition, an interesting set of experiments especially with the longer sequence. Plus you have three different settings: easy, hard, and random to simulate different varying conditions.\n\nThe introduction is quite motivating and makes it quite easy to understand the motivation. Plus you provide a plethora of previous works and mention what you are particularly working on in terms of negative transfer. For Figure 1, that is a good example to show how the negative transfer happens and makes it very easy to understand why you are wanting to work on this problem, much kudos."
                },
                "weaknesses": {
                    "value": "Experiments:\nFor the experiments with CReLU or InFeR, why is R&D not compared, it would help bolster your method if in both scenarios R&D shows benefit. If not, it would be helpful to state why R&D is not put here. I can understand from logic, you state that it is not helpful. Yet, you used R&D, in a separate application, so it feels like there is a missing component since you used R&D in a different application.\n\nWriting & Visualization:\nFor Figures 3 and 5, it may benefit to have each SAC and PPO separate. I understand that the paper limit is a hard problem. Consider in the supplementary material, to have each SAC and PPO plots in there so it makes it easier to see the separation. \n\nFor some of the captions like Table 1 or Figure 6, you state the information but it would be helpful to provide a conclusion of the results so it can spell to the reader what the message of the figure or table is.\n\nAlso in the second paragraph you mention that you ran 10 different random seeds,  how is that different from Figure 2 where you mentioned 3 seeds? May you please provide more details into it?\n\nIn 7.3 you mention statistical significance but where are the p-values?"
                },
                "questions": {
                    "value": "Please refer to the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2524/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2524/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2524/Reviewer_oDoz"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2524/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698378955190,
            "cdate": 1698378955190,
            "tmdate": 1699636189003,
            "mdate": 1699636189003,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kOn1wz26i2",
                "forum": "o7BwUyXz1f",
                "replyto": "wq7bawA8u9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2524/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2524/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments for Reviewer oDoz"
                    },
                    "comment": {
                        "value": "### Weakness 1: Why is R&D not compared with CReLU or InFeR?\n\nThank you for the valuable suggestion. We included the results from R&D in the Supplementary Materials. In this result, R&D effectively resolves the negatvie transfer. Though some tasks (e.g. the tasks after learning the tasks in 'Sweep group') are still hard to be learned in SAC, now the tasks in 'Sweep' group can be learned when those tasks lie in the second task. Furthermore, many tasks do not suffer from negative transfer in PPO.\n\n### Weakness 2: Suggestions about writing and visualization\n\n- Firstly, apologies for the confusion caused by the typo. We also used 10 random seeds in the experiments for Figure 2.\n- We will take into account the other suggestions later on. Thank you for the feedback.\n\n### Weakness 3: Why has the p-value not been provided?\n\n- While we did not directly provide p-values in the Supplementary Materials, we have provided enough information through standard deviation for some inference.\n- It seems that the term \"statistical significance\" may have caused some misunderstanding. We will make corrections to clarify this."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2524/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736643070,
                "cdate": 1700736643070,
                "tmdate": 1700736643070,
                "mdate": 1700736643070,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "72sxD1MyLR",
            "forum": "o7BwUyXz1f",
            "replyto": "o7BwUyXz1f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2524/Reviewer_u4tG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2524/Reviewer_u4tG"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, they address the issue of negative transfer in continual reinforcement learning by adding reset and distill strategies. They use the resetting strategy for online actor-critic and use distillation for the offline actor. They show on Meta-world tasks that these strategies mitigate both catastrophic negative transfer and forgetting in CRL."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The papers is well written and organized."
                },
                "weaknesses": {
                    "value": "- The innovation is not enough as a contribution.\n- Other works show that resetting works for continual learning. It is not new to add Resetting and KL divergence to the loss function to mitigate forgetting. \n- In the section 5.1, it compares fig 2 with fig 4 roughly. However it is not enough for the claim. It is needed to compare precisely the numbers in the two figures. \n- There is not evidence or proof for remark 1 to say the reason is reward signal. There are different reasons for loss of plasticity. \n- How is the offline actor trained?"
                },
                "questions": {
                    "value": "- What is the beneficial use of R&D vs just training each task from scratch like just doing resetting?\n- The results in table 1 are very close together. It is not sufficient to compare just the numbers. It needs to clarify if they are significantly different. \n- What is exactly the expert buffer?\n- In the baselines, it would be worth to compare with just resetting agent or training an agent from scratch.\n- What is the evidence for Remark 1? any proof?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2524/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780479934,
            "cdate": 1698780479934,
            "tmdate": 1699636188936,
            "mdate": 1699636188936,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4RI9JeOBF7",
                "forum": "o7BwUyXz1f",
                "replyto": "72sxD1MyLR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2524/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2524/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments for Reviewer u4tG"
                    },
                    "comment": {
                        "value": "### Weakness 1: The innovation is not enough and other works show that resetting works for continual learning\n\nWe appreciate the comments and would like to respond; however, we are facing challenges due to the limited availability of specific details. As far as we know, ClonEx introduced behavioral cloning (KL divergence) to prevent forgetting, but it does not address negative transfers. If there are other works with parameter resetting and KD that we missed, we would really appreciate it if you could let us know.\n\n### Weakness 2: Figure 2 and 4 should be compared precisely\n\nWe believe you had this question based on the following statement in Section 5.2 of our manuscript.\n\n> The findings reveal that fine-tuning with CReLU and InFeR yields similar success rates when compared to the results presented in Figure 2.\n\nWhat we wanted to emphasize with Figure 4 is that the solutions designed to address traditional capacity/plasticity loss, such as InFeR and CReLU, do not address negative transfer. For this purpose, we believed it was more important to compare the results with learning a single task from scratch rather than with finetuning (Figure 2). We have recognized that the wording in the manuscript could be potentially misleading, so we will fix it.\n\n\n### Weakness 3: There is no evidence or proof for Remark 1\n\nFor the response to Remark 1, please refer to the global comments.\n\n### Weakness 4: How is the offline actor trained?\n\nFirst, a randomly initialized network (online actor) is used to learn each task. We use a known RL algorithm (SAC and PPO in our case) for training. In the process of training the online actor, there will be a replay buffer stored. If you use an on-policy algorithm such as PPO, you need to save the replay buffer after training. The states and actions in this stored replay buffer contain the knowledge of the online actor. By distilling this knowledge to the offline actor, the offline actor learns.\n\n### Question 1: What is the beneficial use of R&D compared to just training each task from scratch?\n\nIf we train each task with a re-initialized network, it will forget everything learned in the previous tasks, even though it can learn the current task well. Continual learning is not just about effectively learning the current task; it is crucial not to forget the content of previously learned tasks. Simple resetting alone cannot preserve performance on tasks learned earlier, so additional mechanisms are necessary. We solved the forgetting problem by adding behavioral cloning of the previous tasks to the knowledge distillation for the current task.\n\n### Question 2: The results of Table 1 are very close and it is not sufficient to compare just the numbers\n\nAdditional explanations about the interpretation of the results of Table 1 have been provided in the global comments. So please refer to them for further clarification.\n\n### Question 3: What is exactly the expert buffer?\n\nLet's consider learning tasks A\u2192B sequentially. The policy immediately after learning task A can be seen as an expert for task A. To retain the learned information at this point, behavioral cloning in continual learning involves storing the actions of the expert for states in task A in a buffer. Later, when learning task B, regularization is performed using the state-action pairs in this buffer. In summary, the buffer containing state-action pairs for the previously learned task is referred to as the expert buffer.\n\n### Quenstion 4: In the baselines, it would be worth to compare with just resetting agent or training an agent from scratch.\n\nAs mentioned earlier, simply resetting the agent for each task is not effective in continual learning. While this method may lead to effective learning of the current task, it results in a significant loss of information for previously learned tasks, ultimately leading to very poor performance.\n\n### Question 5: What is the evidence for Remark 1?\n\nPlease refer to the global comment for a response to Remark 1."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2524/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736597047,
                "cdate": 1700736597047,
                "tmdate": 1700736597047,
                "mdate": 1700736597047,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s0rkJeyLtP",
            "forum": "o7BwUyXz1f",
            "replyto": "o7BwUyXz1f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2524/Reviewer_274b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2524/Reviewer_274b"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigated a new empirical phenomenon in continual RL, dubbed catastrophic negative transfer. Particularly, algorithms tend to fail (lack plasticity) in the new task initialized from the previous task, even without incorporating the techniques to mitigate catastrophic forgetting. Next, the authors proposed a new strategy called Reset and Distill highly based on the behavior cloning approach to eliminate the catastrophic negative transfer phenomenon."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The paper is well-organized, well-written and easy to follow.\n\n* The experiments are extensive given the considered algorithms and environments."
                },
                "weaknesses": {
                    "value": "* The conclusion is skeptical and not reliable, which needs further investigation.\n\n* The proposed algorithm is straightforward and lacks technical contribution, although it is effective and technically sound in my opinion."
                },
                "questions": {
                    "value": "The authors claimed a new phenomenon is overlooked in continual RL, but personally, the resulting conclusion is counterintuitive, and skeptical and may not be reliable without further investigations. Here are my explanations.\n\n* Based on my knowledge in Q learning in the tabular setting, the convergence of Q learning (related to theoretical parts in certain stochastic processes and approximations) does not rely on initialization. Please refer to the well-accepted proof in [1], especially Theorem 1, although it is only a manuscript. This implies at least in the tabular setting, even with a very bad initialization, Q learning can always converge to an optimal one in the new environment under mild conditions. This result conflicts with the current manuscript, although this paper targets on deep RL setting. Thus, it is too early to conclude that catastrophic negative transfer commonly exists, and the reason that happens especially in deep RL settings should be carefully investigated and discussed. However, this paper has failed to do that. It would be more reasonable for me if the conclusion is this phenomenon is very unlikely to happen in tabular settings, while it is prone to occur in deep RL cases as the optimization/plasticity is hard in the current task under many initializations from the previous tasks. Therefore, I am not convinced by the current conclusions and strongly suggest more investigation and discussion about this phenomenon.\n\n* The reason behind this phenomenon is also not convincing to me. This paper hypothesizes that the reward in the current tasks is not sufficient to guarantee convergence. Although I mostly agree with the claim, personally the deeper reasons may be the initializations from the previous tasks have a detrimental effect on the current tasks, especially in deep RL settings. Another hypothesis or potential concern is the training time is not sufficient, or the algorithm is not advanced enough to tolerate different initializations. For example, although SAC and PPO are commonly used, they may not be advanced enough to converge very well in these environments. I am thus skeptical of the conclusion. I even think some acceleration techniques used in RL algorithms can also help to mitigate this phenomenon, and thus the issue this paper studies may not be critical.\n\n\n* The proposed algorithm is too straightforward. In my opinion, this resulting algorithm, although I agree it is well-motivated, effective and technically sound, is highly based on behavior cloning. It seems that it is equivalent to behavior cloning that additionally considers the memory buffer in the current task. The deficiency is also apparent as it can almost double the training costs (need to train an extra policy). As such, the proposed algorithm lacks technical contribution.\n\n### More Detailed questions and comments\n\n* What is the difference between the catastrophic negative transfer and plasticity? In my opinion, they are almost the same.\n\n* Although Metaworld is commonly used, the empirical study on this environment may not generalize to broader environments. Also, the results of PPO and SAC cannot be generalized to all RL algorithms as claimed in this paper.\n\n* In Figure 4, why PPO+CReLu performs even worse than PPO in the two-task continual learning? Why CReLu is not useful here? \n\n* How many seeds are used in Table 1? It seems R&D is competitive with ClonEX without significant improvement. \n\n* The considered baselines are limited and the straightforwardly using behavior cloning in the proposed algorithm may not be superior to other state-of-the-art continual RL baselines.\n\nOverall, although this paper studies an overlooked phenomenon and did extensive experiments, empirical results tend to be fragile and may not generalize to broader settings. Some conclusions may be misleading and should be further investigated in the future before made faithfully.\n\n[1] http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2524/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2524/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2524/Reviewer_274b"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2524/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699080264911,
            "cdate": 1699080264911,
            "tmdate": 1699636188839,
            "mdate": 1699636188839,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5fgBwTzfzg",
                "forum": "o7BwUyXz1f",
                "replyto": "s0rkJeyLtP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2524/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2524/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comments for Reviewer 274b (Part I)"
                    },
                    "comment": {
                        "value": "### Question 1: Convergence of Q learning\n\n- First, there is a difference between tabular setting and deep RL initialization. While tabular initializes the output of the Q function directly, initialization of the Q network in deep RL means initialization of the network parameters.\n- This difference is even more pronounced for Q function updates. In the tabular setting, we directly change the output of the Q function according to the Bellman Equation, but we define a loss function and update the Q function by gradient descent in deep RL.\n- As far as we know, convergence in deep RL is not as well understood as in tabular settings. Therefore, a direct comparison between tabular RL and deep RL is somewhat inappropriate. For this reason, we did not address the theoretical convergence in deep RL. However, we have shown through extensive experiments that learning in deep RL can be affected by initialization, and it is not simply due to the loss of plasticity or capacity of the network.\n\n### Question 2: Question about Remark 1\n\n- Please refer to the global comment for a response to Remark 1.\n- We also think that training time can be an important point. In many works, such as [1], the number of frames used per task in the Meta-World is set to 1M. In order to exclude the situation of running out of frames as much as possible, we set the number of frames per task to 3M, and then conducted the experiment by selecting only the tasks that can be learned well within 3M from scratch with 10 random seeds. Given that these tasks are effectively trained within 3M steps using well-known RL algorithms such as SAC or PPO, it is difficult for us to regard this as an issue.\n- Regarding the acceleration technique, we first thought that the policy trained on the previous task is not good at exploration, so we tried to train it by adding an auxiliary loss that forces it to some extent. However, this method cannot solve negative transfer.\n- Also, the ClonEx[1] that we adopted as a baseline not only utilizes behavioral cloning to prevent forgetting but also applies several exploration techniques to enhance learning efficiency. However, as evident from our experimental results, these methods did not effectively address negative transfer.\n- Thus, in our opinion, currently known acceleration techniques are not designed with negative transfer in mind, so it is unlikely that they can fundamentally solve it.\n\n\n### Question 3: The proposed algorithm is too straightforward and is highly based on behavioral cloning\n\nThe main contribution of our method is to reset the network before learning each task to avoid negative transfer. We also applied knowledge distillation to transfer this information to the continuous learner. It may seem similar to behavioral cloning, but it is only used as a means to an end. What we want to emphasize is that even this simple method can effectively alleviate negative transfer. Also, although our method takes two processes to learn the current task, it is not a big problem because the process of distilling the knowledge of online actors (learners from scratch) to offline actor (continual learner) requires very little time compared to learning online actors.\n\n### Question 4: What is the difference between the catastrophic negative transfer and plasticity?\n\nWe have presented a comprehensive analysis of the distinctions between negative transfer and capacity/plasticity loss. Your reference to this analysis would be greatly appreciated.\n\n### Question 5: The empirical study on this environment may not generalize to broader environment\n\nWe could have drawn even stronger conclusions if we had conducted experiments on different environments. However, MetaWorld itself utilizes a substantial number of tasks, and notably, all tasks share the same state space and actions (although the distribution of states may vary). We chose to conduct experiments in MetaWorld because we believed that these characteristics most clearly highlight the issue of negative transfer that we aim to address in continual RL.\n\n### Question 6: Why CReLU is not useful in Figure 4?\n\nThrough our experiments, we confirmed that learning a single task becomes challenging when applying CReLU. You can observe this from the results labeled 'From Scratch'. We believe that the observed performance degradation is a result of the difficulty in learning the single tasks. While the exact aspects causing these differences have not been identified, exploring them falls outside the scope of our investigation.\n\n### Question 7: It seems R&D is competitive with ClonEx without significant improvement\n\nThe results in Table 1 were negative transfer and forgetting measures computed based on the results shown in Figure 3 and 5. Therefore, a total of 10 random seeds were employed. Additional explanations regarding the interpretation of the results have been provided in the global comments; please refer to them for further clarification."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2524/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736528247,
                "cdate": 1700736528247,
                "tmdate": 1700736528247,
                "mdate": 1700736528247,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]