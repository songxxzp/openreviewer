[
    {
        "title": "Text2Reward: Dense Reward Generation with Language Models for Reinforcement Learning"
    },
    {
        "review": {
            "id": "C81YNNAhLY",
            "forum": "tUM39YTRxH",
            "replyto": "tUM39YTRxH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5039/Reviewer_bPzM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5039/Reviewer_bPzM"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method for leveraging language models to write code computing reward functions for simulated reinforcement learning tasks, given a textual description of the task.\n\nTheir method makes use of an expert provided environment \u201cabstraction\u201d (a specification) written as a sequence of pythonic class declarations, and combines it with a human provided textual instruction describing the desired trainable task. This is used to prompt GPT-4 into generating python code for computing rewards. Optionally, the prompt given to the language model can be augmented with few-shot examples of expert and previously-generated reward functions, retrieved from a database using embedding similarity between desired and stored instructions.\n\nWithin the proposed framework, it is also possible to iteratively improve the generated reward function by having a human provide written and descriptive feedback given a video from a policy learnt with the current generated reward. Such feedback is introduced in the language model prompt and used to generate a new version of the reward function code.\nIn a series of experiments over a variety of simulated environments, the authors demonstrate the effectiveness of the approach, while generally showing few-shot and iterative prompting to improve results. Overall, generated reward functions appear to be similar in performance to expert provided rewards."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper addresses the important problem of dense reward specification in reinforcement learning, using a method based on zero-shot and few-shot prompting of a language model, something not shown before except for concurrent work **[1]**.\n\nThe authors show that their method is competitive with human expert reward specification in a broad series of experiments. They moreover demonstrate one such policy to transfer successfully to real world execution.\n\nThe authors also include an analysis of the failure modes of reward specification, specifically for the cases in which the generated reward function code leads to a python runtime error.\n\n**[1]** Ma et al., Eureka: Human-Level Reward Design via Coding Large Language Models, https://eureka-research.github.io/"
                },
                "weaknesses": {
                    "value": "The paper essentially proposes prompting techniques for code generation with pre-trained language models accessible via API, specifically GPT-4. Despite its interesting conclusion and results, it consists at most of an interesting observational study over the capabilities of GPT-4, as the prompting techniques appear straightforward, fundamentally easy to execute manually by any user of the GPT-4 online api platform in a process of trial and error for reward design. I would not consider this a problem worthy of rejection per se, as similar \u201cprompting techniques\u201d results have been published before in machine learning venues, such as the famous \u201cchain of thought\u201d prompting technique. Still, it is my opinion that this limits the significance of the work, as GPT-4 simply fills in the shoes of an expert reinforcement learning reward coder on well known simulation environments (while \u201cchain of thought\u201d prompting was an innovation in how to query a model for more general purpose NLP benchmarks).\n\nMore importantly, to my understanding, no truly novel environments are tackled in this paper. For most of the shown tasks, GPT-4 can reasonably be expected to draw from training data containing countless reward function specifications from those environments, if not reward functions for the desired task itself.\n\nMoreover, the reliance of the paper on GPT-4 makes the results inherently irreproducible, as OpenAI does not have a policy of indefinitely supporting api access to specific snapshot versions of their models (specifically, GPT-4-0314 will be deprecated on June 13th 2024 at the earliest)."
                },
                "questions": {
                    "value": "* Would it be possible to show how this method generalizes to environments that are sure not to have been included in the training data of the model? (e.g: a simple simulated environment designed specifically for this paper)\n* Did you try your method on open source or otherwise publicly available models that would allow for a snapshot of the results to be preserved? If so, how do they compare to GPT-4?\n\nEdit: I raised the score according to my latest comment."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5039/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5039/Reviewer_bPzM",
                        "ICLR.cc/2024/Conference/Submission5039/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5039/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698341106516,
            "cdate": 1698341106516,
            "tmdate": 1700581998507,
            "mdate": 1700581998507,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wt0AsqmtHd",
                "forum": "tUM39YTRxH",
                "replyto": "C81YNNAhLY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5039/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5039/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer bPzM,\n\nThank you for your kind feedback and constructive comments. And thanks for your recognition of our work in addressing the important problem of dense reward specification in reinforcement learning, using language model prompting. \n\nWe have revised our manuscript and are addressing the specific questions in the following reply, including:\n- discussion on data leakage and generalization ability\n- incorporation of experiments on Llama-2 and Code-Llama on 10 tasks of MetaWorld; discussion and improvement on reproducibility\n\nPlease let us know if you have any further questions and we can provide any additional clarifications to help finalize your assessment and rating of our paper.\n\n----\n\n### **W1: The significance of our work**\n**A:** We appreciate the reviewer's perspective. However, we respectfully disagree it is limited to the significance of the method this work proposed. To clarify, we want to emphasize the promise of this work for different communities (RL, Robotics, and NLP) as follows:\n\nFor the **reinforcement learning community**, it is promising to use LLMs to automate policy learning by generating reward functions that are difficult to write down by hand. It is not only an important application scenario for replacing human experts in writing rewards functions but also a novel way that has never been shown before to pour LLMs\u2019 knowledge and abilities into the RL for robotics and control, as well as games. Our work shows promising initial results for further exploration in the future (as shown in the main experiments of Figure 2). \n\nFor the **robotics community**, one of the most important things in the field of robotics is how to learn low-level motor skills at large scales efficiently, scalably, and automatically (or in a self-supervised manner) [1][2]. We believe that by leveraging LLMs, our method (automatic text-to-reward) takes a solid step toward automatically learning large-scale motor skills.\n\nFor the **natural language processing community**, our work proposes text-to-reward as a hard task for code generation and shows a huge gap in performance between the most advanced model and ones from the open-source community (as shown in the newly added experiments on Llama-2 and Code-Llama in Appendix F), showing the potential of improving in this aspect for people in the NLP community. \n\nOverall, our method could be an important direction in the future as a novel way to pour the knowledge learned from LLMs into general RL, low-level robotics skills learning and control, which are not the focus of previous studies that focus on high-level planning, and the method could get more effective with the growth of LLMs.\n\n[1] Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Zackory Erickson, David Held, Chuang Gan, RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation https://arxiv.org/abs/2311.01455 \n\n[2] Open X-Embodiment: Robotic Learning Datasets and RT-X Models, https://arxiv.org/abs/2310.08864 \n\n----\n\n### **W1 & Q1: Generalization to environments not included in GPT-4's training data.**\n**A:** Thanks for your question! We understand your concern regarding the potential data leakage of environment and reward code in GPT-4 and other LLMs, which might lead one to assume that the model's ability to generate high-quality code stems from exposure to \"golden environments\" and \"golden examples.\"\n\nHowever, we would like to clarify **three sources of evidence** from our paper and previous work:\n1. Evidence from the input side (environment and prompts) and output side (generated reward function) of our proposed method for reward function generation;\n2. Evidence from the locomotion novel skills experiments (including backflip, frontflip, wave legs) part in our paper;\n3. Evidence from previous research showed the generalization capability on code generation problems of new domains that are not included in the training set.\n\n**1. Evidence from the input side (prompt) and output side (generated reward function) of our proposed method for reward function generation.**\n\nAs much as you suggest using a simple, purpose-designed simulated environment, we would like to clarify that the Python classes and properties for our prompts were crafted by our team and differ significantly from the environment's original code. We then map the generated code to the environment's original code using a one-to-one deterministic mapping."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700128556259,
                "cdate": 1700128556259,
                "tmdate": 1700128556259,
                "mdate": 1700128556259,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MM1yLzWogQ",
                "forum": "tUM39YTRxH",
                "replyto": "s0Vx8mxOj0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5039/Reviewer_bPzM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5039/Reviewer_bPzM"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Dear Authors,\nI thank you for your long and detailed reply. After considering your points, I elected to raise my score to a 6.\nYour explanation of your use of abstract Python class representations, together with your other points related to generalization, convinced me that complete memorization of reward functions should not be an issue with your approach.\nAs for your comparison with open source models, the negative result is worrying insofar as the success of the techniques described in your paper explicitly depends on the scale and (more importantly) the data distribution used to train GPT-4 specifically, which is not known to us. This is why I will not raise my score more than this."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581950258,
                "cdate": 1700581950258,
                "tmdate": 1700581950258,
                "mdate": 1700581950258,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DrwOPh39nW",
            "forum": "tUM39YTRxH",
            "replyto": "tUM39YTRxH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5039/Reviewer_VQaY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5039/Reviewer_VQaY"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a method to use LLMs to write dense reward functions to train policies from natural language instructions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "************************************************************Evaluation Comprehensiveness:************************************************************ The authors utilized 17 tasks in ManiSkill2 and Metaworld, 6 in Mujoco, and also a few real world experiments \u2014 this is quite comprehensive compared to the average ICLR RL paper.\n\n****************************Experiments:**************************** \n\n- Real world experiments are always nice!\n- Human feedback experiments are also great and demonstrate that on certain tasks the method can improve the policy learning performance \u2014 something that isn\u2019t always straightforward even with humans redesigning reward functions to try to improve polichy learning\n\n******************Results:****************** I think results are solid compared to the oracle.\n\n**********Motivation:********** The paper motivation is nice \u2014 reducing human effort in scaling policy training by using LLMs for python/numpy reward function design."
                },
                "weaknesses": {
                    "value": "************************************************************Contribution over prior work:************************************************************ I\u2019m not too convinced on the major **********technical********** contribution over Yu et al. 2023 (Language to Rewards for Skill Synthesis). Compared to that paper, the main claimed novelty is dense vs sparse reward and the use of standard python code: \u201cDifferent from recent work\u2026.our free-form dense reward code has a wider coverage of tasks and can use established coding packages.\u201d But that paper also uses very pythonic code, furthermore utilizes sparse reward mainly due to using MPC. I\u2019m not too convinced that the pythonic \u2192 python and MPC \u2192 RL are large technical contributions on their own. This should be clarified more specifically/clearly in the paper if there is another technical contribution over Yu 2023, and if not, then is one of the main reasons for my score.\n\n************************Experiments:************************\n\n- The authors should compare against Yu 2023, especially if claiming their dense free-form reward + use of established coding packages can result in superior performance. The comparison isn\u2019t exactly 1-1 given the claims, but currently there is no comparison to any baseline to contextualize the performance of the method. In fact, I think a comparison with Yu 2023 + RL would be fairest, as Yu 2023 likely can use RL instead of MPC without change.\n- Open-source LLMs: Utilizing closed-source LLMs has obvious downsides, e.g., reproducibility (API backend can change at any time) and access to academic researchers (cost per token vs able to be used on a standard GPU setup). It would be beneficial to the community to demonstrate some results with some smaller open source models like LLaMa-2.\n\n**************************Minor Issues:**************************\n\n- 4.1: Appendex \u2192 Appendix\n- I think it\u2019d be nice to have a few small examples in the **********main paper********** of generated reward functions (not full things, just a few lines). This makes the experiment section more readable without needing to jump around to the appendix."
                },
                "questions": {
                    "value": "Putting it here because this is not a \u201cweakness\u201d as this paper just came out: Eureka has recently come out (https://eureka-research.github.io/), and as concurent work, what do the authors think they contribute compared to Eureka?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5039/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5039/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5039/Reviewer_VQaY"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5039/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698379991197,
            "cdate": 1698379991197,
            "tmdate": 1700251949948,
            "mdate": 1700251949948,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m2zlPwnNHC",
                "forum": "tUM39YTRxH",
                "replyto": "DrwOPh39nW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5039/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5039/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer VQaY,\n\nThank you for your appreciation and detailed evaluation of our work. We are glad you think the evaluation of our method is comprehensive and your appreciation of the real-robot experiments and human-feedback findings. We have revised our manuscript and addressed several points you mentioned. \n\nWe have revised our manuscript and are addressing the specific questions in the following reply, including: \n1. discussion on technical contributions\n2. incorporation of 2 baselines on 6 tasks of ManiSkill2 to justify technical contributions\n3. incorporation of experiments of LLama-2 and Code Llama on 10 tasks of MetaWorld; add discussion and improvement on reproducibility\n\nPlease let us know if you have any further questions and we can provide any additional clarifications to help finalize your assessment and rating of our paper.\n\n-----\n\n### **W1: Technical contribution compared to Yu et al. 2023**\n**A:** Thanks for your question and advice! We understand your primary concern pertains to the comparison with the L2R paper by Yu et al., 2023, in terms of technical contributions. We wish to clarify there are fundamental differences between L2R and our method (T2R).\n\nOur method's technical contribution extends beyond L2R in **three key aspects**:\n1. **L2R can only use limited reward terms v.s. T2R can use any reward term**\n2. **L2R can only use a sum of reward terms v.s. T2R can write stage and dense rewards such as if-else statements and can incorporate any other Python feature**\n3. **L2R is model-based and needs huge expert effort (for building the MPC model and writing reward term templates and even weights) v.s. T2R is model-free and can easily be transferred to new environments and real-world application**\n\nTo demonstrate these, we have incorporated **two new experiments**:\n1. We introduce a new baseline of \"L2R prompt + RL\" in the main body of our paper and test the baseline on 6 ManiSkill2 tasks. **\"L2R prompt + RL\" was only deployable on 2 tasks due to the limitation of its reward terms (contribution#1), falling short in the remaining 4 tasks, underscoring the limitations of the task coverage of L2R.**\n2. We introduce an upgraded version of L2R --- \"oracle-sparse-reward + RL\" --- in Appendix E and test this baseline on 6 ManiSkill2 tasks to further demonstrate the importance of stage and dense reward. **While \"oracle-sparse-reward + RL\" yielded results comparable to the zero-shot setting for 3 simpler tasks, it completely can not work (i.e. success rate \u2248 0) on the remaining 3 more challenging tasks, underscoring the limitations of sparse rewards (contribution#2).**\n\nFor more detailed explanation of these **three differences**:\n1. **L2R can only use limited reward terms v.s. T2R can use any reward term:**\n   1. The use of Model Predictive Control (MPC) within the L2R framework restricts the reward term to a twice-differentiable norm with its minimum at zero, typically the L2 distance.\n   2. This presents a limitation when dealing with the ManiSkill2 benchmark, which uses a point cloud to depict complex, articulated rigid bodies with intricate surfaces, offering a more realistic representation of real-world scenarios. L2R\u2019s inability to go beyond the L2 distance measure between vectors restricts its representation of such complex geometries.\n   3. In contrast, our method's flexibility supports a broader spectrum of coding packages and styles, thus enhancing its adaptability across various environments and tasks.\n   4. Our experimental results further highlight this distinction: when testing the baseline on 6 ManiSkill2 tasks, \u201cL2R prompt + RL\u201d was only deployable in 2 tasks due to the limitation of its reward terms, falling short in the remaining 4 tasks."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124549673,
                "cdate": 1700124549673,
                "tmdate": 1700124549673,
                "mdate": 1700124549673,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9UxP8vQoRC",
                "forum": "tUM39YTRxH",
                "replyto": "DrwOPh39nW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5039/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5039/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "2. **L2R can only use a sum of reward terms v.s. T2R can write stage and dense rewards such as if-else statements:**\n   1. Our use of dense rewards, as opposed to sparse rewards, allows for a more nuanced and continuous evaluation of agent behaviors, crucial for complex tasks that involve many steps.\n   2. For example, the stack cube task needs \"pick up cubeA, place it on cubeB, release cubeA and make sure cubeA is static after releasing\". However, L2R will lead to a contradiction between the reward term that encourages \"pick up cubeA\" and \"release cubeA\" when writing them in a simple summation expression.\n   3. To show the necessity of dense rewards, we trained RL with oracle-sparse-reward on ManiSkill2 as an additional experiment in Appendix E. This baseline adapts the oracle expert-written reward codes to mirror the L2R's format (the sum of a set of individual terms) by removing all if-else statements and only keeping functional reward terms, while disregarding the original L2R's inability to utilize point clouds for distance calculations. While \"oracle-sparse-reward + RL\" yielded results comparable to the zero-shot setting for 3 simpler tasks, it completely can not work (i.e. success rate \u2248 0) on the remaining 3 more challenging tasks, underscoring the limitations of sparse rewards.\n3. **L2R is model-based and needs additional expert effort v.s. T2R is model-free and can easily be transferred to new environments and real-world application:**\n   1. The efficacy of the L2R approach is significantly contingent on the performance of Model Predictive Control (MPC). While it is a well-accepted notion that model-based methods, like MPC, outperform model-free methods when an accurate world model is accessible, the practical deployment of MPC is constrained by the requirement for such a model. Crafting this world model demands additional expertise and is not straightforwardly translatable to real-world applications due to this dependency.\n   2. The selection of reward term, design reward term template, and even finding appropriate reward term weight all needs an extra large amount of expert effort.\n\n----\n\n### **W2: Use of open-source LLMs for reproducibility and accessibility.**\n**A:** Thanks for your suggestion! As you mentioned, it is a common practice to use the advanced GPT models from OpenAI or close source model PaLM from Google for novel applications and studies in previous work (Chain-of-Thought[1], Zero-shot planner[2], SayCan[3], ReAct[4]). For the reproducibility, we used open-source algorithms and environments and provided full reproducible prompts and code. Following your suggestion, we also add experiments in the application of our method using open-source or publicly available models on Llama-2 and Code-Llama (Llama-2 further pretrained on code corpus). We include these findings in our revised manuscript and provide a more comprehensive evaluation of our method's capabilities across different models in Appendix F. The results show that the open-source models still have a huge gap with the most advanced models in the difficult task of reward function generation and reflect the necessity of using the most advanced one to demonstrate the possibility of our method.\n\n[1] Jason Wei, Xuezhi Wang, Dale Schuurmans, et al,. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.\nhttps://arxiv.org/abs/2201.11903\n\n[2] Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch. Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. \nhttps://arxiv.org/abs/2201.07207\n\n[3] Michael Ahn, Anthony Brohan, Noah Brown, et al., Do As I Can, Not As I Say: Grounding Language in Robotic Affordances\nhttps://arxiv.org/abs/2204.01691\n\n[4] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao. ReAct: Synergizing Reasoning and Acting in Language Models.\nhttps://arxiv.org/abs/2210.03629\n\n----\n\n### **Minor Issues 1: Typographical errors and suggestions to include reward function examples in the main paper.**\n**A:** We appreciate your suggestions. The typographical errors have been corrected in our revised manuscript. While we acknowledge the value of including reward function examples for clarity, space constraints necessitate a concise presentation. Therefore, only a key example is showcased in Figure 1. We intend to include more comprehensive examples in the camera-ready version, where space permits.\n\n----\n\n### **Q1: Contribution compared to concurrent work, such as Eureka.**\n**A:** Eureka represents an important concurrent development in the field, we all address the important problem of dense reward specification in reinforcement learning, using a method based on zero-shot and few-shot prompting of a language model, as Reviewer#4 pointed out. We believe that our work, alongside Eureka, contributes to the broader understanding and capabilities of LLMs in policy training and reinforcement learning."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124839617,
                "cdate": 1700124839617,
                "tmdate": 1700127116889,
                "mdate": 1700127116889,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PKPSu5n4Ph",
                "forum": "tUM39YTRxH",
                "replyto": "9UxP8vQoRC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5039/Reviewer_VQaY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5039/Reviewer_VQaY"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. Overall I have improved my perception of this paper after the replies and new experiments. A couple of things:\n\n**Comparison to L2R**\nI like the new experiments and they resolve my concerns. But I think you should make the stated differences clearer in the related works section of the text, which hasn't been updated.\n\n> The use of Model Predictive Control (MPC) within the L2R framework restricts the reward term to a twice-differentiable norm with its minimum at zero, typically the L2 distance.\n\nMany MPC algorithms work fine without these assumptions, so I would not claim this as a differentiating factor. \n\n> L2R is model-based and needs additional expert effort v.s. T2R is model-free and can easily be transferred to new environments and real-world application:\n\nI also still do not believe this is a huge difference. L2R produces a reward term that is usable by any optimizer. That can be an MPC optimization algorithm or a traditional RL algorithm. This is simply a choice of using MPC vs RL to **optimize**, which is independent of the **quality** of the reward function, as the per-state/action rewards of T2R can be directly given to an MPC-based optimizer like CEM and vice-versa with L2R.\n\n\n\n> L2R can only use a sum of reward terms v.s. T2R can write stage and dense rewards such as if-else statements and can incorporate any other Python feature\n\nI now understand what the authors mean by \"dense\" and \"sparse\" rewards in this paper. But these terms are different than how they are used traditionally in RL papers, so I think the paper should be updated to correct this. Both L2R and T2R use \"dense\" rewards in the sense that there are non-zero, per-state rewards that are given to an optimizer. The authors are stating \"dense\" means the ability to use if-else conditions and other more advanced program structures, hence L2R is \"sparse.\" But this is not how the RL community uses the word \"sparse,\" which typically refers to not having **any** useful reward signal for most states in a trajectory. I highly suggest changing how the paper characterizes \"dense\" vs \"sparse\"; the authors should instead highlight the linear sum of reward terms of L2R vs the advanced code reward functions that T2R can produce.\n\n**OPen source LLMs**\n\nThank you for adding this comparison, this is great.\n\n\n\nOverall I like the new version of the paper a lot better along with the many new experiments with L2R and open source LLMs. But I would still like to see some changes (or to be proven wrong in an additional author response) with respect to comparisons to prior work like L2R, as I believe some of the statements in the paper are incorrect and/or make faulty comparisons. \n\nI have raised my score accordingly and am willing to do so further after additional discussion."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176655933,
                "cdate": 1700176655933,
                "tmdate": 1700176655933,
                "mdate": 1700176655933,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uAbN5qXzKS",
                "forum": "tUM39YTRxH",
                "replyto": "LfOCuGhw2Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5039/Reviewer_VQaY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5039/Reviewer_VQaY"
                ],
                "content": {
                    "title": {
                        "value": "Updated score again"
                    },
                    "comment": {
                        "value": "Thanks for the response and changes. \n\nAll of my main concerns are addressed and I have updated my score accordingly. Pending seeing the final discussion btwn the other reviewers and the authors, I currently think this paper should be accepted."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700251929700,
                "cdate": 1700251929700,
                "tmdate": 1700251929700,
                "mdate": 1700251929700,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pCZXpMBSgf",
            "forum": "tUM39YTRxH",
            "replyto": "tUM39YTRxH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5039/Reviewer_XwRM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5039/Reviewer_XwRM"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Text2Reward, an automated generation framework of dense reward functions based on large language models. T2R produces interpretable, free-form dense reward codes using a compact representation of the environment source code. T2R is able to outperform human-written reward codes on a range of tasks and allow iterative refinement with human feedback."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper studies the pertinent problem of automated reward design using LLMs. Given that reward design is a fundamental challenge in RL and that LLMs for decision making have largely been limited to high-level planning tasks, this paper offers a fresh perspective and a nice solution to the growing literature of LLMs for problem design and low-level robotic control.\n\nThis paper's method is novel and more flexible than a prior work (Yu et al., 2023) in that it does not require extensive manual templating per robotic embodiment or task and is capable of generating free-form reward functions. It incorporates a compact representation of the environment, background knowledge (e.g., available function APIs), and/or few-shot examples to successfully do so. Finally, this paper demonstrates interesting use case of the proposed method, such as real-world evaluation as well as learning from human feedback.  \n\nThe paper is well-written and free of grammatic errors."
                },
                "weaknesses": {
                    "value": "1. The primary weakness of the paper is that most evaluation tasks are from benchmarks that have been released before GPT-4's knowledge cutoff date (September, 2021). Mujoco and Metaworld tasks have been extensively studied in the reinforcement learning literature; ManiSkill2, though released recently, have many overlapping tasks with ManiSkill, which was released in mid 2021; in particular, most of the tasks, to the best of my knowledge, were in the original ManiSkill benchmark. Given this, it is not clear whether the reward design capability of T2R can readily transfer to an unseen task in a new simulator. \n\n2. Relatedly, the \"novel\" behavior on the Mujoco locomtoin tasks have appeared in prior literature; for example, Hopper back flip is shown in Christiano et al., 2017. It's unclear whether T2R has benefited from that knowledge possibly being in the training set of the backbone LLM. \n\n3. Most manipulation tasks are of \"pick-and-place\" or opening/closing/pushing nature. These are also the most common types of manipulation tasks that the RL literature has studied. It is possible that GPT-4 is generally adept at writing reward functions for those task types. \n\n3. T2R appears to still work best with few-shot examples. In many tasks that do not belong to a family of tasks introduced by a benchmark, providing few-shot examples can still be difficult. \n\n3. For each task, only one reward function is reported. It is not clear whether T2R is robust to stochasticity in LLM generation."
                },
                "questions": {
                    "value": "1. There are 11 tasks in MetaWorld, but only 6 of them have results in the paper?\n\n2. Is there a reason few-shot examples are not used for MetaWorld?\n\n3. How robust is T2R to different random generations? The temperature used for the LLM generation is not shown. \n\n4. Can T2R work for non pick-and-place, opening/closing/pushing robotic manipulation tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5039/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698595626804,
            "cdate": 1698595626804,
            "tmdate": 1699636493250,
            "mdate": 1699636493250,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6uLCZrJrfe",
                "forum": "tUM39YTRxH",
                "replyto": "pCZXpMBSgf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5039/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5039/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer XwRM,\n\nThank you for your thoughtful review and appreciation of our work! We are glad to hear that you acknowledge our contribution to expanding the research of Large Language Models (LLMs) to low-level robotic control. Text2Reward (T2R) avoids the need for extensive manual templating for each robotic form or task and is adept at creating versatile, free-form reward functions. \n\nWe have revised our manuscript and are addressing the specific questions in the following reply, including:\n- clarifications on details \n- discussion on data leakage and generality. \n\nPlease let us know if you have any further questions and we can provide any additional clarifications to help finalize your assessment and rating of our paper.\n\n-----\n\n### **W 1,2,3: Data leakage in GPT-4**\n**A:** Thanks for your question! We understand your concern regarding the potential data leakage of reward code in GPT-4 and other LLMs, which might lead one to assume that the model's ability to generate high-quality code stems from exposure to \"golden examples.\"\n\nHowever, we would like to clarify three sources of evidence from our paper and previous work:\n1. Evidence from the locomotion novel skills experiments (including backflip, frontflip, wave legs) part in our paper;\n2. Evidence from the input side (prompt) and output side (generated reward function) of our proposed method for reward function generation;\n3. Evidence from previous research showed the generalization capability on code generation problems of new domains that are not included in the training set.\n\n**1. Evidence from the locomotion novel skills experiments (including backflip, frontflip, wave legs) part in our paper**\n\nWe did experiments on learning novel skills in MuJoCo, as shown in Table 1 and Figure 4, T2R generates reward codes under our created new instructions for novel skills. We also show the generated reward function codes in Appendix D.2.\n\nAnd for your concern about Christiano et al., 2017, as far as we know, although a hopper backflip was demonstrated in their work, this behavior is learned from human preferences using a neural network reward model, rather than using a symbolic reward code. Furthermore, our generated reward function (shown in Appendix D.2) is very different from what they showed as an expert-written case in their blog (https://openai.com/research/learning-from-human-preferences) of this work.\n\n**2. Evidence from the input side (prompt) and output side (generated reward function) of our proposed method for reward function generation.**\n\n**From the input end,** one piece of evidence, as shown in Appendix C, is that the prompts we provided to LLMs like GPT-4 for our T2R method are **derived from expertly abstracted class Python representations, with specific attributes and function names coined by us, not exact snippets from the ManiSkill2 or MetaWorld GitHub repositories**. This rules out the possibility of the language model plagiarizing directly through context memory since previous work [1] has attempted to perturb instructions used for code generation and found significant performance drops. \n\nOn the other hand, we also have preliminary results that entering direct commands like \"generate a reward function for crawler\" or \"pick the cube\" into GPT-4, or inputting the ManiSkill2 environment code context into GPT-4 to let it complete the `compute_dense_reward` section, none of which were successful. This not only demonstrates the inadequacy of mere memorization but also validates the effectiveness of our method."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122798844,
                "cdate": 1700122798844,
                "tmdate": 1700122798844,
                "mdate": 1700122798844,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4vVouOGfIN",
                "forum": "tUM39YTRxH",
                "replyto": "20okd9wsZU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5039/Reviewer_XwRM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5039/Reviewer_XwRM"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nI have read your responses and believe that most of my concerns have been adequately addressed. That said, I think the approach is still limited in the sense that the algorithm does not systematically handle the case when the reward generation is not good enough either with zero-shot or few-shot prompting. Providing few-shot examples for reward functions also appear more difficult than other use cases of few-shot prompting. For these reasons, given my initial high score, I have elected to keep my rating."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674266963,
                "cdate": 1700674266963,
                "tmdate": 1700674266963,
                "mdate": 1700674266963,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DHH1jLUgfp",
            "forum": "tUM39YTRxH",
            "replyto": "tUM39YTRxH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5039/Reviewer_fmuH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5039/Reviewer_fmuH"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method to generate dense rewards for continuous control RL tasks using LLMs and an API summary of the environment. The method runs either completely zero-shot on top of the prompt with environment information, or in a few-shot manner by concatenating the nearest neighbor task specification and expert reward into the context. In addition, after RL training, human feedback can be solicited, leading to further refinement of the reward code by the LLM, which leads to hopefully stronger RL. Across a number of tasks, the zero-shot dense reward code is competitive with the expert code, and in some settings few-shot generation can improve the results, though in others it is harmful."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Interesting application of a new tool! Using LLMs to generate reward code seems like an easy way to simplify problems we may not already have solutions for, but can describe in language, and is a completely different way around the sparse reward problem\n- Zero-shot results seem pretty strong across all environments\n- Nice results on new tasks that (as far as I know) we don't have expert reward for (e.g. Hopper flipping)"
                },
                "weaknesses": {
                    "value": "- There is no qualitative analysis/discussion of what the source of the improvement is:\n  - Why does Zero-shot outperform Few-shot on Turn Faucet, Open Cabinet Door, Open Cabinet Drawer?\n  - Why does Few-shot fail to outperform Oracle even though Oracle is in context (Lift Cube, Turn Faucet, Push Chair, Open Cabinet Drawer)?\n  - In the cases that few-shot improves on zero-shot, what is the source of this improvement?\n- There are a lot of missing details in the experiments:\n  - There is only one example of human feedback (Figure 6) and it is in a schematic diagram for a task that does not have ground truth, I would like to see a few traces of the whole round (generation, feedback, generation, feedback, generation) in order to understand what exactly is happening\n  - For results in Table 1 and Figure 4 on novel behaviors, the standards for human evaluation and who conducted the evaluation (the authors, peers from the same institution, crowdsourced to a platform) are missing\n  - The experiments in Table 1 and Figure 4 are supposedly conducted in a zero-shot setting (caption in Figure 4), yet Figure 6 gives a schematic for ambiguity resolution which would imply a few-shot result for novel behaviors, an experiment which I do not see in the paper and appendices.\n  - I do not see the choice of $k$ for the number of neighbors that appear in the few-shot context\n  - For generated rewards in Appendix D.1 on Pick Cube, the few-shot vs. oracle code is almost indistinguishable except for 2 constants (cube placed at goal reward, grasped reward). Given this difference is so small, it seems important to know what the human feedback was: are we just getting lucky?\n  - As before, given that the few-shot vs. oracle code is so close on Pick Cube (the only example we have to judge), why is it the case that the few-shot generation is underperforming oracle generation in other settings (Lift Cube, Turn Faucet, Open Cabinet Drawer, Push Chair)?\n  - Is it always the case that the Oracle code for the task is put into the context for Few-shot?\n- Section 4.2 is about code errors that occur before any RL happens, and this seems like a necessary filtering step, but I think having an example of the generation/feedback process is much more important in the main body than Table 2.\n- How long does the iteration loop take? Each iteration requires training policies, so it is quite expensive, and it may be nice to think about early evaluation\n- It would be nice to include code examples for the novel behavior tasks to see what is happening. Given there is no baseline in this case, simply presenting a quantitative evaluation without any analysis is a little sparse..."
                },
                "questions": {
                    "value": "See weaknesses above, I'm most interested in analysis on the source of the improvements in each of the tasks, and how the language model goes about creating reward for a novel task"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5039/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5039/Reviewer_fmuH",
                        "ICLR.cc/2024/Conference/Submission5039/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5039/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719933425,
            "cdate": 1698719933425,
            "tmdate": 1700692577206,
            "mdate": 1700692577206,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OD6HhEY8lU",
                "forum": "tUM39YTRxH",
                "replyto": "DHH1jLUgfp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5039/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5039/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer fmuH,\n\nThank you for your careful review and constructive comments. We are glad you describe our paper as an interesting application with nice results across environments. \n\nAt a very high level, we would like to clarify several terms to make sure that we are on the same page. By human oracle we mean the human-written dense rewards provided by the environment (tuned by the authors of the environment paper). By few-shot, we mean using related human-written dense rewards as in-context reward examples, so few-shot itself does not include any human feedback or interactive generation. Furthermore, in the few-shot setting, for each task, we remove its oracle code from the retrieval pool to make sure that the LM does not cheat. \n\nWe have revised our manuscript and are addressing the specific questions in the following reply, including:\n - Clarification on details\n - Additional examples provided in the paper\n\nPlease let us know if you have any further questions and we can provide any additional clarifications to help finalize your assessment and rating of our paper.\n\n----\n\n### **W1: There is no qualitative analysis/discussion of what the source of the improvement is:**\n\n**W1.1 & W1.2: Why does zero-shot sometimes outperform few-shot? Why does few-shot fail to outperform oracle even though oracle is in context?**\n\n**A:** Thanks for your question! We observed that the LM tends to copy code snippets from the few-shot code when provided. If the human oracle is not optimal, the few-shot human oracles might inadvertently introduce biases or constraints, limiting the LM's creative reward generation. \n\n**W1.3: In the cases that few-shot improves on zero-shot, what is the source of this improvement?**\n\n**A:** Robotic tasks are related, especially the dense stages rewards. When few-shot improves on zero-shot, the few-shot examples likely provide relevant, task-specific stage rewards based on which the LM can rewrite. \n\n------\n\n### **W2: There are missing details in the experiments:**\n\n**W2.1: Clarification on the human feedback process and its trace in the experiments.**\n\n**A:** In the revised manuscript Appendix D.3, we added a detailed example showing the traces of the whole round to show how human feedback improves the reward code step by step.\n\n**W2.2: Human evaluation for locomotion**\n\n**A:** The authors evaluated each rollout. We add this detail to the revised manuscript.\n\n**W2.3: Clarification on the setting of Figure 6**\n\n**A:** The locomotion example in Figure 6 is zero-shot + human feedback, but not few-shot (because we do not have a human oracle for all locomotion tasks). \n\n**W2.4: The choice of $k$ for the number of neighbors in few-shot context.**\n\n**A:** We set $k$ as 1 in all experiments of few-shot settings.\n\n**W2.5 & W2.6 & W2.7: Few-shot and oracle are sometimes non-distinguishable. Few-shot sometimes underperform oracle. Is it always the case that the Oracle code for the task is put into the context for Few-shot?**\n\n**A:** We design the experiment so that the oracle code for the task is never in the context or few-shot. For each task, we remove its oracle code from the retrieval pool to make sure that the LM does not cheat. This explains why few-shot is not necessarily better than oracle because it cannot copy the oracle. When few-shot and oracle are similar, one potential reason is that there is a similar task in the few-shot example, and the LM can adapt that oracle code to fit into this task. \n\n-------\n\n### **W3: The importance of code errors section vs. generation/feedback process details.**\n\n**A:** Thank you for your suggestion! We wish to clarify that when using GPT-4, syntax errors are generally not encountered, as indicated in Table 2, where direct generation has only a 10% error rate. Furthermore, we found that by feeding the error messages from the cases with syntax errors back to GPT-4, all such errors can be rectified. Given that it has become commonplace to use error messages to guide GPT-4 in correcting syntax issues, especially considering that reducing syntax error rates is not the focus of our paper, we choose to not add this as a section.\n\n------\n\n### **W4: Duration of the iteration loop and consideration of early evaluation.**\n\n**A:** We provide information on the time taken for each iteration loop with the hardware we used and discuss the potential for early evaluation strategies to improve efficiency in Appendix A.\n\n------\n\n### **W5: Inclusion of code examples for novel behavior tasks.**\n\n**A:** Just as you mentioned, to enhance clarity and understanding, we include reward code examples generated for novel behavior tasks in Appendix D.2. This will provide a clearer picture of how the LM approaches these tasks and the nature of the generated rewards."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700119459004,
                "cdate": 1700119459004,
                "tmdate": 1700120043033,
                "mdate": 1700120043033,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OZaAdkxkEf",
                "forum": "tUM39YTRxH",
                "replyto": "OD6HhEY8lU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5039/Reviewer_fmuH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5039/Reviewer_fmuH"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for your response, many of my questions have been addressed in a limited manner. I still think there's relatively little clarity as to what is actually happening:\n\n> There is no qualitative analysis...\n\nThe included results are in the appendix without accompanying discussion. At the very least, something of this nature should be in the main text because just numbers hide any ability to build on top of your work. This goes for both subpoints.\n\n> There are missing details...\n\nThanks for some of these changes. The choice of k should be reflected in the section and Table titles in the main text given it's consistent always. In addition the details on the fact that the oracle code is never in context seems like it should be in the main text, and it makes the close agreement between the few-shot and oracle examples even more confusing: it seems that the oracle code is getting leaked in this scenario.\n\n> The importance of code errors sections vs. generation/feedback process details.\n\nI agree that checking syntax is not an important contribution of the work, which is why I advocated for including something like what is now Appendix D.3 in the main text. I generally believe that GPT can correct its own execution errors, and this doesn't seem fundamental to your work, so the choice of Section 4.2 is odd to me.\n\n> Duration of the iteration loop...\n\nThanks for these clarifications.\n\n> Inclusion of code examples...\n\nThanks for the inclusions. I should note that I've become aware after the fact that it's possible to make hopper front and backflip with only a constant policy, and the same is true of the ant lying down, so I'm not sure how convincing these are as novel tasks any longer, but I still consider the paper interesting."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501683619,
                "cdate": 1700501683619,
                "tmdate": 1700501683619,
                "mdate": 1700501683619,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RK3EHUDMkv",
                "forum": "tUM39YTRxH",
                "replyto": "DHH1jLUgfp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5039/Reviewer_fmuH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5039/Reviewer_fmuH"
                ],
                "content": {
                    "comment": {
                        "value": "> For your concerns regarding the qualitative analysis...\n\nI much prefer this change. I do think that this section should refer to both precise performance curves in Figure 2 and examples of generated code in the Appendix (where included, like with Pick Cube) as this makes the information location job much easier on the reader. I don't think this section answers my previous concern that oracle code was being leaked in the case of Pick Cube, and I still don't have a response to this...\n\n> Detail clarification\n\nI appreciate these changes.\n\n> The importance of code errors...\n\nI think the structure is more informative now.\n\nI think given the many changes I would currently support changing my score from 6 to 8. I would prefer seeing the minor change above before the camera ready, though I realize discussion closes soon."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692395069,
                "cdate": 1700692395069,
                "tmdate": 1700692533658,
                "mdate": 1700692533658,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]