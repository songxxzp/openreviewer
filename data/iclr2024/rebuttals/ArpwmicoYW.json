[
    {
        "title": "FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in Medical Image Analysis"
    },
    {
        "review": {
            "id": "pd70CuFVSA",
            "forum": "ArpwmicoYW",
            "replyto": "ArpwmicoYW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3718/Reviewer_iDy7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3718/Reviewer_iDy7"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to promote the group fairness of the deep learning model in medical image analysis, with a specific focus on validation fairness. The authors propose a parameter-efficient fine-tuning method to update parameters regarding fairness. The proposed method is validated on five medical imaging datasets and outperforms compared methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper studies promoting group fairness, which is an important topic.\n- The motivation is well demonstrated.\n- The overall framework design is easy to follow.\n- The proposed method outperforms the compared methods."
                },
                "weaknesses": {
                    "value": "- In the method part, the authors limit the method fairness in binary classification. It has not mentioned how to extend the methodology for multi-classification.\n- The first challenge of PEFT is related to the dataset itself, which is not a challenge for fairness.\n- The method details are not clear. E.g., how to solve the BLO problem by using TPE with SH.\n- It is not clear how to split the train/val/test data.\n- Since this method utilizes validation data to tune the model, it is not proper to report the validation AUC; instead, test AUC should be reported.\n- The experiment only validates the AUC within subgroups, more comprehensive metrics are expected (e.g., equal opportunity.)"
                },
                "questions": {
                    "value": "- Why the metric for fair learning is to minimize the largest loss of a subgroup instead of pursuing a uniform loss distribution among subgroups?\n- How to explain the differences between masks by using different optimizing objectives?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3718/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3718/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3718/Reviewer_iDy7"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3718/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698589987590,
            "cdate": 1698589987590,
            "tmdate": 1699636328178,
            "mdate": 1699636328178,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zPaypJH4sX",
                "forum": "ArpwmicoYW",
                "replyto": "pd70CuFVSA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3718/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3718/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the effort and time spent reviewing our paper, and we are grateful that you highlight that our method performs well, its design is easy to follow and is well motivated, and focuses on important topic. We respond to the weaknesses and questions next.\n\n**Binary and multi-class classification**: We focus on binary classification problems as these are the ones typically considered in medical settings, and following the recent MEDFAIR benchmark (Zong 2023). However, FairTune trivially extends to multi-class classification if desired. The only required change is to replace Binary Cross Entropy loss with Multi-Class Cross Entropy in Eq (2) $L^{base}$, and adjust the AUC in $L^{fair}$ to multi-class AUC. \n\n**Challenge of PEFT**: We are not sure we understand the reviewer's question. Perhaps you mean that the challenge of optimising PEFT architecture is not necessarily a fairness specific challenge? We agree that PEFT architecture optimization can be conducted with the goal of maximising conventional accuracy on a dataset, rather than maximising fairness. However, as our focus is on fairness, our contribution is to introduce a methodology that enables PEFT architecture  to be optimised for fairness, and to demonstrate the empirical results that this methodology is successful. The fact that one can use related techniques to achieve other goals besides fairness does not detract from this contribution. \n\n**Details of the method**: Our method tries to optimize the mask that specifies how we perform PEFT in a way to promote fairness. We optimize either 36 or 12 element masks (36 one in the main scenario - corresponding to normalization, attention and MLP layers for each of the 12 blocks of the ViT model). We use the default settings of Tree-structured Parzen Estimator (TPE) and Successive-Halving (SH) from the Optuna bilevel optimization library. TPE is a smart Bayesian Optimisation strategy for sampling architecture configurations that are the most promising to try next. While SH maintains computational efficiency by early termination of architecture configurations that are not promising. More details of their hyperparameters are now given in the Appendix, as well as the details of how we split data into train/val/test. In all these splits we have followed MEDFAIR. We have added these further details into the revised version of the paper as part of the Appendix.\n\n**Validation AUC**: There seems to be a misunderstanding. Our main results in Table 1 and Table 2 all report Test AUC. We've updated the paper to make this even more unambiguous."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3718/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413127856,
                "cdate": 1700413127856,
                "tmdate": 1700413127856,
                "mdate": 1700413127856,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eGjLJM4xv3",
                "forum": "ArpwmicoYW",
                "replyto": "pd70CuFVSA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3718/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3718/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (2/2)"
                    },
                    "comment": {
                        "value": "**Additional metrics & motivation for fair learning**: Thanks for the feedback. We agree that other metrics such as uniform loss or EqOpp, etc. are widely used. We emphasise that although they are common, they have also been widely criticised in the literature for being pareto inefficient and potentially violating ethical non-maleficence (Beauchamp 2003, Chen 2018, Ustun 2019, Zietlow 2022, etc). For example, it's possible to fully satisfy the EqOdds and EqOpp criteria by providing zero-accuracy for all subgroups, which would be strictly worse than the status quo. Therefore we followed the recommendation of GroupDRO (Sagawa 2019) and the recent MedFAIR (Zong 2023) and focused both our evaluation and our learning objective on the *most disadvantaged subgroup* metric (minAUC).\n\nTo provide more explanation, EqOpp, EqOdds and other metrics can be satisfied by both the utopia situation (100% for all subgroups) as well as pathological situations (0% for all subgroups). Meanwhile, minAUC is not satisfied by the pathological situation, and it is only fully satisfied by the utopia situation (100% for all subgroups). Thus perfect minAUC is a sufficient condition for utopia, while the others can be sufficient but are not necessary conditions. \n\nWhile we prefer minAUC as a metric as explained above, for completeness we now also report other metrics - in line with requests of the other reviewers, we have decided to include Equalized Odds Difference (EOddsD) and Demographic Parity Difference (DPD). The results are in the revised paper's Table 3. We can see that FairTune does quite a good job of satisfying the EOddsD and DPD objectives, even though our algorithm optimises for minAUC. Where other methods outperform FairTune on these metrics, they are worse on both overall AUC and minAUC, thus being completely pareto dominated by FairTune. \n\nFinally, we remark that, as discussed in Sec 4.3, etc, our overall contribution and framework is agnostic to the specific meta-objective used for FairTuning. If a user for some reason really wanted to optimize for e.g. EOddsD rather than minAUC, then this is a trivial hyper-parameter to change for FairTune, which we expect would push the results towards minimising EOddsD in favour of minAUC. \n\n\n**Mask differences**: It is not easy to completely explain the specific masks discovered by different objectives. If there was a straightforward explanation for this, we could directly hand-design the masks rather than needing to automatically search for them. However, for example, Figure 4 shows that the minAUC objective prefers not to fine-tune the last few MLP layers, while the overall AUC objective does fine-tune them. This suggests that those later MLP layers are susceptible to overfitting to spurious correlations in the training set that can be detrimental to the disadvantaged subgroup within the testing set."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3718/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700413191776,
                "cdate": 1700413191776,
                "tmdate": 1700413191776,
                "mdate": 1700413191776,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DMfpe2XbM9",
                "forum": "ArpwmicoYW",
                "replyto": "eGjLJM4xv3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3718/Reviewer_iDy7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3718/Reviewer_iDy7"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the rebuttal. Some of my concerns have been addressed, and below are my remaining concerns.\n\n**Binary and multi-class classification**:  Multi-class classification is also typical in medical settings. For example, the mentioned HAM10000 in this paper is a multi-classification dataset. Whether this method is still effective on multiple classification scenarios is underexplored. The authors are encouraged to present experimental results to support their claim that \u201cThe only required change is to replace Binary Cross Entropy loss with Multi-Class Cross Entropy, and adjust the AUC into multi-class AUC.\u201d\n\n\n\n**Challenge of PEFT**: As the focus is on fairness, is this methodology applicable to other finetuning methods or full-finetuning?\n\n\n\n**Additional metrics & motivation for fair learning**:  What is the trivial hyper-parameter to change for FairTune? Do you mean change from AUC to EOddsD? Is it possible to minimize both metrics in the future? Some discussions on this would be helpful."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3718/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721406623,
                "cdate": 1700721406623,
                "tmdate": 1700721406623,
                "mdate": 1700721406623,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "R3jMGQPtuU",
            "forum": "ArpwmicoYW",
            "replyto": "ArpwmicoYW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3718/Reviewer_XV7U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3718/Reviewer_XV7U"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on an important field in AI, which is achieving group fairness in models, especially in medical diagnosis. They argue that this is essential but challenging due to the fairness generalisation gap where bias emerges during testing. The authors introduce a bi-level optimisation approach called FairTune, which optimises parameter-efficient fine-tuning (PEFT) techniques to balance model fit and fairness generalisation. The empirical results in the paper show that the proposed method enhances fairness across multiple medical imaging datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper proposed a new method to finetune the pretrained model, which is potentially benefits and convenient to the current hype of foundation models or large models that require large-scale pretraining. \n\nThe proposed PEFT achieves the best performance when compared with other fairness finetuning approaches. \n\nThe paper is well written and motivates clearly as well."
                },
                "weaknesses": {
                    "value": "Given the model is proposed for finetuning a pre-trained model, could the authors provide some results that using the proposed approach on finetuning Masked Autoencoder or MOCO to see if this can improve fairness for self-supervised pretraining? \n\nFor evaluation metrics in fairness, DPD and DEOdds are very common to validate an algorithm's fairness, could the authors evaluate their methods on some of the datasets using those two metrics? \n\nThe datasets compared only contains a limited number of attributes, could the authors compare their approaches to fairness medical dataset containing more sensitive attributes such as the \"Luo, Yan, et al. \"Harvard Glaucoma Fairness: A Retinal Nerve Disease Dataset for Fairness Learning and Fair Identity Normalization.\" arXiv preprint arXiv:2306.09264 (2023).\""
                },
                "questions": {
                    "value": "Please see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3718/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3718/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3718/Reviewer_XV7U"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3718/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698677256673,
            "cdate": 1698677256673,
            "tmdate": 1699636328095,
            "mdate": 1699636328095,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RYE5w1d6p8",
                "forum": "ArpwmicoYW",
                "replyto": "R3jMGQPtuU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3718/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3718/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We appreciate the encouraging review as well as the effort and time spent reviewing our paper. We are grateful that you highlight FairTune is clearly motivated, gives strong results and aligns well with the current interest of the ML community in foundation models. We respond to the weaknesses and questions next.\n\n**Fine-tuning of self-supervised pre-trained model**: Thanks for the suggestion. We have selected the masked autoencoder approach (MAE) for this evaluation and ran the experiments. The results confirm that FairTune still provides benefit when using self-supervised pre-training. Please see Table 4 in the revised paper. \n\n**Additional metrics**:  Thanks for the feedback. We agree that Equalized Odds Difference (EOddsD) and Demographic Parity Difference (DPD) are widely used. We emphasise that although they are common, they have also been widely criticised in the literature for being pareto inefficient and potentially violating ethical non-maleficence (Beauchamp 2003, Chen 2018, Ustun 2019, Zietlow 2022, etc). For example, it's possible to fully satisfy the EOddsD and DPD criteria by providing zero-accuracy for both subgroups, which would be strictly worse than the status quo. Therefore we followed the recommendation of GroupDRO (Sagawa 2019) and the recent MEDFAIR (Zong 2023) and focused our evaluation on the *most disadvantaged subgroup* metric (minAUC). This metric is not vulnerable to satisfaction by the potential pathological outcomes that can satisfy EOddsD and DPD. \n\nWhile we prefer minAUC as a metric as explained above, for completeness we now also report EOddsD and DPD in the revised paper's Table 3. We can see that FairTune does quite a good job of satisfying the EOddsD and DPD objectives, even though our algorithm optimises for minAUC. Where other methods outperform fairtune on these metrics, they are worse on both overall and minAUC, thus being completely pareto dominated by FairTune. \n\nFinally, we remark that, as discussed in Sec 4.3, etc, our overall contribution and framework is agnostic to the specific meta-objective used for FairTuning. If a user for some reason really wanted to optimize for e.g. EOddsD rather than minAUC, then this is a trivial hyper-parameter to change for FairTune, which we expect would push the results towards minimising EOddsD in favour of minAUC. \n\n**Harvard-GF3300 dataset**: Thanks for the suggestion to include this dataset, we have now evaluated FairTune and all other approaches also on this dataset and added the results into our paper. The results are consistent with our results on other datasets and show the clear benefits that FairTune brings in terms of achieving superior fairness and overall performance."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3718/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700412948407,
                "cdate": 1700412948407,
                "tmdate": 1700412948407,
                "mdate": 1700412948407,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "piuike8Gwd",
            "forum": "ArpwmicoYW",
            "replyto": "ArpwmicoYW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3718/Reviewer_gkxu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3718/Reviewer_gkxu"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of minimizing demographic bias in AI models used for medical diagnosis. The authors highlight the fairness generalization gap, where deep learning models can fit training data perfectly and exhibit fairness during training but show bias during testing when performance differs across subgroups. To tackle this issue, they propose a bi-level optimization approach called FairTune. FairTune optimizes the learning strategy based on validation fairness by adapting pre-trained models to medical imaging tasks using parameter-efficient fine-tuning techniques. The authors demonstrate empirically that FairTune improves fairness on various medical imaging datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper recognizes the fairness generalization gap, where deep learning models exhibit perfect fairness during training but bias emerges during testing when generalization performance differs across subgroups.\n\n2. This work introduce a parameter-efficient fine-tuning technique as an effective workflow for adapting pre-trained models to downstream medical imaging tasks.\n\n3. The paper is easy to follow."
                },
                "weaknesses": {
                    "value": "1. The absence of widely-used fairness metrics, such as Demographic Parity Difference [1,2,3] and Difference of Equalized Odds [1], in this work raises concerns about the completeness of the evaluation. Including these fairness metrics is essential for making the results more convincing.\n\n2. The benchmarking presented in the study appears to be incomplete. To provide a comprehensive comparison, it is advisable to include at least two additional fairness-aware methods in the experiments: Fair Supervised Contrastive Loss [4] and Group Distributionally Robust Optimization [5].\n\n3. Considering the relevance of MedFair [6], which evaluates fairness across various datasets, especially the significant CheXpert dataset for assessing fairness in medical applications, it would be beneficial to adhere to the experimental protocol and employ CheXpert for evaluating the proposed FairTune.\n\n4. It is worth noting that bi-level optimization can be computationally intensive and time-consuming due to the iterative optimization required in both inner and outer loops.\n\n5. When optimizing for fairness during fine-tuning, there is a potential concern regarding the impact on generalization performance, especially for unseen data or different subgroups. It would be valuable to clarify whether there are mechanisms in place to mitigate any adverse effects on generalization.\n\nReferences:\n\n[1] Alekh Agarwal, Alina Beygelzimer, Miroslav Dud\u00edk, John Langford, and Hanna M. Wallach. A reductions approach to fair classification. In ICML, volume 80 of Proceedings of Machine Learning Research, 60\u201369. PMLR, 2018.\n\n[2] Alekh Agarwal, Miroslav Dud\u00edk, and Zhiwei Steven Wu. Fair regression: quantitative definitions and reduction-based algorithms. In ICML, volume 97 of Proceedings of Machine Learning Research, 120\u2013129. PMLR, 2019.\n\n[3] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning. fairmlbook.org, 2019.\n\n[4] Sungho Park, Jewook Lee, Pilhyeon Lee, Sunhee Hwang, Dohyung Kim, Hyeran Byun; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 10389-10398\n\n[5] Sagawa, S., Koh, P. W., Hashimoto, T. B., & Liang, P. (2019, September). Distributionally Robust Neural Networks. In International Conference on Learning Representations.\n\n[6] Zong, Y., Yang, Y., & Hospedales, T. (2022, September). MEDFAIR: Benchmarking Fairness for Medical Imaging. In The Eleventh International Conference on Learning Representations."
                },
                "questions": {
                    "value": "Please refer to point 1, 2, and 3 in the weaknesses to provide more convincing empirical evidence.\n\nMoreover, I noted that the code repository mentioned in the abstract has not been established. Providing access to the implementation code would greatly enhance the comprehensibility of this research during the review process."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3718/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698729631986,
            "cdate": 1698729631986,
            "tmdate": 1699636328018,
            "mdate": 1699636328018,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "amb1IgapzV",
                "forum": "ArpwmicoYW",
                "replyto": "piuike8Gwd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3718/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3718/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the effort and time spent reviewing our paper, and we are grateful you highlight that our work recognizes the fairness generalization gap and provides an effective workflow for adapting pre-trained models to downstream medical imaging tasks. We respond to the weaknesses and questions next.\n\n**Additional metrics**: Thanks for the feedback. We agree that Equalized Odds Difference (EOddsD) and Demographic Parity Difference (DPD) are widely used. We emphasise that although they are common, they have also been widely criticised in the literature for being pareto inefficient and potentially violating ethical non-maleficence (Beauchamp 2003, Chen 2018, Ustun 2019, Zietlow 2022, etc). For example, it's possible to fully satisfy the EOddsD and DPD criteria by providing zero-accuracy for both subgroups, which would be strictly worse than the status quo. Therefore we followed the recommendation of GroupDRO (Sagawa 2019) and the recent MEDFAIR (Zong 2023), and focused our evaluation on the *most disadvantaged subgroup* metric (minAUC). This metric is not vulnerable to satisfaction by the potential pathological outcomes that can satisfy EOddsD and DPD. \n\nWhile we prefer minAUC as a metric as explained above, for completeness we now also report EOddsD and DPD in the revised paper's Table 3. We can see that FairTune does quite a good job of satisfying the EOddsD and DPD objectives, even though our algorithm optimises for minAUC. Where other methods outperform FairTune on these metrics, they are worse on both overall and minAUC, thus being completely pareto dominated by FairTune. \n\nFinally, we remark that, as discussed in Sec 4.3, etc., our overall contribution and framework is agnostic to the specific meta-objective used for FairTuning. If a user for some reason really wanted to optimize for e.g. EOddsD rather than minAUC, then this is a trivial hyper-parameter to change for FairTune, which we expect would push the results towards minimising EOddsD in favour of minAUC. \n\n**Code**: We've now added the code as part of the supplementary material. Apologies for not including it as part of the initial submission.\n\n**Additional baselines**: We now add FSCL [4] into the evaluation and our results show that while this method leads to improvements over training from scratch, it is not as good as FairTune. GroupDRO was already evaluated as part of MEDFAIR, and shown to be worse than MEDFAIR's properly tuned Full FT baseline which we also use here. Furthermore, we now also compare FairTune to another recently proposed method, FairPrune [R1], that has been shown to outperform various other methods, including AdvConf [R2], AdvRev [R3], DomainIndep [R4], and OBD [R5]. Our results clearly show that FairTune outperforms both FSCL and FairPrune."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3718/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700412810007,
                "cdate": 1700412810007,
                "tmdate": 1700412810007,
                "mdate": 1700412810007,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ON5nyQIbTG",
                "forum": "ArpwmicoYW",
                "replyto": "piuike8Gwd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3718/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3718/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (2/2)"
                    },
                    "comment": {
                        "value": "**Computational costs**: FairTune is associated with larger computational costs for the fine-tuning stage as it performs optimization on how the fine-tuning is done. In practice, while the absolute cost of FairTune is larger than conventional fine-tuning, it is not prohibitively costly, eg: 14GPUh vs 48GPUh, as discussed in Sec 5.  This is a one-off cost that allows us to make many fairer predictions. And as fairness is key in medical applications, the additional overhead is likely to be worth it. That said, we used a fairly simple HPO algorithm to expedite our research on fair fine-tuning, given the manageable absolute costs involved. There is scope for future extensions to reduce the computational cost, e.g., with more sophisticated gradient based search as discussed in various surveys (Liu et al, arXiv'21, Sinha et al, IEEE Trans Evo Comp'18, Hospedales et al, IEEE TPAMI'21).\n\nIn the meanwhile, to expedite large datasets, it is possible to perform FairTune search only on a subset of the dataset to keep the costs low, before actually conducting the fine-tuning on the full dataset. Subsampling for hyperparameter optimization has been used with promising results [R6, R7] and can be used to make FairTune scalable also to these settings. Random subsampling has shown promising results already [R8] and we can extend it by randomly sampling while maintaining the ratios of sensitive attributes. We have successfully used this strategy to apply FairTune to the suggested CheXpert dataset during the short rebuttal window. \n\n**CheXpert dataset**: Thanks for the reminder. We have now added results on this benchmark, expediting FairTune's runtime by the simple heuristic of performing architecture search on a subset of the examples before fine-tuning the chosen architecture on the full dataset. The results are strong for FairTune and demonstrate that our approach is able to scale to large datasets by using subsampling.\n\n**Generalization**: Thanks for re-iterating this point. Our motivating observation from Figure 1 aligns with the reviewer's point that there is a risk of achieving good performance and good fairness on the training data, while simultaneously achieving poor generalisation on held out data. However, while many methods optimising for fairness may lead to worse generalization on held out data, our results in Table 1 suggest that our method does not suffer from it. This is because our meta-objective for hyperparameter optimization (Eq 2) *optimises for performance on held out validation data*, rather than for performance on seen training data. This is an important difference between FairTune and the vast majority of existing approaches such as GroupDRO, FairPrune, FSCL and others. \n\nReferences:\n\n[R1] Yawen Wu, Dewen Zeng, Xiaowei Xu, Yiyu Shi, Jingtong Hu. Fairprune: Achieving fairness through pruning for dermatological disease diagnosis. In MICCAI, 2022.\n\n[R2] Zhang, Brian Hu, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial learning. In AIES, 2018.\n\n[R3] Eric Tzeng, Judy Hoffman, Trevor Darrell, Kate Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, 2015.\n\n[R4] Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, Olga Russakovsky. Towards fairness in visual recognition: Effective strategies for bias mitigation. In CVPR, 2020.\n\n[R5] LeCun, Yann, John Denker, and Sara Solla. Optimal brain damage. In NeurIPS, 1989.\n\n[R6] Jae-hun Shim, Kyeongbo Kong, and Suk-Ju Kang. Core-set sampling for efficient neural architecture search. In ICML Workshop on Subset Selection in ML, 2021.\n\n[R7] Savan Visalpara, Krishnateja Killamsetty, and Rishabh Iyer. A data subset selection framework for efficient hyper-parameter tuning and automatic machine learning. In ICML Workshop on Subset Selection in ML, 2021.\n\n[R8] Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, Frank Hutter. Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets. In AISTATS, 2017."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3718/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700412879735,
                "cdate": 1700412879735,
                "tmdate": 1700412879735,
                "mdate": 1700412879735,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DpHMiNZWVa",
            "forum": "ArpwmicoYW",
            "replyto": "ArpwmicoYW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3718/Reviewer_Dgwu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3718/Reviewer_Dgwu"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces FairTune, a fine-tuning method for pre-trained models that aims to improve fairness with respect to sensitive attributes. The contribution lies in developing a technique that minimizes disparities in model performance between different demographic groups while maintaining high overall predictive accuracy. The method is demonstrated across various datasets and benchmarks, particularly in medical imaging, using the AUROC metric for evaluation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. FairTune provides a new pathway and improvement in reducing bias in AI models.\n2. The paper conducted extensive testing over multiple datasets.\n3. It leverages an ablation study to show the effectiveness of each component of the tuning process."
                },
                "weaknesses": {
                    "value": "The paper may not fully address the computational costs or scalability issues associated with FairTune. Please see the questions for more details."
                },
                "questions": {
                    "value": "1. The code link is not available.\n2. Can the authors examine the proposed FairTune on dataset with larger \"Gap\"? In Table 1, the Gaps for the datasets are relatively small.  Some improvements were limited, compared with full fine-tune. \n3. Can the authors provide insights into the computational overhead introduced by FairTune compared to traditional fine-tuning methods?\n4. What are the scalability considerations for applying FairTune to very large datasets or models?\n5. How sensitive is FairTune to the choice of sensitive attributes, and can it adapt to scenarios with multiple overlapping sensitive categories?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3718/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699179955708,
            "cdate": 1699179955708,
            "tmdate": 1699636327922,
            "mdate": 1699636327922,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tiwskgrtWN",
                "forum": "ArpwmicoYW",
                "replyto": "DpHMiNZWVa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3718/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3718/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We appreciate the encouraging review as well as the effort and time spent reviewing our paper. We are grateful that you highlight FairTune provides a new pathway and improvement in reducing bias in AI models and includes extensive evaluation. We respond to the weaknesses and questions next.\n\n**Computational costs**: FairTune is associated with larger computational costs for the fine-tuning stage as it performs optimization on how the fine-tuning is done. In practice, while the absolute cost of FairTune is larger than conventional fine-tuning, it is not prohibitively costly, eg: 14GPUh vs 48GPUh, as discussed in Sec 5.  This is a one-off cost that allows us to make many fairer predictions. And as fairness is key in medical applications, the additional overhead is likely to be worth it. That said, we used a fairly simple HPO algorithm to expedite our research on fair fine-tuning, given the manageable absolute costs involved. There is scope for future extensions to reduce the computational cost, e.g., with gradient based search as discussed in various surveys (Liu et al., arXiv'21, Sinha et al., IEEE Trans Evo Comp'18, Hospedales et al., IEEE TPAMI'21).\n\n**Scalability**: For large datasets it is possible to perform FairTune architecture search without any further algorithmic improvements, simply by performing HPO on a subset of the dataset to keep the costs low, before actually conducting the fine-tuning on the full dataset. Subsampling for hyperparameter optimization has been used with promising results [R1, R2] and can be used to make FairTune scalable also to these settings. Random subsampling has shown promising results already [R3] and we can extend it by randomly sampling while maintaining the ratios of sensitive attributes. We demonstrate this is a useful strategy by using subsampling for the search on the large CheXpert dataset, where we also demonstrate excellent results.\n\n**Code**: We've now added the code as part of the supplementary material. Apologies for not including it as part of the initial submission.\n\n**Evaluation on datasets with larger gaps**: It is important to observe that while FairTune leads to small gaps, other baselines evaluated, for example Training from scratch and Linear readout lead to large gaps of e.g. around 10 or 20 in some setups (HAM10000 - age and Papila gender especially). As a result we already evaluate scenarios that have large gaps, and we see that FairTune is able to reduce the gaps to very small values in these cases, which is a highly desirable property to have.\n\n**Sensitivity to choice of sensitive attributes and overlapping scenarios**: Our current evaluation includes 4 sets of sensitive attributes: age, gender, skin type, and race. To investigate the case of overlapping sensitive attributes, we intersected age and gender attributes using the CheXpert dataset. Our results show FairTune leads to consistent improvements even under this scenario. The details are given in the Appendix and results are shown in Table 5.\n\nReferences:\n\n[R1] Jae-hun Shim, Kyeongbo Kong, and Suk-Ju Kang. Core-set sampling for efficient neural architecture search. In ICML Workshop on Subset Selection in ML, 2021.\n\n[R2] Savan Visalpara, Krishnateja Killamsetty, and Rishabh Iyer. A data subset selection framework for efficient hyper-parameter tuning and automatic machine learning. In ICML Workshop on Subset Selection in ML, 2021.\n\n[R3] Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, Frank Hutter. Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets. In AISTATS, 2017."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3718/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700412625987,
                "cdate": 1700412625987,
                "tmdate": 1700412625987,
                "mdate": 1700412625987,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]