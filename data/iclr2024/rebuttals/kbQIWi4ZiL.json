[
    {
        "title": "Unsupervised combinatorial optimization under complex conditions: Principled objectives and incremental greedy derandomization"
    },
    {
        "review": {
            "id": "KJdrBMi8f1",
            "forum": "kbQIWi4ZiL",
            "replyto": "kbQIWi4ZiL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1650/Reviewer_yrHB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1650/Reviewer_yrHB"
            ],
            "content": {
                "summary": {
                    "value": "This paper considered the problem of unsupervised combinatorial optimization under complex conditions. The proposed UCOM2 consists of a principled probabilistic objective construction scheme and a derandomization scheme. The authors provided some theoretical results for the proposed scheme and applied them to various complex conditions, such as cardinality constraints and non-binary decisions. The authors also performed some experiments and showed that UCOM2 is general and practical."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Undoubtedly, this paper generalized some results of the references Karalias \\& Loukas (2020) and Wang et al. (2022). To the reviewer's best understanding, the core contributions are two folds: (1) the author claimed the expectation $\\tilde{f}:[0,1]^n\\rightarrow \\mathbb{R}$ of an optimization objective $f:\\\\{0,1\\\\}^n\\rightarrow \\mathbb{R}$, defined by $\\mathbb{E}_{X\\sim p}f(X)$, is differentiable and entry-wise concave with respect to $p$; (2) the authors conducted incremental greedy derandomization. Overall, this paper is well-written (except mathematical expressions are ugly organized, suggest using the align environment in Latex) and mathematically solid."
                },
                "weaknesses": {
                    "value": "Unfortunately, some proofs of the core theorems are in questions.\n\n(1) Page 16, Proof of Theorem 1: Please explain the fourth and fifth equality, i.e., first and second equalities as follows.\n\\begin{align}\n&\\sum\\_X\\prod\\_{v\\in V\\_X}p\\_v\\prod\\_{u\\in[n]\\setminus V\\_X}(1-p\\_u)g(X)\\\\\\\\\n=&\\sum\\_X\\prod\\_{v\\in V\\_X,v\\neq i}p\\_v\\prod\\_{u\\in[n]\\setminus V\\_X,v\\neq i}(1-p\\_u)(p\\_ig({\\rm{der}}(i,1;p))+(1-p\\_i)g({\\rm{der}}(i,0;p)))\\\\\\\\\n=&p\\_i\\tilde{g}({\\rm{der}}(i,1;p))+(1-p\\_i)\\tilde{g}({\\rm{der}}(i,0;p))\n\\end{align}\nThe reviewer suspected that the first equality is wrong and the authors in fact showed $\\tilde{g}$ is linear with respect to $p$.\n\n(2) Page 17, Proof of Lemma 4: Please explain the following equalities,\n\\begin{align}\n\\tilde{f}\\_{OS}(p^\\prime;i,h)&=p^\\prime\\_{v\\_1}d\\_1+(1-p^\\prime\\_{v\\_1})p^\\prime\\_{v\\_2}d\\_2+\\cdots+(\\prod\\_{j=1}^{n-1}(1-p^\\prime\\_{v\\_j}))p^\\prime\\_{v\\_n}d\\_n\\\\\\\\\n&=\\sum\\_{j<i}\\prod\\_{k=1}^{j-1}(1-p\\_{v\\_k})p\\_{v\\_j}d\\_j+0+\\sum\\_{j^\\prime>i}\\prod\\_{1\\leq k^\\prime\\leq j^\\prime-1,k^\\prime\\neq i}(1-p\\_{v\\_k^\\prime})p\\_{v\\_j^\\prime}d\\_{j^\\prime}\\\\\\\\\n&=\\tilde{f}\\_{OS}(p;i,h)-q\\_jd\\_j+\\frac{p\\_{v\\_j}}{1-p\\_{v\\_j}}\\sum\\_{j^\\prime>j}q\\_{j^\\prime}d\\_{j^\\prime}.\n\\end{align}\nThe reviewer suspected that the second equality is wrong.\n\n(3) Page 17, Proof of Theorem 3: Please explain\n\\begin{align}\n\\sum\\_{X\\in d^n}(\\prod\\_{v\\in [n]\\setminus \\\\{i\\\\}}p\\_{vX\\_v})p\\_{iX\\_i}g(X)=\\sum\\_{r\\in d}\\tilde{f}({\\rm{der}}(i,r;p))\\leq \\tilde{f}(p).\n\\end{align}"
                },
                "questions": {
                    "value": "Thanks the authors for providing the implemented codes. It would be great if the authors could provide readme.txt or demo codes that the reviewer can reproduce the experiment results. As for the experiments, the reviewer has one question, how to choose the parameter $\\beta$? Have the authors done some ablation studies on the choices of $\\beta$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1650/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1650/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1650/Reviewer_yrHB"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1650/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698751394893,
            "cdate": 1698751394893,
            "tmdate": 1700657371379,
            "mdate": 1700657371379,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cSLoXWzkky",
                "forum": "kbQIWi4ZiL",
                "replyto": "KJdrBMi8f1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment 1: Regarding Proof of Theorem 1"
                    },
                    "comment": {
                        "value": "**Reviewer\u2019s Comment:**\n\n- `Page 16, Proof of Theorem 1: Please explain the fourth and fifth equality, i.e., first and second equalities as follows.`\n\\begin{align}\n&\\sum_X\\prod_{v\\in V_X}p_v\\prod_{u\\in[n]\\setminus V_X}(1-p_u)g(X)\\\\\\\\\n=&\\sum_X\\prod_{v\\in V_X,v\\neq i}p_v\\prod_{u\\in[n]\\setminus V_X,v\\neq i}(1-p_u)(p_ig({\\rm{der}}(i,1;p))+(1-p_i)g({\\rm{der}}(i,0;p)))\\\\\\\\\n=&p_i\\tilde{g}({\\rm{der}}(i,1;p))+(1-p_i)\\tilde{g}({\\rm{der}}(i,0;p))\n\\end{align}\n- `The reviewer suspected that the first equality is wrong and the authors in fact showed` $\\tilde{g}$ `is linear with respect to` $p$\n\n\n**Response:**\n\n- **Recall** **Theorem 1:** For any $g:\\{0,1\\}\\to\\mathbb R$, $\\tilde{g}: [0,1]^n \\to \\mathbb R$ with $\\tilde{g}(p)=\\mathbb E_{X \\sim p} g(X)$ is differentiable and entry-wise concave w.r.t $p$.\n- **Regarding the first equality:** Thanks for pointing out the unclear part. This part indeed did not have enough clarity with some notations ($g({\\rm{der}}(i,0;p))$ and $g({\\rm{der}}(i,1;p))$) not well defined. We have revised the equations and added more details:\n\n$$\n\\begin{align}\n&\\sum_{X} \\prod_{v \\in V_X} p_v \\prod_{u \\in [n] \\setminus V_X} (1 - p_u) g(X) \\\\\\\\\n    =&\\sum_{X \\colon i \\in V_X}\\left(\\prod_{v\\in V_X,v\\neq i}p_v\\prod_{u\\in[n]\\setminus V_X}(1-p_u)\\right) p_i g(X) + \\sum_{X \\colon i \\notin V_X}\\left(\\prod_{v\\in V_X}p_v\\prod_{u\\in[n]\\setminus V_X,u\\neq i}(1-p_u)\\right) (1 - p_i) g(X) \\\\\\\\\n    =&p_i \\sum_{X \\colon i \\in V_X}\\prod_{v\\in V_X,v\\neq i}p_v\\prod_{u\\in[n]\\setminus V_X}(1-p_u) g(X) + (1 - p_i) \\sum_{X \\colon i \\notin V_X}\\prod_{v\\in V_X}p_v\\prod_{u\\in[n]\\setminus V_X,u\\neq i}(1-p_u) g(X)\\\\\\\\\n    =&p_i \\tilde{g}(\\operatorname{der}(i, 1; p)) + (1 - p_i) \\tilde{g}(\\operatorname{der}(i, 0; p))\n\\end{align}\n$$\n\n- **Regarding $\\tilde{g}$ is linear with respect to $p$:** The reviewer is correct and our statement is still valid. This is because linearity is a special case of entry-wise concavity. We only mentioned entry-wise concavity because that is what we need."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699951223083,
                "cdate": 1699951223083,
                "tmdate": 1699963896818,
                "mdate": 1699963896818,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "58xy5hrN1l",
                "forum": "kbQIWi4ZiL",
                "replyto": "KJdrBMi8f1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment 2: Regarding Proof of Lemma 4"
                    },
                    "comment": {
                        "value": "**Reviewer\u2019s Comment:**\n\n- `Page 17, Proof of Lemma 4: Please explain the following equalities,`\n $$\n\\begin{align}\n\\tilde f_{\\mathrm{os}}(p^\\prime;i,h)&=p^\\prime_{v_1}d_1+(1-p^\\prime_{v_1})p^\\prime_{v_2}d_2+\\cdots+(\\prod_{j=1}^{n-1}(1-p^\\prime_{v_j}))p^\\prime_{v_n}d_n \\\\\\\\\n&=\\sum_{j<i}\\prod_{k=1}^{j-1}(1-p_{v_k})p_{v_j}d_j+0+\\sum_{j^\\prime>i}\\prod_{1\\leq k^\\prime\\leq j^\\prime-1,k^\\prime\\neq i}(1-p_{v_k^\\prime})p_{v_j^\\prime}d_{j^\\prime}\\\\\\\\\n&=\\tilde f_{\\mathrm{os}}(p;i,h)-q_jd_j+\\frac{p_{v_j}}{1-p_{v_j}}\\sum_{j^\\prime>j}q_{j^\\prime}d_{j^\\prime}.\n\\end{align}\n$$\n- `The reviewer suspected that the second equality is wrong.` \n\n**Response:**\n- **Recall the current Lemma 4:** For any $p \\in [0, 1]^n$ and $i \\in [n]$, we define $q_i \\coloneqq (\\prod_{i' = 1}^{i-1} (1-p_{v_{i'}}))p_{v_{i}}$, the coefficient of $d_i$ in $\\tilde f_{os}$. Then $\\Delta \\tilde f_{\\mathrm{os}}(v_j, 0, p; i, h) = - q_j d_j + \\frac{p_{v_j}}{1-p_{v_j}}\\sum_{j' > j} q_{j'}d_{j'}$, and $\\Delta \\tilde f_{\\mathrm{os}}(v_j, 1, p; i, h) = \\sum_{j' > j} q_{j'}(d_j - d_{j'}), \\forall j \\in [n]$.\n- Thanks for pointing out the unclear part. **The final equation is correct**, while **some symbols** (e.g., $i$ and $j$) **were used twice with different meanings**, which possibly caused confusion. We have revised the notations and added more details:\n$$\n\\begin{align}    \n&\\tilde f_{\\mathrm{os}}(p^\\prime; i,h) \\\\\\\\\n=& p^\\prime_{v_1} d_1 + (1 - p^\\prime_{v_1})p^\\prime_{v_2} d_2 + \\cdots + (\\prod_{s = 1}^{n-1} (1-p^\\prime_{v_s}))p^\\prime_{v_{n}} d_n \\\\\\\\\n=& \\sum_{s < j} \\prod_{k = 1}^{s - 1} (1-p_{v_k})p_{v_{s}} d_s + 0 + \\sum_{t > j} \\prod_{1 \\leq k \\leq t - 1, k \\neq j} (1-p_{v_{k}}) p_{v_{t}} d_{t}\\\\\\\\\n=& \\sum_{s < j} q_s d_s + 0 + \\sum_{j' > j} \\frac{1}{1 - p_{v_j}} q_{j'} d_{j'} \\\\\\\\\n=& \\sum_{s = 1}^{n} q_s d_s - q_jd_j + \\sum_{j' > j} \\frac{p_{v_{j}}}{1 - p_{v_{j}}} q_{j'} d_{j'} \\\\\\\\\n=& \\tilde f_{\\mathrm{os}}(p; i,h) - q_j d_j + \\frac{p_{v_j}}{1-p_{v_j}}\\sum_{j' > j} q_{j'}d_{j'}.\n\\end{align}\n$$\n    \n- We have also updated the notations in the statement of Lemma 4 accordingly: For any $p \\in [0, 1]^n$ and $j \\in [n]$, we define $q_j \\coloneqq (\\prod_{k = 1}^{j-1} (1-p_{v_{k}}))p_{v_{j}}$, the coefficient of $d_j$ in $\\tilde f_{os}$. Then \n$\\Delta \\tilde f_{\\mathrm{os}}(v_j, 0, p; i, h) = - q_j d_j + \\frac{p_{v_j}}{1-p_{v_j}}\\sum_{j^\\prime > j} q_{j^\\prime}d_{j^\\prime}$, and $\\Delta \\tilde f_{\\mathrm{os}}(v_j, 1, p; i, h) = \\sum_{j^\\prime > j} q_{j^\\prime}(d_j - d_{j^\\prime}), \\forall j \\in [n]$."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699952694580,
                "cdate": 1699952694580,
                "tmdate": 1700314279421,
                "mdate": 1700314279421,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0iU0ENiYYK",
                "forum": "kbQIWi4ZiL",
                "replyto": "KJdrBMi8f1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment 3: Regarding Proof of Theorem 3"
                    },
                    "comment": {
                        "value": "**Reviewer\u2019s Comment:**\n\n- `Page 17, Proof of Theorem 3: Please explain`\n    \n    $$\n    \\begin{align}\n    \\sum_{X\\in d^n}(\\prod_{v\\in [n]\\setminus \\{i\\}}p_{vX_v})p_{iX_i}g(X)=\\sum_{r\\in d}\\tilde{f}({\\rm{der}}(i,r;p))\\leq \\tilde{f}(p).\n    \\end{align}\n    $$\n    \n\n**Response:**\n\n- ********************Recall Theorem 3:******************** For any function $g: d^n \\to \\mathbb R$, $\\tilde{g}: [0, 1]^{n \\times c} \\to \\mathbb R$ with $\\tilde{g}(p) = \\mathbb E_{X \\sim p} g(X)$ is differentiable and entry-wise concave, where $\\mathbb E_{X \\sim p} g(X) = \\sum_{X \\in d^n} \\Pr_p[X] g(X)$ with $\\Pr_p[X]= \\prod_{v \\in [n]} p_{vX_v}$.\n- Thanks for pointing out the unclear part. Our proof was indeed incomplete with some equations missing. We have revised our proof as follows:    \n$$\n\\begin{align} \u00a0 \u00a0 &\\tilde g(p) \\\\\\\\ \u00a0 \u00a0=&\\mathbb E_{X \\sim p} g(X) \\\\\\\\ \u00a0 \u00a0=&\\sum_{X \\in d^n} \\Pr_{p}[X] g(X)\\\\\\\\ \u00a0 \u00a0=&\\sum_{X \\in d^n} \\prod_{v \\in [n]} p_{vX_v} g(X)\\\\\\\\ \u00a0 \u00a0=&\\sum_{X \\in d^n} (\\prod_{v \\in [n] \\setminus \\{i\\}} p_{vX_v}) p_{iX_i} g(X)\\\\\\\\ \u00a0 \u00a0 \u00a0 \u00a0=&\\sum_{r \\in d} \\sum_{X \\colon X_i = r} (\\prod_{v \\in [n] \\setminus \\{i\\}} p_{vX_v}) p_{iX_i} g(X)\\\\\\\\ \u00a0 \u00a0=&\\sum_{r \\in d} \\sum_{X \\colon X_i = r} (\\prod_{v \\in [n] \\setminus \\{i\\}} p_{vX_v}) p_{ir} g(X)\\\\\\\\ \u00a0 \u00a0=&\\sum_{r \\in d} p_{ir} \\sum_{X \\colon X_i = r} \u00a0(\\prod_{v \\in [n] \\setminus \\{i\\}} p_{vX_v}) g(X)\\\\\\\\ \u00a0 \u00a0=&\\sum_{r \\in d} p_{ir} \\sum_{X} \u00a0(\\prod_{v \\in [n] \\setminus \\{i\\}} p_{vX_v}) \\mathbb{1}(X_i = r) g(X)\\\\\\\\ \u00a0 \u00a0=&\\sum_{r \\in d} p_{ir} \\tilde g(\\operatorname{der}(i, r; p)) \u00a0 \u00a0 \u00a0 \\end{align}\n$$"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699952846575,
                "cdate": 1699952846575,
                "tmdate": 1699952860287,
                "mdate": 1699952860287,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uAKrccqwoc",
                "forum": "kbQIWi4ZiL",
                "replyto": "KJdrBMi8f1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment 4: Regarding Using \u201cAlign\u201d for Equations"
                    },
                    "comment": {
                        "value": "**Reviewer\u2019s Comment:**\n\n- `Mathematical expressions are ugly organized, suggest using the align environment in Latex`\n\n**Response:**\n\n- Thanks for the advice and the detailed review of our proofs. We have made equations in our proofs look better by using the \u201calign\u201d environment suggested by the reviewer.\n- In the main text, we currently do not have enough space due to the page limit, but we will try to reorganize the contents and beautify the equations for better readability in the camera-ready version."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699952903340,
                "cdate": 1699952903340,
                "tmdate": 1699952903340,
                "mdate": 1699952903340,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5yZabzDXmB",
                "forum": "kbQIWi4ZiL",
                "replyto": "KJdrBMi8f1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment 5: Regarding Code"
                    },
                    "comment": {
                        "value": "**Reviewer\u2019s Comment:**\n\n- `Thank the authors for providing the implemented codes. It would be great if the authors could provide readme.txt or demo codes so that the reviewer can reproduce the experiment results.`\n\n**Response:**\n\n- Thanks for pointing out the unclear part of our code.\n- In each folder, we already put a `training_cmd.txt` file explaining the code and providing example training commands, and the detailed hyperparameters can be found in Appendix G.1 (see the \u201chyperparameter fine-tuning\u201d parts in G.1.2 and G.1.3).\n- We have further added a `readme.txt` file to provide overall guidance to readers. Please check the updated supplementary material."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699952926127,
                "cdate": 1699952926127,
                "tmdate": 1699952926127,
                "mdate": 1699952926127,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xdIMxBSz1C",
                "forum": "kbQIWi4ZiL",
                "replyto": "KJdrBMi8f1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment 6: Regarding Constraint Coefficient $\\beta$"
                    },
                    "comment": {
                        "value": "**Reviewer\u2019s Comment:**\n\n- `As for the experiments, the reviewer has one question, how to choose the parameter` $\\beta$`? Have the authors done some ablation studies on the choices of` $\\beta$`?`\n\n**Response:**\n\n- **********************************************************************************For facility location and maximum coverage:********************************************************************************** In Appendices G.1.2 and G.1.3, we discussed how we choose the hyperparameters for the proposed method UCom2 and the CardNN baseline method by Wang et al. (2023). For UCom 2, we fine-tune the learning rate and constraint coefficient (i.e., $\\beta$).\n    - For random graphs, we choose the best hyperparameter setting w.r.t. the objective on the training set, because the distribution of the training set and the distribution of the test set are the same.\n    - For real-world graphs, we choose the smallest graph in each group of datasets as the validation graph, and we choose the best hyperparameter setting w.r.t. the objective on the validation graph. In each experiment, the training set, validation set, and test set are pairwise disjoint.\n- **********************************************************************************For robust coloring:********************************************************************************** We do not fine-tune the hyperparameters for the proposed method UCom2. Instead, we consistently use learning rate $\\eta = 0.1$, and the constraint coefficient $\\beta$ is set as the highest soft penalty, i.e., $\\max_{e = (u, v) \\in E_s} \\log (1 - P(e))$.\n- We have further conducted ablation studies on the choices of $\\beta$, we summarize our observations as follows:\n    - **********************************************************************************For facility location and maximum coverage:**********************************************************************************\n        - For random graphs, since the distribution of the training set and the distribution of the test set are the same, the originally used $\\beta$ values perform well, usually the best among the candidates.\n        - For real-world graphs, the originally used $\\beta$ values do not achieve the best performance in some cases. In our understanding, this is because we use the smallest graph in each group of datasets as the validation graph, while the smallest graph possibly has a slightly different data distribution from the other graphs in the group, i.e., the test set.\n        - Overall, certain **sensitivity** w.r.t $\\beta$ can be observed, but usually, multiple $\\beta$ values can achieve reasonable performance.\n    - **********************************************************************************For robust coloring:**********************************************************************************\n        - Overall, all the candidate $\\beta$ values can achieve similar performance.\n        - In other words, the performance of our method is not very sensitive to the value of $\\beta$ on robust coloring.\n- The detailed results are as follows. The numbers here are objectives, and **the results with the originally used constraint coefficient (NOT the best results) are marked in bold**.\n\n| facility location (smaller the better) | rand500 | rand800 | starbucks | mcd | subway |\n| --- | --- | --- | --- | --- | --- |\n| $\\beta = 1e-1$ | 2.50 | 2.47 | 0.31 | 1.02 | 1.75 |\n| $\\beta = 1e-2$ | **2.53** | **2.39** | **0.31** | **1.05** | **1.89** |\n| $\\beta = 1e-3$ | 3.19 | 2.79 | 1.85 | 1.41 | 3.83 |\n\n| maximum coverage (larger the better) | rand500 | rand1000 | twitch | railway |\n| --- | --- | --- | --- | --- |\n| $\\beta = 10$ | 43744.80 | 87165.08 | 33801.80 | **7639.00** |\n| $\\beta = 100$ | 44382.36 | 88543.73 | **33828.40** | 7628.00 |\n| $\\beta = 500$ | **44608.67** | **89306.85** | 33825.80 | 7601.50 |\n\n| robust coloring (smaller the better) | collins | collins | gavin | gavin | krogan | krogan | ppi | ppi |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| constraint coefficient $\\beta$ | 18 colors | 25 colors | 8 colors | 15 colors | 8 colors | 15 colors | 47 colors | 50 colors |\n| $\\frac{1}{2}\\beta_0$ | 78.32 | 15.61 | 46.56 | 6.70 | 52.04 | 0.87 | 2.93 | 1.01 |\n| $\\beta_0$ (originally used) | **82.26** | **15.16** | **42.99** | **6.72** | **52.44** | **0.87** | **2.93** | **1.01** |\n| $2\\beta_0$ | 81.17 | 15.83 | 44.96 | 6.77 | 55.25 | 0.87 | 2.93 | 1.01 |\n\n- We have included the ablation studies in the revised manuscript too. See Appendix G.3.4 in the revised manuscript for more details."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699953021113,
                "cdate": 1699953021113,
                "tmdate": 1699961842176,
                "mdate": 1699961842176,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JUkwFVRW7z",
                "forum": "kbQIWi4ZiL",
                "replyto": "KJdrBMi8f1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment 7: Regarding Contributions"
                    },
                    "comment": {
                        "value": "**Reviewer\u2019s Comment:**\n\n- `The core contributions are two folds: (1) the author claimed the expectation` $\\tilde{f}:[0,1]^n\\rightarrow \\mathbb{R}$ `of an optimization objective` $f:\\{0,1\\}^n\\rightarrow \\mathbb{R}$`, defined by` $\\mathbb{E}_{X\\sim p}f(X)$`, is differentiable and entry-wise concave with respect to` $p$`; (2) the authors conducted incremental greedy derandomization.`\n\n**Response:**\n\n- Thanks for your detailed review and your acknowledgment of our contributions, especially theoretical contributions. We would like to further clarify our contributions.\n- Our contributions regarding objective construction are **more than just theoretical results** (i.e., Theorems 1 and 3), but also include a practical and principle scheme, as well as the derivations for the considered complex conditions. In other words, we not only prove that such good probabilistic objectives exist, but also (1) show how to construct them in general and (2) showcase derivations for specific cases. Let us elaborate below.\n- **Our derivations on the considered complex conditions are nontrivial.** In Section 4, for each considered complex condition, we derive a **probabilistic objective** and **incremental updates** that abide by the proposed schemes. Such derivations (see Lemmas 1-8 and their proofs) are nontrivial, containing solid technical contributions. Specifically, the results by Karalias & Loukas (2020) and Wang et al. (2022) cannot produce such derivations.\n- **Our schemes provide practical general guidelines.** Using the results by Karalias & Loukas (2020) and Wang et al. (2022), when encountering a combinatorial optimization (CO) problem with complex conditions (e.g., any problem considered in this work), one can only know what properties a good probabilistic objective should satisfy, but do not have any practice guideline for how to construct a good one. By contrast, our scheme provides a practical guideline for any problem:\n    - If the encountered problem involves any complex condition considered in this work, our derivations can be immediately used. We have shown the generality of the considered conditions by various problems in Section 5 and Appendix F. That is, we expect that the considered conditions are involved in many problems.\n    - Even if the encountered problem involves some conditions that are not considered in this work, one can still follow our template in Section 4 to construct probabilistic objectives (and incremental updates) for those conditions.\n    - After dealing with each condition involved in the encountered problem, one can always follow our template in Section 5 to obtain a theoretically desirable probabilistic objective (and incremental updates) for the whole problem."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699953115991,
                "cdate": 1699953115991,
                "tmdate": 1699953115991,
                "mdate": 1699953115991,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kUlL5sGaCM",
                "forum": "kbQIWi4ZiL",
                "replyto": "cSLoXWzkky",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Reviewer_yrHB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Reviewer_yrHB"
                ],
                "content": {
                    "title": {
                        "value": "Response to the revised proof of Theorem 1"
                    },
                    "comment": {
                        "value": "1. Thanks for the revised proof. It's much clear now, but there are some small mistakes. The second $v\\neq i$ above should be $u\\neq i$.\n\n2. The reviewer understand linearity is a special case of concavity. The question is why not saying \"linear\" rather than \"concavity\" in Theorem 1? Could all the analyses be true for entry-wise linear function $\\tilde{f}$ throughout this paper?"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699961446463,
                "cdate": 1699961446463,
                "tmdate": 1699961446463,
                "mdate": 1699961446463,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lNFqbtuzOW",
                "forum": "kbQIWi4ZiL",
                "replyto": "KJdrBMi8f1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer yrHB,\n\n1. Thanks for your reply! We have just corrected that typo.\n2. We only mentioned entry-wise concavity because that is what we need for Target 1, especially the good property (P4). We can definitely state the \"linearity\" in Theorem 1, but this does not enable us to have stronger claims for our other theoretical results. Importantly, yes, all the analyses throughout this paper are still true for entry-wise linear functions. More specifically, all the analyses only require concavity, having a stronger condition (i.e., linearity) is definitely fine.\n\nIf there are still unclear points, please let us know!\n\nBest,\n\nSubmission1650 Authors"
                    },
                    "title": {
                        "value": "RE: Response to the revised proof of Theorem 1"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699962785143,
                "cdate": 1699962785143,
                "tmdate": 1699963806046,
                "mdate": 1699963806046,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Eei3oaNUEP",
                "forum": "kbQIWi4ZiL",
                "replyto": "58xy5hrN1l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Reviewer_yrHB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Reviewer_yrHB"
                ],
                "content": {
                    "title": {
                        "value": "Response to the revised proof of Lemma 4"
                    },
                    "comment": {
                        "value": "Thanks for your clarification. Unfortunately, there are some small mistakes. The third term in your second identity is wrong, i.e., $p_{v_k}d_{k}$ is wrong. Also, you haven't considered the case where $p_{v_j}=1$."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302454139,
                "cdate": 1700302454139,
                "tmdate": 1700302454139,
                "mdate": 1700302454139,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QGPWMvtpQW",
                "forum": "kbQIWi4ZiL",
                "replyto": "0iU0ENiYYK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Reviewer_yrHB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Reviewer_yrHB"
                ],
                "content": {
                    "title": {
                        "value": "Response to the revised proof of Theorem 3"
                    },
                    "comment": {
                        "value": "Thanks for the revised proof. The reviewer is wonder whether the condition, $\\sum_{r\\in d}p_{ir}\\tilde{f}(der(i,r;p);G)\\leq \\tilde{f}(p;G)$, has been used. If not, how the authors claimed \"completing the proof on P entry-wise concavity\"?"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700306265263,
                "cdate": 1700306265263,
                "tmdate": 1700306265263,
                "mdate": 1700306265263,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T7e2hSpbDz",
                "forum": "kbQIWi4ZiL",
                "replyto": "KJdrBMi8f1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "title": {
                        "value": "RE: Response to the revised proof of Lemma 4"
                    },
                    "comment": {
                        "value": "Dear Reviewer yrHB,\n\nReally thank you for your careful review. We are sorry for being careless and for the small typos.\n1. You are right! The $p_{v_k} d_k$ should be $p_{v_t} d_t$. We have corrected that in the updated manuscript.\n2. When $p_{v_j} = 1$, we would indeed have \"division by zero\" problems. \n- We have considered that and we have covered that special case in footnote 6 in the main text (see page 5).\n- In practice (i.e., in our implementation), we use a small $\\epsilon > 0$ and make sure that each $p_i \\in [\\epsilon, 1 - \\epsilon]$ for numerical stability. We have further clarified this in the updated manuscript (see the \"implementation details\" in Section 4.1 and the updated footnote 6).\n\nWe sincerely thank you for your detailed review again, which really improves the quality of the mathematical language in our manuscript.\n\nBest,\n\nSubmission1650 Authors"
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700314197224,
                "cdate": 1700314197224,
                "tmdate": 1700314390463,
                "mdate": 1700314390463,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xRCfHHapBy",
                "forum": "kbQIWi4ZiL",
                "replyto": "KJdrBMi8f1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "title": {
                        "value": "RE: Response to the revised proof of Theorem 3"
                    },
                    "comment": {
                        "value": "Dear Reviewer yrHB,\n\nThanks for your reply! Let us clarify it. As mentioned in Section 4.5:\n- For non-binary cases, a function $\\tilde{f}: [0, 1]^{n \\times c} \\to \\mathbb R$ is *entry-wise concave* if $\\sum_{r \\in d} p_{ir} \\tilde{f}(\\operatorname{der}(i, r; p); G) \\leq \\tilde{f}(p; G), \\forall G, p, i$.\n- In Theorem 3, we want to prove that this function $\\tilde g$ is entry-wise concave. Note that the symbols are different in the definition ($\\tilde f$) and in the statement ($\\tilde g$).\n- Therefore, we need to prove that $\\sum_{r \\in d} p_{ir} \\tilde{g}(\\operatorname{der}(i, r; p); G) \\leq \\tilde{g}(p; G), \\forall G, p, i$.\n- In our proof, we show that for each $p$ and $i$, $\\tilde{g}(p) = \\sum_{r \\in d} p_{ir} \\tilde{g}(\\operatorname{der}(i, r; p))$, which, similar to the situation in Theorem 1, implies that $\\tilde{g}(p) \\geq \\sum_{r \\in d} p_{ir} \\tilde{g}(\\operatorname{der}(i, r; p))$, completing the proof for entry-wise concavity. We have further clarified this in our updated manuscript.\n\nIn our understanding, our previous statement and proof might be slightly confusing in that\n1. We did not mention $G$ in our proof, but we had $G$ in our definition of entry-wise concavity.\n- We have removed $G$ in our definition of entry-wise concavity, since we realized that the definition of a function $[0, 1]^{n \\times c} \\to \\mathbb R$ (or $[0, 1]^{n} \\to \\mathbb R$ for binary cases) should not depend on a specific input graph $G$.\n- We have revised the related definitions in our updated manuscript (see Sections 2.2.2 and 4.5).\n2. Inequality ($\\leq$) was used in the statement, while we showed equality ($=$) in our proof.\n- This is a similar issue as we have discussed for Theorem 1. Equality is a special case of, and thus stronger than, inequality. We have further clarified this above and explicitly mentioned that in our proof in the updated manuscript.\n3. The symbol $\\tilde f$ was used in the definition, while $\\tilde g$ was used in the statement.\n- For that, we believe it is proper to use different symbols in definitions and theorems, and we have further clarified this above.\n\nThank you again for your reply. We are always glad to clarify anything you find unclear! You have been very helpful in improving the clarity of our manuscript.\n\nBest,\n\nSubmission1650 Authors"
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700317305940,
                "cdate": 1700317305940,
                "tmdate": 1700317493430,
                "mdate": 1700317493430,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UbHtdfAcjn",
                "forum": "kbQIWi4ZiL",
                "replyto": "xdIMxBSz1C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Reviewer_yrHB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Reviewer_yrHB"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for conducting the ablation studies for the hyperparameter $\\beta$. (This could be a naive question.) Why not the authors choose a larger graph as the validation graph to increase the stability of the performance for real-world graphs? How small is the graph (number of vertices and edges) that the authors choose compared with other training graphs?"
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461600991,
                "cdate": 1700461600991,
                "tmdate": 1700461600991,
                "mdate": 1700461600991,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SwwFxSz7Az",
                "forum": "kbQIWi4ZiL",
                "replyto": "P9bQmGYNdN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Reviewer_yrHB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Reviewer_yrHB"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your hard work. The reviewer is very appreciate for the authors' work and discussions. The rating has been changed to \"marginally above the acceptance threshold\". All the best!"
                    }
                },
                "number": 34,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657485431,
                "cdate": 1700657485431,
                "tmdate": 1700657485431,
                "mdate": 1700657485431,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cEJTzu0sFt",
            "forum": "kbQIWi4ZiL",
            "replyto": "kbQIWi4ZiL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1650/Reviewer_RnVS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1650/Reviewer_RnVS"
            ],
            "content": {
                "summary": {
                    "value": "The unsupervised probabilistic method for combinatorial optimization is a hot topic in recent machine learning community. The paper proposes UCom2 as a unified framework with principled probabilistic objective construction scheme that provably satisfies the good properties, and a fast and effective de-randomization scheme with a quality guarantee. Under this framework, the paper conduct intensive experiments on combinatorial optimizations with hard constraints and obtains the state-of-the-art performance among unsupervised probabilistic methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper formally gives principled criteria for objective functions and de-randomization, which completes the framework of unsupervised probabilistic methods for combinatorial optimization. Also, the paper proves the framework is simple but guaranteed to be effective.\n* Then paper conduct intensive experiments to empirically demonstrate UCom2 is general and practical. The experiment settings are detailedly provided and the comparison with baselines are properly discussed."
                },
                "weaknesses": {
                    "value": "* Though provides a unified view, the proposed framework is basically doing the same thing as previous methods. Leading the novelty is limited. \n* The ad-hoc incremental difference is designed for each problem. Also, only evaluating the difference is a commonly used method to reduce computation. To me, it is more like an engineering effort rather than a machine learning method.\n* Minor: the paper looks a bit crowded, making reading a bit tired."
                },
                "questions": {
                    "value": "* I am wondering whether the author tried to compare the performance of UCom2 with supervised learning. I am interested in their gap.\n* Since Ucom2 designed a principle de-randomization, I am curious whether it is possible to make the objective function being de-randomization arrogant?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1650/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698874883360,
            "cdate": 1698874883360,
            "tmdate": 1699636093166,
            "mdate": 1699636093166,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LhUtenq9n0",
                "forum": "kbQIWi4ZiL",
                "replyto": "cEJTzu0sFt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment 1: Regarding Novelty"
                    },
                    "comment": {
                        "value": "**Reviewer\u2019s Comment:**\n\n- `Though provides a unified view, the proposed framework is basically doing the same thing as previous methods. Leading the novelty is limited.`\n\n**Response:**\n\n- We believe that there is nothing wrong with \u201cstanding on the shoulders of giants\u201d. Although based on existing works, our results are still **theoretically novel** and **empirically useful**.\n- **Our derivations on the complex conditions are nontrivial.** We would like to emphasize our contributions regarding our **derivations** of probabilistic objectives and incremental updates. We first propose two **high-level schemes** (Schemes 1 & 2) in Section 3. Then, in Section 4, following the two high-level schemes, for each considered complex condition, we derive a **probabilistic objective** and **incremental updates** that abide by the proposed schemes. Such derivations (see Lemmas 1-8 and their proofs) are nontrivial, containing solid technical contributions. Specifically, the results by Karalias & Loukas (2020) and Wang et al. (2022) cannot produce such derivations.\n- **Our schemes provide practical general guidelines.** Using the results by Karalias & Loukas (2020) and Wang et al. (2022), when encountering a combinatorial optimization (CO) problem with complex conditions (e.g., any problem considered in this work), one can only know what properties a good probabilistic objective should satisfy, but do not have any practice guideline for how to construct a good one. By contrast, our scheme provides a practical guideline for any problem:\n    - If the encountered problem involves any complex condition considered in this work, our derivations can be immediately used. We have shown the generality of the considered conditions by various problems in Section 5 and Appendix F. That is, we expect that the considered conditions are involved in many problems.\n    - Even if the encountered problem involves some conditions that are not considered in this work, one can still follow our template in Section 4 to construct probabilistic objectives (and incremental updates) for those conditions.\n    - After dealing with each condition involved in the encountered problem, one can always follow our template in Section 5 to obtain a theoretically desirable probabilistic objective (and incremental updates) for the whole problem.\n- **Our derandomization scheme is theoretically and empirically better.** The proposed derandomization scheme achieves a stronger quality guarantee than the existing ones by Karalias & Loukas (2020) and Wang et al. (2022). Specifically, Karalias & Loukas (2020) provided a Markov bound based on random sampling, Wang et al. (2022) provided a deterministic bound based on iterative rounding, and we further improved it with local minimality of the final derandomized decision. See also our ablation study in Appendix G.3.2 comparing our greedy derandomization and iterative rounding by Wang et al. (2022), where we empirically validated the superiority of greedy derandomization over iterative rounding."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699950309238,
                "cdate": 1699950309238,
                "tmdate": 1699950309238,
                "mdate": 1699950309238,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AwVCPWQL0v",
                "forum": "kbQIWi4ZiL",
                "replyto": "cEJTzu0sFt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment 2: Regarding Generality and Significance of Incremental Difference"
                    },
                    "comment": {
                        "value": "**Reviewer\u2019s Comment:**\n\n- `The ad-hoc incremental difference is designed for each problem. Also, only evaluating the difference is a commonly used method to reduce computation. To me, it is more like an engineering effort rather than a machine learning method.`\n\n**Response:**\n\n- We would like to clarify that incremental differences (and probabilistic objectives) are designed for each **complex condition** instead of each problem. For each considered complex condition, we derive a probabilistic objective and incremental updates that abide by the proposed schemes. Such derivations are nontrivial, containing solid **technical contributions**.\n    - See also our response to Comment 1.\n- The complex conditions considered in this work are **commonly involved in many problems** with real-world meanings and applications, as shown in Section 5 and Appendix F.\n- For each problem, we first analyze which complex conditions it involves and then **reuse and combine (instead of constructing from scratch)** the incremental differences (and probabilistic objectives) we have derived for each involved condition.\n- We believe that deriving and using incremental differences are **mathematical and algorithmic** efforts, instead of an \u201cengineering effort\u201d, and deriving the incremental differences per se is a solid **technical contribution**."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699950343893,
                "cdate": 1699950343893,
                "tmdate": 1699950343893,
                "mdate": 1699950343893,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aDfWjaFnmG",
                "forum": "kbQIWi4ZiL",
                "replyto": "cEJTzu0sFt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment 3: Regarding Supervised Learning"
                    },
                    "comment": {
                        "value": "**Reviewer\u2019s Comment:**\n\n- `I am wondering whether the author tried to compare the performance of UCom2 with supervised learning. I am interested in their gap.`\n\n**Response:**\n\n- As discussed in existing works, using supervised learning for combinatorial optimization might encounter the following problems:\n    - it can lead to training instability and/or poor generalization [r1, r2, r3]\n    - it needs labels, i.e., optimal solutions for training instances, which are expensive to obtain, especially when the considered problem is NP-hard and/or the problem instances are large [r1, r3, r4]\n- Even obtaining the \u201clabels\u201d for the training instances would take too long, because all the problems we consider in our experiments are NP-hard, and the scales of our training instances are relatively large. Moreover, even with the labels obtained, the training is expected to be difficult.\n- Indeed, the existing supervised-learning-based methods for combinatorial optimization have been limited to small instances and specific problems (e.g., TSP, routing). Notably, some efforts have been made to improve the generalizability [r5, r6], and we definitely would like to explore supervised learning methods for CO (especially CO with complex conditions) as a future direction.\n- References:\n    - [r1] Karalias and Loukas. \"Erdos goes neural: an unsupervised learning framework for combinatorial optimization on graphs.\" NeurIPS\u201920\n    - [r2] Joshi et al. \"On learning paradigms for the travelling salesman problem.\" arXiv:1910.07210\n    - [r3] Wang et al. \"Unsupervised learning for combinatorial optimization with principled objective relaxation.\" NeurIPS\u201922\n    - [r4] Yehuda et al. \"It\u2019s not what machines can learn, it\u2019s what we cannot teach.\" ICML\u201920\n    - [r5] Fu et al. \"Generalize a small pre-trained model to arbitrarily large TSP instances.\" AAAI\u201921\n    - [r6] Joshi et al. \"Learning the travelling salesperson problem requires rethinking generalization.\" Constraints 27.1-2 (2022): 70-98."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699950413998,
                "cdate": 1699950413998,
                "tmdate": 1699950413998,
                "mdate": 1699950413998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JvOv3OsVmY",
                "forum": "kbQIWi4ZiL",
                "replyto": "cEJTzu0sFt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment 4: Regarding Manuscript Density"
                    },
                    "comment": {
                        "value": "**Reviewer\u2019s Comment:**\n\n- `Minor: the paper looks a bit crowded, making reading a bit tiring.`\n\n**Response:**\n\n- Sorry for the inconvenience. We have many contents to be put within the page limit.\n- It might cause confusion if we reorganize the contents during the rebuttal period, but we will try to reorganize the contents for better readability in the camera-ready version."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699950437789,
                "cdate": 1699950437789,
                "tmdate": 1699950437789,
                "mdate": 1699950437789,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sd26OiOLHh",
                "forum": "kbQIWi4ZiL",
                "replyto": "cEJTzu0sFt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment 5: Regarding Derandomization Arrogant Objectives"
                    },
                    "comment": {
                        "value": "**Reviewer\u2019s Comment:**\n\n- `Since Ucom2 designed a principle de-randomization, I am curious whether it is possible to make the objective function being derandomization arrogant?`\n\n**Response:**\n\n- In our understanding, the reviewer is asking \u201cCan we design objective functions regardless of the derandomization method?\u201d\n- If so, the answer is yes! We are already doing this. Our objective construction scheme does not rely on any specific derandomization method, and the differentiable optimization step is independent of the derandomization step.\n- Further improving the derandomization scheme for UL4CO is actually one of our future directions.\n- Please let us know if we misunderstood the question or if there are still unclear points."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699965652137,
                "cdate": 1699965652137,
                "tmdate": 1699965652137,
                "mdate": 1699965652137,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o2w3DjXOkI",
                "forum": "kbQIWi4ZiL",
                "replyto": "cEJTzu0sFt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer RnVS,\n\nThank you for your insightful review again!\n\nWe are wondering how much our responses have addressed your concerns, and whether you have further questions and comments.\nIf you have any specific questions you would like us to elaborate on, please let us know. We value your insights and are open to further discussion to enhance our work.\n\nThank you once again for your time.\n\nBest,\n\nSubmission1650 Authors"
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700458084159,
                "cdate": 1700458084159,
                "tmdate": 1700458084159,
                "mdate": 1700458084159,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WGebbdJcVC",
            "forum": "kbQIWi4ZiL",
            "replyto": "kbQIWi4ZiL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1650/Reviewer_M2Ai"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1650/Reviewer_M2Ai"
            ],
            "content": {
                "summary": {
                    "value": "The authors are motivated by the research in unsupervised combinatorial optimization and propose to extend prior work in this topic to more complex optimization problems. In particular, they seek to develop a principled approach to constructing probabilistic objectives and effective derandomization scheme with guarantee on solution quality."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The ideas around unsupervised combinatorial optimization are interesting and authors place their contribution well by discussing prior results and adequately motivating their work. The theoretical quality guarantee from derandomization scheme seems to be a reasonable contribution."
                },
                "weaknesses": {
                    "value": "Since the paper builds on the specific work Karalias & Loukas (2020) and Wang et al. (2022), I was not able to get a solid understanding of conceptual contributions of the paper. Theoretical results - as claimed by the authors - are follow fairly standard arguments. They are based on standard (basic) optimization analysis. While the tightness of these results to the original approach proposed by  Karalias & Loukas might be worthwhile, we do not get a sufficient understanding of the generality of these results. I would have appreciated seeing a more pointed discussion on why rounding/derandomization schemes in classical combinatorial optimization are not helpful here? Without such a consideration, the contribution might myopically advance the idea of pushing differentiable optimization into combinatorial optimization, but might miss on building on rich set of existing results in combinatorial optimization."
                },
                "questions": {
                    "value": "- how does the result on goodness of greedy derandomization compare with similar ideas in combinatorial optimization? \n- what do we mean by problems with \"complex conditions\"? \n- what are the features of the problems studied in the experiments section that enable a good solution guarantee after derandomization (vs not)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1650/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1650/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1650/Reviewer_M2Ai"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1650/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699357136543,
            "cdate": 1699357136543,
            "tmdate": 1699636093081,
            "mdate": 1699636093081,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "19SkWm6JUJ",
                "forum": "kbQIWi4ZiL",
                "replyto": "WGebbdJcVC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment 1: Regarding \u201cComplex Conditions\u201d"
                    },
                    "comment": {
                        "value": "**Reviewer\u2019s Comment:**\n\n- `What do we mean by problems with \"complex conditions\"?`\n\n**Response:**\n\n- As mentioned in the introduction, in this work, \u201ccomplex conditions\u201d include **complex optimization objectives and/or constraints that are mathematically hard to handle**.\n- More specifically, for such conditions, **deriving probabilistic objectives and incremental updates is nontrivial**, which is one of our main conceptual and technical contributions.\n- It is **non-trivial to formally define** \u201ccomplex conditions\u201d, but we can summarize some **common characteristics** of the complex conditions considered in this work, i.e., the considered complex conditions usually involve a combination of **multiple dependent** sub-events:\n    - **Cardinality constraints:** the selection of multiple elements, where the dependency lies in that if one element is not selected, another element can be selected without the cardinality unchanged, i.e., whether an element can be selected without violating the constraints depends on how many other elements are selected.\n    - **Optimum w.r.t. a subset:** the optimum of multiple choices in the subset, where the dependency lies in that an element in the subset is optimal (and thus used to compute the objective) only if all the better elements are not chosen.\n    - **Covered:** the coveredness of multiple elements in the considered set.\n    - **Cliques:** the adjacency between each chosen node pair, where the dependency lies in that, whether a node violates the constraints depends on whether other nodes that are not adjacent to it are chosen.\n    - **Non-binary decisions:** in general, the number of sub-events increases significantly with non-binary decisions, e.g., when the number of decisions increases from 2 to 4, the number of possible sub-events gets squared ($2^n \\to 4^n$).\n    - **Uncertainty:** in general, with intrinsic uncertainty in the problem, we need to consider multiple possible scenarios even when we fix decisions, which increases the total number of possible sub-events we need to consider.\n    - One direct consequence is that it is **non-trivial** to construct the corresponding probabilistic objectives by **enumerating** all possible situations, which often requires $\\Omega(2^n)$ evaluations of the objective function, and the number of required evaluations is even higher with non-binary decisions and/or uncertainty.\n- We show **various examples** of complex conditions (see Sec. 4) and problems involving such conditions (see Sec. 5 and Appendix F)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699950017316,
                "cdate": 1699950017316,
                "tmdate": 1699961429417,
                "mdate": 1699961429417,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TMeMUBP6Gp",
                "forum": "kbQIWi4ZiL",
                "replyto": "WGebbdJcVC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment 2: Regarding Existing Techniques in Classical Combinatorial Optimization"
                    },
                    "comment": {
                        "value": "**Reviewer\u2019s Comment:**\n\n- `I would have appreciated seeing a more pointed discussion on why rounding/derandomization schemes in classical combinatorial optimization are not helpful here. Without such a consideration, the contribution might myopically advance the idea of pushing differentiable optimization into combinatorial optimization, but might miss on building on the rich set of existing results in combinatorial optimization.`\n- `How does the result on the goodness of greedy derandomization compare with similar ideas in combinatorial optimization?`\n\n**Response:**\n\n- Thanks for the advice on enriching the discussions on the connections to the existing rounding/derandomization techniques in classical combinatorial optimization (CO). Such discussions have been indeed missing in this line of research on probabilistic-method-based UL4CO.\n- We found that the existing rounding/derandomization techniques in classical CO, especially randomized rounding, are **actually helpful** and have been **implicitly generalized by existing works**.\n    - The **sampling** in the result by Karalias & Loukas (2020) (see Theorem 5 in Appendix A.3) can be seen as a generalization of **independent randomized rounding**, where each entry is derandomized by an independent Bernoulli trial.\n    - The **iterative rounding** by Wang et al. (2022) (see Theorem 6 in Appendix A.3) can be seen as a generalization of randomized rounding using **the method of conditional probabilities**.\n- We **could not find** existing randomized rounding techniques similar to our derandomization scheme, which implies that the idea in our derandomization scheme might be even useful in randomized rounding for classical CO.\n- Regarding the question `How does the result on the goodness of greedy derandomization compare with similar ideas in combinatorial optimization?`, as mentioned above, we believe that some ideas of randomized rounding **have been implicitly generalized** by Karalias & Loukas (2020) and Wang et al. (2022), while as shown in this work, the proposed greedy derandomization scheme achieves a stronger theoretical guarantee and better empirical performance.\n- We **could not find** other advanced rounding/derandomization schemes that are directly applicable to our case. It would be highly appreciated if the reviewer could suggest some specific rounding/derandomization schemes in classical combinatorial optimization we can consider and apply.\n- Again, we would like to emphasize our contributions regarding our nontrivial **derivations** of probabilistic objectives and incremental updates. For classical CO methods using integer/linear programming, the objective construction/relaxation is usually done by simply relaxing the range of variables without changing the formula. Such a naive way might violate the good properties (see Target 1 in Section 3.1) in the UL4CO setting, and can be improper.\n- We believe that the solutions obtained by our method (or machine learning for CO in general) **can be immediately helpful for classical CO**, e.g., providing initial solutions, and fast evaluation. Also, we definitely would like to explore the possibilities of incorporating existing classical CO techniques into differentiable combinatorial optimization."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699950060205,
                "cdate": 1699950060205,
                "tmdate": 1699961438177,
                "mdate": 1699961438177,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dMDDCtDQCM",
                "forum": "kbQIWi4ZiL",
                "replyto": "WGebbdJcVC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment 3: Regarding Conceptual Contributions"
                    },
                    "comment": {
                        "value": "**Reviewer\u2019s Comment:**\n\n- `Since the paper builds on the specific work by Karalias & Loukas (2020) and Wang et al. (2022), I was not able to get a solid understanding of the conceptual contributions of the paper. Theoretical results - as claimed by the authors - follow fairly standard arguments. They are based on standard (basic) optimization analysis. While the tightness of these results to the original approach proposed by Karalias & Loukas might be worthwhile, we do not get a sufficient understanding of the generality of these results.`\n\n**Response:**\n\n- The reviewer seemingly focused on greedy derandomization *per se* as an isolated component. We would like to clarify that \n  - The goodness of greedy derandomization relies on the goodness of probabilistic objectives (specifically, the entry-wise concavity of the probabilistic objective, which is ensured by our principled construction), i.e., \"how to construct good probabilistic objectives such that greedy derandomization works well on them\" is nontrivial. Overall, **Schemes 1 and 2 synergize as a whole** and we should see the values of each scheme considering that they complement each other.\n  - Our derivations on each complex condition are nontrivial, which we will elaborate on below.\n- **Our derivations on the complex conditions are nontrivial.** We would like to emphasize our contributions regarding our **derivations** of probabilistic objectives and incremental updates. We first propose two **high-level schemes** (Schemes 1 & 2) in Section 3. Then, in Section 4, following the two high-level schemes, for each considered complex condition, we derive a **probabilistic objective** and **incremental updates** that abide by the proposed schemes. Such derivations (see Lemmas 1-8 and their proofs) are nontrivial, containing solid technical contributions. Specifically, the results by Karalias & Loukas (2020) and Wang et al. (2022) cannot produce such derivations.\n- **Our schemes provide practical general guidelines.** Using the results by Karalias & Loukas (2020) and Wang et al. (2022), when encountering a combinatorial optimization (CO) problem with complex conditions (e.g., any problem considered in this work), one can only know what properties a good probabilistic objective should satisfy, but do not have any practice guideline for how to construct a good one. By contrast, our scheme provides a practical guideline for any problem:\n    - If the encountered problem involves any complex condition considered in this work, our derivations can be immediately used. We have shown the generality of the considered conditions by various problems in Section 5 and Appendix F. That is, we expect that the considered conditions are involved in many problems.\n    - Even if the encountered problem involves some conditions that are not considered in this work, one can still follow our template in Section 4 to construct probabilistic objectives (and incremental updates) for those conditions.\n    - After dealing with each condition involved in the encountered problem, one can always follow our template in Section 5 to obtain a theoretically desirable probabilistic objective (and incremental updates) for the whole problem.\n- **Our derandomization scheme is theoretically and empirically better.** As the reviewer pointed out, the proposed derandomization scheme achieves a stronger quality guarantee than the existing ones by Karalias & Loukas (2020) and Wang et al. (2022). Specifically, Karalias & Loukas (2020) provided a Markov bound based on random sampling, Wang et al. (2022) provided a deterministic bound based on iterative rounding, and we further improved it with local minimality of the final derandomized decision. See also our ablation study in Appendix G.3.2 comparing our greedy derandomization and iterative rounding by Wang et al. (2022), where we empirically validated the superiority of greedy derandomization over iterative rounding.\n- **Our theoretical results are nontrivial and general.**\n    - Theorem 1 (principled objective construction) is nontrivial, where we show that the expectation of **any discrete function** is differentiable and entry-wise concave, which enables us to principally construct good probabilistic objectives (see Scheme 1).\n    - Theorem 2 (goodness of greedy derandomization) is nontrivial. The statement depends on a desirable probabilistic objective, which is guaranteed by our objective construction scheme. This shows the **synergy** between our two proposed schemes.\n    - Theorems 3 and 4 regarding problems with non-binary decisions are novel, since **non-binary decisions were not discussed** in the existing works by Karalias & Loukas (2020) and Wang et al. (2022).\n    - Our main theoretical results (Theorems 1-4) are **general**, holding true **for any complex condition** and thus for any problem involving such conditions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699950120490,
                "cdate": 1699950120490,
                "tmdate": 1700734433449,
                "mdate": 1700734433449,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FipSkJvGKR",
                "forum": "kbQIWi4ZiL",
                "replyto": "WGebbdJcVC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment 4: Regarding the Features of the Problems Enabling a Good Solution Guarantee"
                    },
                    "comment": {
                        "value": "**Reviewer\u2019s Comment:**\n\n- `What are the features of the problems studied in the experiments section that enable a good solution guarantee after derandomization (vs not)?`\n\n**Response:**\n\n- The **main theorems** on the goodness of greedy derandomization (Theorems 2 and 4) hold true **for any problem**.\n    - **For any problem**, the derandomized decision is guaranteed to be no worse than the initial probabilistic decision obtained by differentiable optimization. See (G2) in Theorem 2.\n    - **For any problem**, the derandomized decision is guaranteed to be a local minimum. See (G3) in Theorem 2.\n- We chose the problems because they **involve the complex conditions** we considered, and have **real-world meanings and applications**.\n- We **did not choose problems in favor of our method**. Notably, discrete greedy algorithms are used as a baseline method in each problem in our experiments and the proposed method often outperforms greedy algorithms."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699950156522,
                "cdate": 1699950156522,
                "tmdate": 1699961456588,
                "mdate": 1699961456588,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VrOFv30GOG",
                "forum": "kbQIWi4ZiL",
                "replyto": "TMeMUBP6Gp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Reviewer_M2Ai"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Reviewer_M2Ai"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699964267900,
                "cdate": 1699964267900,
                "tmdate": 1699964267900,
                "mdate": 1699964267900,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9sew0On9Sp",
                "forum": "kbQIWi4ZiL",
                "replyto": "WGebbdJcVC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1650/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer M2Ai,\n\nThank you for your thorough review of our paper again. \n\nSince the discussion phase is close to the end, we would like to inquire if our responses have addressed your concerns, and if so, whether you could consider changing your rating.\n- The reviewer seemingly focused on greedy derandomization *per se* as an isolated component. We would like to clarify that (1)  \"how to construct good probabilistic objectives such that greedy derandomization works well on them\" is nontrivial (Theorem 2 requires the entry-wise concavity of the objective ensured by Theorem 1; Schemes 1 and 2 synergize as a whole), and (2) our derivations for each condition are nontrivial (Lemmas 1-8).\n- Please check our responses above for more details.\n\nWe remain fully committed to addressing any questions you may have by the end of the discussion phase.\n\nWe sincerely appreciate your time and effort in reviewing our paper. We eagerly await your response!\n\nBest,\n\nSubmission1650 Authors"
                    }
                },
                "number": 36,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1650/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697967231,
                "cdate": 1700697967231,
                "tmdate": 1700699403517,
                "mdate": 1700699403517,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]