[
    {
        "title": "Balance Beam: adaptive computation for affordable training and inference with high-throughput offloading for LLMs"
    },
    {
        "review": {
            "id": "9DGwtUvjyl",
            "forum": "lCxic1APct",
            "replyto": "lCxic1APct",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7297/Reviewer_ttTz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7297/Reviewer_ttTz"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a workflow to address the challenges of training and inference of LLMs. The workflow focuses on dynamic analysis of model-system compatibility and prioritizing computational intensity over data movement. The methods involve a hyperparameter selection strategy, and better offloading scheme."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper focuses on a critical and urgent issue, the training and inference efficiency of LLMs.\n2. The paper provides an image on GCP to reproduce their partial results."
                },
                "weaknesses": {
                    "value": "### Major issue 1\nThe paper \"**theoretically**\" analyzes the hyper-parameter selection process in Section 3.1 and provides experimental\nvalidation in Section 5.1. However, Section 3.1 cannot demonstrate effective theoretical information.\n1. The paper recommends a minimum batch size for achieving high throughput, and caching will not be employed if this threshold cannot be met. Is this threshold a hyper-parameter need to be configured? The threshold is definitely related to hardware specifications and model architectures. It is hard for other users to apply this technique. They have to tune this parameter empirically instead of configure it theoretically.\n2. The method can select appropriate batch sizes by assessing the working memory requirements per token during benchmarking. It is unclear for readers the details of the selection.\n3. The paper propose to to retain only one layer on the accelerator at any given time during inference, for checkpoint selection. Is this strategy guaranteed optimal theoretically? I think the setting of checkpoint depends on the hardware specifications and model architectures. I think the proposed is not optimal at all times.\n\nHyper-parameter balancing strategies in Section 3.1 is not a solid theoretical analysis. It seems a case study for a specific hardware/software settings and it is difficult to transfer to other settings. Users cannot follow their strategy to set hyper-parameters in an optimal way. \n\n### Other major issues\n1. The paper claims that Balance Beam is \"an workflow to optimize the trade-off between latency and throughput performance of LLMs\". However, there is no discussion on the latency in the proposed method and experiments. From my understanding, the column-wise traversal of Figure 1.a will increase the latency significantly compared to the row-wise traversal.\n2. The evaluation results are based on the authors' implementations, for both baseline and the proposed method. However, the baseline may be much weaker than the current SOTA solution. Is it possible to import a public SOTA implementation and conduct comparisons based on that?\n3. The paper use FLOPS to quantify the arithmetic intensity. FLOPS is a measure of computer performance, while arithmetic intensity is the ratio of total floating-point operations to total data movement. They do not match.\n\n### Minor issues\n1. It is not clear whether the proposed method can be applied to other hardware settings, such as other GPUs, TPUs and large scale.\n2. the FlexGen proposed in (Sheng et al., 2023) have showed -> has shown\n3. Table 2. Optimal -> Ours"
                },
                "questions": {
                    "value": "What are the limitations and potential negative impact of Balance Beam?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7297/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698563892029,
            "cdate": 1698563892029,
            "tmdate": 1699636871727,
            "mdate": 1699636871727,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "A67WO693nw",
            "forum": "lCxic1APct",
            "replyto": "lCxic1APct",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7297/Reviewer_uQcc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7297/Reviewer_uQcc"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on some hyperparameters ( KV cache, batch size, and #gradient checkpoints ) with a newly invented column-wise traversal approach. Via tuning those hyperparameters with some systematic optimization, this paper shows 2x speedups compared to baseline. This paper also proposes the approach can be applied to fine-training scenarios. Gets high acceleration on fine-tuning tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The main strength of this paper is that the newly invented FlexGen approach can be applied to training scenarios, providing less data traffic. It has been impossible to scale batch size due to memory capacity in previous offloading training scenarios. \n\nThe paper introduces new hyperparameters into offloading, so tuning hyperparameters (KVcache, batch size, and gradient checkpoints) looks important for better utilization. For OPT-175B, turning off KVcache shows optimal performance, which is especially interesting.\n\n Providing real value for the experiments make it easy for comparing with different papers. In addition, if the codes are contributed to popular framework, it would be very helpful to apply common system optimization methods to derive higher throughput with newly invented methods."
                },
                "weaknesses": {
                    "value": "I have concerns on aspects of novelty, practicality and evaluation.\n### Novelty\n- The main contribution of the paper is in providing a tunable hyperparameter space. However, the proposed hyperparameters are not newly discovered, but already existing knobs. \nPersonally, I find it interesting that the popularly used KV caching can be a new hyperparameter, and that putting it together with other parameters is a meaningful work. \nHowever, I don't think the novelty is enough to be published in ICLR.\n\n- The techniques provided from section 3.2 are mostly what's commonly used, especially in ZeRO-offload. Even though the paper does not strongly claim those as the contribution, it is not okay to introduce them in the methods section, without any citations.\n\n\n### Practicality\n- This paper suggests the batch size and sub batch size as the key hyperparameter. \nHowever, unlike inference, changing the batch size has impact on the final accuracy. Because of this, it shouldn't be tuned just for the throughput. \nIn addition, I was a little disappointed that the paper just provides the results from various tuning points. At first, I was expecting a systematic/automatic tuning software or a policy, or at least a guideline on how to achieve the goal. Without them, the contribution is quite limited.\n\n### Evaluation\n- The authors seem to acknowledge that there is accuracy impact on using large batch sizes for training (from section 6.1), but there is no evaluation on the accuracy.\n\n- Some experiments can be misleading. For training, \u2018with optimal hyperparameter settings\u2019 seems to use different effective batch sizes, and only FLOPS results are given. If effective batch size is different, the throughput difference would come from additional update steps, and this should be considered. \n\n\n### Writing\n- It's relatively minor compared to other issues, but the writing needs some improvements. For example, the paper does not clearly distinguish the training and inference. Even though they share a lot in common, some guide is needed for the readers. In addition, the description of generative inference with KV-cache is not enough to understand this work. Even though the KV cache is getting its popularity, this paper lacks enough information to understand the key aspects, such as why utilizing KV-cache is good for transformer inference.\n\n\nThere are some ambiguities and minor mistakes in this paper. \nIf Fig.1-(b) is placed next to Fig.3, it could be better to see with Sec.3.2.\nIn Sec.1, \u20185000\u2019 and \u2018s\u2019 are placed in different lines, which makes it confusing. \nIn Sec.1, Insert \u2018and\u2019 between \u2018batch size\u2019 and \u2018number of gradient checkpoints.\u2019 \nI can\u2019t find any references for gradient checkpoints and recompute."
                },
                "questions": {
                    "value": "For evaluation, are training FLOPS results measured with different effective batch sizes? \nFor training, does this work utilize only host memory? (No storage for training? )"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7297/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819445619,
            "cdate": 1698819445619,
            "tmdate": 1699636871583,
            "mdate": 1699636871583,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "ZONnBICcDv",
            "forum": "lCxic1APct",
            "replyto": "lCxic1APct",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7297/Reviewer_uduE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7297/Reviewer_uduE"
            ],
            "content": {
                "summary": {
                    "value": "The paper argues that the system hyperparameters are challenging to tune in a distributed setup and design and introduce a workflow to mitigate these challenges. The results show promising improvement over SOTA framework in inference as well as training."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "$\\mathtt{+}$ The results are promising and cover both inference and training of LLMs across a range of benchmarks.\n\n$\\mathtt{+}$ The framework seems to be straightforward to use."
                },
                "weaknesses": {
                    "value": "$\\mathtt{-}$ The contribution of the paper is limited and it is not clear whether the presented results are generalizable to other networks and setup.\n\n$\\mathtt{-}$ The majority of presented techniques are already explored and the paper mainly focuses on changing to system hyperparameters for a particular setup.\n\n$\\mathtt{-}$ The choice of the baseline is not well-supported and it is not clear whether the baseline implementation was also fully optimized for the target platform."
                },
                "questions": {
                    "value": "(Q1) Did you also tune the system hyperparameters of the baseline? \n\n(Q2) How does your approach scales to larger network (beyond 4 GPUs)?\n\n(Q3) I am curious to see whether the proposed approach has any negative impact on accuracy/training convergence/etc.? Did you see any degradation?\n\n(Q4) I commend the authors for devoting almost a page to future works. However, I think the suggested future work actually are really important to see the potential benefit of the work. In its current form, the paper seems to have a very narrow scope and no clarity on how to apply in different situations and scenarios."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7297/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699166653270,
            "cdate": 1699166653270,
            "tmdate": 1699636871469,
            "mdate": 1699636871469,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]