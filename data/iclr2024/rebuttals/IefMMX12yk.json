[
    {
        "title": "Lightweight Graph Neural Network Search with Graph Sparsification"
    },
    {
        "review": {
            "id": "aj2q9ksHth",
            "forum": "IefMMX12yk",
            "replyto": "IefMMX12yk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9366/Reviewer_hwTX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9366/Reviewer_hwTX"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel approach called GASSIP (Lightweight Graph Neural Architecture Search with Graph Sparsification and Network Pruning) for automating the design of efficient Graph Neural Networks (GNNs). It highlights the needs for lightweight GNAS and data sparsification to reduce resource requirements. GASSIP employs operation pruning and curriculum graph data sparsification to iteratively optimize GNN architectures and graph data, resulting in more efficient and accurate lightweight GNNs. Experimental results demonstrate its superiority over traditional GNNs and GNAS, achieving substantial performance improvements with significantly reduced search time."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tGASSIP is shown to significantly improve the efficiency of GNAS by reducing search time while maintaining or even enhancing the performance of GNNs.\n2.\tThe writing is easy to follow."
                },
                "weaknesses": {
                    "value": "1.\tThe research problem is not novel to the community. There have been a series of work for the co-optimization of neural architecture and data, even at the domain of graph NAS and graph data. \n2.\tI am still concerned about the motivation to work on graph NAS. Different to other neural architectures, there are only several layers in GNNs, and the number of candidate operations is limited. I believe if one only applies the popular toolkit of hyperparameter tuning, the much higher performance will be obtained. \n3.\tThe unstructured pruning of model weights makes no sense in the practical efficiency improvement. Based on the current parallel hardware (with processing of single instruction multiple data), the unstructured matrix multiplication has almost the same cost with the dense matrix multiplication."
                },
                "questions": {
                    "value": "1.\tPlease address my concerns listed in the weaknesses. \n2.\tIt is unscalable to apply a learnable mask with shape of N\\timesN in graph data. The node number in most of the graphs are at the scale of millions or even billions. \n3.\tFollowing the last question, I need to check the possibility of applying this work in the benchmark datasets of ogbn-products and paper100m.\n4.\tGraph sparsification is a very old topic, and there have been many researches being conducted to provide the principle in how to remove edges without affecting the graph structural properties (e.g., adjacency eigenvalues). For example, one can remove edges based on degrees of 1/d_i + 1/d_j, where d_i and d_j are the degrees of node i and j, respectively. It is easy to remove more than 90% of edges but maintain the comparable performance [1].\n[1] Lov\u00e1sz, L\u00e1szl\u00f3. \"Random walks on graphs.\" Combinatorics, Paul erdos is eighty 2.1-46 (1993): 4\n5.   How many edges can be deleted in the adopted datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9366/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9366/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9366/Reviewer_hwTX"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9366/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698556567477,
            "cdate": 1698556567477,
            "tmdate": 1699637178641,
            "mdate": 1699637178641,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DHQKfXpJMy",
                "forum": "IefMMX12yk",
                "replyto": "aj2q9ksHth",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9366/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9366/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer hwTX (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your feedback and the raised questions. We are happy that you found our proposed approach GASSIP is novel and our effort to improve the efficiency of GNAS is significant. We are glad to address the concerns you mentioned:\n\n**Q1: The research problem of the co-optimization of neural architecture and data, even in the domain of graph NAS and graph data is not novel.**\n\nA1: Thank you for your opinion. **On one hand**, co-optimization of the neural architecture and graph structure is essential to empowering the GNAS ability, given the discrete space of graph structure makes the traditional optimization that solely considers NAS achieve unsatisfactory performance when applied to GNNs [1]. Therefore, the problem of how to co-optimize GNAS with the underlying graph structure is still not fully solved. **On the other hand**, as we mentioned in the introduction, the main purpose of this paper is to search for lightweight GNN (i.e., lightweight GNN design), which is the first work to search for a lightweight GNN design while considering sparsifying the graph structure simultaneously. In order to select useful information in the graph structure to help search, we propose a novel graph learning algorithm (by co-optimizing the graph structure in Eq. (3)) as you pointed out.\n\n**Q2: The motivation to work on graph NAS.**\n\nA2: Thank you for the comment, though we respectfully disagree with this opinion. It is non-trivial to combine the strength of AutoML and graph machine learning given the following challenges [1-2]: 1) Unlike audio, image, or text, which has a grid structure, graph data lies in a non-Euclidean space. Thus, graph machine learning usually has unique architectures and designs (e.g., neighborhood aggregation functions, pooling functions, and edge dropout strategies ) and could also be constructed very deep [3-4]. For example, typical NAS methods focus on the search space for convolution and recurrent operations, which is distinct from the building blocks of GNNs. 2) Graph tasks per se are complex and diverse, ranging from node-level to graph-level problems, and with different settings, objectives, and constraints. How to impose proper inductive bias and integrate domain knowledge into a graph AutoML method is indispensable. 3) As the results are shown in Table 3 of AutoGL [5], naively applying HPO tuning methods could not surpass the performance of the specifically designed GNAS methods. Therefore, if one can simply use a hyperparameter tuning toolkit on GNNs without considering the GNN architecture to obtain SOTA performance, that could be remarkable progress in this research community.\n\n**Q3: The unstructured pruning of model weights makes no sense in the practical efficiency improvement.**\n\nA3: Thank you for pointing out this concern. Pruning with a mask is a common approach in literature[6]. In practice, the pruning mask becomes quite sparse after several iterations, therefore the pruning is mostly sparse matrix multiplication, which is more efficient compared with dense matrix multiplication. We have included this observation in the revision.\n\n**Q4: It is unscalable to apply a learnable mask with the shape of N\\timesN in graph data.**\n\nA4: Sorry for this confusion. In practice, we use sparse matrix-based implementation, which means that our learnable mask is $|E|$ and is applied on the edge_index which has shape $|E| \\times 2$, where $|E|$ is the number of edges in the graph. Given the graph is naturally sparse in practice, therefore it is not the bottleneck for extending to graphs of billions of nodes. We have improved this detail in our revision.\n\n**Q5: The possibility of applying this work in the benchmark datasets of ogbn-products and paper100m.**\n\nA5: Thank you for this concern. For the issue of large-scale datasets, please kindly refer to the reply to the common question. In short, it is possible to apply this work in large-scale graphs by combining our current implementation with a sampling technique. However, naively applying sampling techniques to the current supernet training in GNAS will result in consistency collapse issues [7]. Since our main focus is to search for lightweight GNNs with limited computational resources, we would like to leave this design of scalable and optimized sampling strategy as a future work to extend GASSIP to graph with a much larger scale."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568292785,
                "cdate": 1700568292785,
                "tmdate": 1700568340416,
                "mdate": 1700568340416,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rweW2DIszD",
                "forum": "IefMMX12yk",
                "replyto": "aj2q9ksHth",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9366/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9366/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer hwTX (2/2)"
                    },
                    "comment": {
                        "value": "**Q6: Graph sparsification is a very old topic.**\n\nA6: Thank you for this opinion and the example. We agree that the problem of graph sparsification is a long-standing research problem. However, it becomes unclear how the performance would be when a new method is applied to the sparsified graph structure. As shown in Figure 4 in the Appendix of our manuscript, the performance varies quite differently when the sparsity of the graph differs. Therefore, it is non-trivial to adopt a GNN architecture to a sparsified graph while maintaining its performance, let alone optimizing the target GNN to be lightweight simultaneously. \n\nAnother illustration of your example that removes edges based on degrees could fail under a new setting is the adversarial attack on GNNs [8], where the degree-based method is a usually adopted baseline but has fair performance. This indicates that the problem could evolve therefore the research on graph sparsification still needs to be explored to tackle the new challenges.\n\n**Q7: How many edges can be deleted in the adopted datasets?**\n\nA7: Thank you for this question. The number of edges that are deleted in the adopted graphs is controlled by a hyperparameter $p%$, and it would be as low as 5% in the experiments.\n\n[1] Automated Machine Learning on Graphs: A Survey, IJCAI 2021\n\n[2] Graph Neural Architecture Search: A Survey, TST 2022\n\n[3] DeepGCNs: Making GCNs Go as Deep as CNNs, TPAMI 2021\n\n[4] Training Graph Neural Networks with 1000 Layers, ICML 2021\n\n[5] AutoGL: A Library for Automated Graph Learning, ICLR 2021 GTRL workshop\n\n[6] A Unified Lottery Ticket Hypothesis for Graph Neural Networks, ICML 2021\n\n[7] Large-scale graph neural architecture search, ICML 2022\n\n[8] Adversarial Attacks on Node Embeddings via Graph Poisoning, ICML 2019"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568323558,
                "cdate": 1700568323558,
                "tmdate": 1700568323558,
                "mdate": 1700568323558,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MqJPeccgXL",
            "forum": "IefMMX12yk",
            "replyto": "IefMMX12yk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9366/Reviewer_CX38"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9366/Reviewer_CX38"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new approach to lightweight graph neural network architecture search called GASSIP. What sets it apart is that during the search process, it jointly considers graph data sparsification and operation pruning, allowing the discovered sub-architectures to achieve better performance with fewer parameters. It also exhibits a degree of robustness. Ultimately, it yields both sparse graphs and lightweight sub-architectures, enhancing search efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. First jointly considers operation pruning and graph data sparsification in graph neural architecture search, which can efficiently search lightweight GNNs. \n2. Uses curriculum learning strategy in graph data sparsification, which can more accurately identify redundant edges and obtain a sparse graph structure beneficial for downstream tasks"
                },
                "weaknesses": {
                    "value": "1. Considers graph sparsification and operation pruning at the same time, but does not provide a theoretical analysis of whether this iterative optimization converges.\n2. Insufficient experiments on large-scale graph data: The large-scale graph data set used in the experiments of this article only contains OGBN-ARXIV. Therefore, more experiments on large-scale graph data are needed to verify the performance of the GASSIP method."
                },
                "questions": {
                    "value": "1. Is the search result sensitive to the choice of random seed? \n2. Simultaneous optimization of operation pruning and graph data sparsification may interfere with each other and lead to performance degradation. Could you provide some theoretical analysis on the convergence of the joint optimization process?\n3. In equation (3), operation pruning and graph sparsification are combined with a logical OR operation. What is the rationale behind this design choice?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9366/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698648018033,
            "cdate": 1698648018033,
            "tmdate": 1699637178533,
            "mdate": 1699637178533,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zhzZR4vcLR",
                "forum": "IefMMX12yk",
                "replyto": "MqJPeccgXL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9366/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9366/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer CX38"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for your detailed comments and insightful questions. We are happy to address the concerns you mentioned:\n\n**Q1: Theoretical convergence analysis.**\n\nA1: Thank you for pointing out this weakness. We admit that it is difficult to conduct a theoretical analysis on the convergence of our iterative optimization algorithm without proper assumptions as the convergence of single graph sparsification proved in [1]. Therefore, we include this as one limitation of our work as well as a future direction. On the other hand, we found that GASSIP converges well during practice.  Table 2 in Section 5.3 provides **searching time comparison** in comparison with other baselines. We could see that our proposed method is far faster than DARTS+UGS (a first-search-then-prune method that disentangles the co-optimization method by first searching architectures and then conducting network pruning and graph data sparsification). This indicates that both simultaneous optimization of operation pruning and graph data sparsification and first-search-then-prune show satisfactory convergence behavior in practice.\n\n**Q2: Is the search result sensitive to the choice of random seed?**\n\nA2: Thank you for this question. Here are the experimental results for GASSIP on node classification that is averaged for 100 runs (mean\u00b1std) using different random seeds:\n\n| Method   | Cora | CiteSeer | PubMed | Physics | Ogbn-Arxiv |\n\n| GASSIP | 83.20\u00b10.42 | 71.41\u00b10.57 | 79.50\u00b10.30 | 98.46\u00b10.06 | 71.30\u00b10.23 |\n\nWe can observe that the stds are relatively small, therefore the searched result is not sensitive to the choice of random seed. We have added this observation to the experimental results.\n\n**Q3: In equation (3), operation pruning and graph sparsification are combined with a logical OR operation. What is the rationale behind this design choice?**\n\nA3: We are sorry for this confusion. The symbol denotes the element-wise product operation. We have updated our manuscript to reflect this accordingly.\n\n**Q4: Experiments on large-scale graphs.**\n\nA4: Thank you for this concern. For the issue of large-scale datasets, please kindly refer to the reply to the common question. Since it requires calculating the output of every operation within the supernet on the full graph to estimate reliable architecture parameters, it is difficult for most of the GNAS methods to scale to Ogbn-product and Ogbn-Paper100M without a specifically designed sampling strategy.\n\n[1] Graph Sparsification by Universal Greedy Algorithms, Journal of Computational Mathematics 2023"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568153681,
                "cdate": 1700568153681,
                "tmdate": 1700568153681,
                "mdate": 1700568153681,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Z2Yp4ywhAo",
            "forum": "IefMMX12yk",
            "replyto": "IefMMX12yk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9366/Reviewer_WS6Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9366/Reviewer_WS6Y"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed an lightweight GNAS algorithm. It iteratively optimizes graph data and architecture through curriculum graph sparsification and operation-pruned architecture search."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper proposes a GNAS method with graph sparification, which is an interesting exploration.\n2. The paper applies the method to extensive experiments and shows good results on multiple datasets."
                },
                "weaknesses": {
                    "value": "1. GNAS methods nowadays have been expanded to large-scale datasets, while the paper only showed the results on Physics and Ogbn-Arxiv datasets. Could you please give a more overall perfomance comparison with other GNAS methods like GUASS  on large-scale OGB datasets? \n\n   [1] Large-scale graph neural architecture search, ICML 2022.\n\n2. More comparsion on the unified benchmark will be appreciated, such as NAS-Bench-Graph.\n\n   [2] Benchmarking Graph Neural Architecture Search, NIPS 2022.\n\n3. Cited works in Section 2 are mostly before 2022 and the methods compared in Table 1 are all before 2021 which make the work out-dated. I do know there were multiple GNAS and graph sparsification methods proposed in 2022/2023. Maybe more cutting-edge research work as well as comparisions should be added. \n\n4. The first contribution is proprosing a operation-pruned search method with learnable weight mask. However, the work of HM-NAS introduced this hierarchical masking on redundant operations, edges, and even the weights of supernet. It seems like transfering the idea on graphs. Maybe you should add this work and discuss the noble part of the first contribution compared with the learnable weight mask idea on edges in HM-NAS.  \n\n   [3] HM-NAS: Efficient Neural Architecture Search via Hierarchical Masking, ICCV 2019.\n\n6. I feel a little confused about the workflow in Figure 1. I understand that the iterative training process of structure mask is between the gradient update of operations and architectures. However, the training process probably need to point back to the architecture searching part to illustrate the interactive training not directly getting the final sparsed graph and connecting to the pruned architecture. Maybe the training part and the procedures after binarizing masks can be seperated."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9366/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698670084121,
            "cdate": 1698670084121,
            "tmdate": 1699637178378,
            "mdate": 1699637178378,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OWj2d8rBn3",
                "forum": "IefMMX12yk",
                "replyto": "Z2Yp4ywhAo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9366/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9366/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer WS6Y"
                    },
                    "comment": {
                        "value": "Thank you for your time and detailed feedback. We are happy to adapt our work to answer your questions and include all your suggested changes.\n\n**Q1. Comparison with GUASS**\n\nA1: Thank you for this question. For the issue of large-scale datasets, please kindly refer to the reply to the common question. Since it requires calculating the output of every operation within the supernet on the full graph to estimate reliable architecture parameters, it is difficult for most of the GNAS methods to scale to Ogbn-product and Ogbn-Paper100M without a specifically designed sampling strategy. We, therefore, report the results compared with GUASS on the scalable benchmarks as below:\n\n| Method   | Cora | CiteSeer | PubMed | Physics | Ogbn-Arxiv |\n\n| GUASS  | 82.05\u00b10.21 | 70.80\u00b10.41 | 79.48\u00b10.16 | 96.76\u00b10.08 | 71.85\u00b10.41 |\n\n| GASSIP | 83.20\u00b10.42 | 71.41\u00b10.57 | 79.50\u00b10.30 | 98.46\u00b10.06 | 71.30\u00b10.23 |\n\nWe can find that GASSIP achieves better performance than GUASS on smaller graphs, but GUASS could handle graphs with more nodes and edges (Ogbn-Arxiv) as it is specially developed for large-scale datasets. However, our trained model is more lightweight and therefore can be applied in practical scenarios where computational resources are limited, which is not applicable to GUASS.\n\n**Q2: Comparision with NAS-Bench-Graph.**\n\nA2: Thank you for bringing up this suggestion. However, NAS-Bench-Graph only designs general search space without considering the pruning on either the graph structure or the GNN architecture. As a result, directly applying GASSIP on the NAS-Bench-Graph is inappropriate and induces less meaningful comparison. We appreciate your opinion and would leave the efforts to develop a unified benchmark for lightweight GNAS as future work.\n\n**Q3: Maybe more cutting-edge research work as well as comparisons should be added.**\n\nA3: Thank you for this suggestion. We have done our best to conduct supplementary research on the GNAS and graph sparsification methods proposed in 2022/2023. We have included them [1-4] in the Related Work section with the corresponding discussion. We have also included GUASS as an additional baseline as you suggested. Please provide any missing related work that you imply and we will also include them in the discussion. Nevertheless, to the best of our knowledge, our proposed GASSIP is the first work to search for a lightweight GNN design while considering sparsifying the graph structure simultaneously.\n\n**Q4: Difference with HM-NAS.**\n\nA4: Thank you for the information about HM-NAS and we are sorry for not being aware of this wonderful work before. We have added the corresponding discussion with HM-NAS in the related works. Specifically, HM-NAS aims to improve the architecture search performance by loosening the hand-designed heuristics constraint with three hierarchical masks on operations, edges, and network weights. In contrast, our focus is different from HM-NAS as we aim to search for a lightweight GNN considering co-optimizing the graph structure. To achieve this lightweight goal, a mask for network weight is naturally introduced, which is commonly used for network pruning.\n\n**Q5: The workflow in Figure 1.**\n\nA5: Thanks for your suggestion and we are sorry for the confusion. The \"sparsified graph\" mentioned in the original figure refers to the masked graph rather than the final binarized sparsed\u00a0graph. The arrow is not directly connecting to the pruned architecture but connecting two different iterative parts. We adjust the arrows to make this clear. After revision, we do not include the final mask binarizing step which is illustrated in Line 7 in Algorithm 2. Figure 1 only displays the iterative training process which is shown in Lines 1-6 in Algorithm 2. We have also updated the caption of Figure 1 to make this illustration clear.\n\n[1] DFG-NAS: Deep and Flexible Graph Neural Architecture Search, ICML 2022\n\n[2] Do Not Train It: A Linear Neural Architecture Search of Graph Neural Networks, ICML 2023\n\n[3] Ricci Curvature-Based Graph Sparsification for Continual Graph Representation Learning, TNNLS 2023\n\n[4] DSpar: An Embarrassingly Simple Strategy for Efficient GNN Training and Inference via Degree-Based Sparsification, TMLR 2023"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568084496,
                "cdate": 1700568084496,
                "tmdate": 1700568084496,
                "mdate": 1700568084496,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MonVI8jSKj",
                "forum": "IefMMX12yk",
                "replyto": "Z2Yp4ywhAo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9366/Reviewer_WS6Y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9366/Reviewer_WS6Y"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThanks for your feedback. The response to the scalability concern is not convincing for me. The authors argued that the extension to heavy-weight scenarios with very large-scale graphs is not the main focus of our work. In my view, large-scale graphs have not direct relationship with the meaning of \u2018heavy-weight\u2019. \u2018Light-weight\u2019 is reflected in the complexity of GNN. Instead, search a lightweight GNN for large-scale graphs could also be considered in the paper. Intuitively, both graph sparsification and operation pruning help to improve the efficiency of searching a lightweight GNN for large-scale graphs. Maybe, joint optimization of graph sparsification and GNN NAS is not a scalable solution.\n\nConsidering this concern, I keep my rating unchanged."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652584702,
                "cdate": 1700652584702,
                "tmdate": 1700652670354,
                "mdate": 1700652670354,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]