[
    {
        "title": "ControlVideo: Training-free Controllable Text-to-video Generation"
    },
    {
        "review": {
            "id": "WLE6qF9WVd",
            "forum": "5a79AqFr0c",
            "replyto": "5a79AqFr0c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4289/Reviewer_EvXG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4289/Reviewer_EvXG"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a trainig-free framework to produce videos based on provided text prompts and motion sequences. An interleaved-frames smoother and a fully cross-frame interaction mechanism with a hierarchical sampler are proposed to enhance the quality of synthesized videos. The authors demonstrate that they achieve sota performance by entensive experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy-to-follow. \n2. The proposed method is highly efficient and does not need training at all. Nevertheless, the quality of synthesized videos are not bad."
                },
                "weaknesses": {
                    "value": "1. The proposed components (inter-frame interpolation, and cross-frame attention) are more-or-less explored in recent works. As such, I'm uncertain if this research will provide substantial insights to the community.\n\n2. The proposed metrics (frame consistency, and prompt consistency) leverage CLIP model. Given that the CLIP model primarily operates in a deeply semantic and abstract domain, it often misses finer image details. Consequently, I'm inclined to think that the suggested metric might not adequately assess temporal consistency (for example, critizing jittering and discontinuity). Thus, the assertion that the proposed methods attain improved temporal consistency seems to lack robust quantitative backing.\n\n3. This training-free framework cannot capture fine-grained motion pattern in video. Therefore, I believe it may not be the optimal approach for producing high-quality video content. Instead, I think finetuning the model on large-corpus video data might help improve the quality."
                },
                "questions": {
                    "value": "Just as stated in the weakenss section, I am skeptical about the potential of training-free framework in video generation area. I'd be keen to hear the authors discuss potential future research directions in this direction."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4289/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4289/Reviewer_EvXG",
                        "ICLR.cc/2024/Conference/Submission4289/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4289/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698558887480,
            "cdate": 1698558887480,
            "tmdate": 1700631149504,
            "mdate": 1700631149504,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2EOt0cH90p",
                "forum": "5a79AqFr0c",
                "replyto": "WLE6qF9WVd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4289/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4289/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EvXG"
                    },
                    "comment": {
                        "value": "Dear reviewer, thanks for your insightful comments. We address the comments below and will include them in revision.\n\n**[W1]: The proposed components are more-or-less explored in recent works.**\n\nOur key insights lie in enhancing temporal consistency and continuity of videos, including three folds:\n\n**(i)** First of all, we introduce interleaved-frame smoother to alternately stabilize the video latents during sampling, and show that adding random noise produces more realistic videos ([[DDPM results](https://controlvideov1.github.io/#Non-deterministric%20DDPM%20sampler)], refer to [Q3 of Reviewer XMqd] for details)\n\n**(ii)** The hierarchical sampler achieves efficient long-video generation in commodity GPUs (See [[long-video results](https://controlvideov1.github.io/#Long%20video%20generation)]).\n\n**(iii)** We also empirically demonstrate the superior consistency and quality of fully cross-frame interaction (See [[cross-frame attention ablations](https://controlvideov1.github.io/#Effect%20of%20fully%20cross-frame%20interaction%20and%20interleaved-frame%20smoother)]).\n\n**[W2]: More robust metric for temporal consistency.**\n\nFollowing [1], we employ the optical-flow based warping error to robustly evaluate temporal continuity, where the optical flows comes from source videos.\nThe following table reports the warping error comparisons of different methods.\nFrom the table, ControlVideo achieves a significantly lower warping error than other baselines, demonstrating its superiority in enhancing continuity.\nIn addition, one can see that Canny-based models achieve lower warping error than depth-based ones, as Canny edges have more similar structure with optical flow.\n\n| Model                   | Condition       | Warping error ($\\times 10^{-2}$) |\n| ----------------------- | --------------- | ---------------------- |\n| Tune-A-video            | DDIM inversion  | 18.16                  |\n| Text2Video-Zero         | Canny edges     | 8.76                   |\n| **ControlVideo (Ours)** | **Canny edges** | **2.75**               |\n| Text2Video-Zero         | Depth maps      | 10.36                  |\n| **ControlVideo (Ours)** | **Depth maps**  | **5.81**               |\n\n\n**[W3 & Q1]: The potential of training-free framework in video generation.**\n\nAlbeit training on large-corpus videos can help improve the video consistency, there are three factors limiting further research:\n**(i)** Extremely high requirements for massive computational resources, which usually are unavailable for researchers from universities. \n**(ii)** Absence of sufficient video-caption datasets. Despite requiring modeling more complex video distributions, existing public video datasets only have limited number of videos compared to image datasets, e.g., WebVid-10M vs. LAION-5B.\n**(iii)** Lower frame quality than image generative models. Existing video datasets usually contain low-quality factors (e.g., blur and watermarks), and training on them may lead to visibly lower frame quality than the image counterparts. \n\nDue to limited computation resources, we investigate the effectiveness of proposed modules under the \"training-free\" setting, and demonstrate superior performance than some finetuning based methods, i.e., Tune-A-Video and Follow-Your-Pose.\nSince \"training-free\" ControlVideo has shown promisingly temporal consistency and continuity, it is natural to take it as a well-initialized model for the following finetuning.\nCompared to training from the scratch, it has two potential advantages:\n**(i)** Reduce the requirements for massive computation resources and data. The well-initialized model only needs to learn fine-grained motion pattern from videos.\n**(ii)** Alleviate low-quality effects from training videos. Requiring less finetuning in video dataset, ControlVideo may better preserve frame quality from the image counterparts."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4289/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700454488204,
                "cdate": 1700454488204,
                "tmdate": 1700454698181,
                "mdate": 1700454698181,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ofADvnNigx",
                "forum": "5a79AqFr0c",
                "replyto": "ISTADUp02O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4289/Reviewer_EvXG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4289/Reviewer_EvXG"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks the authors for clarification and providing results over additional metric. My concerns are addressed. Therefore, I would like to raise my score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4289/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631131472,
                "cdate": 1700631131472,
                "tmdate": 1700631131472,
                "mdate": 1700631131472,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "T1ClNll9KI",
            "forum": "5a79AqFr0c",
            "replyto": "5a79AqFr0c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4289/Reviewer_aLwG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4289/Reviewer_aLwG"
            ],
            "content": {
                "summary": {
                    "value": "This work focuses on training-free controllable text-to-video generation tasks. It introduces an interleaved-frame smoother method to generate smoother frames. Additionally, it modifies cross-frame interaction to better utilize Stablediffusion's weights, enhancing frame continuity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The writing is clear and easy to follow.\n- It is a training-free method, not relying on large-scale training, and has low computational resource requirements.\n- The ablation experiments are well-designed and easy to understand."
                },
                "weaknesses": {
                    "value": "- Overall, the innovation is average; applying ControlNet to video editing or generation is straightforward and easily thought of.\n- The experiments are not comprehensive; there are too few baseline comparisons, and the experimental validation is limited to just over 20 examples, making the results less convincing.\n- Limited by the absence of structure condition, this method can mainly edit videos with similar motion. Its effectiveness diminishes for videos with different motions or poses."
                },
                "questions": {
                    "value": "As shown above, despite the method's average innovation and some shortcomings, I believe the exploration in this direction is worthwhile. \n- I hope the authors can complete more experiments and cases, preferably providing an analysis of failure cases. \n- Relying solely on the demo examples provided in the paper makes it challenging to be fully convinced.\n- If the authors can address my concerns, I will consider giving a higher score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no need"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4289/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698656424122,
            "cdate": 1698656424122,
            "tmdate": 1699636396694,
            "mdate": 1699636396694,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TLcK9QIpgy",
                "forum": "5a79AqFr0c",
                "replyto": "T1ClNll9KI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4289/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4289/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aLwG"
                    },
                    "comment": {
                        "value": "Dear reviewer, thanks for your insightful comments. We hope our below replies could address the comments, and will include them in revision.\n\n**[W1]: Average innovation.**\n\nAlbeit applying ControlNet to video generation is straightforward, our key insights lie in improving the temporal consistency and continuity, including three folds:\n\n**(i)** First of all, we introduce interleaved-frame smoother to alternately stabilize the video latents during sampling, and show that adding random noise produces more realistic videos ([[DDPM results](https://controlvideov1.github.io/#Non-deterministric%20DDPM%20sampler)], refer to [Q3 of Reviewer XMqd] for details)\n\n**(ii)** The hierarchical sampler achieves efficient long-video generation in commodity GPUs (See [[long-video results](https://controlvideov1.github.io/#Long%20video%20generation)]).\n\n**(iii)** We also empirically demonstrate the superior consistency and quality of fully cross-frame interaction (See [[cross-frame attention ablations](https://controlvideov1.github.io/#Effect%20of%20fully%20cross-frame%20interaction%20and%20interleaved-frame%20smoother)]).\n\n**[W2 & Q2]: Comparisons with more baselines.**\n\nApart from original 30 examples, we have added 38 new examples to [[qualitative results](https://controlvideov1.github.io/#Qualitative%20comparisons)], including another two baselines of video editing (FateZero[1] and Vid2Vid-Zero[2]).\nFrom [[qualitative results](https://controlvideov1.github.io/#Qualitative%20comparisons)], ControlVideo generates videos with better consistency and continuity than other approaches.\nIn contrast, FateZero may fail to align with text prompts, e.g., Iron man in [[human-pose results](https://controlvideov1.github.io/#Human%20pose)], while Vid2Vid-zero produces videos with more visible flickers.\n\n**[W3 & Q1]: Failure cases analysis.**\n\nWe provide two additional failure cases in [[failure-case results](https://controlvideov1.github.io/#Trade-off%20between%20text%20prompt%20and%20motion)].\nAs one can see, when input text prompts (e.g., rabbit) seriously conflict with input motion (e.g., cow), the synthesized videos usually tend to align with input motion, ignoring the implicit structure in text prompts.\nTo increase the ratio of text prompts over structure, we decrease the scale of ControlNet $\\lambda$ to 0.3 ($\\lambda=1$ by default).\nIn [[failure-case results](https://controlvideov1.github.io/#Trade-off%20between%20text%20prompt%20and%20motion)], it can be seen that $\\lambda=0.3$ achieves a better trade-off between two input conditions than $\\lambda=1$.\nIn the future, we will explore how to adaptively modify input motions according to text prompts, so that users can create more vivid videos.\n\n[1] Chenyang Qi, et al. FateZero: Fusing Attentions for Zero-shot Text-based Video Editing. In ICCV 2023.\n\n[2] Wen Wang, et al. Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models. Arxiv 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4289/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700454333747,
                "cdate": 1700454333747,
                "tmdate": 1700454551163,
                "mdate": 1700454551163,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bM87LlRDtA",
            "forum": "5a79AqFr0c",
            "replyto": "5a79AqFr0c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4289/Reviewer_HJ3a"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4289/Reviewer_HJ3a"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes ControlVideo, a training-free framework that can produce high-quality videos based on the provided text prompts and motion sequences (e.g., different modalities). ControlVideo adapts a pre-trained text-to-image model (i.e., ControlNet) for controllable text-to-video generation. The paper introduces an interleaved-frame smoother that alternately smooths out the latents of successive three-frame clips by updating the middle frame with the interpolation among the other two frames in the latent space, aiming to stabilize the temporal continuity of the generated videos. Besides, a fully cross-frame interaction mechanism is exploited to further enhance the frame\nconsistency, and a hierarchical sampler is employed to produce long videos more efficiently. Experimental results demonstrate that the proposed ControlVideo outperforms the state-of-the-art baselines both quantitatively and qualitatively."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is clearly written, well organized, and easy to follow. The symbols, terms, and concepts are adequately defined and explained. The language usage is good.\n\n- The proposed method is simple and easy to understand. Sufficient details are provided for the readers.\n\n- The experiments are generally well-executed. The empirical results show the effectiveness of the proposed method, showing certain advantages over state-of-the-art baselines."
                },
                "weaknesses": {
                    "value": "- The qualitative results showcase certain advantages of the proposed method over state-of-the-art baselines in controllable text-to-video generation. However, by checking the provided video results, the temporal consistency can still be improved. Also, in some cases, the background looks unchanged. Some visual details can still be improved. Providing more discussions on these could strengthen this paper further.\n\n- The fully cross-frame interaction mechanism considers all frames as the reference, which thus increases the computational burden. What is the intuition to consider all the frames as a large image? Why not select some key frames to reduce redundant information? It is interesting to provide more discussions and analysis on this.\n\n- The paper mentioned that the proposed interleaved-frame smoother is performed on predicted RGB frames at timesteps {30, 31} by default. It can be more interesting if more studies and analyses on different steps to apply such a mechanism are provided.\n\n- It seems the interleaved-frame smoother still brings more computational cost and affects the model efficiency due to the additional conversion and interpolation steps."
                },
                "questions": {
                    "value": "- Why does the hierarchal sampler improve model efficiency? It seems all the frames still need to be generated, although it is a top-down generation from key frames.\n\n- It is suggested to remove some content about the background and preliminary since such information is well-known.\n\n- The reviewer is interested if the proposed ControlVideo can be extended to generate more challenging new information, such as a novel view/unseen part of an object.\n\n-  Will the authors release all the code, models, and data to ensure the reproducibility of this work?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4289/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813332784,
            "cdate": 1698813332784,
            "tmdate": 1699636396614,
            "mdate": 1699636396614,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9JPpgKwqsy",
                "forum": "5a79AqFr0c",
                "replyto": "bM87LlRDtA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4289/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4289/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HJ3a (Part 1/2)"
                    },
                    "comment": {
                        "value": "Dear reviewer, thanks for your insightful comments.We provide our replies to the comments below, and hope they could address your concerns.\n\n**[W1]: Temporal consistency to be improved and unchanged background.**\n\n**(i) Temporal inconsistency.**\nFor some videos containing flickers, there are two potential reasons:\n(a) The input motion contains visible flickers and may result in the flickers in synthesized videos. \n(b) Sometimes, the scene correspondence in motions cannot be well maintained through cross-frame attention only, leading to incoherence in appearance.\nFor (a), we can leverage video-based annotator[1] or smoothing filters[2] to obtain more stable motion sequence.\nFor (b), one may first estimate the correspondence of motions with[3,4], and then recalibrate the attention weights based on the correspondence. \n\n\n**(ii) Unchanged background.**\nIn such cases, the background part of input motion is unchanged, and its synthesized background will also be static via DDIM sampling.\nAs shown in [[DDPM results](https://controlvideov1.github.io/#Non-deterministric%20DDPM%20sampler)], when ControlVideo employs non-deterministic DDPM sampling, it produces more vivid videos with lively background.\nKindly refer to [Q3 of Reviewer XMqd] for more details.\n\n**[W2]: Consider all frames as a large image and select some key frames**\n\n**(i)** \nFor original self attention in Stable Diffusion (SD), both its queries and keys come from the same image.\nSimilarly, fully cross-frame attention computes queries and keys with the same frames, and may better inherit high-quality and consistent generation than other cross-frame mechanisms.\n**(ii)** We conduct ablation experiments on different number $k$ of key frames, and the results are given in the following table and [[keyframes results](https://controlvideov1.github.io/#Effect%20of%20fully%20cross-frame%20interaction%20and%20interleaved-frame%20smoother)].\nWith the increasing of $k$, ControlVideo obtains better performance in temporal consistency, i.e., higher frame consistency and lower warping error.\nAlso, larger $k$ visibly reduces frame incoherence in appearance, e.g., the orientation of elephant.\n\n| Number of keyframes                     | k=1 (first-only) | k=2 (sparse-casual) | k=4   | k=8   | k=15 (fully) | k=15  w/ smoother |\n| --------------------------------------- | ---------------- | ------------------- | ----- | ----- | ------------ | ----------------- |\n| **Frame consistency $\\uparrow$**        | 94.92            | 95.06               | 95.18 | 95.30 | `95.36`      | **96.83**         |\n| **Warping error ($\\times 10^{-2}$) $\\downarrow$** | 8.91             | 7.05                | 6.62  | 6.27  | `5.93`       | **2.75**          |\n| **Time cost (min)$\\downarrow$**         | 1.2              | 1.5                 | 1.9   | 2.5   | 3.0          | 3.5               |\n\n**[W3]: Apply smoother at different timesteps.**\n\nThe following table and [[smoother-timesteps results](https://controlvideov1.github.io/#Which%20timesteps%20does%20interleaved-frame%20smoother%20perform)] show ablation experiments in different timesteps (refer to Appendix C for more details).\nAs one can observe, applying the smoother at timesteps {30,31} effectively deflickers the video while ensuring its frame quality. \nIn contrast, using the smoother at timesteps {48,49} or timesteps {0,1} lead to slight flickering or distortion, degrading the effectiveness of smoother.\n\n| Timestep choices                        | {}    | {48,49} | {30,31}   | {20,21} | {10,11} | {0,1} |\n| --------------------------------------- | ----- | ------- | --------- | ------- | ------- | ----- |\n| **Frame consistency $\\uparrow$**        | 95.36 | 95.97   | **96.83** | 96.82   | 96.81   | 96.78 |\n| **Warping error ($\\times 10^{-2}$) $\\downarrow$** | 5.93  | 4.87    | **2.75**  | 2.92    | 3.24    | 3.89  |\n\n**[W4]: More computational cost of smoother.**\n\nFrom above table and [[cross-frame attention and smoother](https://controlvideov1.github.io/#Effect%20of%20fully%20cross-frame%20interaction%20and%20interleaved-frame%20smoother)], the proposed smoother greatly improves temporal continuity of synthesized videos, and the extra 0.5 min can be seen as acceptable.\n\n**[Q1]: Why does the hierarchical sampler improve model efficiency?**\nThe hierarchical sampler is designed to improve efficiency of fully cross-frame attention, especially in producing long videos.\nGiven $N$ frames, $K$ key frames and $T$ tokens for each frame, the complexity for fully attention is $O(T^2 \\cdot N^2)$, while that of hierarchical sampler is $O(T^2 \\cdot (K^2 + \\frac{N^2}{K}))=O(T^2 \\cdot (N + N^{\\frac{3}{2}}))$ ($K=\\sqrt{N}$ by default).\nWhen producing a video of 100 frames, the fully attention requires about 50 mins while the hierarchical sampler only takes about 10 mins."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4289/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700454207367,
                "cdate": 1700454207367,
                "tmdate": 1700454207367,
                "mdate": 1700454207367,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aOtemRuSfO",
            "forum": "5a79AqFr0c",
            "replyto": "5a79AqFr0c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4289/Reviewer_XMqd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4289/Reviewer_XMqd"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces \"ControlVideo,\" a training-free framework that significantly improves text-driven video generation. It addresses issues like appearance inconsistency and flickers in long videos through innovative modules for frame interaction and smoothing. ControlVideo outperforms existing methods, efficiently generating high-quality videos within minutes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "\u2022\tThe proposed method is straightforward, easily implementable, and reproducible, making it accessible for further research and application.\n\n\u2022\tThe paper introduces novel techniques for long video generation, and the \"interleaved-frame smoother\" effectively improves frame consistency.\n\n\u2022\tThe results demonstrate improvements over existing methods, substantiating the paper's claims."
                },
                "weaknesses": {
                    "value": "\u2022\tWhile the full-attention mechanism and \"interleaved-frame smoother\" enhance frame consistency, they also significantly increase the computational time.\n\n\u2022\tThe background appears to flicker in relation to the foreground in some examples. For instance, in the \"James Bond moonwalk on the beach, animation style\" video on the provided website, the moon inconsistently appears and disappears.\n\n\u2022\tThe paper lacks quantitative comparisons with Text2Video-Zero in the context of pose conditions, which could be a significant oversight given the importance of pose in video generation."
                },
                "questions": {
                    "value": "\u2022\tCould you provide additional results for long video generation to further validate the method's efficacy?\n\n\u2022\tIs there a potential solution to the flickering background issue mentioned in the second weakness?\n\n\u2022\tWould it be possible to employ a non-deterministic DDPM-style sampler as an alternative to DDIM?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4289/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698946777899,
            "cdate": 1698946777899,
            "tmdate": 1699636396530,
            "mdate": 1699636396530,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RME2d4BRp0",
                "forum": "5a79AqFr0c",
                "replyto": "aOtemRuSfO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4289/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4289/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XMqd"
                    },
                    "comment": {
                        "value": "Dear reviewer, thanks for your insightful comments. We provide our replies to the comments below, and hope they could address your concerns. \n\n\n**[W1]: Increasing computational time.**\n\nThe following table and [[cross-frame attention and smoother](https://controlvideov1.github.io/#Effect%20of%20fully%20cross-frame%20interaction%20and%20interleaved-frame%20smoother)] show ablation experiments of the fully cross-frame attention and interleaved-frame smoother.\nAs one can see, the proposed two modules significantly improve both video consistency and continuity, with acceptable increase in computational time.\n\n| Cross-frame attention   | Frame consistency $\\uparrow$ | Warping error ($\\times 10^{-2}$) $\\downarrow$ | Time cost (min)$\\downarrow$ |\n| ----------------------- | ---------------------------- | ----------------------------------- | --------------------------- |\n| First-only              | 94.92                        | 8.91                                | 1.2                         |\n| Sparse-causal           | 95.06                        | 7.05                                | 1.5                         |\n| Fully                   | 95.36                        | 5.93                                | 3.0                         |\n| Fully + Smoother (Ours) | **96.83**                    | **2.75**                            | 3.5                         |\n\n\n**[W2 & Q2]: Flickering background issue.**\n\nFor some cases with flickering background, there are two potential reasons:\n(a) The input motion contains visible flickers and may bring flickering controls to synthesized background. \n(b) Sometimes, the scene correspondence in motions cannot be well maintained through cross-frame attention only, producing videos with incoherent appearance.\nFor (a), we can leverage video-based annotator[3] or smoothing filters [4] to obtain more stable motion sequence.\nFor (b), we first estimate the correspondence of motions with [5,6], and then recalibrate the attention weights based on the correspondence.\n\n**[W3]: Quantitative comparisons with Text2Video-Zero in pose conditions.**\n\nIn terms of pose condition, we compare our ControlVideo with Text2Video-Zero in the test set of Fashion-Text2Video dataset[1], which contains 380 video-prompt pairs in total.\nFrom the following table, ControlVideo is superior to Text2Video-Zero by frame consistency, while with marginal gains in prompt consistency.\n\n| Model               | Condition | Frame consistency | Prompt consistency |\n| ------------------- | --------- | ----------------- | ------------------ |\n| Text2Video-Zero     | Pose      | 91.85             | 28.86              |\n| ControlVideo (Ours) | Pose      | **93.24**         | **28.94**          |\n\n\n**[Q1]: Additional results for long video generation.**\n\nIn addition to the original two long videos, we have added another three ones to [[long-video results](https://controlvideov1.github.io/#Long%20video%20generation)].\nBenefiting from our hierarchical sampler, it only takes $\\sim$ 10 minutes to generate a video with 100 frames in one NVIDIA RTX 2080Ti.\n\n**[Q3]: Non-deterministic DDPM-style sampler.**\n\nYes, our ControlVideo can also employ a non-deterministic DDPM-style sampler during inference.\nFollowing Eq.12 in DDIM[2], one can predict $z_{t-1}$ from $z_t$ via (i.e., line 10 of Alg.1 in paper):\n\n$z\\_{t-1} = \\sqrt{\\alpha\\_{t-1}} \\tilde{z}\\_{t\\rightarrow 0} + \\sqrt{1 - \\alpha\\_{t-1} - \\sigma\\_t^2} \\cdot \\epsilon\\_\\theta(z_t, t, c, \\tau) + \\sigma\\_t \\epsilon_t$.\n\nwhere $\\epsilon\\_t$ is random Gaussian noise and $\\sigma\\_t = \\lambda \\cdot \\sqrt{(1 - \\alpha\\_{t-1})/(1 - \\alpha\\_{t})}\\sqrt{1 - \\alpha\\_{t}/\\alpha\\_{t-1}}$ controls the level of random noise.\n\n[[DDPM results](https://controlvideov1.github.io/#Non-deterministric%20DDPM%20sampler)] presents the generated videos of ControlVideo at different noise levels.\nNotably, as the noise level increases, ControlVideo generates more photo-realistic videos with dynamic details, e.g., ripples in the water.\n\n[1] Yuming Jiang, et al. Text2Performer: Text-Driven Human Video Generation. In ICCV 2023.\n\n[2] Jiaming Song, et al. Denoising Diffusion Implicit Models. In ICLR 2021.\n\n[3] Xuan Luo, et al. Consistent Video Depth Estimation. In SIGGRAPH 2020.\n\n[4] Press W.H, et al. Savitzky-Golay smoothing filters. In Computers in Physics.\n\n[5] David Lowe, et al. Distinctive image features from scale-invariant keypoints. In IJCV 2004.\n\n[6] Luming Tang, et al. Emergent Correspondence from Image Diffusion. In NeurIPS 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4289/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700454015739,
                "cdate": 1700454015739,
                "tmdate": 1700707425398,
                "mdate": 1700707425398,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]