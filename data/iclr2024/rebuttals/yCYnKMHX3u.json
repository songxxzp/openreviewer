[
    {
        "title": "MultiLayerDiffusion: Composing Global Contexts and Local Details in Image Generation"
    },
    {
        "review": {
            "id": "bOedJtjhvb",
            "forum": "yCYnKMHX3u",
            "replyto": "yCYnKMHX3u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4039/Reviewer_7GYr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4039/Reviewer_7GYr"
            ],
            "content": {
                "summary": {
                    "value": "This paper offers separate control over global and local contexts through the utilization of user-specified region masks and prompts for various conditions. This algorithm facilitates precise guidance for image generation and enables localized region editing while preserving the integrity of other regions within the image."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This method empowers precise control over both global and local regions, providing a promising avenue for controlled image generation."
                },
                "weaknesses": {
                    "value": "1. The proposed algorithm seems to be an incremental work on MultiDiffusion and Training-free layout control, while primarily relying on Training-free layout control. The key distinction between the proposed algorithm and the aforementioned methods remains somewhat unclear. Is the primary differentiation the replacement of global condition with local guidance in instances where local guidance is employed?\n\n2. The method section presents challenges in terms of comprehension, largely due to the excessive use of undefined notations. For instance:\n   - In the Algorithm:\n     1) What does the symbol \"$f$\" represent in Line 3?\n     2) How should we interpret \"$\\epsilon_j^b$\" in Line 8?\n   - In Section 3.2:\n     1) Could you provide a more detailed explanation of the introduced diffusion layer? A mathematical definition or implementation code would be beneficial.\n     2) Equation (6) contains numerous subscripts, including \"$j$\", \"$i$\", \"$b$\", and \"$n$\", while the result introduces a new subscript \"$l$\". Guidance on how to select \"$j$\", \"$i$\", \"$b$\", \"$n$\" for a specific \"$l$\" would be appreciated.\n\n3. Could you provide insights into the methodology for selecting global scales and local scales? This appears to be a pivotal component in combining the various forms of guidance.\n\n4. The experimental section is notably incomplete. There is a lack of quantitative comparisons with competing methods, and an ablation study is conspicuously absent. These omissions limit the comprehensive evaluation of the proposed approach's performance."
                },
                "questions": {
                    "value": "See Weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4039/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697618098375,
            "cdate": 1697618098375,
            "tmdate": 1699636367012,
            "mdate": 1699636367012,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3aZcbh1Sp4",
                "forum": "yCYnKMHX3u",
                "replyto": "bOedJtjhvb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4039/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4039/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the review.\n\n**Notations**  \n$f$ represents the layout control, which updates the latent variables.  \n$e^{b}_{j}$ is the score function corresponding to the condition $c^b_i$, which is the global context of a local detail $c^n_j$.\n\n\n**Novelty**  \nExisting methods including MultiDIffusion and Composable diffusion, utilize classifier-free guidance. This approach interprets the distinction between conditional noise and unconditional noise (derived from blank text) as global guidance. In our method, we consider 'dog' in a global prompt as an unconditional dog and 'black long-haired dog' in a local prompt as a conditional dog. Then, we compute the difference between them as local guidance.\n\nWe are currently conducting additional evaluations, including quantitative comparisons. See the general response above for our answer."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700422252485,
                "cdate": 1700422252485,
                "tmdate": 1700422252485,
                "mdate": 1700422252485,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TY8LmIIlsv",
                "forum": "yCYnKMHX3u",
                "replyto": "bOedJtjhvb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4039/Reviewer_7GYr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4039/Reviewer_7GYr"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "After reviewing the author's responses, I maintain my original decision. I recommend that the authors revise the paper to enhance clarity and comprehension."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638252674,
                "cdate": 1700638252674,
                "tmdate": 1700638252674,
                "mdate": 1700638252674,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PsbsNpR2Zm",
            "forum": "yCYnKMHX3u",
            "replyto": "yCYnKMHX3u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4039/Reviewer_m9tU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4039/Reviewer_m9tU"
            ],
            "content": {
                "summary": {
                    "value": "The method proposes a training free sampling method to compose both local details (i.e., object attributes) and global context such as a text prompt that describes the general scene using text-to-image diffusion models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- the paper is easy to understand."
                },
                "weaknesses": {
                    "value": "- The writing quality is subpar and needs to be further polished. For example, all quotes are using the wrong quotation marks. The writing needs to further improve and a lot of redundant context and information. \n- The experiment is quite limited, where only qualitative results are provided in the main paper. Thus, the performance of method is quite unclear.\n- The method novelty is limited, since it simply combines techniques proposed in composable diffusion [1] and lay-out control methods [2] for better compositional generation. I don't see any insightful contributions provided by the paper.\n\n[1] Liu et al., Compositional Visual Generation with Composable Diffusion Models. ECCV 2022 \\\n[2] Chen et al., Training-free layout control with cross-attention guidance. CVPR 2023"
                },
                "questions": {
                    "value": "- I think its worth providing quantitative metrics to showcase the method's performance.\n- I do think this paper needs much more efforts to polish and run extensive experiments and ideally provide contributions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4039/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4039/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4039/Reviewer_m9tU"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4039/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698624840182,
            "cdate": 1698624840182,
            "tmdate": 1699636366939,
            "mdate": 1699636366939,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ogw638Wxpk",
                "forum": "yCYnKMHX3u",
                "replyto": "PsbsNpR2Zm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4039/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4039/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank your comments"
                    },
                    "comment": {
                        "value": "Thanks for the review.\n\n**Novelty**  \nOur approach does not merely combine layout control [1] and composable diffusion [2] in a straightforward manner. Figure 6 illustrates the comparison between this simple combination and our method. Our approach excels in effectively integrating the global context and local details.\n\nWe are currently conducting additional evaluations, including quantitative comparisons. See the general response above for our answer."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700421214658,
                "cdate": 1700421214658,
                "tmdate": 1700421214658,
                "mdate": 1700421214658,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Cr6QayOmD5",
            "forum": "yCYnKMHX3u",
            "replyto": "yCYnKMHX3u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4039/Reviewer_h5z1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4039/Reviewer_h5z1"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new framework for simultaneous control over the global contexts and the local details in T2I without requiring additional training or fine-tuning. The key idea of the paper in attaining  detailed visual control is to decompose the complex prompts into manageable concepts and controlling object details while preserving global contexts. Experiments demonstrate the utility of the proposed approach in both generation and editing settings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed idea of layered generation, i.e. decomposing prompts and then guiding the global prompt with each local prompt in the image generation process is very interesting. This is more attractive from practical (e.g. industrial) use cases and is potentially widely applicable as the proposed method doesn\u2019t require additional training and the accurate segmentation masks."
                },
                "weaknesses": {
                    "value": "Although the proposed method is interesting, I feel the paper has few important weaknesses: Firstly, I couldn\u2019t find any quantitative evaluations in the paper. I checked the supplementary and couldn\u2019t find it there either. It is hard to understand the utility of the proposed method to a wide range of prompts without such a comparison. For example, I couldn\u2019t find strong evidence as to why LayoutGPT style methods are not better than the proposed approach. Secondly, it feels to me that the results (layers) with the proposed approach, might not fuse together well in the final generation. It would be helpful to see more qualitative results (supplementary demo shows a very limited samples and the composition looks artificial in those outputs - potentially limiting the applicability of the proposed approach in diverse use cases)."
                },
                "questions": {
                    "value": "Please see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4039/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698868802868,
            "cdate": 1698868802868,
            "tmdate": 1699636366880,
            "mdate": 1699636366880,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zZtA6s6oSq",
                "forum": "yCYnKMHX3u",
                "replyto": "Cr6QayOmD5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4039/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4039/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank your comments"
                    },
                    "comment": {
                        "value": "Thanks for the review.\n\n**Novelty**  \nLayoutGPT utilizes GLIGEN which is trained to control object layouts, while our method can employ any pre-trained diffusion models, making it widely applicable to various visual domains, such as animation.\n\nWe are currently conducting additional evaluations, including quantitative comparisons. See the general response above for our answer."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4039/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700421040841,
                "cdate": 1700421040841,
                "tmdate": 1700421040841,
                "mdate": 1700421040841,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]