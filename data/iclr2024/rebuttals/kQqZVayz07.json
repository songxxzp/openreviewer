[
    {
        "title": "Aligning Agents like Large Language Models"
    },
    {
        "review": {
            "id": "PaEKjJQqv2",
            "forum": "kQqZVayz07",
            "replyto": "kQqZVayz07",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5797/Reviewer_B6a1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5797/Reviewer_B6a1"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates how to align large-scale imitation learning agents with supervised finetuning (SFT) and reinforcement learning from preferences. Specifically, this paper utilizes imitation learning for pretraining, supervised learning for finetuning, and REINFORCE for learning from preferences. Initial empirical findings show the effectiveness of the procedure of LLM for aligning agents in a complex 3D environment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- A descent paper, easy to follow.\n- Interesting idea to explore LLM technique for RL agent."
                },
                "weaknesses": {
                    "value": "- Although the idea is interesting, the technique is too incremental, integrating IL for pretraining, IL for SFT, and REINFORCE for alignment.\n- With the incremental technique, I would like to see more sufficient and interesting empirical findings. However, this paper does not contain the discussion including but not limited to:\n1. Scaling Law. Whether model size scales, data scales, then the performance improves still holds for this setting?\n2. In-context Learning. Whether preferences with some demonstrations in the model context can be captured?\n3. Longer model context. Currently, the context length is 32 timesteps. Compared to typical LLMs with at least 2k steps, isn\u2019t it too small to memorize useful information in the context?\n4. During alignment, try PPO algorithm, which is verified as effective by many papers. Furthermore, why this paper utilizes an **undiscounted** version of REINFORCE? How about direct policy optimization (DPO) that utilizes supervised learning style alignment? As this paper only finetunes the last layer, how about full-parameter finetuning? How about LoRA?\n5. Comparison and discussion of offline-to-online reinforcement learning.\n6. Generalization. How about more environments or multi-task setting?"
                },
                "questions": {
                    "value": "Why is the imitation learning in the first stage claimed as \"unsupervised pretraining\" in Figure 1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5797/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5797/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5797/Reviewer_B6a1"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5797/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698312478863,
            "cdate": 1698312478863,
            "tmdate": 1699636610494,
            "mdate": 1699636610494,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O40kNkLRLe",
                "forum": "kQqZVayz07",
                "replyto": "PaEKjJQqv2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5797/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer B6a1 (Part 1/2)"
                    },
                    "comment": {
                        "value": "We thank you for your review and feedback on our work, and we\u2019re pleased that you found the core idea interesting. \n \n> Although the idea is interesting, the technique is too incremental, integrating IL for pretraining, IL for SFT, and REINFORCE for alignment. \n\nRegarding novelty, we agree that the individual components of our work, and even the combination of imitation learning and reinforcement learning from preferences, is not novel, as discussed extensively in our related work in Section 2. However, there have since been many improvements to the basic pipeline of imitation learning followed by RLHF (introduced by [1]) in the context of large language models which have not been investigated back in the context of agents. Our main contribution is to apply this modern pipeline to a visually rich and complex domain with real human data, with the application of recent advances to improve the efficiency of the costly online alignment. For example: \n\n \n**1. Separation of imitation learning into pre-training on general data and fine-tuning on limited task-specific data**\n\nPre-training on general text followed by instruction fine-tuning is essential for training helpful LLMs, but in the context of agents the usual procedure is to only imitate task-specific data. We demonstrate in Sections 4.1 and 4.2 that similar pre-training followed by task-specific fine-tuning is beneficial for task performance in our context for agents. We\u2019ve also now added an additional ablation in Appendix D that demonstrates that the pre-training stage is beneficial over imitating the task-specific behaviour alone. \n\n**2. Use of Agent Model / Encoder for Initialising Reward Model**\n\nWe demonstrate in Section 4.4 that utilising the agent model trained with an imitation learning loss as the initialisation for the reward model significantly improves the performance of the reward model and improves the feasibility of training a reward model with limited costly human feedback. While this is now typical for LLM reward models, the demonstration that this also applies in our case of reward modelling from pixels for decision-making agents across data sizes is a valuable contribution. \n\n**3. Introduction of Preference Fine-Tuning on Highest Reward Trajectories**\n\nWe demonstrate in Section 4.5 that additional fine-tuning on a subset of the trajectories used for training the reward model with greatest reward significantly improves the efficiency of the subsequent alignment of the model with online reinforcement learning. This approach was concurrently introduced for language models [2], but is novel in the context of agents as far as we are aware. This is a big quality of life improvement for an end-user or game-designer that would like to align a (large) generalist agent, since fine-tuning on existing data is much less expensive than online reinforcement learning in a complex domain. \n\n\nRegarding other potential directions for investigation that you mention:\n\n> Scaling Law. Whether model size scales, data scales, then the performance improves still holds for this setting? \n\nRegarding the scaling of the model, we\u2019ve also added additional results on how model size affects performance in our added ablation of the pre-training stage in Appendix D. As well as demonstrating the benefit of the unsupervised pre-training stage with supervised fine-tuning rather than training from scratch on the fine-tuning dataset, we found that larger model sizes (up to the 100M parameters we considered) are beneficial for our task even for training only on the more limited fine-tuning data. \n \n\n> In-context Learning. Whether preferences with some demonstrations in the model context can be captured? \nLonger model context. Currently, the context length is 32 timesteps. Compared to typical LLMs with at least 2k steps, isn\u2019t it too small to memorize useful information in the context? \n\nOur context corresponds to 32 previous timesteps as you mention. Since this is sufficient to determine the agent\u2019s current location and motion, it is possible for the agent to learn our desired navigation behaviour with this context. While longer context could be interesting in more general settings given the partial observability of the environment, we note that this would also be significantly more expensive in terms of compute and memory for little gain, given that there is a lot more information contained within each image of context than there is in each word of context. \n \nInvestigating whether demonstrations could be incorporated in the agent\u2019s context for in-context learning is an interesting idea, but out of scope of this work on efficiently aligning agents with preferences."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659193060,
                "cdate": 1700659193060,
                "tmdate": 1700659590907,
                "mdate": 1700659590907,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i3JKUxXQqK",
                "forum": "kQqZVayz07",
                "replyto": "B2yCLRAbse",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5797/Reviewer_B6a1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5797/Reviewer_B6a1"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your feedbacks, though I disagree that some of the topics I mention are out of scope in this paper. I decide to keep my score unchanged."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715802377,
                "cdate": 1700715802377,
                "tmdate": 1700715802377,
                "mdate": 1700715802377,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WEkRhPwjQX",
            "forum": "kQqZVayz07",
            "replyto": "kQqZVayz07",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5797/Reviewer_Lh2V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5797/Reviewer_Lh2V"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to apply the methodologies for alignment problem in large language models to game agents in a console game. The authors test the method in a console game (called Bleeding Edge) to align the behaviour of an agent follow human behaviour in a specific game mechanics to spawn off from one of the three starting points. The methodologies follows the InstructGPT procedures of 1. unsupervised pre-training the agent 2. Fine-tuning with demonstrated data 3. Using a reward function and align the deployed model for certain behaviour."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is written nicely and easy to follow. The proposed game is interesting and different from the rest of the community."
                },
                "weaknesses": {
                    "value": "No human behaviour data is going to be released. The game environment is not going to be released. This makes the reproducibility of the results impossible.\n\nLimited details are provided of the environment, such as the internal mechanism of the game, how it simulates the physics. These critical details are essential given the game is not widely known in the research community.\n\nThe task is not particularly challenging and focus on a niche mechanics in the game. The task is only focusing on a small part of the full game, with a maximum rollout episode of 100 timesteps (equivalent of 10 seconds of game play). The task is comparably simpler than a lot of the locomotive control problems in the simulated experiments, with only 16-dim action space.\n\nNo benchmark is established for the task. Please compare the result with some basic model-free or model-based RL agents. Even classical control algorithms should be able to provide decent benchmarks.\n\nSignificant amount of work is needed for the paper to make the cut for the quality of ICLR."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5797/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5797/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5797/Reviewer_Lh2V"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5797/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698389124047,
            "cdate": 1698389124047,
            "tmdate": 1699636610346,
            "mdate": 1699636610346,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "epH6R1cdIH",
                "forum": "kQqZVayz07",
                "replyto": "WEkRhPwjQX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5797/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Lh2V (Part 1/2)"
                    },
                    "comment": {
                        "value": "We thank you for your review and honest feedback on our work. We appreciate that you found the paper easy to follow, but would like to clarify our motivation and contribution to the community.  \n \n> Weaknesses: \n> \n> No human behaviour data is going to be released. The game environment is not going to be released. This makes the reproducibility of the results impossible. \n\nOur main contribution is our analysis of applying the modern LLM training pipeline to such a challenging environment, which as you mention is interesting and different to those environments usually used by the community. On the other hand, as a result of being a real video game, there are many challenges and limitations of using Bleeding Edge as an environment, such as the hardware requirements (the game has to be run in real time on dedicated hardware for rendering) that make it unsuitable for release in its current state. As a result of these challenges, we do not claim to be providing a benchmark or dataset paper. However, we carefully provided our entire training procedure in Section 4 (and Appendix A) and all training details in Appendix B in order to make our results reproducible by practitioners looking to apply our procedure in other environments, even if it is not possible for them to be precisely replicated in Bleeding Edge. \n\nTo be even more concrete, in addition to our general procedure the main contributions from our analysis are as follows:  \n \n**1. Separation of imitation learning into pre-training on general data and fine-tuning on limited task-specific data**\n\nPre-training on general text followed by instruction fine-tuning is essential for training helpful LLMs, but in the context of agents the usual procedure is to only imitate task-specific data. We demonstrate in Sections 4.1 and 4.2 that similar pre-training followed by task-specific fine-tuning is beneficial for task performance in our context for agents. We\u2019ve also now added an additional ablation in Appendix D that demonstrates that the pre-training stage is beneficial over imitating the task-specific behaviour alone. \n\n**2. Use of Agent Model / Encoder for Initialising Reward Model**\n\nWe demonstrate in Section 4.4 that utilising the agent model trained with an imitation learning loss as the initialisation for the reward model significantly improves the performance of the reward model and improves the feasibility of training a reward model with limited costly human feedback. While this is now typical for LLM reward models, the demonstration that this also applies in our case of reward modelling from pixels for decision-making agents across data sizes is a valuable contribution. \n\n**3. Introduction of Preference Fine-Tuning on Highest Reward Trajectories**\n\nWe demonstrate in Section 4.5 that additional fine-tuning on a subset of the trajectories used for training the reward model with greatest reward significantly improves the efficiency of the subsequent alignment of the model with online reinforcement learning. This approach was concurrently introduced for language models [1], but is novel in the context of agents as far as we are aware. This is a big quality of life improvement for an end-user or game-designer that would like to align a (large) generalist agent, since fine-tuning on existing data is much less expensive than online reinforcement learning in a complex domain. \n \n> Limited details are provided of the environment, such as the internal mechanism of the game, how it simulates the physics. These critical details are essential given the game is not widely known in the research community. \n\nThe environment we consider in this work is a real AAA video game, and so has relevant information publicly available [2,3] (including regarding the physics engine, PhysX [4]). We described what we believed to be the relevant aspects of the game for our research in Section 3, but have now also included links to these additional references in our paper, so thank you for highlighting that these additional details may also be of interest to the reader.  \n\n**References:**\n\n[1] Reinforced Self-Training (ReST) for Language Modeling, https://arxiv.org/abs/2308.08998\n\n[2] https://www.bleedingedge.com/en\n\n[3] https://www.pcgamingwiki.com/wiki/Bleeding_Edge\n\n[4] https://www.nvidia.com/en-gb/geforce/technologies/physx/"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658327284,
                "cdate": 1700658327284,
                "tmdate": 1700658346429,
                "mdate": 1700658346429,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qkgSjfniHT",
                "forum": "kQqZVayz07",
                "replyto": "WEkRhPwjQX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5797/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Lh2V (Part 2/2)"
                    },
                    "comment": {
                        "value": "> The task is not particularly challenging and focus on a niche mechanics in the game. The task is only focusing on a small part of the full game, with a maximum rollout episode of 100 timesteps (equivalent of 10 seconds of game play). The task is comparably simpler than a lot of the locomotive control problems in the simulated experiments, with only 16-dim action space. \n> \n> No benchmark is established for the task. Please compare the result with some basic model-free or model-based RL agents. Even classical control algorithms should be able to provide decent benchmarks. \n\nSimilarly, we feel that we should clarify the motivation of our work here: the navigation task we consider is representative of a behaviour that an end user might want an agent to complete. This task necessitates our proposed procedure since there is no reward function available. Therefore, how can we efficiently train an agent to complete such a behaviour? Even with access to the reward model to provide a single reward per trajectory (which already requires a reasonable policy/data to train), it would not be possible to use any model-free / model-based / classical control algorithms as baselines since they would never reach the goal to receive signal given we are training in an open 3D world from pixels only. Therefore we must use limited task-relevant / demonstration data for imitation learning. However, imitation learning may not provide exactly the preferred behaviour and is not very robust online due to distribution shift, particularly from pixels, necessitating final alignment with online preference-based reinforcement learning to reliably achieve the preferred behaviour, as demonstrated by our results in Section 4. \n \nWe hope that our responses above have helped to clarify the contribution of our work, and that you can now better appreciate the value of our insights and contribution towards bridging the gap between the LLM and gaming agent communities."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658725825,
                "cdate": 1700658725825,
                "tmdate": 1700661443344,
                "mdate": 1700661443344,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "99I5Uktqji",
                "forum": "kQqZVayz07",
                "replyto": "epH6R1cdIH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5797/Reviewer_Lh2V"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5797/Reviewer_Lh2V"
                ],
                "content": {
                    "title": {
                        "value": "Maintaining the score"
                    },
                    "comment": {
                        "value": "Thanks for the author's feedback. \n\nAfter reviewing the feedback and other reviews, I am maintaining my score. There exists major flaws in the set up of the problem given its limited experiments and diversity."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718783854,
                "cdate": 1700718783854,
                "tmdate": 1700718783854,
                "mdate": 1700718783854,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PisGdWsdrJ",
            "forum": "kQqZVayz07",
            "replyto": "kQqZVayz07",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5797/Reviewer_S6ki"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5797/Reviewer_S6ki"
            ],
            "content": {
                "summary": {
                    "value": "Paper investigates the LLM-alike pretraining - instruction fine-tuning - RLHF scheme in game AI domains. Specifically, it focuses on a game called Bleeding Edge, and for each stage, the resulting agent is mainly evaluated on a target task where the goal is to reach left/middle/right jumping pad. The results show that 1) after instruction fine-tuning, the overall performance on the target task can be improved; 2) RLHF can help align the instruction-tuned model to a certain aspect (ex. left jumping pad) of the task but the aligned model can be hard to be re-aligned to some other aspects, echoing the importance of maintaining diversity during the alignment stage."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "+The topic studied here is important. Albeit the success of the training-tuning-alignment scheme of large language models, its effectiveness in other domains has not been fully verified. This paper has made a valuable contribution to exploring this idea of agent learning in game AI and the results do show some promises. I believe it could drive the interest of audiences from both LLM and game AI communities (and possibly more).\n\n+Although the task considered in this paper can be relatively simple (left/middle/right jumping pad), it is indeed investigated thoroughly, and yield some interesting (but not surprising) results, including the struggle on re-alignment."
                },
                "weaknesses": {
                    "value": "Overall, I think the idea that this manuscript tries to put up with is clear and neat, but it can be a bit premature in terms of the width and depth of the investigation. Some substantial augmentation on the experiment part should be done before it can be accepted by a major conference. Here are some suggestions:\n\n-Width: the authors have claimed they \"investigate how the procedure for aligning LLMs can be applied to aligning agents from pixels in a complex 3D environment\". Although I do agree that the environment can be visually perplexed, the task, however, might not be challenging enough. Is the goal hard to be recognized/identified due to the complextity of the 3D environment? Are there many distractions or interferences? Is the goal semantically rich therefore understanding it could become a challenge? Does the human preference over such goal go beyond simple multi-choice selection? Unfortunately, I find many of these aspects of interest when exploring pretraining - instruction-tuning - RLHF in game AI unchecked in this paper. Point is, this scheme has been verified in the language domain teaming with many of these aforementioned challenges and it actually work pretty well. Therefore, investigating this on a different domain (game AI), but without on par challenges, could be less illuminating to the community.\n\n-Depth: although the paper offers rich variants of reward models in the RLHF experiments section, some aspects can still be missing. To name a few: the scale of the model(both the base model and the reward model), error range, etc. These will help with a better understanding on applying RLHF in a new domain."
                },
                "questions": {
                    "value": "See \"weaknesses\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5797/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698642036018,
            "cdate": 1698642036018,
            "tmdate": 1699636610230,
            "mdate": 1699636610230,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aRhdf4eu4B",
                "forum": "kQqZVayz07",
                "replyto": "PisGdWsdrJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5797/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer S6ki (Part 1/2)"
                    },
                    "comment": {
                        "value": "We thank you for your very constructive and helpful review. We\u2019re pleased that you appreciate the importance of the topic, the valuable contribution we have made to exploring this idea, and our thorough investigation.  \n \nTo address your concerns regarding the scope of our analysis: \n\n> -Width: the authors have claimed they \"investigate how the procedure for aligning LLMs can be applied to aligning agents from pixels in a complex 3D environment\". Although I do agree that the environment can be visually perplexed, the task, however, might not be challenging enough. Is the goal hard to be recognized/identified due to the complextity of the 3D environment? Are there many distractions or interferences? Is the goal semantically rich therefore understanding it could become a challenge? Does the human preference over such goal go beyond simple multi-choice selection? Unfortunately, I find many of these aspects of interest when exploring pretraining - instruction-tuning - RLHF in game AI unchecked in this paper. Point is, this scheme has been verified in the language domain teaming with many of these aforementioned challenges and it actually work pretty well. Therefore, investigating this on a different domain (game AI), but without on par challenges, could be less illuminating to the community. \n\nWe believe that our task is sufficiently challenging, and more importantly, illustrative for our analysis. Since we are using a real AAA video game with real human data from visual-only states, obtaining any behaviour reliably is challenging. The navigation task we considered is complex enough to necessitate our procedure, since there is no reward function to use for RL (and the preference reward model is too sparse for tabula rasa RL given the 3D environment), meaning RL alone is not possible. Therefore we must use limited task-relevant / demonstration data for imitation learning. However, imitation learning may not provide exactly the preferred behaviour and is not very robust to online distribution shift, particularly when learning from pixels, necessitating final alignment with online preference-based reinforcement learning to reliably achieve the preferred behaviour, as demonstrated by our results in Section 4. The task also still involves all the complexities of real human data, with the full visual input and gamepad action output spaces. \n \nAs you highlight, this approach has demonstrated success in the language domain, but it is not trivial to apply it to agents in the game domain. In particular, there is significant offline to online distribution shift in our task due to character selection and visual modifications as we discuss in our new Appendix E, as well as the context (equivalent to prompting) that the agent is provided with at initialization as we discuss in Section 4.1. Additionally, learning from pixels and in an environment in which online reinforcement learning can only be performed in real time (due to game engine constraints) provides a much more challenging domain than language benchmarks. Our work therefore demonstrates what can be learned and applied from progress in the language domain, while highlighting additional challenges in the game domain, as in Section 4.5."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657338837,
                "cdate": 1700657338837,
                "tmdate": 1700657338837,
                "mdate": 1700657338837,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AwCKfKboV4",
            "forum": "kQqZVayz07",
            "replyto": "kQqZVayz07",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5797/Reviewer_SbPz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5797/Reviewer_SbPz"
            ],
            "content": {
                "summary": {
                    "value": "Similar to recent work in finetuning LLMs to align them with human preferences, the authors propose a process for aligning game agents that have been initially pretrained on a diverse, multimodal dataset via imitation learning. The paper studies the specific setting of taking an agent pretrained on diverse human behaviors in a 3D game (Bleeding Edge), and finetuning the agent policy to focus on only navigating to a chosen one of the three possible locations. The overall process consists of initial pretraining on a diverse dataset of human behaviors, next finetuning on a smaller set of curated, high quality demonstrations for a particular behavior, then finally finetuning the policy with a learned preference-based reward model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The overall motivation of studying how well imitation-learning + RL with preference-based reward models applies aligning generally capable game agents is interesting and meaningful. \n- The additional experiments showing the efficacy of combining online learning with the learned reward model and additional finetuning on the top 20% of offline trajectories (as ranked by the same reward model) is interesting and a meaningful contribution, as most prior works only focus on either the offline or online settings individually rather than combining them.  \n- The experiments and discussion are thorough and detailed \u2013 in particular, it\u2019s helpful to note why the policy finetuned for the right jumppad was less effective, despite the symmetry of the map, and to clearly show the gap in performance when building a reward model off of the pretrained policy encoder rather than using a random one."
                },
                "weaknesses": {
                    "value": "- The paper shows that the same overall process for finetuning LLMs can also be applied to agents within this game. However, the overall method has limited novelty \u2013 as noted in the related works section, the general process of bootstrapping RL with imitation learning and further finetuning agent behaviors with reward models learned from human preferences have both been studied in prior work training RL agents outside of LLMs. \n- The authors note that unlike prior work, their goal is to train \"a generally capable base agent that can be aligned by game designers or end-users to perform different tasks or behave with different styles\". This is a meaningful contribution even if the overall process itself is not novel, however, I believe that the current set of behaviors studied in this work (navigating to 3 different locations) is too limited. The paper would be greatly strengthened if the sets of behaviors or tasks demonstrated by the base and fine-tuned agents were more complex. While I understand the merit of first studying the approach in a simpler setting to disentangle what the agent is learning at each stage, the paper would be more complete with additional experiments showing the methodology also works on learning more complex behaviors, as this is the underlying motivation of the work. \n- Rather than just having the bar plots in the first two stages and then reward curves for the final stage, the paper presentation would be strengthened with an overarching connecting figure showing a heat-map of how the agent behaviours across the map evolve at each stage (first showing diverse locations visited, then focusing on the jumppads evenly, then focusing on just the left/right one)."
                },
                "questions": {
                    "value": "While the more poor performance from the agent finetuning to the right jumppad is likely due to the lack of data and diversity as noted by the authors, this is also exactly the problem that RL should be able to address. Have the authors tried more sophisticated RL algorithms for encouraging diverse exploration (adding in an entropy bonus, injecting randomness, more complex intrinsic rewards on top of the human preferences)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5797/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698711011124,
            "cdate": 1698711011124,
            "tmdate": 1699636610112,
            "mdate": 1699636610112,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RxujT7NGew",
                "forum": "kQqZVayz07",
                "replyto": "AwCKfKboV4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5797/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer SbPz (Part 1/2)"
                    },
                    "comment": {
                        "value": "We thank you for your comprehensive review and helpful suggestions. We\u2019re pleased that you appreciate the meaningful motivation, the multiple contributions of our paper, and our thorough experiments and discussion. \n\nTo address your concerns: \n\n> The paper shows that the same overall process for finetuning LLMs can also be applied to agents within this game. However, the overall method has limited novelty \u2013 as noted in the related works section, the general process of bootstrapping RL with imitation learning and further finetuning agent behaviors with reward models learned from human preferences have both been studied in prior work training RL agents outside of LLMs.  \n\nRegarding novelty, we agree that the individual components of our work, and even the combination of imitation learning and reinforcement learning from preferences, is not novel, as discussed extensively in our related work in Section 2. However, there have since been many improvements to the basic pipeline of imitation learning followed by RLHF (introduced by [1]) in the context of large language models which have not been investigated back in the context of agents. Our main contribution is to apply this modern pipeline to a visually rich and complex domain with real human data, with the application of recent advances to improve the efficiency of the costly online alignment. For example: \n\n**1. Separation of imitation learning into pre-training on general data and fine-tuning on limited task-specific data**\n\nPre-training on general text followed by instruction fine-tuning is essential for training helpful LLMs, but in the context of agents the usual procedure is to only imitate task-specific data. We demonstrate in Sections 4.1 and 4.2 that similar pre-training followed by task-specific fine-tuning is beneficial for task performance in our context for agents. We\u2019ve also now added an additional ablation in Appendix D that demonstrates that the pre-training stage is beneficial over imitating the task-specific behaviour alone. \n\n**2. Use of Agent Model / Encoder for Initialising Reward Model**\n\nWe demonstrate in Section 4.4 that utilising the agent model trained with an imitation learning loss as the initialisation for the reward model significantly improves the performance of the reward model and improves the feasibility of training a reward model from pixels with limited costly human feedback. While this is now typical for LLM reward models, the demonstration that this also applies in our case of reward modelling from pixels for decision-making agents across data sizes is a valuable contribution. \n\n**3. Introduction of Preference Fine-Tuning on Highest Reward Trajectories**\n\nWe demonstrate in Section 4.5 that additional fine-tuning on a subset of the trajectories used for training the reward model with greatest reward significantly improves the efficiency of the subsequent alignment of the model with online reinforcement learning. This approach was concurrently introduced for language models [2], but is novel in the context of agents as far as we are aware. This is a big quality of life improvement for an end-user or game-designer that would like to align a (large) generalist agent, since fine-tuning on existing data is much less expensive than online reinforcement learning in a complex domain.  \n\nWe hope these three main examples highlight the novelty of our contribution in the context of aligning agents. \n\n**References:**\n\n[1] Reward learning from human preferences and demonstrations in Atari, https://arxiv.org/abs/1811.06521 \n\n[2] Reinforced Self-Training (ReST) for Language Modeling, https://arxiv.org/abs/2308.08998"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655841067,
                "cdate": 1700655841067,
                "tmdate": 1700655841067,
                "mdate": 1700655841067,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]