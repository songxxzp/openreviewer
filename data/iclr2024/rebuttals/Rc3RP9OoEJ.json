[
    {
        "title": "In-context Prompt Learning for Test-time Vision Recognition with Frozen Vision-Language Model"
    },
    {
        "review": {
            "id": "0ud6ozyDBk",
            "forum": "Rc3RP9OoEJ",
            "replyto": "Rc3RP9OoEJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5321/Reviewer_8csS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5321/Reviewer_8csS"
            ],
            "content": {
                "summary": {
                    "value": "This paper adapts visual prompt tuning to TTA of vision-language models. They use a token net and more examples to increase the generalization of vision-language models on domain-specific tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The introduction of visual prompt tuning is interesting."
                },
                "weaknesses": {
                    "value": "The pros and cons of the proposed method are not discussed thoroughly."
                },
                "questions": {
                    "value": "- 1. Why use the term \"in-context\"? What is the \"context\" for an incoming test sample? What is the relationship between the context samples and the test sample?\n\n- 2. The goal of TTA is to address the domain shift on the fly. The introduction of the context samples makes the model task-specific. For each dataset in Table 2 and Table 3, do we need to construct a dataset-specific context?\n    - 2.1 If the answer is YES, I think this is not a TTA method. It is a domain/dataset-specific 1-shot or few-shot approach.\n    - 2.2 If the answer is NO, the method is like [TPT with CoOp weights]. It is a one-shot training + test time visual prompt tuning. However, TPT+CoOp has much better performance.\n    - 2.3 The authors should further clarify how the context samples are selected. In Table 4, we can see the context samples are vital in the proposed method.\n\n- 3. Please also provide the inference time of the proposed method to evaluate the efficiency."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5321/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698043648130,
            "cdate": 1698043648130,
            "tmdate": 1699636534099,
            "mdate": 1699636534099,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iMdfgQF4yz",
                "forum": "Rc3RP9OoEJ",
                "replyto": "0ud6ozyDBk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5321/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5321/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed review of our paper and valuable feedback! We are encouraged that you found our work interesting and promising. In the following, we will answer your questions one by one.\n\n>Q1: Why use the term \"in-context\"? What is the \"context\" for an incoming test sample? What is the relationship between the context samples and the test sample?\n\nA1: i) In-context learning, a new paradigm in NLP, enables large language models to infer unseen tasks by conditioning on specific in-context examples. To harness this emergent ability in large vision-language models like CLIP, we develop a method to learn adaptive visual prompts. This is achieved using just a few image-label pairs, which act as contextual information, aiding the model to swiftly adapt to new test samples. Therefore, these pairs are referred to as in-context examples.  \n\nii) For an incoming test sample, the \"context\" is in-context samples, i.e., several provided image-label pairs, to let the model do task like these in-context samples do on the test sample.\niii) The context samples only provide the task information to do on the test sample.\nThey are usually from the same target dataset, while there is no other relationships between them, e.g., category.\n\n>Q2: The goal of TTA is to address the domain shift on the fly. The introduction of the context samples makes the model task-specific. For each dataset in Table 2 and Table 3, do we need to construct a dataset-specific context? If the answer is YES, I think this is not a TTA method. It is a domain/dataset-specific 1-shot or few-shot approach.\n\nA2:  Yes. We need to construct a dataset-specific context for each dataset in Table 2 and Table 3.\n\nHowever, our InCP is not a domain/dataset-specific 1-shot or few-shot approach.\nThe reasons are: i) domain-specific few-shot approach is for training process, while our InCP is in the test time; ii) few-shot approach needs at least one sample from each category for training, while in-context examples in our InCP have no this constraint. We only sample five in-context examples no matter what category they belong to. iii) few-shot approaches like CoOP [1]  and CoCoOP [2] employ a shared prompt for all test samples, whereas our InCP adopts an adaptive, sample-specific prompt for each individual test sample.\n\n\n>Q3: The authors should further clarify how the context samples are selected.\n\nA3: We employ only a few number (i.e., 5) of in-context examples. Those examples are randomly chosen from a designated subset of the target dataset, termed \"candidate in-context examples\". Such a subset is created by selecting one image-label pair from each category, ensuring a diverse and representative sample pool.\n\n\n>Q4: Please also provide the inference time of the proposed method to evaluate the efficiency.\n\nA4:  Thanks for your suggestion.\nWe report the inference time of the proposed method in the following table.\n\n| Method                 | Flower102 Infer. Time (\u2193) | Flower102 Top 1 acc. (\u2191) | Pets Infer. Time (\u2193) | Pets Top 1 acc. (\u2191) | Cars Infer. Time (\u2193) | Cars Top 1 acc. (\u2191) | Caltech101 Infer. Time (\u2193) | Caltech101 Top 1 acc. (\u2191) |\n| --- | --- | --- | --- | ---| ---| ---| --- | ------------------------- |\n| TPT [1] | 97.22  | 68.98     | 111.31       | 87.79    | 322.59  | 66.87    | 86.25  | 94.16     |\n| InCP (Ours)            | **23.79**                 | **72.27**                | **16.31**            | **90.62**           | **69.05**            | **67.54**           | **35.54**                  | **94.69**                 |\n\n\n[1] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. In IJCV, 2022.\n\n[2] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In CVPR, 2022."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5321/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487782871,
                "cdate": 1700487782871,
                "tmdate": 1700487782871,
                "mdate": 1700487782871,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aVnrLSAVDC",
                "forum": "Rc3RP9OoEJ",
                "replyto": "iMdfgQF4yz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5321/Reviewer_8csS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5321/Reviewer_8csS"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for the author's review.\n\nWell, I think the dataset-specific context selection process will largely limit the application of the proposed methods. The creation of the context put a strong prior on the method. Further discussion and justification are needed.\n\nSo I maintain my previous score, i.e., marginally below."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5321/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704944599,
                "cdate": 1700704944599,
                "tmdate": 1700704944599,
                "mdate": 1700704944599,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QJ1CWzYmaz",
                "forum": "Rc3RP9OoEJ",
                "replyto": "0ud6ozyDBk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5321/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5321/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback! We provide responses to your comments below:\n\n>Q1: I think the dataset-specific context selection process will largely limit the application of the proposed methods. The creation of the context put a strong prior on the method. Further discussion and justification are needed.\n\nA1: While our method is originally tailored for specific datasets, its adaptation to large-scale datasets such as ImageNet allows the extraction of significant context information. This context information serves as an in-context prompt that can be applied to different datasets or downstream tasks. The versatility introduced through this approach enables our method to generalize across various datasets, thereby enhancing its overall applicability."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5321/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734236630,
                "cdate": 1700734236630,
                "tmdate": 1700738637012,
                "mdate": 1700738637012,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I2CWk3KlMp",
            "forum": "Rc3RP9OoEJ",
            "replyto": "Rc3RP9OoEJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5321/Reviewer_QS1Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5321/Reviewer_QS1Q"
            ],
            "content": {
                "summary": {
                    "value": "This paper focused on the task of test-time prompt learning for vision-language models like CLIP, and proposed a visual in-context learning method to conduct one-time prompt tuning using both labeled samples as context and unlabeled test sample in a semi-supervised learning manner. The method updates both visual prompt and language prompt. Experiments are conducted on fine-grained classification datasets and distribution shift setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The task of test-time prompt tuning is relatively new and worth investigating. The idea of in-context prompt learning for vision-language models is novel to me.\n\n+ The proposed method achieved consistent improvement on fine-grained classification datasets, and competitive performance on distribution shifting datasets."
                },
                "weaknesses": {
                    "value": "- (Major concern) This work proposed a in-context prompt learning method that used labeled samples as context during test stage. However, different from traditional in-context learning that used additional samples as a part of prompts and did not tune any parameters, these method used additional samples to tune the network rather than serve as part of prompt, which is like traditional semi-supervised learning pipeline but with fewer data and training loops. I am wondering whether the terminology of \"in-context prompt learning\" is proper. More evidence or reference on the terminology definition is expected during discussion.\n\n- (Major concern) Random sampling may cause the information leakage of test set. Sec. 3.1 and Appendix A.2 indicate that the labeled data are randomly sampled for different test samples, which means that the model may see a wide range of test samples during evaluation, although they are not simultaneously seen. Compared to few-shot tuning setting, only a fixed set of labeled data is accessed. As shown in Appendix A.2, a fixed set of labeled data underperforms random sampling. I am wondering whether the performance gain is due to the information leakage caused by random sampling. A strict implementation would be using sampling from the same subset of labeled data for rather than the whole test set.\n\n- The figures are confusing and not well designed. First, in Figure 2, it is not clear what does token space mean. The number of input tokens are 4 and the number of output tokens are 2. Does it mean the number of input tokens equals to the number of words in the prompt, and the token net aims to compress the tokens? Second, in Figure 3, the module and tokens are denoted in many colors, but it is hard to understand what each color indicates, and what is the difference between different tokens in (c) and (d). Third, in Figure, the dimension of $P_t$ (i.e., 3) does not equal to the number of prefix tokens (i.e., 4). Is it in intention? Figure 1(c) is also confusing to me to understand what happened during the cyclic learning stage.\n\n- Difference between this method and fully test-time adaptation. According to Table 1, the main difference is that this method used additional labeled data for tuning the prompts. If so, the novelty is just additional annotations in a semi-supervised manner, which is limited.\n\n- The ablation studies in Tables 4 and 6 are not comprehensive, which are only on three datasets compared to the main results on Table 2. Why only these three datasets are selection? How are they representative?"
                },
                "questions": {
                    "value": "Please see Weaknesses for detailed comments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5321/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698091127542,
            "cdate": 1698091127542,
            "tmdate": 1699636533992,
            "mdate": 1699636533992,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Qo6WuJ3uNy",
                "forum": "Rc3RP9OoEJ",
                "replyto": "I2CWk3KlMp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5321/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5321/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed review of our paper and constructive feedback! We are encouraged that you found our task promising and our idea novel. We have revised our paper and addressed your concerns. Below, we detail each change made in response to your feedback.\n\n\n>Q1: I am wondering whether the terminology of \"in-context prompt learning\" is proper. More evidence or reference on the terminology definition is expected during discussion. \n\nA1:  We follow existing works [1,2,3] for this ``in-context learning'' concept, in which the model is updated using in-context examples.\nMeanwhile, CLIP itself is not able to conduct in-context learning task.\nTo equip CLIP with this ability, our InCP introduces learnable prompt for each test sample in test-time stage.\nIn this way, the model can automatically understand the underlying task with in-context examples. For sample selection, in-context learning has no constraint on category in testing stage. The in-context samples can either share the same category as the current test sample or the irrelevant category. For sample quantity, in-context learning uses a small, arbitrary set of labeled samples. Therefore, our InCP belongs to the in-context learning area.\n\n>Q2: I am wondering whether the performance gain is due to the information leakage caused by random sampling. A strict implementation would be using sampling from the same subset of labeled data for rather than the whole test set.\n\nA2: There is no information leakage in our InCP.\nActually, the selected 5 in-context examples do not share the same label with the test sample.\nThese 5 in-context examples are selected from a fixed subset of labeled data, which is composed by randomly sampling 1 sample from each category.\nFor each test sample, we use the same subset rather than the whole test set, which is the same setting as you suggested.\n\n>Q3: The figures are confusing and not well designed. First, in Figure 2, it is not clear what does token space mean. The number of input tokens are 4 and the number of output tokens are 2. Does it mean the number of input tokens equals to the number of words in the prompt, and the token net aims to compress the tokens? Second, in Figure 3, the module and tokens are denoted in many colors, but it is hard to understand what each color indicates, and what is the dfference between different tokens in (c) and (d). Third, in Figure, the dimension of (i.e., 3) does not equal to the number of prefix tokens (i.e., 4). Is it in intention? Figure 1(c) is also confusing to me to understand what happened during the cyclic learning stage.\n\nA3: i) \"token space\" in Figure 2 refers to the embedding space of text tokens and visual tokens. The number of input tokens equals to the number of words in the prompt and the output tokens.\nHere, the 4 to 2 means the dimension decrease, not the token number. Sorry for the confusion of token number.\nWe modify Figure 2 in the manuscript for clarification.\nii) The orange token represents a learnable visual token, the light purple tokens represent static visual tokens, the light gray tokens represent static text tokens, and the light orange tokens represent static prefix tokens in both Figure 3 (c) and (d). We update Figure 3 in the manuscript.\niii) Sorry for the confusion. The number of output tokens should be the same as the prefix tokens (i.e., 4). We modify it in the manuscript. iv) During the cyclic learning stage, we first optimize text prompt and then visual prompt in a sequential process. We update Figure 1 (c) for clarification.\n\n\n>Q4: Difference between this method and fully test-time adaptation. According to Table 1, the main difference is that this method used additional labeled data for tuning the prompts. If so, the novelty is just additional annotations in a semi-supervised manner, which is limited.\n\nA4: Fully test-time adaptation is a self-supervised learning method, which applies its self-supervised loss on unlabeled test sample to achieve the generalization capability of model on unseen domains. We argue that self-supervised learning, without task-specific supervision or model modifications, is insufficient for test-time adaptation, and thus propose in-context prompt learning for model adaptation. \n\nSemi-supervised learning typically incorporates labeled data during the training phase, amalgamating it with unlabeled data to fine-tune the model and improve its performance on unlabeled samples. Labeled data in semi-supervised learning often shares categories with the unlabeled data. In our method, there is no inherent relationship between in-context examples (labeled data) and the test sample, as they are both drawn from the same domain dataset. Our approach does not necessitate any category information about the test sample, distinguishing it from semi-supervised learning methods."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5321/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487512580,
                "cdate": 1700487512580,
                "tmdate": 1700487512580,
                "mdate": 1700487512580,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Cg9PQ5gqEr",
                "forum": "Rc3RP9OoEJ",
                "replyto": "6jR1EOKXuI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5321/Reviewer_QS1Q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5321/Reviewer_QS1Q"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the feedback. However, the rebuttal didn't address my concerns well.\n\nFirst, the authors claim that in-context learning means `the model is updated using in-context examples.` I share the same concern with Reviewer k8E2 that it is more like few-shot learning. I remain concerned about the terminology and whether the comparisons with previous prompt tuning methods are fair.\n\nSecond, the authors claim that `the selected 5 in-context examples do not share the same label with the test sample`. Does that mean the model can simply avoid predicting these five categories as they are confirmed not the label of the test sample?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5321/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686223745,
                "cdate": 1700686223745,
                "tmdate": 1700686223745,
                "mdate": 1700686223745,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0ljFrqhdYU",
            "forum": "Rc3RP9OoEJ",
            "replyto": "Rc3RP9OoEJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5321/Reviewer_k8E2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5321/Reviewer_k8E2"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a prompt learning method for vision-language models. It proposed to optimize both textual and visual prompts jointly (via cyclic learning) using an unsupervised objective on the test sample along with a supervised objective on some few-shot (in-context) examples. Experiment results show that it performs better than existing single-modality prompt learning methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed prompt learning method connects textual and visual prompts, which sounds well-motivated. Experiment results also support the idea.\n2. The ablation studies are well-designed and comprehensive."
                },
                "weaknesses": {
                    "value": "1. The method and experiment sections need more elaboration. Currently, there are many unclear points about the method itself and experiment set up. The implementation detail in Appendix A.1.2 doesn\u2019t cover all the details. (See my questions below.)\n2. I don\u2019t think it is appropriate to call the proposed method an \u201cin-context learning\u201d method. In-context learning in both the language and vision literature does not involve optimizing any parameters including exterior ones like prompts. I think the method proposed in this work is more like few-shot learning, which updates parameters on some example images and then uses the updated parameters to run inference on the test sample.\n3. It needs to provide more details about how Token Net is trained. If it is randomly initialized and only optimized for one step at test time, I don\u2019t think it can \u201ctranslate\u201d text tokens into visual prompts that the vision encoder comprehends. \n4. If I understand correctly, the InCP in Section 4.1 is learned using in-context examples from the target dataset. However, CoOp and CoCoOp are learned on the ImageNet dataset. A fair comparison would be to report CoOp/CoCoOp\u2019s results using the same few-shot examples as InCP."
                },
                "questions": {
                    "value": "1. In Figure 4, it seems like both $P_v$ and $P_t$ and the token net are trainable. But the learning objective in Eq (2) only involves $P_v$?\n2. Section 3.4 can be more elaborated. Currently, it\u2019s not very clear. For example, it seems like cyclic prompt learning involves multi-step optimization, while in the paragraph above section 3.4, it says the method is a single-step optimization. \n3. During testing, when a new test sample comes in, do you reset $P_v$, $P_t$, and TokenNet back to its initial state? Or are the parameter updates accumulated?\n4. Is the Token net separately trained before test-time adaptation or randomly initialized? How are $P_v$ and $P_t$ initialized?\n5. How many tokens do you use for $P_v$ and $P_t$? How big is the TokenNet?\n6. How many in-context examples do you use?\n7. I don\u2019t find the \u201cgeneric-language prompt\u201d baseline in Table 6."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5321/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5321/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5321/Reviewer_k8E2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5321/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698604558748,
            "cdate": 1698604558748,
            "tmdate": 1699636533899,
            "mdate": 1699636533899,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Dk8Q1w62yG",
                "forum": "Rc3RP9OoEJ",
                "replyto": "0ljFrqhdYU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5321/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5321/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed review of our paper and constructive feedback! We are encouraged that you found our method well-motivated, our experiment comprehensive, our ablation studies well-designed clear. We are addressing each of your comments and questions below:\n\n>Q1: I don\u2019t think it is appropriate to call the proposed method an \"in-context learning\" method. In-context learning in both the language and vision literature does not involve optimizing any parameters including exterior ones like prompts. I think the method proposed in this work is more like few-shot learning, which updates parameters on some example images and then uses the updated parameters to run inference on the test sample.\n\nA1: i) We follow existing works [1,2,3] for this \"in-context learning\" concept, in which the model is updated using in-context examples.\nMeanwhile, CLIP itself is not able to conduct in-context learning task. To equip CLIP with this ability, our InCP introduces learnable prompt for each test sample in test-time stage.\nIn this way, the model can automatically understand the underlying task with in-context examples.\n\nii) Our work is different from few-shot learning.\nThe main difference lies in two factors, i.e., sample selection and quantity.\n(a) For sample selection, few-shot uses strict categories with specific number of samples, which are widely used in training stage.\nDifferently, in-context learning has no constraint on category.\nThe in-context samples in testing stage can either share the same category as the current test sample or the irrelevant category. It is also impractical to know the exact category of unlabeled test sample in advance. \n(b) For sample quantity, few-shot learning requires a predefined number of samples from each category, while in-context learning uses a small, arbitrary set of labeled samples-commonly just five samples. Therefore, our InCP belongs to the in-context learning area, rather than the few-shot learning.\n\n\n>Q2: It needs to provide more details about how Token Net is trained. If it is randomly initialized and only optimized for one step at test time, I don\u2019t think it can \"translate\" text tokens into visual prompts that the vision encoder comprehends.\n\nA2: Yes, the token net is randomly initialized.\nThe feasibility of token net is based on CLIP, where the textual and visual encoders are already aligned well in the embedding space. The token net is initialized by text tokens, and then transferred to visual tokens with a few optimization steps on visual examples, which is easy to be converged.\n\n\n>Q3: If I understand correctly, the InCP in Section 4.1 is learned using in-context examples from the target dataset. However, CoOp and CoCoOp are learned on the ImageNet dataset. A fair comparison would be to report CoOp/CoCoOp\u2019s results using the same few-shot examples as InCP.\n\n\nA3: Thanks for your advice. We provide CoOp/CoCoOp\u2019s results using the same examples as InCP in the following table.\nIt shows that our InCP achieves better performance than CoOp and CoCoOp on this setting. Following your suggestion, we add this table in the manuscript for fair comparison.\n\n| Method (Top 1 acc.) | Flower | DTD   | Pets  | Cars  | Caltech |\n| ------ | :------: | :-----: | :-----: | :-----: | :----------: |\n| CoOP [2]  | 66.10   | 30.97 | 82.77 | 60.20  | 90.26      |\n| CoCoOP [3] | 67.23  | 31.72 | 83.14 | 59.78 | 90.43      |\n| InCP (Ours)    | **71.13** | **47.34** | **90.6**  | **67.54** | **94.69** |\n\n\n>Q4: In Figure 4, it seems like both $P_{\\text{v}}$ and $P_{\\text{t}}$ and the token net are trainable. But the learning objective in Eq (2) only involves $P_{\\text{v}}$?\n\nA4: Thanks for pointing out this. The set of $P_{\\text{v}}$, $P_{\\text{t}}$ and token net are all trainable in our method. The completed objective function is presented below. With step is 1 (i.e., $s=1$) for visual prompt learning, the objective function is $$\\mathcal{L}(x_t,\\mathcal{D},P_{\\text{v}},P_{t},\\theta)=\n\\underset{P_\\text{v}, \\theta}{\\mathrm{argmin}} \\{L(x_t, P_\\text{v}) + \\sum_{(x_i,y_i) \\in \\mathcal{D}} \\lambda L(x_i, y_i, P_\\text{v}, P_\\text{t}, \\theta)\\}.$$ With step is 2 (i.e., $s=2$) for text prompt learning, the objective function is $$\\mathcal{L}(x_t,\\mathcal{D},P_{\\text{v}},P_{\\text{t}},\\theta)=\\underset{P_\\text{t}}{\\mathrm{argmin}} \\{L(x_t, P_\\text{t}) + \\sum_{(x_i,y_i) \\in \\mathcal{D}} \\lambda L(x_i, y_i, P_\\text{v}, P_\\text{t}, \\theta)\\},$$\nwhere $x_t$ is unlabeled test sample, $\\mathcal{D}$ contains in-context examples, $\\theta$ is the parameter of token net, and $s=1,2$ are the step numbers of model optimization for visual and text prompt, respectively."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5321/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487189810,
                "cdate": 1700487189810,
                "tmdate": 1700487189810,
                "mdate": 1700487189810,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XLI1nf5Xqc",
            "forum": "Rc3RP9OoEJ",
            "replyto": "Rc3RP9OoEJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5321/Reviewer_3epP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5321/Reviewer_3epP"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method called \"visual In-Context Prompt learning (InCP)\" to enhance the CLIP model's performance in various downstream tasks. InCP enables a pre-trained vision-language model to adapt to new tasks by leveraging in-context examples without changing its core parameters. The key contributions of the paper include the introduction of InCP as an effective method for incorporating in-context information, exploration of language descriptions for visual prompt initialization, and achieving state-of-the-art results across diverse downstream datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper presents a new method by combining natural language processing and computer vision insights.\n+ The paper maintains high quality through a well-structured methodology and rigorous experiments, ensuring robustness and reliability in its findings.\n+ The paper is exceptionally clear, providing a strong background and conveying complex concepts effectively, making it accessible to a wide audience."
                },
                "weaknesses": {
                    "value": "- Efficiency is a crucial factor in test-time adaptation, given the significance of swiftly adapting to new environments. It is imperative that the paper includes explicit reporting and a comparative analysis of inference time metrics for InCP, especially when compared to existing methods like TPT. \n- This paper requires clarification regarding InCP's performance on the SUN397 dataset (utilized in TPT) in Table 1 and the ImageNet dataset (also used in TPT) in Table 2. Providing a comprehensive comparison of InCP's performance on these specific datasets will significantly enhance the paper's clarity and the reader's understanding.\n- The performance improvement in Table 2 appears to be marginal. Further explanation or additional results may be needed to demonstrate the significance of the improvement."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5321/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766271662,
            "cdate": 1698766271662,
            "tmdate": 1699636533792,
            "mdate": 1699636533792,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kPMh0XoBEc",
                "forum": "Rc3RP9OoEJ",
                "replyto": "XLI1nf5Xqc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5321/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5321/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comprehensive review, valuable comments, and constructive feedback. We are encouraged that you found our idea insightful, our method well-structured, our experiments rigorous, and our work exceptionally clear. We provide point-to-point responses to your comments below:\n\n>Q1: Efficiency is a crucial factor in test-time adaptation, given the significance of swiftly adapting to new environments. It is imperative that the paper includes explicit reporting and a comparative analysis of inference time metrics for InCP, especially when compared to existing methods like TPT.\n\nA1: The inference time is shown in the following table. All experiments are conducted on one 2080 Ti GPU, and inference time (Infer. Time) is calculated in minutes. The table shows that needs significant inference time due to augmenting images by 64 times. In contrast, our InCP only uses few in-context examples (i.e., 5) without any augmentation, requiring less inference time. This table clearly shows the efficiency of our method.\n\n| Method                 | Flower102 Infer. Time (\u2193) | Flower102 Top 1 acc. (\u2191) | Pets Infer. Time (\u2193) | Pets Top 1 acc. (\u2191) | Cars Infer. Time (\u2193) | Cars Top 1 acc. (\u2191) | Caltech101 Infer. Time (\u2193) | Caltech101 Top 1 acc. (\u2191) |\n| --- | --- | --- | --- | ---| ---| ---| --- | ------------------------- |\n| TPT [1] | 97.22  | 68.98     | 111.31       | 87.79    | 322.59  | 66.87    | 86.25  | 94.16     |\n| InCP (Ours)            | **23.79**                 | **72.27**                | **16.31**            | **90.62**           | **69.05**            | **67.54**           | **35.54**                  | **94.69**                 |\n\n>Q2: This paper requires clarification regarding InCP's performance on the SUN397 dataset (utilized in TPT) in Table 1 and the ImageNet dataset (also used in TPT) in Table 2. Providing a comprehensive comparison of InCP's performance on these specific datasets will significantly enhance the paper's clarity and the reader's understanding.\n\nA2: The performance on ImageNet and SUN397 datasets is shown in the following table.\nIt shows that our InCP achieves best performance compared with previous zero/few-shot methods.\nIn particular, we only use 5 samples yet is better than few-shot methods [2,3].\n\n| Method        | Type       | ImageNet  | SUN397    |\n| ------------- | :----------: | :---------: | :---------: |\n| CLIP-ViT-B/16 | Zero-shot  | 66.73     | 62.59     |\n| Ensemble [1]     | Zero-shot  | 68.34     | 65.63     |\n| TPT [1]          | Zero-shot  | 68.98     | 65.50      |\n| CoOP [2]         | Few-shot   | 71.51     | 64.15     |\n| CoCoOP [3]        | Few-shot   | 71.02     | 66.89     |\n| MaPLe [4]        | Few-shot   | 70.72     | 67.01     |\n| PromptSRC [5]    | Few-shot   | 71.27     | 67.10     |\n| InCP (Ours)         | In-context | **71.62** | **67.93** |\n\n\n>Q3: The performance improvement in Table 2 appears to be marginal. Further explanation or additional results may be needed to demonstrate the significance of the improvement.\n\nA3: As few-shot methods, CoOP [2] and CoCoOP [3] fine-tune the prompt on ImageNet dataset using 16-shot training data per category and evaluate the generalization performance on downstream tasks. As a self-supervised method, TPT is not trained on ImageNet. It utilizes the self-supervised loss with unlabeled test samples to achieve its zero-shot generalization ability. Our InCP follows TPT, while only using a few in-context examples as domain-specific context information. Table 2 in the main paper shows that our method outperforms TPT by notable margins on two datasets (i.e., 72.27\\% vs. 68.98\\% on Flower dataset, and 70.26\\% vs. 68.04\\% on UCF101 dataset). Our InCP can also consistently outperforms CoOP and CoCoOP when using unlabeled ImageNet data (i.e., 70.26\\% vs. 68.44\\% on UCF101 dataset), and using the same in-context examples (i.e., 71.13\\% vs. 67.23\\% on Flower dataset). The following table shows the comparison with CoOp and CoCoOp using the same in-context examples.\n\n| Method | Flower | DTD   | Pets  | Cars  | Caltech |\n| ------ | :------: | :-----: | :-----: | :-----: | :----------: |\n| CoOP [2]  | 66.10   | 30.97 | 82.77 | 60.20  | 90.26      |\n| CoCoOP [3] | 67.23  | 31.72 | 83.14 | 59.78 | 90.43      |\n| InCP (Ours)    | **71.13** | **47.34** | **90.6**  | **67.54** | **94.69** |\n\nReferece:\n\n[1] Manli Shu, et al. Test-time prompt tuning for zero-shot generalization in vision-language models. In NeurIPS, 2022.\n\n[2] Kaiyang Zhou, et al. Learning to prompt for vision-language models. In IJCV, 2022\n\n[3] Kaiyang Zhou, et al. Conditional prompt learning for vision-language models. In CVPR, 2022.\n\n[4] Muhammad Uzair Khattak, et al. Maple: Multi-modal prompt learning. In CVPR, 2023.\n\n[5] Muhammad Uzair khattak, et al. Self-regulating prompts: Foundational model adaptation without forgetting. In ICCV, 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5321/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483545503,
                "cdate": 1700483545503,
                "tmdate": 1700483699301,
                "mdate": 1700483699301,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]