[
    {
        "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization"
    },
    {
        "review": {
            "id": "pDA7ZPw5zd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1478/Reviewer_Hrx6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1478/Reviewer_Hrx6"
            ],
            "forum": "KOZu91CzbK",
            "replyto": "KOZu91CzbK",
            "content": {
                "summary": {
                    "value": "The paper propose a RLHF way to tune the prompt for the LLM agent."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors design a RLHF framework to tuning the prompt for the agents and have better scores on multiple tasks."
                },
                "weaknesses": {
                    "value": "I think the main weakness is that this kind of prompt tuning may not be necessary for a well trained agent. The agent should be well tuned to understand all kinds of different prompts. So the problem itself may not be very significant. Instead of fixing the agents and tuning the prompts, tuning the agents from RLHF feedback may have a more significant effect for the LLM. But this is just my opinion. The authors can discuss whether it's correct or not."
                },
                "questions": {
                    "value": "See the weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1478/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697383764203,
            "cdate": 1697383764203,
            "tmdate": 1699636076797,
            "mdate": 1699636076797,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QCBAN52pG8",
                "forum": "KOZu91CzbK",
                "replyto": "pDA7ZPw5zd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1478/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1478/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for review - necessity of exploration, prompt tuning vs LLM tuning"
                    },
                    "comment": {
                        "value": "Dear Reviewer Hrx6,\n\nWe are sincerely grateful to the reviewer for the informative feedback, which has helped improve our paper (please see the updated paper and appendix.) Please see our point-to-point response below.\n\n> Q1: I think the main weakness is that this kind of prompt tuning may not be necessary for a well trained agent. The agent should be well tuned to understand all kinds of different prompts. So the problem itself may not be very significant. \n\nA1: Thanks for pointing it out! In light of your comments, we\u2019ve added GPT-4, which has impressive decision-making capabilities, as the agents for comparison. Please see the updated Table 4, Fig. 4 and 6. In general, we find that even with GPT-4 as the agent, our Retroformer still shows significant improvements after just 1 episode. This is because even with a well-trained agent like GPT-4, it still needs exploration to cross off some incorrect answers and choose correct trajectories in the next round. If there is anything unclear about the updated results, please kindly let us know!  \n\n> Q2: Instead of fixing the agents and tuning the prompts, tuning the agents from RLHF feedback may have a more significant effect for the LLM. But this is just my opinion. The authors can discuss whether it's correct or not.\n\nA2: Thanks for the question! In light of the comment, we\u2019ve included one RL baseline, i.e., Soft Actor-Critic [1] that directly finetune the LLM agent online with environment feedback. For a fair comparison with the language agent methods in this paper, which all showed improvements in 5 episodes, we ran the RL method for 5 episodes but did not observe any improvement across all 3 environments. These results show that tuning the agent prompts turn out to be much more efficient than tuning the agent LLMs directly, under our problem setting. \n\n|     Method |     #Params |     #Retries |     HotPotQA |   |     AlfWorld |   |     WebShop |\n|------------|-------------|--------------|--------------|---|--------------|---|-------------|\n| SAC        |     2.25M   | N=1          | 27           |   | 58.95        |   | 30          |\n|            |             | N=4          | 27           |   | 59.7         |   | 30          |\n\n[1] Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" International conference on machine learning. PMLR, 2018."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1478/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641459289,
                "cdate": 1700641459289,
                "tmdate": 1700641539750,
                "mdate": 1700641539750,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GaVvBZn0iG",
            "forum": "KOZu91CzbK",
            "replyto": "KOZu91CzbK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1478/Reviewer_SkxB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1478/Reviewer_SkxB"
            ],
            "content": {
                "summary": {
                    "value": "Summary: the paper introduces Retroformer, an algorithm for training a LLM to optimize a retrospective model which provides feedback on another \"agent\" model's trajectory, incorporating it into a prompt which the agent LLM can condition on for its next trial at the task.\n\nThey do this by rolling out an agent LLM in the environment (where observations and actions are all text), prompting the retrospective model on the trajectory and the final reward (which is computed heuristically), then prompting the retrospective model to output text which reflects on what went wrong and what the agent can do better next time. The actor model then conditions on this text.\n\nThey create many rollouts using this procedure, score them, and finetune the retrospective agent using PPO.\n\nOn HotPotQA, Alfworld, and Webshop, Retroformer shows modest success improvements over Reflexion, the previous SOTA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is easy to read and positions itself clearly with respect to related work. \n- It addresses a challenge which has not been solved yet - the problem of how to do RL finetuning with language agents.\n- The proposed algorithm can be used in conjunction with a hosted actor LLM (e.g. GPT4) which cannot be finetuned. This seems important for making this algorithm useful for users.\n- The results show consistent improvements over Reflexion."
                },
                "weaknesses": {
                    "value": "- The paper's examples of bad/good prompts do not obviously show that the finetuned reflection model produces better prompts. For instance, in Figure 5, the Reflexion plan is better formatted but still doesn't point out that it was incorrect to return two series rather than one. It would be useful to see an analysis of how often the plan correctly identifies an improved plan (e.g. have a human hand-label 100 prompts for Reflexion and 100 from the frozen LM and record their success rates.)\n-\n- See my questions/suggestions below"
                },
                "questions": {
                    "value": "- I'm confused how the \"f1 score\" reward works.\n- I'd like to see the following additional curves added to Fig 4 and 6 (possibly in an appendix), which might make it clearer how the finetuning contributes:\n  - Retroformer, rolling out the base policy (before RL finetuning).  This would make it easier to see how much of the improvement above Reflexion is due to finetuning vs other algorithmic details.\n  - Retroformer, rolling out the base policy, but with GPT-3 as the reflection model. This would answer the question of whether it's typically a better strategy to finetune a small model or just prompt a more capable cloud model.\n- I'd recommend writing out the exact algorithm you used for PPO finetuning, especially since when PPO is used with language models slight details from the original algo are changed. Also, it's typically an online algorithm, so it would be helpful to detail any changes you made to make it work well in the offline setting.\n- Equation 6 seems to be optimizing for each (x, y) pair individually (i.e. not treating a sequence of (x1, y1), (x2, y2) (x3, y3)... as a trajectory. Is this correct? If so, I'd recommend making this clearer in the text."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1478/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1478/Reviewer_SkxB",
                        "ICLR.cc/2024/Conference/Submission1478/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1478/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698282521441,
            "cdate": 1698282521441,
            "tmdate": 1700705749956,
            "mdate": 1700705749956,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uTWzlEaI19",
                "forum": "KOZu91CzbK",
                "replyto": "GaVvBZn0iG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1478/Reviewer_SkxB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1478/Reviewer_SkxB"
                ],
                "content": {
                    "title": {
                        "value": "Correcting a typo"
                    },
                    "comment": {
                        "value": "In my \"Weaknesses\" section, I realized I accidentally said \"Reflexion\" where I meant \"Retroformer.\" Sorry for any confusion!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1478/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450975259,
                "cdate": 1700450975259,
                "tmdate": 1700450975259,
                "mdate": 1700450975259,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XVpMujbeHL",
                "forum": "KOZu91CzbK",
                "replyto": "GaVvBZn0iG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1478/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1478/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for review - correctness of plans, f1 score, two additional curves (Part I)"
                    },
                    "comment": {
                        "value": "Dear Reviewer SkxB,\n\nWe appreciate that you read our paper very carefully and your informative feedback, which has helped improve our paper. Below please see our response to your concerns. In case you find anything unclear in the paper, please kindly let us know.\n\n> Q1: It would be useful to see an analysis of how often the plan correctly identifies an improved plan, e.g. have a human hand-label 100 prompts for Reflexion and 100 from the frozen LM and record their success rates.\n\nA1: Thanks for the comment! Given the limited time, we\u2019ve manually labeled 5 reflection responses in the HotPotQA dataset, including the \u201cTeen Titans\u201d examples mentioned in the review. We found that with human labeling of the root cause of failure and next plans, the agent LM can always succeed in the next trial. For the reflection responses from the frozen LM, i.e, the Reflexion method, 1 out of 5 tasks succeeds in the next trial and with our Retrofomer fine tuned by policy gradient, under the same content for the remaining prompt, 3 out of 5 tasks succeed. \n\nFor the 100 tasks, one can refer to Figure 4. For example, under GPT-4, at episode 1, Reflexion has 48 successful tasks while Retroformer (r=4) has 51 successful tasks. This means that 8 (48-40) plans of frozen language model identifies an improved plan and 11 (51-40) plans of Retroformer identifies an improved plan, which shows the effectiveness of our approach. \n\nWe appreciate your suggestion for hand labeling, and we will label (the rest of) the reflection responses and add them for fine tuning the Retrospective model, as an ablation model for human-in-the-loop expert demonstrations, similar to the Dagger RL method .\n\n[1] Ross, St\u00e9phane, Geoffrey Gordon, and Drew Bagnell. \"A reduction of imitation learning and structured prediction to no-regret online learning.\" Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2011.\n\n> Q2: I'm confused how the \"f1 score\" reward works.\n\nA2: Thanks for spotting this! We\u2019ve updated the Appendix C.3 to include the description of reward functions used in 3 environments. To answer your question, I quote the updated descriptions below.\n\n> F1 reward is used in the HotPotQA environment for comparing the matching of a generated answer to a question against the ground truth answer.  After removing the stopwords in both answers, we calculate the number of common tokens in two answers. Then Precision is # of common tokens divided by # of generated answer tokens and the Recall is # common tokens divided by # ground truth answer tokens. We can then compute f1 from precision and recall. \nThis reward was defined in the ReAct paper, which is used as the baseline model in this paper.\n\n[2] Yao, Shunyu, et al. \"React: Synergizing reasoning and acting in language models.\" arXiv preprint arXiv:2210.03629 (2022).\n\n> Q3-1: I'd like to see the following additional curves added to Fig 4 and 6 (possibly in an appendix), which might make it clearer how the finetuning contributes: Retroformer, rolling out the base policy (before RL finetuning). This would make it easier to see how much of the improvement above Reflexion is due to fine tuning vs other algorithmic details.\n\nA3-1: Thanks for the comment! In light of the suggestions, we\u2019ve added the curve Retroformer (GPT-3, lora r=0) to Fig 4 and 6 to denote the base policy Retroformer agent with the frozen longchat-3b as the reflection model (before RL finetuning). The improvement due to finetuning is now entirely represented by the area, for example, of the curve Retroformer (GPT-3, lora r=4) above Retroformer (GPT-3, lora r=0).  \n\n> Q3-2: I'd like to see the following additional curves. Retroformer, rolling out the base policy, but with GPT-3 as the reflection model. This would answer the question of whether it's typically a better strategy to finetune a small model or just prompt a more capable cloud model.\n\nA3-2: Thanks for the suggestions! The Reflexion (GPT-3) curve in Figure 4 and 6 is actually the requested \u201cRetroformer, rolling out the base policy, but with GPT-3 as the reflection model\u201d.(By the way, we believe you meant GPT-4 for the reflection model as a more capable cloud model; please let us know if we are wrong.) \nTherefore, We\u2019ve also tried using GPT-4 as the reflection model, see Reflexion (GPT-4) and found that, for example, Retroformer (GPT-3/4, r=4) still outperformed it by a lot in all environments. This demonstrates that for generating reflective responses in a given environment, fine tuning a small model is more effective than prompting a more capable general-purpose model."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1478/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641083642,
                "cdate": 1700641083642,
                "tmdate": 1700641083642,
                "mdate": 1700641083642,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QUVLpNR7Jr",
                "forum": "KOZu91CzbK",
                "replyto": "GaVvBZn0iG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1478/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1478/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for review - PPO algorithm table,  equation 6 presentation (Part II)"
                    },
                    "comment": {
                        "value": "> Q4: I'd recommend writing out the exact algorithm you used for PPO finetuning, especially since when PPO is used with language models slight details from the original algo are changed. Also, it's typically an online algorithm, so it would be helpful to detail any changes you made to make it work well in the offline setting.\n\nA4: Thanks for pointing it out! We\u2019ve included an algorithm table for the PPO training in Appendix C.1 - Algorithm 1. We\u2019ve also added a description of the three steps.\n\n> The offline PPO algorithm we used for finetuning the Retrospective component in this paper is presented below in \\cref{Algo: PPO}. It contains three steps: offline data collection, reward model learning, and policy gradient finetuning. We use the offline ratings data to train a reward model first, and plug in the reward model for PPO finetuning\n\n> Q5: Equation 6 seems to be optimizing for each (x, y) pair individually (i.e. not treating a sequence of (x1, y1), (x2, y2) (x3, y3)... as a trajectory. Is this correct? If so, I'd recommend making this clearer in the text.\n\nA5: Thanks for carefully reading our paper! The presentation of this equation is actually correct.  $x$ in the equation refers to the instruction prompt, which includes textual descriptions of the agent trajectory, episodic returns, environment feedback at the end of one episode and $y$ denotes the reflective response generated by the Retrospective component taking this prompt as input. Therefore, for fine tuning the Retrospective model (Retroformer\u2019s task), there is only one step in this model\u2019s trajectory. In light of your comment, we\u2019ve updated the presentation to make it clearer!\n\n> (note there is only 1 step in the Retrospective model\u2019s trajectory in its episode)"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1478/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641366594,
                "cdate": 1700641366594,
                "tmdate": 1700643517625,
                "mdate": 1700643517625,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TxsgXUpmiF",
                "forum": "KOZu91CzbK",
                "replyto": "XVpMujbeHL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1478/Reviewer_SkxB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1478/Reviewer_SkxB"
                ],
                "content": {
                    "title": {
                        "value": "Score raised, one question remaining"
                    },
                    "comment": {
                        "value": "Hi,\n\nThanks for addressing my comments thoroughly! I especially appreciate the training-free curve in Fig 4 and 6 \u2013 this makes me a lot more confident that the finetuning of the retrospective model is helping.  As a result, I\u2019ve raised my score.\n\n> The Reflexion (GPT-3) curve in Figure 4 and 6 is actually the requested \u201cRetroformer, rolling out the base policy, but with GPT-3 as the reflection model\u201d\n\nYou're right I meant GPT4. I understand that the algorithms are identical, but I had assumed that your Reflexion baseline used the prompt from the Reflexion paper, and the Retroformer results used different prompts (which makes it hard to tell if the improvement comes from just better prompt tuning). If the prompts are different, then I still think adding the curve I suggested (equivalently, running Reflexion with the Retroformer prompt) would be good. If the prompts are identical, then this is great and makes your comparisons more convincing, so I\u2019d recommend highlighting this explicitly. (Apologies if it\u2019s already mentioned in the paper and I missed it)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1478/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705717774,
                "cdate": 1700705717774,
                "tmdate": 1700705717774,
                "mdate": 1700705717774,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UoFfFj8nHb",
            "forum": "KOZu91CzbK",
            "replyto": "KOZu91CzbK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1478/Reviewer_96C3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1478/Reviewer_96C3"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces \"Retroformer\", a framework designed to enhance LLM-assited agents. The Retroformer system comprises two language models, the actor and the retrospective model. The actor model executes tasks while the retrospective model provides feedback to the actor model, allowing it to self-improve. Retroformer employs both short-term and long-term memory to shape the rewards. Short-term memory is represented by the actor model's action history while long-term memory is created by appending summaries of previous failures to the task prompt. Experimental results demonstrates the effectiveness of Retroformer in various tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The overall framework designed by Retroformer is interesting and alleviates some of the shortcomings in previous works.\n- The paper is well-structured with clear explanations of terminology and content, aiding in readability."
                },
                "weaknesses": {
                    "value": "- The improvements brought by Retroformer are limited. There are no significant improvements in HotPotQA and WebShop, only meaningful improvement is observed in AlfWorld.\n- The experiments are not solid enough. It lacks comparisons with RL methods that have been recognized to perform well within the same framework of interaction-feedback-learning. Additionally, there is no comparison with the currently acknowledged GPT-4 model, which has impressive decision-making capabilities, making it insufficient to demonstrate the contribution of this work.\n- Only the prompt used in the HotPotQA environment is displayed, and it is difficult to determine whether special prompt engineering is needed in different environments. Therefore, it is insufficient to verify the claim of 'automatically tunes the language agent prompts'."
                },
                "questions": {
                    "value": "- The feedback obtained from the interaction between the agent and the environment is usually sparse, and the computed rating $r=G_{k,i+1}-G_{k,i}$ represents the difference in return between two consecutive episodes. This means that the data used for finetuning retrospect models are not significantly different within a certain period. How does Retroformer address the overfitting issue caused by fine-tuning on a large amount of repetitive data?\n- Are there any differences in the prompts used by Retroformer and baselines methods, and are there experimental results to analyze this part through ablation analysis?\n- What are the possible reasons for limited performance improvements in HotPotQA and WebShop?\n- How much efficiency is gained by using a retrospective model to tune prompts, rather than directly tuning the LLMs?\n- Are there any details about the hyperparameters of PPO?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1478/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698741737153,
            "cdate": 1698741737153,
            "tmdate": 1699636076631,
            "mdate": 1699636076631,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KVYeFtWyj0",
                "forum": "KOZu91CzbK",
                "replyto": "UoFfFj8nHb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1478/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1478/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the review - justification for performance improvement and RL baseline (Part I)"
                    },
                    "comment": {
                        "value": "Dear Reviewer 96C3,\n\nWe greatly appreciate your thorough and constructive comments, many of which have helped improve our paper. Both the paper and appendix have been updated extensively to include as much detail as possible. Please see our point-to-point response to your concerns below.\n\n> Q1: The improvements brought by Retroformer are limited. There are no significant improvements in HotPotQA and WebShop, only meaningful improvement is observed in AlfWorld.\n\nA1: Thanks for the comments. We\u2019d like to discuss them in the following three aspects: (1) Improvements in HotPotQA, (2) Improvements in WebShop, and (3) Improvements against RL methods.\n\n**(1) Improvements in HotPotQA** \n\nWe believe that there are significant improvements in HotpotQA by using Retroformer. For example, as in Table 2, Retrofomer (GPT-3, lora r=4) was improved by 14% in success rate within just one episode. The improvement is also 6% higher than the Reflexion (GPT-3) method that uses a frozen model without our proposed RLHF finetuning. In light of your comments, we\u2019ve also tested GPT-4 as an agent in the updated experiments and found even GPT-4 gets stuck at 54% success rate. We believe that 54% is the upper bound of the success rate under existing LLMs.\nIt deserves notice that HotPotQA, which is a text-based game, has an extremely large state and action space (you can treat it as a RL problem with a continuous action of 768 dimensions). It should take more than 500 episodes to observe certain improvements in some much simpler text-based environments in [1]. Our improvements (14%) by Retroformer in just one episode are significant. \n\n[1] Yuan, Xingdi, et al. \"Counting to explore and generalize in text-based games.\" arXiv preprint arXiv:1806.11525 (2018).\n\n**(2) Improvements in WebShop** \n\nThanks for carefully reading our paper! However, the difficulty of solving this web shop environment is well known in all recent language agent publications [2,3,4] that use this environment. As we stated in our previous submission, we agree that the performance improvement by Retroformer over the frozen baselines is observed but the improvements may be limited. This limitation of improvement was due to the fact that \u201cweb browsing requires a significant amount of exploration with more precise search queries\u201d. Nevertheless, our Retroformer method is still performing better than Reflexion and React across episodes, which use a frozen model without our proposed RLHF fine tuning. In light of your comments, we\u2019ve added GPT-4 agents in the updated experiments and found similar results in the updated Figure 6 (b). \n\nWe\u2019ve explained these results in the updated draft in Section 4 - Web Browsing.\nThe results probably indicate that the verbal feedback approach (Reflexion, Retroformer) is not an optimal method for this environment, but our fine-tuning method still proves effective. \n\n[2] Yao, Shunyu, et al. \"React: Synergizing reasoning and acting in language models.\" arXiv preprint arXiv:2210.03629 (2022).\n[3] Shinn, Noah, Beck Labash, and Ashwin Gopinath. \"Reflexion: an autonomous agent with dynamic memory and self-reflection.\" arXiv preprint arXiv:2303.11366 (2023).\n[4] Liu, Xiao, et al. \"Agentbench: Evaluating llms as agents.\" arXiv preprint arXiv:2308.03688 (2023)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1478/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639818197,
                "cdate": 1700639818197,
                "tmdate": 1700639818197,
                "mdate": 1700639818197,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pqXFrHv6fZ",
                "forum": "KOZu91CzbK",
                "replyto": "UoFfFj8nHb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1478/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1478/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the review - performance improvement against RL baseline and under GPT-4 (Part II)"
                    },
                    "comment": {
                        "value": "**(3) Improvements against RL methods**\n\nIn light of your comments, to show the improvements against traditional RL methods, we added one method, i.e., Soft Actor-Critic [5] as the baseline. \nWe have updated the experiment results in Table 2 and added the implementation details of the RL method in the updated Appendix C.2, which is quoted below.\n\u201cWe include one online RL algorithm, i.e., Soft Actor-Critic [5],  or SAC as a baseline model for comparison. Given that the three environments are text-based games, inspired by [1], we do mean-pooling for the embeddings of the generated text outputs, such as ``Search[It Takes a Family]'' as the agent actions. Therefore, the action space is continuous and is of 768 dimensions. We apply LoRA adapters with r=4 on the agent Action model instantiated from longchat-16k, and use SAC to do the online updates, with discount factor gamma=0.99, interpolation factor polyak=0.995, learning rate=0.01, entropy regularization alpha=0.2, and batch size=8.\u201d\n\nFor a fair comparison with the language agent methods in this paper, which all showed improvements in 5 episodes, we run SAC for 5 episodes but do not observe significant improvement across all 3 environments in the updated Table 2. \n|     Method |     #Params |     #Retries |     HotPotQA |   |     AlfWorld |   |     WebShop |\n|------------|-------------|--------------|--------------|---|--------------|---|-------------|\n| SAC        |     2.25M   | N=1          | 27           |   | 58.95        |   | 30          |\n|            |             | N=4          | 27           |   | 59.7         |   | 30          |\n\nThis is mostly due to the fact that the three text-based games have an extremely large state and action space, and thus online exploration and learning within 5 episodes online cannot improve the LLM model with an extensive parameter count. Under this setting, verbal reinforcement with an LLM that is finetuned offline for generating effective reflective feedback (i.e., our Retroformer) is much more effective. \n\nTo summarize for Q1, we do believe the improvements brought by Retroformer are not limited, especially if the reviewer considers how traditional RL methods perform under the same problem setting. We do observe significant improvements under HotPotQA and AlfWorld. Webshop requires a significant amount of exploration with more precise search queries but our Retroformer method still performs better than Reflexion and React across all episodes. The results under Webshop indicate that the verbal feedback approach (Reflexion, Retroformer) is not an optimal method for this environment, but our fine-tuning method still proves effective.  If there is anything unclear, please kindly let us know!\n\n[5] Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" International conference on machine learning. PMLR, 2018.\n\n> Q2: The experiments are not solid enough. It lacks comparisons with RL methods that have been recognized to perform well within the same framework of interaction-feedback-learning. \n\nA2: Thanks for raising the concern! Please refer to the detailed response above for the section Improvements against RL methods. In summary, we\u2019ve added Soft Actor-Critic as the baseline. We run SAC for 5 episodes but do not observe significant improvement across all 3 environments with the updated results in Table 2. \n|     Method |     #Params |     #Retries |     HotPotQA |   |     AlfWorld |   |     WebShop |\n|------------|-------------|--------------|--------------|---|--------------|---|-------------|\n| SAC        |     2.25M   | N=1          | 27           |   | 58.95        |   | 30          |\n|            |             | N=4          | 27           |   | 59.7         |   | 30          |\n\n> Q3: Additionally, there is no comparison with the currently acknowledged GPT-4 model, which has impressive decision-making capabilities, making it insufficient to demonstrate the contribution of this work.\n\nA3: Thanks for pointing it out! In light of your comment, we have updated the paper draft by adding experiments using GPT-4  (See the updated Table 2, Fig. 4 and 6). In general, we observe consistent improvements at episode 0 where verbal feedback is not applied. This is because of the better decision-making capabilities of GPT-4. Furthermore, we still observe consistent improvements by Retroformer under GPT-4,  which again demonstrates the effectiveness of verbal feedback with a well-trained actor (e.g., GPT-4) together with a retrospective component tuned with our policy gradient method. We have updated the draft to include this analysis of GPT-4 results. If there is anything unclear, please kindly let us know!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1478/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640043777,
                "cdate": 1700640043777,
                "tmdate": 1700640082178,
                "mdate": 1700640082178,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h2Ns2CrbI5",
                "forum": "KOZu91CzbK",
                "replyto": "UoFfFj8nHb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1478/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1478/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the review - tuning prompt vs LLMs, hyperparameters (Part IV)"
                    },
                    "comment": {
                        "value": "> Q8: How much efficiency is gained by using a retrospective model to tune prompts, rather than directly tuning the LLMs?\n\nA8: Thanks for the question. In light of your comments, we added one RL baseline (Soft Actor-Critic) in the updated experiments which directly tunes the LLMs instead of tuning the prompts. For a fair comparison with the language agent methods in this paper, which all showed improvements in 5 episodes, we run SAC for 5 episodes but do not observe improvement across all 3 environments. These results show that tuning the agent prompts are much more efficient than tuning the agent LLMs directly, under our problem setting. \n|     Method |     #Params |     #Retries |     HotPotQA |   |     AlfWorld |   |     WebShop |\n|------------|-------------|--------------|--------------|---|--------------|---|-------------|\n| SAC        |     2.25M   | N=1          | 27           |   | 58.95        |   | 30          |\n|            |             | N=4          | 27           |   | 59.7         |   | 30          |\n\n> Q9: Are there any details about the hyperparameters of PPO?\n\nA9: Thanks for pointing it out. We have updated in Appendix C.1 the details about the hyperparameters of PPO. We\u2019ve also written out the exact algorithm we used for PPO finetuning in the updated Appendix C.1. Please kindly let us know if anything is not clear.\n\n> The offline PPO algorithm we used for finetuning the Retrospective component in this paper is presented below in Algorithm 1. It contains three steps: offline data collection, reward model learning, and policy gradient finetuning. We use the offline ratings data to train a reward model first, and plug in the reward model for PPO finetuning."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1478/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640599262,
                "cdate": 1700640599262,
                "tmdate": 1700640610234,
                "mdate": 1700640610234,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pbhxWCMcaQ",
                "forum": "KOZu91CzbK",
                "replyto": "h2Ns2CrbI5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1478/Reviewer_96C3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1478/Reviewer_96C3"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' response, however, I still have concerns about some issues.\n\n> about the sparse feedback\n\nI don't think collecting offline data fundamentally addresses the issue I raised. Although the case used by the authors is not that serious (but still sparse), in some more general and difficult tasks it's often more challenging to address the sparsity problem. I think Retroformer lacks a filter processing for the data.\n\n> about comparison with RL methods\n\nConsidering that SAC is not a current SOTA method in RL, and the relatively modest performance difference observed on the WebShop task, I believe that the performance improvement offered by Retroformer might not be significant, especially when considering its substantial resource consumption compared to traditional methods. If possible, I would suggest the authors include more baselines to further validate this.\n\n> about comparison with GPT4\n\nThe authors might have misunderstood my initial concern. I hope to see a full performance comparison between Retroformer and pure GPT4. My concern is that considering GPT4 has already shown enough excellent decision-making performance in some work and does not depend on extra feedback information, is it necessary to design a framework like Retroformer? Therefore, I want to see such a comparison to confirm this."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1478/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707227017,
                "cdate": 1700707227017,
                "tmdate": 1700707227017,
                "mdate": 1700707227017,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dnm3iSGjhu",
                "forum": "KOZu91CzbK",
                "replyto": "4coweoGlfc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1478/Reviewer_96C3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1478/Reviewer_96C3"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's response.\n\n- Regarding sparse feedback, my concern is that considering the small differences between consecutive episodes, even with processing such as retaining only higher rating data, they will still be very similar or even repeated. Thus, using episode return calculation as a rating design seems confusing to me.\n- Regarding comparison with RL methods, as in some recent LLM-based works [1], they compared various RL baselines. In [1], some RL methods [2,3] even outperformed other LLM baselines. Therefore, I think simply comparing with ReAct, Reflexion and SAC is not sufficient to demonstrate the superiority of Retroformer. However, considering the differences in experimental environments and tasks, the baselines I listed may not necessarily need to be compared. Instead, I suggest that the authors consider selecting a wider range of different types of baselines to fully demonstrate Retroformer's superiority.\n\n[1] SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning.\n\n[2] Mastering diverse domains through world models.\n\n[3] Uncertainty-driven exploration for generalization in reinforcement learning."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1478/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727980961,
                "cdate": 1700727980961,
                "tmdate": 1700727980961,
                "mdate": 1700727980961,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J4tnTdU7X1",
                "forum": "KOZu91CzbK",
                "replyto": "UoFfFj8nHb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1478/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1478/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to sparse feedback and further comparisons with additional RL methods"
                    },
                    "comment": {
                        "value": "Dear Reviewer 96C3,\n\nThanks for the prompt response. We are glad that we are aligned that the previous concerns on the comparisons with GPT4 have been resolved.\n\n1. For the **sparse feedback problems**, let me try to explain this step by step to demonstrate why it is not sparse or repeated. \nFor example, the agent was asked to answer a question in the HotpotQA by using Wikipedia APIs. In the first episode, it failed ($G_0=0$), and then multiple different reflections are generated, in which one reflection helps the agent succeed in the next round ($G_1=1$),; the other one doesn't help so the agent failed again in the next trial ($G_1=0$),. Then the first reflection is good and rated higher ($r=G_1-G_0=1$ and the second reflection is bad and rejected ($r=G_1-G_0=0$). The trajectory in the second episode is even not used for offline training in this example. Can the reviewer explain why the small differences between consecutive episodes or similarity could cause a problem for RLHF fine-tuning?\n\n2. For continuous RL control problem, we are very confident that soft-actor critic is still the most stable SOTA baseline according to our knowledge. That is the reason why we chose it in the beginning during rebuttal. Furthermore, we do not believe any online traditional RL algorithm can learn within 1 episode (~6 steps) in these complex text-world environments (note **all state-of-the-art RL algorithms** including [2,3] in the paper [1] cited by reviewer have been trained for **1M steps**).  Could you please provide one specific RL baseline you would like us to try further, besides SAC, given the limited amount of time available? \n\nWith best regards, Authors of submission 1478"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1478/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730257008,
                "cdate": 1700730257008,
                "tmdate": 1700731032268,
                "mdate": 1700731032268,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7oRPH8GdGM",
                "forum": "KOZu91CzbK",
                "replyto": "J4tnTdU7X1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1478/Reviewer_96C3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1478/Reviewer_96C3"
                ],
                "content": {
                    "comment": {
                        "value": "- In the example you provide, there is a very clear reward, i.e., 1 for success and 0 for failure. However, in broader environments, providing such distinct feedback for an environment or task becomes challenging. In other words, more general environments often present situations like  $G_0=0.5, G_1=0.51$, resulting in $r=0.01$. And for most episodes data, the computed rating $r$ might consistently remain small, hindering RLHF's ability to effectively distinguish useful episodes. Consequently, this aspect of Retroformer's design poses a significant limitation, rendering it less suitable for general tasks.\n- Considering the rebuttal deadline, it seems no more time to have further discussions. I can only suggest in a more general sense that the authors should consider comparing other kind of LLM-based methods or some exploration-based RL baselines (e.g., [1], [3] in the previous response, respectively) instead of limiting comparisons to refine-based methods like ReAct and Reflexion."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1478/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738710863,
                "cdate": 1700738710863,
                "tmdate": 1700738710863,
                "mdate": 1700738710863,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]