[
    {
        "title": "Towards Neural Architecture Search through Hierarchical Generative Modeling"
    },
    {
        "review": {
            "id": "8vHf4FPTT7",
            "forum": "y3qpL2Ioys",
            "replyto": "y3qpL2Ioys",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6512/Reviewer_2D3t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6512/Reviewer_2D3t"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an efficient method of constructing and searching through neural architecture search spaces. Their method consists of multiple stages. In the first stage, a graph variational autoencoder is trained to produce embeddings of different architectures on the cell level. The authors add a margin loss comparing architectures using zero-cost proxies to encourage the architectures to cluster. The resulting latent space is clustered using a gaussian mixture model. \nIn order to sample architectures given a well-performing reference architecture in the high dimensional latent space of the VAE, the authors employ a conditional, continuous normalizing flow model. In order to create the macro architectures, the authors propose to fine-tune a decoder-only generative transformer model on architectures found via evolutionary optimization, which uses a zero-cost proxy for its search. \nThe authors demonstrate competitive performance with both aligned methods on CIFAR-10 and CIFAR-100 as well as overall NAS methods on Imagenet."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The method is well motivated, each problem encountered in the architecture generation process is clearly explained and their approach to solve it makes sense to me.\n- The authors openly state the limitations and assumptions made by their framework, I appreciate the level of detail.\n- Overall clearly written paper."
                },
                "weaknesses": {
                    "value": "- CIFAR-10, CIFAR-100 and Imagenet are benchmarks which the community has collectively overfit on. If time permits do you think you could provide results on a different modality, e.g. an NLP task?\n- To better understand the variance of your method, could you rerun the architecture selection multiple times and also report conf. intervals over the variance of each architecture when run for 3-5 random initializations?\n- As stated in their limitations, the method largely relies on ZC proxies for both G-VAE and the generation of the macro architecture. \n- Code was not submitted alongside the submission."
                },
                "questions": {
                    "value": "- How do you estimate the number of components for the GMM?\n- Why is the cost of the ES for training the SG such a major bottleneck when using ZC proxies? Did you parallelize ES?\n\n\nTypos (only minor, just listing them for completeness):\n- 'even the best searching algorithm' -> 'even the best search algorithm'\n- '... impressive results for modelling highly-dimensional conditional ...' -> '... impressive results for modelling high-dimensional conditional ...'\n- 'to a continues latent space' -> 'to a continuous latent space'"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6512/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6512/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6512/Reviewer_2D3t"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6512/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697805088873,
            "cdate": 1697805088873,
            "tmdate": 1699636731428,
            "mdate": 1699636731428,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f1NISyPNWG",
                "forum": "y3qpL2Ioys",
                "replyto": "8vHf4FPTT7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6512/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6512/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your positive feedback."
                    },
                    "comment": {
                        "value": "We are deeply grateful for your positive evaluation and insightful comments on our manuscript. Your feedback not only encourages us but also enhances the quality of our research. Regarding the issues you have raised, we hope the following answers are satisfied. Please let us know if you see any further issues in the paper that are unclear or need to be addressed. \n  \n\n> If time permits do you think you could provide results on a different modality, e.g. an NLP task?We\n\nSince generalizability was a common concern among reviewers, we decided to include experiments on NB360. We hope this should be more than enough to satisfy this request. Please see the common reply summarising new experiments.\n\n>To better understand the variance of your method, could you rerun the architecture selection multiple times and also report conf. intervals over the variance of each architecture when run for 3-5 random initializations?\n\nPlease see the common reply summarising new experiments.\n\n>As stated in their limitations, the method largely relies on ZC proxies for both G-VAE and the generation of the macro architecture.\n\nRecent work in zero-cost metrics has shown that zero-cost metrics-based NAS can produce promising performance networks compared with the training-based NAS approach. Admittedly, the zero-cost metrics performance can vary when ranking models from different tasks and different search spaces. However, in our generator, we are not strictly based on the assumption that model performance ranking strictly follows the zero-cost metrics ranking, but \u2018similar zero-cost metrics will lead to relatively similar performance\u2019. This is different from existing zero-cost metrics NAS.\n\n>How do you estimate the number of components for the GMM?\n\nTo decide the number of components for the GMM we increase their number, starting from 16, and calculate the Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC). We stop when they plateau.\n\n>Why is the cost of the ES for training the SG such a major bottleneck when using ZC proxies? Did you parallelize ES?\n\nPlease note we report GPU-hours, i.e., cost and not wallclock time \u2014 to avoid confusion, we will change the term used in the paper accordingly. This cost is subject to parallelization, following common practices. Since all steps can be parallelized trivially, we can achieve almost linear speedup if we use multiple GPUs, e.g., with 4 GPUs we can finish everything within ~8h.\n\nRegarding the relative cost of ES to the rest of the system. The bulk of the cost comes from calculating proxies for models, so it is going to be directly proportional to the amount of \u201clabelled examples\u201d mentioned in Tab. 5 (compared to calculating proxies, operating on graphs is very fast, e.g., when training a G-VAE or running a metrics predictor). As such, getting the training data for G-VAE and the SG should take similar time (50k vs 60k), but note that the micro model uses only up to 64 channels (Eq. 2) and our macro search space allows for <=1024, so the average model is much larger in the macro stage.\n\nFinally, we did not dedicate a lot of time optimising our code and there are some opportunities to achieve non-negligible speedups. To give an example, when computing the vector of proxies we currently compute each of: NASWOT, SNIP-SSNR and FLOPs completely independently from each other, resulting in 3 forward passes and 1 backward pass per model, but it clearly should be possible to optimise it such that only 1 forward and 1 backward is needed per model. This would reduce the time needed for data generation by up to 2x."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6512/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725701680,
                "cdate": 1700725701680,
                "tmdate": 1700725701680,
                "mdate": 1700725701680,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jzMtWM6MPH",
            "forum": "y3qpL2Ioys",
            "replyto": "y3qpL2Ioys",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6512/Reviewer_sRLa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6512/Reviewer_sRLa"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to reduce the dependency of NAS on the search space design to improve NAS's applicability to less-studied tasks. The core technique is a new hierarchical generative model pretrained using a metric space of graphs and their zero-cost similarity. Experiments are conducted on several standard benchmark datasets, including CIFAR and ImageNet."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-motivated. \n2. Improving the applicability of NAS to less-studied tasks is a fundamental challenge for NAS and has large practical values."
                },
                "weaknesses": {
                    "value": "1. The proposed method still heavily depends on human prior knowledge (e.g., reference design, zero-cost metric). I do not see any improvements in improving NAS's applicability to less studied domains. \n2. The generalization of the proposed method is a big question. As far as I know, the generalization ability of zero-cost metrics is a bit limited. As the proposed method is based on zero-cost metrics, I think it is necessary to verify the proposed method's generalization ability.\n3. The idea of using a hierarchical design to reduce the space is not novel.\n4. I feel the current experiment design is not a good fit for this paper. I am more interested in seeing experiments on less studied domains instead of these standard benchmark datasets. If the author can show one practical case where their method can clearly outperform conventional NAS, this paper will be much stronger."
                },
                "questions": {
                    "value": "Please check my comments above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6512/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698280492708,
            "cdate": 1698280492708,
            "tmdate": 1699636731294,
            "mdate": 1699636731294,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9wlVKwiAIq",
                "forum": "y3qpL2Ioys",
                "replyto": "jzMtWM6MPH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6512/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6512/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback."
                    },
                    "comment": {
                        "value": "Thank you for dedicating your time and review our manuscript. We appreciate your comments and acknowledge the concerns you have raised, we want to make further clarification as follows:\n\n> The proposed method still heavily depends on human prior knowledge (e.g., reference design, zero-cost metric)\n\nRegarding the reference design, we would like to point out that in our experiments we used cluster centres for that, which are decided automatically and do not require any prior knowledge. Although we discussed the possibility of improving results further if a good reference point was known, we did not rely on that when running our method. Therefore, we believe the reviewer\u2019s point is reaching too far in that aspect.\n\nRegarding zero-cost proxies, we agree we rely on them, which involves certain risks, as mentioned in the limitations. Still, the proxies are a way of probing into the characteristics of neural networks, like #params or #flops; if anything, we would consider our approach of clustering cell designs according to ZC proxies to be rather automated and require very little human input. We strongly disagree with the implied statement that building on top of the existing knowledge should be considered a weakness.\n\nHaving said that, we understand the reviewer is also making a point about the applicability of our method to less-studied tasks. While we disagree with the points above, we do agree with concerns about generalizability, considering our original submission focused on well-studied tasks only. To address this, we have run our method on a suite of tasks from NB360, please find the details in the common reply summarising new experiments.\n\n>The generalization of the proposed method is a big question. \n\nPlease see the results on NB360 in the common reply.\n\n>The idea of using a hierarchical design to reduce the space is not novel.\n\nWe never claim the idea of using a hierarchical design is novel on its own \u2014 if the reviewer got an impression that\u2019s the case, please let us know which part of the paper made them think so and we will revise it. The novelty of the paper lies in designing a hierarchical generative system, which is the first of its kind, to the best of our knowledge.\n\n>I feel the current experiment design is not a good fit for this paper. I am more interested in seeing experiments on less studied domains instead of these standard benchmark datasets. \n\nPlease see the results on NB360 in the common reply."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6512/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725646878,
                "cdate": 1700725646878,
                "tmdate": 1700725646878,
                "mdate": 1700725646878,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uLlpdx0N0u",
            "forum": "y3qpL2Ioys",
            "replyto": "y3qpL2Ioys",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6512/Reviewer_LpBV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6512/Reviewer_LpBV"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies a zero-cost NAS that navigates an extremely large, general-purpose search space to produce network of good quality. The proposed schema considers micro level cell designs at first, then leverages a transformer generator to produce macro architectures for a given task and architectural constraints. Numerical experiments on CIFAR and ImageNet validate the efficacy of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is written well and technically sound. \n- The topic of using generative model into ZC NAS is interesting."
                },
                "weaknesses": {
                    "value": "- The design conflicts the target pain point. The target pain point raised in the introduction is to eliminate the need of designing search space manually. However, this paper still manually designs a search space and develop an algorithm upon it, see page 4. \n\n- The proposed method seems not generic and time-consuming. The proposed methods require a transformer generator to produce macro architectures which seems require time-consuming pretraining, fine-tuning, and may be task and search space specific."
                },
                "questions": {
                    "value": "See the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6512/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6512/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6512/Reviewer_LpBV"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6512/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698639279236,
            "cdate": 1698639279236,
            "tmdate": 1700033282289,
            "mdate": 1700033282289,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZFroLwasRT",
                "forum": "y3qpL2Ioys",
                "replyto": "uLlpdx0N0u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6512/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6512/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback."
                    },
                    "comment": {
                        "value": "Thank you for the time and effort you have invested in reviewing our manuscript. We greatly appreciate your insights and acknowledge the points you have raised. Regarding your concerns, we have provided additional clarification as follows and we are committed to enhancing our experiments and manuscript, hope that the revision will satisfy your concern.\n\n> The design conflicts the target pain point.\n\nWe apologise for not making our point sufficiently clear \u2014 this is a side effect of limited space, so please let us explain our reasoning in more detail.\n\nFirst, it is impossible to have an operational codebase that would be able to train models without defining its ramifications. This means any NAS system requires a search space that is designed manually \u2014 even if this search space is implicit \u2014 because of the need to write functioning code that would be able to execute networks. This is a fundamental issue and we do not see a way around it. We want to make this point clear because the way in which the reviewer raised their point (\u201cthis paper still manually designs a search space and develops an algorithm upon it\u201d) directly touches upon it; there is no way to perform NAS differently than designing a search space and developing an algorithm upon it, any deviations from this rule are likely to be nothing more than semantic nuances (e.g., what exactly is called a search space).\n\nInstead, what is usually understood behind the term \u201cmanually designed\u201d search space is the amount of manual effort that is needed to construct a search space in which good models can be found fast enough. The trade-off between search space size and time needed to search is again fundamental \u2014 we can easily think about a search space of all possible models that can be expressed in, e.g., Pytorch, but searching in such a search space is going to be extremely difficult. Consequently, every NAS method performs some kind of regularisation of this huge search space by considering only its subset, to make the problem tractable. The question becomes: how big can the subset be and if there are any constraining factors. This leaves us with a spectrum, where a method can be considered more \u201cautomated\u201d if it retains strong searching performance without limiting the number or the qualitative properties of models in the search space. We would argue that our method advances in both aspects. On one hand, our search space is extremely expressive, to the best of our knowledge the largest among any works that achieve competitive ImageNet performance. On the other hand, we do not impose constraints such as the ability to construct a supernet (with the size of our search space, we would run out of GPU memory very fast if we tried that), etc. As such, the effort to design our search space is rather minimal \u2014 we simply include as many operations from the literature as possible, expose as many parameters (channels, stride) as possible and only make sure any network can be executed correctly. Considering all that and the fact that we can obtain strong performance fast, we would argue it is justified to say our method is \u201cmore automated\u201d than the existing ones and that it helps minimise manual effort (we will revise our paper to make it clear it is not our goal to completely eliminate manual design, since this cannot be done)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6512/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725514770,
                "cdate": 1700725514770,
                "tmdate": 1700725514770,
                "mdate": 1700725514770,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jzLN5R035C",
            "forum": "y3qpL2Ioys",
            "replyto": "y3qpL2Ioys",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6512/Reviewer_vDUp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6512/Reviewer_vDUp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed efficient hierarchical generative modelling for neural architecture search using zero-cost proxies. NAS as a field often relies strongly on well-designed search spaces. Design of such search spaces is non-trivial especially in new domains and research areas. This paper addresses this disadvantage by  exploiting an extremely large, general-purpose search space efficiently, by training a two-level\nhierarchy of generative models. First level of the conditional generative process focusses on micro-cell design using conditional continuous normalizing flow and the second level uses an transformer to sequentially model the macro architecture. What makes this approach effective in these larger spaces is the ability to exploit task-agnostic zero cost proxy scores to pre-train the generative model. The method is evaluated on the cifar10, cifar100 and the imagenet1-k dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: I find the main contribution of the paper of modelling a 2-level hierarchical search space (which is very expressive) novel and interesting. Furthermore this paper effectively avoids the expensive training cost of generative nas models by relying on zero-cost proxies for pre-training\n\nClarity: I found the presentation clear in most parts (refer to questions for things that are unclear)\n\nSignificance: This paper in my opinion proposes an interesting way to exploit zero cost proxies to search in a large and expressive search spaces. The results in my opinion are competitive and significant"
                },
                "weaknesses": {
                    "value": "- Given the search space design and inability to construct a supernet, fair comparison with effective nas strategies becomes challenging in this case. For example the OFA search space in table 3 is very different from the search space of GraphNet. Hence it becomes difficult to understand if the gains are attributed to search space design itself or the NAS approach. I recommend comparison with black box nas methods on exactly the GraphNet space(eg: regularized evolution [1], hierarchical nas [2])\n\n- The method uses a pretrained GPT-Neo-125M model. Hence the actual search cost also implicitly inclues the pre-training cost of this model, which should ideally be added to the search cost computation (table 1,2,3). \n\n- Study is limited to convolutional spaces. It becomes natural to question the robustness of the proxies to the recent transformer baeed search spaces in NAS\n\n- Performance on Imagenet is still dominated by methods like Once-For-All which models a simple chain structured space  in Table-3 (contrary to the more expressive space here). \n\n- I encourage the authors to release code to foster reproducibility in NAS\n\n[1] Real, E., Aggarwal, A., Huang, Y. and Le, Q.V., 2019, July. Regularized evolution for image classifier architecture search. In Proceedings of the aaai conference on artificial intelligence (Vol. 33, No. 01, pp. 4780-4789).\n\n[2] Schrodi, S., Stoll, D., Ru, B., Sukthanker, R., Brox, T. and Hutter, F., 2022. Towards discovering neural architectures from scratch. arXiv preprint arXiv:2211.01842."
                },
                "questions": {
                    "value": "- Refer points in weaknesses\n\n- Could the search space design and search methodology be extended to transformer spaces like AutoFormer [1] or HAT [2]?\n\n- Could the authors ablate the choice of T-CET as a proxy in macro architecture generation across different proxy choices? How robust/sensitive is the search to different choices of (strong) zero-cost proxies?\n\n- Could the authors ablate the gains from each of the two phases of hierarchical generative modelling? ie fixing the cell and only performing macro search and vice-versa. \n\n- Could the authors study insights derived from NAS method? Which architectural designs tend to be more impactful than the others?\n\n[1] Chen, M., Peng, H., Fu, J. and Ling, H., 2021. Autoformer: Searching transformers for visual recognition. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 12270-12280).\n\n[2] Wang, H., Wu, Z., Liu, Z., Cai, H., Zhu, L., Gan, C. and Han, S., 2020. Hat: Hardware-aware transformers for efficient natural language processing. arXiv preprint arXiv:2005.14187."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6512/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6512/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6512/Reviewer_vDUp"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6512/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698899736901,
            "cdate": 1698899736901,
            "tmdate": 1699636731043,
            "mdate": 1699636731043,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cFqNKqRZJ8",
                "forum": "y3qpL2Ioys",
                "replyto": "jzLN5R035C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6512/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6512/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback."
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable and constructive comments on our work. We hope to have answered all of your questions satisfactorily below. Please let us know if you see any further issues in the paper that are unclear or need to be addressed. \n\n> I recommend comparison with black box nas methods on exactly the GraphNet space(eg: regularized evolution [1], hierarchical nas [2])\n\n\nWe have reported the performance of regularized evolution with T-CET in our GraphNet space in Tab. 2 (c.f. Evo(T-CET)). As suggested by the reviewer, we ran additional experiments using regularized evolution with a reduced training scheme (50 epochs training, same as NAS-Bench-201). We refer to this new baseline as Evo(reduced-training) in the following. Under a similar search budget with Evo(T-CET) (~20 GPU hours), we were able to train ~40 models using Evo(reduced-training), and the results are shown in the table below. \n\nTo further demonstrate the effectiveness of our approach, in addition to running black box NAS methods directly on the GraphNet space, we also conducted experiments where we run Evo(reduced-training) on an optimised search space produced by our method (SG+CCNF+Evo(reduced-training) in the following table), under a similar search budget (~20 GPU hours). In this setting, we first use our sequence generator (SG) to generate the macro structure while for each cell we use CCNF to propose 5 candidates. We then construct a search space for running Evo(reduced-training) by considering any combination of the 5 generated cells (note: the macro models used 4 cells in this case). In total, the optimised search space is of size 5^4.\n\nWe observe that under a similar search budget, regardless of requiring training or not, regularized evolution, i.e. both Evo(reduced-training) and Evo(T-CET), struggle to find good performing models on the large GraphNet space. On the other hand, when equipped with our hierarchical generative approach, the optimized search space significantly bolsters performance of both the standard and the zero-cost evolution.\nInterestingly, SG+CCNF+ZCNAS achieved a bit higher result in this experiment, compared to running reduced training in the optimized space. This should not be a surprise, however, if we recall that reduced training is known to be a somewhat bad proxy, and T-CET is known to be especially good for datasets like CIFAR-10. Still, reduced training did reasonably well, and much better than when running in a full space; we include it for the sake of completeness. We also suspect it might be a better choice for tasks on which T-CET might fail.\n\n\n| Methods             | CIFAR-10(Acc %) |\n|---------------------|-----------------|\n| Evo(reduced-training)    | 90.32           |\n| Evo(T-CET)          | 96.4            |\n| SG+CCNF+Evo(reduced-training) | 97.3            |\n| SG+CCNF+ZCNAS\t| 97.6\t|\n\n\n> The method uses a pretrained GPT-Neo-125M model. Hence the actual search cost also implicitly inclues the pre-training cost of this model, which should ideally be added to the search cost computation (table 1,2,3).\n\nWe agree there is a hidden cost in needing a pre-trained GPT model - we will include a note about that in the paper for the sake of completeness (note that the authors of GPT-Neo did not disclose the time needed to train their models, so we will rely on how costly it is to train similar ones). However, we are not sure if a straightforward attribution of this cost to our method is completely fair. GPT models are trained to perform NLP tasks and there is no clear relation between those and NAS. This is in contrast to some NAS methods that might require starting from, e.g., an open-sourced pretrained supernet - in this case, the connection to performing NAS is clear and including supernet pretraining cost is justified. The point is: if we consider GPT training cost to be part of the search cost, it gives a false impression of having to pay it each time we search for a new model (which is clearly not the case); even if we consider it a pre-cost, it gives a false impression that it has to be paid each time we want to extend/change our NAS settings (such as including more models in the search space, etc.) - again, this is not the case. As such, we believe it should be classified as \u201cpre-pre-cost\u201d, but there is no clear consensus within the NAS community on how (or even if) such cost should be reported, as far as we know. In particular, many existing methods might have paid analogous cost (i.e., with similar level of indirection) but did think about reporting it. Of course, because of that we are happy to engage in discussion with the reviewer to work out the best way of doing so - please let us know your thoughts about our arguments and the current way in which we include the cost in our paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6512/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725298964,
                "cdate": 1700725298964,
                "tmdate": 1700725298964,
                "mdate": 1700725298964,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]