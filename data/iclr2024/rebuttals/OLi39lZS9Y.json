[
    {
        "title": "Learning to Solve New sequential decision-making Tasks with In-Context Learning"
    },
    {
        "review": {
            "id": "En8pGEamQd",
            "forum": "OLi39lZS9Y",
            "replyto": "OLi39lZS9Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6225/Reviewer_CT2T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6225/Reviewer_CT2T"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of in-context learning in sequential decision-making settings. The paper finds that it is important for the context to contain full trajectories to cover potential situations at deployment time. The authors provide experiments on MiniHack and Procgen benchmarks, showing the method can generalize to new tasks with just a few expert demonstrations, without weight updates. The work claims to be the first to demonstrate that transformers can generalize to entirely new tasks in these benchmarks using in-context learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well written and easy to follow.\n- The paper shows some nice experiments on the MiniHack and Procgen environments, showing how in-context learning can perform well on unseen tasks.\n- The paper's setting of in-context learning in decision-making problems is an interesting problem to study."
                },
                "weaknesses": {
                    "value": "- The paper highlights the ability to perform well on unseen tasks but this actually relies on having a lot of demos from related tasks. Can the authors better clarify the relationship between the data they train on and the unseen tasks they evaluate on? \n- Novelty-wise, the method is very similar to works like Prompt-DT, except actually requires stronger data assumptions (full expert demos). \n- A lot of the insights in the empirical study are not that interesting, e.g. the paper highlights results like showing that in-context learning improves with trajectory burstiness, but it is not surprising that having demos similar to the query inside the context improves the performance. Can the authors give more clarity on what the most surprising, interesting takeaways are from the study?"
                },
                "questions": {
                    "value": "See weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6225/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698716369786,
            "cdate": 1698716369786,
            "tmdate": 1699636679899,
            "mdate": 1699636679899,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DWnnu3knuq",
                "forum": "OLi39lZS9Y",
                "replyto": "En8pGEamQd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6225/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6225/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comments. In this response we will respond to all the comments and questions the reviewer raised. \n1. **Relying on a lot of expert demonstrations** - Note that we use only 7 expert demonstrations for each level at test time which we\u2019d argue is not a large number for learning (as echoed by reviewer qktf) entirely new tasks with unseen states, actions, dynamics, and rewards. \n2. **Relationship between the train data and test data** - We ask the reviewer to look at the common response where we clearly explained the difference between the train and test tasks.\n3. **Comparison with Prompt DT** -  Please refer to our common response where we explain in greater detail about this.\n4. **Surprising aspect of our work** - Our paper is **first to demonstrate few-shot learning of new Procgen and MiniHack tasks from only a handful of demonstrations, without any online feedback, after training on a relatively small number of other tasks from the same domain (11 in the case of procgen and 12 in the case on MH)**. \nWe believe this result is by no means trivial (and it surprised us) given that prior work was either only able to generalize to minor task variations (slightly different reward functions but same states, actions and dynamics, as in PromptDT) or required orders of magnitude more compute and training environments to generalize to more challenging tasks (as in Ada which trains for 100B timesteps on a task pool size of 25B, however at that point it\u2019s not clear test tasks are OOD from train tasks, whereas in our case they are clearly OOD). Note that the train and test tasks are vastly different having different states, actions, dynamics, and reward functions. While we agree the approach is simple we believe this is in fact a strength of our work as it should be easy to adapt, apply to other settings, scale, and build upon by the community. \nOur paper performs an extensive analysis of different design choices and implementation details demonstrating their importance in getting the method to work. We believe these insights will be of interest to researchers and practitioners alike. In particular, we demonstrate a minimum model size, burstiness, data size and diversity are needed to obtain strong results. \nIn addition, section 5.2 discusses in depth the limitations of this approach and the fact that it doesn\u2019t work as well in certain settings with high stochasticity or high-stakes scenarios where a single wrong action can be fatal. Again, this supports the point that it is by no means \u201ca given\u201d that this approach works. In fact we believe there is a need for more research in this direction to develop methods that are robust to highly stochastic and unforgiving environments. It\u2019s possible that offline learning and transformers are not even the best solution to these settings. All of this is to say that the results are much more nuanced and we try to convey this in our paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6225/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172465250,
                "cdate": 1700172465250,
                "tmdate": 1700172465250,
                "mdate": 1700172465250,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FgkO08RjAV",
            "forum": "OLi39lZS9Y",
            "replyto": "OLi39lZS9Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6225/Reviewer_F4ph"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6225/Reviewer_F4ph"
            ],
            "content": {
                "summary": {
                    "value": "This work targets the setting of zero-shot and few-shot learning, where the train and test MDPs contain completely separate games (tasks) - in contrast to previous works where the train and test sets contain the same game but with different levels.  The paper proposes to adapt the causal transformer model to this few-shot setting by first training expert agents on each of the training tasks, and then collecting a dataset from the experts\u2019 trajectories to train the transformer model. The authors propose to train the transformer using multi-trajectory sequences rather than single-trajectory sequences and to construct the multi-trajectory training so that the context contains at least one trajectory from the same level as the query. At test time, for the few-shot setting, the transformer is conditioned on 1-7 full expert trajectories, while for the zero-shot setting the transformer is not conditioned on any expert trajectories.\n\nThe authors compare their results to two baselines: BC and hashmap, and performed an extensive ablation study containing the dataset sizes, task diversity, environment stochasticity, and trajectory burstiness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The paper suggests a new setting that has not been studied before - to test on games withheld during training by utilizing expert policies that were trained on the training set (a separate set of games).  \n\n* The results show a clear advantage to the proposed approach over the baselines and the authors performed an extensive ablation study."
                },
                "weaknesses": {
                    "value": "* The proposed approach seems as a small adaptation of pre-existing approaches, i.e. causal transformer with multi-trajectory training, to new benchmarks (MiniHack and Procgen) in the offline setting.\n\n* There is no comparison to other offline methods such as CQL [1]\n\n* It is mentioned in the paper that all the results are produced using 3 seeds. In my opinion, for such noisy benchmarks evaluating on only 3 seeds is not enough to reliably estimate the mean and variance. \n\n* The results are not clear to me - for example, in Figure 4 the episodic return is very low compared to the score reported by [2]."
                },
                "questions": {
                    "value": "I would like to ask the author to address the following questions: \n\n1. For the few-shot evaluation (when testing the model): is the expert policy, which creates the few trajectories (1-7) for conditioning the transformer, trained on the test games or the training games? \n2. Are the above few trajectories (1-7) sampled from the same level as the query level? \n3. Why is the return in Figure 4 so low compared to the return reported in [2]?\n4. Is the Procgen dataset evaluated on the easy or hard difficulty mode?\n5. Are the results in Figure 3 normalized?\n\n\nA technical detail: \n* In the first paragraph of the background - /mu the initial state distribution is not defined.\n\n\n\n\n\n[1] Kumar, Aviral, et al. \"Conservative q-learning for offline reinforcement learning.\" Advances in Neural Information Processing Systems (2020): 1179-1191.\n\n[2] Cobbe, Karl, et al. \"Leveraging procedural generation to benchmark reinforcement learning.\" International conference on machine learning. PMLR, 2020"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6225/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6225/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6225/Reviewer_F4ph"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6225/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763113242,
            "cdate": 1698763113242,
            "tmdate": 1699636679770,
            "mdate": 1699636679770,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S5YE8molQE",
                "forum": "OLi39lZS9Y",
                "replyto": "FgkO08RjAV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6225/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6225/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time. Here are the responses to the questions -\n\n1. **Novelty** - We wrote a detailed comment about the novelty aspect of our work and how it is different from previous works. Could you please point us to the works, apart from the previous works considered in the common response, which are seemingly similar to the problem setting we consider? \n2. **Comparison with CQL** - We refer the reviewer to look at the common response where we do comparison of our method with all the related previous works including CQL. \n3. **Number of seeds** - We also ran 5 seed experiments for procgen experiments and we didn\u2019t notice any big differences in the conclusions. We will update the appendix with 5 seed results. \n4. **Comparing with Procgen[2]**  -  The results in the original Procgen paper are for a completely different setting than ours so this is not a fair comparison. **In the Procgen paper, agents are trained with PPO on multiple levels of a single game (e.g. Ninja) and tested on unseen levels from the same game (i.e. also Ninja), thus probing IID generalization In our paper, agents are trained offline on data from multiple tasks (e.g. Ninja) and tested on entirely new tasks at test time (e.g. Jumper) thus probing OOD generalization which is more challenging. ** Thus, the lower performance in our paper is expected. \n5. **Clarity about test demonstrations**  -  The expert demonstrations used at test time come from an expert policy trained on that particular environment. For example, when evaluating our transformer model on Ninja task, we use an expert PPO policy trained only on Ninja to collect expert trajectories and condition on those demonstrations. \n6. **Few-shot trajectories sampled from the same level?** Yes, the trajectories are from the same level we are testing on. Note that, this is a challenging setting (look at common response for more details) where test tasks are OOD. Moreover, the environments are stochastic which adds more challenges. Hence the model might still struggle to generalize to OOD tasks just from a few demonstrations.  We will clarify this in the paper. \n7. **Procgen task difficulty** - We consider \u201ceasy\u201d difficulty mode in our experiments.\n8. **Normalized scores** - No, we do not normalize anything and we report the raw scores for both MiniHack and Procgen."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6225/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172345930,
                "cdate": 1700172345930,
                "tmdate": 1700172345930,
                "mdate": 1700172345930,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KAgDPWn7gm",
                "forum": "OLi39lZS9Y",
                "replyto": "ezFhhG2DrK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6225/Reviewer_F4ph"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6225/Reviewer_F4ph"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their answers and clarifications. Some of my concerns were clearly addressed. However, I still have reservations about the quality of the evaluations and the presented results. Especially regarding limited baseline comparisons. \nAlthough the setting presented in the paper is indeed different from the setting that existing works (Prompt DT, CQL, Algorithmic Distillation, etc.) are targeting and were designed for, the proposed algorithm relies heavily on offline datasets (sampled from experts that were trained on different games). Therefore, comparing the proposed method to the performance of a leading offline baseline (trained in the offline setting, only on the training games) would highlight the performance benefit of the proposed few-shot adaptation phase. This will contribute to the understanding of the importance of the different components of the algorithms. \n\nIn addition, training on only 3 seeds is not enough to draw definitive conclusions. I would urge the author to show results trained on more seeds (or at least a subset of the results). \nFor these reasons, I\u2019ll keep my score unchanged for now."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6225/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684208248,
                "cdate": 1700684208248,
                "tmdate": 1700684208248,
                "mdate": 1700684208248,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QaEs9P1mWv",
            "forum": "OLi39lZS9Y",
            "replyto": "OLi39lZS9Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6225/Reviewer_phhR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6225/Reviewer_phhR"
            ],
            "content": {
                "summary": {
                    "value": "This work studies the use of transformers for generalization to new tasks from MiniHack and Procgen. Their experiments show that a model pre-trained on different levels and tasks of Procgen can learn a new task from a few demonstrations of the task. The transformer is trained with demonstration contexts that can either be from the same or different levels as the query. There is also additional empirical analysis on different variables, such as whether the context is from the same level, environment stochasticity, and task diversity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This work shows that transformers can perform few-shot imitation learning on new Procgen tasks, which has not been explicitly shown previously.\n\n- The paper is written pretty well at a low-level and tries to be thorough in its experiments through additional experiments to understand failure modes and effect of different environmental and algorithmic factors.\n\n- Given the significance of in-context learning in large language models, it seems timely and appropriate to study it in the context of decision-making.\n\n- The environment stochasticity result is pretty interesting, i.e., that the model can learn copying behavior if the training environments are deterministic."
                },
                "weaknesses": {
                    "value": "- Existing papers such as Prompt-DT (Xu et al, 2022) and AdA (Team et al, 2023) have shown similar results (in some cases, with even less presumptive data than full demonstrations) though in different domains. So it's perhaps not too surprising that we see this type of generalization in Procgen as a result. The main result that models trained with demonstrations in the context can perform better than models without the demo context is also expected.\n\n- There are a couple of claims that do not seem sufficiently supported: (1) meta-RL methods \"tend to be difficult to use in practice and require more than a handful of demonstrations or extensive fine-tuning,\" (2) \"in sequential decision-making it is crucial for the context to contain full trajectories (or sequences of predictions) to cover the potentially wide range of states the agent may find itself in at deployment\" (see Questions). \n\n- The concepts of burstiness from Chan et al and trajectory burstiness have a pretty weak relation. In the case of this paper, it seems pretty clear from the get-go that demonstration contexts from the same level as the query would be more relevant than from any other levels."
                },
                "questions": {
                    "value": "- What does trajectory burstiness mean for the zero-shot model in Fig. 6(a)?\n\n- \"[Meta-RL methods] tend to be difficult to use in practice and require more than a handful of demonstrations or extensive fine-tuning\" --> Including some of these comparisons in the experiments would be help support this statement.\n\n- \"Our key finding is that in contrast to (self-)supervised learning where the context can simply contain a few different examples (or predictions), in sequential decision-making it is crucial for the context to contain full trajectories (or sequences of predictions)\nto cover the potentially wide range of states the agent may find itself in at deployment.\" --> Full trajectories as opposed to what? The experiments only show comparisons between full demos vs no demos, and didn't study other potential contexts, such as partial demos or non-expert trajectories. I think a study of different potential contexts and what is required for in-context learning would be interesting.\n\n- \"This means that the agent manages to perform well on the new task even without copying actions from its context. This suggests the model is leveraging information stored in its weights during training, also referred to as in-weights learning\" --> Could you elaborate on how not copying the context actions suggests in-weights learning as opposed to ICL? This conclusion seems to equate ICL with the ability to copy context actions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6225/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818527281,
            "cdate": 1698818527281,
            "tmdate": 1699636679654,
            "mdate": 1699636679654,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "viXZbtdO7n",
                "forum": "OLi39lZS9Y",
                "replyto": "QaEs9P1mWv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6225/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6225/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for thoughtful comments. We appreciate the reviewers comments on strengths of our work. In this response, we will address the questions you raised in your review. \n\n1. **PrompDT and AdA**: We would like the reviewer to refer to the common response where we contrast our method with prior work. We differ from PromptDT paper in terms of many aspects including multiple environment training, scale of our models, OOD evaluations etc. With regards to AdA, we would like to highlight that the setting is different from our work. AdA considers online RL setting with transformers in contrast with offline few-shot imitation learning setting, and requires orders of magnitude more compute and training environments to generalize to more challenging tasks (100B timesteps on a task pool size of 25B). Regarding surprising aspects of the results, we want to emphasize again that the setting we consider in this paper is very challenging and to the best of our knowledge this setting hasn\u2019t been explored in prior work.\n2. **Meta RL** - The problem setting considered in our paper differs from that of Meta-RL, where training is online or offline and, at test time, the agent learns from online interactions and feedback (i.e., rewards) within a new environment. In contrast, our paper focuses on the few-shot imitation learning setting, where training occurs offline based on expert demonstrations from a set of tasks, and at test time, the agent learns a new task from only a handful of offline demonstrations without interactions or feedback in the new environment. Therefore, while Meta-RL is loosely related \u2013 a fact we acknowledge in the related work section \u2013 these approaches are not directly comparable, as they operate under vastly different assumptions. However, following your suggestion, we will rephrase that sentence to ensure our claims are well supported by our experiments.\n3. **Copying Actions** - Thank you for pointing this out. Recent works in ICL, such as 'Incontext Learning and Induction Heads'[1] by Anthropic, suggest that 'induction heads' are a key feature in transformer models for ICL. Induction head is  a circuit whose function is to look back over the sequence for previous instances of the current token (call it A ), find the token that came after it last time (call it B ), and then predict that the same completion will occur again (e.g. forming the sequence [A][B] \u2026 [A] \u2192 [B] ). This is what we mean by copying in our context. We understand that the term 'copying' is used loosely in the paper which caused this confusion. We will rephrase our sentence structure to reflect this understanding, including relevant citations.\n4. **Partial Demonstrations** - We want to highlight that we consider partial demonstrations in our procgen experiments because of the long episode lengths in some of the procgen tasks. \n5. **What does burstiness 0.0 mean?** Burstiness 0.0 means that in the entire dataset, there is a zero probability of finding the sequences which are bursty. \n6. **Comparison to burstiness in Chan et al.** We think the burstiness in chan et al and trajectory burstiness are closely connected. Chan et al defines a bursty sequence as a sequence where there are multiple occurrences of the same classes which are similar to query (Figure 1 in [1]). Similarly, in our work, a trajectory bursty sequence is a sequence where there are multiple trajectories which are from the same level as the query."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6225/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172237247,
                "cdate": 1700172237247,
                "tmdate": 1700172237247,
                "mdate": 1700172237247,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hwOmf3eVtI",
            "forum": "OLi39lZS9Y",
            "replyto": "OLi39lZS9Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6225/Reviewer_qktf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6225/Reviewer_qktf"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of in-context learning for decision-making. A method of training transformers is proposed where expert demonstrations are generated across many tasks and the model is expected to predict expert behavior from this context. The method is demonstrated on procgen and nethack, two challenging RL settings.  It is shown that in-context learning can be achieved from a handful of demonstrations in order to generalize to new test tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problem is important and interesting: the in-context abilities of transformers for decision-making problems is comparatively understudied relative to supervised learning problems. This paper contributes to a growing understanding of decision-making with transformers.\n\nThe generalization of the method to entirely new tasks in procgen and nethack is impressive as these are challenging settings and each task is quite different from the others. There are really only a handful of training tasks.\n\nThe analytical studies are thorough and mostly informative, especially the one showing how the performance varies with the number of training tasks and the one on failure modes."
                },
                "weaknesses": {
                    "value": "Overall, I think this is a good paper with a thorough analysis, but there are two main weaknesses of the paper: clarity and novelty/significance.\n\nClarity: both the problem setting and the methodology of the training are not very clear and this makes it difficult to understand the significance of the results. \n\n- During testing, the agent is given a handful of expert demonstrations. Are these all demonstrations on the same task and same level? If not, how does this work for the baselines hashmap if they are using demos from different levels? If so, why is the transformers trained with several sequences of demos from different levels? Why not just train with demonstrations from the same level and task always?\n- Related to this, how do I interpret this, which suggests that all demos in the context come from the same level: \u201cwe collect offline data from 11 Procgen tasks and train a transformer on Procgen sequences compromising of five episodes from the same level.\u201d What does burstiness even mean here if the levels are never varied?\n- What does it mean for BC-1 to condition on \u2018one demonstration\u2019? Does this mean you give it full demonstration in the same task and same level? In other words, does the context look like this: [expert demo, history observed so far]. How would this be different from your method if you were just limited to training on just two sequences? \n- What is the maximal achievable reward in each of the environments? This could be helpful to better understand the final results.\n\nIt would further be helpful to distinguish the work from prior methods better. A more thorough comparison would help readers with a better understanding of the present problem setting and method.\n\n- The method appears to be very similar to Prompt-DT [1] perhaps without the return conditioning. There\u2019s already a short discussion in the related but this ought to be carefully dissected, I think.\n- The method is also very similar to DPT [2], which also considered training and conditioning on expert demonstrations to solve new tasks. If there is a difference, both of these papers seem like highly relevant baselines.\n- It is also likely worth distinguishing the method with other in-context RL works like [3] and [4].\n\nBeyond transformers, there\u2019s additional work on meta/few-shot imitation learning that could be helpful to discuss.\n\nAs a result, the overall takeaways are a bit hard to discern. It\u2019s clear now that there are multiple solid contributions in this paper (a method and a thorough analysis), but I think the takeaways could be better communicated.\n\n[1] Xu M et al. Prompting decision transformer for few-shot policy generalization. International conference on machine learning 2022.\n\n[2] Lee JN et al. Supervised Pretraining Can Learn In-Context Reinforcement Learning. arXiv preprint arXiv:2306.14892. 2023.\n\n[3] Lu C et al. Structured state space models for in-context reinforcement learning. arXiv preprint arXiv:2303.03982. 2023.\n\n[4] Laskin M et al. In-context reinforcement learning with algorithm distillation. arXiv preprint arXiv:2210.14215. 2022."
                },
                "questions": {
                    "value": "See above section for specific questions. Misc:\n\n- How long are each of these sequences? I.e. what is T?\n- In what settings would you expect this method would work (or these analysis be useful) under the current assumptions, beyond gameplaying?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6225/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699237624820,
            "cdate": 1699237624820,
            "tmdate": 1699636679552,
            "mdate": 1699636679552,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jMIYYEUW9C",
                "forum": "OLi39lZS9Y",
                "replyto": "hwOmf3eVtI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6225/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6225/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response [Part 1/2]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful comments and feedback. We appreciate the reviewer acknowledging the challenges posed by our setting, our thorough experimental analysis and discussion on limitations of our work.  In this response, we hope to successfully answer your remaining concerns.\n\n1. **Similarities with PromptDT** - We wrote a detailed response to this concern in the \u201ccommon response\u201d section above where we contrast our work with  all the papers mentioned in your review. We encourage you to read the response and let us know if you have any additional questions. \n2. **Additional discussion on Related works** - We agree that additional discussion about recent works would be beneficial. We will add the works you mentioned in your review and extend our related works section in the appendix. \n3. Regarding Evaluation\n    * **Few-shot evaluation** - During few-shot evaluation, the model is given access to a limited number of expert demonstrations from the task on which it is being evaluated. For instance, when assessing performance on a Ninja task from Procgen, we condition our model on a few expert demonstrations of the Ninja task, specifically from the same level on which it will be tested. We then unroll the policy of the transformer in the online environment and measure the obtained reward. This itself is a challenging evaluation because the model never saw the \u201cNinja\u201d task during the training, hence OOD, and learning from just a handful of demonstrations in this scenario could be very challenging.\n    * **Hashmap evaluation** - Hashmap baseline evaluation is similar to that of the model. Again, taking the example of the \u201cNinja\u201d task, we hash a few expert demonstrations coming from the same level on which it will be tested. This baseline suffers if the environment is stochastic but could act as an oracle in case the environment is deterministic. Finally, we use the same demonstrations for all the baselines and our method for fair comparison. \n    * **Training** - When the trajectory burstiness probability is 1.0 (which is the case for figure 3 and 4), in each sequence the demonstrations come from the same level of a given task. So each sequence in the training set looks like this [ demonstration 1 from task A level A, demonstration 2 from task A level A, demonstration 3 from task A level A, demonstration 4 from task A level A \u2026.]. In this work, we have 10k levels per  task and 11 training tasks in case of procgen resulting in 11 * 10k = 110K training sequences.\n4. **Sequence construction** - If all or at least two demonstrations are coming from the same level in a given sequence, then the sequence is called a bursty sequence with trajectory burstiness probability 1.0. This means that the relevant information required to solve the task is entirely in the sequence hence this design aids in-context learning. However, note that the trajectories, despite being from the same level, may vary because of the inherent environment stochasticity which means that the model still needs to generalize to some extent  to handle these cases. \n5. **Behavioral Cloning (BC-1) baseline** - Yes, BC-1 means during the *evaluation time* (and *not* during the training), we condition the model with one expert demonstration. This is different from our method because during training BC-1 and BC-0 are trained with a single trajectory sequence and they only differ just in the way they are evaluated. The sequence construction looks exactly like you rightly mentioned. The aim of BC-1 baseline is to check if the model is ever making use of the expert demonstration info during the test time. However, this evaluation could be somewhat OOD for the model to generalize which is reflected in the results. \n6. **Upper bound of performance** - We will add these numbers in the appendix. However, we do want to highlight that, to our knowledge, there are not so many works which consider the setting we consider in our paper so it is hard to benchmark our current results. Ideally we want to move our numbers close to that of an oracle which is obtained by doing online RL from scratch in the test environment and these are already reported in [1]."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6225/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172045818,
                "cdate": 1700172045818,
                "tmdate": 1700172045818,
                "mdate": 1700172045818,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]