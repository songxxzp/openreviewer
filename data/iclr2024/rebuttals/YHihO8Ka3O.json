[
    {
        "title": "FedGT: Federated Node Classification with Scalable Graph Transformer"
    },
    {
        "review": {
            "id": "QvKCdZxkWj",
            "forum": "YHihO8Ka3O",
            "replyto": "YHihO8Ka3O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3964/Reviewer_nEXk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3964/Reviewer_nEXk"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a scalable Federated Graph Transformer\n(FedGT) to address the data heterogeneity and missing link challenges. In contrast to GNNs that follow message-passing schemes and focus on local neighborhoods, Graph Transformer has a global receptive field to learn long-range dependencies and is, therefore, more robust to missing links. Moreover, a novel personalized aggregation scheme is proposed. Extensive experiments show the advantages of FedGT over baselines in 6 datasets and 2 subgraph settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.The paper is well-written and organized. The details of the models are described clearly and are convincing.\n2.The limitations of applying GNNs for subgraph federated learning are clearly illustrated in Figure 1 and Figure 4 in appendix. The motivation for leveraging graph transformers is easy to understand.\n3.The authors proposed a series of effective modules to tackle the challenges, including scalable graph transformers, personalized aggregation, and global nodes. The contribution is significant enough.\n4.FedGT is compared with a series of SOTA baselines, including personalized FL methods, federated graph learning methods, and adapted graph transformers. Extensive experiments on 6 datasets and 2 subgraph settings demonstrate\nthat FedGT can achieve state-of-the-art performance."
                },
                "weaknesses": {
                    "value": "1.The authors are suggested to clearly discuss the case studies in the main paper.\n2.Leveraging local differential privacy mechanisms to protect privacy in FL is not new.\n3.Please provide more explanations of the assumptions in Theorem 1."
                },
                "questions": {
                    "value": "1.Can the authors introduce more about the roles of global nodes in FedGT?\n2.Is FedGT applicable to other subgraph settings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3964/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698164170169,
            "cdate": 1698164170169,
            "tmdate": 1699636357603,
            "mdate": 1699636357603,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nMfzuKS4oN",
                "forum": "YHihO8Ka3O",
                "replyto": "QvKCdZxkWj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nEXk"
                    },
                    "comment": {
                        "value": "Thanks for your detailed comments and appreciation!  \n**Q1**: The authors are suggested to clearly discuss the case studies in the main paper.  \n**R1**: Thanks for the suggestion! Due to the limit of the main paper, we provided some case studies such as the t-SNE visualizations of node embeddings and global nodes in Figure 9. We also provided more experimental results in Appendix. E. We will bring the case studies and more experiment results to the main paper in the final version. \n\n**Q2**: Leveraging local differential privacy mechanisms to protect privacy in FL is not new.  \n**R2**: Local differential privacy is not our main contribution in FedGT. Following previous works [1,2], local differential privacy is added to further protect privacy. Compared with previous works on subgraph federated learning (e.g., the baseline methods we compare in our paper), FedGT has comparable or better privacy protection levels.\n\n[1] Chuhan Wu, Fangzhao Wu, Yang Cao, Yongfeng Huang, and Xing Xie. Fedgnn: Federated graph\nneural network for privacy-preserving recommendation. KDD, 2021  \n[2] Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang, and Xing Xie. Privacy-preserving news\nrecommendation model learning. EMNLP, 2020.  \n\n**Q3**: Please provide more explanations of the assumptions in Theorem 1.  \n**R3**: The assumptions in Theorem 1 are mainly two-fold: (1) Firstly, we assume the attention score function is Lipschitz continuous with constant C. We follow this assumption used in previous works e.g., [3]. Moreover, as indicated by [4], the self-attention is Lipschitz if the inputs are bounded and compact. In the data preprocessing, the input node features are normalized. Therefore, the assumption is satisfied in experiments; (2) Secondly, we assume nodes are equally distributed to the global node. In our experiments, we observe that the clustering in FedGT is not sensitive to graph partitioning and the nodes generally distribute evenly to the global nodes. For example, we probed the distribution of nodes to 10 global nodes in a client on the Cora dataset. There are a total 247 nodes in the client and the number of assigned nodes to global nodes is (27, 28, 22, 24, 25, 22, 21, 24, 25, 29), which is quite close to the uniform distribution. To further satisfy the assumption, we can use balanced clustering that ensures nodes are equally distributed to the global nodes. Therefore, these assumptions are reasonable for FedGT.\n\n[3]Kezhi Kong et al., GOAT: A Global Transformer on Large-scale Graphs, ICML 2023  \n[4]Kim, Hyunjik et al., The lipschitz constant of self-attention. ICML, 2021.  \n\n**Q4**: Can the authors introduce more about the roles of global nodes in FedGT  \n**R4**: The global nodes play critical roles in FedGT. Firstly, in the hybrid attention, each node attends to the sampled neighbors in the local subgraph and a set of global nodes representing the global context. As discussed in Sec.4.1 and the experiments, global nodes can supplement the missing information of missing links. Secondly, the personalized aggregation scheme calculates the similarity scores based on global nodes from different clients because global nodes can reflect the corresponding data distribution of local subgraphs. Therefore, global nodes serve as the nexus between hybrid attention and personalized aggregation.\n\nIn FedGT, the global nodes are dynamically updated with an online clustering algorithm (Algorithm 1). The global nodes can be regarded as the cluster centroids of node representations and reflect the overall data distribution of the local subgraph.\n\n**Q5**: Is FedGT applicable to other subgraph settings  \n**R5**: Thanks for the questions! In FedGT, we mainly follow the setting of previous works such as [1]. In the future, we may explore other settings such as clients with varying numbers of nodes. Due to the limit time of rebuttal, we will incorporate systematic studies in the future version."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152403594,
                "cdate": 1700152403594,
                "tmdate": 1700152403594,
                "mdate": 1700152403594,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WgAoaSyH0v",
                "forum": "YHihO8Ka3O",
                "replyto": "nMfzuKS4oN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3964/Reviewer_nEXk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3964/Reviewer_nEXk"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thanks the authors for the detailed rebuttal. Most of my concerns are well addressed. As noted by all the reviewers, the idea of leveraging graph transformer architecture for federated graph learning is both novel and well-motivated. Therefore, I recommend accepting this paper, which will be an important basline for federated graph learning area."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703320753,
                "cdate": 1700703320753,
                "tmdate": 1700703320753,
                "mdate": 1700703320753,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HG76TReo2z",
            "forum": "YHihO8Ka3O",
            "replyto": "YHihO8Ka3O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3964/Reviewer_uFDb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3964/Reviewer_uFDb"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a scalable Federated Graph Transformer (FedGT) for subgraph federated learning, which addresses the challenges of missing links between subgraphs and subgraph heterogeneity. It uses a hybrid attention scheme to reduce complexity while ensuring a global receptive field and computes clients\u2019 similarity for personalized aggregation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper is easy to read, and generally well-written.\n2.\tThe idea of using the Graph Transformer to address the issue of missing links across clients is well-motivated."
                },
                "weaknesses": {
                    "value": "1.\tHow to aggregate global nodes is not clearly illustrated. On page 6, the authors state, \u201cthe global nodes are first aligned with optimal transport and then averaged similar to Equation 8\u201d. However, it is unclear which optimal transport method is applied and how the similarity between global nodes from different clients is calculated. The authors should clarify whether the normalized similarity \u03b1_ij used for model parameters is also employed for global nodes or if a different similarity calculation is used. Besides, in Algorithm 3 lines 11 and 13, the aligning process for the global nodes seems to be performed twice, which needs a clearer explanation.\n\n2.\tSince the weighted averaging of local models, i.e., Equation (8), is the same in [1], the authors should provide a discussion or experiment to explain why their similarity calculation is superior to that in [1].\n\n3.\tTo show the convergence rate, Figure 5 and Figure 6 did not contain FED-PUB, which is the runner-up baseline in most cases. \n\n4.\tIn the ablation study, the authors only conduct experiments on w/o global attention and w/o personalized aggregation. Results of w/o the complete Graph Transformer (i.e., without local attention) should also be provided.\n[1] Baek J, Jeong W, Jin J, et al. Personalized subgraph federated learning[C]//International Conference on Machine Learning. PMLR, 2023: 1396-1415."
                },
                "questions": {
                    "value": "1.\tThe authors opt for a consistent number of global nodes n_g across all clients. However, how does the methodology account for scenarios in which clients have a varying number of nodes, with some having significantly more and others noticeably fewer? Is there a suggested approach for determining varying n_g values that are customized to each client\u2019s node count?\n\n2.\tIn the typical federated learning framework, the number of training samples is considered when aggregating the model parameters. However, Equation (8) only uses the normalized similarity for the weighted aggregation. Why can we ignore the number of training samples here? Or do we assume the number of training samples is equivalent across clients?\n\n3.\tThe Hungarian algorithm only finds a bijective mapping while optimal transport can be generalized to many-to-many cases, could the authors explain the reason for making a one-to-one alignment of the global nodes?\n\n4.\tSince the global nodes are dynamically updated during the training, and the representations of the nodes are not stable at the beginning of the training, would this impact the effectiveness of similarity calculation based on the global nodes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3964/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3964/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3964/Reviewer_uFDb"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3964/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698645675259,
            "cdate": 1698645675259,
            "tmdate": 1700640616530,
            "mdate": 1700640616530,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YGrzbX8I7i",
                "forum": "YHihO8Ka3O",
                "replyto": "HG76TReo2z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uFDb"
                    },
                    "comment": {
                        "value": "Thanks for your valuable questions and suggestions!  \n**Q1**: How to aggregate global nodes is not clearly illustrated. On page 6, the authors state, \u201cthe global nodes are first aligned with optimal transport and then averaged similar to Equation 8\u201d. However, it is unclear which optimal transport method is applied and how the similarity between global nodes from different clients is calculated. The authors should clarify whether the normalized similarity \u03b1_ij used for model parameters is also employed for global nodes or if a different similarity calculation is used. \nBesides, in Algorithm 3 lines 11 and 13, the aligning process for the global nodes seems to be performed twice, which needs a clearer explanation.\n\n**R1**: Thanks for the question. For aggregating the global nodes, the optimal transport algorithm in Equation 7 is used to align the global nodes and calculate similarity. The normalized similarity \u03b1_ij used for model parameters is also employed for global nodes.\nWe will state this more clearly in our revised paper. \n\nAs for Algorithm 3 lines 11 and 13, the aligning process is only done once in our algorithm. We restate \u201cObtain $\\hat{\\mu}_i^{(r)}$ with aligning and weighted averaging.\u201d to emphasize the global nodes are first aligned and then averaged. We will add more explanations along with the algorithm.\n\n**Q2**: Since the weighted averaging of local models, i.e., Equation (8), is the same in [1], the authors should provide a discussion or experiment to explain why their similarity calculation is superior to that in [1].\n\n [1] Baek J, Jeong W, Jin J, et al. Personalized subgraph federated learning[C] International Conference on Machine Learning. PMLR, 2023: 1396-1415.\n\n**R2**: Thanks for the question! The main difference between our weighted averaging and [1] is how to calculate the similarity between clients. [1] proposes to use functional similarity to measure the similarity between clients. However, the quality of the calculated similarity is influenced by the randomly generated input graph (stochastic block model used in [1]). The situation becomes worse when the generated input graph has a different distribution from the original data on each client.\nMoreover, the generation and transmission of the generated input graph induces extra computation and communication costs. \n\nIn our paper, we use the global nodes from each client to calculate the similarity between clients. The global nodes are intrinsic to FedGT and can well reflect the data distribution of each client. The calculated similarity is more stable and not influenced by other artifacts. Therefore, our similarity calculation is superior to that in [1].\n\nWe will add these explanations to our paper.\n\n**Q3**: To show the convergence rate, Figure 5 and Figure 6 did not contain FED-PUB, which is the runner-up baseline in most cases.  \n**R3**: In Figures 5 and 6, we select several representative baselines to compare with FedGT. For example, FedPer is the representative personalized FL baseline; FedSage+ is the representative subgraph FL baseline; GraphGPS is the representative graph transformer baseline. In the revised version, we added FED-PUB to compare with FedGT. We can observe that FedGT has better convergence performance than the other baselines including FED-PUB.\n\n**Q4**: In the ablation study, the authors only conduct experiments on w/o global attention and w/o personalized aggregation. Results of w/o the complete Graph Transformer (i.e., without local attention) should also be provided.  \n**R4**: Thanks for the suggestion! In the revised paper, we include the results of w/o graph transformer in our ablation studies (Figure 9). We observed that the performance of FedGT w/o graph transformer drops a lot, which further verifies the effectiveness of graph transformer in subgraph federated learning. \n\n**Q5**: The authors opt for a consistent number of global nodes n_g across all clients. However, how does the methodology account for scenarios in which clients have a varying number of nodes, with some having significantly more and others noticeably fewer? Is there a suggested approach for determining varying n_g values that are customized to each client\u2019s node count?  \n**R5**: Thanks for the question! In our paper, we follow the setting of [1] where the number of nodes of each client is roughly the same. In scenarios where clients have varying numbers of nodes, the number of global nodes n_g can be set proportional to the number of nodes. We will systematically explore such settings in our future works."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152741972,
                "cdate": 1700152741972,
                "tmdate": 1700183513950,
                "mdate": 1700183513950,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xDGhrM2uoA",
                "forum": "YHihO8Ka3O",
                "replyto": "HG76TReo2z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uFDb"
                    },
                    "comment": {
                        "value": "**Q6**: In the typical federated learning framework, the number of training samples is considered when aggregating the model parameters. However, Equation (8) only uses the normalized similarity for the weighted aggregation. Why can we ignore the number of training samples here? Or do we assume the number of training samples is equivalent across clients?\n\n**R6**: Thanks for the question! In our work, we follow the setting of previous works such as [1] where the number of nodes of local graphs is roughly the same. Therefore, we only need to consider the normalized similarity for aggregation and ignore the difference in the number of training samples, which is the same as the aggregation scheme in FED-PUB [1]. In future works, we will systematically explore the influence of the number of nodes and consider it in aggregations. \n\n**Q7**: The Hungarian algorithm only finds a bijective mapping while optimal transport can be generalized to many-to-many cases, could the authors explain the reason for making a one-to-one alignment of the global nodes?\n\n**R7**: In FedGT, the Hungarian algorithm is used to calculate the pairwise similarity for personalized aggregation. We use the Hungarian algorithm for its easy implementation and stable performance. To the best of our knowledge, we did not find a suitable many-to-many optimal transport applicable to our setting. Therefore, we adopt the Hungarian algorithm in FedGT. We may explore other optimal transport algorithms in our future works.\n\n**Q8**: Since the global nodes are dynamically updated during the training, and the representations of the nodes are not stable at the beginning of the training, would this impact the effectiveness of similarity calculation based on the global nodes?\n\n**R8**: The global nodes are dynamically updated during the training and the performance has fluctuations at the beginning. However, based on the observation, FedGT converges very quickly after several iterations (e.g., Figures 5 and 6). Therefore, the update of global nodes will not impact the effectiveness of similarity calculation."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152897876,
                "cdate": 1700152897876,
                "tmdate": 1700183772637,
                "mdate": 1700183772637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FRSNpXPD7S",
                "forum": "YHihO8Ka3O",
                "replyto": "YGrzbX8I7i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3964/Reviewer_uFDb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3964/Reviewer_uFDb"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "Thanks for the authors\u2019 response. The authors have clarified the aggregation of global nodes and added further experiments on convergence comparison and ablation study. However, as the authors admit, the proposed method\u2019s applicability is limited to scenarios where clients possess the same number of nodes. Consequently, I have reconsidered my evaluation and decided to raise my score to 5."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640679814,
                "cdate": 1700640679814,
                "tmdate": 1700640679814,
                "mdate": 1700640679814,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cgsAQbaUvq",
                "forum": "YHihO8Ka3O",
                "replyto": "HG76TReo2z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further rebuttal with more results on unbalanced settings"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the response and support! We are happy to know that most of your questions are addressed. As for the default setting that clients possess a similar number of nodes, we follow the same setting with previous works on subgraph federated learning [1, 2]. In our revised version, we further add additional results in the unbalanced setting (Appendix E.11, Figure. 10, Table 11). The number of nodes of clients can vary **10 times**. We compared FedGT with the most competitive baseline FED-PUB. The unbalanced setting is more challenging due to fewer nodes and more missing links. However, we can observe that FedGT is **robust to the distribution shifts and can consistently achieve better results than baselines**.  \n\n| Model                  | Cora           | CiteSeer       | PubMed         | Computer       | Photo          | ogbn-arxiv     |\n|------------------------|----------------|----------------|----------------|----------------|----------------|----------------|\n| FED-PUB (default)      | 81.45\u00b10.12     | 71.83\u00b10.61     | 86.09\u00b10.17     | 89.73\u00b10.16     | 92.46\u00b10.19     | 66.35\u00b10.16     |\n| FedGT (default)        | **81.49**\u00b10.41     | **71.98**\u00b10.70     | **86.65**\u00b10.15     | **90.59**\u00b10.09     | **93.17**\u00b10.24     | **67.79**\u00b10.11     |\n| FED-PUB (unbalanced)   | 73.51\u00b10.40     | 64.32\u00b10.81     | 79.44\u00b10.56     | 85.69\u00b10.48     | 84.87\u00b10.42     | 62.46\u00b10.25     |\n| FedGT (unbalanced)     | **76.46**\u00b10.35     | **66.72**\u00b10.77     | **83.29**\u00b10.48     | **86.47**\u00b10.55     | **86.74**\u00b10.36     | **65.03**\u00b10.36     |\n\n\nWe hope our further results and explanations can address your concerns. We will be very grateful if you consider increasing to positive scores to support our work!\n\n[1] Ke Zhang, Carl Yang, Xiaoxiao Li, Lichao Sun, and Siu Ming Yiu. Subgraph federated learning with missing neighbor generation. In NeurIPS 21  \n[2] Jinheon Baek, Wonyong Jeong, Jiongdao Jin, Jaehong Yoon, and Sung Ju Hwang. Personalized subgraph federated learning. In ICML 23"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699582602,
                "cdate": 1700699582602,
                "tmdate": 1700711921599,
                "mdate": 1700711921599,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DU3AsjAHlS",
            "forum": "YHihO8Ka3O",
            "replyto": "YHihO8Ka3O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3964/Reviewer_jUaw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3964/Reviewer_jUaw"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose to use Graph Transformer and optimal-transport-based personalized aggregation to alleviate the fundamental problems in the subgraph federated learning algorithm such as missing links and subgraph heterogeneity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) Leverages graph transformer architecture within subgraph FL for the first time in the federated graph learning literature.\n\n(2) The algorithm is compatible with local DP.\n\n(3) Experimentally shows that Transformers are useful for subgraph federated learning. \n\n(4) Theoretical analysis of global attention being able to capture and approximate information in the whole subgraph is provided."
                },
                "weaknesses": {
                    "value": "(1) How Graph Transformer deals with the missing links is unclear. \n\n(2) The assumption that nodes are equally distributed to the global nodes seems unrealistic due to graph partitioning.\n\n(3) Theorem is not rigorous as it is a known fact that more nodes less error [1]\n\n(4) Local LDP does not guarantee privacy for sensitive node features, edges, or neighborhoods on\ndistributed graphs [2,3]. Using LDP does not reflect an actual privacy guarantee for this case.\n\n[1] Kim, Hyunjik, George Papamakarios, and Andriy Mnih. \"The Lipschitz constant of self-attention.\" International Conference on Machine Learning. PMLR, 2021.\n[2] Imola, Jacob, Takao Murakami, and Kamalika Chaudhuri. \"Locally differentially private analysis of graph statistics.\" 30th USENIX security symposium (USENIX Security 21). 2021.\n[3]Kasiviswanathan, Shiva Prasad, et al. \"Analyzing graphs with node differential privacy.\" Theory of Cryptography: 10th Theory of Cryptography Conference, TCC 2013, Tokyo, Japan, March 3-6, 2013. Proceedings. Springer Berlin Heidelberg, 2013."
                },
                "questions": {
                    "value": "(1) Could you please compare FedGT with FedDEP [1]? \n\n\n\n[1] Zhang, Ke, et al. \"Deep Efficient Private Neighbor Generation for Subgraph Federated Learning.\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3964/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788786822,
            "cdate": 1698788786822,
            "tmdate": 1699636357405,
            "mdate": 1699636357405,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YMP7KSJzjx",
                "forum": "YHihO8Ka3O",
                "replyto": "DU3AsjAHlS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jUaw"
                    },
                    "comment": {
                        "value": "We thank you for the detailed reviews and constructive comments!  \n**Q1**\uff1aHow Graph Transformer deals with the missing links is unclear.  \n**R1**\uff1aWe discussed how Graph Transformer deals with missing links in introduction, Section 4, and Appendix B.\nFedGT tackles missing link issues with the powerful graph transformer architecture and the global nodes. Most GNNs follow a message-passing paradigm that is likely to make false predictions with altered or missing links. In contrast, Graph Transformer is robust to missing links due to its global attention scheme. Moreover, the global nodes in FedGT capture the global context and get further augmented with personalized aggregation, which can supplement the missing information of cross-subgraph links. Finally, extensive experiments on 6 datasets demonstrate FedGT can overperform state-of-the-art baselines based on GNNs. As pointed by other reviewers, dealing with missing links with graph transformer is well motivated.\nWe will state more clearly in our revised paper.\n\n**Q2**: The assumption that nodes are equally distributed to the global nodes seems unrealistic due to graph partitioning.  \n**R2**: In FedGT, each client curates a set of global nodes to approximate the global context of the corresponding subgraph. As in Algorithm 1, the global nodes are updated with an online clustering algorithm as the cluster centers. In our experiments, we observe that the clustering in FedGT is not sensitive to graph partitioning and the nodes generally distribute evenly to the global nodes. \nFor example, we probed the distribution of nodes to 10 global nodes in a client on the Cora dataset. There are a total 247 nodes in the client and the number of assigned nodes to global nodes is (27, 28, 22, 24, 25, 22, 21, 24, 25, 29), which is quite close to the uniform distribution.\nTo further satisfy the assumption, we can use **balanced clustering** [1,2] that ensures nodes are equally distributed to the global nodes. Therefore, the assumption is realistic for FedGT.\n\n[1]Weibo Lin et al., Balanced clustering: a uniform model and fast algorithm. IJCAI, 2019.  \n[2]Bienkowski, Marcin, et al. Improved Analysis of Online Balanced Clustering.\" International Workshop on Approximation and Online Algorithms. 2021.  \n\n**Q3**: Theorem is not rigorous as it is a known fact that more nodes less error [3]  \n**R3**: Thanks for mentioning the related work [3]. Generally, our theorem aligns well with the fact that more nodes less error. In Figure 5(b), we show that the accuracy generally increases with more global nodes. Due to the randomness of training and the redundant noise brought by global nodes, the classification may have fluctuations with a larger number of global nodes.\n\n[3] Kim, Hyunjik, George Papamakarios, and Andriy Mnih. \"The Lipschitz constant of self-attention.\" International Conference on Machine Learning. PMLR, 2021.  \n\n**Q4**: Local LDP does not guarantee privacy for sensitive node features, edges, or neighborhoods on distributed graphs [2,3]. Using LDP does not reflect an actual privacy guarantee for this case.  \n**R4**: \nWe thank the reviewer for mentioning the related works [4,5] on the private analysis of graph data. We will cite and discuss them in our paper. In our work, we do not directly upload graph statistics. Instead, we apply LDP to the uploaded model parameters and learned global node representations following previous works [6,7] to protect privacy.\nMoreover, local differential privacy is not our main contribution to FedGT. Compared with previous works on subgraph federated learning (e.g., the baseline methods we compare in our paper), FedGT has **comparable or better privacy protection levels**.\n\n[4] Imola, Jacob, Takao Murakami, and Kamalika Chaudhuri. \"Locally differentially private analysis of graph statistics.\" 30th USENIX security symposium (USENIX Security 21). 2021.  \n[5] Kasiviswanathan, Shiva Prasad, et al. \"Analyzing graphs with node differential privacy.\" Theory of Cryptography: 10th Theory of Cryptography Conference, TCC 2013, Tokyo, Japan, March 3-6, 2013. Proceedings. Springer Berlin Heidelberg, 2013.  \n[6] Chuhan Wu, Fangzhao Wu, Yang Cao, Yongfeng Huang, and Xing Xie. Fedgnn: Federated graph\nneural network for privacy-preserving recommendation. KDD, 2021  \n[7] Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang, and Xing Xie. Privacy-preserving news\nrecommendation model learning. EMNLP, 2020."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151850808,
                "cdate": 1700151850808,
                "tmdate": 1700184474407,
                "mdate": 1700184474407,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]