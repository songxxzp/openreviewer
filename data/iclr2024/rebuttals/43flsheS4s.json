[
    {
        "title": "Improving Robustness and Accuracy with Retrospective Online Adversarial Distillation"
    },
    {
        "review": {
            "id": "jLZ9aZzYm1",
            "forum": "43flsheS4s",
            "replyto": "43flsheS4s",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1607/Reviewer_FC1d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1607/Reviewer_FC1d"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the adversarial robustness via model distillation and proposes retrospective online adversarial distillation,  which exploits the student itself of the last epoch and a natural model as teachers to guide target model training. The paper proves the effectiveness of the proposed method through experiments and provides theoretical analysis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method is single-stage adversarial distillation, but this is not the first time.\n2. The authors provide a theoretical and experimental analysis of the label for robust overfitting."
                },
                "weaknesses": {
                    "value": "1. The novelty of this paper is insufficient. It is highly overlapped with previous work, such as the use of soft labels, which was explored in the previous work [1]. Secondly, the use of online training methods was explored in the work of [2], and the use of natural models as teacher was explored in [3].\n\n2. Lack of ablation experiments, such as the impact of natural models, and the selection of robust models (different checkpoints)\n\nref:\n[1] Revisiting Adversarial Robustness Distillation: Robust Soft Labels\nMake Student Better.\n[2]. Alleviating Robust Overfitting of Adversarial Training With Consistency Regularization.\n[3] Learnable Boundary Guided Adversarial Training."
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1607/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698651803900,
            "cdate": 1698651803900,
            "tmdate": 1699636089009,
            "mdate": 1699636089009,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0ClWDvAmTV",
                "forum": "43flsheS4s",
                "replyto": "jLZ9aZzYm1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1607/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1607/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer FC1d"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer for the thoughtful feedback. \n\n**The novelty of this paper is insufficient. It is highly overlapped with previous work, such as the use of soft labels, which was explored in the previous work [1]. Secondly, the use of online training methods was explored in the work of [2], and the use of natural models as teacher was explored in [3].**\n\nThanks for your concerns regarding the novelty of our paper. However, ROAD distinguishes itself from prior works in two main aspects. \n\nFirstly, While there may be similarities with RSLAD in using soft labels, ROAD does not require an adversarially trained model to improve robustness, thereby offering a computational cost advantage. Thus, ROAD can be a promising technique to enhance robustness if we have no pre-trained teacher models. Also, considering that adversarial training typically takes several times longer than conventional training, it can be considered to possess sufficient novelty.\n\nSecondly, our approach diverges from methods that utilize a natural model by implementing an asymmetric knowledge transfer. This is based on the premise that usage of acquired knowledge to the natural and robust models are different as we mentioned in Section 3.2. Our experiments and ablation studies demonstrate that ROAD significantly outperforms other methods in terms of natural accuracy through this asymmetric knowledge transfer. \n\n**Lack of ablation experiments, such as the impact of natural models, and the selection of robust models (different checkpoints)**\n\nWe appreciate your constructive feedback for experiment part. We have conducted several additional experiments including comprehensive ablation studies (effects of interpolation ratio scheduling, asymmetric knowledge transfer) and stability of ROAD. We have updated our results in our paper.\n\nTable 1 shows the effect of interpolation ratio scheduling. We have considered two more scheduling strategies (fixed, linear increasing)\n\nTable 1. Effect of interpolation ratio $\\lambda$\n| Method  | NAT   | AA    |\n|---------|-------|-------|\n| Fixed   | 59.66 | 26.07 |\n| Linear  | 62.56 | 27.23 |\n| Sine    | 62.09 | 27.60 |\n\nTable 2 shows the effect of transferring asymmetric knowledge. We have prepared the symmetric version of ROAD: the natural model achieves knowledge via not soft labels but KL-divergence, typically seen in conventional online distillation. We have reused $\\gamma$ for simplicity and symmetric knowledge transfer. \n\nTable 2. Effect of transferring robust knowledge to natural model\n\n| Method               | NAT   | AA    |\n|----------------------|-------|-------|\n| ROAD ($\\gamma = 1$)(KL) | 57.91 | 27.30 |\n| ROAD ($\\gamma = 2$)(KL) | 60.00 | 27.65 |\n| ROAD ($\\gamma = 3$)(KL) | 60.45 | 27.36 |\n| ROAD ($\\gamma = 4$)(KL) | 60.65 | 27.36 |\n| ROAD                 | 62.09 | 27.60 |\n\nTable 3 shows the stability of ROAD. We have trained TRADES and ROAD 5 times with different random seeds. As we can observe in Table 3, ROAD shows high stability while outperforms TRADES in both natural accuracy and robustness.\n\nTable 3. Variance across multiple reruns \n\n|  | Run | NAT (ROAD) | NAT (TRADES) | PGD-20 (ROAD) | PGD-20 (TRADES) | PGD-100 (ROAD) | PGD-100 (TRADES) | MIM-10 (ROAD) | MIM-10 (TRADES) | AA (ROAD) | AA (TRADES) |\n|--------|-----|------------|--------------|---------------|----------------|----------------|------------------|---------------|----------------|----------|-------------|\n|        | 1   | 62.09      | 60.53        | 33.74         | 29.96          | 33.81          | 29.87            | 34.43         | 30.65          | 27.60    | 25.01       |\n|        | 2   | 61.80      | 60.61        | 33.53         | 30.32          | 33.48          | 30.24            | 34.19         | 31.07          | 27.41    | 25.59       |\n|        | 3   | 62.01      | 59.88        | 33.68         | 30.32          | 33.67          | 30.20            | 34.47         | 31.23          | 27.66    | 25.69       |\n|        | 4   | 61.62      | 60.34        | 33.88         | 30.24          | 33.69          | 30.01            | 34.45         | 31.03          | 27.68    | 25.43       |\n|        | 5   | 62.08      | 60.10        | 33.36         | 30.35          | 33.28          | 30.19            | 34.15         | 31.02          | 27.14    | 25.46       |\n| Avg.   |     | **61.92**  | 60.29        | **33.63**     | 30.24          | **33.58**      | 30.10            | **34.33**     | 31.00          | **27.49**| 25.44       |\n| Std.   |     | 0.07       | 0.23         | 0.18          | 0.15           | 0.18           | 0.10             | 0.13          | 0.20           | 0.20     | 0.24        |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700119234169,
                "cdate": 1700119234169,
                "tmdate": 1700362598347,
                "mdate": 1700362598347,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K3strkSgJF",
                "forum": "43flsheS4s",
                "replyto": "0ClWDvAmTV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1607/Reviewer_FC1d"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1607/Reviewer_FC1d"
                ],
                "content": {
                    "title": {
                        "value": "Reply to author"
                    },
                    "comment": {
                        "value": "Thanks to the author for the reply. From the author's response and the original submitted manuscript, the proposed method combines some previous methods. the contribution is just incremental. \nAnother question is why adversarial distillation (especially using natural models as teachers) can significantly improve the natural accuracy of adversarial models. There are no theoretical explanations.\n\nSo I keep my rating."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642747001,
                "cdate": 1700642747001,
                "tmdate": 1700642747001,
                "mdate": 1700642747001,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W4S0xM8vmX",
            "forum": "43flsheS4s",
            "replyto": "43flsheS4s",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1607/Reviewer_m9Uu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1607/Reviewer_m9Uu"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel technique designed to address the challenges inherent in balancing the trade-off between robust accuracy and natural accuracy while considering the computational overhead associated with adversarial distillation methods. The proposed method, referred to as ROAD, introduces a single-step training approach that incorporates self-distillation, employing the previous epoch's network as the teacher model. Additionally, a natural model is concurrently trained to provide guidance in the context of natural images, thus enhancing the aforementioned trade-off. ROAD leverages the utilization of soft-labels to penalize overconfident predictions, fostering collaborative learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper addresses an important topic in the adversarial training domain.\n-  It is well written and easy to follow. The algorithms and methodology is well explained\n- The results cover three different networks and attacks (including AA)"
                },
                "weaknesses": {
                    "value": "**Motivation:**\nThe assertion positing that overconfidence serves as a factor impeding the generalization capacity of robust models, as detailed in Section 3.1.1, warrants substantiation through references or empirical analysis within an AT context. It remains unclear how the resolution for penalizing overconfident predictions is incorporated via the utilization of soft labels.\n\nIt is worth noting that self-distillation has already demonstrated its efficacy in numerous studies pertaining to supervised and adversarial training methods. These approaches often involve the utilization of a prior model's time-stamp or an Exponential Moving Average (EMA) model for the purpose of regularization. Furthermore, online collaborative training has been well-established in the existing literature, e.g. ACT [2] and CAD [4] among others. The incorporation of soft labels and label-smoothing techniques in AT is studied, so it's somewhat perplexing, as their novelty is not readily apparent.\n\n**Complexity:**\n\n- The ROAD method still maintains two separate networks.\n- It necessitates the retention of an additional copy of the network (from the last epoch) in GPU memory and, in aggregate, entails three supplementary forward propagation steps.\n\n**Baselines:**\n\n- The paper appears to lack references to several relevant baselines, encompassing both new and established methods [1-4].\nIn particular, the omission of comparative data with respect to collaborative and online distillation training techniques, such as ACT and MAT, is noteworthy. Moreover, the results of these techniques in the paper do not match those from the baseline references [1], specifically as presented in Table 3. The paper briefly mentions IAD but fails to provide a comparative analysis of its results.\n- Further inconsistencies arise from comparisons with the RSLAD paper, wherein the AA and other attack-related metrics appear to surpass the corresponding figures reported in the ROAD paper. Similar discrepancies are apparent in the case of SEAT results.\n- A comprehensive evaluation of the ROAD method would ideally encompass additional benchmarks, including but not limited to Weight Averaging [5] and AWP [6].\n\nReferences:\n\n[1] Mutual Adversarial Training: Learning together is better than going alone\n\n[2] Adversarial Concurrent Training: Optimizing Robustness and Accuracy Trade-off of Deep Neural Networks\n\n[3] RELIABLE ADVERSARIAL DISTILLATION WITH UNRELIABLE TEACHERS\n\n[4] Improving adversarial robustness through a curriculum-guided reliable distillation\n\n[5] ROBUST OVERFITTING MAY BE MITIGATED BY PROPERLY LEARNED SMOOTHENING \n\n[6] Adversarial Weight Perturbation Helps Robust Generalization"
                },
                "questions": {
                    "value": "- The related works section requires an update, incorporating more relevant and recent works.\n- Please review the baseline models and consider adding more baselines (check the previous section for reference).\n- In Section 3.1.2, the RSLAD also utilizes soft labels. Can we access the reliability diagram for it and obtain further information about the differences between these two methods?\n- In Section 3.3, the second term of the objective function resorts to TRADES (involving the KL loss between robust and natural accuracy). Is this necessary when a separate model exists solely for natural images?\n- The hyperparameter lambda is based on a hypothesis (that robust models are substantially poor in natural accuracy at early stages of training, as discussed in Section 3.2). Can we see some supporting evidence or results for this hypothesis? Ablations with different schedules for lambda to evaluate its impact on the results would be beneficial.\n- Is the primary improvement stemming from the natural model, self-distillation, or solely from soft-labels? Further ablation experiments are necessary by removing each component individually.\n- In Figure 4, does the orange line include guidance from the natural model with one-hot labels? What are the results when both self and natural model guidance objectives are replaced with one-hot labels?\n- In Section 4.5, the computational cost should be compared with other online distillation methods.\n- In Figure 5 (a) and (b), is the observed effect due to gamma or lambda? Why is there a significant difference between the CIFAR-10 and CIFAR-100 datasets? Hyperparameters should ideally not be highly sensitive to datasets and settings to develop more generalizable solutions.\n- clarification and explanation of the differences between various concepts related to label smoothing"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1607/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796606727,
            "cdate": 1698796606727,
            "tmdate": 1699636088913,
            "mdate": 1699636088913,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1nDLovkorS",
                "forum": "43flsheS4s",
                "replyto": "W4S0xM8vmX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1607/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1607/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer m9Uu part1"
                    },
                    "comment": {
                        "value": "Thanks for your insightful comments and detailed review! We prepare pointwise answers for your concerns and questions.\n\n**It remains unclear how the resolution for penalizing overconfident predictions is incorporated via the utilization of soft labels.**\n\nWe have provided theoretical proofs in the respective appendix and supported our claims empirically through reliability diagrams and ECE results. However, we acknowledge that this may not suffice for your assessment. Therefore, we have also included the calibration performance of ROAD to further demonstrate the effects of utilizing last epoch predictions in Appendix D.3.\n\n**Concerns of novelty.**\n\nThanks for addressing your concerns about the novelty of our paper. Our method achieves novelty in two key aspects. \n Firstly, unlike other approaches that require the addition of extra models to enhance robustness, such as those using EMA or prior model's timestamps, our method stands out by not necessitating the use of additional models. We only store the predictions of last epoch. This feature offers a clear advantage in terms of GPU memory usage efficiency and it is more applicable in devices with limited resources. Secondly, our approach diverges from conventional methods that employ a natural model and learns collaboratively by implementing asymmetric knowledge transfer. This is based on the premise that usage of acquired knowledge to the natural and robust models are different as we mentioned in Section 3.2. We conducted an ablation experiments of the effects and updated in our revised paper.\n\n**The ROAD method still maintains two separate networks. It necessitates the retention of an additional copy of the network (from the last epoch) in GPU memory and, in aggregate, entails three supplementary forward propagation steps.**\n\nSorry about the ambiguity of our paper about the implementation details of ROAD. ROAD maintains only one separate network, the natural model. We store the past predictions of the last epoch instead of copying the network which can be shown in Figure 1. This can be also seen in our publicly provided code. Thus, as we can observe in the experiment section, ROAD does not require a substantial amount of memory. However, we can also consider using a separate model if the predictions are massive to store, such as tokens for text classification. \n\n**The paper appears to lack references to several relevant baselines, encompassing both new and established methods**\n\nThanks for your constructive feedback. We agree with the importance and relevance of ACT and MAT and updated our related works in our revised paper. \n\n**Further inconsistencies arise from comparisons with the RSLAD paper, wherein the AA and other attack-related metrics appear to surpass the corresponding figures reported in the ROAD paper. Similar discrepancies are apparent in the case of SEAT results.**\n\nWe would like to highlight that there are variations in the implementation details between the original paper and ours, such as the total number of epochs (reduced from 300 to 200), changes in learning rate scheduling (from divided by 10 at the 215th, 260th, and 285th epochs, to the 100th, 150th, and 175th epochs), and adjustments in weight decay (from 2e-4 to 5e-4) for a fair comparison. Also, please consider that the RSLAD paper reports similar results to our paper in Appendix Table 9 when RSLAD is trained through self-distillation (RN-18 --> RN-18). Furthermore, it should be noted that SEAT, in their official code, opted for a maximum perturbation bound of 0.031, as opposed to our choice of 8/255, and a step size of 0.007 instead of 2/255. These differences may significantly impact the performance of robustness.\n\n**A comprehensive evaluation of the ROAD method would ideally encompass additional benchmarks, including but not limited to Weight Averaging and AWP**\n\nThanks for your recommendation ! Due to the lack of time and computational resources, we will conduct the experiments you suggested at a later date and update the paper accordingly. \n\n**Please review the baseline models and consider adding more baselines (check the previous section for reference).**\n\nThanks for your nice suggestion. We add KD+SWA[1] and IAD[2] to our baseline which is similar with ROAD in the fact that using teacher models. \n\n**In Section 3.1.2, the RSLAD also utilizes soft labels. Can we access the reliability diagram for it and obtain further information about the differences between these two methods?**\n\nWe have added the reliability diagrams of ARD, RSLAD, and ROAD in the appendix of our updated paper. The Expected Calibration Error (ECE) of ROAD is 3.77, demonstrating superior calibration performance with the lowest ECE. Furthermore, we observe that ROAD tends to generate under-confident predictions, in contrast to the over-confident predictions commonly associated with ARD and RSLAD."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700119251258,
                "cdate": 1700119251258,
                "tmdate": 1700185077134,
                "mdate": 1700185077134,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6xmSkNNJkr",
                "forum": "43flsheS4s",
                "replyto": "cqidT36R1m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1607/Reviewer_m9Uu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1607/Reviewer_m9Uu"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the authors' diligent efforts in conducting additional experiments and presenting their findings in the rebuttal. The clarification provided regarding the separate network (where predictions are saved instead of the previous epoch-model) is noted and appreciated.\n\nHowever, I have some reservations regarding certain claims made in the rebuttal, particularly concerning the assertion that the method stands out by not necessitating additional models. My concern arises from the fact that there remains the utilization of an additional natural model along with additional prediction memory. In contrast, there exist alternative adversarial distillation (AD) methods that operate with only two networks, without the incorporation of extra memory, and some employ weight averaging techniques without the use of additional natural models.\n\nFurthermore, I observed discrepancies in the training of RSLAD, specifically with different hyperparameters than those originally specified in the paper. To ensure a fair and accurate comparison, it would be advisable to adhere to the hyperparameters reported as the best-tuned in the original paper, ensuring consistency in the perturbation bound.\n\nGiven the overlapping fundamental concepts shared with ACT (and partially with MAT), it might be insightful to include a comparative analysis, considering that ACT involved robust and natural model distillation but without the incorporation of soft labels.\n\nRegarding the AWP results, I would like to inquire about the rationale behind retraining the model for comparison purposes instead of utilizing the results from the original work for a comparative analysis.\n\nThe observed sensitivity to hyperparameters raises a significant concern. Considering the aforementioned points, the claims of novelty in the rebuttal still appear to be subject to question. Therefore, I tend to keep my initial rating.\n\nI hope these comments and suggestions are helpful in refining the manuscript further. I appreciate the authors' dedication and willingness to address these concerns."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645469286,
                "cdate": 1700645469286,
                "tmdate": 1700645469286,
                "mdate": 1700645469286,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qVZOoshGam",
            "forum": "43flsheS4s",
            "replyto": "43flsheS4s",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1607/Reviewer_zap7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1607/Reviewer_zap7"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel method called \"Retrospective Online Adversarial Distillation\" (ROAD) to improve the robustness of Deep Neural Networks (DNNs) against adversarial attacks while also maintaining high natural accuracy (accuracy on clean data). Unlike conventional Adversarial Distillation (AD) methods which involve training a robust teacher model and then transferring the knowledge to a student model, ROAD utilizes the student model from the last epoch and a natural model (trained with clean data) as teachers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. the idea of extending from distillation to self-distillation is very natural and expected to work out well. \n\n2. the idea of the asymmetry between how robust model and natural model influences each other is also very interesting, although the choices do not seem to be sufficiently discussed. \n\n3. the model achieves reasonably good empirical performances."
                },
                "weaknesses": {
                    "value": "1. the paper involves several interesting design that collaboratively contribute to strong performances, thus a more detailed ablation study (more than the one the authors offered) is probably necessary, for examples\n    - why is the asymmetry of how robust model and natural model influence each other is necessary? The current ablation study only touches briefly on the removal of these losses. However, since the authors emphasized on this asymmetry, it will be quite essential to discuss what if we use the same way of how these models influences each other. For example, what if we use soft labels from natural model also, or what if when we train the natural models, we use KL regularization also. \n\n2. The \"LS\" method does not seem to ever get spelled out, thus very hard to evaluate the relevant discussions. The paper cited is not immediately about the topics discussed in this paper. \n    - As a result, I do not see how \"Effects on utilizing last epoch predictions\" in ablation study is relevant."
                },
                "questions": {
                    "value": "1 the current ablation study in Figure 4 seems to suggest that self-distillation does not matter that much, a major performance boost comes from the natural model part, which seems quite counter-intuitive. It could be helpful if the authors explain more about this. \n\n2. in table 1 and 2, it will be helpful to have a natural model for references."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1607/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810621771,
            "cdate": 1698810621771,
            "tmdate": 1699636088843,
            "mdate": 1699636088843,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7FqTlDyHDv",
                "forum": "43flsheS4s",
                "replyto": "qVZOoshGam",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1607/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1607/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer zap7"
                    },
                    "comment": {
                        "value": "Thanks for your positive feedback and your suggestions about the experiments. We provide pointwise responses below.\n\n**The paper involves several interesting design that collaboratively contribute to strong performances, thus a more detailed ablation study (more than the one the authors offered) is probably necessary, for examples why is the asymmetry of how robust model and natural model influence each other is necessary? The current ablation study only touches briefly on the removal of these losses. However, since the authors emphasized on this asymmetry, it will be quite essential to discuss what if we use the same way of how these models influences each other. For example, what if we use soft labels from natural model also, or what if when we train the natural models, we use KL regularization also.**\n\n We acknowledge that our ablation study may have been insufficient and, in response, have included two additional studies in Figure 4 in our revised version. Following your suggestion, we conducted a performance comparison experiment where the natural model in ROAD receives knowledge via KL-divergence similar to standard online distillation. Additionally, we reuse $\\gamma$ as a hyper-parameter for the KL-divergence term to exchange symmetric knowledge between robust model and natural model. As shown in Table 1, regardless of the $\\gamma$ value, our findings confirm that the original ROAD's asymmetric knowledge transfer method is superior to the symmetric knowledge exchange in natural accuracy.\n\nTable 1. Effect of transferring robust knowledge to natural model \n| Method               | NAT   | AA    |\n|----------------------|-------|-------|\n| ROAD ($\\gamma = 1$)(KL) | 57.91 | 27.30 |\n| ROAD ($\\gamma = 2$)(KL) | 60.00 | 27.65 |\n| ROAD ($\\gamma = 3$)(KL) | 60.45 | 27.36 |\n| ROAD ($\\gamma = 4$)(KL) | 60.65 | 27.36 |\n| ROAD                 | 62.09 | 27.60 |\n\n**The \"LS\" method does not seem to ever get spelled out, thus very hard to evaluate the relevant discussions. The paper cited is not immediately about the topics discussed in this paper. As a result, I do not see how \"Effects on utilizing last epoch predictions\" in ablation study is relevant.**\n\n\u201cEffects on utilizing last epoch predictions\u201d is designed to show the performance of self-adversarial distillation using last epoch predictions. We select label smoothing(LS) and adversarial knowledge distillation (AKD) as baselines because these methods also employ soft labels to improve robustness, similar to our method. However, we also agree that the corresponding ablation study does not need immediate attention, so we moved it to the appendix. \n\n**Figure 4 seems to suggest that self-distillation does not matter that much, a major performance boost comes from the natural model part, which seems quite counter intuitive.**\n\nWhile it may seem counterintuitive, it is actually because replacing the soft labels of the natural model with one-hot labels and allowing the natural model to unilaterally transfer knowledge to the robust model can actually lead to a detrimental decrease in robustness. However, it is important to note that utilizing self-distillation, as opposed to not using it at all, results in a performance increase of over 1\\% in the perspective of PGD-20. We have updated Figure 4 to make this effect more readily apparent in our revised version.\n\n**It will be helpful to have a natural model for references.**\n\nAppreciate your suggestion! We add the performance of the natural model to Table 1 and Table 2 in our revised version."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700118563766,
                "cdate": 1700118563766,
                "tmdate": 1700118563766,
                "mdate": 1700118563766,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CR6q2GGY4a",
            "forum": "43flsheS4s",
            "replyto": "43flsheS4s",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1607/Reviewer_cBNA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1607/Reviewer_cBNA"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents ROAD, an adversarial self-distillation approach designed to tackle over-confidence issues and improve the tradeoff between clean and robust accuracy of robust models in Adversarial Training (AT). The method moderates over-confidence by generating soft labels for adversarial examples, merging predictions from the last epoch model with the original hard labels. Additionally, to maintain clean accuracy, a regularizer is introduced that aligns the predictions of the robust model with those of a natural model on clean samples. Experimental results exhibit superior performance on both natural accuracy and robustness compared with both AT and Adversarial Distillation (AD) methods among various evaluated scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1: This paper primarily focuses on adversarial self-distillation, a promising avenue for future adversarial machine learning research due to its lower resource demands compared to AD.\n\nS2: This paper introduces a simple yet effective framework that excels in both natural accuracy and robustness for adversarial self-distillation. The exhibited experimental results also partially reveal the ineffectiveness of existing AD methods under this scenario."
                },
                "weaknesses": {
                    "value": "W1: Certain key aspects of the presented theory in Section 3.1.1 appear ambiguous from my vantage point. The authors attempt to convince that the soft label proposed in Equation 1 can prevent the model\u2019s predictions from becoming overly confident because the proposed soft label can implicitly provide smaller weights to adversarial samples which have a drastically increased confidence. The authors prove this by demonstrating the norm of the gradient of the loss w.r.t the output logits $ \\frac{\\partial \\mathcal{L}}{\\partial z_{t, i}^{\\prime}} $ obtained with the proposed soft label will become smaller than that obtained with the original hard label if the confidence of the model output for $x\\prime$ increases in this epoch. However, existing works generally propose to constrain a relatively larger norm of gradient $\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{t}} $ [1] to mitigate the overconfidence issue. It appears that the authors do not provide sufficient proof that the effect of the introduced gradient norm scaling factor w.r.t to the logit also works for the gradient w.r.t to the model weights $\\theta$.\n\nW2: Several statements presented in Section 3.2 are deemed to be either inaccurate or not adequately substantiated. For example, the referenced studies do not specifically support the claim that 'distilling knowledge from the static natural teacher may impair robustness.' It is strongly advised that the structure and argumentation of this section be thoroughly revised for clarity and accuracy.\n\nW3: The logical progression within Section 3 is obscure, and the transitions between subsections lack fluidity, challenging the reader's ability to discern the author's objectives. Furthermore, the exposition accompanying Equation 3 falls short of providing sufficient elucidation or the underlying intuition for introducing the Robustness Enhancement term in the construction of the final objective function.\n\n[1] Tao Li, Yingwen Wu, Sizhe Chen, Kun Fang, Xiaolin Huang: Subspace Adversarial Training. CVPR 2022: 13399-13408"
                },
                "questions": {
                    "value": "Q1: Considering Weakness W1, does the proof presented in Section 3.1.1 extend to ensuring that adversarial samples, which induce a significantly larger gradient norm with respect to the model weights $\\theta$, can also be effectively penalized?\n\nQ2: Because the paper appears to lack a comprehensive exploration of the tuning strategy of the hyperparameter $\\lambda$ introduced in Equation 1, could you elucidate on the potential effects of employing a constant value for $\\lambda$, or linearly increase the value of $\\lambda$ instead of using the sine increasing schedule?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1607/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1607/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1607/Reviewer_cBNA"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1607/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699014190530,
            "cdate": 1699014190530,
            "tmdate": 1699636088773,
            "mdate": 1699636088773,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SkiLXOwenb",
                "forum": "43flsheS4s",
                "replyto": "CR6q2GGY4a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1607/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1607/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate about your detailed review and meaningful feedback. We provide pointwise responses below. \n\n**Considering Weakness W1, does the proof presented in Section 3.1.1 extend to ensuring that adversarial samples, which induce a significantly larger gradient norm with respect to the model weights $\\theta$, can also be effectively penalized?**\n\nWe have thoroughly read the suggested paper and appreciate its relevance to the broader field. However, we believe that directly linking our work to the findings of this paper may not be the most appropriate approach. \nThe suggested paper addresses the issue of \\textit{robust overfitting} which is a phenomenon that test accuracy on robustness drops significantly while train accuracy on robustness increases . In our understanding, the concepts of overconfidence and robust overfitting, while related, however are distinct. For instance, an overfitted model might exhibit low confidence when tested on unseen data from a different domain which is hard to say that the model is overconfident.\n\nMeanwhile, we have theoretically and empirically validated our assumption as detailed in our paper. To further support our argument, we have also included experiment results on the calibration performance of ROAD in the appendix D.3 in our updated paper. \n\n**Several statements presented in Section 3.2 are deemed to be either inaccurate or not adequately substantiated. For example, the referenced studies do not specifically support the claim that 'distilling knowledge from the static natural teacher may impair robustness.' It is strongly advised that the structure and argumentation of this section be thoroughly revised for clarity and accuracy.**\n\nThanks for your detailed feedback! We have revised our sentences in Section 3.2. The sentences are \n\n``However, experimental results presented in the literature [1,2] demonstrate that distilling knowledge from the static natural model can reduce robustness, indicating that it is not the proper approach.\"\n\nAlso, we want to clarify that our statements in Section 3.2 are sufficiently substantiated. The referenced studies support the claim explicitly or implicitly that static natural teacher can reduce robustness. For example, In section 4.4 of [1], they provide experimental results that training with natural soft labels (NSLs) crafted by the natural model can significantly harm robustness of the student model and claim it in the last sentence in the paragraph. Additionally, In table 1 of [2], the table shows that the student model with a natural model teacher has lower accuracy under AutoAttack when compared with no teacher.\n\nWe hope that the revised sentences address your concerns adequately.\n\n**The logical progression within Section 3 is obscure, and the transitions between subsections lack fluidity, challenging the reader's ability to discern the author's objectives. Furthermore, the exposition accompanying Equation 3 falls short of providing sufficient elucidation or the underlying intuition for introducing the Robustness Enhancement term in the construction of the final objective function.**\n\nWe strongly agree that explanations about the method can be confusing to the readers. To enhance clarity and facilitate easier reading, we have added more details to the introductory part of Section 3, clearly delineating the roles of each subsection. In addition, we understand your concerns about Robustness Enhancement term and add proper elucidations. The sentences are \n\n``This regularization term causes loss in natural accuracy as a trade-off for improved robustness. Nevertheless, this loss of accuracy can be recovered by the subsequent term.\"\n\n**Because the paper appears to lack a comprehensive exploration of the tuning strategy of the hyper-parameter \n introduced in Equation 1, could you elucidate on the potential effects of employing a constant value for \n$\\lambda$, or linearly increase the value of $\\lambda$ instead of using the sine increasing schedule?**\n\nThat is a nice feedback. Following your suggestion, we have conducted experiments that verify our selection of sine scheduling and updated in our paper. As shown in Table 1, utilizing last epoch predictions show an overall improvement in robustness\ncompared to PGD-AT which trained with one-hot labels. In addition, we observe that the fixed policy results in over\n1\\% lower natural accuracy compared to other strategies.\n\nTable 1. Effect of scheduling interpolation ratio $\\lambda$\n| Method        | NAT   | AA    |\n|---------------|-------|-------|\n| PGD-AT        | 57.23 | 25.13 |\n| Ours (Fixed)  | 56.20 | 26.16 |\n| Ours (Linear) | 57.55 | 25.88 |\n| Ours (Sine)   | 57.39 | 26.17 |\n\nRef\n\n[1] Revisiting Adversarial Robustness Distillation: Robust Soft Labels Make Student Better, ICCV 2021 \n\n[2] On the benefits of knowledge distillation for adversarial robustness, Arxiv 2022"
                    },
                    "title": {
                        "value": "Response to reviewer cBNA"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1607/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700118238089,
                "cdate": 1700118238089,
                "tmdate": 1700119526723,
                "mdate": 1700119526723,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]