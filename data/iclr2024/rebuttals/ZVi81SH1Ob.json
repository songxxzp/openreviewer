[
    {
        "title": "Neural Collapse meets Differential Privacy:  Curious behaviors of NoisySGD with Near-Perfect Representation Learning"
    },
    {
        "review": {
            "id": "ybVEZ7aWZe",
            "forum": "ZVi81SH1Ob",
            "replyto": "ZVi81SH1Ob",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8521/Reviewer_dsyA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8521/Reviewer_dsyA"
            ],
            "content": {
                "summary": {
                    "value": "Recent study by De et al. (2022) reports that large-scale representation learning via pre-training on a gigantic dataset significantly enhances differentially private learning in downstream tasks. While the exact behaviors of NoisySGD on these problems remain intractable to analyze\ntheoretically, the authors consider an idealized setting of a layer-peeled model for representation learning by neural collapse.\n\nThe writing is good and the results seem interesting, which attracts me to check their proof. The proofs are very simple. $M_k,k=1,\\cdots,K$ form an ETF frame, which separate categories very well, the zero initialization makes $f_0(M_k)=0$ for all $k=1,\\cdots,K$, which is very weird. The one-step NoisyGD seems not useful at all.\n\n I am not sure your results are for NoisySGD or NoisyGD. In introduction section, your statements are all about NoisySGD, but the other parts for NoisyGD. Moreover, there is no definition about NoisySGD at all in the whole paper."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The presentation is good."
                },
                "weaknesses": {
                    "value": "The results are not meaningful."
                },
                "questions": {
                    "value": "How about $f_W(x)=Wx+b$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8521/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734522527,
            "cdate": 1698734522527,
            "tmdate": 1699637065017,
            "mdate": 1699637065017,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H1DfCZGw8h",
                "forum": "ZVi81SH1Ob",
                "replyto": "ybVEZ7aWZe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8521/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8521/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "It appears that our main contribution may have been overlooked by the reviewer. We would like to highlight it once more and also address the reviewer's question as follows.\n\n\n\n\n$\\mathbf{Comments:}$ $M_k, k = 1,\\cdots, K$ form an ETF frame, orm an ETF frame, which separate categories very well, the zero initialization makes $f_0(M_k) = 0$\n, which is very weird. \n\n$\\mathbf{Response:}$ We would like to clarify that $0$ initialization is ubiquitous in training SGD or DP-SGD and we are not sure which part is weird.\nEven though, $f_0(M_k) = 0$ for all $k$, the training process will push the parameter to fit the the separate categories after iterations.\n\n$\\mathbf{Comments:}$ The one-step NoisyGD seems not useful at all.\n\n$\\mathbf{Response:}$\nFor perfect collapse, the fact that 1-iteration, full-batch GD suffices, and the fact that it is free of hyperparameters (any learning rate, any class imbalance, any dimension, any meaningful loss function)  come at a surprise in a sense that \u2014 even such a much weaker and non-adaptive algorithm works.\n\n In more realistic, imperfect collapse scenarios, our theory can be extended to multiple iterations. However, it is obvious that dimension dependency can not be mitigated using multi-iterations. Therefore, it is not clear why one-iteration NoisyGD would be deemed without merit.\n\n$\\mathbf{Comments:}$ I am not sure your results are for NoisySGD or NoisyGD. In introduction section, your statements are all about NoisySGD, but the other parts for NoisyGD. Moreover, there is no definition about NoisySGD at all in the whole paper.\n\n$\\mathbf{Response:}$ Our theoretical framework specifically addresses NoisyGD. In our experimental analysis, however, we assess the performance of both NoisyGD and NoisySGD. We appreciate the reviewer highlighting the omission of NoisySGD's definition, and we have now included it in Section 2.2.\n\n\n\n\n\n$\\mathbf{Comments:}$ The proofs are very simple. \n\n$\\mathbf {Response:}$\nWe agree that our analysis is simple, but in light of Point 1,2,3 in the official rebuttal above, we believe the simplicity of the analysis is a feature rather than a bug! After all, we believe the reviewer agree that our results are interesting and new.  \nTo reiterate, we would like to highlight once more the contributions and novel findings of our study.\nIsn\u2019t it better to have a simpler proof of a fundamental result than a highly technical (and harder to verify) proof?\n\n\n1. It appears that our crucial observation--- that DP fine-tuning is non-robust to perturbations---was not acknowledged by the reviewer, a point we emphasized in Sections 3.2 and 3.3.\nThe \u201cApproximate Neural Collapse\u201d case shows that the remarkable property is quite fragile if we train with standard DP-GD or DP-SGD.  This actually led to various actionable algorithmic modifications to these private learning methods that make these methods substantially more robust \u2014 in theory and in practice! \n\n2.  In the perfect neural collapse case, our analysis also works perfectly for DP-SGD and it works for multiple iterations. As we stated above, many interesting facts come at a surprise in a sense that \u2014 even such a much weaker and non-adaptive algorithm works.\n\n3. Existing analysis of DP-GD and DP-SGD focuses on suboptimality in the surrogate losses.  The sample complexity is polynomial  $d/\\gamma$  or $d/ \\sqrt{\\gamma}$ (in the strongly convex case) to achieve $(1-\\gamma)$-accuracy.  We actually directly studied the ``0-1\" loss!  And we showed that the sample complexity is  $\\log(1/\\gamma)$.       This is an exponential improvement and shows an exponential benefit of strong representation learning.\n\n$\\mathbf{Questions:}$ How about $f_W(x) = Wx + b$?\n\n$\\mathbf {Response:}$ In the case of perfect collapse, incorporating an offset term $b$  is unnecessary. For binary classification problem with $\\alpha$-class imbalance, straightforward computation reveals that the output of NoisyGD is given by $(n e_1,(1 - 2\\alpha)) + \\mathcal{N}(0,\\sigma^2 I)$ where the $1 - 2\\alpha$ component arises from the offset $b$ and the direction associated with $b$ is simply Gaussian noise. This noise could potentially detract from performance.\n\nFor a non-perfect collapse scenario, a similar computation indicates that an offset $b$ does not eliminate the perturbation. Consequently, the outcome retains its dimension-dependence."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8521/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700254691097,
                "cdate": 1700254691097,
                "tmdate": 1700276998873,
                "mdate": 1700276998873,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "blelSCbpzp",
                "forum": "ZVi81SH1Ob",
                "replyto": "H1DfCZGw8h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8521/Reviewer_dsyA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8521/Reviewer_dsyA"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for your reply. I keep my score since the result may be not meaningful."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8521/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704203488,
                "cdate": 1700704203488,
                "tmdate": 1700704203488,
                "mdate": 1700704203488,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hjuwIiHE3K",
            "forum": "ZVi81SH1Ob",
            "replyto": "ZVi81SH1Ob",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8521/Reviewer_KJYE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8521/Reviewer_KJYE"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies theoretical analysis for differentially private fine-tuning under neural collapse. Specifically, this paper shows that if the neural collapse is assumed, and we only fine-tune last layer, the accuracy bound is indepedent of dimension and only related to privacy parameter. If the neural collapse is not perfect on private dataset, this paper also shows that perturbation on the features, class imbalance would require the accuracy to be depedent on the dimension. This paper also propose data normalization and PCA to mitigate this non-robustness issue."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper provides first theoretical understanding of DP fine-tuning reduces the error rate on down-streaming task. The setting of neural collapse is interesting and may be enlightening for potential future research."
                },
                "weaknesses": {
                    "value": "1. Typos: In theorem 2, is it $\\gamma$ accuracy or $1-\\gamma$ accuracy?\n2. All of the proofs only analyze one-step Noisy-GD algorithm under a very strong neural collapse assumption. This setting is too simple and might not be reflecting what is happening in De at al (2022). If perfect neural collapse holds, then there is no need for further training. For example,  in theorem 2, you can set the clipping threshold G to be very small (near zero) to get near zero error rate. This suggests that you don't have to train on the private data if the neural collapse is assumed. This bound might not be very useful.\n3. The proposed tricks are not demonstrated empirically on real datasets.\n4. The proof is simple and the technical contribution is limited."
                },
                "questions": {
                    "value": "1. Is there any empirical improvement by using the proposed data normalization and PCA tricks? I am curious because DP-PCA would also needs privacy-utility trade-off that needs to be accounted."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8521/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699585265163,
            "cdate": 1699585265163,
            "tmdate": 1699637064881,
            "mdate": 1699637064881,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X8YB9i7ZkI",
                "forum": "ZVi81SH1Ob",
                "replyto": "hjuwIiHE3K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8521/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8521/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments. It appears there may have been some misunderstandings in the review, potentially due to our failure to emphasize certain aspects within our original manuscript. We would like to address these points in the following response\n\n\n$\\mathbf{Weakness 1:}$ Typos: In theorem 2, is it $\\gamma$ accuracy or $1 - \\gamma$ accuracy?\n\n$\\mathbf{Response:}$ Thank you for pointing it out. It is $1 - \\gamma$ accuracy and we have corrected it.\n\n$\\mathbf{Weakness 2:}$ All of the proofs only analyze one-step ... might not be very useful. &\n$\\mathbf{Weakness 4:}$ The proof is simple and the technical contribution is limited.\n\n$\\mathbf{Response:}$ We have emphasized our main contributions in the official comments in details and we would like to emphasize them again here shortly.\n\n1. In addition to examining the ideal scenario of perfect neural collapse, our research also explores the concept of approximate collapse, which is more prevalent in practical situations. Our results suggest that the extraordinary dimension-independent properties observed during perfect neural collapse is inherently fragile. This non-robustness is also evident in our empirical studies for both NoisyGD and NoisySGD using real datasets!\n\n2. We wish to clarify for the reviewer that the example where the $G$ is near zero, as mentioned by the reviewer, does not align with our theory. In fact, in the proof, we choose $G$ to be the sensitivity of the gradient, and our theory is valid only when $G\\geq 1$ for perfect collapse, or when $G \\geq \\sqrt{1 + \\beta^2 p}$ for imperfect collapse.  It is worth noting that even if we clip the data point with a small $G$, this does not alter our results. In fact, in the case where $K=2$, as detailed in the proof of Theorem 3 (similar results hold for the more general case, Theorem 2, and the imperfect case in Section 3.2 and Section 3.3) in Appendix A.2, $n/2$ is scaled to $Gn/2$ if $G<1$, and a smaller $G$ does not imply a smaller mis-classification error.\n\n3. Existing analysis of DP-GD and DP-SGD focuses on suboptimality in the surrogate losses.  The sample complexity is polynomial  $d/\\gamma$  or $d/ \\sqrt{\\gamma}$ (in the strongly convex case) to achieve $(1-\\gamma)$-accuracy.  We actually directly studied the ``0-1\" loss!  And we showed that the sample complexity is  $\\log(1/\\gamma)$. This is an exponential improvement and shows an exponential benefit of strong representation learning.\n\n4. We agree that our analysis is simple, but in light of those points in this response and the official comments above, we believe the simplicity of the analysis is a feature rather than a bug! After all, we believe the reviewer agree that our results are interesting and new.  Isn\u2019t it better to have a simpler proof of a fundamental result than a highly technical (and harder to verify) proof?\n\n$\\mathbf{Weakness 3:}$ The proposed tricks are not demonstrated empirically on real datasets.\n\n$\\mathbf{Response:}$ We wish to clarify for the reviewer that our investigation includes empirical evidence of the non-robustness of DP-finetuning to perturbations using real datasets. Specifically, our Figure 2(b), which uses a real dataset, demonstrates that SGD without DP is robust to increasing dimensions. Conversely, the accuracy of DP-SGD decreases significantly with rising dimensions.\n\nAdditionally, the phenomenon of neural collapse is a widely observed phenomenon in deep learning, as evidenced by various references cited in our paper, including the study available at https://www.pnas.org/doi/10.1073/pnas.2103091118. Related to transfer learning, a notable reference is a recent paper presented at NeurIPS, which provides evidence of neural collapse during the fine-tuning process (https://openreview.net/pdf?id=xQOHOpe1Fv).\n\n$\\mathbf{Questions:}$ Is there any ... to be accounted.\n\n$\\mathbf{Response:}$ DP-PCA has been widely adopted in DP-SGD to enhance empirical performance, as evident in ``Deep Learning with Differential Privacy\" (https://arxiv.org/abs/1607.00133). We provide a perspective from neural collapse theory to explain the efficacy of PCA.\nDue to the time constraints of the rebuttal period, we have not conducted experiments using DP-PCA at this stage.\n\nThe reviewer is correct that DP-PCA requires a privacy-utility trade-off. However, the impact of the privacy budget on PCA's accuracy is not the focus of our paper.\nIt's important to note that the recent theory on DP-PCA, such as the one described in ``https://arxiv.org/abs/2205.13709\", cannot be directly applied to our setting since their accuracy is measured by the angle between eigenvectors.\nIn contrast, our investigation into the 0-1 loss highlights the importance of the infinity norm between eigenvectors. \nTherefore, it would be interesting to establish theoretical guarantees for DP-PCA under the infinity norm."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8521/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700254023858,
                "cdate": 1700254023858,
                "tmdate": 1700254023858,
                "mdate": 1700254023858,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7jPah5ha11",
            "forum": "ZVi81SH1Ob",
            "replyto": "ZVi81SH1Ob",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8521/Reviewer_MQQr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8521/Reviewer_MQQr"
            ],
            "content": {
                "summary": {
                    "value": "The authors use neural collapse theory to analyze the behavior of last-layer fine-tuning with DP. They show that dimension independence emerges in a certain sense under perfect neural collapse and that small perturbations in the train and test data can disturb this independence. They show that data normalization and dimensions reduction can recover the dimension independence in the face of such perturbations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The general phenomenon explored in this paper (i.e. the empirical success of DP deep fine-tuning in high dimensions) is very interesting and timely.\n2. The results are presented with a high degree of technical precision and fluency.\n3. The result of Theorem 2 seems surprising and interesting to me, although I don't yet have a strong intuitive understanding of the proof."
                },
                "weaknesses": {
                    "value": "1. **It may be difficult for some readers to understand:** Given the topic, I imagine many readers will be familiar with differential privacy but less familiar with neural collapse. As a result, the third paragraph of the introduction and the corresponding figure 1 will be meaningless to them without more explanation. Some of the introductory material is present in section 2.2, but it is a bit technical and not well-suited to newcomers. I would recommend giving a high level explanation of neural collapse in the introduction to help readers."
                },
                "questions": {
                    "value": "1. Introduction, paragraph 1, last line: do you mean \"no-privacy utility tradeoff\"\n2. Bottom of page 7, Sigma is missing a backslash.\n3. The dimension independence of Theorem 5 requires $\\beta_0$ to scale with $p$, but the non-robustness results in 3.2 and 3.3 would also become dimension independent if $\\beta$ chosen to vary with $p$ in a similar way. Because of this, it's not clear what we are gaining from dimension reduction in section 4.2.\n2. It's not clear to me how this analysis of neural collapse applies to full fine tuning. The success of DP full fine tuning is most surprising because of the many total parameters of the networks (not just in the last layer). Neural collapse may explain some of the dynamics of last layer fine-tuning as presented in this paper, but clearly something interesting must be happening at intermediate layers in the full fine-tuning setting. I think it would be very helpful to mention whether there is any way that these results might shed light on the dynamics of intermediate layers."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8521/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699604527884,
            "cdate": 1699604527884,
            "tmdate": 1699637064784,
            "mdate": 1699637064784,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "N2auJPaXKS",
                "forum": "ZVi81SH1Ob",
                "replyto": "7jPah5ha11",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8521/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8521/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful for your valuable feedback. In particular, Question 4 has provided us with insightful perspectives. We would like to offer the following response in return.\n\n$\\mathbf{Weakness:}$ It may be difficult for some readers to understand... I would recommend giving a high level explanation of neural collapse in the introduction to help readers.\n\n$\\mathbf{Response:}$ Thank you for these comments. We have added a high-level explanation of the neural collapse and the $K$-simplex equiangular tight frame (ETF) to the manuscript in the introduction. Neural collapse is the phenomenon where features corresponding to different classes are separable enough. An illustrative example for the case where $K=2$ is as follows: an ETF $M$ can be represented as $M = [-m_1, m_1]$ for some vector $m_1 \\in \\mathbb{R}^{p}$, where the features corresponding to the label $y=1$ are $m_1$, and the features corresponding to the label $y=-1$ are $-m_1$.\n\n$\\mathbf{Question 1:}$ Introduction, paragraph 1, last line: do you mean \"no-privacy utility tradeoff\".\n\n$\\mathbf{Response:}$ Thank you for pointing out this typo. We have corrected it.\n\n$\\mathbf{Question 2:}$ Bottom of page 7, Sigma is missing a backslash.\n\n$\\mathbf{Response:}$ Corrected.\n\n$\\mathbf{Question 3:}$ The dimension independence of Theorem 5 requires $\\beta_0$ to scale with $p$, but the non-robustness results in 3.2 and 3.3 would also become dimension independent if $\\beta$ chosen to vary with $p$ in a similar way. Because of this, it's not clear what we are gaining from dimension reduction in section 4.2.\n\n$\\mathbf{Response:}$ As detailed in Section 4.3, for PCA we have established that $\\beta_0 \\leq \\frac{1}{\\sqrt{m}}$ with high probability  where $m$ represents the sample size used in PCA, effectively scaling $p$ to $p/\\sqrt{m}.$\nOur theoretical framework, at this stage, does not encompass DP-PCA, which is actually implemented in DP-SGD. Should DP-PCA be taken into account,\n$\\beta_{0}$ would be influenced by the privacy budget. While our current paper does not concentrate on this aspect, understanding the convergence of the eigenvector in DP-PCA, particularly in terms of the infinity norm, necessitates a separate and thorough analysis.\n\n$\\mathbf {Question 4:}$\nIt's not clear to me how this analysis of neural collapse applies to full fine tuning ... these results might shed light on the dynamics of intermediate layers.\n\n$\\mathbf{Response:}$ Thank you for drawing attention to this intriguing topic. Our empirical findings suggest that, except in the final layer, neural collapse is typically not observed\u2014even in settings without Differential Privacy. This is also corroborated by another paper \"Exploring Deep Neural Networks via Layer-Peeled Model: Minority Collapse in Imbalanced Training\" (https://arxiv.org/pdf/2101.12699.pdf), which explains that, from a mathematical standpoint, an Equiangular Tight Frame (ETF) is the optimal solution for Empirical Risk Minimization (ERM) when considering the features of the last layer.\n\nRecent research, such as \"The Tunnel Effect: Building Data Representations in Deep Neural Networks\" by Masarczyk et al., presented at NeurIPS 2023, has made some novel observations. These include the discovery that the last few layers of deep neural networks exhibit low-rank structures, and their performance is resemblance to that of linear models. Within this context, neural collapse in the final layer can be seen as a special case of these low-rank structures, which are also present in transfer learning scenarios.\n\nGiven these developments, it would be a valuable line of inquiry for our future research to examine whether fine-tuning the last few layers with NoisySGD could reveal low-rank structures, like neural collapse --- or perhaps a partially neural collapse influenced by the noise. Due to time limitations, we will defer this investigation to subsequent studies."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8521/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700253365795,
                "cdate": 1700253365795,
                "tmdate": 1700253365795,
                "mdate": 1700253365795,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]