[
    {
        "title": "A Progressive Training Framework for Spiking Neural Networks with Learnable Multi-hierarchical Model"
    },
    {
        "review": {
            "id": "urywRzBM3L",
            "forum": "g52tgL8jy6",
            "replyto": "g52tgL8jy6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission67/Reviewer_LLZy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission67/Reviewer_LLZy"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes the limitations of the vanilla LIF neuron model, including the problems of gradient vanishing and exploding, as well as the inability to differentiate the current response by extracting past information. Based on these analyses, the authors propose a novel learnable multi-hierarchical model that has a wider calculation scope along the time dimension by incorporating the functions of dendrite and soma. Additionaly, they further design a progressive STBP training framework for the LM-H model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1.The proposed model is neural-inspired.  \n\n2.The work is solid. The authors provide rigorous theoretical analysis of the LM-H model and demonstrate that the LIF model is merely a subset of the LM-H model.\n\n3.The progressive training method efficiently solves the multi-parameter learning problem of the LM-H model.\n\n4.Experimental results demonstrate the significant advantages of the LM-H model across multiple datasets."
                },
                "weaknesses": {
                    "value": "1.The authors illustrate the issues of gradient vanishing and exploding faced by the LIF neuron. However, they have not demonstrated how the proposed LM-H model addresses these problems.\n\n2.The figures displayed in this paper solely depict the vanilla LM-H model. It would be good if the authors can also integrate the radical version of the LM-H model into these figures."
                },
                "questions": {
                    "value": "1. Please provide a more comprehensive explanation and analysis of the radical version of the LM-H model, and discuss its advantages compared to the vanilla LM-H model.\n\n2. Regarding the hybrid training framework of the LM-H model, the author used the conversion framework based on the IF model during the ANN-SNN conversion phase. How about LIF neurons? Does it fit into this framwork?\n\n3. The authors have illustrated the distinction between their work and that of TC-LIF in the related works section. I suggest adding it into the introduction as well."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission67/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698575398883,
            "cdate": 1698575398883,
            "tmdate": 1699635931258,
            "mdate": 1699635931258,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Y86TRJ0LE2",
                "forum": "g52tgL8jy6",
                "replyto": "urywRzBM3L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission67/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission67/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer LLZy"
                    },
                    "comment": {
                        "value": "## To Reviewer LLZy\n> How the proposed LM-H model addresses the issues of gradient vanishing and exploding faced by the LIF neuron?\n\nThanks for your comments! We have provided a thorough analysis and a detailed explanation for this concern. Please refer to **To All Reviewers**.\n\n> It would be good if the authors can also integrate the radical version of the LM-H model into these figures.\n\nThanks for your suggestion! We have revised the figure and submitted a new version.\n\n> Please provide a more comprehensive explanation and analysis of the radical version of the LM-H model, and discuss its advantages compared to the vanilla LM-H model.\n\nAs shown in Eq.6, the overall proportion term for historical information is represented as $(\\mu_D)^{t-k}$, indicating that the extracted proportions at different time-steps follow an exponential distribution. In contrast, in Eq.7, the corresponding term changes to $\\prod_{j=k+1}^{t} \\mu_{D,j}$, allowing for more freedom and randomness in the extracted proportions. In addition, in the radical version, relevant factors that regulate the membrane potential ($\\mu_{S,t}, \\lambda_{S,t}$) and input current ($\\lambda_{D,t}$) are independently assigned at each time-step, making the information representation range of the radical version wider than that of the vanilla version. We have added more details of the comparsion in Section A.5 of the revised version.\n\n\n> The author used the conversion framework based on the IF model during the ANN-SNN conversion phase. How about LIF neurons? Does it fit into this framwork?\n\nYes, it also fits into this framwork. According to our proposed hybrid training framework, if we initially convert a pre-trained ANN to the IF model, then set the membrane leaky constant $\\lambda^l$ in each layer as a learnable parameter and adopt STBP for training, we can also incorporate the LIF model into the same framework. However, as shown in Eq.6, the LIF model can be regarded as a special case of the LM-H model, with its representation range being limited to a subset of the LM-H model's representation range. Consequently, we are inclined to believe that the significance of hybrid training based on the LIF model is quite limited.\n\n\n> The authors have illustrated the distinction between their work and that of TC-LIF in the related works section. I suggest adding it into the introduction as well.\n\nThanks for pointing it out. We have added the relevant content in the introduction of our revised version."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission67/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700134679319,
                "cdate": 1700134679319,
                "tmdate": 1700134679319,
                "mdate": 1700134679319,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VB0XDsitqz",
            "forum": "g52tgL8jy6",
            "replyto": "g52tgL8jy6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission67/Reviewer_T4SQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission67/Reviewer_T4SQ"
            ],
            "content": {
                "summary": {
                    "value": "This work proposed a new neuron model, Learnable Multi-hierarchical (LM-H) model, to effectively extracting global information along the time dimension and propagate gradients in deep networks. In the LM-H model, the scaling factors from the dendrite layer regulate the proportion of historical information extracted by the model, while the factors from the soma layer determine the degree of potential leakage and the intensity of the input current at present."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed LMH model, along with GLIF, TC-LIF, enriches the family of the spiking neuron models. This approach is unique and original, as it combines existing ideas in a new way to solve a problem in Spiking Neural Networks. The proposed model and training algorithm address the deficiencies of the widely adopted LIF model and offer a new approach to solving problems in this field."
                },
                "weaknesses": {
                    "value": "While the proposed model and training algorithm are unique and original, the paper could benefit from a more detailed discussion of why they propose the new model, how they differ from existing methods, and what specific contributions they make to the field of Spiking Neural Networks. \nWhat are the significant deficiencies in deep layer gradient calculation and capturing global information on the time dimension, as mentioned in this paper?"
                },
                "questions": {
                    "value": "Does the author address the gradient vanishing & exploding problem in deep residual architectures with the new LMU model? \nHow? \nThe authors listed results of cifar data set on resnet, can you provide results on vgg as well?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission67/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission67/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission67/Reviewer_T4SQ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission67/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699208097277,
            "cdate": 1699208097277,
            "tmdate": 1700436600489,
            "mdate": 1700436600489,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Tl81MHNqLU",
                "forum": "g52tgL8jy6",
                "replyto": "VB0XDsitqz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission67/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission67/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer T4SQ (Part I)"
                    },
                    "comment": {
                        "value": "## To Reviewer T4SQ\n> What are the significant deficiencies in deep layer gradient calculation? Does the author address the gradient vanishing & exploding problem in deep residual architectures with the new LM-H model? How?\n\nThanks for your comments! We have provided a thorough analysis and a detailed explanation for this concern. Please refer to **To All Reviewers**.\n\n> What are the significant deficiencies in capturing global information on the time dimension, as mentioned in this paper?\n\nIn terms of capturing global information, the LIF model is limited compared to the LM-H model, as depicted in Eq.6. The LIF model can only consider the second term of the equation, which represents the current representation. This limitation arises from the nature of spiking neurons (LIF model), which can transmit binary information only once per time-step. If the membrane potential and spike firing information from different time-steps are not comprehensively utilized, the representation capability of the output spike sequence becomes restricted. Consequently, this limitation can have an impact on the performance of SNNs. Table 1 in this paper, along with Table R1, can demonstrate the significant performance gap between the vanilla LIF model and our LM-H model, particularly when applied to large-scale datasets.\n\n\n\n> The paper could benefit from a more detailed discussion of why they propose the new model, how they differ from existing methods, and what specific contributions they make to the field of Spiking Neural Networks.\n\nThanks for your valuable feedback! We would like to clarify that our motivation for proposing the LM-H model primarily stems from the limitations of the existing LIF model in addressing deep gradient computation and global information extraction challenges. While other advanced models such as GLIF and TC-LIF have been proposed by researchers in the SNN community, we believe that the LM-H model offers four distinct contributions that previous works have not encompassed:\n\n(1) **The relationship between the LM-H model and the LIF model is elucidated through rigorous mathematical analysis, highlighting the superiority of the LM-H model.** We demonstrate that the LM-H model encompasses the LIF model as a special case (Eq.5-Eq.6), while also showcasing the superior information extraction capabilities of the LM-H model. This analysis ensures that the theoretical performance of the LM-H model is at least on par with the LIF model, as acknowledged by Reviewer LLZy.\n\n\n(2) **The LM-H model possesses a remarkable capability to dynamically and flexibly extract global information across different time steps**. Through detailed analysis, we identify the specific functions of membrane-related parameters and set appropriate learning ranges for them. This allows the LM-H model at the $t$-th timestep to effectively incorporate relevant information, including input current and membrane potential from previous $t-1$ steps. This capability to consider information from multiple time steps simultaneously sets the LM-H model apart from other advanced models.\n\n(3) **The LM-H model demonstrates superior scalability and transferability.** From a computational perspective, the LM-H model is a more general form of the LIF model. In terms of neuronal structure, the LM-H model incorporates a dendrite layer to store historical information for each time-step. When $\\mu_D=\\mu_S=0, \\lambda_D=1$, the dendrite layers become invisible, and the LM-H model degenerates into a single-layer state consistent with the LIF model. These two aspects establish a close relationship between the LM-H model and the LIF model, facilitating easy migration of various training frameworks associated with the LIF model (such as hybrid training and online training) to the LM-H model.\n\n\n(4) **The progressive STBP training framework is novel.** This novel training framework starts with the parameter settings of the LIF model as the initial state and dynamically optimizes membrane-related parameters during the training process to achieve the optimal state of the LM-H model. This design concept of \"from special to general\" allows for efficient and effective training of the LM-H model."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission67/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700134535047,
                "cdate": 1700134535047,
                "tmdate": 1700134616375,
                "mdate": 1700134616375,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kFfCGjexsr",
                "forum": "g52tgL8jy6",
                "replyto": "VB0XDsitqz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission67/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission67/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer T4SQ (Part II)"
                    },
                    "comment": {
                        "value": "> The authors listed results of cifar data set on resnet, can you provide results on vgg as well?\n\nThanks for your suggestion! We have conducted additional experiments using the VGG-13 architecture on the CIFAR-10 and CIFAR-100 datasets. As shown in Tab. R4, our results demonstrate superior performance compared to previous SOTA methods, even when employing smaller network structures and fewer time-steps. We have also included these content in the Appendix A.6 of our submitted version.\n\n**Table R4: Comparison with previous SOTA works on CIFAR datasets, VGG structure.**\n| Dataset     | Method | Architecture | Time-steps | Accuracy(%) |\n| ----------- | ------ | ------------ | ---------- | ----------- |\n|CIFAR-10|Diet-SNN [1]|VGG-16|5|92.70|\n|CIFAR-10|Real Spike [2]|VGG-16|5, 10|92.90, 93.58|\n|**CIFAR-10**|**Ours**|**VGG-13**|**4**|**93.98**|\n|**CIFAR-10**|**Ours(radical version)**|**VGG-13**|**4**|**94.80**|\n|CIFAR-100|Diet-SNN [1]|VGG-16|5|69.67|\n|CIFAR-100|RecDis-SNN [3]|VGG-16|5|69.88|\n|CIFAR-100|Real Spike [2]|VGG-16|5, 10|70.62, 71.17|\n|**CIFAR-100**|**Ours**|**VGG-13**|**4**|**73.99**|\n|**CIFAR-100**|**Ours(radical version)**|**VGG-13**|**4**|**74.79**|\n\n[1] Nitin Rathi and Kaushik Roy. DIET-SNN: A low-latency spiking neural network with direct input encoding and leakage and threshold optimization. IEEE Transactions on Neural Networks and Learning Systems, pp. 1\u20139, 2021.\n\n[2] Yufei Guo, Liwen Zhang, Yuanpei Chen, Xinyi Tong, Xiaode Liu, YingLei Wang, Xuhui Huang, Zhe Ma. Real spike: Learning real-valued spikes for spiking neural networks. European Conference on Computer Vision. 2022.\n\n[3] Yufei Guo, Xinyi Tong, Yuanpei Chen, Liwen Zhang, Xiaode Liu, Zhe Ma, and Xuhui Huang. RecDis-SNN: Rectifying membrane potential distribution for directly training spiking neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 326\u2013335, 2022."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission67/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700134583802,
                "cdate": 1700134583802,
                "tmdate": 1700134641268,
                "mdate": 1700134641268,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EEkJMS0RVw",
                "forum": "g52tgL8jy6",
                "replyto": "kFfCGjexsr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission67/Reviewer_T4SQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission67/Reviewer_T4SQ"
                ],
                "content": {
                    "comment": {
                        "value": "Upon revisiting the manuscript and carefully considering the author's responses, I acknowledge the efforts made to address my concerns. The clarifications provided have significantly improved my understanding of certain aspects of the work. As a result, I am willing to adjust my initial score from 5 to 6."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission67/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700436539715,
                "cdate": 1700436539715,
                "tmdate": 1700436539715,
                "mdate": 1700436539715,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VAdUBqpS0S",
            "forum": "g52tgL8jy6",
            "replyto": "g52tgL8jy6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission67/Reviewer_uzHG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission67/Reviewer_uzHG"
            ],
            "content": {
                "summary": {
                    "value": "The authors of this paper address the limitations of the widely used LIF model in SNN by proposing a novel LM-H model. The LM-H model overcomes issues related to gradient calculation in deep networks and capturing global information along the time dimension. The authors also develop a progressive training algorithm specifically for the LM-H model. Experiments on various datasets demonstrate the superior performance of the LM-H model and the training algorithm compared to previous state-of-the-art SNN approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1 Did a lot baselines and achieved SOTA performance\n2 A novel neuron model LH-M that is a extension of LIF model"
                },
                "weaknesses": {
                    "value": "1. In the context of deep residual architectures, LIF neurons are known to exhibit issues to either vanishing or exploding gradients. How has the LH-M model effectively addressed and mitigated these challenges?\n\n2. Both ImageNet and CIFAR datasets focus on static image classification, whereas the LH-M model incorporates historical data and demonstrates superior performance. Could you elucidate the underlying reasons for this enhanced performance in the context of LH-M's utilization of historical information?\n\n3. How was the conversion from Artificial Neural Networks (ANN) to Spiking Neural Networks (SNN) executed, given that ANNs do not inherently incorporate temporal information? Considering the significance of historical data within the LH-M model, how was this temporal aspect effectively integrated during the conversion process?"
                },
                "questions": {
                    "value": "1. LIF neurons have a gradient vanishing or exploding problem in deep residue architecture, how did LH-M solved this problem?\n\n2. ImageNet and CIFAR are all static image classification datasets, but LH-M involves historical information and performed better. Can you give an explaination?\n\n3. How did you perform ANN2SNN conversion? ANN do not include temporal information, but in LH-M historical information is an important property."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission67/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission67/Reviewer_uzHG",
                        "ICLR.cc/2024/Conference/Submission67/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission67/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699544152578,
            "cdate": 1699544152578,
            "tmdate": 1700735061789,
            "mdate": 1700735061789,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RLfSj2nIMN",
                "forum": "g52tgL8jy6",
                "replyto": "VAdUBqpS0S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission67/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission67/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer uzHG"
                    },
                    "comment": {
                        "value": "## To Reviewer uzHG\n> LIF neurons have a gradient vanishing or exploding problem in deep residual architecture, how did LM-H solved this problem?\n\nThanks for your comments! We have provided a thorough analysis and a detailed explanation for this concern. Please refer to **To All Reviewers**.\n\n\n> ImageNet and CIFAR are all static image classification datasets, but LH-M involves historical information and performed better. Could you elucidate the underlying reasons for this enhanced performance in the context of LH-M's utilization of historical information?\n\nThanks for your insightful comment! While ImageNet and CIFAR are static datasets, our experimental process, as consistent with previous works, involves inputting each image for multiple time-steps. Since spiking neurons (model) can only emit binary spikes (either firing or not firing) at each time-step, the representation range of information transmitted at a single time-step is inherently limited. Therefore, in order to enhance the representation ability of spiking neurons, it becomes necessary to consider the membrane potential ($\\forall j\\in[1, t-1], \\boldsymbol{v}^l[j]$) and spike firing status ($\\forall j\\in[1, t-1], \\boldsymbol{s}^l[j]$) at each previous time-step when deciding whether to fire a spike at the current time-step. This utilization of historical information allows for a comprehensive decision-making process. In fact, regardless of the input data type (static or neuromorphic), as long as we adopt multi-step training, the LM-H model can observe a broader range of information compared to the LIF model, thereby demonstrating its superior performance, as illustrated in Eq.6 in the main text.\n\n\n> How did you perform ANN2SNN conversion? ANN do not include temporal information, but in LM-H historical information is an important property.\n\nThanks for pointing it out. We would like to clarify that our hybrid learning method can be divided into two stages. In the first stage, we adopt traditional ANN-SNN Conversion framework to obtain a LM-H model under the special case, which indeed does not contain temporal information. As shown in Eq.11 and its corresponding analysis, we derived that the most accurate mathematical mapping relationship between the converted LM-H model and the pre-trained ANN model (i.e., the minimum conversion error) is established under the condition $\\mu_D=0,\\lambda_D=1,\\mu_S+\\lambda_S=1$. Therefore, in the first stage of hybrid training, we replace the activation functions in the pre-trained ANN model with the LM-H models, adhering to the special case ($\\mu_D=0,\\lambda_D=1,\\mu_S+\\lambda_S=1$) layer by layer. \n\nHowever, in the second stage of the training process, all the membrane-related parameters of the LM-H model ($\\mu_D,\\lambda_D,\\mu_S,\\lambda_S$) are set as learnable parameters. Throughout the progressive training process, these parameters are dynamically optimized and updated to effectively extract temporal information. The detailed description and specific procedure of the algorithm can be found in Appendix A.4 of the revised version, where a comprehensive explanation is provided."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission67/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700134414793,
                "cdate": 1700134414793,
                "tmdate": 1700134414793,
                "mdate": 1700134414793,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XLWsRPV0lh",
                "forum": "g52tgL8jy6",
                "replyto": "VAdUBqpS0S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission67/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission67/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the time and look forward to your feedback."
                    },
                    "comment": {
                        "value": "Dear Reviewer uzHG,\n\nAs the Author-Review Discussion period will be closed within a few hours, we would like to briefly summarize your concerns and our relevant responses as follow: \n1. The first concern is about the gradient calculation problem in deep residual architecture. We have provided a thorough analysis to discuss the specific difference of the gradient calculation between the vanilla LIF and LM-H model, as shown in To All Reviewers.\n2. The second concern is about the LH-M model's utilization of historical information on static datasets. We have made a detailed explanation to point out that the LM-H model can observe a broader range of information compared to the LIF model as long as we adopt multi-step training, thereby demonstrating its superior performance.\n3. The third question is about the utilization of temporal information in our hybrid training framework. We have clarified that our hybrid learning method can be divided into two stages. Throughout the progressive training stage, the membrane-related parameters are dynamically optimized and updated to effectively extract temporal information, the detailed description can be found in Appendix A.4 of our revised version.\n\nBased on these facts and positive feedback from other reviewers, we sincerely hope that you can reconsider your initial rating. If you have any further comment, please let us know and we are glad to address your concern.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission67/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732746427,
                "cdate": 1700732746427,
                "tmdate": 1700733196438,
                "mdate": 1700733196438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JHEDGc98g2",
                "forum": "g52tgL8jy6",
                "replyto": "VAdUBqpS0S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission67/Reviewer_uzHG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission67/Reviewer_uzHG"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comment"
                    },
                    "comment": {
                        "value": "I appreciate the authors' thorough and well-structured rebuttal, which has addressed my concerns effectively. It is evident that they have made solid progress in refining their work and providing convincing arguments to support their claims. Additionally, I believe that authors proposed a novel and effective spiking neuron model for SNN community. Taking these revisions into account, I am inclined to raise the paper's score."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission67/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735048763,
                "cdate": 1700735048763,
                "tmdate": 1700735048763,
                "mdate": 1700735048763,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "B7SjTtX2Ep",
            "forum": "g52tgL8jy6",
            "replyto": "g52tgL8jy6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission67/Reviewer_SiwN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission67/Reviewer_SiwN"
            ],
            "content": {
                "summary": {
                    "value": "The paper extends the LIF model for SNN to a more generalized version called LM-H, enhances its flexibility by making certain parameters learnable, and designs a progressive learning procedure to effectively train the network. Some experiments on relatively small datasets were presented to show that the proposed approach is superior to relevant prior methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The LM-H model and the learning algorithm were respectively inspired by and similar to existing works, however put together the paper still proposed a novel and practical framework for SNN learning."
                },
                "weaknesses": {
                    "value": "The experiments only covered several relatively small datasets. The ImageNet 200 dataset was the largest one in the paper is actually a tiny subset of ImageNet. Datasets like ImageNet 1k/22k would be more convincing to validate the practical value of the proposed approach.\n\nPerformance comparison in the experiment section is somewhat inconsistent, e.g. only ImageNet 200 results included the radical version showing better performance.\n\nGrammar errors scatter through the paper, further proof-reading is suggested."
                },
                "questions": {
                    "value": "Why only small datasets were experimented upon, was it because the proposed approach has scalability issues on larger and more practical datasets?\n\nWhy the performance of the radical version was only presented for ImageNet while missing for other datasets, was it because that it didn't show better accuracy?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission67/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699575652730,
            "cdate": 1699575652730,
            "tmdate": 1699635931012,
            "mdate": 1699635931012,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gY3W9G1BtY",
                "forum": "g52tgL8jy6",
                "replyto": "B7SjTtX2Ep",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission67/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission67/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer SiwN"
                    },
                    "comment": {
                        "value": "## To Reviewer SiwN\n> Why only small datasets were experimented upon, was it because the proposed approach has scalability issues on larger and more practical datasets?\n\nThank you for your suggestion! We have conducted additional experiments on the ImageNet-1k dataset and compared the performance of our model with previous SOTA approaches using the ResNet-34 architecture. Due to the time constraints during the rebuttal period, we directly evaluated the performance of our radical version. As illustrated in Tab. R2, it is evident that our method outperforms other SOTA approaches by at least 2% with the same (or fewer) time-steps, which clearly demonstrates the superior scalability of the LM-H model on large-scale datasets. Due to page constraints, we have included these content in the Appendix of the revised paper (Tab. S3 of Section A.5). In the final version, we will incorporate it into the main text.\n\n\n**Table R2: Comparison with previous SOTA works on ImageNet-1k dataset.**\n| Dataset     | Method | Architecture | Time-steps | Accuracy(%) |\n| ----------- | ------ | ------------ | ---------- | ----------- |\n|ImageNet-1k|STBP-tdBN [1]|ResNet-34|6|63.72|\n|ImageNet-1k|TET [2]|ResNet-34|6|64.79|\n|ImageNet-1k|RecDis-SNN [3]|ResNet-34|6|67.33|\n|ImageNet-1k|MBPN [4]|ResNet-34|4|64.71|\n|ImageNet-1k|SEW ResNet [5]|ResNet-34|4|67.04|\n|ImageNet-1k|GLIF [6]|ResNet-34|4|67.52|\n|**ImageNet-1k**|**Ours**|**ResNet-34**|**4**|**69.73**|\n\n> Why the performance of the radical version was only presented for ImageNet while missing for other datasets, was it because that it didn't show better accuracy?\n\nThanks for pointing it out. We have expanded our comparative experiments to include both the vanilla version and the radical version on CIFAR-10 and CIFAR-100 datasets, as shown in Tab. R3.  Given the inherent capability of the residual structure to enhance accuracy, particularly on relatively simple datasets, we have also incorporated the commonly used VGG structure to facilitate a clearer comparison between the two versions. As depicted in Table R3, the radical version, by regulating gradient calculation and information extraction, continues to exhibit better performance than the vanilla version on the CIFAR-10 and CIFAR-100 datasets. Furthermore, during the experimental process, we observed that the radical version also displayed faster convergence in terms of training accuracy, particularly when dealing with large-scale datasets. Due to page constraints, we have included these content in the Appendix of the revised paper (Tab.S3 of Section A.5, Tab.S4 of Section A.6). In the final version, we will incorporate it into the main text.\n\n\n**Table R3: Comparison of vanilla and radical versions on CIFAR-10/100 datasets.**\n| Dataset     | Method | Architecture | Time-steps | Accuracy(%) |\n| ----------- | ------ | ------------ | ---------- | ----------- |\n|CIFAR-10|vanilla version|VGG-13|4|93.98|\n|CIFAR-10|**radical version**|**VGG-13**|**4**|**94.80**|\n|CIFAR-10|vanilla version|ResNet-18|4|95.62|\n|CIFAR-10|**radical version**|**ResNet-18**|**4**|**95.82**|\n|CIFAR-100|vanilla version|VGG-13|4|73.99|\n|CIFAR-100|**radical version**|**VGG-13**|**4**|**74.79**|\n|CIFAR-100|vanilla version|ResNet-18|4|78.58|\n|CIFAR-100|**radical version**|**ResNet-18**|**4**|**78.90**|\n\n\n> Grammar errors scatter through the paper, further proof-reading is suggested.\n\nThanks for your advice\uff01We have carefully corrected the grammar errors in this paper and uploaded a new version.\n\n[1] Hanle Zheng, Yujie Wu, Lei Deng, Yifan Hu, and Guoqi Li. Going deeper with directly-trained larger spiking neural networks. In AAAI Conference on Artificial Intelligence, pp. 11062\u201311070, 2021.\n\n[2] Shikuang Deng, Yuhang Li, Shanghang Zhang, and Shi Gu. Temporal efficient training of spiking neural network via gradient re-weighting. International Conference on Learning Representations, 2022.\n\n[3] Yufei Guo, Xinyi Tong, Yuanpei Chen, Liwen Zhang, Xiaode Liu, Zhe Ma, and Xuhui Huang. RecDis-SNN: Rectifying membrane potential distribution for directly training spiking neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 326\u2013335, 2022.\n\n[4] Yufei Guo, Yuhan Zhang, Yuanpei Chen, Weihang Peng, Xiaode Liu, Liwen Zhang, Xuhui Huang, and Zhe Ma. Membrane potential batch normalization for spiking neural networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023.\n\n[5] Wei Fang, Zhaofei Yu, Yanqi Chen, Tiejun Huang, Timoth\u00e9e Masquelier, and Yonghong Tian. Deep residual learning in spiking neural networks. In Advances in Neural Information Processing Systems. 2022.\n\n[6] Xingting Yao, Fanrong Li, Zitao Mo, and Jian Cheng. GLIF: A unified gated leaky integrate-and-fire neuron for spiking neural networks. In Advances in Neural Information Processing Systems, 2022."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission67/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700134333908,
                "cdate": 1700134333908,
                "tmdate": 1700134333908,
                "mdate": 1700134333908,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OGvD8nP4rI",
                "forum": "g52tgL8jy6",
                "replyto": "B7SjTtX2Ep",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission67/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission67/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the time and look forward to your feedback."
                    },
                    "comment": {
                        "value": "Dear Reviewer SiwN,\n\nAs the Author-Review Discussion period will be closed within a few hours, we would like to briefly summarize your concerns and our relevant responses as follow: \n1. The first concern is about the scalability of our proposed method on larger datasets. We have conducted additional experiments on the ImageNet-1k dataset and compared the performance of our model with previous SOTA approaches. The experimental results in Tab.R2 have demonstrated the superior scalability of the LM-H model on large-scale datasets.\n2. The second concern is about the comparison between the vanilla version and the radical version. We have expanded our comparative experiments in Tab.R3 to include both the vanilla version and the radical version on CIFAR-10 and CIFAR-100 datasets. The experimental results can verify the effectiveness of the radical version.\n3. The third question is about the further proof-reading of this paper. We have carefully corrected the grammar errors in this paper and uploaded a new version.\n\nBased on these facts and positive feedback from other reviewers, we sincerely hope that you can reconsider your initial rating. If you have any further comment, please let us know and we are glad to address your concern.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission67/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732578503,
                "cdate": 1700732578503,
                "tmdate": 1700733179808,
                "mdate": 1700733179808,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]