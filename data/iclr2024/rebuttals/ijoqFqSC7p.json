[
    {
        "title": "FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling"
    },
    {
        "review": {
            "id": "85rEzsnT5s",
            "forum": "ijoqFqSC7p",
            "replyto": "ijoqFqSC7p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission611/Reviewer_17jd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission611/Reviewer_17jd"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a training-free method for extending capabilities of short video diffusion models to generate temporally coherent, longer videos, as well as incorporate multi-text conditioning. The authors propose a novel noise schedule and fused window-based temporal attention to enable more in-distribution, coherent longer generations. In order to enable multi-text conditions, the authors introduce a motion injection method based on conditioning different text prompts at different stages of diffusion sampling."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well written, and easy to understand\n- The proposed noise schedule + temporal attention modification is interesting as a method to enforce better long-term consistency in video\n- Incorporating the proposed method is fairly simple, as it does not require any extra training on a pretrained short text-video diffusion model\n- Experiments are concrete, and show much quality generations compared baselines"
                },
                "weaknesses": {
                    "value": "- From looking at the generated videos, although the proposed method can more cleanly generate longer videos, it seems that the spatial structure of the video (e.g. location of a cat) is very similar throughout the entire video. I believe this may be due to the repetitive nature of shuffled noise reptitions which are generally highly correlated with the structure of the resulting video. So it seems that the method may have a hard time generating more dynamic changes in long videos, such as a cat walking across the screen or scene / camera changes. Could the authors comment on this, or if there are generated video examples with larger structural changes through the video?"
                },
                "questions": {
                    "value": "- How would the proposed window fusing (weighted by frame index distance) compared to a simpler scheme such as just merging the current and/or prior window (i.e. similar to a one or two hot version of the frame index weighting).\n- Have the authors explored other noise augmentation other than shuffling? In general, shuffling does not seem to be the most intuitive method for perturbing gaussian noise, as it also constrains the epsilons (per frame) to the original samples.\n- Section 3.1 mentions that \"temporal attention is order independent\". Does this imply that the VideoLDM does not have temporal positional embeddings? Or how would it be order independent if it did?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission611/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698349591316,
            "cdate": 1698349591316,
            "tmdate": 1699635988822,
            "mdate": 1699635988822,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "StUCRInsrY",
                "forum": "ijoqFqSC7p",
                "replyto": "85rEzsnT5s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission611/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission611/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 17jd"
                    },
                    "comment": {
                        "value": "Thanks for your valuable comments. We summarize and answer your questions below.\n\n> **Q1: Spatial structure of the video (e.g. location of a cat) is very similar throughout the entire video**\n\nIn some cases, the displacement of the subject is limited due to the reused noises of FreeNoise. However, FreeNoise does not obliterate motion variation or thoroughly fix the spatial structure of objects. Please refer to the common response for more details.\n\n> **Q2: Advantages of the proposed weighted window fusion compared to the naive version that uses one-hot weights**\n\nThe naive version tends to cause abrupt content changes at window boundaries, and the weighted window fusion can suppress such artifacts by smoothing the temporal transition.\n\n> **Q3: Exploration of other noise augmentation other than shuffling**\n\nWe have explored some other strategies in our early experiments. We have tried mixed noise and progressive noise to make the fragments generated by each window more correlated [1]. However, it brings poor quality results due to the training-inference gap. In addition, we have tried to flip noise frames spatially. Although it brings more new content, abrupt changes in content are also introduced. We have uploaded video results to the anonymous website: https://free-noise.github.io/ (A: Other Noise Scheduling).\n\n> **Q4: Clarification on the statement that temporal attention is order independent**\n\nSorry for the confusion. Through the paper, we use a pre-trained video diffusion model, whose temporal blocks consist of temporal convolution and temporal Transformer with no temporal positional embeddings used. However, if the temporal Transformer uses temporal positional embeddings, the statement that temporal attention is order independent is no longer correct because the position embedding is order sensitive. At that time, temporal attention could be viewed as the superposition of order-independent parts and order-dependent parts. As a result, our method also works for T2V models that use temporal positional embeddings, such as the higher-resolution results (576x1024 model variant of VideoCrafter) demonstrated on our anonymous webpage.\n\n[1] Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission611/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302197178,
                "cdate": 1700302197178,
                "tmdate": 1700302197178,
                "mdate": 1700302197178,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ocHusff0tI",
                "forum": "ijoqFqSC7p",
                "replyto": "StUCRInsrY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission611/Reviewer_17jd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission611/Reviewer_17jd"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your response. From the further samples provided it seems that although the model can generate dynamic motion, it can only really do so within the training horizon of the base video prediction model, and the method has more trouble generating consistent dynamic motion across longer temporal resolutions (e.g. locations of entities \"snapping back\" to their starting locations). However, it is still able to better generate slight variations of motion and background across time compared to baseline methods, and could be useful for potential future applications. My concerns were only partially addressed, so I will maintain my score (6)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission611/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535008349,
                "cdate": 1700535008349,
                "tmdate": 1700535008349,
                "mdate": 1700535008349,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E3UyQAZzRu",
                "forum": "ijoqFqSC7p",
                "replyto": "85rEzsnT5s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission611/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission611/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Explanation of \"snapping back\""
                    },
                    "comment": {
                        "value": "Thanks for your impartial and reasonable evaluation. While our FreeNoise is effective in most aspects, it cannot fully address some of the inherent defects in the base T2V model. The \"snapping back\" phenomenon is also present in the base model (without FreeNoise) if the subject moves off the screen because the text prompt restricts that there must be an on-screen subject existing (e.x. the description that \"a running horse\" does not hold if the horse disappears). In real videos, it is common that the lens moves with the subject. When the subject moves off the screen, usually a new clip with a new scene will appear. Consequently, the T2V model lacks data supervision to learn how to let the subject come back naturally. In demonstrated examples shown by recent state-of-the-art EMU VIDEO[1], most movements are also described as \"the lens moving with the subject\". \n\nIn the current stage, a reasonable solution to the \"snapping back\" phenomenon is generating a new with a new scene after the subject moves off the screen. It can be achieved by either (1) Instant switching: manually switching to a new video clip or (2) gradient transition: sampling new noise and using the new prompt for the frames after the subject moves off the screen. \n\nWe hope this explanation provides clarity on the observed phenomenon. We sincerely appreciate your insightful suggestions and recognition of our paper. We also look forward to seeing more advanced video models developed to apply FreeNoise in the future. \n\n\n[1] EMU VIDEO: Factorizing Text-to-Video Generation by Explicit Image Conditioning"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission611/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545974904,
                "cdate": 1700545974904,
                "tmdate": 1700546134992,
                "mdate": 1700546134992,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TFV7NkJIg4",
            "forum": "ijoqFqSC7p",
            "replyto": "ijoqFqSC7p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission611/Reviewer_C7UC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission611/Reviewer_C7UC"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to extend the generative capabilities of pre-trained video diffusion models without incurring significant computational costs. It contains three different components, window-based temporal attention, noise rescheduling, motion injection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method is cleverly represented and very easy to follow.\n2. The proposed method is much more efficient than the baseline method Gen-L [1].\n3. The observation and Analysis part is well-designed and inspiring, and I appreciate this section.\n\n\n\n[1] Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising"
                },
                "weaknesses": {
                    "value": "1. Demo quality. From the demos, we can see that the motions of most generated results are restricted. For example in the ablation study, when the horse running, the background and the position of the horse do not change (even though the legs are moved), which is not a reasonable motion. Therefore, I would say that the proposed method corrupts the motions of the original diffusion models.\n\n2. Many ideas of the paper are used in previous works already. (1) For noise rescheduling, Reuse and Diffuse [1] proposed to reuse previous noise in the later frames. (2) For window-based temporal attention, Align Your Latents [2] applies a similar idea for long video generation, which does not change the temp conv part, but uses sliding local attention to resue the trained temporal attention. I think there's no intrinsic difference. (3) The motion injection part: interpolating the context is already proposed in Gen-L [3].\n\n3. The author says they picked only 100 prompts for quantitative experiments and then generated 2400 videos. This statement seems not explicit.\n\n\n[1] Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation\n[2] Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models\n[3] Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising"
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission611/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission611/Reviewer_C7UC",
                        "ICLR.cc/2024/Conference/Submission611/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission611/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698591769257,
            "cdate": 1698591769257,
            "tmdate": 1700648658340,
            "mdate": 1700648658340,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OgQac805Q4",
                "forum": "ijoqFqSC7p",
                "replyto": "TFV7NkJIg4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission611/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission611/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer C7UC"
                    },
                    "comment": {
                        "value": "Thanks for your valuable comments. We summarize and answer your questions below.\n\n> **Q1: Demo quality: motions of most generated results are restricted**\n\nIn some cases, the movement of the subject is limited due to the reused noises of FreeNoise. However, FreeNoise does not obliterate motion variation or thoroughly fix the spatial structure of objects. Please refer to the common response for more details.\n \n> **Q2: Many ideas of the paper are used in previous works already.**\n\n(1) Reusing previous noise in later frames (Reuse and Diffuse [1]): [1] extends the noises by simply adding new noises to the reused noises which is significantly different with our noise rescheduling. In practice, the noise reusing scheme of [1] is infeasible to our proposed method because newly added noises will hurt content consistency across frames. Besides, [1] can not be regarded as a previous work since its first submission to arXiv was on **Sept 7th** and the submission date of ICLR is **Sept 28th**.  \n\n(2) Sliding local attention (Align Your Latent [2]): this is a commonly used strategy, which can not address the longer video generation itself, as illustrated in Figure 1 (d). In Table 1 and Figure 4, we treat it as one baseline and our results are significantly better than it. In FreeNoise, we divide the whole clips into several overlapped windows with a fixed stride. When the stride = 1, it degenerates into naive sliding attention. However, stride = 1 means that we can not conduct noise rescheduling, causing repetitive results. Therefore, we need to use window-based attention (stride > 1), which is then combined with our proposed weighted fusion of local window attention and noise rescheduling, making FreeNoise effective and non-trivial.\n\n(3) Context interpolation (Gen-L [3]): Simply applying textual interpolation is infeasible to achieve multi-prompt video generation. Our contribution lies in the novel insight and thorough exploration of how to make it work, i.e. using a basic prompt through certain layers and timesteps, then injecting the target prompt for the rest.\n\n> **Q3: The experiment statement is unclear: 2400 videos are generated from 100 prompts for quantitative evaluation**\n\nSorry for the confusion. Each prompt is sampled 6 times with different random noises. For calculating the FVD and KVD between longer generated videos (64 frames) and normal inference (16 frames), we cut the long video into 4 segments with 16 frames. Therefore, a total of 2400 videos are used for evaluation for each method.\nWe have clarified this statement in the revision. In addition, following the suggestion of Reviewer pTfs, we now use 512 prompts from a standard evaluation paper EvalCrafter [4] to rerun the evaluation. Each prompt is sampled four times with different initial noises and a total of 2048 videos are generated for each inference method.\n\n[1] Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation  \n[2] Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models   \n[3] Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising  \n[4] EvalCrafter: Benchmarking and Evaluating Large Video Generation Models"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission611/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302069853,
                "cdate": 1700302069853,
                "tmdate": 1700302069853,
                "mdate": 1700302069853,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wu82nhNHab",
                "forum": "ijoqFqSC7p",
                "replyto": "TFV7NkJIg4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission611/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission611/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Reply"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your great efforts in reviewing this paper. Your constructive advice and valuable comments really help improve our paper. Considering the approaching deadline, please, let us know if you have follow-up concerns. We sincerely hope you can consider our reply in your assessment, and we can further address unclear explanations and remaining concerns if any.\n\nOnce more, we appreciate the time and effort you've dedicated to our paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission611/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547030243,
                "cdate": 1700547030243,
                "tmdate": 1700547030243,
                "mdate": 1700547030243,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1INo7cwf2D",
                "forum": "ijoqFqSC7p",
                "replyto": "TFV7NkJIg4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission611/Reviewer_C7UC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission611/Reviewer_C7UC"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback.  \nHere is my concern,\n\n1. About sliding local attention in align your latents, the videos presented in [align-your-latents](https://research.nvidia.com/labs/toronto-ai/VideoLDM/) has much better stability and visual quality in adjacent frames than the sliding baseline proposed in this paper. So I think the author wrongly implement the sliding baseline and not achieves the fair comparison result.\n\n2.  The authors changed their base model for several times. Previously, they said they use the baseline of VideoLDM. Then, they changed the base model in paper into VideoCrafter. While the experimental results remain the same, making the experimental results not reliable.\n\n3. The author said simple interpolation of text embedding is infeasible to achieve multi-text conditioned generation, which seems to deliberately belittles the results of Gen-L. We could see from its project page that it do achieve smooth contexts transition in some videos. \n\nTherefore, I think the author didn't strictly and fairly implement the baseline comparisons, making the whole experimental results not reliable.\n\nI decide to degrade my rate to reject temporally."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission611/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648630355,
                "cdate": 1700648630355,
                "tmdate": 1700648675584,
                "mdate": 1700648675584,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "louOef1EG3",
            "forum": "ijoqFqSC7p",
            "replyto": "ijoqFqSC7p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission611/Reviewer_pTfs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission611/Reviewer_pTfs"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to unlock the long video generation ability of a pretrained text-to-video generation model. The major technical components of this method include: 1. The analysis of artifacts and causes when generating long videos. 2. A noise schedule for long video generation. 3. Windowed attention fusion to keep attention perception field while avoiding content jump between windows, 4. Motion injection for varied textual prompts. The experiments show FreeNoise is a competitive long video generator."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This is a technically solid paper. Long video generation is a tricky long-standing problem. The authors propose a series of insights and techniques that have sufficient novelty to address the difficulties:\n1.\tThe analysis of long video artifacts and causes is valuable for developing better video generators. \n2.\tThe noise scheduling and window-based attention fusion address the long video difficulties mentioned in the analysis. They are simple yet effective. Window-based attention fusion addresses the notorious content jump problem, which will likely help develop future video generation foundation models.\n3.\tFreeNoise does not require additional UNet forward propagation. Therefore, the inference cost overshoot is low. \n4.\tThe qualitative results are marvelous. In human preference evaluation, FreeNoise still achieves the best. The authors provide an anonymous website to show more visual results. The image definition and motion consistency of FreeNoise are both good.\n5.\tThe motion injection technique successfully preserves video contents and drives the video to follow a new text prompt.\n6.\tThe qualitative ablations show each technical component of FreeNoise is effective and important."
                },
                "weaknesses": {
                    "value": "Major concerns: \n1.\tI\u2019m interested in detailed experiment settings. Please include diffusion sampler configurations, sample resolutions, frame stride, etc. in your future revision.\n2.\tPlease add a pipeline figure. It is not very easy to fully understand how and where FreeNoise is working on generating long videos.\n3.\tIs direct inference, sliding, GenL, and FreeNoise sharing the same pretrained text-to-video model? If it is, then the evaluation is very convincing since they can all generate the same short video using the prompts but only FreeNoise can achieve good long video results.\n\nMinor concerns:\n1.\tPage 7. In the second line. A full stop is missing before \u2018Obviously\u2019.\n2.\tIn which case FreeNoise may fail? A discussion over the limitations is welcomed.\n3.\t100 evaluation prompts have limited diversity. If it is feasible, please add more evaluation prompts to make the comparison more convincing."
                },
                "questions": {
                    "value": "1.\tThe authors claim FreeNoise achieves the maximum long video of 512 frames due to GPU memory limit. Is it possible to unlock even long video generation ability by using the CPU offload technique? \n2.\tWith the help of ControlNet, is it possible to generate more diverse motion with FreeNoise?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission611/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698630899771,
            "cdate": 1698630899771,
            "tmdate": 1699635988667,
            "mdate": 1699635988667,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ViBsGn3kP1",
                "forum": "ijoqFqSC7p",
                "replyto": "louOef1EG3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission611/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission611/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pTfs"
                    },
                    "comment": {
                        "value": "Thanks for your valuable comments. We summarize and answer your questions below.\n\n> **Q1: Detailed experiment settings**\n\nThanks for your advice. We have added more experiment details in our revised paper. Due to space limitations, we have moved this part to the appendix.\n\n> **Q2: Are direct inference, sliding, GenL, and FreeNoise sharing the same pretrained text-to-video model?**\n\nYes. All the methods are evaluated with the same pre-trained T2V model. In addition, their inference parameters (e.x. DDIM steps, scale of classifier-free guidance) are all consistent except for the design of the method itself.\n\n> **Q3: In which case FreeNoise may fail? A discussion over the limitations is welcomed.**\n\nFreeNoise has certain limitations in introducing new content along with the frame length increasing because repeated noises are used. Besides, FreeNoise is a tuning-free approach, it inherits the video generation quality from the base T2V model while can not address the originally existing failure cases.\n\n> **Q4: More evaluation prompts for comparison**\n\nThanks for your advice. We use 512 prompts from a standard evaluation paper EvalCrafter [1] to rerun the evaluation. Each prompt is sampled four times with different initial noises and totally 2048 videos are generated for each inference method. The updated results are provided in the revised paper (Table 1).\n\n> **Q5: Unlock even longer video generation by using CPU offload technique**\n\nIt is feasible but the inference time is crazy for the CPU. In the future, we will explore some technologies to reduce computation [2] and that will be slightly friendly to the CPU.\n\n> **Q6: With the help of ControlNet, is it possible to generate more diverse motion with FreeNoise?**\n  \nThis is an interesting idea. We have added a depth2video demo at the anonymous website: https://free-noise.github.io/ (B6: Depth2Video). FreeNoise works for ControlNet and additional condition helps to generate more diverse motions. However, naively applying FreeNoise with ControlNet can not work perfectly, because the frame-wise variated depth conditions introduce extra variation to the context. It requires further exploration to make this combination work properly.\n\n[1] EvalCrafter: Benchmarking and Evaluating Large Video Generation Models   \n[2] LCM-LoRA: A Universal Stable-Diffusion Acceleration Module"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission611/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301706405,
                "cdate": 1700301706405,
                "tmdate": 1700301833988,
                "mdate": 1700301833988,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xc8vcjOw1j",
                "forum": "ijoqFqSC7p",
                "replyto": "louOef1EG3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission611/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission611/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Reply"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your great efforts in reviewing this paper. Your constructive advice and valuable comments really help improve our paper. Considering the approaching deadline, please, let us know if you have follow-up concerns. We sincerely hope you can consider our reply in your assessment, and we can further address unclear explanations and remaining concerns if any.\n\nOnce more, we appreciate the time and effort you've dedicated to our paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission611/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546990947,
                "cdate": 1700546990947,
                "tmdate": 1700546990947,
                "mdate": 1700546990947,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rW1zw5AFeg",
            "forum": "ijoqFqSC7p",
            "replyto": "ijoqFqSC7p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission611/Reviewer_q7Nz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission611/Reviewer_q7Nz"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the effect of initial noise on a diffusion model for video generation, and thus proposes a method to extend the ability of a pre-trained model to generate long videos without fine-tuning, by rescheduling the initial noise of the video frame and by using a window-based temporal attention to achieve long-range visual consistency. finally, a new method of injecting motion trajectories is proposed, which allows the model to generate videos in response to multiple text prompt."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The proposed method is simple and effective to expand the model's ability to generate long videos without fine-tuning the model.\n\n2) The use of noise reschedule and window-based attention fusion allows for more consistent video generation.\n\n3) Motion inject allows the model to be fed with a variety of text prompts to generate longer videos with richer meanings."
                },
                "weaknesses": {
                    "value": "1) The method described in this paper lacks suitable diagrams to help illustrate it.\n\n2) The proposed NOISE RESCHEDULING may limite the content variances of video generation since longer videos are produced by repeating the noises for the short ones. As is shown by the examples, the generated long videos looks like a short video that loops multiple times. I wonder whether this way can produce authentic long videos that contain continous various motions."
                },
                "questions": {
                    "value": "Try adding more diagrams to better explain the methods in the article, such as noise rescheduling and an overview of the pipeline for generating a video using the methods mentioned in the article."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission611/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727719969,
            "cdate": 1698727719969,
            "tmdate": 1699635988580,
            "mdate": 1699635988580,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4zBvauIpnu",
                "forum": "ijoqFqSC7p",
                "replyto": "rW1zw5AFeg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission611/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission611/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer q7Nz"
                    },
                    "comment": {
                        "value": "Thanks for your valuable comments. For your raised concerns, please refer to our responses to the \"Common Response\" and \"Ability to Generate Videos with Significant Movement\". And we have added the pipeline diagram to help understand FreeNoise in the first revision (Figure 3)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission611/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700300843105,
                "cdate": 1700300843105,
                "tmdate": 1700300843105,
                "mdate": 1700300843105,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]