[
    {
        "title": "VQ-TR: Vector Quantized Attention for Time Series Forecasting"
    },
    {
        "review": {
            "id": "Qrz0pREYWk",
            "forum": "IxpTsFS7mh",
            "replyto": "IxpTsFS7mh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4792/Reviewer_9deP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4792/Reviewer_9deP"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript proposes a transformer architecture for time series forecasting, that uses vector quantization to scale the encoder sequence size, resulting in faster training time, less memory usage, and time series forecasting results from multiple metrics."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "To adopt vector quantization in Transformer based models for time series forecasting is novel to the best of my knowledge. \nThe manuscript is overall well and clearly written.\nThe proposed method has the potential to benefit the time series modelling community."
                },
                "weaknesses": {
                    "value": "- From Fig. 3, the influence of the codebook size J to the VQ-TR seems not stable. A more detailed discussion on this might help potential users to select this hyperparameter when using the model.\n\n - The manuscript claims that\n>...methods based on Transformers (Vaswani et al., 2017) have dominated the state-of-the-art, outperforming both classical autoregressive approaches, as well as deep learning approaches using Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs).\n\n   However, in e.g. [1], which is not a Transformer based model, results of CRPS are better than the ones reported in the manuscript.\\\n   A discussion (or comparison) on [1] might help to promote the contribution of the manuscript.\n\n - The manuscript assumes that the distribution of the predicted time point follows a (conditional) Gaussian distribution. Is there any theoretical or empirical evidence supporting this assumption? In e.g. [2], the paper also proposes a model for probabilistic time series forecasting, with a more solid assumption that the frequencies follow Gaussian distribution. A discussion on this might help to understand this Gaussian assumption.\n\n - In Sec. 4.2, the use of ```\\citet{}``` and ```\\citep{}``` needs to be carefully handled. \n\n\n\n\n---\n[1] Kashif Rasul et al. Multivariate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows, ICLR 2021\\\n[2] Zhongjie Yu et al. Predictive Whittle networks for time series. UAI 2022"
                },
                "questions": {
                    "value": "- Is it true that the model as proposed in the manuscript, works for univariate time series only?\n\n - Can you elaborate the influence of different codebook sizes?\n\n - How is the context window length C selected?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4792/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4792/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4792/Reviewer_9deP"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4792/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698531079887,
            "cdate": 1698531079887,
            "tmdate": 1699636461785,
            "mdate": 1699636461785,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fNq93BPQqB",
                "forum": "IxpTsFS7mh",
                "replyto": "Qrz0pREYWk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4792/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4792/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9deP"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper, for your insightful comments, and for acknowledging the novelty and benefit of our methodology! We will fix the formatting issues you noted regarding citations, and we provide detailed responses to your questions and criticisms below.\n\n# Influence of Codebook Size\n\nPlease see our general comments to all reviewers (titled: \u201cResponse RE Codebook Sizes\u201d) for a very detailed discussion about the influence of codebook size, and the stability of this effect. In summary, the main takeaway from the paper should be that for the most part performance is fairly robust to codebook size, with changes in mean performance due to varying codebook size that are very small in absolute value. However, we do see evidence that in some cases, especially for datasets where the underlying structure is simpler and strong regularization is needed (i.e. Traffic and Taxi), there is an empirical benefit of using especially small codebook sizes. We will add additional discussions to the final paper to make the influence of changing the codebook size more clear.\n\n# Assumption of Gaussian Model\n\nTo clarify, we do *not* actually assume that the time series data is conditionally Gaussian. In section 2.1 we use conditional Gaussian distributions as an example of how we might define a concrete probabilistic model for time series that is parameterized by transformers, but as mentioned there this is only an example. In practice, we can use our VQ-TR methodology with any distribution class for the emission heads. In other words, the user specifies some parametric class of distributions (e.g. Gaussian, student-t, negative binomial, etc.), which is what we call the \u201cemission head\u201d, and the VQ-TR model predicts the distribution parameters (the shape and number of these parameters depends on the user\u2019s chosen class) of the next data point. Since these predicted parameters index a class of probability distributions, this corresponds to predicting the distribution of the next data point. In practice, in our experiments, we use VQ-TR with either implicit quantile, student-t, or negative binomial emission heads; see our response to reviewer XX8d for our rationale of deciding which distribution was used on which dataset. In all cases, this is a modeling choice that of course won\u2019t be exactly correct, but we believe these are flexible enough choices to learn good probabilistic models (as evidenced by our empirical results). Also, note that in no cases do we actually use Gaussian distributions with VQ-TR in our experiments. We will make this more clear in our final paper.\n\n# Reported Performance in Our Results versus in Literature\n\nIn short, the CRPS metric you are most likely referring to in [1] is different from the CRPS metric we are computing in our paper; the former takes smaller values than the latter. This is a common misunderstanding in the literature, and we appreciate the opportunity to clarify.\n\nIn more explicit detail, note that [1] is a multivariate model, that learns the full joint distribution of all time series together; the results you most likely refer to are in terms of the CRPS-SUM metric, which is different from the univariate metrics CRPS. Importantly for comparing these works, CRPS-SUM is smaller than the corresponding CRPS metric. The appendix of [1] contains the CRPS table where one observes a recurring pattern in time series research, namely that the univariate models (i.e. all those considered in our paper) typically perform better than their multivariate counterparts for the smallish datasets being used in scientific research. This phenomenon can also be observed in the DLinear or PatchTST and similar papers where even a simple linear model is shown to be better than transformer-based models, although this point of multivariate-vs-univariate is not made clear in those works either.\n\n# VQ-TR with Univariate versus Multivariate Time Series\n\nThere is no reason why the VQ-TR architecture cannot be used for multivariate time series modeling. That being said, as discussed above, it is known that doing separate univariate time series modeling for each time series tends to outperform multivariate approaches for the kinds of smallish datasets used in scientific research, so therefore our implementation of VQ-TR used in our experiments is univariate (as with all baselines compared against.)\n\n# How is Context Length Chosen\n\nSee our general response to all reviewers for detail about this (titled: \u201cResponse RE Context Window Sizes\u201d). In short, the context window is a hyperparameter that is optimized on a per method/dataset basis."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700163001270,
                "cdate": 1700163001270,
                "tmdate": 1700163001270,
                "mdate": 1700163001270,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jjcZ3GlchP",
                "forum": "IxpTsFS7mh",
                "replyto": "fNq93BPQqB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4792/Reviewer_9deP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4792/Reviewer_9deP"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the answer to my questions, and most of my questions are addressed.\\\nMy remaining questions are: \n1) In my humble opinion, if conditional Gaussian is in the end not used, then using it as an example is a bit misleading.\n2) A more precise description of the multivariate time series question is, when the current version of VQ-TR is used for general multivariate time series, does it model each time series independently, or jointly?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508890786,
                "cdate": 1700508890786,
                "tmdate": 1700508890786,
                "mdate": 1700508890786,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X9hVm2djNG",
                "forum": "IxpTsFS7mh",
                "replyto": "oDjuUps0eq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4792/Reviewer_9deP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4792/Reviewer_9deP"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the follow-up answer. I am keeping my positive score for now and can increase the score when I see an updated revision that addressing the issues."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602041330,
                "cdate": 1700602041330,
                "tmdate": 1700602041330,
                "mdate": 1700602041330,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5LuMjBYV15",
            "forum": "IxpTsFS7mh",
            "replyto": "IxpTsFS7mh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4792/Reviewer_JnRg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4792/Reviewer_JnRg"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the challenges in probabilistic time series forecasting, which involve long sequences, and the need for a large number of samples for accurate inference. The paper highlights that current state-of-the-art methods based on Transformers are computationally inefficient due to their quadratic complexity in sequence length and primarily focus on non-probabilistic point estimation. To overcome these limitations, the paper introduces a novel transformer architecture called VQ-TR. VQ-TR utilizes a discrete set of latent representations in the Attention module, enabling linear scaling with the sequence length and providing effective regularization to prevent overfitting. The authors compare several Transformer-based time series forecasting methods for probabilistic forecasting, and VQ-TR demonstrates competitive performance in forecasting accuracy, computational efficiency, and memory usage."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The manuscript is well-written, with clear and easily understandable ideas. The paper offers a thorough evaluation, encompassing multiple baseline models across various benchmark datasets. It effectively illustrates the impact of the VQ module, not only in terms of performance enhancement but also in computational and memory efficiency."
                },
                "weaknesses": {
                    "value": "The paper suffers from a significant lack of novelty, as its approach closely mirrors that of VQ-AR by Rasul et al. (2022). Essentially, the method merely replaces the RNN with a Transformer model. Given this redundant contribution, the paper falls short of meeting the standards expected for a conference like ICLR and might be better suited as a workshop paper.\n\nFurthermore, in Figure 3, the analysis of how codebook size affects overall performance, fails to provide a clear conclusion or discernible trends regarding the impact of codebook size. It appears that determining the correct size is critical, and achieving the desired performance may require a careful HP tuning for a given dataset."
                },
                "questions": {
                    "value": "How many attempts were done to find the right codebook size for the presented experiments? Adding the computing costs of tuning the codebook size, how well VQ-TR compares with methods that provides comparable performance, when trained using default settings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4792/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4792/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4792/Reviewer_JnRg"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4792/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768035617,
            "cdate": 1698768035617,
            "tmdate": 1700692045182,
            "mdate": 1700692045182,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EUdxidYkVF",
                "forum": "IxpTsFS7mh",
                "replyto": "5LuMjBYV15",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4792/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4792/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JnRg"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper, for your thoughtful feedback, and for acknowledging our thoroughness in demonstrating the empirical benefit of VQ-TR. We provide detailed comments to your major criticisms and questions below.\n\n# Novelty of our Methodology\n\nWe would like to very respectfully, but firmly, push back against the assertion that our methodology is only a minor variation on existing methods. We provide a detailed discussion about this in our general response to all reviewers  (titled: \u201cResponse RE VQ-TR vs VQ-AR\u201d), but in summary, this assertion is not correct. As explained there, replacing the RNN in VQ-AR with a transformer would not give VQ-TR, and would not change the \u201cquadratic in context length\u201d transformer complexity. Rather, VQ-TR is a different methodology, based on a novel way of placing the VQ module inside the transformer (rather than on the transformer outputs, which is what a simple extension of VQ-AR to transformers would do). Given this, we strongly believe that our work contains strong methodological novelty appropriate for ML conference publication. However, we apologize that this novelty was as not clear as it could have been, and we will make this more clear in our final paper.\n\n# Impact of Codebook Size on Cost and Performance\n\nRegarding your points about the importance of codebook size on performance and the cost of codebook size optimization, we provide a very detailed discussion of this in our general response to all reviewers (titled: \u201cResponse RE Codebook Sizes\u201d). To summarize the discussion there, although at first glance Figure 3 may make it look like VQ-TR is sensitive to the codebook size, it is really not. Both the observed mean effects of changing the codebook sizes, and the variance of performance, are very small in absolute terms, especially when compared with the difference in performance between methods. In other words, one of the key takeaways from Figure 3 should be that VQ-TR is *not* overly sensitive to codebook size.\n\nDespite this lack of sensitivity, however, we agree with you that if one wants to fully optimize performance, then it is relevant to consider the computational impact of codebook size. We provide some detail in our general response to all reviewers about the effect of codebook size on computational cost. In short, increasing codebook size does not increase the cost overly much, and even increasing it to very large values beyond what is needed for good performance results in a computational cost that is at worst on par with other methods. Since, as evidenced by Figure 3, we can always obtain (approximately) optimal performance with relatively small codebook sizes, our results in Figure 2 should be fairly reflective of the actual computational cost of codebook size optimization in practice. However, in order to be extra thorough, we will include in the appendix a similar plot as in Figure 2 for a wide range of codebook sizes, and we will add an extra discussion along these lines.\n\nRegarding the questions about how many hyperparameter configurations we tried for VQ-TR, and the cost of optimizing VQ-TR versus running baselines with default values, we note that for *all* methods in our main empirical evaluation we performed hyperparameter optimization over a wide range of values for all important hyperparameters on a per-dataset basis (which included the codebook size in the case of VQ-TR). While of course, it would be possible to compare the computational cost of VQ-TR with full hyperparameter optimization versus the computational costs of baselines with no hyperparameter optimization, this seems unfair and inappropriate, as this is not how their forecasting performance was compared. Although of course getting the most performance out of VQ-TR requires some hyperparameter tuning, this is also true for all other methods, and in all cases, the computational cost is inflated by the number of hyperparameter configurations used. Furthermore, as discussed above, and contrary to the premise of this question, VQ-TR is *not* very sensitive to the exact codebook size as long as it is within some range of values that seems generically reasonable for all datasets that we investigated (e.g. between 10 and 100).\n\nFinally, regarding your comment about the lack of clear trends in the codebook size comparison, while we agree that there aren\u2019t extremely strong trends that are consistent across every dataset, we do see some interesting trends in some datasets. We provide a more detailed discussion about this in our response to all reviewers, and make the case more explicitly that the performance trend in the Taxi and Traffic datasets is interesting, and indeed evidence of a beneficial regularizing effect of VQ. We will make our explanation of this trend more clear in the final paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700162870684,
                "cdate": 1700162870684,
                "tmdate": 1700162870684,
                "mdate": 1700162870684,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "thJhXyFxoo",
                "forum": "IxpTsFS7mh",
                "replyto": "EUdxidYkVF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4792/Reviewer_JnRg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4792/Reviewer_JnRg"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors for addressing my concerns with detailed explanations. Although incorporating the VQ module directly into the transformer enhances computational efficiency, the proposed method and its motivations are still largely influenced by VQ-AR. Additionally, I agree with other reviewers that the paper could be interesting for the broader machine learning community, not just in time series forecasting. Therefore, I am changing my score to positive."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691960397,
                "cdate": 1700691960397,
                "tmdate": 1700691960397,
                "mdate": 1700691960397,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IB7DrZrH1I",
            "forum": "IxpTsFS7mh",
            "replyto": "IxpTsFS7mh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4792/Reviewer_XX8d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4792/Reviewer_XX8d"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces VQ-TR, combining vector quantization and the transformer architecture for time series forecasting. Specifically, vector quantization limits query vectors to be one of a fixed size codebook, which implies that any subsequent cross/self-attention computation scales with the codebook size instead of the context length. A masked attention decoder is used to 'unroll' the forecast. Apart from favorable computational properties, the method appears to clearly outperform the SoTA in transformer-based point and probabilistic forecasting."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper proposes a simple and highly practical tool for forecasting, and helps resolve an ongoing discussion in the forecasting community about the use of transformers by positing that a 'quantized' small-scale transformer already outperforms many of the current architectures in use.\n\nThe extensive experimental results posted are themselves of interest to the community and the work can potentially benefit other areas where transformers are often applied."
                },
                "weaknesses": {
                    "value": "Motivation, exposition and the key idea follow closely from VQ-AR. This is however justified since the computational benefits to the transformer architecture are stronger. However, the contrast could be made clearer in the paper.\n\nAlso, key design decisions about experiments including number of layers, codebook size used to produce the tables are currently only available in the appendix. Please consider moving these important details forward.\n\nImportantly, the experiment presentation is somewhat misleading. The main paper only compares against transformer baselines where the proposed method appears to categorically outperform the SoTA, however the non-transformer based baselines are moved to the appendix where results are mixed against non-transformer baselines; and less competitive than what the main paper Section 4.4 claims. \n\nA last open point that should be addressed is the choice of the distribution output per dataset. How was this choice made for the baselines? The fact that -iqn and -t survive for VQ-TR in wiki and taxi respectively invites the question how these hyperparameters were selected."
                },
                "questions": {
                    "value": "- Why does the model have higher runtime than TFT? Despite the comment that similar batch sizes were used, one would expect that the quantized architecture still outperforms TFT. \n- In the appendix the authors write: \"Note that we can afford to use a longer context length of C = 20 \u00d7 P for VQ-TR due to its memory efficiency, as noted in Figure 2.\" Was the context length fixed across models or did VQ-TR get a head start?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4792/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4792/Reviewer_XX8d",
                        "ICLR.cc/2024/Conference/Submission4792/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4792/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779999974,
            "cdate": 1698779999974,
            "tmdate": 1700170460151,
            "mdate": 1700170460151,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "urNwSzw31T",
                "forum": "IxpTsFS7mh",
                "replyto": "IB7DrZrH1I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4792/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4792/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XX8d"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper, and for your helpful comments. We especially appreciate you for recognizing the practical nature of our contribution, as well as the extensiveness of our comparison! Indeed, we believe that we have gone above and beyond to provide an empirical comparison of probabilistic time series forecasting performance for modern methodologies that is far more extensive than anything else we are familiar with in the literature. We provide specific responses to most of your criticisms and questions below. For our response to your comment about the closeness of our methodology with VQ-AR, and about what context windows were used in experiments, please see our general responses to all reviewers (titled: \u201cResponse RE VQ-TR vs VQ-AR\u201d and \u201cResponse RE Context Window Sizes\u201d).\n\n# Experimental Details in Appendix\n\nWe appreciate your feedback about this issue. While we believe that it is common practice for ML conference papers to provide many of these kinds of details in the appendix, we agree that it would make the paper more readable to provide some of this info in the main paper, such as e.g. the codebook size used for our computational comparison in Figure 2. We will make some adjustments along these lines.\n\n# Presentation of Experiment Results\n\nWhile it is true that the results for VQ-TR seem slightly more mixed in Table 3 than Table 1, for the most part (with one notable exception described below) the same kinds of trends are present. In more detail:\n\n1. On Electricity, Traffic, and Taxi, VQ-TR clearly outperforms other methods\n\n2. On Solar and Wikipedia, VQ-TR is roughly on par with the best performing methods, with a tiny difference in each metric versus the best performing method\n\n3. On Exchange, it is true that some of the simpler benchmarks (e.g. ETS) clearly outperform VQ-TR, but this also holds for all transformer-based methods in Table 1. This is because the Exchange dataset is a time series of currency exchange rates, which is more or less a random walk with extremely little predictive information. Therefore, it is difficult to outperform simple classical methods like ETS on this kind of dataset. Despite this, we do believe it is significant that, although transformer-based approaches do not seem to be ideal for datasets like Exchange, VQ-TR still outperforms the other transformer-based methods here (which we believe also validates our claim about the regularizing benefit of vector quantization!)\n\nIt was not our intention to hide less favorable results in the appendix, rather for space reasons we decided to focus in the main paper on the most relevant comparisons against other transformer-based approaches. That being said, with the exception of the Exchange dataset, as explained above, the same kinds of trends that hold in Table 1 also hold in Table 3, and we believe that what we presented in the main paper is accurate and not misleading. However, we understand that the performance on Exchange requires some extra explanation and context, which we will add.\n\n# Choice of Output Distributions\n\nIn order to have a more direct one-to-one comparison versus VQ-AR, for each dataset we used the same kind of distribution head for VQ-TR as the best performing distribution head of VQ-AR. In general, we used -t for datasets with continuous data, and -nb for datasets with count based data, with the exception that we used -iqn for Solar and Wikipedia (because this is what performed best for VQ-AR). In addition, we used different distribution heads for different datasets in order to highlight and reinforce the fact that the output head can be changed within the VQ-TR framework. Of course, one could perform explicit hyperparameter optimization on the choice of distribution head (which we did not do for the above reasons), and if we did this the performance of VQ-TR would potentially be even better. We will clarify this.\n\nIn addition, for the Taxi dataset VQ-TR used a negative binomial head (same as VQ-AR), the -t suffix for Taxi is a typo that we will fix for the final paper.\n\n# Why does TFT have a Shorter Running Time\n\nThe reason for this is that, unlike the other transformer-based methods, TFT by design only has a single self-attention module. Therefore, hyperparameter changes to increase the complexity/capacity of the model can only go towards layers outside of the attention mechanism, so no matter what hyperparameters are chosen it is always relatively fast compared with the other transformer-based baselines. That being said, it still has quadratic in context length scaling. Note that our comparison in Figure 2 does not show the asymptotic scaling of computational cost as context length increases, rather we show what the computation cost looks like in practice for a reasonably typical context length on this dataset. Also, it should be noted that although TFT is often fast in practice, we find that it performs relatively poorly on many datasets."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700162631289,
                "cdate": 1700162631289,
                "tmdate": 1700162631289,
                "mdate": 1700162631289,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4vkFkdKSy1",
                "forum": "IxpTsFS7mh",
                "replyto": "urNwSzw31T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4792/Reviewer_XX8d"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4792/Reviewer_XX8d"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your detailed comments. I concur with most points. I stand by my original statement of \"motivation, exposition and the key idea follow closely from VQ-AR\" but that the computational benefits are much more significant in the proposed architecture. I would also still urge the authors to discuss the nature of experimental results clearly in the main paper, that the results can be mixed based on the nature of the dataset. \n\nThe authors have sufficiently addressed my concerns. I believe the potential benefits of the proposed method are far reaching and the paper is of interest to the ML community also outside time series forecasting. I will therefore revise my review."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4792/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700170383441,
                "cdate": 1700170383441,
                "tmdate": 1700170383441,
                "mdate": 1700170383441,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]