[
    {
        "title": "Global minima, recoverability thresholds, and higher-order structure in GNNs"
    },
    {
        "review": {
            "id": "s8M1z9LsMI",
            "forum": "qqDeICpLFo",
            "replyto": "qqDeICpLFo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4027/Reviewer_QsGF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4027/Reviewer_QsGF"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the performance of Graph Neural Networks (GNN) from three perspective on cSBM datasets.\nFirst, the authors analyze the accuracy performance of GCN on cSBM datasets.\nThen, the authors study the empirical performance of four classic GNNs on cSBM datasets with varied properties.\nFinally, a discussion of the impact of high-order structure on GNNs is included."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The accuracy analysis on cSBM datasets reveal the cases when GCNs perform better, and why GCNs achieve optimality on cSBM graphs.\n2. The empirical discussion over GNNs' performance on different data regimes is inspiring.\n3. The paper is well written and can be clearly understood."
                },
                "weaknesses": {
                    "value": "1. Analysis into heterophily on cSBM datasets have been made in [1][2].\n2. The theoretical results are limited to 1-2 layer GCNs, and may not be inspiring enough for designing better GNNs.\n3. The empirical discussion is mostly restricted to 2-class cSBM datasets, so a gap exists between empirical results and real-world scenarios.\n4. It would be better if more explanation about how the empirical results are affected by the GNN architecture is provided.\n\n[1] Chien, Eli, et al. \"Adaptive Universal Generalized PageRank Graph Neural Network.\" International Conference on Learning Representations. 2020.\n\n[2] Ma, Yao, et al. \"Is Homophily a Necessity for Graph Neural Networks?.\" International Conference on Learning Representations. 2021."
                },
                "questions": {
                    "value": "1. What's the physical meaning about Theorem 3? It would be better if more explanation is included.\n2. What's the relationship between theoretical and empirical results?\n3. Intuitively, the high-order structure will be affected by the one-hop structure. On a homophilic graph, its one-hop and high-order neighbors would both be homophilic.  \nHow to ensure the high-order impact is completely erased?\nDoes it suggest that designing high-order GNNs is meaningless?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4027/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4027/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4027/Reviewer_QsGF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4027/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698313379409,
            "cdate": 1698313379409,
            "tmdate": 1699636365618,
            "mdate": 1699636365618,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2eybqB83Ff",
                "forum": "qqDeICpLFo",
                "replyto": "s8M1z9LsMI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4027/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4027/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for reviewing, and we are glad you find our empirical results inspiring and that the paper was well written! We have carefully considered your comments and made several major improvements to the paper in response. We believe that the impact of the paper will be much greater because of your feedback. We note that revisions have been highlights in blue for the reviewers convenience.\n\n# Higher Order Structure\nYour comments about higher order structure were particularly interesting to us. First, you asked how we ensured that higher order structure is completely erased, particularly since homophily implies some sort of higher order structure automatically. This is true, and we actually had to clarify some of the language of our paper because this sort of higher order structure is impossible to erase while retaining homophily. Rather, the key point is that other higher order structures can be substantially erased, such as recurring motifs, degree-degree correlations, and spatially correlated edge structures. \n\nThe mechanism is a rewiring algorithm where the rewiring ensures that the number of edges between clusters and the degree of each node are respected, but everything else should be randomized. After enough swapping, the resulting graph is basically sampled from a cSBM, which doesn\u2019t have any higher-order structure beyond what is implied by the SBM parameters (e.g. homophily). \n\nAs far as we can tell, this does not mean that designing higher-order GNNs is meaningless, in fact maybe just the opposite. Higher order structure is present in data, and low-order GNNs cannot handle it well, but it seems probable that higher order GNNs might be designed that could specifically handle some of these structures (e.g. recurrent motifs) particularly well. This is, of course, the exact sort of question that we hope our work brings attention to.\n\n# Comments on Limitations\nWith regards to limitations, we agree that it would be great to handle deep GNNs, but already in the two-layer case, the nonlinearities are unfortunately quite formidable. Fortunately, the 2-layer case is quite commonly used, so we hope that our results are applicable. Similarly, we found the reviewer\u2019s comments about overemphasizing the 2-class empirical results prescient. Our solution has been to change our two figures that used to highlight two-class results to now focus on the more interesting multiclass case. The rest of the discussion section has been rebalanced accordingly to balance binary and multiclass results.\n\nWe have also added some additional intuition about Theorem 3 in a Remark. Basically, the accuracy estimate correctly combines the relative abundances of various local graph structures and the degree to which they are helpful.\n\n# Additional Points\nAnalysis into heterophily on cSBM datasets have been made in [1][2].\n- We now cite these sources in the appropriate place. In particular, our work agrees with (sometimes contradictory) prior works by showing that performance on heterophilic data sets can be good or bad depending on the strength of the heterophily and the signal strength from the features. We additionally contribute by presenting the accuracy transition in detail and considering a wider range of architectures (e.g. SAGE, Transformer) and cSBM distributions (e.g. degree correction).\n\nIt would be better if more explanation about how the empirical results are affected by the GNN architecture is provided.\n- Agreed. We have added some additional discussion to the main text and more substantial additional discussion to Appendix C2."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4027/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111320340,
                "cdate": 1700111320340,
                "tmdate": 1700111320340,
                "mdate": 1700111320340,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uItSylAlOE",
                "forum": "qqDeICpLFo",
                "replyto": "2eybqB83Ff",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4027/Reviewer_QsGF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4027/Reviewer_QsGF"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the authors' comprehensive responses and efforts in revising the manuscript.\n\nThe updated draft has partially addressed my concerns. Nevertheless, I find that the revised content still falls short in furnishing tangible insights for the enhancement of GNN architectures or in deepening our understanding of specific GNN components. While it is common to incorporate high-order information in current GNN models, the practical applicability of the presented theoretical results remains limited. Consequently, I have decided to keep my original score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4027/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621294402,
                "cdate": 1700621294402,
                "tmdate": 1700621294402,
                "mdate": 1700621294402,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HqFp0OFxql",
            "forum": "qqDeICpLFo",
            "replyto": "qqDeICpLFo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4027/Reviewer_yxXE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4027/Reviewer_yxXE"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the application of graph neural networks to the Contextual Stochastic Block Model (cSBM).\n\nFirst, the study begins by examining the accuracy of a single node in a single-layer (simple) graph convolutional network without non-linearities, conditioned on the graph structure.  Subsequently, the paper demonstrates that, under certain conditions (which are satisfied by the cSBM), the expected log-likelihood of a two-layer GCN with ReLU activation is lower-bounded by the expected log-likelihood without the ReLU activation. The accuracy for a two-layer (simple) graph convolutional network without non-linearities is also explored, and the formula for the accuracy conditioned on the graph structure is provided, specifically for a single node.\n\nThe paper concludes with an extensive set of numerical simulations, benchmarking various graph neural networks on the contextual stochastic block model and on other standard benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The examination of graph neural networks' performance on contextual stochastic block models is an important and ongoing area of research. The theoretical analysis employs some interesting techniques, including the use of symmetry. Additionally, the experiments are quite extensive and could offer motivation for future work,"
                },
                "weaknesses": {
                    "value": "Theoretical results in this paper appear to be somewhat limited, as they focus on computing accuracy at single nodes and rely on fixed GCNs without training. Stronger and more informative findings can already be found in the existing literature, where guarantees for the performance of trained GCNs have been provided (for example, see [1,2])\n\n[1] Wu et al. - A non-asymptotic analysis of oversmoothing in graph neural networks\n\n[2] Baranwal, - Graph convolution for semisupervised classification: Improved linear separability and out-of-distribution generalization"
                },
                "questions": {
                    "value": "- Regarding the notation used, it's unclear whether 'W' is meant to represent a row or column vector. If 'z_i' is also a vector, it appears there might be a dimension mismatch in the final equation on page 3.\n\n- Page 5, is the \"i\" in the equations \" ... [X](i)  \" a fixed a priori node i ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4027/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817147580,
            "cdate": 1698817147580,
            "tmdate": 1699636365522,
            "mdate": 1699636365522,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KgYYBXPk0y",
                "forum": "qqDeICpLFo",
                "replyto": "HqFp0OFxql",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4027/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4027/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your review, and we are glad that you noticed how we employed symmetry in a new way and found our numerical results interesting. We are sure that the paper is stronger with your feedback. We note that revisions have been highlights in blue for the reviewers convenience.\n\nWe are somewhat confused with regards to the limitations you see in our theoretical results and would appreciate any clarification you can offer. In short, it appears that you see weaknesses in that (1) we are focusing on the accuracy at single nodes and (2) we do not consider trained GNNs. With respect to the first objection, we perhaps should clarify that, particularly in Theorem 1 (formerly Theorem 2\u2013we renumbered), the accuracy is actually the generalization accuracy over the entire cSBM family, and similarly with Proposition 1, part 3, so we are a bit unclear on what is meant by focusing on single nodes. Secondly, with respect to trained GNNs, our results in (new) Theorem 1 apply to both trained and untrained GNNs, and we even identify the global minimizer and its accuracy. We do not address gradient descent or similar algorithms, but this does not seem to be the spirit of your comment.\n\nOur best guess for the cause of this disconnect is that our paper is fundamentally about non-linear graph models, whereas the papers you referenced in your review focus on linear models. Other reviewers noted various deficiencies in our presentation in that section that obscured our main point. While our paper does reference linear GNNs, the majority of our theory results (and all of our simulations) focus on the behavior of genuinely nonlinear models. We are sometimes able to bound the performance of the nonlinear models in terms of the corresponding linear models. We have substantially rewritten the theory section to more clearly reflect these facts\u2013apologies if the original submission was unclear. Hopefully that section is a much more enjoyable and enlightening read now. All of this is of course a guess, and we welcome clarification if we are off track.\n\nFinally, we note that we have clarified the final section on the detrimental impact of higher order graph structure on GNN performance, clarifying that this section is pointing out a fundamental weakness of current GNN architectures and pointing out possible directions for future improvements."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4027/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111310652,
                "cdate": 1700111310652,
                "tmdate": 1700111310652,
                "mdate": 1700111310652,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vIxJdi8gOV",
                "forum": "qqDeICpLFo",
                "replyto": "KgYYBXPk0y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4027/Reviewer_yxXE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4027/Reviewer_yxXE"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for taking the time to respond to my comments and for incorporating my suggestions into your manuscript. \n\nHowever, I still believe that both the theoretical and experimental contributions presented in your paper are incremental with respect to the existing literature. For this reason, I will maintain my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4027/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631411267,
                "cdate": 1700631411267,
                "tmdate": 1700631411267,
                "mdate": 1700631411267,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UtLdVDWP01",
            "forum": "qqDeICpLFo",
            "replyto": "qqDeICpLFo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4027/Reviewer_SYz5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4027/Reviewer_SYz5"
            ],
            "content": {
                "summary": {
                    "value": "The authors analyze GNN architectures performance on different tasks. Theoretically they establish that for contextual Stochastic Block Model, linear GCN up to 2 layers attains maximum accuracy - in a certain sense. Empirically, they test different GNN architectures on a variety of datasets and study the effect of edge feature and higher-order structures. In conclusion, GNNs are shown to be more suited to learn simpler models such as cSBM and struggles with noisy data and higher-order structure information."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Experimental details are interesting and cover a wide range of datasets, both synthetic and from the real world. The conclusion of the paper is thought provoking, in showing that GNN is volatile to higher-order structures, and is more capable of learning simpler, albeit noisy, feature data. \n\nTheoretical results seem correct and self-contained. The take away from the theorems in this paper, in that there are many regimes where linear GNNs are enough to learn the optimal (in some sense) classifier is interesting and corroborated by other research in the literature. The conclusions drawn from the paper is more or less in-tune with current understanding of GNN performance under cSBM, in that nonlinearities need to be re-thought."
                },
                "weaknesses": {
                    "value": "Unfortunately, I believe that the author missed a couple of key literature pieces. For instance, Wu, Chen, Wang and Jababaie \u2018A non-asymptotic analysis of oversmoothing in GNN\u2019 in ICLR2023. As a result, some key results of the paper have been shown or can be derived from existing results in the literature. For instance, Theorem 1 of the paper is established in Lemma 1 of Wu, Chen, Wang and Jababaie (granted that this paper do not make clear the distinction between homophily and heterophily regime, the results of Theorem 1 is a few lines away from their Lemma). In Wu et al, the authors also noted the ineffectiveness of nonlinearities (in particular ReLU) in Appendix K1, which may also give rise to much of the results of Theorem 2 in the current paper under review. Therefore, much of the theoretical contribution in this paper has already been established elsewhere. \n\nThere are also many (for some, major) notational issues with presentation (a few of which that I caught is deferred to the Question section)."
                },
                "questions": {
                    "value": "- Pg 3: n_out \u201cthe number of nodes in other classes\u201d, do you mean \u201cthe number of neighbors in other classes\u201d? Also, n_in and n_out are very dependent on i - the vertex, so i\u2019d suggest writing n_in(i), for example, to avoid confusion (since you\u2019ve already written \\mathcal{N}(i), for example).\n- Pg 3: Assuming that the means are of opposite sign does not cause loss of generality only in the case when the number of classes in 2. Can the results be generalized to more than 2 classes?\n- Pg 3: First bullet point of Theorem 1. Do you mean \u201chas the same distribution as\u201d? (the equation is a random variable, not a distribution).\n- Pg 3: First bullet point of Theorem 1. There seems to be some transpose or dot missing when taking the inner product of vectors (eg - mW should be either m^\\top W or m \\cdot W).\n- Pg 4: Second bullet point of Theorem 1. What is the notation y[X], is it the same as y(X)? \n- Pg 4: Third bullet point of Theorem 1. The term \u201cmaximum accuracy\u201d also lacks a clear definition in the main paper. \n- Pg 6: Statement of Theorem 3. The linear model still has a nonlinear sigma written in it. \n- Remark 2: What does \u201cextremely dense\u201d mean? Graphs sampled from SBMs are naturally dense (number of edges of quadratic order of number of vertices). \n- Equation 7 (page 14). The first derivative is also 0 when n_in = n_out (is it possible for some setting of the cSBM such that n_in = n_out with high probability?) Perhaps this is mentioned later on below equation 15 but it\u2019s not clear what is the scope of \u201cwe make no claims\u201d and if it\u2019s wide enough, should be reflected in the main statement in the main paper. For instance, there are phase transition results for SBM that suggest that if lambda < 1 then the SBM is not distinguishable from Erdos-Renyi model with average degree d."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4027/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822276736,
            "cdate": 1698822276736,
            "tmdate": 1699636365449,
            "mdate": 1699636365449,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i6Ejdq3sIA",
                "forum": "qqDeICpLFo",
                "replyto": "UtLdVDWP01",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4027/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4027/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "# Main items\nThank you for carefully reading our paper! We found these comments very interesting and are glad that you found the paper thought provoking. We particularly appreciate the pointers to other literature, not all of which we were aware of. We now reference these works in the appropriate places. We do wish to clarify the relationship between our work and these in this comment as well. Revisions have been highlights in blue.\n\nSpecifically, in the paper of Wu et al. ICLR 2023, Lemma 1 is indeed closely related to our Theorem 1. There is a difference in normalization of the graph operator and possibly in the homophily assumption, but we expect that these are not essential differences. Indeed, we would not be surprised if versions of our Theorem 1 were known elsewhere. Therefore, we have reclassified our Theorem 1 as a Proposition instead and explicitly cited the related Lemma 1 from Wu et al. nearby to clarify that similar facts were already in the literature.\n\nWe do, however, feel that our presentation has caused a lack of clarity about the relationship between our last two theorems and the rest of the literature, including Appendix K1 of Wu et al. For example, Proposition 6 in Wu et al.\u2019s appendix states that adding a ReLU activation function after a sequence of linear aggregation layers does not improve classification. This may have a bearing on our Theorem 1 (now Proposition 1), but we actually view it as quite different from our Theorems 2 and 3 (now Theorems 1 and 2), which deal with a nonlinearity interleaved between the linear layers. More generally, the larger purpose of our theoretical section is to show that the nonlinear models cannot perform better than linear models under certain circumstances. We have rewritten section 4.2 with this distinction in mind and hope that it is clearer for the reviewer.\n\n# Other items:\nPg 3: n_out \u201cthe number of nodes in other classes\u201d, do you mean \u201cthe number of neighbors in other classes\u201d? Also, n_in and n_out are very dependent on i - the vertex, so I\u2019d suggest writing n_in(i), for example, to avoid confusion (since you\u2019ve already written \\mathcal{N}(i), for example).\n- changed to n_in(i)\n\nPg 3: Assuming that the means are of opposite sign does not cause loss of generality only in the case when the number of classes in 2. Can the results be generalized to more than 2 classes?\n- We have not generalized it here, but we have explored an increased number of classes in the empirical work. Due to the symmetries it is possible the proof would extend.\n\nPg 3: First bullet point of Theorem 1. Do you mean \u201chas the same distribution as\u201d? (the equation is a random variable, not a distribution).\n- changed to \u201chas the same distribution as\u201d\n\nPg 3: First bullet point of Theorem 1. There seems to be some transpose or dot missing when taking the inner product of vectors (eg - mW should be either m^\\topW or m\\cdot W).\n- changed to m\\cdot W\n\nPg 4: Second bullet point of Theorem 1. What is the notation y[X], is it the same as y(X)?\n- Fixed, changed all to y[X]\n\nPg 4: Third bullet point of Theorem 1. The term \u201cmaximum accuracy\u201d also lacks a clear definition in the main paper.\n- Fixed, changed to Maximum expected accuracy for an arbitrary node\n\nPg 6: Statement of Theorem 3. The linear model still has a nonlinear sigma written in it.\n- Fixed, we have removed the sigma. \n\nRemark 2: What does \u201cextremely dense\u201d mean? Graphs sampled from SBMs are naturally dense (number of edges of quadratic order of number of vertices).\n- This is an interesting point. Traditionally, in the SBM literature, two main regimes are studied, one where the expected degrees are held constant as the number of nodes approaches infinity, and another where they grow by a logarithmic factor. (For a good review, see Abbe, Community Detection and Stochastic Block Models: Recent Developments, Journal of Machine Learning Research 18 (2018) 1-86). In either case, the graph is far from having O(n^2) edges. Our remark about \u201cextremely dense\u201d refers only to the case where almost all edges are present. We added a clarifying remark.\n\nEquation 7 (page 14). The first derivative is also 0 when n_in=n_out (is it possible for some setting of the cSBM such that n_in = n_out with high probability?) Perhaps this is mentioned later on below equation 15 but it\u2019s not clear what is the scope of \u201cwe make no claims\u201d and if it\u2019s wide enough, should be reflected in the main statement in the main paper. For instance, there are phase transition results for SBM that suggest that if lambda < 1 then the SBM is not distinguishable from Erdos-Renyi model with average degree d.\n- Thank you for pointing out this discrepancy in the paper. We would like to clarify this both here and in the appendix. If n_in=n_out then regardless of the weight matrix or theta we have that the probability evaluates to .5 (This can be seen in part 2 of proposition 1). Thus every weight matrix is optimal. We have updated the appendix to reflect this fact."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4027/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111305702,
                "cdate": 1700111305702,
                "tmdate": 1700111305702,
                "mdate": 1700111305702,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HxdgjsX91y",
                "forum": "qqDeICpLFo",
                "replyto": "i6Ejdq3sIA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4027/Reviewer_SYz5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4027/Reviewer_SYz5"
                ],
                "content": {
                    "comment": {
                        "value": "I'd like to thank the the authors for their comments and revision of their draft. I was not able to go into the revised draft into as much details as the first draft but have confirmed that the authors have fixed most of the minor concerns raised. However, I believe that the theoretical contributions are now rather minor. At the same time, I believe that the conclusions drawn from the paper, both theoretical and empirical, is based on the oversimplified model of cSBM - a model class that was originally developed to understand complicated theoretical properties in a sandboxed manner. On these simple datasets, it is not surprising that more complicated structures/architectures are not necessary, but the same conclusion is far from being conclusive on realistic datasets, at least based on the evidence presented in the paper. Therefore I am keeping the same score."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4027/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587758382,
                "cdate": 1700587758382,
                "tmdate": 1700587758382,
                "mdate": 1700587758382,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u0bQ0gNmw2",
            "forum": "qqDeICpLFo",
            "replyto": "qqDeICpLFo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4027/Reviewer_WetA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4027/Reviewer_WetA"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors study the accuracy of GNNs with respect to the contextual stochastic block model (cSBM) for data generation. In Theorem 1, for one-layer GNNs, they prove some formulae for the accuracy under this model, which are derived based on the Gaussian error function, and then they show that a linear classifier can be as good as the best GNN. In Theorem 3, they derive accuracy formulae for two-layer linear networks. The paper is concluded with experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- highly motivated problem\n- having many experiments"
                },
                "weaknesses": {
                    "value": "- the paper is not well-written; extensive revision is required to make the contributions clear\n- the setup is limited and it is not clear whether the linear classifier is good beyond the assumptions\n- nice theoretical results but I guess they are highly tied to the assumptions (limited)"
                },
                "questions": {
                    "value": "- I think the theoretical contributions of the paper are nice but unfortunately not enough and limited to particular assumptions \n\n- Section 4.2 is not well written. It is not clear what the authors want to say and it does not have flow. I strongly recommend rewriting it.\n\n- The definition of $\\sigma$ in page 4 is missing. It first seems it is a function, but apparently it is a constant?\n\n\n-----------------------------------\nAfter the rebuttal: I appreciate the authors for their response and revision; they have made significant changes to enhance the quality of the paper. Since they addressed my questions and comments, I have decided to slightly increase my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4027/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4027/Reviewer_WetA",
                        "ICLR.cc/2024/Conference/Submission4027/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4027/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698895195315,
            "cdate": 1698895195315,
            "tmdate": 1700791861838,
            "mdate": 1700791861838,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0FoGORHknh",
                "forum": "qqDeICpLFo",
                "replyto": "u0bQ0gNmw2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4027/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4027/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to read our paper! It appears there are two main questions affecting this review: the role of assumptions of linearity and difficulties with the presentation of Theorem 2 (now Theorem 1). We note that revisions have been highlights in blue for the reviewers convenience.\n\nFirst, regarding Section 4.2, we apologize the section was not easier to read, and we have done a significant rewrite, adding in particular guiding Remarks in appropriate places and clarifying the notation. This theorem is really very nice, along with its proof, and we hope the reviewer will take a second look at the much better presentation. In particular, Theorem 2 implies that Theorem 3 is actually a bound on the accuracy of nonlinear GNNs as well. Your comment has substantially increased the quality of this paper, for which we thank you.\n\nRegarding limitations of GNNs when the assumptions do not hold, this is of course a valid point, and we have rewritten the paper to emphasize that the superiority of linear classifiers in general is not one of the claims of the paper. The object of interest here is the nonlinear GNN, and we are using the performance of linear GNNs as a tool to get there. For example, Theorems 2 and 3 both tell us significant facts about nonlinear GNNs. Namely, unless certain asymmetries are included in the data generation model, there is no theoretical reason that the nonlinearities will be helpful. From this we can get tight GNN accuracy bounds purely in terms of linear models (which turn out to be a special case of the nonlinear GNNs, with the right parameter choices). This motivates research into better models than cSBM for the data and, more importantly, a deeper understanding about what significant features of the data should be incorporated into such a model. For example, ideas like homophily and Gaussian features are simply not enough to correctly motivate nonlinear GNNs theoretically. We have significantly enhanced the presentation and discussion in the paper to help get these points across better.\n\nSmaller point: $\\sigma$ was defined on page 4, but its usage here without a subscript is apparently unclear. We have added text explicitly clarifying the role of sigma here."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4027/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111280203,
                "cdate": 1700111280203,
                "tmdate": 1700111280203,
                "mdate": 1700111280203,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]