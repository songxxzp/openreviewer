[
    {
        "title": "Benchmarking Algorithms for Federated Domain Generalization"
    },
    {
        "review": {
            "id": "BJqZdGYbIQ",
            "forum": "wprSv7ichW",
            "replyto": "wprSv7ichW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7354/Reviewer_EtKR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7354/Reviewer_EtKR"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the intersection between Federated Learning and Domain Generalization, namely, Federated Domain Generalization (FDG), in which different domains are separated among clients that will collaboratively learn a model that generalizes to unseen domains. This paper pushes on an important direction: on the methodology behind evaluating FDG algorithms. In this sense, the authors: (i) Present an interesting review of existing practice in FDG; (ii) Propose a novel way of partitioning clients in FDG; (iii) Propose new metrics for evaluating the hardness of benchmarks; (iv) provide extensive evaluation of FDG methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper plays the same role of [Gulrajani and Lopez-Paz, 2021] for FDG, i.e., an important paper that provides an in-depth discussion about how to evaluate existing methods. In this angle, the paper provides extensive experimentation and interesting insights. In this sense, the paper is quite important for the field.\n\nFurthermore, as the authors discuss in the paper, the federated setting poses __new challenges__ to DG. This is especially related to the new partition method that the authors propose, and the main novelty of the paper. This contribution helps merging the fields of federated learning and domain generalization, making the evaluation of FDG algorithms more realistic. As a consequence, this direction is quite impactful and helpful for the field of FDG."
                },
                "weaknesses": {
                    "value": "Overall, I have no major concerns with this paper. My only critique is a (minor) lack of clarity. In section 4., the term ERM is never defined, and the empirical risk minimization method is never properly described. While, for knowledgeable audiences, this does not hurt the understanding of the paper in general, this makes the paper harder to read for beginners. I suggest authors add a description of the ERM in page 3, on the Federated DG paragraph."
                },
                "questions": {
                    "value": "__Q1.__ Since Domain Adaptation is a related problem to Domain Generalization, is it possible to apply the proposed partitioning scheme to Federated Domain Adaptation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7354/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7354/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7354/Reviewer_EtKR"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7354/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698140089770,
            "cdate": 1698140089770,
            "tmdate": 1699636879545,
            "mdate": 1699636879545,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7JpyK01gMU",
                "forum": "wprSv7ichW",
                "replyto": "BJqZdGYbIQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7354/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7354/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EtKR (1/1)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's careful reading and recognition of this work. Especially for the suggestions on expanding the data partition method on other scenarios. We next provide a detailed reply as below.\n\n> My only critique is a (minor) lack of clarity. In section 4., the term ERM is never defined, and the empirical risk minimization method is never properly described. While, for knowledgeable audiences, this does not hurt the understanding of the paper in general, this makes the paper harder to read for beginners. I suggest authors add a description of the ERM in page 3, on the Federated DG paragraph.\n\nThanks for pointing this out! We have revised manuscript on page 3:\n\u201cThe most common objective in Federated learning is empirical risk minimization (ERM), which minimizes the average loss over the given dataset.\u201d\n\n> Since Domain Adaptation is a related problem to Domain Generalization, is it possible to apply the proposed partitioning scheme to Federated Domain Adaptation?\n\nThank you for your insightful question. Yes, our partition method is general for partitioning of any D distributions onto C clients; the distributions could be defined based on: \n- Domain labels as in DG in our paper or domain adaptation as reviewer mentioned.\n- Class labels as in class-heterogeneous FL.\n- Sensitive attributes in FL fairness [1,2].\n\nWe have added the discussion in the main paper at the start of section 3 heterogeneous partitioning method. \n\nModification on page 3: \u201cGenerally, our partition method effectively handles partitioning $D$ types of integer-numbered objects into $C$ groups. It's broadly applicable, suitable for domain adaptation, ensuring fairness in Federated Learning (FL), and managing non-iid FL regimes. \u201d\n\n[1] Mohri, Mehryar, Gary Sivek, and Ananda Theertha Suresh. \"Agnostic federated learning.\" International Conference on Machine Learning. PMLR, 2019.\n\n[2] Du, Wei, et al. \"Fairness-aware agnostic federated learning.\" Proceedings of the 2021 SIAM International Conference on Data Mining (SDM). Society for Industrial and Applied Mathematics, 2021."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7354/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378207640,
                "cdate": 1700378207640,
                "tmdate": 1700397876522,
                "mdate": 1700397876522,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3r1BjLzPvn",
                "forum": "wprSv7ichW",
                "replyto": "7JpyK01gMU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7354/Reviewer_EtKR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7354/Reviewer_EtKR"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you for your response, and for integrating my suggestions into your paper. In my view this paper can have an important impact on the field of federated domain generalization.\n\nAs I stated in my initial review, I had no major concens with this paper. As a result, I keep my score __8: Accept, Good Paper__."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7354/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484247829,
                "cdate": 1700484247829,
                "tmdate": 1700484247829,
                "mdate": 1700484247829,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v5GdOYhI2b",
            "forum": "wprSv7ichW",
            "replyto": "wprSv7ichW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7354/Reviewer_AnGM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7354/Reviewer_AnGM"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a benchmark for domain generalization (DG) in federated learning. Specifically, they (1) develop a novel method to partition a DG dataset to any number of clients, (2) propose a benchmark methodology including four important factors, (3) experiment with a line of baselines and datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper contributes to an important topic. I believe federated DG is an important problem in FL, especially cross-device FL when the trained FL global model need to generalize to a large amount of clients that do not participate in FL training. \n2. The paper is well-written and easy to follow. \n3. The proposed benchmark includes a variety of datasets and algorithms. \n4. The experiments regarding the number of clients is highly related to cross-device FL, and indicate an important drawback in the current federated DG experiments."
                },
                "weaknesses": {
                    "value": "1. In the context of domain generalization, we are particularly concerned with whether models trained on limited source domains can generalize to new target domains. This paper also uses held-out domains (in Appendix C.2). I believe that this aspect should be more explicitly explained in the main body of the text; otherwise, readers might easily misconstrue that all domains were used to construct training clients, which is misleading. \n2. Error bars are not provided for experiment results, the conclusion may be influence by random fluctuation. \n\nMinor: \n1. Page5 line 12: homogeneous -> homogeneity\n2. When considering the second kind of DG, it is relevant to \u201cperformance fairness\u201d in FL, which encourage a uniform distribution of accuracy across clients. Although works in this direction might emphasize more on participating clients, I believe at least the AFL algorithm [1] can be a good supplement to the benchmark. \n\n[1] Mehryar Mohri, Gary Sivek, Ananda Theertha Suresh. Agnostic Federated Learning. ICML 2019."
                },
                "questions": {
                    "value": "1. In Eq. (1), two kinds of DG is mentioned. Which kind of DG is mainly used in your benchmark? \n2. The performance for some algorithm, for example, FedSR, is very low, and consistently lower than FedAvg. Could you explain the reason behind?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7354/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7354/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7354/Reviewer_AnGM"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7354/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802351063,
            "cdate": 1698802351063,
            "tmdate": 1699636879393,
            "mdate": 1699636879393,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jmfaqGCaR7",
                "forum": "wprSv7ichW",
                "replyto": "v5GdOYhI2b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7354/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7354/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AnGM (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for the positive feedback and construcive suggestions. We responded to the questions and suggestions you made individually below.\n\n> In the context of domain generalization, we are particularly concerned with whether models trained on limited source domains can generalize to new target domains. This paper also uses held-out domains (in Appendix C.2). I believe that this aspect should be more explicitly explained in the main body of the text...\n\nGood point. We agree that our setup of held-out validation domains is of great importance and need better visibility. We have added the corresponding part to the main paper section 4.3, page 7 per the reviewer\u2019s suggestion, where we emphasize that we are using validation domain dataset: \u201cIn DG task, we cannot access the test domain data. However, we are particularly concerned about the model performance outside the training domains, thus we preserve a small portion of the domains we can access as held-out validation domains, and the held-out validation domains are used for model selection and early stopping. \u201d\n\n\n> Error bars are not provided for experiment results, the conclusion may be influenced by random fluctuation.\n\nGreat suggestion. Following the Reviewer's suggestion, we have included in the revised manuscript error bar in Section 4.3. In particular, we have included the standard deviation of the repeated experiments on PACS, CelebA, Cameylon17. Due to the computational challenges of training many different models with different settings, we keep working on the results on Py150, CivilComments, IWildCam and FEMNIST datasets, and we will include them in the final version.\n\n> Page5 line 12: homogeneous -> homogeneity\n\nThanks, fixed.\n\n> When considering the second kind of DG, it is relevant to \u201cperformance fairness\u201d in FL, which encourages a uniform distribution of accuracy across clients. Although works in this direction might emphasize more on participating clients, I believe at least the AFL algorithm [1] can be a good supplement to the benchmark.\n\nWe appreciate the reviewer's mention of this work. In a nutshell, the Reviewer is right:  Agnostic Federated Learning (AFL) shares similarities with Domain Generalization in a Federated context. This is evident as both approaches address scenarios where the test distribution diverges from the training distribution. Thus we agree that AFL is a good method to evaluate especially when tackling subpopulation shift tasks. We have added a discussion in the revised paper page 17.  Furthermore, we are currently working on implementing AFL in our benchmark, especially for CelebA, CivilComments, which are designed for the sub-population shift task. Our benchmark could also help the evaluation of the future work in this line of research.\n\nModification on paper page 17: \u201cAgnostic Federated Learning (AFL) [1,2] share similarities with Domain Generalization in a Federated context. This is evident as both approaches address scenarios where the test distribution diverges from the training distribution. AFL constructs a framework that naturally yields a notion of fairness, where the centralized model is optimized for any target distribution formed by a mixture of the client distributions. Thus, AFL is a good method to evaluate especially when tackling subpopulation shift tasks.\u201d\n\n> In Eq. (1), two kinds of DG are mentioned. Which kind of DG is mainly used in your benchmark?\n\nGreat question. We used both formulations. We elaborate this both in terms of methods and datasets.\n\nRegarding methods, since test domains are inaccessible for direct optimization, DG methods typically start from one of the two theoretical formulations and seek to find an approximate objective on training domains to work with. For instance, Fish starts with the average-case objective, and IRM, GroupDRO starts with the worst-case objective. We included diverse methods from both formulations.\n\nRegarding datasets, PACS, FEMNIST, Camelyon17, and IWildCam focus on averaging test domain performance, aligning with the first DG objective for evaluation. Among them, IWildCam uses the F1 Score, which is a weighted average, placing greater emphasis on rarer species. In contrast, CelebA and CivilComments prioritize worst domain accuracy, aligning with the second DG objective. Further, Py150 is unique in targeting a specific sub-population accuracy, termed 'method-class accuracy.' This is particularly relevant for Py150's application in code completion, where the accuracy of predicting method and class names is critical.\n\n[1] Mohri, Mehryar, Gary Sivek, and Ananda Theertha Suresh. \"Agnostic federated learning.\" International Conference on Machine Learning. PMLR, 2019.\n\n[2] Du, Wei, et al. \"Fairness-aware agnostic federated learning.\" Proceedings of the 2021 SIAM International Conference on Data Mining (SDM). Society for Industrial and Applied Mathematics, 2021."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7354/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377109967,
                "cdate": 1700377109967,
                "tmdate": 1700397914318,
                "mdate": 1700397914318,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ptcTsJFlOz",
                "forum": "wprSv7ichW",
                "replyto": "v5GdOYhI2b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7354/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7354/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AnGM (2/2)"
                    },
                    "comment": {
                        "value": "> The performance for some algorithms, for example, FedSR, is very low, and consistently lower than FedAvg. Could you explain the reason behind?\n\nThank you for your question. First, we would like to point out that we replicated the results in the experimental setting of the FedSR paper, where their evaluation on the PACS dataset is only based on 3 clients. In our benchmark, we observed two new phenomena for FedSR:\n- the degradation in performance when the number of clients is large (increase up to $100$)\n- the hyperparameters are also more sensitive compared to their 3 clients setting. \n\nAs suggested by reviewer AnGM, we conducted repeated experiments (Sec. 4.3), where we provided each method with a larger hyperparameter tuning budget, thus, FedSR achieves better results than in the original experiments. Even with an increased budget, we observed that FedSR is still significantly worse than FedAvg ($40\\\\%$ vs $90\\\\%$ on PACS). Thus, it does not alter the conclusion of the original manuscript.\n\n**A potential explanation of the phenomena**\n\nThe convergence analysis is well established for FedAvg in the literature, see [3] and subsequent works. However, there\u2019s no theoretical guarantees for FedSR, where the objective function is constructed by the summation of local data loss and local non-smooth penalty. We suspect that the extra heterogeneous local nonsmooth penalty terms introduced in FedSR potentially diverts each client's learning trajectory, leading to harder convergence when client number increases. \n\n[3] Stich, Sebastian U. \"Local SGD converges fast and communicates little.\" arXiv preprint arXiv:1805.09767 (2018)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7354/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377207318,
                "cdate": 1700377207318,
                "tmdate": 1700378726511,
                "mdate": 1700378726511,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gnIpFPUaD1",
                "forum": "wprSv7ichW",
                "replyto": "ptcTsJFlOz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7354/Reviewer_AnGM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7354/Reviewer_AnGM"
                ],
                "content": {
                    "title": {
                        "value": "Thanks!"
                    },
                    "comment": {
                        "value": "Thanks a lot for your rebuttal! I think the additional information you provide is beneficial to the paper. I understand that providing error bars for all experiments can be challenging in such a short period of time during rebuttal, but I do believe that a good and tested benchmark is beneficial for the whole community. Good luck!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7354/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685902659,
                "cdate": 1700685902659,
                "tmdate": 1700685902659,
                "mdate": 1700685902659,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RYl98WV4Jy",
            "forum": "wprSv7ichW",
            "replyto": "wprSv7ichW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7354/Reviewer_PJ3y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7354/Reviewer_PJ3y"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a benchmark for federated domain generalization, which is a challenging problem that requires learning a model that can generalize to heterogeneous data in a federated setting. The paper presents a novel data partitioning method that can create heterogeneous clients from any domain dataset, and a benchmark methodology that considers four factors: number of clients, client heterogeneity, dataset diversity, and out-of-distribution generalization. The paper also evaluates 13 Federated DG methods on 7 datasets and provides insights into their strengths and limitations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper proposes a comprehensive and rigorous benchmark for Federated DG that covers various aspects of the problem and can be easily extended to new datasets and methods.\n* The paper provides a clear and detailed description of the data partitioning method and the benchmark methodology, as well as the implementation details of the methods and datasets.\n* The paper conducts extensive experiments and analyzes the results from different perspectives."
                },
                "weaknesses": {
                    "value": "The paper mainly summarizes the experimental observations, but does not offer much theoretical analysis or explanation for why some methods perform better than others in certain scenarios, which would be helpful to gain more insights into the FDG problem and the design of effective methods."
                },
                "questions": {
                    "value": "No."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7354/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7354/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7354/Reviewer_PJ3y"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7354/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700971962272,
            "cdate": 1700971962272,
            "tmdate": 1700971962272,
            "mdate": 1700971962272,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]