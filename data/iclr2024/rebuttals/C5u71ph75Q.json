[
    {
        "title": "Internal-Coordinate Density Modelling of Protein Structure: Covariance Matters"
    },
    {
        "review": {
            "id": "sLbTWShieE",
            "forum": "C5u71ph75Q",
            "replyto": "C5u71ph75Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3708/Reviewer_U1Uk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3708/Reviewer_U1Uk"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a generative model for protein angles such that they try to not change absolute coordinates too much. The paper takes a functional Lagrangian perspective.\n\n--\n\nPost-response update: The mathematical concerns were adressed during discussion. I think this is a good and clever contribution to the field, while still having some weakness in demonstrating its ML significance."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The main idea of deriving how angles and coordinates are coupled through Lagrangian is outstanding. The paper is well written and method is well derived until around half-way. I found this paper super interesting to read, and this is definitely a promising approach.\n- The results show relatively good generation of variations wrt reference, but there are still quite a lot of room to improve."
                },
                "weaknesses": {
                    "value": "- The math in this paper is sloppy and incomplete, and I\u2019m not convinced that the method is correctly derived. I suspect that the Lagrangian is incorrectly derived, but I would be happy to see full derivations of the method to verify if it is. The results have some issues, which implies that something is perhaps wrong.\n- I could not understand the generative model with VAEs and Unets, or the experiments. I\u2019m not sure what the paper or the models try to demonstrate or achieve, despite the clear problem statement of finding safe distributions of angles wrt coordinates. I think the paper deviates from the original problem quite far into somewhere else. I can\u2019t really interpret if the results are good or not.\n- The ML significance of the method is ultimately a bit light. There are only a few experiments, and no comparisons to other ML models. It seems that this paper is about finding variations in proteins. These are conformers, and there is already ML works on doing generative models for those. Comparisons to those are needed, and the paper should better positions itself to wider literature. Currently the paper takes too narrow and isolated perspective to the problem domain."
                },
                "questions": {
                    "value": "- Why do you need to include the integral=1 constraint? Isn\u2019t p automatically a proper distribution?\n- I find the notation to be a bit odd. The eq 5 should be defined as a lagrangian; it's not a KL anymore. It has sufficient and necessary conditions for minima, and these should be properly formulated and presented. I\u2019m not sure what is a \u201cfunctional derivative of KL wrt p(Delta x)\u201d. What is a functional derivative? How can you take a derivative wrt a density evaluation at one point? Which point? All of them? Only one? Can you present this concept in more detail? There is an adhoc flavour to this, and I suspect something that the rigor is insufficient to actually claim you are doing a Lagrangian. You need to rigorously derive and present the Lagrangian formalism. If you want to take a functional viewpoint, then you need to derive the functional representations of the objects, eg. a hilbert representation of the distributions.\n- Why does the \\int p d term vanish in the derivative? Why does p(deltak) vanish as well? Surely this should not happen: Shouldn't you need to apply product rule and get Dp * log p/q + p * D(logp/q), where \u201cD\u201d denoted derivatives. I am not convinced this is correct. Please include all intermediate steps so we can verify its correct. Please include rigorous math that shows all the derivatives properly.\n- Similarly, I have trouble understanding eq 7. What does the approx do? How good of an approximation this is? Where does this come from? Please include all intermediate derivations, properly introduce and expose your math, and please include citations to textbooks or works where similar approaches have been done earlier. \n- How does an i\u2019th angle relate to m\u2019th atom i\u2019th coordinate? How come you have the same i'th index for two different things (phi/phi and x/y/z)?\n- Why is there no i in eq 8 suddenly? What is i and j? Why do we take a dot product? I have trouble following the story.\n- If you always normalise all Gaussians, why did you include the int=1 constraint?\n- I\u2019ve lost track where eq 9 comes from. What is this? Aren't Cm user defined constants: why are we now solving for their values? Also aren\u2019t you solving for the lambdas? Why are there none here? It seems that again quite a lot of intermediate derivations are missing, and I can\u2019t verify that this is correct.\n- What does it mean that you predict lambdas with a network? Err\u2026 Lagrangian multipliers can\u2019t be just chosen at will or predicted: the lagrangian formalism dictates under which coefficient values you can say something about your solution optimality. If you don\u2019t follow the lagrangian approach rigorously, you have no guarantees of what you end up with, and it might be of little use, or not do what you think it does. Are you perhaps not even doing a lagrange method but instead perhaps doing a penalty method? Can you clarify the setting and the goals and the overall workflow here? \n- I\u2019m lost on what are you trying to achieve with the VAE, and what\u2019s the point. So you take angles, encode them into latents, and reconstruct the angles. Why is this useful? What are we trying to achieve? If the reason you have a VAE is to evaluate derivatives, then something is wrong. Surely you should be able to just compute your derivatives directly?\n- So the VAE reconstruction as fed to a Unet that outputs lagrange multipliers\u2026 This feels adhoc. Why is it useful to predict lagrange multipliers? Intuitively this sounds wrong: the lagrange multipliers need to be at equilibrium point under lagrange conditions (etc), and not just predicted by some neural network. Can you elaborate what are you trying to do, and what role the lagrange has here? How much rigor are you even aiming at? \n- It seems that in the end you only produce a kind of soft regularisation by something that is inspired or slightly flavored with Lagrangians, but you are not actually doing Lagrangian optimisation. Given that the section 3 is named \u201cconstraints\u201d I was under the impression that you are actually doing Lagrangian constraints, but I don\u2019t think this is the case. Can you clarify what role the Lagrangian has?\n- I\u2019m quite confused what is the overall pipeline, and why you are doing what you are doing. I thought that you wanted to include constraints such that varying angles doesn't change the absolute coordinates too much. But in fig 3 the C\u2019s are not even present anymore, and it seems that the constraints now come dynamically from a Unet, based on coordinate means. What if the coordinate variances are large? What guarantees the Unet to give sensible constraints? How do we even define sensible constraints? What does the final N(mu,tildeSigma) represent, or try to do? Does this distribution still have something to do with constraints? What does \u201cweighed by Lagrange multipliers\u201d mean? Surely you can\u2019t just multiply your equation with some lambda\u2019s: the interpretation does not make sense if the lambdas are not chosen according to the Lagrangian theory.\n- What is the univariate Gaussian baseline? Is this some equation in the paper, or some completely new thing? Can you give a definition? What is empirical estimator? What is standard estimator? What is OAS? Can you elaborate what you estimating, and what\u2019s the story here? I think you are estimating something from the data, but not sure what. Maybe the precision structure? But why do you need the precision.. I thought that you needed the distribution of internal angles that are safe wrt too large absolute coordinate changes. Does this play a role in the experiments?\n- What are local and global constraints? What is \u201cvalid\u201d ramachandran distribution? What does \u201cimproved\u201d 3D variance mean? What do you improve? What is \u201cfull covariance matrix\u201d? Can you give a definition. Why is it full? How big is it? Covariance of what..? \u201cthat approximates distribution better\u2026\u201d Which distribution? Can you give a definition? What it the prior? Is it the eq 1? What is sigma_k,data? \u201cunderestimates the fluctuations\u201d What fluctuations? Of whose? What is the \u201cstandard estimator\u201d? What are you trying to achieve in the experiments? Can you clarify these points?\n- How does the VAE impose global constraints? What are global constraints?\n- Fig4: What is ramachandran reference? Where does the reference come from? What does it represent? What are the Rama samples? Where do they come from? Do these have something to do with data, or do they come from the model? What is MD reference? What does it represent or do?\n- Fig5: What does this figure show? Where do these energy landscapes come from? What did you learn here? Did you just learn to match the MD reference panel with the VAE? What are the axes of this panel? I\u2019m pretty lost what you did in the experiments, or what are you trying to demonstrate. The experiments do not discuss constraints or varying the angles in coordinate-safe ways at all, so I\u2019m wondering if this is the goal anymore. Can you help me understand?\n- What does \u201cfitting density of structural ensemble\u201d mean? What is an ensemble? Why is this an open problem? Are you talking about conformers?\n- \u201cWe obtain samples that are guaranteed to fulfil physical constraints..\u201d. With the amount of \\approx in the paper, this is very hard to believe. Do you really guarantee **this? In what part of the paper is this guarantee made?\n\nAs you can see I lost track around halfway point when the story changes from safe angular variations to generating proteins. I don't see how these two stories connect. I'm looking forward to clarifications."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3708/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3708/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3708/Reviewer_U1Uk"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3708/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698700379086,
            "cdate": 1698700379086,
            "tmdate": 1700557887329,
            "mdate": 1700557887329,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jTdNmReiP0",
                "forum": "C5u71ph75Q",
                "replyto": "sLbTWShieE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3708/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3708/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer U1Uk [1 of 2]"
                    },
                    "comment": {
                        "value": "We thank reviewer U1Uk for their interest in our paper and their in-depth review. We have attempted to pool some of the concerns and questions together below and we hope to have addressed all points, but please let us know if something is missing. We look forward to further discussions.\n\n* **Central goal of the method and role of the VAE**\n\n  The central problem in this paper is density modelling, i.e. modelling protein fluctuations. We do density modelling in terms of internal coordinates, while imposing constraints in 3D space. These 3D constraints are global constraints which determine the overall shape of the protein, as opposed to local constraints which ensure e.g. valid bond lengths and no local clashes. We show how we can infer a covariance matrix over internal coordinates from a single mean structure using the Lagrange formalism. Subsequently, we train a reconstruction VAE on a collection of protein structures, also called a protein ensemble. A protein ensemble can be a collection of structures that exhibit small fluctuations (unimodal setting) or larger conformational changes (multimodal setting). The VAE decoder outputs a mean over internal coordinates, which is used to infer the covariance matrix corresponding to that mean, and together they parameterize a multivariate Gaussian distribution, from which we can draw samples. Through encoding and decoding angles (reconstruction), the decoder learns to map from $z$ to valid internal coordinate means, such that in generative mode, we can simply draw samples from the prior over $z$ and get valid output distributions. Through the latent variable, the VAE can capture multiple conformational modes by predicting different means and corresponding covariance matrices.\n\n* **Baselines**\n\n  We agree that the results are more meaningful with additional baselines, please see general response. We have also elaborated the explanation of the baselines to be more understandable.\n\n* **Predicting Lagrange multipliers**\n\n  First of all, we would like to emphasize that in our setting the constraints, $C$, are not user-defined, but are to be inferred by the model based on the input data. Having said that, we absolutely concur that in an ideal setting, one would solve for the Lagrange multipliers given the constraints, whether they are user-defined or predicted by a model. However, for the constraints we want to impose we cannot solve for $\\lambda$ in closed form. Therefore we opted for predicting the Lagrange multipliers directly, which is essentially an indirect way of predicting $C$ (in other words, the lambdas are a proxy for $C$). To illustrate this, we have included a new appendix C.1 (see also general response), which shows the correspondence between constraints $C$ (calculated using Eq. 9) and atom fluctuations across VAE samples.\n\n  Moreover, one of the newly added baselines, \"$\\kappa$-prior (learned)\", is a VAE trained to predict the variance directly, without including our constraints. This baseline does much worse compared to the full setup with constraints, which indicates that the constraints we incorporate are meaningful.\n\n* **Mathematical derivations**\n\n  We apologize for any lack of clarity and rigor regarding the mathematical derivations. As described in the general response, we have now elaborated the relevant sections (3.1-3.4) for improved understanding. We have incorporated all suggestions to the best of our ability, and would be happy to make further clarifications if necessary.\n\n* **Ramachandran distributions**\n\n  Ramachandran plots are a common way to visualize dihedral distributions by plotting the torsional angles around the $N-C_\\alpha$ bonds ($\\phi$) and $C_\\alpha-C$ bonds ($\\psi$) against each other (See section 4.2, \"Unimodal\"). The resulting distribution shows clusters for different secondary structural elements, e.g. beta sheets, right-handed helices, and left-handed helices. In our application, we can check qualitatively if the distribution is valid by comparing to the reference distribution (either simulation-based: MD, or experimental: NMR). We have updated Figure 4 to have more clear labels concerning the Ramachandran distributions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3708/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700183825779,
                "cdate": 1700183825779,
                "tmdate": 1700183825779,
                "mdate": 1700183825779,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gAHU62ArTd",
                "forum": "C5u71ph75Q",
                "replyto": "sLbTWShieE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3708/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3708/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer U1Uk [2 of 2]"
                    },
                    "comment": {
                        "value": "This is our continued response to reviewer U1Uk.\n\n* **\"Full\" covariance matrix**\n\n  The full covariance matrix refers to the covariance over all dihedrals and bond angles (named $\\boldsymbol{\\kappa}$ in the paper), as opposed to a more standard setting of only considering the diagonal (i.e. the variance). For $M$ atoms, this corresponds to a matrix with $(2 \\times M - 5)^2$ entries. The reason we emphasize the \"fullness\" of the covariance matrix is that we can infer it from predicting only M Lagrange multipliers.\n\n* **Prior**\n\n  We agree that the term \"prior\" can be a bit confusing, since there is also a prior for the latent space of the VAE. We now clearly separate this \"$z$-prior\" from the \"$\\kappa$-prior\", where the latter is our prior belief over the distribution of $\\Delta \\kappa$, based on the variance over these internal coordinates in the training set. The reciprocal of this variance, makes up the diagonal of our prior precision matrix. This is shown in Figure 3 and explained in Section 3.1 (Setup) and Section 3.5 (\"VAE model architecture\").\n\n* **TICA explanation**\n\n  We have now elaborated our explanation for the TICA plots, please see general response.\n\n* **Density modelling: an open problem?**\n\n  Density modelling is an established and very active field of research, e.g. [1, 2, 3, 4] (as also mentioned in response to reviewer jsg1), and is not considered a solved problem.\n\n* **\u201cWe obtain samples that are guaranteed to fulfil physical constraints of the protein topology (e.g. bond lengths, and bond angles)\u201d**\n\n  What we are saying in this specific sentence is that, by construction, the sampled structures will be guaranteed to adhere to a correct local protein topology, purely by choosing to model the protein in internal coordinate space. This means e.g. valid bond lengths and no local clashes. Chemical integrity is one of the main benefits of working with internal coordinate representations, as was also outlined in the second paragraph of Section 2.1. The approximations made in the paper only relate to global constraints. We have added the word ``local\u2019\u2019 to this sentence to make the distinction more clear.\n\n\\\n*References*\n\n1\\) No\u00e9, Frank, et al. \"Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning.\" Science 365.6457 (2019): eaaw1147.\n\n2\\) Zhong, Ellen D., et al. \"CryoDRGN: reconstruction of heterogeneous cryo-EM structures using neural networks.\" Nature methods 18.2 (2021): 176-185.\n\n3\\) Ingraham, John, et al. \"Illuminating protein space with a programmable generative model.\" BioRxiv (2022): 2022-12.\n\n4\\) Arts, Marloes, et al. \"Two for one: Diffusion models and force fields for coarse-grained molecular dynamics.\" Journal of Chemical Theory and Computation 19.18 (2023): 6151-6159."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3708/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700184215481,
                "cdate": 1700184215481,
                "tmdate": 1700184215481,
                "mdate": 1700184215481,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m5cVPGxsiX",
                "forum": "C5u71ph75Q",
                "replyto": "gAHU62ArTd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3708/Reviewer_U1Uk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3708/Reviewer_U1Uk"
                ],
                "content": {
                    "title": {
                        "value": "resp"
                    },
                    "comment": {
                        "value": "**On Lagrangian.** I\u2019m still confused what are you actually doing. Are you doing the method of Lagrange multipliers (ie. https://en.wikipedia.org/wiki/Lagrange_multiplier), or something else? I don\u2019t think you are since you should compute the stationary points of the lagrangian wrt all variables, and currently you are doing something else. I then wonder what is the \u201cLagrange formalism\u201d you are talking about. Does the procedure you do in sec 3.0-3.4 have a name, or precedent in literature? Is this a known procedure or something you invented? Does it have something to do with the above Wikipedia page? I\u2019m really struggling to find a common ground or basis of understanding.\n\nFrom my perspective you introduce a well-defined Lagrangian, and then apply some algebra to find some relationships between lambda\u2019s and other variables. Ok, sure, but I don\u2019t think this means in general anything (!). The constraints are only satisfied at stationary points of the Lagrangian, that is, when we have zero gradient wrt lambdas. I don\u2019t think this is satisfied, so then I\u2019m unsure what the result means, or if it has any meaning. In the end you don\u2019t even do any optimisation wrt the Lagrangian, which is the premise of Lagrange method: we want to do constrained optimisation. So err.. I\u2019m pretty lost. Can you clarify or help me understand?\n\n**Role of C.** You mention that C\u2019s are not user defined. Err\u2026 what? My understanding is that C is the variance of the absolute coordinates when your angles follow some distribution. Surely you would then set this by hand to eg. 0.5 \u00e5ngstroms (or something) and find an angle distribution that will give you this. But if the C\u2019s are not user defined, then this intuition goes out the window. Can you help me understand what the C\u2019s mean? It seems that there are also two different Cs, the C_m and C_n.. \n\n**Guarantees.** Can you clarify what guarantees of physical constraints are you after in this paper? I thought the whole point of the paper was to obtain angle distributions such that you have no absolute coordinate clashes that come from compounding angles over the protein backbone. But in your response you say that you want to adhere to local topology. Ok, but isn\u2019t this trivial by just predicting angles that are not too crazy? Surely pretty much any model is locally non-clashing (ie. the neighboring atoms don\u2019t clash). Can you clarify?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3708/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470153408,
                "cdate": 1700470153408,
                "tmdate": 1700470153408,
                "mdate": 1700470153408,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2tLnwfaddT",
                "forum": "C5u71ph75Q",
                "replyto": "p4O756L7g6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3708/Reviewer_U1Uk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3708/Reviewer_U1Uk"
                ],
                "content": {
                    "title": {
                        "value": "resp"
                    },
                    "comment": {
                        "value": "Ok, now the Lagrangian stuff makes sense. I'll raise my score.\n\nIn future I would recommend the author's to try to frame the work more around ML objectives. I believe the work would be more convincing if you would incorporate some (eg. reconstruction) benchmarks, and perhaps apply the work towards conformer generation which seems to be an ideal usecase for this."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3708/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557778316,
                "cdate": 1700557778316,
                "tmdate": 1700557778316,
                "mdate": 1700557778316,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xzZcWkP96X",
            "forum": "C5u71ph75Q",
            "replyto": "C5u71ph75Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3708/Reviewer_jsg1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3708/Reviewer_jsg1"
            ],
            "content": {
                "summary": {
                    "value": "The authors parameterize protein structure with internal coordinates and estimate those quantities under the framework of a multivariate gaussian distribution. The authors then train a neural network to simulation data to estimate these parameters, and then sample from it, comparing to the simulation they trained on."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think it is great the authors are able to define the limitations of their work with respect to the amount of data available for a protein of interest.\n\nThe description of number of parameters to be estimated (2.1) is very clear and helpful for intuition of the model components.\n\nI really like the \u201cVariance along the atom chain\u201d figures. I think they are really clear and show where the \u201cwiggles\u201d happen."
                },
                "weaknesses": {
                    "value": "I still have difficulty with the motivation of using ML models trained on simulation data. Why not just run the simulation then? What does this improve beyond the simulation framework?\n\nI would like the TIC plots in Figure 5 better explained. Also, why does it seem that there are many points cut off from the figure?\n\nIt is odd to me that there are no scalar metrics in this work, especially those that allow comparison to other methods.\n\nWhile I understand the potential for this approach, I feel like the authors could do a better job of describing how this method could actually solve understanding of potential protein structure fluctuations."
                },
                "questions": {
                    "value": "\u201cTo ensure this, we add an auxiliary regularizing loss in the form of a mean absolute error over \u03bb \u22121 ,\u201d Does this not introduce a Laplace likelihood (or prior) into the posterior? It\u2019d be great to explore that relationship and decision a bit more.\n\nIn Figure 4, looking at the Ramachandran plots, it seems like the samples are not capturing the actual variance in the data. Why is this? Is a Gaussian prior the right prior? What about Laplace or StudentT? Are there other parameters that could fully capture this variability, assuming that it is real?\n\nI would be curious of proteins that have very large degrees of freedom, or move a lot during function."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3708/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722838104,
            "cdate": 1698722838104,
            "tmdate": 1699636326783,
            "mdate": 1699636326783,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XeP4iHIEYx",
                "forum": "C5u71ph75Q",
                "replyto": "xzZcWkP96X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3708/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3708/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer jsg1"
                    },
                    "comment": {
                        "value": "We sincerely thank reviewer jsg1 for their interesting questions and helpful suggestions. We will answer in detail below.\n\n* **Density modelling (ML models trained per system)**\n\n  In this paper, we are training one model per protein using simulation data or experimental data to model the density over internal coordinates. Density modelling is an established and active field of research, e.g. [1, 2, 3, 4], with direct and indirect applications. Capturing a distribution from which we can sample cheaply (which is enhanced by the fact we model the backbone only) is useful for data augmentation, especially in a low data setting. Moreover, density modelling per protein can be seen as a step towards modelling complexes, doing cheap simulations, analyzing function and, ultimately, generalize across proteins.\n\n* **TIC plots and quantitative metrics**\n\n  We thank the reviewer for the great suggestions, and refer to the general response for our answer.\n\n* **How could this method help us understand protein fluctuations?**\n\n  This is a very interesting point, and we would argue that our method works in the opposite direction. We incorporate our knowledge of protein fluctuations as an inductive bias in the proposed method, thereby creating a more meaningful way to incorporate constraints.\n\n* **Regularizer**\n\n  We agree that our auxiliary loss corresponds to multiplying with a Laplace prior (L1 loss). We chose this auxiliary loss because our approximation only holds for smaller fluctuations. Without the regularizer,the  global constraints can become too weak, and the covariance matrix can become invalid. The hyperparameters $a$ and $w_{\\mathrm{aux}}$ can be used for tuning (see appendix D).\n\n* **Figure 4: capturing the actual variance**\n\n  We agree that our method does not always capture the true variance of the data. We believe this to be partly due to hyperparameter choice, the first-order approximation for fluctuations (Eq. 7) and, indeed, the Gaussian assumption. However, we stress that modelling a complex protein like 1pga in a limited data regime is highly challenging, and it might be that our mean encoder and decoder are not expressive enough in this setting. One possible solution could be introducing a hierarchical VAE to model larger local fluctuations, but this is beyond the scope of the current paper.\n\n* **Proteins with large degrees of freedom**\n\n  We believe our multimodal test cases to exhibit large degrees of freedom, as folding and unfolding involves dramatic conformational changes. However, we would like to emphasize that our general-purpose method is designed for improved modelling of smaller fluctuations, and that large degrees of freedom can be realized through model architecture choices. For example, our proof-of-concept VAE realizes large conformational changes through different predicted means (and corresponding covariance matrices).\n\n\\\n*References*\n\n1\\) No\u00e9, Frank, et al. \"Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning.\" *Science* 365.6457 (2019): eaaw1147.\n\n2\\) Zhong, Ellen D., et al. \"CryoDRGN: reconstruction of heterogeneous cryo-EM structures using neural networks.\" *Nature methods* 18.2 (2021): 176-185.\n\n3\\) Ingraham, John, et al. \"Illuminating protein space with a programmable generative model.\" *BioRxiv* (2022): 2022-12.\n\n4\\) Arts, Marloes, et al. \"Two for one: Diffusion models and force fields for coarse-grained molecular dynamics.\" *Journal of Chemical Theory and Computation* 19.18 (2023): 6151-6159."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3708/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700183370170,
                "cdate": 1700183370170,
                "tmdate": 1700183573895,
                "mdate": 1700183573895,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6N1mIHGRyY",
            "forum": "C5u71ph75Q",
            "replyto": "C5u71ph75Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3708/Reviewer_QUDF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3708/Reviewer_QUDF"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the internal-coordinate density modeling of protein structures. One issue in internal-coordinate modeling is: small fluctuations in internal coordinates can lead to large fluctuations of atoms remotely.\n  - The authors first conduct some simulations in section 2.2 to show that even while sampling from the true mean/variance of internal-coordinates, the global atom positions fluctuate a lot.\n  - The authors then proposed adding a constraint on the variance of atom positions, and finding another multivariate Gaussian model on internal-coordinates that is closest (in terms of KL divergence) to the given distribution, while satisfying the variance constraints. This is realized by an approximate solution to Lagrangian formalism.\n  - The overall framework is VAE, with the above-mentioned correction to the Gaussian distribution.\n  - Experiments are conducted mostly on MD simulation data in two scenarios: where the structures have small fluctuations and large fluctuations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper tackles a well-motivated issue in internal-coordinate modeling.\n  - The proposed solution makes sense under the framework of multivariate Gaussian model.  - The experiments do not include enough baselines to validate the practical usefulness of this method. Many related DL methods are mentioned in related work, but they are not compared in the experiments. Although the authors mentioned the main focus of this work is internal-coordinate modeling, I am still interested in a more thorough comparison."
                },
                "weaknesses": {
                    "value": "- The experiments do not include enough baselines to validate the practical usefulness of this method. Many related DL methods are mentioned in related work, but they are not compared in the experiments. Although the authors mentioned the main focus of this work is internal-coordinate modeling, I am still interested in a more thorough comparison."
                },
                "questions": {
                    "value": "1. Section 3.5: U-net is used to estimate the values for the Lagrange multipliers \\lambda for each constraint. Is there any supervision or auxiliary loss on \\lambda?\n  2. Restricting the fluctuation will restrict the representation power of the model. How to balance that except for hand-tuning the hyper-parameters?\n  3. Any analysis to compare the models learned on the unimodal scenario and multimodal scenario?\n  4. Can the model generalize to generate conformers for new proteins?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3708/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828049722,
            "cdate": 1698828049722,
            "tmdate": 1699636326682,
            "mdate": 1699636326682,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b71gN1oxxe",
                "forum": "C5u71ph75Q",
                "replyto": "6N1mIHGRyY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3708/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3708/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer QUDF"
                    },
                    "comment": {
                        "value": "First of all, we would like to thank reviewer QUDF for their insightful comments, which we will address point-by-point below.\n\n* **Baselines**:\n\n  We agree that adding more baselines is very valuable. Please refer to our general response for the newly included baselines.\n\n* **Auxiliary loss on $\\lambda$**\n\n  It is indeed important to have a regularizer on $\\lambda$ values, which we employ in the form of an MAE auxiliary loss (see Section 3.5).\n\n* **Balancing the representation power of the model**\n\n  In the current setup, balancing the representation power of the model is mostly done through hyperparameter tuning (see also appendix D). Through the latent variable model (VAE), we gain some flexibility by predicting different means (and corresponding covariance matrices). However, in a low data regime and especially for more complex proteins like 1pga, it is still non-trivial to do perfect density modelling regardless of the chosen architecture.\n\n* **Unimodal scenario vs multimodal scenario**\n\n  We stress that our main contribution is to model small fluctuations correctly, which is also where we see the most distinct difference between our method and the baselines. We included the multimodal scenario to show how our general-purpose method can potentially be used to model larger conformational changes, which is mostly realized through the latent variable mapping to different means.\n\n* **Generalization across proteins**\n\n  Generalization to new systems is currently beyond the scope of the paper, as this would require a large and diverse dataset, which we don\u2019t have at our disposal. However, we suggest that our method for incorporating constraints in internal-coordinate modelling could facilitate the next steps towards generalization when incorporated in a highly expressive model that is trained on sufficient amounts of data."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3708/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700183120229,
                "cdate": 1700183120229,
                "tmdate": 1700649187731,
                "mdate": 1700649187731,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]