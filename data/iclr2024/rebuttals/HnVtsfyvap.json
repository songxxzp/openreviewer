[
    {
        "title": "Label-efficient Training of Small Task-specific Models by Leveraging Vision Foundation Models"
    },
    {
        "review": {
            "id": "50BPGKVjHH",
            "forum": "HnVtsfyvap",
            "replyto": "HnVtsfyvap",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6234/Reviewer_Ph8P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6234/Reviewer_Ph8P"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a method called \"task-oriented knowledge transfer\" for training smaller, task-specific models by leveraging knowledge from large Vision Foundation Models (VFMs). This approach involves three steps:\n\n1. Fine-tuning the VFM on the target task using labeled data.\n2. Transferring knowledge from the fine-tuned VFM to the target model using an unlabeled dataset (transfer set) based on the knowledge distillation framework.\n3. Fine-tuning the target model with labeled target task data.\n\nAn alternative method, called \"task-agnostic knowledge transfer\", involves distilling knowledge from the frozen VFM image encoder to the target model image encoder and then fine-tuning the target model using labeled data. \n\nThe paper makes a comparison of task-oriented knowledge transfer with task-agnostic transfer, direct web-scale CLIP pretraining, and supervised ImageNet-1k pretraining. The authors show that the task-oriented knowledge transfer method performs better on many benchmarks.\n\nThe study also emphasizes the importance of the transfer set, where using a transfer set with an image distribution similar to the target task image distribution leads to better performance. For cases where a large target task-related transfer set is not readily available, the paper proposes a solution: curating task-related transfer sets using image retrieval with images from the limited labeled target task dataset as queries. Authors show that this method improved segmentation performance when compared to using a generic transfer set."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality:\n- Proposes a new task-oriented knowledge transfer approach to leverage large pretrained VFMs to train small specialized models for new tasks with limited labeled data. \n- While retrieval-based strategies have been explored before, using retrieval to curate task-related transfer sets specifically for knowledge transfer is a creative application.\nHowever, the core ideas of knowledge distillation and leveraging related datasets are not entirely new. The novelty lies in how these techniques are tailored and applied.\n\nQuality:\n- Comprehensive experiments comparing task-oriented transfer to baselines like task-agnostic transfer, CLIP pretraining, and ImageNet pretraining.\n- Also has an ablation studies analyzing impact of transfer set distribution.\n\nClarity:\n- The comparisons to baselines and ablation studies are well-presented.\n- The limitations of the approach are clearly stated.\n\nSignificance:\n- Enables leveraging powerful VFMs for specialized small model training under limited target data regimes.\n- Shows the potential to learn specialized models even for domains not well-covered by web data."
                },
                "weaknesses": {
                    "value": "- The curated task-related transfer sets are shown to be effective, but the retrieval approach to create them is rather simple. There is scope to explore more sophisticated retrieval and data selection techniques like active learning, core-set selection, adversarial filtering etc. This could potentially lead to even better task-specific transfer sets.\n- The paper could benefit from a comparative analysis with a few other knowledge transfer approaches. This would help provide a clearer picture of how the proposed method stands against existing techniques."
                },
                "questions": {
                    "value": "- The paper mentions that the target models may inherit the biases of the foundation models. It would be beneficial to discuss potential ways to address this issue.\n- Lack of detail on fine-tuning VFMs: The paper mentions using labeled target task data to fine-tune VFMs but doesn't provide much detail on this process. More information on how VFMs are fine-tuned could help readers better understand the complete methodology."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6234/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697939369282,
            "cdate": 1697939369282,
            "tmdate": 1699636681267,
            "mdate": 1699636681267,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pnxCwMHwD4",
                "forum": "HnVtsfyvap",
                "replyto": "50BPGKVjHH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6234/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6234/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your constructive feedback."
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive and constructive comments, and address their concerns below. We hope that this rebuttal addresses all their questions and convinces them to raise their ratings. Please let us know if any additional details/clarifications are needed. \n\n**Sophisticated curation approaches:**\nThe main focus of this paper is on leveraging VFMs to effectively pretrain small task-specific models, not the specific strategy used for curating the transfer set. We would like to highlight that, in all our experiments, pretraining using VFMs is better than ImageNet/CLIP pretraining even when generic CC3M dataset is used for knowledge transfer. So, it is not absolutely necessary to use task-related transfer set to outperform ImageNet/CLIP pretraining. Using a task-related transfer set just makes the performance gap even bigger. Our results on ADE20K dataset show that a simple image retrieval approach can also be effective in curating good transfer sets. We argue that **simplicity** should be considered as one of the strengths of our approach. We agree with the reviewer that a more sophisticated curation process could further improve the results, and we plan to explore this direction in our followup works.\n\n**Additional comparisons:**\nImageNet pretraining and CLIP pretraining are the most popular and widely-used pretraining approaches in the context of transfer learning. Hence, we used them for comparison in this paper. To be more comprehensive, we performed additional experiments comparing the proposed approach with an alternative transfer learning strategy: self-supervised pretraining followed by finetuning. Please see the results here: https://openreview.net/forum?id=HnVtsfyvap&noteId=7vBTW2DhMA\n\nOur main goal is to establish \u201cAdapt VFM \u2192 Transfer knowledge across architectures \u2192 Finetune small model\u201d as a highly effective strategy for training small task-specific models. In this work, we used supervised finetuning strategy for adapting VFMs and feature/task-prediction matching for transferring knowledge across architectures, and showed that we already outperform other widely-used approaches. More sophisticated finetuning/distillation approaches can easily be incorporated into our general strategy to further improve the performance.\n\n**Details of finetuning VFMs:**\nThe process of finetuning VFMs is similar to the process of finetuning small models. We attach a randomly initialized task-specific head to a pretrained VFM encoder, and finetune the entire model. The details of task-specific heads are described in Appendix A.1. The optimizer,  learning rates, number of epochs and image augmentations used for finetuning VFMs are same as the ones used for finetuning small models. Please see Appendix A.2 for details. We will make these details clearer in the final draft. Please let us know if there are any specific details that we may have missed. \n\n**Biases:**\nA VFM pretrained on web data may have its own inherent biases that may be useful/harmful for a particular target task. The proposed approach advocates for adapting pretrained VFMs to target task/domain before transferring their knowledge to small models. This strategy gives us an opportunity to enhance/suppress desired/irrelevant knowledge in VFMs by using appropriate target domain data. Using a well-curated and balanced target domain set for knowledge transfer can also help us mitigate some of the biases. This is an interesting line of research for followup works."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6234/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699724645594,
                "cdate": 1699724645594,
                "tmdate": 1699724789714,
                "mdate": 1699724789714,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GBGwZP3Feg",
                "forum": "HnVtsfyvap",
                "replyto": "pnxCwMHwD4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6234/Reviewer_Ph8P"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6234/Reviewer_Ph8P"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the follow up answer."
                    },
                    "comment": {
                        "value": "I appreciate the author's answers. My rating stays the same."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6234/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632217795,
                "cdate": 1700632217795,
                "tmdate": 1700632217795,
                "mdate": 1700632217795,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pec4YCfsC4",
            "forum": "HnVtsfyvap",
            "replyto": "HnVtsfyvap",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6234/Reviewer_H2Cx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6234/Reviewer_H2Cx"
            ],
            "content": {
                "summary": {
                    "value": "This work study the problem of knowledge transfer from vision foundation model. Specifically, VFM are use to train a small task-specific dataset with limited labeled training data."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ Proposed a three stages training strategy to train small DNN models with limited labeled training data by utiziling knowledge from VFMs. The VFM is first finetuned with the target task data and task-specific head, hence the finetuned CFM is more suitable for transfers task-oriented knowledge to the target small model with a transfer set.\n+ This work observe that the transfer dataset distribution play an important role on the knowledge transfer, and empirically validate several approaches to curate the transfer dataset.\n+ The resulting models (demonstrated with two VFMs and two mobile target architectures) show good performance than task-agnostic VFM distillation or pre-trained models. The empirical results support several insights that are benefitial to the research community."
                },
                "weaknesses": {
                    "value": "This work show a task-oriented knowledge transfer strategy. The method is simple and intuitive. One might be complaining all the computation tools are from prior art, and the technical novelty is low. But i think such work bring good contribution to the research community, to more effectively train specialist small model. I don't see major flow on the proposed approach."
                },
                "questions": {
                    "value": "On Fig 2(a), what are the three lines for DINOV2 - MobileViT-V2 - CC3M?\n\nNote: the uploaded supplementary material is almost identical with the main paper, but with broken reference in text."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6234/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698306931889,
            "cdate": 1698306931889,
            "tmdate": 1699636681135,
            "mdate": 1699636681135,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hzsVWBmXUi",
                "forum": "HnVtsfyvap",
                "replyto": "pec4YCfsC4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6234/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6234/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the positive review."
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for their positive feedback and for explicitly calling out that this work brings good contributions to the research community. We believe that **simplicity** is one of the greatest strengths of our approach and we appreciate the reviewer for supporting us on this. We hope that this rebuttal addresses all their questions and convinces them to raise their ratings. Please let us know if any additional details/clarifications are needed. \n\n**Fig. 2(a):** \nSorry to say that we did not understand your question fully. The red curve in all the plots corresponds to fine-tuned VFM. Hence, when the VFM is DINOV2, the red curves correspond to fine-tuned DINOV2 (not fine-tuned OpenCLIP). We will add a separate legend for plots corresponding to DINOV2 to clarify this. After the submission, we learned that some of the plots are not showing up correctly in some viewers. If this the case, please view the paper using Adobe/Chrome.\n\n**Supplementary material:**\nThank you for pointing this out.  A copy of the main paper was uploaded as supplementary material by mistake. \n\n**New additional results:**\nTo further strengthen the paper, we conducted additional experiments. Please see https://openreview.net/forum?id=HnVtsfyvap&noteId=7vBTW2DhMA"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6234/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699721460046,
                "cdate": 1699721460046,
                "tmdate": 1699721460046,
                "mdate": 1699721460046,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IsOMQxazrL",
                "forum": "HnVtsfyvap",
                "replyto": "hzsVWBmXUi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6234/Reviewer_H2Cx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6234/Reviewer_H2Cx"
                ],
                "content": {
                    "comment": {
                        "value": "I acknowledge the author rebuttal. I will consider the rebuttal and fellow reviewers' opinion for final rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6234/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720720459,
                "cdate": 1700720720459,
                "tmdate": 1700720720459,
                "mdate": 1700720720459,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Zbv1NunWC7",
            "forum": "HnVtsfyvap",
            "replyto": "HnVtsfyvap",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6234/Reviewer_KUqe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6234/Reviewer_KUqe"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies how we can train a smaller model for a new target task with limited training data using vision foundation models. They propose two frameworks task-agnostic and task-oriented for distilling data from foundation model to smaller model. Task agnostic or task oriented knowledge distillation involves creation of target set to transfer information from vision foundation model to smaller model. They show that their task agnostic approach or task oriented approach beats the models pretrained on large datasets like Imagenet or web-scale CLIP pretraining."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Simple and clean experiments. Paper is well written as well. They experimented with different datasets including imabalnced ones showing the effectiveness of their approach.\n- The paper shows that task agnostic and task oriented knowledge transfer beats the imagenet pretraining and CLIP based pre-training."
                },
                "weaknesses": {
                    "value": "- Most of the findings in the paper are obvious like task oriented or task agnostic knowledge transfer would lead to better performance than model pre-trained on CLIP or Imagenet because we are creating transfer set similar to target dataset.\n- Transfer set creation process is mostly heuristic based and it's one of the important factors to get good performance on the target task."
                },
                "questions": {
                    "value": "- Based on the figure 5 (left) if we use random transfer set the performance of the model on target task is similar to models pretrained on Imagenet or CLIP based pretraining. \n- Additionally, it's essential to clarify whether the target models, specifically MobileViT-v2, are pre-trained. If they are indeed pre-trained, the comparison between a 'normal' pre-trained model and a fine-tuned model (IM-Pretrain, CLIP-Pretrain), versus a model that undergoes pre-training followed by further fine-tuning with task-specific or task-agnostic training (Task-Agn, Task-Oriented), may lack meaningful context."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6234/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6234/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6234/Reviewer_KUqe"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6234/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698354811543,
            "cdate": 1698354811543,
            "tmdate": 1699636681012,
            "mdate": 1699636681012,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B1rBecDGaN",
                "forum": "HnVtsfyvap",
                "replyto": "Zbv1NunWC7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6234/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6234/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the positive review."
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for their positive feedback and address their concerns below. We hope that this rebuttal addresses all their questions and convinces them to raise their ratings. Please let us know if any additional details/clarifications are needed. \n\n**Obvious findings:**\nAs far as we know, there is no existing work that concretely establishes the findings presented in this paper about leveraging VFMs for training mobile models on a new task with limited labeled data. Especially, we show that\n\n1. Knowledge transfer (both task-oriented and task-agnostic) from a VFM (even with generic CC3M transfer set) is a better pre-training strategy for small models when compared to supervised ImageNet or web-scale CLIP pretraining. \n2. Task-oriented knowledge transfer from VFMs is better than task agnostic transfer. \n\nThe reviewer seems to be under the impression that proposed knowledge transfer outperforms ImageNet/CLIP pretraining because we are curating transfer sets. We would like to emphasize that knowledge transfer outperforms ImageNet/CLIP pretraining even when using the generic CC3M transfer set. A curated transfer set just makes the performance gap even bigger. We strongly believe that these non-trivial findings corroborated by our thorough experimental validation would be very useful to the community, especially given that VFMs are becoming larger and difficult to deploy as time progresses.\n\n**Transfer set creation:**\nThe main focus of this paper is on leveraging VFMs to effectively pretrain small task-specific models, not the specific strategy used for curating transfer set. Our results on ADE20K dataset show that a simple image retrieval approach can also be effective in curating good transfer sets. We agree that a  more sophisticated curation process could further improve the results. However, that is not the main focus of this paper and we plan to explore this direction in our followup works.\n\n**Random transfer set performance:**\nRandom transfer set performance in Fig. 5(a) is not similar to ImageNet/CLIP pretraining as claimed by the reviewer. It indeed outperforms ImageNet/CLIP pretraining. Below are the ADE20K segmentation performances when using 4800 labeled images:\n\n| Fig. 5(a) - Random subset of YFCC (154K images) | 35.7 |\n|---|---|\n| Table 3 - ImageNet pretraining | 33.1 |\n| Table 3 - CLIP pretraining | 27.6 |\n\n\n**Additional pretraining:**\nWhen using the proposed knowledge transfer approaches (task-oriented or task-agnostic), target models are trained from scratch. We agree that starting knowledge transfer process from an already pretrained model would be an unfair comparison. We did not do that. We will make this clear in the final draft.\n\n**New additional results:** \nTo further strengthen the paper, we conducted additional experiments. Please see https://openreview.net/forum?id=HnVtsfyvap&noteId=7vBTW2DhMA"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6234/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699723116283,
                "cdate": 1699723116283,
                "tmdate": 1699723116283,
                "mdate": 1699723116283,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wYxQZ2tLgC",
            "forum": "HnVtsfyvap",
            "replyto": "HnVtsfyvap",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6234/Reviewer_M3yw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6234/Reviewer_M3yw"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates how to construct a compute-efficient model from a pre-trained vision model for a downstream task with limited labeled training examples. To approach the task, the authors propose to first fine-tune the pre-trained vision model with the labeled data, distill it into a smaller model with a transfer set, and then fine-tune the distilled model with the labeled dataset. The effects of using in-domain and out-of-domain transfer sets are investigated and experiments are conducted to show the superiority of the approach against multiple baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The investigation of using non-in-domain data for distillation and selecting relevant data for distillation is interesting."
                },
                "weaknesses": {
                    "value": "1. Lacking Novelty: In essence, the proposed approach is knowledge distillation. Under the setup the authors consider, the proposed approach is the most straightforward approach one could consider. While the investigation of the use of not in-domain transfer dataset is interesting, the finding \u2014 when sufficient in amount, in-domain transfer set is better \u2014 is somewhat expected (e.g prior work[1] on open-set semi-supervised learning has shown that out-of-domain example can hurt performance of semi-supervised algorithm). It would be more interesting if the authors investigated the use of data from different domains (medical, satellite, clipart, etc) for distillation and proposed a strategy for selecting the right data for distillation. The current submission does contain some investigation on selecting the right data for distillation (section 3.5), but the investigation is not thorough enough to convince the reviewer that their strategy is applicable for different target tasks and with data from different domains. \n2. Problematic Baselines: One would consider using the task-agnostic approach if fine-tuning the pre-trained models could be an issue (perhaps due to compute-constraint) but since the authors only consider compute-constraint during inference (instead of training), it seems unnatural to consider task-agnostic distillation as a baseline. In addition, the CLIP-pretrain baseline is trained on an internal dataset. Without knowing the distribution/statistics of the data and how the dataset compares to CC3M or DataComp-1B, it is difficult to understand whether the CLIP-pretrain baseline is worse because contrastive pretraining is not the right approach or the data used for training is flawed \n\nCitation:\n[1] Saito, Kuniaki, Donghyun Kim, and Kate Saenko. \"Openmatch: Open-set semi-supervised learning with open-set consistency regularization.\" Advances in Neural Information Processing Systems 34 (2021): 25956-25967."
                },
                "questions": {
                    "value": "Questions:\n1. 3.1 Alternative Approaches CLIP-Pretrain: The authors mentioned using a loss function similar to CLIP. Could the authors clarify what the difference is? \n2. Have the authors tried distilling from the patch tokens from CLIP? \n\n\nSuggestions:\n1. Additional baseline: CLIP is a weakly supervised pre-training approach, the reviewer recommends adding self-supervised pre-training approaches such as DINO.\n2. Transfer set: The use of not in-domain transfer set is worth more explanation. Right now, there is not much explanation/description on the use of not in-domain transfer set (e.g. why an in-domain transfer set is difficult to obtain).\n3. Distillation mechanism: Distilling patch features is important for the segmentation task. However, there are only a few sentences describing the process. The reviewer recommends expanding the section on loss function either via using equations (the use of contrastive loss for distillation is uncommon) or by providing a figure explaining the process. \n\nPre-rebuttal rating: Overall, the paper contains some interesting investigations on how to use distillation for constructing a small model from a large model pre-trained on different source tasks. However, the investigations are so far incomplete and the quality is not up to par with ICLR\u2019s standards. The reviewer thus recommends rejecting the submission."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6234/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6234/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6234/Reviewer_M3yw"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6234/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698706282829,
            "cdate": 1698706282829,
            "tmdate": 1699636680872,
            "mdate": 1699636680872,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2lBBYSzWuJ",
                "forum": "HnVtsfyvap",
                "replyto": "wYxQZ2tLgC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6234/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6234/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your constructive feedback."
                    },
                    "comment": {
                        "value": "We thank the reviewer for their constructive suggestions and address their concerns below. We hope that this rebuttal addresses all their questions and convinces them to raise their ratings. Please let us know if any additional details/clarifications are needed. \n\n**Novelty:**\nAs far as we know, there is no existing work that concretely establishes the findings presented in this paper about leveraging VFMs for training small models on a new task with limited labeled data. Especially, we show that\n\n1. Knowledge transfer (both task-oriented and task-agnostic) from a VFM (even with generic CC3M transfer set) is a better pre-training strategy for small models when compared to ImageNet/CLIP pretraining. \n2. Task-oriented knowledge transfer from VFMs is better than task-agnostic transfer. \n\nWe strongly believe that our findings corroborated by thorough experimental validation would be very useful to the community, especially given that VFMs are becoming larger and difficult to deploy as time progresses. \n\nThe main focus of this paper is on leveraging VFMs to effectively pretrain small task-specific models, not the specific strategy used for curating the transfer set. Please note that knowledge transfer is better than ImageNet/CLIP pretraining even when generic CC3M transfer set is used. So, it is not necessary to use task-related transfer set to outperform ImageNet/CLIP pretraining. Using a task-related transfer set just makes the performance gap bigger. Our results on ADE20K show that a simple image retrieval approach can be effective in curating good transfer sets. While a more sophisticated curation process could further improve the results and **transfer set curation** is an interesting topic worth exploring in further detail, it is not the main focus of this work. However, we agree that it would be an excellent topic for followup works.\n\n**Baselines:**\nSorry to say that we did not understand why task-agnostic distillation is \u201cunnatural\u201d.  Task-agnostic distillation is a valid approach to leverage VFMs, and hence we compared it with the proposed task-oriented knowledge transfer in a fair manner using same amount of labeled and unlabeled data.\n\n**CLIP dataset:**\nWe use the 1.1B image-text pairs used in the following papers: \u201cSTAIR: Learning Sparse Text and Image Representation in Grounded Tokens\u201d and \u201cRangeAugment: Efficient Online Augmentation with Range Learning\u201d.\nViT-B/16 and ViT-H/16 CLIP models trained on this dataset achieve 72.8% and 78.6% ImageNet zero-shot accuracy which are comparable to OpenCLIP models trained on DataComp-1B. These results confirm that the dataset we use is a high quality dataset.\n\n**CLIP-Pretrain:**\nSorry for the confusion. Our loss function is same as the loss function in the original CLIP paper. By \u2018similar\u2019 we simply meant \u2018same\u2019.\n\n**Distilling patch tokens from CLIP:**\nWe tried this in our early experiments but did not observe promising results. DINOV2 patch features are significantly better than OpenCLIP patch features for segmentation. This is because DINOV2 has been explicitly trained to produce good patch features while OpenCLIP was trained only with a loss on CLS features. Below are the performances of OpenCLIP-ViT-L/14 and DINOV2-ViT-L/14 models on the ADE20K segmentation dataset with Deeplab-V3 segmentation head:\n\n| Labeled images | 1200 | 2401 | 4802 | 9605 |\n|---|---|---|---|---|\n| OpenCLIP | 31.82 | 36.69 | 40.22 | 42.85 | \n| DINOV2 |  39.32 | 42.76 | 46.35 | 49.42 |\n\nDINOV2 outperforms OpenCLIP by a significant margin. Hence, we did not further explore the direction of distilling inferior CLIP patch features. The following paper also observed that patch features of CLIP models trained with CLS token are not good for dense predictions tasks: \u201cPerceptual Grouping in Contrastive Vision-Language Models\u201d.\n\n**Comparison with DINO:** Please see https://openreview.net/forum?id=HnVtsfyvap&noteId=7vBTW2DhMA\n\n**Not in-domain transfer set:**\nIf we have large enough task-related data, we should always use that as transfer set. However, the amount of readily available task-related data depends on the domain/task. For example, the number of images in AD20K dataset is around 20K which is not large enough as demonstrated by our experiments. We get better results by using not in-domain CC3M transfer set instead of the limited in-domain ADE20K dataset (see Fig. 4). We will make this more clear in the final draft.\n\n**Distillation loss:**\nWe would like to clarify that we do not use contrastive loss for distilling patch features. As mentioned in Sec 3.1 (towards the end of the paragraph on loss function), we use standard cosine similarly loss for patch feature distillation. Contrastive loss is used when distilling global image features following the popular contrastive representation distillation approach. Yonglong Tian, Dilip Krishnan, Phillip Isola, \u201cContrastive Representation Distillation\u201d, ICLR 2020. We will describe the loss functions in further detail in the final version."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6234/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699729642565,
                "cdate": 1699729642565,
                "tmdate": 1699729712630,
                "mdate": 1699729712630,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Gjx03TGVe7",
                "forum": "HnVtsfyvap",
                "replyto": "2lBBYSzWuJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6234/Reviewer_M3yw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6234/Reviewer_M3yw"
                ],
                "content": {
                    "title": {
                        "value": "Responses to the rebuttal"
                    },
                    "comment": {
                        "value": "The reviewer thanks the authors for the detailed responses and clarifications!\n\nRe Novelty: \n1. [1] considers training small models with limited labeled data using large pre-trained models. The differences in setup between [1] and the submission are (a) multiple pre-trained models are available and (b) no fine-tuning of the pre-trained models is allowed. The reviewer would argue that the setup considered in [1] is much more realistic since with the convenience of huggingface, multiple pre-trained models are accessible; it is unclear why one would restrict themselves to just a single pre-trained model as considered in the submission. Besides, the premise of the work is to construct compute-efficient models. Extrapolating from the development of large models in NLP plus the current rate of development for vision frontier models [2], the reviewer would imagine that next-generation frontier models would be much bigger than what we have right now --- so big that it might not even be possible for a lot of organizations/users to even fine-tune these models (e.g GPT-4) without reliance on corporate-level computes. The fact that the proposed approach relies on fine-tuning the pre-trained models first seems restrictive, assuming we are gonna get larger frontier models. \n2. Also, similar results on knowledge distillation/transfer outperforming ImageNet pretraining as also been reported [1]. It is not surprising that similar results would hold for CLIP. The use of a generic CC3M transfer set is interesting. Still, given that both the transfer set and most downstream tasks are internet imagery, the results are again not beyond expectation. \n3. task-oriented vs task-agnostic: the pre-trained models are already fine-tuned to be more well-suited for the downstream tasks so it is not beyond expectation that the task-oriented approach is much better. \n\nRe Baselines:\n1. The reviewer apologizes for the poor choice of words. Instead of \"unnatural\", \"unfair\" would be the more fitting option. \n2. The reviewer thinks the task-agnostic distillation is unfair since the task-oriented approach leverages fine-tuning which would consume much more compute than the task-agnostic approach. The reviewer cannot help but wonder: with the same amount of compute, could the task-agnostic approach outperform the task-oriented approach? For instance, with the same amount of compute, one could construct an ensemble of downstream models and then further distill this ensemble into a single model. Prior work has shown that distilling an ensemble can yield more performant models[3] and it is unclear whether distilling an ensemble would be better than the proposed task-oriented approach. \n3. Also, since supervised fine-tuning is on the table, why not consider semi-supervised fine-tuning as well?\n\n\n\n\n\nReferences: \n\n[1] Borup, Kenneth, Cheng Perng Phoo, and Bharath Hariharan. \"Distilling from Similar Tasks for Transfer Learning on a Budget.\". In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 11431-11441. ICCV, 2023. \n\n[2] Dehghani, Mostafa, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner et al. \"Scaling vision transformers to 22 billion parameters.\" In International Conference on Machine Learning, pp. 7480-7512. PMLR, 2023.\n\n[3] Malinin, Andrey, Bruno Mlodozeniec, and Mark Gales. \"Ensemble distribution distillation.\" arXiv preprint arXiv:1905.00076 (2019).\n\n[4] Abuduweili, Abulikemu, Xingjian Li, Humphrey Shi, Cheng-Zhong Xu, and Dejing Dou. \"Adaptive consistency regularization for semi-supervised transfer learning.\" In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6923-6932. 2021."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6234/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363368897,
                "cdate": 1700363368897,
                "tmdate": 1700363368897,
                "mdate": 1700363368897,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qeVKvcjXjd",
                "forum": "HnVtsfyvap",
                "replyto": "wYxQZ2tLgC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6234/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6234/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional response to reviewer M3yw"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer again for their time and feedback. We hope that this response address their concerns and they would reconsider their score.\n\nWe appreciate the reviewer's effort in bringing [1] to our attention. We will include related discussion in our revised version. Similar to our work, [1] focuses on learning a small task-specific model with limited labeled data, and shows that knowledge distillation from the right teacher model (the focus of their work) is better than ImageNet pretraining. However, there are many differences between our work and [1], and our work conveys several important and novel messages that are missing in [1]:\n\n**Comparison with CLIP**  [1] does not compare with CLIP training. We show that knowledge transfer from VFMs outperforms CLIP training. We would like to respectfully disagree with the reviewer comment \"It is not surprising that similar results would hold for CLIP\". None of the existing works show how effective web-scale CLIP pretraining is for transfer learning in the context of small models. In fact, one would have expected that due to the scale of data, CLIP pretraining may outperform ImageNet pretraining and may even be competitive with knowledge transfer. **Our experiments surprisingly show the opposite trends**.\n\n**Works without target domain unlabeled data** [1] assumes the availability of target domain unlabeled dataset. In contrast, we show that even when target-task unlabeled data is unavailable, knowledge transfer from VFMs using generic internet imagery is highly effective. We think **this is interesting and beyond expectation**. Note that HAM10K data is quite different from typical internet imagery and still knowledge transfer using CC3M dataset outperforms other popular pretraining strategies.\n\n**Experiments with VFMs**\nNote that [1] does not experiment with VFMs. Their teacher models are relatively small CNNs trained on small-scale classification datasets. So, the utility of [1] is unclear in the context of large scale VFMs trained on web-scale datasets. Different from [1], we experiment with VFMs and present many interesting results.\n\n\u2028**Transfer set curation** We a present a simple and effective transfer set curation method using retrieval, and further demonstrate the importance of a balanced curation with respect to seed query set (Figure 5). In contrast, [1] assumes that a large target domain unlabeled dataset is always available.\n\n**Knowledge transfer formulation** [1] assumes that source models are classification models. They use a distillation loss that matches the target and source model predictions in the label space of source model (see Eq. (3) in [1]).  Unfortunately, most of the existing VFMs (CLIP, DINOV2, SAM, etc.) are not supervised classification models, and the formulation of [1] is not directly applicable to them. One could extend [1] to VFMs by using feature matching instead of source label matching as the loss function. Then, it becomes similar to our task-agnostic transfer. As shown in our results, task-agnostic transfer is inferior to task-oriented transfer.\n\n**Works well for segmentation task** [1] only experimented with classification tasks. We show the effectiveness of knowledge transfer from VFMs for semantic segmentation task.\n\n**Comparison with SSL** [1] does not compare with the DINO, which is a popular SSL pretraining approach. Our new results show that knowledge transfer from VFMs is significantly better than DINO pretraining.\n\n\n**Training compute** Note that our work focuses on inference time compute constraints and hence on small task-specific models. While experimenting under a fixed training budget is also an interesting problem, that is not the focus of this work. That said, finetuning the VFMs we used in this work is not as costly as the reviewer might be thinking. The key thing to notice here is that **VFMs are fine-tuned only on a small labeled dataset**. The actual compute used for finetuning VFM is a small fraction of the compute used for distillation in our experiments. For example, finetuning OpenLCIP-ViT-L on Places365 dataset with 50 images/class for 200 epochs takes around 16 A100 GPUh, and distilling frozen OpenCLIP model on full unlabeled Places dataset for 200 epochs takes around 370 A100 GPUh. Also, as VFMs become larger and larger in the future, one could use efficient fine-tuning approaches, such as LoRa[2], which have been shown to be quite effective in the NLP community.  \n\n\n**Using multiple models and semi-supervised fine-tuning**  We agree that we can further improve the proposed approach by semi-supervised finetunig of VFMs or by using multiple VFMs. We will look into these directions in the followup works. However, this **doesn\u2019t change the significance of any of the conclusions presented in this paper**. Note that, while finetuning a VFM on a small labeled dataset is computationally inexpensive, finetuning it in a semi-supervised fashion on large unlabeled dataset may not be."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6234/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700373360161,
                "cdate": 1700373360161,
                "tmdate": 1700469162354,
                "mdate": 1700469162354,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]