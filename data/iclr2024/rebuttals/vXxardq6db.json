[
    {
        "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns"
    },
    {
        "review": {
            "id": "51aEAegduG",
            "forum": "vXxardq6db",
            "replyto": "vXxardq6db",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5654/Reviewer_jAXk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5654/Reviewer_jAXk"
            ],
            "content": {
                "summary": {
                    "value": "The authors of this paper describe their methodology as a transformation of a Transformer network from LayerNorm to RMSNorm. They implement an approach involving the application of orthogonal-matrix transformations and the selective removal of columns and rows from the transformed weight matrices. This process is aimed at reducing the overall model size while preserving performance integrity. The results of their research demonstrate a significant improvement in perplexity on benchmark datasets, OPT and Llmas2, aligning with the 2:4 scheme and underscoring the substantial enhancement in model efficiency and accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The formulation is clear and enhanced by illuminating diagrams for better comprehension.\n* The experimental results illustrate the method's effectiveness, establishing a well-defined trade-off between accuracy and sparsity."
                },
                "weaknesses": {
                    "value": "The experimental section has certain shortcomings:\n\n1. The experiment section does not comprehensively address the comparison between SliceGPT and SparseGPT. While 2:4 sparsity implies a 50% compression rate, Table 1 exclusively showcases SliceGPT with up to 30% compression. This limitation hinders a clear conclusion regarding the superior performance of SliceGPT over SparseGPT.\n\n2. The absence of inference time data for SparseGPT in the experiments makes it challenging to convincingly demonstrate the superior efficiency of SliceGPT.\n\n3. The paper lacks a comparative analysis with state-of-the-art pruning methods such as low-rank approximation, unstructured sparsity, and block sparsity. The omission of these comparisons limits the paper's ability to establish the competitiveness of SliceGPT within the broader context of pruning techniques."
                },
                "questions": {
                    "value": "* Can you show the performance (perplexity, inference time) of SliceGPT at 50% sparsity?\n* Can you show the inference time of SparseGPT in comparsion to SliceGPT under the same experimental setup?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concern."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5654/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5654/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5654/Reviewer_jAXk"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5654/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698413917910,
            "cdate": 1698413917910,
            "tmdate": 1700070814354,
            "mdate": 1700070814354,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cIuAqIH54E",
                "forum": "vXxardq6db",
                "replyto": "51aEAegduG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5654/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5654/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Hi reviewer jAXk, thanks for taking the time to review our work. \n\n> _experiment section does not comprehensively address the comparison between SliceGPT and SparseGPT_\n\nWe are working on this, see main rebuttal. \n\n> _The absence of inference time data for SparseGPT in the experiments makes it challenging to convincingly demonstrate the superior efficiency of SliceGPT. (and Q2)_\n\nThis was a challenge for us: the SparseGPT authors do not provide a way to verify and kind of speedup using their method. See for example this [github issue](https://github.com/IST-DASLab/sparsegpt/issues/15).  The issue is that pytorch does not provide off-the-shelf support for structured sparsity. This means that at the moment, it\u2019s not possible to verify end-to-end speedup for SparseGPT. Our work-around is to provide end-to-end results for SliceGPT, and apples-to-apples comparisons with SparseGPT on their terms. Please see main rebuttal for more details and forthcoming experiment.  \n\n> _lack of comparative analysis with state-of-the-art pruning methods such as low-rank approximation, unstructured sparsity, and block sparsity._ \n\nYou\u2019re right \u2013 we should add some rationale here. \n\n1. different sparsities. We compared to SparseGPT 2:4 sparsity because this is the SOTA method for LLMs. SparseGPT also provides unstructured sparsity and other sparsity structures, *but these provide little to no speedup*, which is the motivation the developers of  2:4 sparsity. This is why we chose SparseGPT 2:4 as a baseline, because it\u2019s the strongest competitor. If we\u2019re doing better than this, we\u2019re doing well. \n\n2. Low rank approximations. We did make reference to low-rank works, which take each weight matrix and replace it with a pair of low-rank matrices. To our knowledge, no one has applied this idea to LLMs, and as we discussed in the text, it seems unlikely that they could improve the performance of LLMs. Of course, the [LoRA method]( https://arxiv.org/abs/2106.09685) has been very influential for fine-tuning, but we\u2019re not aware of a method to use low-rank approximations in improving deployment of LLMs. \n\n3. Other works. We\u2019ve added reference to [LLM-pruner](https://arxiv.org/pdf/2305.11627.pdf), a modern method for pruning LLMs. LLM-pruner provides less speedup than SliceGPT (see their table 3), requires fine-tuning post sparsity, and does not retain perplexity as well as our SliceGPT method (see their table 1). We\u2019ve also added more context around other pruning methods, see response to reviewer 96nz. \n\n>_Can you show the performance (perplexity, inference time) of SliceGPT at 50% sparsity? (Q1)_\n \nYes, of course. We ran a small experiment for you just now: on OPT66B with 50% removal, SliceGPT achieves a wikitext-2 ppl of 11.39 (against a dense baseline of 9.34). Fixing the sequence length to 128, we were able to achieve a batchsize of 32 on 1x H100 card, leading to a throughput of 440 tokens/s. For the dense model, we required 2x H100 cards and could only fit a batchsize of 16, leading to 141 tokens/s. That\u2019s a 3.12x speedup, and we only use one card! Please read our main rebuttal to understand the nuance of comparing SliceGPT with SparseGPT, as well as a more comprehensive experiment on the effect of batchsize."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699882790784,
                "cdate": 1699882790784,
                "tmdate": 1699882790784,
                "mdate": 1699882790784,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ac12tNqjvx",
                "forum": "vXxardq6db",
                "replyto": "cIuAqIH54E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5654/Reviewer_jAXk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5654/Reviewer_jAXk"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed explanation. It's now evident to me that SliceGPT employs compression on both activations and weights, so 25% removal is achieving a similar speedup with a 2:4 sparsity on weights. I can observe that SliceGPT is able to perform comparably to Sparse GPT (2:4) in terms of both speedup and perplexity. \n\nA significant contribution appears to be SliceGPT's flexibility, allowing for adjustable slicing levels in both activations and weights, and its compatibility with existing PyTorch operations.\n\nThe authors have adequately addressed my concerns and provide more comprehensive analysis. I have adjusted my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700070790563,
                "cdate": 1700070790563,
                "tmdate": 1700070790563,
                "mdate": 1700070790563,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JC2HR8LnSv",
            "forum": "vXxardq6db",
            "replyto": "vXxardq6db",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5654/Reviewer_fXjP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5654/Reviewer_fXjP"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces SliceGPT, a method to reduce the size of matrices for inference of LLMs. The method uses orthogonal matrices to project to a lower-dimensional space the weight matrices, these orthogonal matrices being constructed using PCA."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well written and pleasant to follow. Ideas are simply explained and figures are helping the understanding. \n- Experimental results are convincing.\n- I think this method could be really used in practice to reduce inference time."
                },
                "weaknesses": {
                    "value": "- I think section \"layernorm transformers can be converted to RMSnorm\" is not well motivated. Could the authors explain more in details the subtleties of this section and why it was written? I may have missed the point.\n- I'll wait for other reviewers weaknesses to see whether I agree with them."
                },
                "questions": {
                    "value": "- Do the authors plan to release the code? I think open sourcing it is very important for the community.\n- The latex is broken, citations are not redirecting, I think your should recompile the pdf.\n- Could the authors comment on the use of a random projection (which is orthogonal in expectation, as in sketching methods) compared to $Q_\\ell$ computed using by PCA, which is more expensive?\n- In practice, not all layers may be equivalent signal-wise, could the authors comment on the possible use of weight watcher ( https://github.com/CalculatedContent/WeightWatcher ) to analyze how to select a different projection dimension for each layer? This question is purely curiosity but I think, combining both SliceGPT and weight watcher could greatly improve the method.\n- p8: what do the authors mean by \"using dense kernels in our compressed models\"? Did they code specific kernels for SliceGPT?\n- I think the authors should write a small proof of Equation (2) to increase the readability of the paper. Can the authors provide it in their answer?\n- \"Theorem\" is too strong for Theorem 1, I suggest \"Proposition\" or \"Remark\".\n- p4: typo: OBC instead of OBS.\n\nOverall I liked the paper and the method, and satisfying answers to my questions and weaknesses would make me consider increase my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5654/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698683615522,
            "cdate": 1698683615522,
            "tmdate": 1699636588484,
            "mdate": 1699636588484,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Er3kE8ppJt",
                "forum": "vXxardq6db",
                "replyto": "JC2HR8LnSv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5654/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5654/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> _layernorm transformers can be converted to RMSnorm\" is not well motivated._\n\nApologies if this wasn\u2019t clear. SliceGPT depends on the computational-invariance trick. In turn, this depends on the commutation property of equation (2) \u2013 you can apply an orthogonal transform before-and-after RMSNorm without changing the model output. The trick does not work for LayerNorm, so we have to convert a network to RMSNorm before we apply SliceGPT. \n\n> _Do the authors plan to release the code?_\n\n100%, we feel the same as you. It\u2019s difficult for us to release pre-publication without de-anonymizing ourselves, but our code will be on github with an MIT license as soon as we\u2019re published. If you would like to inspect the code this week, we\u2019re happy to arrange that (though it will take a while for us to go through the code, to be certain that we\u2019re not de-anonymizing ourselves). \n\n> _recompile the pdf_\n\nWill do! We blame overleaf \ud83d\ude0a\n\n> _use of a random projection_\n\nWe could indeed use a random projection, but it would break eq. 2. It\u2019s not clear to us that random projections could help here, but we hope that you experiment with our code and let us know on release!\n\n> _not all layers may be equivalent signal-wise_\n\nYou\u2019re right! We\u2019re preparing such an experiment, see main rebuttal.\n\n> _what do the authors mean by \"using dense kernels in our compressed models\"?_\n\nAh, this is something we haven\u2019t communicated very well. Sparse models, like those produced by SparseGPT, are actually not easy to run. Pytorch does not have  off-the-shelf support (kernels) for 2:4 structured sparsity yet. In fact, those authors decline to provide actually accelerated models in this Github issue: https://github.com/IST-DASLab/sparsegpt/issues/15. We simply meant that the models produced by SliceGPT are easy to run in standard pytorch. We were trying to be diplomatic about their (very cool) contribution, whilst letting you know that our code is easy to run. \n\n> _small proof of Equation (2)_\n\nSure, we can add one to the appendix. \n\n> _\"Theorem\" is too strong for Theorem 1, I suggest \"Proposition\" or \"Remark\"._\n\nSure, fixed. \n\n> _Typo_\n\nFixed, thanks."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699882330571,
                "cdate": 1699882330571,
                "tmdate": 1699882330571,
                "mdate": 1699882330571,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O8jLf0I59r",
                "forum": "vXxardq6db",
                "replyto": "JC2HR8LnSv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5654/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5654/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Update"
                    },
                    "comment": {
                        "value": "Hi Reviewer fXjP,\n\nWe hope we managed to give satisfying answers to your questions in our reply above. One of your questions which we did not fully answer was about varying the dimension of each layer: we added experiment (3) in the main rebuttal which demonstrates the feasibility of this, with performance (perplexity) gains for OPT models but degradation in Llama models. We believe varying sparsity/slicing by layer is an important area of research, and hope that a future work can devise a such a scheme for SliceGPT that works for more models than just OPT.\n\nShould you have any further questions, please let us know. If not, we would be very grateful if you were to consider increasing your score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261086629,
                "cdate": 1700261086629,
                "tmdate": 1700261086629,
                "mdate": 1700261086629,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KIqN3z4aMK",
                "forum": "vXxardq6db",
                "replyto": "O8jLf0I59r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5654/Reviewer_fXjP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5654/Reviewer_fXjP"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for the extensive experiments they did in their rebuttal.\nWhile I think a more principled way to choose the slicing level layer-wise would greatly improve this paper, I think the authors did some interesting preliminary experiments in that direction. \\\nI am therefore increasing my score from 6 to 7. However, since this year at ICLR, we can give only a 6 or a 8, $\\textbf{I am keeping my score at 6, but it should be understood as a 7.}$"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700324841025,
                "cdate": 1700324841025,
                "tmdate": 1700324841025,
                "mdate": 1700324841025,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lOwJWQiDrC",
            "forum": "vXxardq6db",
            "replyto": "vXxardq6db",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5654/Reviewer_96nz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5654/Reviewer_96nz"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a technique for pruning neurons in Tranformer architectures based on a clever application of orthogonal matrices, which enables PCA-based elimination of rows and columns throughout the architecture. The authors evaluate their technique on a range of large language models from the OPT and Llama-2 families and demonstrate improvements in inference runtime on GPUs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper was very well written and organized. I found the method easy to understand. The insights that underpin the method (e.g., invariance to repeated application of orthogonal matrices) are clever and I think the PCA-based pruning of rows and columns in weight matrices is nicely grounded relative to other neuron pruning techniques."
                },
                "weaknesses": {
                    "value": "I think there are two main weaknesses in this paper. First, the authors don\u2019t acknowledge prior work on neuron pruning. Admittedly most of the papers that I\u2019m aware of on this topic focus on convolutional neuron networks. But, some of the methods are likely to provide a reasonable baseline for the proposed technique. I\u2019ve cited some potentially relevant papers below [1, 2, 3, 4, 5].\n\nSecond, the results in Table 1 suggest to me that 2:4 sparsity is preferable to the proposed technique? If I understand correctly, 2:4 will remove 50% of the weights in the model and the results in Table 1 show that it suffers less quality degradation than removing 30% of the parameters with SliceGPT. Based on this, I expect 2:4 sparsity would show larger inference runtime savings for a given quality than the results in Table 2.\n\n[1] https://arxiv.org/abs/1708.06519\n\n[2] https://arxiv.org/abs/1707.06342\n\n[3] https://arxiv.org/abs/1707.01213\n\n[4] https://arxiv.org/abs/1707.06168\n\n[5] https://arxiv.org/abs/1810.05270"
                },
                "questions": {
                    "value": "I have no additional question."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5654/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762702196,
            "cdate": 1698762702196,
            "tmdate": 1699636588389,
            "mdate": 1699636588389,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l6tpgaXTpr",
                "forum": "vXxardq6db",
                "replyto": "lOwJWQiDrC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5654/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5654/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Hi Reviewer 96nz, thanks so much for your comments. Here are our replies to your queries. \n\n> _(1) don\u2019t acknowledge prior work on neuron pruning_\n\nYou\u2019re right, this is our bad, and we\u2019ve added a new section to the text including those references and more. One thing to note is that LLMs are _so_ much bigger than convnets, that some previous method don\u2019t apply. For example, LeCun\u2019s original OBS method requires O(N^5) operations to prune an NxN weight matrix (an N^3 matrix inversion for each element you want to prune). Completely infeasible on LLMs, where N might be 12k. Nonetheless, we should have cited more work in the pruning literature, even though we didn\u2019t build on it directly. \n\n> _(2) Second, the results in Table 1 suggest to me that 2:4 sparsity is preferable to the proposed technique?_\n\nSee our main rebuttal on comparison with SparseGPT."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699882097329,
                "cdate": 1699882097329,
                "tmdate": 1700260253189,
                "mdate": 1700260253189,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JsZz0rkqdI",
                "forum": "vXxardq6db",
                "replyto": "lOwJWQiDrC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5654/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5654/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Update"
                    },
                    "comment": {
                        "value": "Hi Reviewer 96nz,\n\nYou were concerned about the performance of SliceGPT compared with SparseGPT in terms of inference runtime and accuracy. We think our main rebuttal provides a clearer comparison of the two methods: we show in experiment (1) that SliceGPT (25%) is the same speed as SparseGPT (50%), before we take improved data movement (and increased possible batch size) into account. Experiment (2) demonstrates the increase in batch size possible with SliceGPT leads to throughput of up to 6.26x that of the dense model, due to memory savings.\n\nShould you have any further questions, please let us know. If not, we would be very grateful if you were to consider increasing your score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260521644,
                "cdate": 1700260521644,
                "tmdate": 1700260521644,
                "mdate": 1700260521644,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NbEcY7ak4n",
            "forum": "vXxardq6db",
            "replyto": "vXxardq6db",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5654/Reviewer_gLsa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5654/Reviewer_gLsa"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces SliceGPT - a new approach for compressing large language models. By deleting rows and columns based on computational invariance, SliceGPT can significantly reduce the computation and memory required for inference while maintaining high accuracy. The evaluation demonstrates that this method is effective for large models such as OPT-66B and Llama-70B."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- A novel method of compression based on computational invariance\n- No special code is required to run the compressed models and achieve speedup and memory savings\n- Works for Llama-70B and OPT-66B\n- It is well-written and easy to follow"
                },
                "weaknesses": {
                    "value": "- The accuracy loss is not \"negligible\". With 25% sparsity, the perplexity of Llama-2-70B on WikiText2 increases from 3.32 to 4.89, which is similar to a dense Llama-2-13B. However, a 25% sparse Llama-2-70B has much more parameters than a dense Llama-2-13B.\n- The speedup is not impressive.\n- Compared to the quantization-based method, there is no advantage."
                },
                "questions": {
                    "value": "1. In Table 2, it is not fair to multiply the number of GPUs by the total latency and get \"GPUms\". Huggingface Transformers implements naive model parallelism (or device placement, or pipeline parallelism without pipelining) method to parallelize the models, which means that only one GPU is active at a time. A correct implementation of tensor parallelism or pipeline parallelism will give different results. Considering this, the latency speedup is less impressive.\n2. Give the same parameter count budget or inference latency budget, how does this method compare to quantization-based method?\n3. The \"computational invariance\" trick is similar to a trick in SmoothQuant[1] (equation 3). Both of them multiplicate some matrices between the X and W, so it is good to do some comparison here.\n\n[1] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5654/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794702731,
            "cdate": 1698794702731,
            "tmdate": 1699636588254,
            "mdate": 1699636588254,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lsb6Panjiy",
                "forum": "vXxardq6db",
                "replyto": "NbEcY7ak4n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5654/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5654/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Hi Reviewer gLsa, thanks so much for your comments. Here are our replies to your queries. \n\n> _The accuracy loss is not neglibible_\n\nWe agree that this isn\u2019t the right word \u2013 we\u2019ve changed the text. We like to point out though that the accuracy loss is less than for SparseGPT (and we have an apples-to-apples latency comparison coming in the next day or so). \n\n> _The speedup is not impressive._\n \nWe respectfully disagree! For sparsity/pruning methods, our speedup is pretty good. For example, see the recent LLM-pruner paper. (2305.11627.pdf (arxiv.org) At 20% pruning of LLama 7b, their wikitext ppl is 17.39 at best (up from a baseline of12.62, see their Table 1), and the speedup is less than 15% (see their Table 3). We\u2019re also adding experiments on batch-size (see main rebuttal), where we expect to see much larger throughput increases. \n\n> _Compared to the quantization-based method, there is no advantage (and Q2)_\n\nWe agree that quantization is absolutely key for deploying LLMs, and contributes a huge speedup. But pruning and quantization can work together to make LLM deployment cheaper and faster.  Most pipelines use pruning and quantization, along with finetuning/continued pre-training. See for example [Olive](https://microsoft.github.io/Olive/).\n\nThere is nothing to prevent a user from applying quantization on top of SliceGPT, and we anticipate that this will give the best speedup. We\u2019d like to emphasize a key point of SliceGPT: that the activations are smaller, leading to less data movement on/between devices, which is something that quantization cannot do. The experiment in our main rebuttal will show this clearly. \n\n> _it is not fair to multiply the number of GPUs by the total latency_\n\nYou\u2019re right. If we were to deploy using a more sophisticated deployment that allowed continuous batching, we\u2019d see a speedup of by a factor of up-to the number of GPUs used. Since part of our speedup comes from reduced number of GPUs, the speedup will be less impressive. However, the total energy consumed will still be in proportion to the numbers in our text. We\u2019ve updated the text to reflect this nuance, here\u2019s what we added:\n\n_Our HuggingFace-based testing does not enjoy continuous batching, where the next batch can be loaded onto the first GPU whilst the second GPU processes the current batch. This means that in terms of inference time, the dense-model could be improved more than our sliced model in terms of GPUms. Nonetheless, our measurements \\emph{do} reflect the energy-usage per token in such a deployment._\n\n> _The \"computational invariance\" trick is similar to a trick in SmoothQuant_\n\nYou\u2019re right, SmoothQuant also modifies the model, before quantizing. We\u2019ve added a reference to this effect, thanks!\nAgain, we're not competing with SmoothQuant, or any other quantization method. We fully intend for SliceGPT to be used in pipelines where quantization is also applied."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699882019748,
                "cdate": 1699882019748,
                "tmdate": 1700259732693,
                "mdate": 1700259732693,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mZ6m8xq77S",
                "forum": "vXxardq6db",
                "replyto": "NbEcY7ak4n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5654/Reviewer_gLsa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5654/Reviewer_gLsa"
                ],
                "content": {
                    "title": {
                        "value": "Keeping my initial score"
                    },
                    "comment": {
                        "value": "I've read the rebuttal and appreciate the authors' improvements to their experiments. However, they still note key issues like accuracy loss and limited benefits compared to quantization. So, I'll stick with my initial score."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632597763,
                "cdate": 1700632597763,
                "tmdate": 1700632611681,
                "mdate": 1700632611681,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wzanu4qE6s",
            "forum": "vXxardq6db",
            "replyto": "vXxardq6db",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5654/Reviewer_7EAi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5654/Reviewer_7EAi"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to idea of using computation invariance for row-/column-wise sparsification. The authors leverages the idea of pre- and post-multiplying each block in a transformer model by orthogonal matrices that warrants computational invariance of each block. On the surface, adding new operations increases the raw FLOPs. However, following this technique, the authors show that they can sparsify most of operations in a transformer models, including attention and FFN layers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "$\\mathtt{+}$ The idea of computational invariance and re-purposing additional computation for higher opportunity for sparsification is interesting and warrants further investigation.\n\n$\\mathtt{+}$ The results are promising and show the benefits across a range of SOTA models. The comparison with SparseGPT technique is also valuable."
                },
                "weaknesses": {
                    "value": "$\\mathtt{-}$ The paper lacks sufficient insights of how the rows and columns are sparsified. It was not clear whether some operations are friendlier to row vs. column sparsification or this is a byproduct of the computational invariance approach.\n\n$\\mathtt{-}$ The paper compares accuracy with 2:4 structured sparsity but does not provide head-to-head comparison with SparseGPT (2:4) in terms of latency. \n\n$\\mathtt{-}$ One of the premises of the paper is memory saving, but going through the results it is not clear how the memory savings are in comparison to 2:4 sparsity. Showing a trade-off possibly can clarify this point."
                },
                "questions": {
                    "value": "I think if the authors could clarify the following questions/comments and include few additional results, the quality of the paper could significantly increase:\n\n(Q1) Show latency comparison across different baselines, (a) Dense, (b) SliceGPT, (c) SparseGPT. \n\n(Q2) I may have missed this in the paper, but can you please clarify how you decide on row/column sparsity and how you select them? If the sparsed rows/columns are spread across the matrix, how do you manage to do the multiplication while getting latency benefits? or the overall benefits are derived from memory savings?\n\n(Q3) Do you have any insights as which operation/layer is more sensitive to sparsification? Have you thought of not uniformly sparsifying all the layers? Can looking into the range of values in the weight matrices provide insights on how to apply the sparsificiation (both degree and pattern)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5654/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698932150222,
            "cdate": 1698932150222,
            "tmdate": 1699636588150,
            "mdate": 1699636588150,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1kr7sw0Zi9",
                "forum": "vXxardq6db",
                "replyto": "wzanu4qE6s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5654/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5654/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Hi Reviewer 7EAi, thanks so much for your comments. Here are some replies to your queries. \n\n> _insights of how the rows and columns are sparsified (Q2)_\n\nWe should have made this clear \u2013 we delete the rows / columns that correspond to the smallest eigenvalues. By convention, these are the last rows/columns. Since each deletion (D in equation 9) happens between a pair of blocks, the same number of rows must be deleted from an input matrix as columns from the preceding output matrix. Figure 4 shows the deletions as hatched areas. \n\nSince we use the eigenvalues of the covariance matrix of X as the rotation, after applying Q the network remains invariant (as per thm1) but the variance of each column of X will be equal to the eigenvalues. You can see a plot of some example eigenvalue decays in the supplementary material. If the variance of a column is small, we can assume it is zero always: this is the same as deleting the corresponding column of the preceding weight matrix and the corresponding row of the subsequent weight matrix. \n\n>  _does not provide head-to-head comparison with SparseGPT (Q1)_\n\n> _One of the premises of the paper is memory saving\u2026_\n\n> _which operation/layer is more sensitive to sparsification? (Q3)_\n\nWe are addressing each of these with a new small experiment, see main rebuttal."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699881790390,
                "cdate": 1699881790390,
                "tmdate": 1699881790390,
                "mdate": 1699881790390,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0sc9hmTvcx",
                "forum": "vXxardq6db",
                "replyto": "wzanu4qE6s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5654/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5654/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Update"
                    },
                    "comment": {
                        "value": "Hi Reviewer 7EAi,\n\nYou were concerned that we hadn\u2019t provided a head-to-head comparison with SparseGPT, and that the memory savings over SparseGPT weren't clear. We think our main rebuttal fixes this: we show in experiment (1) that 25% slicing is the same speed as 50% sparsity, before we take improved data movement (and increased possible batch size) into account. Experiment (2) demonstrates the increase in batch size possible with SliceGPT leads to throughput of up to 6.26x that of the dense model, due to memory savings.\n\nWe also demonstrate the feasibility of applying varying slicing by layer - with performance improvements in OPT but the opposite in Llama models, in experiment (3).\n\nShould you have any further questions, please let us know. If not, we would be very grateful if you were to consider increasing your score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5654/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259633999,
                "cdate": 1700259633999,
                "tmdate": 1700260400252,
                "mdate": 1700260400252,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]