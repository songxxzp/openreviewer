[
    {
        "title": "UNR-Explainer: Counterfactual Explanations for Unsupervised Node Representation Learning Models"
    },
    {
        "review": {
            "id": "QHHAVfQgXz",
            "forum": "0j9ZDzMPqr",
            "replyto": "0j9ZDzMPqr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8687/Reviewer_WfpS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8687/Reviewer_WfpS"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a new method to obtain counterfactual (CF) explanations from unsupervised node learning. Their method uses a learned unsupervised node learning method to get embeddings. They then define their method via the importance function which they try to maximize while minimizing the edge alterations to the graph (minimizing the counterfactual explanation). They provide an upper bound on the Importance function. \n\nThey alter a Monte Carlo Tree Search (MCTS) method to get the CF. The MCTS uses the Importance function as a reward which looks for subgraphs that are important but sparse. They show that their MCTS does not degrade the expressiveness of subgraph explanations like the vanilla MCTS would by altering the reward function. They do this by choosing the action where the upper confidence boundary (UCB a term used in their MCTS which dictates which edge to take in traversal) is larger. They also show analysis showing that the UCB for an edge that leads to a new node is greater thus prioritizing exploring new paths. They claim these alterations to the vanilla MCTS leads to more expressive explanations which are partially supported by theoretical claims. \n\nThey also have quite a large and expansive set of experiments. They do experiments on 3 synthetic datasets (BA-Shapes, Tree-Cycles, and Tree-Grid) and 3 real datasets (Cora, CiteSeer, and PubMed), parameter sensitivity study, ablation study and a case study. They conduct their experiments on 8 methods (including theirs) on several metrics: Precision/Recall, Validity, size of the model and Importance. They show promising behaviour of their method in comparison to other methods in many settings with various evaluation metrics. They also show a case study on the NIPS dataset a social network of citations. They show by perturbing explanation graphs on a particular author they can obtain a graph that belongs to a different author that belongs to a different subfield of ML. They also conduct an ablation study showing variants of the MCTS algorithm which is fundamental to their methodology. They show their variant of MCTS can find expressive explanations (high importance score) while being efficient. Finally they also show experiments of parameter sensitivity. They show what effects the choice of the restart parameter, perturbation parameter, and the number of neighbors has on their method. They do this by varying the choice of hyperparameter and evaluating the importance score on the Cora Dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is very well written. The design of the paper from problem definition, to methodology, to experimental evaluation follows clearly and is well designed. The authors also motivate their work by addressing the problem in a well defined manner. The Importance measure is novel and inventive way to quantify the counterfactual explanation. The alterations to the MCTS to construct these counterfactual explanations is reasonable and well grounded by theory to supplement their decisions. The paper also employs theory on the Importance measure to show an upper bound. \n\nThe experimental list is fairly exhaustive and shows superior performance to several other methods in multiple datasets and cases. The case study is a nice touch to display their method\u2019s ability to obtain meaningful counterfactual explanations. The ablation study shows that their variant of MCTS can find expressive subgraphs while being efficient in comparison to other tree search methods. Finally, having a study to show their methods sensitivity/robustness to choices of hyperparameters is important for anyone seeking to employ this method."
                },
                "weaknesses": {
                    "value": "There could be more discussion on experiments where the UNR-Explainer underperforms compared to other methods. \n\nAlso further explanation on certain hyperparameter choices could be made more clear for the readers. Such as the choice of k in each experimental setting. A discussion on when to use a particular larger/smaller value of k would be interesting. The authors do have experiments showing the sensitivity of the number of neighbors, they also have a limited discussion on this phenomenon. However this hyperparameter is central to their method (their importance measure is heavily influenced by it) a discussion to explain what settings would require very large k vs very small k would be beneficial to solidify their work although it is not necessary."
                },
                "questions": {
                    "value": "Although the authors provided a study of hyperparameter sensitivity, why did they select k=5. Clearly, as seen in the experiments the choice of k does seem to impact the importance score. More discussion of the effect of the choice of k would be beneficial for practitioners. \n\nAlso their method seems to underperform in the synthetic experiments particularly with the precision measure. Significantly smaller methods do better than UNR-Explainer in these settings which is seemingly consistent throughout the synthetic experiments. Any discussion as to why this is would be beneficial to readers and the authors."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8687/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8687/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8687/Reviewer_WfpS"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8687/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698003586963,
            "cdate": 1698003586963,
            "tmdate": 1699637089122,
            "mdate": 1699637089122,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ltp8L1EDNL",
                "forum": "0j9ZDzMPqr",
                "replyto": "QHHAVfQgXz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8687/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WfpS"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your valuable feedback for reviewing this paper. For the potential concerns you bring up, we would like to address them as follows.\n\n### **W1 \\& Q2: Clarity of disscusion**\n\nThank you for your constructive comment. In the BA-Shape dataset, there are 80 house-structured motifs, each comprising 5 nodes, which are considered the correct answers. Only a few edges in these motifs are connected to the base BA graph. In our experiments, we focused on evaluating the performance of explanation generation for nodes within these house motifs. In such cases, when employing 1hop-2N and 1hop-3N for searching explanations, the precision tends to be high. This is due to the increased probability that the explanation will include another node from the house-structured motif, thereby aligning with the ground truth. Similarly, for other synthetic datasets constructed in the same manner, 1hop-2N and 1hop-3N exhibit high precision but low recall.\n\n### **W2 \\& Q1: More discussion of choosing $k$**\n\nThank you for your valuable question. In this paper, $k=5$ was chosen as a hyper-parameter because it performed well on average across evaluation measures such as importance, fidelity, and validity. However, we acknowledge that practitioners might select different values for $k$ depending on the desired level of interpretation. Specifically, $k$ is closely associated with the detail of the generated explanation and the number of hops, so its selection may vary based on the domain expert's judgment. For instance, in a social network analysis, a $k$  value close to the average node degree may be suitable for explanations focusing on immediate connections, such as close friends. Conversely, for understanding important local clusters, a higher $k$ might be more appropriate. This decision can be informed by domain knowledge."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576444904,
                "cdate": 1700576444904,
                "tmdate": 1700576444904,
                "mdate": 1700576444904,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Qi4fBjHFhU",
            "forum": "0j9ZDzMPqr",
            "replyto": "0j9ZDzMPqr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8687/Reviewer_TnSw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8687/Reviewer_TnSw"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method called UNR-Explainer for generating counterfactual explanations in unsupervised node representation learning models. The goal of these explanations is to provide information for understanding unsupervised downstream tasks. UNR-Explainer performs Monte Carlo Tree Search to find the explanation subgraph. The subgraph importance is measured by the change of the top-k nearest neighboring nodes after perturbation. UNR-Explainer is evaluated on six datasets including both synthetic ones and real-world ones, and UNR-Explainer is shown to outperform existing explanation methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. UNR-Explainer shows the good quantitative performacne, and the case study on NIPS shows UNR-Explainer can select qualititatively meaningful subgraphs.\n\n2. The importance metric proposed in Equation 1 is novelt to me.\n\n3. Time complexity analysis and discussion of limitation are both included in the appendix.\n\n4. Code is provided for reproducibility."
                },
                "weaknesses": {
                    "value": "1. Lacking discussions. Some baseline methods considered in the experiment section are very simple but achieve strong performance without discussion or analysis. See question 1 as well.\n\n2. Efficiency. MCTS-based explanation can be slow than other explanation methods, e.g., gradient-based methods, especially on large graphs. This is verified by the time complexity as well.\n\n3. Presentation can be further improved. Some figures have text that is too small to read. For example, embedding labels in figure 2."
                },
                "questions": {
                    "value": "1. In Table 1 for the synthetic datasets, the naive random selection baselines 1hop-2N and 1hop-3N achieve the best results in terms of precision. Why? Any discussions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8687/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8687/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8687/Reviewer_TnSw"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8687/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698657905979,
            "cdate": 1698657905979,
            "tmdate": 1699637089001,
            "mdate": 1699637089001,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DSN5eJeQAU",
                "forum": "0j9ZDzMPqr",
                "replyto": "Qi4fBjHFhU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8687/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TnSw"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your valuable feedback for reviewing this paper. For the potential concerns you bring up, we would like to address them as follows.\n\n### **W1 \\& Q1: Improving discussions**\n\nThank you for your constructive comment. In the BA-Shape dataset, there are 80 house-structured motifs, each comprising 5 nodes, which are considered the correct answers. Only a few edges in these motifs are connected to the base BA graph. In our experiments, we focused on evaluating the performance of explanation generation for nodes within these house motifs. In such cases, when employing 1hop-2N and 1hop-3N for searching explanations, the precision tends to be high. This is due to the increased probability that the explanation will include another node from the house-structured motif, thereby aligning with the ground truth. Similarly, for other synthetic datasets constructed in the same manner, 1hop-2N and 1hop-3N exhibit high precision but low recall.\n\n\n### **W2: Addressing efficiency**\n\nThank you for raising this important point. UNR-Explainer has a time complexity of $O(t \\cdot n \\cdot |V|)$, where $t$ is the number of iterations, $n$ is the number of nodes in the search tree, and $|V|$ is the number of nodes in the input graph. In the worst case, if $n$ approaches $|V|$, the complexity could become approximately $O({|V|}^2)$. To circumvent this situation, there are two strategies: establishing stopping criteria and setting an upper bound for $n$. Firstly, $t$ is set to 1,000 as a stopping criterion, or the traversal is stopped when a case meeting $Importance = 1.0$ is found. Secondly, the number of nodes in the traversal graph is limited to a constant value; in this paper, we set it to 20. Moreover, in every iteration, we sample at most 3 nodes for the expansion step, which helps avoid the exponential expansion of $n$. For these reasons, the time complexity of UNR-Explainer is approximate $O(|V|)$, because both $t$ and $n$ could be regarded as constants. While most gradient-based methods (e.g., PG-Explainer [1]) have the time complexity of $O(|E|)$, where $|E|$ is the number of edges in the input graph, the complexity of UNR-Explainer could be more efficient than these methods, especially when the input graph is dense.\n\nIn addition to addressing the time complexity issue, the MCTS-based framework finds the explanation subgraphs, which are mostly connected. In contrast, gradient-based methods such as PG-Explainer [1] incorporate a connected loss term in their objective term to achieve connectedness. Despite this, they often fail to consistently generate connected subgraphs, as the additional regularization term does not guarantee generating connected subgraphs.\n\n**Reference**\n\n[1] Luo, Dongsheng, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang. \"Parameterized explainer for graph neural network.\" Advances in neural information processing systems 33 (2020): 19620-19631.\n\n### **W3: Enhancing presentations**\nThanks for your valuable feedback. We revised all figures to improve the presentation of our paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576374921,
                "cdate": 1700576374921,
                "tmdate": 1700576374921,
                "mdate": 1700576374921,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VeDgHeveH3",
            "forum": "0j9ZDzMPqr",
            "replyto": "0j9ZDzMPqr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8687/Reviewer_8JCL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8687/Reviewer_8JCL"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel method for explaining graph neural networks. The authors focus on counterfactual explanations and propose the UNR-Explainer, which aims to identify subgraphs that, when perturbed, lead to significant changes in node embeddings. The paper evaluates various explanation methods in unsupervised settings using synthetic and real-world datasets. The proposed method leverages the Monte Carlo Tree Search (MCTS) for efficient traversal in large search spaces. The paper also provides a theoretical analysis of the upper bound of Importance and discusses the algorithm for calculating Importance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1)\tThe paper tackles CF reasoning in unsupervised settings, a relatively unexplored area potential implications for explainability in graph neural networks and unsupervised learning.\n2)\tThe paper leverages the Monte Carlo Tree Search (MCTS), a technique from reinforcement learning, to efficiently traverse the search space of potential subgraphs. MCTS is known for its effectiveness in large search spaces, making it a suitable choice for this problem.\n3)\tThe paper clearly defines the counterfactual property for unsupervised representation learning models, providing a solid foundation for their method.\n4)\tThe paper includes a theoretical analysis of the upper bound of Importance for GraphSAGE, adding a rigorous foundation to their empirical findings."
                },
                "weaknesses": {
                    "value": "1)\tWhile the paper does evaluate on both synthetic and real-world datasets, it might benefit from testing on more diverse datasets, especially those from different domains or with different characteristics. Information on how the method scales with larger datasets or more complex graphs, and its computational efficiency, would be valuable.\n2)\tI believe the paper would greatly benefit from additional visual illustrations or diagrams to depict the proposed method. Visual aids can provide a clearer understanding and offer readers an intuitive grasp of the methodology. Given the complexity and novelty of the approach, diagrams or flowcharts could enhance comprehension and make the content more accessible to a broader audience."
                },
                "questions": {
                    "value": "1)\tHow does the method scale with larger and more complex graphs? Are there any computational or memory constraints that might limit its applicability to very large datasets?\n2)\tHow sensitive is the method to the degree of perturbation applied to the subgraph? Would minor changes in perturbation lead to significantly different results in the algorithm of importance?\n3)\tGiven the contrastive approach employed by DGI and the inductive learning capability of GraphSAGE, how might these characteristics influence the types of counterfactual explanations generated? Furthermore, how would the proposed counterfactual explanation method adapt and perform when integrated with generative models such as GraphGAE or S2GAE?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8687/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8687/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8687/Reviewer_8JCL"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8687/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756015955,
            "cdate": 1698756015955,
            "tmdate": 1699637088885,
            "mdate": 1699637088885,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8Nwh7jUjxW",
                "forum": "0j9ZDzMPqr",
                "replyto": "VeDgHeveH3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8687/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8JCL [1/2]"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your valuable feedback for reviewing this paper. For the potential concerns you bring up, we would like to address them as follows.   \n\n\n\n### **W1 \\& Q1: Addressing scalability**\n\nThank you for raising this important point. Scalability is an important factor to be considered particularly in real applications on very large graphs. As a matter of time complexity, UNR-Explainer has a time complexity of $O(t \\cdot n \\cdot |V|)$, where $t$ is the number of iterations, $n$ is the number of nodes in the search tree, and $|V|$ is the number of nodes in the input graph. In the worst case, if $n$ approaches $|V|$, the complexity could become approximately $O({|V|}^2)$. To circumvent this situation, there are two strategies: establishing stopping criteria and setting an upper bound for $n$. Firstly, $t$ is set to 1,000 as a stopping criterion, or the traversal is stopped when a case meeting $Importance = 1.0$ is found. Secondly, the number of nodes in the traversal graph is limited to a constant value; in this paper, we set it to 20. Moreover, in every iteration, we sample at most 3 nodes for the expansion step, which helps avoid the exponential expansion of $n$. For these reasons, the time complexity of UNR-Explainer is approximately $O(|V|)$, because both $t$ and $n$ could be regarded as constants. While most gradient-based methods (e.g., PGExplainer [1]) have the time complexity of $O(|E|)$, where $|E|$ is the number of edges in the input graph, the complexity of UNR-Explainer could be more efficient than these methods, especially when the input graph is dense.\n\n**Reference**\n\n[1] Luo, Dongsheng, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang. \"Parameterized explainer for graph neural network.\" Advances in neural information processing systems 33 (2020): 19620-19631.\n\n### **W2: Providing overview of UNR-Explainer**\n\nThank you for your suggestion. We attached the figure describing an overview of UNR-Explainer in a revised version of our paper and attached [link](https://anonymous.4open.science/r/unr0929/overview.jpg).\n\n\n### **Q2: Addressing sensitivity of perturbation effect**\nThanks for your valuable question. As demonstrated in our additional experiments, the sensitivity of the $Importance$ measure in UNR-Explainer varies with different perturbation rates (details available at this [link](https://anonymous.4open.science/r/unr0929/perturbation_param.jpg). Specifically, when the perturbation parameter lies between 0.0 and 0.3, there is a gradual decline in $Importance$ observed on the Cora and CiteSeer datasets. However, for perturbation parameters exceeding 0.4 \u2014 implying that 40% or more of messages that ought to be weakened are retained \u2014 the effectiveness of the perturbation is reduced, leading to a lower $Importance$."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576125287,
                "cdate": 1700576125287,
                "tmdate": 1700576125287,
                "mdate": 1700576125287,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y2FFAXBXMj",
                "forum": "0j9ZDzMPqr",
                "replyto": "VeDgHeveH3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8687/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8JCL [2/2]"
                    },
                    "comment": {
                        "value": "### **Q3-1: Addressing analysis of explanations by models as GraphSAGE vs DGI**\nThanks for bringing up this question. In general, UNR-Explainer yields explanations of varying sizes when used with different node representation models such as GraphSAGE and DGI. Specifically, explanations under the GraphSAGE model average 4.5 nodes and 3.8 edges, while those under DGI average 3.9 nodes and 3.1 edges. Typically, GraphSAGE-generated explanations include more distant nodes and a greater number of connections in the input graph compared to DGI. We have provided visualizations of these explanations via this [link](https://anonymous.4open.science/r/unr0929/cora-gs-dgi.jpg). In Case 1, the explanations under GraphSAGE and DGI are common, while in Case 2, although the overall structure of the explanations is similar, the GraphSAGE explanation includes an additional node at a 2-hop distance from the target node. This trend is even more pronounced in Case 3. We hypothesize that this difference may be attributed to the methods used for defining positive and negative samples during training. GraphSAGE treats nodes near a target node on a fixed-length random walk as positive samples, as implemented in [pytorch-geometric](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/loader/link_neighbor_loader.html#LinkNeighborLoader) with settings [10, 10]. Conversely, DGI employs a corruption function that randomly permutes node features while keeping the edge index constant to generate negative samples. This broader range of positive samples in GraphSAGE could influence UNR-Explainer to generate explanations that encompass more distant nodes and edges.\n\n\n### **Q3-2: Adapting UNR-Explainer to explain generative models**\n\nThank you for your insightful question. Graph Auto Encoders (GAE), such as VGAE [1] and S2GAE [2], are prominent in generation-based unsupervised and self-supervised learning on graphs. These models typically consist of two components: an encoder, which is a graph neural network producing latent representations, and a decoder, which reconstructs the input graph using these representations. The latent representation, containing implicit information, is crucial for downstream tasks like node and graph classification, where GAE models demonstrate superior performance. Hence, there is a significant demand for explaining these models in real-world applications.\n\nUNR-Explainer can be adapted to explain the encoder of GAE-based models like VGAE and S2GAE. This compatibility arises mainly because UNR-Explainer provides node-level explanations, aligning well with the output of the node-level embedding by the encoders in these models. Moreover, UNR-Explainer's ability to provide explanations without relying on class labels makes it potentially suitable for self-supervised learning models.\n\nHowever, adapting UNR-Explainer to GAE models involves defining explanations for generation-based GAEs and designing appropriate experimental settings, especially evaluation metrics. Additionally, the specific processes of each model must be considered. For instance, in VGAE [1], we need to control the randomness of the reparameterization trick to assess the perturbation effect accurately. In the case of S2GAE [2], which uses edge-masking and direction-aware graph masking, these perturbation methods may conflict with UNR-Explainer's perturbation process, necessitating additional adjustments. Moreover, since UNR-Explainer is designed to explain models with only an Encoder part, adapting it to generative models with a Decoder part would require further methodological considerations.\n\nTo our knowledge, explainability for generative models on graphs is one of the less explored areas in research. We are grateful for your question, as it highlights an exciting direction for our future research endeavors.\n\n**Reference**\n\n[1] Kipf, Thomas N., and Max Welling. \"Variational graph auto-encoders.\" arXiv preprint arXiv:1611.07308 (2016).\n\n[2] Tan, Qiaoyu, Ninghao Liu, Xiao Huang, Soo-Hyun Choi, Li Li, Rui Chen, and Xia Hu. \"S2GAE: Self-Supervised Graph Autoencoders are Generalizable Learners with Graph Masking.\" In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, pp. 787-795. 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576199616,
                "cdate": 1700576199616,
                "tmdate": 1700576199616,
                "mdate": 1700576199616,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3R1NugL6HT",
                "forum": "0j9ZDzMPqr",
                "replyto": "Y2FFAXBXMj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8687/Reviewer_8JCL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8687/Reviewer_8JCL"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive feedback. Upon reflection, I would keep my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647128709,
                "cdate": 1700647128709,
                "tmdate": 1700647128709,
                "mdate": 1700647128709,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8qXxxq6QIR",
            "forum": "0j9ZDzMPqr",
            "replyto": "0j9ZDzMPqr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8687/Reviewer_eAe7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8687/Reviewer_eAe7"
            ],
            "content": {
                "summary": {
                    "value": "This work explores explanation generation for unsupervised node representation learning.  The authors propose a Monte Carlo Tree Search (MCTS)-based method to generate counterfactual (CF) explanations. Specifically, this method aims to identify the most important subgraphs that cause a significant change in the k-nearest neighbors of a node. The proposed method is incorporated into unsupervised GraphSAGE and DGI, and the performance on six datasets confirms the efficacy of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It is an interesting research topic to improve the interpretability of unsupervised learning models on graphs. \n2. This can help to find the explanations of  GNN models with unseen downstream tasks.\nThe proposed method is tested on several datasets and shows satisfactory results.\n3. The paper is well-structured and organized."
                },
                "weaknesses": {
                    "value": "1. The work is somehow incremental work. SubgraphX proposed a Monte Carlo tree search algorithm to efficiently explore different subgraphs. Compared with SubgraphX, the authors seem to just add a new policy, \u201crestart\u201d, in the Selection step to mitigate the search bias. The design makes sense but results in limited novelty.\n2. The indicators of counterfactual explanations are not rigorous. The perturbations of the input graph not only change the node embedding of interest ($emb_{v} \\neq emb_{v}^{'}$) but also change other node embeddings. It does not match the Figure 1 (b) and (c) illustrated.\n3. The motivation should be further improved. The authors do not state the challenges of generating counterfactual explanations in unsupervised learning compared with supervised methods, such as CF-GNNExplainer, RCExplainer, and CF2.\n4. The authors do not provide real-world applications or pilot studies to support their claim that \"the perturbation strategy of adding edges or nodes has a significant risk in real-world scenarios\"\n5. Minor error: Page 9 The first line is not left-justified; Measures in Table 1 are not arrowed."
                },
                "questions": {
                    "value": "1. Why do the authors choose the MCTS-based framework rather than other gradient-based or causal-based interpretable methods? Can you show the relevant analyses?\n2. Can the authors state what new challenges your approach addresses compared to existing counterfactual explanation methods on supervised learning?\n3. Can the author give some applications of real-world scenarios or do some pilot studies to show the benefit of the perturbation strategy of only removing edges?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8687/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8687/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8687/Reviewer_eAe7"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8687/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781547775,
            "cdate": 1698781547775,
            "tmdate": 1700722158431,
            "mdate": 1700722158431,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nVNmcEpFNT",
                "forum": "0j9ZDzMPqr",
                "replyto": "8qXxxq6QIR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8687/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eAe7 [1/3]"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your valuable feedback for reviewing this paper. For the potential concerns you bring up, we would like to address them as follows.\n\n### **W1: Addressing novelty**\n\nThank you for raising the important question. We acknowledge that our method is not the first to implement Monte Carlo Tree Search (MCTS) in the graph XAI (Explainable Artificial Intelligence) domain. Prior works, such as RationaleRL [1] and SubgraphX [2], have utilized MCTS in their problem settings, each with a reward function tailored to their specific objectives. However, applying existing MCTS frameworks directly to unsupervised node learning models is challenging. The difficulty lies in defining suitable rewards and managing the increased uncertainty in exploring the graph space.\n\nTo mitigate the challenges, we propose a new procedure that is different from the existing MCTS framework with respect to five key aspects. The distinctions between our model and SubgraphX are summarized in the table below.\nFirst, our $Importance$ function shown in Equation (1) of our main text quantifies the change of top-k neighbors in the embedding space after perturbation, bringing up the benefits of recognizing the significant influence of important subgraph $G_s$ in related downstream tasks. Thus, our contribution not only relies on developing the proper design of MCTS for counterfactual explanations but also sheds light on explaining unsupervised node representation learning. For clarification, we describe the difference between SubgraphX and UNR-Explainer regarding to the design of MCTS as below:\n\n| Design criteria        | SubgraphX          |   UNR-Explainer                |\n|------------------------|--------------------|--------------------------------|\n| Target model           | supervised models  | unsupervised models            |\n| Reward function        | Shapley value      | Importance function in Eq. (1) |\n| Action                 | to prune           | to add                         |\n| Selection              | argmax             | random walk with restart       |\n| Expansion              | without sampling   | with sampling                  |\n| Action value           | mean               | max                            |\n\nConsequently, our design of Monte Carlo Tree Search (MCTS) in UNR-Explainer reduces the computational time by 97.37\\% as [(4.64 - 176.3) \u00f7 176.3] \u00d7100 and performance increased by 1.9\\% as [(0.96 - 0.42) \u00f7 0.942] \u00d7100 in the Table 4 of our paper, compared to SubgraphX-1. The significant improvement in the computational cost indicates the emphasis on MCTS design to tailor its problem settings. To avoid costly and unnecessary exploration, we propose a suitable MCTS framework for counterfactual explanations in unsupervised settings. Additionally, the new selection strategy and reward functions are theoretically analyzed with respect to the bounds of value and expressiveness, respectively.\n\n **Reference** \n \n[1] Jin, Wengong, Regina Barzilay, and Tommi Jaakkola. \"Multi-objective molecule generation using interpretable substructures.\" In International conference on machine learning, pp. 4849-4859. PMLR, 2020.\n\n[2] Yuan, Hao, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. \"On explainability of graph neural networks via subgraph explorations.\" In International conference on machine learning, pp. 12241-12252. PMLR, 2021.\n\n### **W2: Clarity of Figure 1**\n\nThank you for your thoughtful comments. For brevity, we have simplified the counterfactual explanations in Figure-1 (b) and (c). As indicated, when edges near the node of interest are perturbed, the neighboring nodes' positions are affected. In contrast, nodes that are distant from the node of interest, but share structural similarities and similar feature distributions, will maintain their original embeddings unchanged. We have updated the figure to better illustrate this distinction in a revised version of our paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575062833,
                "cdate": 1700575062833,
                "tmdate": 1700575297220,
                "mdate": 1700575297220,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BvhKINWkpZ",
                "forum": "0j9ZDzMPqr",
                "replyto": "8qXxxq6QIR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8687/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eAe7 [2/3]"
                    },
                    "comment": {
                        "value": "### **W3 & Q2: Addressing motivation**\n\nThank you for pointing this out. UNR-Explainer is designed to generate counterfactual explanations in unsupervised learning contexts without relying on labels, contrasting with XAI methods in supervised learning. We highlight the limitations of existing methods when applied to unsupervised learning:\n\n- CF-GNNExplainer [1] as a pioneer of counterfactual explanation for the graph domain enhances explainability. Its loss function is formulated as \n $L = L_{pred}(v, \\bar{v}| f, g) + \u03b2L_{dist}(v,  \\bar{v} | d)$, where $v$ is the original node and $\\bar{v}$ is generated by the CF model. In this function, $f$ represents a predicted class label, and $g$ is the counterfactual (CF) model generating $\\bar{v}$. The term $L_{pred}$ denotes a counterfactual loss ensuring $f(v) \\neq f(\\bar{v})$, while $L_{dist}$ quantified the distance between the input graph and the found explanation. However, as indicated in Definition 2 of our paper, the condition $f(v) \\neq f(\\bar{v})$ in supervised settings can be analogous to $emb_{v} \\neq emb_{v}^{\\prime}$ in unsupervised learning. Yet, $emb_{v} \\neq emb_{v}^{\\prime}$ alone provides limited information for determining the importance of each edge.  To address this and aid in generating beneficial explanations for related downstream tasks, we introduce a novel $Importance$ measure for counterfactual explanation in unsupervised node representation learning.\n\n- RCExplainer [2] tackles the robustness of counterfactual explanations by leveraging the decision boundaries of a GNN in supervised settings. However, the decision region in [2] heavily relies on class labels, making it unsuitable for unsupervised settings where class labels are absent or when no specific downstream tasks are defined.\n\n- CF2 [3] combines factual and counterfactual explanations to address the limitations of each approach. However, the process for generating explanations in CF2, as defined in Equations (10), (11), and (12) in [3], also depends on class labels for graph/node classification. This reliance makes direct application to unsupervised settings challenging.\n\nIn conclusion, UNR-Explainer provides insights into counterfactual explanations in unsupervised settings without relying on class labels. It offers the advantage of describing potential impacts on related downstream tasks, such as link prediction and clustering.\n\n **Reference**\n \n[1] Lucic, Ana, Maartje A. Ter Hoeve, Gabriele Tolomei, Maarten De Rijke, and Fabrizio Silvestri. \"Cf-gnnexplainer: Counterfactual explanations for graph neural networks.\" In International Conference on Artificial Intelligence and Statistics*, pp. 4499-4511. PMLR, 2022.\n\n[2] Bajaj, Mohit, Lingyang Chu, Zi Yu Xue, Jian Pei, Lanjun Wang, Peter Cho-Ho Lam, and Yong Zhang. \"Robust counterfactual explanations on graph neural networks.\"Advances in Neural Information Processing Systems 34 (2021): 5644-5655.\n\n[3] Tan, Juntao, Shijie Geng, Zuohui Fu, Yingqiang Ge, Shuyuan Xu, Yunqi Li, and Yongfeng Zhang. \"Learning and evaluating graph neural network explanations based on counterfactual and factual reasoning.\" InProceedings of the ACM Web Conference 2022*, pp. 1018-1027. 2022."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575148794,
                "cdate": 1700575148794,
                "tmdate": 1700575148794,
                "mdate": 1700575148794,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pQ5hgmfQPg",
                "forum": "0j9ZDzMPqr",
                "replyto": "8qXxxq6QIR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8687/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eAe7 [3/3]"
                    },
                    "comment": {
                        "value": "### **W4 \\& Q3: Addressing real-world scenario**\n\nThank you for your invaluable question. We address the potential risks inherent in explainability techniques, particularly those involving modification methods like adding new nodes or edges, in real-world scenarios. In the context of social network graphs, an explanation graph generated by allowing the addition of nodes to a subgraph (used for explaining link prediction or clustering) might include nodes that are not actually observable. For example, it may suggest connections with individuals who have never met, rendering the explanation invalid for the end-user. Similarly, in citation graphs, if the explanation for link prediction includes a non-existent citation subgraph, it becomes challenging to interpret. When learning from the molecule datasets for drug discovery, permitting the addition of numerous nodes can lead to significant deviations from [chemical valency rules](https://en.wikipedia.org/wiki/Valence_(chemistry)). Such alterations could render the explanations not only inaccurate but also impractical for practical applications. To circumvent these issues, our approach emphasizes the removal of important connections to provide counterfactual explanations. This method ensures that the explanations remain grounded in the actual structure of the dataset, thereby enhancing their validity and applicability.   \n\n\n\n\n\n\n### **W5: Rectifying minor error**\n\nThank you for your thoughtful comment. We revised this issue in the current version of our paper.     \n    \n\n\n\n\n### **Q1: Rationale for MCTS**\n\nThank you for raising the important question. Our approach offers several advantages over gradient-based or causal-based interpretable methods. Firstly, unlike gradient-based methods such as PG-Explainer [1], which attempt to ensure connectedness through a connected loss term but often fail to guarantee connected subgraphs, our MCTS-based framework reliably identifies exploratory subgraphs that are connected. This is because the additional regularization term in gradient-based methods does not consistently lead to connected subgraphs. Secondly, our objective function is not a differential equation, setting it apart from gradient-based methods. While SubgraphX [2] employs the Shapley value for the importance measurement in its MCTS framework, our $Importance$ measure quantifies the change in the k-nearest neighbor nodes after perturbation, making our MCTS-based approach more suitable for exploring significant subgraphs in unsupervised settings.\nAs for causal-based methods like GEM [3] and OrphicX [4], they rely on structural causal models (SCM), which are predominantly based on the assumption of existing class labels. Developing a causal-based model for explaining embedding vectors in unsupervised settings is an intriguing area for future research. However, in the absence of external SCM models, our method solely relies on our $Importance$ function to provide explanations in unsupervised settings.\n\n**Reference**\n\n[1] Luo, Dongsheng, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang. \"Parameterized explainer for graph neural network.\" Advances in neural information processing systems 33 (2020): 19620-19631.\n\n[2] Yuan, Hao, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. \"On explainability of graph neural networks via subgraph explorations.\" In International conference on machine learning, pp. 12241-12252. PMLR, 2021.\n\n[3] Lin, Wanyu, Hao Lan, and Baochun Li. \"Generative causal explanations for graph neural networks.\" In International Conference on Machine Learning, pp. 6666-6679. PMLR, 2021.\n\n[4] Lin, Wanyu, Hao Lan, Hao Wang, and Baochun Li. \"Orphicx: A causality-inspired latent variable model for interpreting graph neural networks.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13729-13738. 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575214102,
                "cdate": 1700575214102,
                "tmdate": 1700575314208,
                "mdate": 1700575314208,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pYkpzJiYmW",
                "forum": "0j9ZDzMPqr",
                "replyto": "pQ5hgmfQPg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8687/Reviewer_eAe7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8687/Reviewer_eAe7"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive feedback. I would increase my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722109924,
                "cdate": 1700722109924,
                "tmdate": 1700722109924,
                "mdate": 1700722109924,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]