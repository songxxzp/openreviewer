[
    {
        "title": "Adaptive Knowledge Transfer for Generalized Category Discovery"
    },
    {
        "review": {
            "id": "K9C5gWdaA0",
            "forum": "j20nMRUWK9",
            "replyto": "j20nMRUWK9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5777/Reviewer_UEjY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5777/Reviewer_UEjY"
            ],
            "content": {
                "summary": {
                    "value": "This paper targets the task of generalized class discovery (GCD), and argues that the explicit knowledge transfer is a necessity that is largely ignored by existing works. To this end, the authors propose to achieve this goal with three steps: 1) knowledge generation, 2) knowledge alignment and 3) knowledge distillation, which are implemented in a two-stage training procedure. The authors conduct extensive experimental evaluations, which demonstrate that the proposed method largely outperforms existing works."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well organized and clearly written. \n- The experiments on six existing benchmarks and a newly introduced iNat21 are extensive.\n- The experimental results showcase the superiority of the proposed method."
                },
                "weaknesses": {
                    "value": "- I believe the idea of knowledge utilization is good, but the proposed terms seem a bit of \"big\" to me. Technically speaking, knowledge generation, alignment and distillation are respectively training on labeled data, filtering out the important feature dimensions and applying contrastive loss. If there is no obvious significance, I would suggest using more focused terms that better convey the precise purpose.\n\n- As far as I am concerned, the most important component is regarding the \"knowledge distillation\" (as shown in Tab. 4), where an InfoNCE-like loss is used to do the trick. Within this loss, the MixUp-based negative sample generation seems an important ingredient, yet with no proper ablation studies. Given the potential similarity between the proposed generation procedure and the ones used in [a,b], the authors should provide more theoretical and empirical insights on the difference and significance.\n\n[a] Openmix: Reviving known knowledge for discovering novel visual categories in an open world (CVPR 2021)\n\n[b] Neighborhood contrastive learning for novel class discovery (CVPR 2021)"
                },
                "questions": {
                    "value": "In Fig. 6, does the visualizations mean that there are more channels transferred in generic datasets like ImageNet, yet less channels transferred in fine-grained datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5777/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698409777222,
            "cdate": 1698409777222,
            "tmdate": 1699636607443,
            "mdate": 1699636607443,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AFFnVhKmv5",
                "forum": "j20nMRUWK9",
                "replyto": "K9C5gWdaA0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5777/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5777/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UEjY"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the thorough evaluation and constructive feedback provided. We hope that the following response could address your concerns.\n\n## Weakness\n\n(1) Thank you for your clarification. We aim to convey a clear overview of explicit knowledge transfer within GCD problems. While we instantiate knowledge generation, alignment, and distillation using a known-class pre-trained model, AL and CSM, and nMI, our framework allows for flexibility and alternative methods can be employed for these components. For instance, as outlined in the paper, the contrastive loss we utilize can be replaced by other knowledge distillation methods. The primary objective of this writing is to elucidate the explicit knowledge transfer framework for GCD problems, highlight the core challenges associated with knowledge transfer in GCD problems, and offer insights for future research.\n\n(2) We respectfully disagree with you on three points:\n1. Knowledge distillation is not the most important component of our method. It holds equal importance with other components, AL and CSM. The ablation study in Tab.4 demonstrates that each component significantly contributes to the model's improvement. As discussed in the paper, not all knowledge in known class data is beneficial for novel class learning. This indicates that nMI does not always enhance the model's performance, as supported by the evidence in Tab.12 in Appendix G. This underscores the importance of AL and CSM.\n2. We would like to clarify that we do not claim novelty in negative sample generation within the paper, which is widely used across various contrastive learning domains. All the other negative sample generation strategies that avoid class collision issues like [2, 3] can also be applied to our method. \n3. Negative sample generation is not an important component in our method. As shown in the table below, the model's performance without the \"negative sample generation\" technique remains satisfiable. The reason we opted to use \"negative sample generation\" is to address the class collision issues of novel classes, as evidenced in the presented results.\n\n| Method | CUB | | | StanfordCars | | | Aircraft | | |\n|-------------------------|--------------------------|-----------------------------------|------------------------------|--------------------------|-----------------------------------|------------------------------|--------------------------|-----------------------------------|------------------------------|\n|                         | All                      | Seen                              | Novel                        | All  | Seen | Novel | All  | Seen | Novel |\n| Ours w/o NSG                 | 67.2                     | 73.2                              | 64.2                         | 58.9 | 80.2 | 48.6  | 54.1 | 60.3 | 51.0  |\n| Ours                     | 67.1                     | 73.7                              | 63.8                         | 59.2 | 79.1 | 49.6  | 55.9 | 60.7 | 53.6  |\n\n## Question\nWe would like to clarify that Fig. 6 illustrates the average outcome of the Channel Selection Matrix generated for each sample in the corresponding class. It can well reflect whether CSM can produce a general selection method for a specific class. However, it does not represent the number of open channels. Additionally, it's worth noting that the values represented by the color depth of each subfigure in Fig. 6 differ, with the maximum value of 0.002 on ImageNet being much smaller than observed on several other datasets. Here, we present the average number of open channels on novel classes in the table below, indicating that fewer channels are transferred in generic datasets, while more channels are transferred in fine-grained datasets.\n\n|                                    | CUB                  | StanfordCars         | Aircraft             | CIFAR 100-80         | ImageNet100         |\n|:-----------------------------------:|----------------------|----------------------|----------------------|----------------------|----------------------|\n| #open channels | 316                    | 284                    | 254                    | 224               | 245 |\n\n[1] Openmix: Reviving known knowledge for discovering novel visual categories in an open world (CVPR 2021)\n\n[2] Neighborhood contrastive learning for novel class discovery (CVPR 2021)\n\n[3] Zheng, Mingkai, Fei Wang, Shan You, Chen Qian, Changshui Zhang, Xiaogang Wang, and Chang Xu. \"Weakly supervised contrastive learning.\" In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10042-10051. 2021."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700423330821,
                "cdate": 1700423330821,
                "tmdate": 1700423330821,
                "mdate": 1700423330821,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Bfo4zX7hWc",
            "forum": "j20nMRUWK9",
            "replyto": "j20nMRUWK9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5777/Reviewer_ZnW3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5777/Reviewer_ZnW3"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the generalized category discovery problem. Three steps are proposed to improve the performance. The first step is to learn a pre-trained model with known classes. Then an adapter layer and a channel selection matrix are further learned for knowledge transfer. The last step is to use knowledge distillation. The experiments are conducted on conventional benchmarks and a new benchmark with different difficulty levels."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) It is clearly written and easy to follow.\n(2) Table 1 and Figure 1 show clear motivation for the proposed ideas.\n(3) The experimental section is comprehensive and the performance gain is significant in some cases."
                },
                "weaknesses": {
                    "value": "(1) The proposed techniques are well-known in the field and I have the feeling that they are not specifically designed for the GCD problem.\n(2) The channel selection seems too simple, I am wondering if there are more sophisticated options.\n(3) The experimental results on some datasets are much better than the others but worse on CUB, It would be good to discuss the possible reasons why this is happening."
                },
                "questions": {
                    "value": "The contrastive loss used in the paper is quite common and it would be good to further discuss the novelty of the method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5777/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698630709811,
            "cdate": 1698630709811,
            "tmdate": 1699636607343,
            "mdate": 1699636607343,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B0FhXrKUJ9",
                "forum": "j20nMRUWK9",
                "replyto": "Bfo4zX7hWc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5777/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5777/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZnW3"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the thorough evaluation and constructive feedback provided. We hope that the following response could address your concerns.\n\n## Weakness\n\n(1) We would like to clarify the novelty of our method and its close relationship with the GCD problem in three points: \n1. Our framework represents a novel approach to addressing GCD challenges, specifically designed for explicit knowledge transfer in GCD tasks. To our best knowledge, no similar framework has been proposed previously for GCD tasks. As discussed in the introduction, knowledge generation, knowledge alignment, and knowledge distillation in our framework aim to generate known class knowledge representation, facilitate more targeted knowledge transfer, and perform knowledge transfer, respectively. Each component is specifically designed for the task of explicit knowledge transfer in GCD.\n2. The techniques proposed in our work are intricately aligned with this innovative framework. Our proposed AL and CSM are unique solutions crafted specifically to tackle knowledge transfer challenges in the GCD problem. AL aligns the known-class representation space to the joint representation space, and CSM selects meaningful channels, enabling more targeted knowledge transfer. To the best of our knowledge, no such techniques have been proposed before.\n3. We utilize the contrastive loss to maximize the mutual information. The contrastive loss we introduce is not a traditional one; rather, it uniquely combines AL and CSM, tailoring it to the intricacies of the GCD problem. This novel contrastive loss enables effective knowledge transfer between known and novel classes, unlike traditional knowledge transfer methods. \n\n(2) We contend that the use of ReLU activation is a deliberate choice based on its simplicity and efficiency. And we believe that simplicity is not a drawback but rather an advantage, contributing to the ease of implementing our proposed framework and inspiring future work in designing more sophisticated mechanisms for better knowledge selection.\n\n(3) We would like to point out that the goal of GCD is to learn a model, that can better classify known classes and cluster novel classes, concurrently. Since most existing methods can adapt the model's learning focus by adjusting the weight of $\\mathcal{L} _ {u}$ in Equ (1), it is not adequate to evaluate the performance of the model based only on a single metric in the GCD problem. A case in point is the CUB dataset, where the DCCL method exhibits superior performance on novel classes compared to all other methods, yet its performance on known classes is inferior. This observation suggests a potential bias in the DCCL method towards novel class data. Unfortunately, comprehensive analysis is difficult as DCCL has not released its code. When considering all metrics comprehensively, our model is also better than other models on CUB.\n\n## Question\n\nPlease refer to the weakness section."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700423241586,
                "cdate": 1700423241586,
                "tmdate": 1700423241586,
                "mdate": 1700423241586,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MOWGpSy6lE",
                "forum": "j20nMRUWK9",
                "replyto": "B0FhXrKUJ9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5777/Reviewer_ZnW3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5777/Reviewer_ZnW3"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your responses."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647606091,
                "cdate": 1700647606091,
                "tmdate": 1700647606091,
                "mdate": 1700647606091,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SMOsmuem5u",
            "forum": "j20nMRUWK9",
            "replyto": "j20nMRUWK9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5777/Reviewer_Npp8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5777/Reviewer_Npp8"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a unique adaptive knowledge transfer framework designed for generalized category discovery, aiming to create a clear connection for knowledge transfer between known and novel classes. The framework is divided into three main components: knowledge generation using a model trained on known class data, knowledge alignment with an adapter layer and a channel selection matrix for more precise knowledge transfer, and knowledge distillation to maximize mutual information between two representation spaces. \n\nExtensive evaluations demonstrate the superiority of this approach over existing methods, and the framework provides a new perspective for advancing knowledge transfer in generalized category discovery, showing great potential to address the challenge of transferring knowledge from known to novel classes effectively."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper introduces a novel framework, meticulously deconstructing the GCD task into three pivotal stages: knowledge generation, knowledge alignment, and knowledge distillation. This explicit dissection embeds a layer of human prior knowledge into the design of the network architecture, showcasing an innovative and finely crafted approach.\n2. The results across the majority of datasets indicate that the method achieves state-of-the-art performance."
                },
                "weaknesses": {
                    "value": "The ablation study section on the Adapter Layer (AL) contains several perplexing aspects that raise questions about the method's efficacy, generalizability, and the readability of the paper.\n\n1. In Table 4, the inclusion of AL appears to result in a performance decline on the Scars benchmark, yet the authors provide no explanation for this, which makes me question the generalizability of the method.\n\n2. In Table 5, the distinction between AL and ours seems to be inadequately clarified, rendering this part somewhat hard to follow."
                },
                "questions": {
                    "value": "As noted in the Weakness, I wonder why there is a observed decline in results on the Scars dataset following the incorporation of the Adapter Layer (AL). Is this reduction in performance linked to specific characteristics of the dataset itself, or are there elements of the method that may not be as effective when applied to Scars? I am looking forward to the authors' elucidation on this matter.\n\nAdditionally, I would like the authors to clarify the distinctions between the rows for AL and ours in Table 5, with a particular emphasis on the columns where the values are 0."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5777/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766942996,
            "cdate": 1698766942996,
            "tmdate": 1699636607245,
            "mdate": 1699636607245,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Hz8kRjh8nH",
                "forum": "j20nMRUWK9",
                "replyto": "SMOsmuem5u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5777/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5777/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Npp8"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the thorough evaluation and constructive feedback provided. We hope that the following response could address your concerns.\n\n(1) Firstly, concerning AL, its objective is to transform the representation space of known classes into a joint representation space that is more advantageous for novel classes. However, because the transformation introduces novel classes, it may have a negative impact on known classes, depending on the relationship between known and novel classes, which is dataset-specific. We further conduct an ablation study on the coarse-grained dataset in Appendix G, where we also observe a performance drop in known classes and an increase in novel classes when applying AL. The potential negative impact of AL underscores the importance of the design of the CSM. It is noteworthy that the features obtained after both AL and CSM can be considered beneficial for novel class learning. This observation is further supported by the results of the ablation study. Secondly, we would like to clarify that our model integrates not only AL but also nMI and CSM. The utilization of all three components collectively results in notable improvements across all known and novel classes in all datasets. This underscores the method's robust generalizability. \n\n(2) Apologies for any confusion. In Table 5, we assess the model with a frozen known-class pre-trained encoder, adapter layer, and cluster head (denoted as 'AL') to evaluate the quality of the Adapter Layer. The adapter layer and cluster head are learned by $\\mathcal{L} _ {base}$. When the number of layers is set to 0, it means we only learn a cluster head. \"Ours\" denotes our final model with all components. Importantly, when the number of layers is set to 0, \"Ours\" directly applies nMI without AL and CSM modules."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700423072578,
                "cdate": 1700423072578,
                "tmdate": 1700423072578,
                "mdate": 1700423072578,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RJh9T97kxL",
            "forum": "j20nMRUWK9",
            "replyto": "j20nMRUWK9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5777/Reviewer_tjGj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5777/Reviewer_tjGj"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of generalized category discovery, which involves the identification of novel classes in unlabeled data by leveraging information from known classes. Existing methods typically employ a shared encoder to transfer information between labeled and unlabeled data. In contrast, this paper presents a novel framework for explicit knowledge distillation, ensuring effective knowledge transfer from labeled to unlabeled data. The framework incorporates an adaptive layer, a channel-wise selection matrix, and a naive mutual information loss. Additionally, the paper introduces a benchmark dataset called iNat21, along with several data split schemes that consider the semantic gap between labeled and unlabeled data. Experimental results demonstrate the superior performance of the proposed method compared to existing state-of-the-art approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) This method demonstrates consistent effectiveness across multiple datasets and split schemes, surpassing the performance of previous state-of-the-art approaches.\n\n(2) The main contribution of this work is the introduction of a channel-wise selection matrix, which plays a crucial role in facilitating effective knowledge distillation.\n\n(3) Furthermore, the proposed framework incorporates a naive Mutual Information loss, which is compatible with various formats such as InfoNCE loss, MSE loss, and KL divergence. This compatibility enhances the flexibility and applicability of the framework in different scenarios."
                },
                "weaknesses": {
                    "value": "(1) Some experiments and tables in this paper lack explanation and details. For example:\na) The \"BL\" and \"Clu\" settings in Table 1 require further clarification.\nb) The specification of the baseline model used in the ablation study needs to be provided.\nc) Implementation details of crNCD are missing.\n\n(2) The key idea of knowledge distillation under the NCD setting has been explored in the literature, for example [1, 2], while discussion and analysis are missing.\n\n(3) The paper lacks an explanation of how the naive Mutual Information loss is derived in the format of InfoNCE from the objective of mutual information.\n\n(4) Herbarium is also a commonly used dataset to evaluate the performance of GCD. However, this is missing in the experiments.\n\n[1] Zhao et al, Novel Visual Category Discovery with Dual Ranking Statistics and Mutual Knowledge Distillation, NeurIPS 2021\n[2] Gu et al. Class-relation Knowledge Distillation for Novel Class Discovery, ICCV 2023"
                },
                "questions": {
                    "value": "(1) The paper mentions training the model for 20 epochs on labeled data in the first stage, but it lacks an explanation for choosing this specific number. It would be beneficial to provide a rationale for selecting 20 epochs and elaborate on how this choice impacts the results.\n\n(2) In the ablation study concerning the adapter layer, each layer is composed of a linear layer and a ReLU layer. It would be helpful to include an experiment using only one ReLU layer to assess the impact of this specific component.\n\n(3) Table 4 exclusively utilizes fine-grained datasets. To provide a more comprehensive evaluation of the proposed method, it would be also important to analyze each component on a coarse-grained dataset.\n\n(4) Previous work [3] suggests that supervised knowledge is beneficial for the task of NCD when the semantics of labeled and unlabeled data are similar. However, this paper demonstrates that even under a data split where the semantic gap between labeled and unlabeled data is significant, knowledge from a fixed labeled encoder remains helpful. It would be helpful to demonstrate how labeled knowledge transfers to unlabeled data across different scales of semantic differences.\n\n(5) The paper mentions the usage of techniques such as dynamic conception generation [4] and cluster size regularization [5]. Analyzing the effects of these techniques would further reveal contributions to the overall performance of the proposed method.\n\n[3] Li, Ziyun, et al. \"Supervised Knowledge May Hurt Novel Class Discovery Performance.\" arXiv preprint arXiv:2306.03648 (2023).\n\n[4] Pu, Nan, Zhun Zhong, and Nicu Sebe. \"Dynamic Conceptional Contrastive Learning for Generalized Category Discovery.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[5] Wen, Xin, Bingchen Zhao, and Xiaojuan Qi. \"Parametric classification for generalized category discovery: A baseline study.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5777/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698776770143,
            "cdate": 1698776770143,
            "tmdate": 1699636607137,
            "mdate": 1699636607137,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ddl7j33FtC",
                "forum": "j20nMRUWK9",
                "replyto": "RJh9T97kxL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5777/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5777/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tjGj"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the thorough evaluation and constructive feedback provided. We hope that the following response could address your concerns.\n\n## Weakness \n(1)  We appreciate your feedback and have addressed the issues in the revised version of the paper. \n\n- \u201cBL\u201d is the baseline model. In detail, the baseline model has the same architecture as the final model and only uses $\\mathcal{L} _ {base}$ in Equ (1) to learn known and novel classes. The loss can influence both the encoder and the classifier. Meanwhile,  \u201cClu.\u201d denotes that we directly cluster the features generated by the pre-trained known-class model. The model of \u201cClu.\u201d is also the model with the same architecture as the final model and uses the same $\\mathcal{L} _ {base}$ to learn known and novel classes. But now, the encoder is frozen, meaning the loss can only influence the classifier. In Appendix H, we also discuss the details of clustering.\n- This baseline model is the same model \u201cBL\u201d that we discussed above.\n- As crNCD is originally designed for NCD problems, we have adapted it to the GCD setting using a widely employed architecture utilized in other GCD methods. Additionally, to mitigate the influence of different self-labeling designs, we employ the same $\\mathcal{L} _ {base}$ (Equ.1) as in our method, instead of using the original $\\mathcal{L} _ {opt}$ from crNCD. The original $\\mathcal{L} _ {opt}$ in crNCD did not perform well in the GCD setting, as indicated in the table below. It is important to note that we have made no modifications to the core loss $\\mathcal{L} _ {rKT}$.\n\n| Method         | CUB | | | StanfordCars | | | Aircraft | | |\n|---------------------------------|--------------------------|-----------------------------------|------------------------------|--------------------------|-----------------------------------|------------------------------|--------------------------|-----------------------------------|------------------------------|\n|                                 | All                      | Seen                              | Novel                        | All  | Seen | Novel | All  | Seen | Novel |\n| $\\mathcal{L} _ {opt}$    + crNCD | 56.9                     | 57.8                              | 56.4                         | 46.6 | 62.3 | 39.1  | 45.2 | 59.0 | 43.4  |\n| $\\mathcal{L} _ {base}$ + crNCD        | 64.1                     | 75.2                              | 58.6                         | 54.8 | 76.5 | 44.3  | 53.1 | 57.0 | 51.3  |\n\n\n(2) We would like to clarify the distinctions between our method and those proposed in [1,2]. We note that a detailed discussion of [2] has been provided in the related work section. \n\n- Here we summarize the three main differences between our methods and [2] for clarity. \n  1. Our paper proposes a novel explicit knowledge transfer framework that introduces a fresh perspective for advancing knowledge transfer and [2] is only one of the concrete implementations of our proposed framework. Compared with the limited form of knowledge transfer in [2], our framework is more flexible and effective.\n  2. [2] performs knowledge distillation on the output space, while we conduct it in the feature space which contains more information. [2] does not employ knowledge selection to filter out harmful knowledge or knowledge alignment to transfer knowledge more efficiently; instead, they use a simple weight function to amplify knowledge distillation when class relations are strong. \n  3. [2] is specifically designed for the NCD problem and it achieves poor results in GCD (table above). \n- Regarding [1], the primary objective of their loss function $\\mathcal{L} _ {sKLD}$ is to enable the model to capture both global and local information within an image. In contrast, our method is specifically designed to facilitate the transfer of known class knowledge to novel class learning. Notably, [1] employs a two-branch learning framework, with one branch focusing on local part-level information and the other on overall characteristics. They transfer knowledge between these two branches. They do not require knowledge alignment because they perform knowledge distillation in the same label space. In contrast, our approach employs an upper branch dedicated to extracting valuable known class knowledge and proposes AL and CSM to do effective knowledge transfer in different representation spaces. These fundamental differences in the purpose and methods distinguish our work from [1].\n\n\n(3) Thank you for your suggestion. In response, we provide the derivation of infoNCE from mutual information and the details can be found in Appendix A."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700422845739,
                "cdate": 1700422845739,
                "tmdate": 1700422915163,
                "mdate": 1700422915163,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TjUK6YOXEe",
                "forum": "j20nMRUWK9",
                "replyto": "RJh9T97kxL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5777/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5777/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tjGj"
                    },
                    "comment": {
                        "value": "(4) Firstly, as evidenced in the table above, directly performing knowledge transfer without AL and CSM may not consistently enhance model performance. This phenomenon confirms that not all supervised knowledge is beneficial. This reaffirms the necessity of introducing AL and CSM to filter and refine the transferred knowledge. Secondly, [3] only do experiments on CIFAR and ImageNet datasets. We follow their method and extend their experiments to iNat21 datasets and find that their conclusions may not hold for iNat21 datasets, emphasizing the dataset-specific nature of their findings. As shown in the table below, the novel class performance of UNO is consistent with our assumption on the data splits, and self-supervised uno, which abandons supervised knowledge, achieves inferior results. The results indicate that \n1. Supervised knowledge is still beneficial in the hard split of the iNat21 dataset. \n2. completely eliminating supervised information is not an optimal choice thus emphasizing the importance of transforming and filtering beneficial components in our proposed knowledge transfer framework.\n   \n| Method       | fine grain | easy | medium | hard |\n|--------------|------------|------|--------|------|\n| UNO          | 52.3       | 48.6 | 46.4   | 43.1 |\n| Self-sup UNO | 32.6       | 31.1 | 30.3   | 27.6 |\n\n  \n\n(5) Firstly, we have to clarify that we do not utilize the \"dynamic conception generation\"[10] technique. Secondly, the cluster size regularization technique is widely employed in unsupervised learning[5, 6] and GCD[7, 8, 9], and it is not our contribution. It ensures meaningful clustering by preventing the model from grouping all data into a single cluster. Therefore, without cluster size regularization, our model and most other GCD models will decrease greatly. Here, we also present the model's performance without cluster size regularization (CSR) in the table below. This table confirms the importance of cluster size regularization as discussed earlier. Nevertheless, our method can still contribute to performance improvement even without cluster size regularization.\n\n| Method | CUB | | | StanfordCars | | | Aircraft | | |\n|-------------------------|--------------------------|-----------------------------------|------------------------------|--------------------------|-----------------------------------|------------------------------|--------------------------|-----------------------------------|------------------------------|\n|                         | All                      | Seen                              | Novel                        | All  | Seen | Novel | All  | Seen | Novel |\n| Baseline w/o CSR    | 39.6  | 47.6  | 35.6  | 33.2  | 59.3  | 20.6  | 35.6  | 37.5  | 34.7 |\n| Ours w/o CSR  | 41.4  | 45.9  | 39.2  | 37.6  | 55.6  | 28.8  | 37.2  | 37.7  | 36.9 |\n| Baseline            | 61.7  | 68.0  | 58.5  | 49.6  | 56.3  | 46.2  | 51.8  | 71.9  | 42.0 |\n| Ours          | 66.8  | 75.6  | 62.5  | 55.6  | 60.5  | 53.1  | 57.6  | 75.9  | 48.8 |\n\n\n[1] Zhao et al, \"Novel Visual Category Discovery with Dual Ranking Statistics and Mutual Knowledge Distillation\", NeurIPS 2021.\n\n[2] Gu et al. \"Class-relation Knowledge Distillation for Novel Class Discovery\", ICCV 2023\n\n[3] Li, Ziyun, et al. \"Supervised Knowledge May Hurt Novel Class Discovery Performance.\" arXiv preprint arXiv:2306.03648 (2023).\n\n[4] Oord AV, Li Y, Vinyals O. \"Representation learning with contrastive predictive coding.\" arXiv preprint arXiv:1807.03748. 2018 Jul 10\n\n[5] Asano YM., Rupprecht C., and Vedaldi A. \"Self-labelling via simultaneous clustering and representation learning.\" In Proc. ICLR, 2020.\n\n[6] Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool. \"Scan: Learning to classify images without labels.\" In Proc. ECCV, 2020.\n\n[7] Fini, Enrico, et al. \"A unified objective for novel class discovery.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n\n[8] Wen, Xin, Bingchen Zhao, and Xiaojuan Qi. \"Parametric classification for generalized category discovery: A baseline study.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[9] Kai Han, Sylvestre-Alvise Rebuffi, Sebastien Ehrhardt, Andrea Vedaldi, and Andrew Zisserman.\nAutonovel: Automatically discovering and learning novel visual categories. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2021.\n\n[10] Pu, Nan, Zhun Zhong, and Nicu Sebe. \"Dynamic Conceptional Contrastive Learning for Generalized Category Discovery.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700422956488,
                "cdate": 1700422956488,
                "tmdate": 1700422987828,
                "mdate": 1700422987828,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HLmS8ykOGF",
            "forum": "j20nMRUWK9",
            "replyto": "j20nMRUWK9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5777/Reviewer_2BvT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5777/Reviewer_2BvT"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of generalized category discovery (GCD). Unlike prior methods that rely on implicit knowledge transfer via shared representation spaces, this work introduces a framework for explicit and adaptive knowledge transfer to facilitate the discovery of novel classes. The proposed method consists of three main steps: (1) capturing known class knowledge through a pre-trained model, (2) transforming this knowledge via an adapter layer and a channel selection matrix for more effective transfer, and (3) employing knowledge distillation to maximize mutual information between representation spaces. It also presents a new benchmark, iNat21, designed with varying levels of difficulty to assess GCD methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1- It introduces an innovative solution to the complex issue of generalized category discovery (GCD)\n\n2- The paper is articulate and well-structured, providing clear explanations for the motivations behind the approach and the intricacies of the loss functions used.\n\n3-The literature review is exhaustive, offering a thorough overview of related work in the area.\n\n4- Ablation studies and experiments are rigorously conducted, encompassing a broad spectrum of the method's components, which solidifies the validity of the research.\n\n5-The introduction of the iNat21 benchmark is a valuable asset that will likely drive and shape future research in GCD."
                },
                "weaknesses": {
                    "value": "A potential weakness of the paper could be the use of ReLU activations, which are known for their \"dead neuron\" issue, potentially leading to some neurons becoming inactive due to poor initialization. This characteristic of ReLU could result in the unintentional filtering out of certain channels that might otherwise be useful for learning a more robust embedding space."
                },
                "questions": {
                    "value": "1-How would the model's performance be impacted if activation functions other than ReLU, such as GeLU, were utilized?\n\n2-Regarding Figure 2, is the 'Cls' in the lower branch designed exclusively for labeled samples? Additionally, in the upper branch, what mechanism allows the model to generate categories for the unknown classes after the classification step?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5777/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819450019,
            "cdate": 1698819450019,
            "tmdate": 1699636607036,
            "mdate": 1699636607036,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RqZ8i0J4aJ",
                "forum": "j20nMRUWK9",
                "replyto": "HLmS8ykOGF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5777/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5777/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2BvT"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the thorough evaluation and constructive feedback provided. We hope that our response could address your concerns.\n\n## Weakness\nWe opt for the ReLU activation function due to its simplicity and efficiency. Following your insights, we have conducted an analysis to count the number of dead neurons in the model, and the results are presented in the table below. Fortunately, our final model utilizing ReLU does not exhibit this issue. We believe this is potentially due to our truncated norm initialization. In the subsequent discussion, we will delve into a comparative analysis of the model's performances when employing ReLU and GeLU.\n\n|                                    | CUB                  | StanfordCars         | Aircraft             | CIFAR 100-80         |\n|:-----------------------------------:|----------------------|----------------------|----------------------|----------------------|\n| #dead neurons | 0                    | 0                    | 0                    | 0               |\n\n## Question\n(1) We appreciate your attention to the activation function choice. As depicted in the table below, adopting GeLU resulted in marginal changes in model performance. These experimental findings indicate that both GeLU and ReLU deliver comparable performances, providing evidence that there are few or no dead neurons in our model. Combining this with the empirical results mentioned above, we maintain that ReLU remains a straightforward and effective choice under a good initialization.\n\n| Method | CUB | | | StanfordCars | | | Aircraft | | |\n|-------------------------|--------------------------|-----------------------------------|------------------------------|--------------------------|-----------------------------------|------------------------------|--------------------------|-----------------------------------|------------------------------|\n|                         | All                      | Seen                              | Novel                        | All  | Seen | Novel | All  | Seen | Novel |\n| GeLU                    | 66.3                     | 73.8                              | 62.5                         | 58.3 | 77.6 | 49.0  | 56.2 | 61.0 | 53.8  |\n| ReLU                    | 67.1                     | 73.7                              | 63.8                         | 59.2 | 79.1 | 49.6  | 55.9 | 60.7 | 53.6  |\n\n\n\n(2) In Figure 2, the 'Cls' in the lower branch and the upper branch both can classify labeled and unlabeled data. The 'Cls' in the upper branch is learned based on $\\mathcal{L} _ {base} $  in Equ (1). The loss term $\\mathcal{L} _ {u} $ in $\\mathcal{L} _ {base} $ is the self-labeling loss. Specifically, it utilizes the predictions of 'Cls' in one view of the image to generate pseudo-labels for the other view of the same image. This mechanism enables the model to assign categories to unlabeled data. As a result, the 'Cls' component can classify both known and novel classes. For a more in-depth explanation of $\\mathcal{L} _ {u}$, we invite you to refer to Appendix D. Your interest is greatly appreciated."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700422587256,
                "cdate": 1700422587256,
                "tmdate": 1700422587256,
                "mdate": 1700422587256,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XlY4HH5vM6",
                "forum": "j20nMRUWK9",
                "replyto": "RqZ8i0J4aJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5777/Reviewer_2BvT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5777/Reviewer_2BvT"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response and for elucidating the concepts discussed in the paper. The suggestion to investigate the impact of actively filtering this vector on overall performance presents an intriguing direction for future research. I appreciate the paper's detailed and engaging examination of the GCD problem. Based on these considerations, I have decided to maintain my original review score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5777/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649627319,
                "cdate": 1700649627319,
                "tmdate": 1700649627319,
                "mdate": 1700649627319,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]