[
    {
        "title": "Elevating Augmentation: Boosting Performance via Sub-Model Training"
    },
    {
        "review": {
            "id": "IsmUZgPAPe",
            "forum": "49CGs58v0J",
            "replyto": "49CGs58v0J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission468/Reviewer_FBYo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission468/Reviewer_FBYo"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new regulation method to improve the performance of deep learning models. The idea is to randomly sample sub-networks from the full-network, and train the sub-networks together with the full network, with the supervision from the full network. The method achieves better performance than other regularization methods on multiple network backbones."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThere are comprehensive experiments and ablation study to show the effectiveness of the proposed method.\n2.\tThe proposed method achieves better performance than other methods on different backbones, although the improvements are not that significant.\n3.\tThe paper is well-written and easy to follow"
                },
                "weaknesses": {
                    "value": "1.\tThe idea of this paper is very similar to GradAug [1] and CoSub [2]. In GradAug, the author proposed to sample sub-networks and train them with the full-networks for improvement representation learning. The difference is that in this work the author did this based on ViT backbones. In CoSub, the author similarly proposed to sample sub-networks and train them together with the full-network. I suggest the author to conduct a thorough discussion and comparison with these methods, and give some insights on why the proposed method could be better or worse.\n2.\tI am not sure about the significance of the proposed method. Compared to the baseline, the proposed method mostly achieves an improvement within 0.5%, but with 1.5X training cost.\n3.\tCould the proposed method be applied to other tasks besides image classification? Such as detection and segmentation? If it is only applicable to image classification, it further limits the significance of the method."
                },
                "questions": {
                    "value": "Please see the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission468/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698617635390,
            "cdate": 1698617635390,
            "tmdate": 1699635973404,
            "mdate": 1699635973404,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "eVoKaxfacS",
            "forum": "49CGs58v0J",
            "replyto": "49CGs58v0J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission468/Reviewer_AdC8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission468/Reviewer_AdC8"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a regularization method called Augmenting Aub-model (AugSub), which imposes consistency between outputs with and without a stochastic masking method (e.g., dropout, drop-path, and random masking used in mask MAE).\nThe paper shows using random masking, AugMask, is the most effective in terms of both efficiency and accuracy.\nThe proposed method constantly improves accuracy from baselines in various models and tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method is simple and easy to use.\n- According to Table 1, the method works well regardless of a drop ratio."
                },
                "weaknesses": {
                    "value": "1. The method is inherently the same as the consistency regularization used in semi-supervised learning [1][2].\nThe difference between them is where the stochasticity is induced. As the authors discuss in the introduction, there are various regularization methods, and I wonder why the authors focus on drop-based techniques.\nI think the proposed method using data augmentation or other techniques instead of drop-based methods should be compared.\nIn addition, I think using the ema model[3] instead of the sub-model is also an important baseline.\n\n2. The paper shows the proposed method works well for various settings but does not explain why the proposed method works.\nThe proposed method can be regarded as a combination of drop-based techniques and soft targets (self-distillation).\nI suspect the soft targets inherently improve accuracy because the effectiveness of the soft targets, including self-distillation and label smoothing, is verified in various tasks and models.\nIf so, drop-based techniques are not essential.\nThus, the author would be better to design the experiments that reveal what is essential.\n\n[1]Oliver, Avital, et al. \"Realistic evaluation of deep semi-supervised learning algorithms.\" 2018  \n[2]Sohn, Kihyuk, et al. \"Fixmatch: Simplifying semi-supervised learning with consistency and confidence.\" 2020  \n[3]Tarvainen, Antti, and Harri Valpola. \"Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.\" 2017"
                },
                "questions": {
                    "value": "- Why does the accuracy of MAE finetuning in Table A.1 decrease from Table 4? (e.g., ViT-L/16 with 50 epochs  reports 85.9 in Table 4 but ViT-L/16 with 75 epochs reports 85.5 in Table A.1.)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission468/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission468/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission468/Reviewer_AdC8"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission468/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698632200647,
            "cdate": 1698632200647,
            "tmdate": 1699635973334,
            "mdate": 1699635973334,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "Rmgy41pUqD",
            "forum": "49CGs58v0J",
            "replyto": "49CGs58v0J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission468/Reviewer_ZArF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission468/Reviewer_ZArF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new regularization technique which applies strong regularization on a dropout-based sub-model instead of the original model. This is because that directly applying the strong regularization on the original model usually leasds to inferior performance. They bypass this by imposing a self-distillation-based relaxed loss to distill knowledge from the original model to the submodel. Experiments mainly on the ImageNet dataset verify the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is easy to follow. I can understand the basic ideas quickly. \n2. The proposed method is simple yet effective. It widely improves the performance of ViTs with a strong training recipe."
                },
                "weaknesses": {
                    "value": "1. The motivation of this paper is clear. It lacks analysis and insight about 1) why imposing strong regularization leads to inferior performance; 2) why using a self-distillation loss and a submodel can solve this problem. Is it because the dropout or the self-distillation loss? What if we use the original loss to train the submodel or additionally use a self-distillation loss to train a strongly regularized model? Basically, it is not clear the relations between regularization\u3001dropuut and self-distillation. No ablation studies about this is provided. \n2. The proposed method would increase the training cost by a large proportion. \n3. This paper provides experimental analysis with a 100-epoch training recipe. However, from my previous experimental experice, applying regularization on models training with fewer epochs lead to different phenomonons."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission468/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission468/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission468/Reviewer_ZArF"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission468/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839680326,
            "cdate": 1698839680326,
            "tmdate": 1699635973266,
            "mdate": 1699635973266,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]