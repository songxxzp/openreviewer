[
    {
        "title": "Is Inverse Reinforcement Learning Harder than Standard Reinforcement Learning?"
    },
    {
        "review": {
            "id": "wqhDkLwY7f",
            "forum": "S24zdyiWDT",
            "replyto": "S24zdyiWDT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7852/Reviewer_cGXj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7852/Reviewer_cGXj"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of inverse reinforcement learning: given observations of an expert policy deployed in a (tabular) MDP, what reward function can one learn which is consistent with the actions of the expert policy? They consider both the offline (batch) setting as well as the online setting. They propose a metric for inverse reinforcement learning, defined in terms of the difference between the learned and ground truth rewards.\n\nFor the offline setting, they propose a pessimistic algorithm and show it achieves poly(S, A, H) sample complexity, as well as some dependence on the concentrability coefficient. For the online setting, they propose an algorithm which also achieves poly(S, A, H) sample complexity (and uses the offline algorithm as a subroutine). There is also an extension to a \"transfer learning\" setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is thorough, detailed, and mathematically rigorous. (As a disclaimer, I only checked in detail the proofs and associated lemmas for Thm 4.2, their main result on the sample complexity of the offline algorithm; but I am familiar with most of the literature and techniques used for online/offline tabular RL, so the results seemed correct to me.)\n- To me, the most interesting contribution is the new notion of \"performance\", which is measured in terms of a uniform distance for a set of \"reward mappings\" (Defn 2.1). I think this notion of metric merits future discussion and is a valuable contribution to this area. However, I have some comments/questions about it - see below.\n- On the algorithmic side, this paper also shows how several existing techniques (i.e., pessimism, reachable state identification / reward free exploration) can also be adapted to the IRL problem. While the algorithms and analysis themselves are not particularly novel, it is nice that we can use well-studied techniques in RL for the IRL problem."
                },
                "weaknesses": {
                    "value": "- Given that this paper proposes new metrics for IRL, I found the discussion / comparison with previous work a bit vague. At times, the language and writing was a bit informal, and sometimes confusing to interpret. \n    - For example, in the appendix you write that \"our method that considers is greater than theirs\". What does it mean for a method to be greater than another method? Did you mean your metric?\n    - You write in C.3 that the \"metric can't capture transitions\". What does this actually mean?\n- I would urge the authors to rewrite their comparison with prior work in Appendix C, focusing on cases where one subsumes the other, and giving concrete examples when your algorithm(s) can achieve guarantees while previous algorithms cannot. What is currently lacking is a distinction in the writing between (1) comparing the quantities $d^\\pi$ and $d^\\mathrm{all}$ themselves to prior work; (2) comparing guarantees that your algorithms can achieve to guarantees that algorithms from prior work can achieve.\n- (minor) weakness: the upper/lower bounds seem to be loose. In particular, it would be good if the authors commented on the fact that the lower bound has a $\\min \\{S, A\\}$ term - where does this come from? can it be improved? In general, the regime that we care about is when $S \\ge A$, so the fact that the rate is only sharp when $S \\le A$ is not very meaningful.\n\nMinor comments:\n- Many typos throughout, especially in the appendix. Some examples:\n    - \"Broaderly\" in the first paragraph.\n    - e.g., at bottom of page 4, \"Given a policy $\\pi$, We\" -> lower case the \"we\".\n    - a missing citation before Corollary 4.3? I see an \"()\".\n    - \"week transferability\" should be \"weak transferability\"?\n    - some typo in the equation (C.6).\n    - At the beginning of page 23, in the first display equation, should it be $a \\in \\mathrm{supp}$ instead of $a \\notin \\mathrm{supp}$?\n- For lower bounds, use $\\Omega$, not $O$.\n- In the algorithm pseudocode, a quantifier over the set $\\Theta$ is missing: The algorithm needs to be run separately for every $\\theta \\in \\Theta$."
                },
                "questions": {
                    "value": "1. I'm a bit confused about the discussion of the Hausdorff metric used by Metelli et al. The quantity that they propose seems to be some notion of diameter. I'm not sure why it should go to zero as you collect more samples, as even when $\\mathcal{R} = \\widehat{\\mathcal{R}}$ the quantity doesn't seem like it should be zero since the set of possible rewards could be large. But your results imply that an upper bound on this quantity goes to zero, so what am I missing here?\n2. While the definition of the metric seems mathematically well defined, it seems a bit weird: to define the reward mapping, you need a value and advantage function (V,A) (e.g., Eq. 3.2). However, inside the definition of the metric, you define a new value function of a particular policy and reward function (e.g., in Eq. 3.1). How do these two value functions relate?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7852/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7852/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7852/Reviewer_cGXj"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7852/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698697736220,
            "cdate": 1698697736220,
            "tmdate": 1699636962619,
            "mdate": 1699636962619,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BNi1d2L0iJ",
                "forum": "S24zdyiWDT",
                "replyto": "wqhDkLwY7f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7852/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7852/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback on our paper. We respond to the specific questions as follows.\n\n> At times, the language and writing was a bit informal and sometimes confusing to interpret.\n\nWe appreciate the reviewer for bringing up this question. We have made extensive revisions and polished Appendix C significantly in the latest version. We have addressed the issues you mentioned, and you can review the latest version.  For example, regarding \"our method that considers is greater than theirs\", detailed explanations are provided in the discussion above Lemma C.1 in the most recent version. Concerning \"metric can't capture transitions\",  comprehensive explanations can be found in the discussion above Proposition C.4 in the latest version.\n\n> I would urge the authors to rewrite their comparison with prior work in Appendix C, focusing on cases where one subsumes the other, and giving concrete examples when your algorithm(s) can achieve guarantees while previous algorithms cannot. What is currently lacking is a distinction in the writing between (1) comparing the quantities $d^{\\pi}$ and $d^{\\sf all}$ themselves to prior work; (2) comparing guarantees that your algorithms can achieve to guarantees that algorithms from prior work can achieve.\n\nThank you very much for your suggestions. We have made substantial revisions in the latest version of Appendix C as per your requests (see Appendix C in the latest version).\n\n\n> (minor) weakness: the upper/lower bounds seem to be loose. In particular, it would be good if the authors commented on the fact that the lower bound has a min{S,A} term - where does this come from? can it be improved? In general, the regime that we care about is when S\u2265A, so the fact that the rate is only sharp when S\u2264A is not very meaningful.\n\nAbout $\\textrm{min}\\\\{S,A\\\\}$ factor: Regarding $\\mathrm{min}\\\\{S,A\\\\}$ term, the introduction of $\\textrm{min}\\\\{S,A\\\\}$ is introduced as follows. In constructing hard instances, there is a step where we need to ensure $\\mathbb{P}_h ( s_i| s_i,a_i)=1$ for $i\\leq \\textrm{min}\\\\{S,A\\\\}$. This particular step is crucial for our proof. Exploring ways to improve the lower bound on $\\textrm{min}\\\\{S, A\\\\}$ could be a promising avenue for future research. We believe our results are still meaningful, and the condition $S < O(A)$ also holds significance. Regarding the lower bound in the online setting, we have made improvements (Theorem G.2), improving the original $\\mathcal{O}(H^2SA\\textrm{min}\\\\{S,A\\\\})$ to $\\mathcal{O}(H^3SA\\textrm{min}\\\\{S,A\\\\})$. Although it does not exactly match the upper bound, it has been significantly tightened. We appreciate the reviewer for bringing up this question.\n\n\nAbout $H$ factor: Regarding the lower bound in the online setting, we have made improvements (Theorem G.2), improving the original $\\mathcal{O}(H^2SA\\textrm{min}\\\\{S,A\\\\})$ to $\\mathcal{O}(H^3SA\\textrm{min}\\\\{S,A\\\\})$. Although it does not exactly match the upper bound, it has been significantly tightened. We appreciate the reviewer for bringing up this question. As for the lower bound in the offline setting, even though we employed the same MDP construction as in proving the lower bound in the online setting, we did not fully exploit the property of our MDP construction, where transitions are different for each h, because of the construction of the behavior policy. This results in our lower bound having a dependence on $H$ of $\\mathcal{O}(H^2)$. How to prove a tighter lower bound for the $H$-factor would be a highly intriguing avenue for future research.\n\n> I'm a bit confused about the discussion of the Hausdorff metric used by Metelli et al. The quantity that they propose seems to be some notion of diameter. I'm not sure why it should go to zero as you collect more samples, as even when $\\mathcal{R}=\\widehat{\\mathcal{R}}$ the quantity doesn't seem like it should be zero since the set of possible rewards could be large. But your results imply that an upper bound on this quantity goes to zero, so what am I missing here?\n\nIf we understood correctly, the reviewer is asking why the Hausdorff metric appears to be a kind of diameter for sets and why it tends to zero? The Hausdorff metric $D^{\\sf H}$ (see Section C.2) is indeed a measure of distance between sets, not a diameter. For instance, when two sets are equal, the Hausdorff metric becomes zero. And so the upper bound goes to zero."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7852/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700521497243,
                "cdate": 1700521497243,
                "tmdate": 1700527122150,
                "mdate": 1700527122150,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rgJ9EDzMER",
                "forum": "S24zdyiWDT",
                "replyto": "wqhDkLwY7f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7852/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7852/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 2"
                    },
                    "comment": {
                        "value": "> While the definition of the metric seems mathematically well defined, it seems a bit weird: to define the reward mapping, you need a value and advantage function (V,A) (e.g., Eq. 3.2). However, inside the definition of the metric, you define a new value function of a particular policy and reward function (e.g., in Eq. 3.1). How do these two value functions relate?\n\n$(V, A) \\in \\mathcal{V} \\times \\mathcal{A}$ represents a set of parameters, not a value function. Each pair $(V, A)$ determines a solution to an IRL problem (Lemma 2.2). Let $r^\\theta = \\mathscr{R}(V, A)$, where $\\mathscr{R}$ is a reward mapping induced by an IRL problem $(\\mathcal{M}, \\pi^E)$. The connection between the parameter $\\theta = (V, A)$ and the value function defined in Eq. 3.1 on Page 3 is as follows: $V^{\\pi^E}_h(s; r^\\theta) = V_h(s)$ and $A^{\\pi^E}_h(s, a; r^\\theta)=A_h(s,a)$.\n\n\n> Many typos throughout, especially in the appendix.\n\nWe thank the reviewer for this feedback. We have addressed and corrected the majority of the typos as suggested."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7852/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700521572110,
                "cdate": 1700521572110,
                "tmdate": 1700527165926,
                "mdate": 1700527165926,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VntBNXxCLx",
                "forum": "S24zdyiWDT",
                "replyto": "rgJ9EDzMER",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7852/Reviewer_cGXj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7852/Reviewer_cGXj"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Acknowledgement"
                    },
                    "comment": {
                        "value": "Thank you for your response. I have no further questions at this point."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7852/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670794858,
                "cdate": 1700670794858,
                "tmdate": 1700670794858,
                "mdate": 1700670794858,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QXmI5G1Zf4",
            "forum": "S24zdyiWDT",
            "replyto": "S24zdyiWDT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7852/Reviewer_uRxz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7852/Reviewer_uRxz"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes RIP and RLE method which opterates in offline and online IRL settings respectively. These methods do have some theoretical guarantees."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper builds metrics for both online and offline IRL settings. \n2. Informed by the pessimism principle, RLP is proposed for offline IRL setting with theoretical guarantees. \n3. RLE achieves great sample complexity compared to other online IRL methods."
                },
                "weaknesses": {
                    "value": "Lack of some experiment results.\n\nI think the theoretical analysis for IRL is important. However, the title of this paper is a little confusing. The sample complexity analysis seems not relevant to this title."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7852/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7852/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7852/Reviewer_uRxz"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7852/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698720343308,
            "cdate": 1698720343308,
            "tmdate": 1699636962500,
            "mdate": 1699636962500,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fM0lPvZRcE",
                "forum": "S24zdyiWDT",
                "replyto": "QXmI5G1Zf4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7852/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7852/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback on our paper. We respond to the specific questions as follows.\n\n> ... the title of this paper is a little confusing.\n  \nOur paper's title \u201cIs Inverse Reinforcement Learning Harder than Standard Reinforcement Learning?\u201d aims to convey that we can apply standard RL techniques to address IRL problems, including techniques such as pessimism and some exploring strategies. I apologize for any confusion. In the latest version, we provide the framework for our IRL algorithms in Section E.4. Here, we give a condition (Condition E.4) that is sufficient to do IRL. Then, we illustrate that pessimism can fulfill Condition E.4. We appreciate the reviewer for bringing up this question."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7852/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700520295703,
                "cdate": 1700520295703,
                "tmdate": 1700527017337,
                "mdate": 1700527017337,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U7i6C8bPr2",
                "forum": "S24zdyiWDT",
                "replyto": "fM0lPvZRcE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7852/Reviewer_uRxz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7852/Reviewer_uRxz"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for your response. I would like to keep my score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7852/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729861354,
                "cdate": 1700729861354,
                "tmdate": 1700729861354,
                "mdate": 1700729861354,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rFXSqdA6bw",
            "forum": "S24zdyiWDT",
            "replyto": "S24zdyiWDT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7852/Reviewer_RnbG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7852/Reviewer_RnbG"
            ],
            "content": {
                "summary": {
                    "value": "This paper gives theoretical guarantees for both online and offline setting in the inverse reinforcement problem."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think this work really pushes the inverse RL community research efforts further by answering:\n\n> can we give theoretical guarantees for the inverse RL algorithm enforced by pessimism for offline and exploration for online settings?\n\nThis is a really nice idea worthy for publication. My score reflect the weaknesses."
                },
                "weaknesses": {
                    "value": "Firstly note that this is an emergency review. I will rely on discussion with authors and reviewers, and other reviews, to decide on the score.\n\nI have only a few weaknesses for this work as follows:\n\n- The main paper writing needs to be improved. Yes, the soundness of this paper maybe good, considering similar pessimism and exploration ideas from past works. But this submission looks like a hurried submission with many typos like `Theoretical low bound in offline setting.`, ` $\\pi_b = \\pi_E ().,$`, `RLP utilizes empirical MDP and Pessimism frameworks`, `for all offline IRL problems with probability at least $1 \u2212 \\delta$, has to take at least ...\u0001 samples in expectation.` (both h.p. and in expectation?!), and so on. In addition to typos, the out-of-margin equation formatting makes a strenuous reading experience. To be honest, I am not sure if the authors can fix this writing issue of 56 pages during the rebuttal period, but I will welcome some attempts since proceedings require good quality.\n\n- The closest work I can think of is [1] that provides theoretical guarantees for imitation learning ($\\approx$ reward-free IRL+RL) in both online and offline data setting. The current work stops at reward learning, that is, the IRL problem. But without the extra RL step using the learned reward, is an incomplete story. The paper talks about similarities with RLHF; yes, there is a connection but one needs to eventually learn the optimal policy. Yes, one can just do planning with the learned reward and learned transition model, but equipping it with traditional model-based guarantees is important for making connections with other relevant works. _Model based guarantee_ will be unsatisfactory since [1] gives results for general function approximation. \n\n- Moreover, this manuscript subsumes many results from (Li et al., 2023) which is a non-peer reviewed work. This makes it hard to check soundness since one needs to evaluate both (at least the relevant parts required for this submission). I am mentioning this due to my emergency review. My score reflects the fact that generative model setting (samples from every state-action pairs $\\approx$ uniform concentrability) in Lindner et al. (2023), is equipped with pessimism and optimism terms to account for partial concentrability using the usual techniques in offline RL (Rashidinejad, et al. 2021) and thereafter.\n\nI am open to discussions with the authors and reviewers to make sure the work quality matches the score, which I believe so at this point, but a potential reach to 6 or 8 definitely exists. All the best for future decisions! \n\n[1] Jonathan D. Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, Wen Sun. Mitigating Covariate Shift in Imitation Learning via Offline Data Without Great Coverage, NeurIPS 2021."
                },
                "questions": {
                    "value": "-na-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7852/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828406964,
            "cdate": 1698828406964,
            "tmdate": 1699636962381,
            "mdate": 1699636962381,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U2E1YDcokv",
                "forum": "S24zdyiWDT",
                "replyto": "rFXSqdA6bw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7852/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7852/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback on our paper. We respond to the specific questions as follows.\n\n> The main paper writing needs to be improved...\n    \nWe sincerely apologize for the confusion caused by writing issues. Thank you very much for pointing out the concrete writing issues, and we have corrected most of them in the latest version. \n\n> The current work stops at reward learning, that is, the IRL problem. But without the extra RL step using the learned reward, is an incomplete story\u2026 Yes, one can just do planning with the learned reward and learned transition model, but equipping it with traditional model-based guarantees is important for making connections with other relevant works. Model-based guarantee will be unsatisfactory since [1] gives results for general function approximation.\n\nRegarding the use of learned rewards, in fact, we do have guarantees for the performance of performing RL algorithms with the learned rewards. I apologize for any confusion; our previous versions did not emphasize this part. In the latest version, we specifically discuss guarantees for performing RL algorithms with learned rewards (see Section C.4 and Corollary I.6, I.7 in the latest version). We appreciate the reviewer for bringing up this question.\n\n[1] introduces an algorithm for imitation learning under function approximation, analyzing numerous concrete examples such as discrete MDPs, linear models, and GP models. However, our paper focuses on Inverse Reinforcement Learning (IRL) within the tabular setting. Regarding \u201cYes, one can just do planning with the learned reward and learned transition model, but equipping it with traditional model-based guarantees is important for making connections with other relevant works. Model-based guarantee will be unsatisfactory since [1] gives results for general function approximation.\u201d, we may not have fully understood your meaning. Could you please provide further clarification or explanation?\n\n> My score reflects the fact that the generative model setting (samples from every state-action pairs \u2248 uniform concentrability) in Lindner et al. (2023), is equipped with pessimism and optimism terms to account for partial concentrability using the usual techniques in offline RL (Rashidinejad, et al. 2021) and thereafter.\n\nWhile the algorithm in  Lindner et al. (2023) also incorporates pessimism, we emphasize a fundamental difference in the purpose of pessimism between  Lindner et al. (2023) and our work. In  Lindner et al. (2023), the $E^h_k(s,a)$ defined in Eq (FBI) serves as an uncertainty measure, and the algorithm minimizes this $E^h_k(s,a)$ at each step to determine the sampling strategy (see Section 6.1 in Lindner et al. (2023)). On the other hand, our introduction of pessimism aims to impose a high penalty on state-action pairs that are rarely visited. This is done to ensure that the recovered rewards satisfy the monotonicity condition: $\\widehat{\\mathscr{R}}_h(s,a)\\leq \\mathscr{R}^\\star_h(s,a)$. This condition is crucial for obtaining guarantees in performing RL algorithms with learned rewards, as detailed in our latest version (see Proposition C.6 and Section E.4).\n\n [1] Jonathan D. Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, Wen Sun. Mitigating Covariate Shift in Imitation Learning via Offline Data Without Great Coverage, NeurIPS 2021."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7852/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700519987333,
                "cdate": 1700519987333,
                "tmdate": 1700526979052,
                "mdate": 1700526979052,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "erwx5fsn7U",
                "forum": "S24zdyiWDT",
                "replyto": "U2E1YDcokv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7852/Reviewer_RnbG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7852/Reviewer_RnbG"
                ],
                "content": {
                    "title": {
                        "value": "Clarification"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response. \n\nRegarding:\n> Yes, one can just do planning with the learned reward and learned transition model, but equipping it with traditional model-based guarantees is important for making connections with other relevant works.\n\nHere I tried to bring up the \"incomplete\" story of IRL. IRL stops at learning the reward functions. But how do you then do policy optimization with this reward function for making comparisons with RL? Of course, one can do model-based (empirical estimates of both reward and transition function) RL. But equipping it with sample complexity results will become crucial.\n\nI do think the authors have improved the manuscript compared to pre-rebuttal stage. I will update my score post authors-reviewers discussions."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7852/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545371744,
                "cdate": 1700545371744,
                "tmdate": 1700545371744,
                "mdate": 1700545371744,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xpDipdoULS",
            "forum": "S24zdyiWDT",
            "replyto": "S24zdyiWDT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7852/Reviewer_9kcm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7852/Reviewer_9kcm"
            ],
            "content": {
                "summary": {
                    "value": "The paper builds on recent works on Inverse Reinforcement Learning (IRL). It introduces a new metric different from the literature to analyze the distance between the feasible set of reward functions compatible with an expert's demonstrations in the limit of infinite samples and the estimated set. Their metric actually considers the distance between two mappings whose image set coincides with the feasible set. Next, the paper analyzes the sample complexity of estimating this mapping in the offline and online (forward model) settings. They provide algorithms for both the settings (and therefore upper bounds to the sample complexity) as well as lower bounds. Finally, the paper provides a sample complexity analysis on the error obtained when doing transfer learning with the estimated mapping, i.e., when transferring the estimated mapping instead of the true one."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- the main strengths are the sample complexity results for the offline and online (forward model) settings for IRL, which are the first results of this kind for IRL;\n- the idea of connecting transferability with concentrability when analyze the transfer learning setting is interesting"
                },
                "weaknesses": {
                    "value": "- no technical novelty in the proofs which are based on (i) previous works on offline RL and (ii) previous works on IRL\n- the lower bounds (both for offline and online settings) are definitely not tight, since they do not depend on the confidence $\\delta$\n- the metric introduced to evaluate the complexity in the offline setting is actually not a metric in the mathematical sense"
                },
                "questions": {
                    "value": "I am willing to adjust my score if the authors successfully answer my questions.\n\n1) Why do you use $d^\\pi$ (Definition 3.1) as a (pre)metric between reward functions? This is not a metric, and therefore the problem that you highlight in Lemma C.4 for the Hausdorff distance between sets sussists also for your (pre)metric $D_\\Theta^\\pi$ (Definition 3.2). It might be that the proposed (pre)metric $D_\\Theta^\\pi$ is zero even if the reward sets do not coincide. \n\n2) What is the rationale behind choosing the (pre)metric in Definition 3.1? In the usual pipeline, the reward recovered by IRL is then used for training RL agents. It does not seem to me that the (pre)metric in Definition 3.1 guarantees anything on how close the performance of the trained RL agent with the learned reward. In \"Towards Theoretical Understanding of Inverse Reinforcement Learning\" the authors focus on the actual distance between reward functions, not induced value functions. Can the author elaborate?\n\n3) Why do your lower bounds not depend on $\\delta$? This is quite significant especially when $\\delta$ is small. Inspecting the proofs in comparison with  \"Towards Theoretical Understanding of Inverse Reinforcement Learning\", it seems that the authors have adapted one construction only (the one that provides the part of the lower bound that does not depend on $\\delta$). Why this choice?\n\n4) How did you manage in the proof of the upper bound the fact that rewards defined as in the ground truth reward mapping are not bounded in $[-1,+1]$ even though the parameters $(V,A)\\in\\mathcal{V}\\times\\mathcal{A}$? In \"Towards Theoretical Understanding of Inverse Reinforcement Learning\", Lemma B.1 (appendix), they normalize the reward functions, but you don't. What allows you to avoid this step?\n\n5) The lower bounds do not match the upper bounds, especially for what concerns the dependence on the horizon H. What is the reason for this gap?\n\nCOMMENTS:\n- a section with the limitations of the results is missing and should be added;\n- the title has nothing to do with the paper; the paper concerns a sample complexity analysis in the IRL setting, stop. Nothing in the paper gives novel results on whether IRL is harder than RL, so the title must be changed;\n- the proof of the second part of Lemma C.1, although easy, is missing;\n- the proof of Proposition C.2 is missing;\n- in Section 1, Introduction, when listing the contributions, authors state that this work contributes at \"providing an answer to the\nlongstanding non-uniqueness issue in IRL\". This statement is factually false. Indeed, the authors investigate IRL as the reconstruction of the feasible reward set (and this formulation is not introduced in the paper), providing novel analysis for the offline and online (with forward model);\n- the use of O-tilde is incorrect for what concerns the dependence on $\\delta$. Conventionally, O-tilde does not hide dependences on $\\log(1/\\delta)$. This is not just a cosmetic comment, but seems to hide an additional term present in the upper bound and not present in the lower bound, spotting an additional term that is not matched;\n- you use the same symbol $d$ for both the visit distribution and the metric, and maybe you could change one of the two symbols to improve the presentation;\n- the paper contains many typos both in the main paper and in the appendix."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7852/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698858038858,
            "cdate": 1698858038858,
            "tmdate": 1699636962237,
            "mdate": 1699636962237,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VcUxweRrQO",
                "forum": "S24zdyiWDT",
                "replyto": "xpDipdoULS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7852/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7852/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback on our paper. We respond to the specific questions as follows.\n\n> Why do you use $d^\\pi$ (Definition 3.1) as a (pre)metric between reward functions?\n\n* Unlike generative models, in the offline setting, there are unreachable state-action pairs. We cannot expect the recovered rewards to be very close to the ground truth reward for every state-action pair. As a result, in the offline setting, many classical metrics such as $L_{\\infty}$ distance between rewards cannot be employed. Therefore, considering the use of policy and value function to define is a very natural choice.    \n\n* $d^\\pi$ (Definition 3.1) can provide guarantees for performing RL algorithms or doing transfer learning with learned rewards (see Section C.2 and I.3).   \n\n\n\n> ...This is not a metric, and therefore the problem that you highlight in Lemma C.4 for the Hausdorff distance between sets sussists also for your (pre)metric $D^\\pi_{\\Theta}$ (Definition 3.2)...It might be that the proposed (pre)metric $D^\\pi_{\\Theta}$is zero even if the reward sets do not coincide.\n\nWe first clarify that Lemma C.4 (Lemma C.3  in the latest version) is intended to compare the Hausdorff metric and mapping-based metric, rather than metrics between rewards. The reason for \u201cthe proposed (pre)metric $D^\\pi_{\\Theta}$is zero even if the reward sets do not coincide\u201d is due to the selection of metric between rewards: $d^{\\pi}$ (it is easy to see that $d^\\pi(r,r')=0$ does not imply $r=r'$). Lemma C.4 (Lemma C.3  in the latest version) aims to demonstrate that $D^{\\mathsf{M}}$ is strictly stronger than $D^{\\mathsf{H}}$ for some $d$, but lacks a discussion of the mapping-based metric in the first version. Therefore, we have modified Lemma C.4 (Lemma C.3  in the latest version) to include a discussion of both the Hausdorff metric and the mapping-based metric (See Lemma C.3 in in the latest version).\n\n> What is the rationale behind choosing the (pre)metric in Definition 3.1?... It does not seem to me that the (pre)metric in Definition 3.1 guarantees anything on how close the performance of the trained RL agent with the learned reward.In \"Towards Theoretical Understanding of Inverse Reinforcement Learning\" the authors focus on the actual distance between reward functions, not induced value functions. Can the author elaborate?\n\nRegarding the rationale behind choosing the (pre)metric in Definition 3.1 and why we do not use the actual distance between reward functions, please refer to the answer to \u201c Why do you use $d^\\pi$ (Definition 3.1) as a (pre)metric between reward functions?\u201d.\n\nRegarding the lack of guarantees for the performance of the trained RL agent with the learned reward, in fact, we do have guarantees for the performance of the trained RL agent with the learned rewards. We apologize for any confusion; our previous versions did not emphasize this part. In the latest version, we specifically discuss guarantees for performing RL algorithms with learned rewards (see Section C.4 and Corollary I.6, I.7). We appreciate the reviewer for bringing up this question.\n\n> Why do your lower bounds not depend on $1-\\delta$?\n\nWe apologize for any confusion caused. We failed to clarify that our lower bounds also hold for small $\\delta$, such as $\\delta \\leq 1/3$ in the first version.  This error has been corrected in our latest version. We appreciate the reviewer for bringing up this question. We believe that our results hold true when $\\delta$ is very small, and thus, they are highly meaningful.\n\n> How did you manage in the proof of the upper bound the fact that rewards defined as in the ground truth reward mapping are not bounded in $ [\u22121,+1]$  even though the parameters $(V,A)\\in\\mathcal{V}\\times \\mathcal{A}$?\n\nAlthough the reward is not bounded within $[-1, 1]$, we emphasize that $\\mathcal{V}\\times \\mathcal{A}$ is bounded (see the Definition of $\\mathcal{V}\\times \\mathcal{A}$ on page 4). Specifically when $(V,A)\\in \\mathcal{V}\\times \\mathcal{A})$, we have $|V_h|_{\\infty},|A_h|_{\\infty}\\leq H-h+1$, which is sufficient for our proof (see the proof of Theorem 4.2)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7852/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518699106,
                "cdate": 1700518699106,
                "tmdate": 1700526991037,
                "mdate": 1700526991037,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vXeX1hatM8",
                "forum": "S24zdyiWDT",
                "replyto": "xpDipdoULS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7852/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7852/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 2"
                    },
                    "comment": {
                        "value": "> The lower bounds do not match the upper bounds, especially for what concerns the dependence on the horizon H. What is the reason for this gap?\n\nRegarding the lower bound in the online setting, we have made improvements (Theorem G.2), improving the original $\\mathcal{O}(H^2SA\\textrm{min}\\\\{S,A\\\\})$ to $\\mathcal{O}(H^3SA\\textrm{min}\\\\{S,A\\\\})$. Although it does not exactly match the upper bound, it has been significantly tightened. We appreciate the reviewer for bringing up this question.\n\nAs for the lower bound in the offline setting, even though we employed the same MDP construction as in proving the lower bound in the online setting, we did not fully exploit the property of our MDP construction, where transitions are different for each h, because of the construction of the behavior policy. This results in our lower bound having a dependence on $H$ of $\\mathcal{O}(H^2)$. How to prove a tighter lower bound for the $H$-factor would be a highly intriguing avenue for future research.\n\nRegarding the $\\mathrm{min}\\\\{S,A\\\\}$ term, the $\\textrm{min}\\\\{S,A\\\\}$ factor is introduced as follows. In constructing hard instances, there is a step where we need to ensure $\\mathbb{P}_h ( s_i| s_i,a_i)=1$ for $i\\leq \\textrm{min}\\\\{S,A\\\\}$. This particular step is crucial for our proof. Exploring ways to improve the lower bound on $\\textrm{min}\\\\{S, A\\\\}$ could be a promising avenue for future research. We believe our results are still meaningful, and the condition $S < O(A)$ also holds significance.\n\n> \u2018The proof of the second part of Lemma C.1, although easy, is missing;\u2019 and \u2018the proof of Proposition C.2 is missing;\u2019\n\nThank you for pointing these out. We have added the proofs in Section C.5. \n\n> In Section 1, Introduction, when listing the contributions, the authors state that this work contributes at \"providing an answer to the longstanding non-uniqueness issue in IRL\". This statement is factually false. Indeed, the authors investigate IRL as the reconstruction of the feasible reward set (and this formulation is not introduced in the paper), providing novel analysis for the offline and online (with forward model);\n     \nWe appreciate the reviewer's feedback. We want to emphasize that, to the best of our knowledge, we are the first to consider IRL using reward mapping instead of just feasible sets. Additionally, we introduce a mapping-based performance metric to address the no-uniqueness problem. We believe our approach differs from previous work (see Section C).\n\n> the title has nothing to do with the paper; the paper concerns a sample complexity analysis in the IRL setting, stop. Nothing in the paper gives novel results on whether IRL is harder than RL, so the title must be changed;\n\nOur paper's title \u201cIs Inverse Reinforcement Learning Harder than Standard Reinforcement Learning?\u201d aims to convey that we can apply standard RL techniques to address IRL problems, including techniques such as pessimism and some exploring strategies. I apologize for any confusion. In the latest version, we provide the framework for our IRL algorithms in Section E.4. Here, we give a condition (Condition E.4) that is sufficient to do IRL. Then, we illustrate that pessimism can fulfill Condition E.4. We appreciate the reviewer for bringing up this question.\n\n\n> About symbols and typos.\n\nWe appreciate this suggestion. We have addressed the modifications related to symbols and typos as suggested in the latest version."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7852/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518884508,
                "cdate": 1700518884508,
                "tmdate": 1700553445315,
                "mdate": 1700553445315,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uPgLmtLwm9",
                "forum": "S24zdyiWDT",
                "replyto": "vXeX1hatM8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7852/Reviewer_9kcm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7852/Reviewer_9kcm"
                ],
                "content": {
                    "title": {
                        "value": "Re: Response Part 2"
                    },
                    "comment": {
                        "value": "I thank the authors for the detailed answers and for having updated the paper. I have carefully re-read the paper and still I have some concerns that have not been fully solved by the authors.\n\n- **[On the use of $d^{pi}$]** While I fully agree on the impossibility of using the $L_\\infty$-norm of the difference between rewards, I don't really agree on the fact that the expectation under $d^{\\pi}$ is a proper index. Indeed, the authors claim that \" $d^{\\pi}$ (Definition 3.1) can provide guarantees for performing RL algorithms or doing transfer learning with learned rewards (see Section C.2 and I.3).\", but looking at Section C.2, only with $d^{all}$ it is possible to have guarantees when performing RL. It doesn't seem that for the offline setting the authors are able to provide guaranteed w.r.t. $d^{all}$. This makes the argument in favor of $d^{\\pi}$ quite weak in my opinion.\n\n- **[On the dependence on $\\delta$]** I am not satisfied by the authors' answer about the role of $\\delta$ in the bounds. The authors claim that the bounds hold for $\\delta \\le 1/3$. However, there is no presence of $\\delta$ in the sample complexity lower and upper bounds. It is hard to believe that such dependence is really not present since when $\\delta \\rightarrow 0$ clearly the sample complexity must diverge to infinity. One possibility is that the authors have hidden it in the $\\widetilde{O}$. I have already pointed out that the **proper use of $\\widetilde{O}$ should not hide the polynomial dependence on $\\log(1/\\delta)$**. This is indeed the case for the upper bounds reported in the paper. However, looking carefully at the lower bounds proofs, there is no dependence on $\\log(1/\\delta)$ (but only on $1-\\delta$). This spots an important lack of tightness since the lower bound does not diverge to infinity when $\\delta\\rightarrow 0$. **I want to stress that this in combination with an inappropriate use of $\\widetilde{O}$ is severely hiding a suboptimality of the lower bound in comparison with the upper bounds**.\n\n- **[Title]** I remain convinced that the chosen title is inappropriate.\n\n- **[Pessimism]** By re-reading the paper, I realized that the use of pessimism in the reward function has a quite strange effect compared to pessimism in standard off-line RL. Here, pessimism is directly applied to the reward function (eq. 4.4) but it does not seem to have a significant role in the ability to achieve the desired results. From a technical perspective, looking at eq. (E.8), if we do not use pessimism we just obtain the bonus $b^\\theta_h(s,a)$ instead of twice the bonus $2 b^\\theta_h(s,a)$; but the pessimism has no impact on the computation of $C^*$. This is radically different compared to the case of off-line RL where the pessimism is essential to obtain the desired covering wrt to the optimal policy (instead of the undesirable uniform covering over all the policies). I am aware that I am raising this point by the end of the discussion period, but I think that **if the role of pessimism is so marginal in the paper (as I suspect at this point) and if without using it, it is possible to derive the same results, this is an important concern to report. I would really appreciate it if the authors could say whether without pessimism they can obtain (apart from constants) the same results.**\n\nI am reserving the possibility to adjust my score after the discussion with the other reviewers."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7852/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647644635,
                "cdate": 1700647644635,
                "tmdate": 1700647644635,
                "mdate": 1700647644635,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v4oK6x76vi",
                "forum": "S24zdyiWDT",
                "replyto": "xpDipdoULS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7852/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7852/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to additional questions"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the additional questions. We respond to them as follows.\n\n> **[On the use of $d^\\pi$]** \u2026 looking at Section C.2, only with $d^{\\rm all}$ it is possible to have guarantees when performing RL. It doesn't seem that for the offline setting the authors are able to provide guaranteed w.r.t. . This makes the argument in favor of $d^\\pi$  quite weak in my opinion.\n\nWe apologize we gave the **wrong pointer** there, it should be Proposition C.6 in Section C.4. \n\nThere we show that, a small $d^\\pi$ where $\\pi$ is any $\\bar\\epsilon$ near-optimal policy, combined with monotonicity of the recovered reward ($\\hat{r}\\le r$ pointwise), ensures an RL guarantee with $\\hat{r}$. So $d^\\pi$ plus this additional monotonicity condition indeed gives an RL guarantee. Further, our RLP algorithm also guarantees monotonicity (see the blue text in Theorem 4.2, bottom of Page 7) along with the $D^{\\pi}$ guarantee. We believe this justifies the $d^\\pi$ metric as well as our offline results. \n\n\n> **[On the dependence on $\\delta$]** \u2026proper use of $\\tilde{O}(\\cdot)$ should not hide the polynomial dependence on $\\log(1/\\delta)$...  This spots an important lack of tightness since the lower bound does not diverge to infinity when $\\delta\\to 0$. \n\nFor our upper bounds, we were indeed using $\\tilde{O}(\\cdot)$ to suppress $\\log(1/\\delta)$ and the other polylog factors, as we explained in each occurrence of it in the upper bounds.\n\nFor our lower bounds, indeed, the statements are only for $\\delta=1/3$ (which then implies the same statement for all $\\delta\\le 1/3$) for which $\\log(1/\\delta)$ is a constant. We have slightly tweaked the statements to clarify this, but we agree with the reviewer that this is looser than what we want.\n\nUpon initial inspection, we believe it is **possible to improve our lower bounds to have an additional $\\log(1/\\delta)$ factor**, by using the inequality $1-{\\rm TV}\\ge \\frac{1}{2}e^{-\\rm KL}\\ge \\delta$ whenever ${\\rm KL}\\le \\log(1/(2\\delta))$ instead of Pinsker\u2019s inequality, and arguing that $n\\le O(\\log(1/\\delta))$ (times the other factors) will result in ${\\rm KL}\\le \\log(1/(2\\delta))$ (as KL scales linearly in the sample size $n$ due to its additivity). However, we need to go through the full argument more carefully. We will include this in our final version if it works out.\n\n> **[Pessimism]** By re-reading the paper, I realized that the use of pessimism in the reward function has a quite strange effect compared to pessimism in standard off-line RL. Here, pessimism is directly applied to the reward function (eq. 4.4) but it does not seem to have a significant role in the ability to achieve the desired results\u2026 if the role of pessimism is so marginal in the paper (as I suspect at this point) and if without using it, it is possible to derive the same results, this is an important concern to report. I would really appreciate it if the authors could say whether without pessimism they can obtain (apart from constants) the same results.\n\nWe agree that for reward estimation ($d^\\pi$ or $d^{\\rm all}$ guarantee) alone, a simple empirical mean estimator is enough, as the reviewer pointed out. \n\nHowever, **pessimism additionally ensures (entrywise) monotonicity of the learned reward** (cf. blue text in Theorem 4.2). The role of this monotonicity is in **downstream RL** (Proposition C.6,  Thm/Corollaries I.4->I.7), where monotonicity is required for the learned policy (from the recovered reward) to have guarantees. \n\nThis is fully aligned with the use of pessimism in standard offline RL (as per the reviewer\u2019s question), where the monotonicity of the rewards ($\\hat{r}\\le r$ pointwise) is crucial for achieving learning guarantees under single-policy concentrability only instead of all-policy concentrability. For example, in offline bandits, we need to rule out seldom-chosen bad arms (not covered by the optimal policy) that happen to have a large empirical reward by a big negative bonus. \n\nIf the reviewer agrees with this, we then also believe this kind of parallellization to standard offline RL gives **some justification to our title**. As an example, we gave a more **modular argument in Section E.4 on how pessimism implies IRL** ($d^\\pi$ bound + monotonicity) in a black-box fashion. (Looking back, we probably should have structured the main results using this, where we first show pessimism implies IRL black-box by this argument, then reduce to standard pessimistic offline RL algorithms to achieve the pessimism. We will think about whether we can restructure our paper along this direction in its next version).\n\n---\nWe appreciate again the reviewer's prompt response to our rebuttal, and would be happy to answer additional questions before the rebuttal period ends."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7852/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677740959,
                "cdate": 1700677740959,
                "tmdate": 1700678085894,
                "mdate": 1700678085894,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]