[
    {
        "title": "Visual Category Discovery via Linguistic Anchoring"
    },
    {
        "review": {
            "id": "hBijXwhta3",
            "forum": "G7waGZjsdt",
            "replyto": "G7waGZjsdt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1136/Reviewer_9cFD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1136/Reviewer_9cFD"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a simple but effective method for generalized category discovery (GCD). The authors make use of a pretrained vision-language model, i.e., CLIP, and propose to discover the novel classes by searching for the nearest words. They formalize a Vision-and-Language Contrastive Loss to achieve this goal. The experimental results on six widely-used GCD benchmarks demonstrated the superiority of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is clearly written and easy to follow.\n- The proposed method is evaluated to be effective."
                },
                "weaknesses": {
                    "value": "- The proposed method relies on an assumption that the classes (including known and unknown ones) can all be described using single English words, which I think might not always hold true, especially in the context of class discovery. As our goal is to discover unknown novel classes, chances are we encounter something completely new -- which are naturally hard to be verbalized, or even beyond our own human vocabulary. In this regard, the practicability of the proposed method may be limited.\n- The proposed method is potentially similar to [a], yet without no proper discussion.\n- Important baselines are missing, such as [b].\n\n[a] CLIP-GCD: Simple Language Guided Generalized Category Discovery. 2023\n\n[b] Parametric Classification for Generalized Category Discovery: A Baseline Study. ICCV 2023"
                },
                "questions": {
                    "value": "What is the impact of the used text corpus on the final performance? For instance, how does it influence the performance with respect to the topic (coverage), quantity, diversity, etc. of the text corpus in use?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752024705,
            "cdate": 1698752024705,
            "tmdate": 1699636039928,
            "mdate": 1699636039928,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "i93hcEiGUz",
            "forum": "G7waGZjsdt",
            "replyto": "G7waGZjsdt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1136/Reviewer_ixQ1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1136/Reviewer_ixQ1"
            ],
            "content": {
                "summary": {
                    "value": "The study introduces a novel approach to generalized category discovery (GCD), which classifies images from partially labeled datasets without prior knowledge of the total number of classes. This method leverages the interplay between visual and textual data through an image-text foundation model, like CLIP, to recognize new categories across both training and testing phases. Unlike traditional GCD methods that require pre-knowledge of category counts, this technique excels in determining the number of categories independently. It employs language-anchored contrastive learning, using a joint embedding space for images and text, and enforces image-text consistency constraints. The process is facilitated by a unique mechanism that assigns text embeddings to images via nearest-neighbor word retrieval from a diverse corpus and cross-attention aggregation, all without needing manual annotations. This language-anchored approach has demonstrated superior performance on established benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1-The paper presents an innovative use of the CLIP model, utilizing it to aid in the discovery of new categories by retrieving relevant words, a technique that is unique to this study.\n\n2-The experimental design is thorough and transparent, with the inclusion of comparative analysis against other methods using CLIP, ensuring that the evaluations are equitable.\n\n3-The paper thoroughly explores and reports on ablation studies, providing a comprehensive understanding of the method's components and their individual contributions."
                },
                "weaknesses": {
                    "value": "1-The method shows particular dependence on prior exposure of the images to the image-text foundation model, such as CLIP. This is evident as it fails to surpass the state-of-the-art (SoTA) for datasets like Aircraft, which presumably are less represented in CLIP's pre-training, whereas it excels with well-represented datasets such as ImageNet.\n\n2- While the application of the proposed method to generalized category discovery (GCD) is novel, the overall approach is rather straightforward and lacks significant technical innovation. It mirrors techniques already prevalent in zero-shot and multimodal learning.\n\n3-The issue of open vocabulary, which is closely related to the challenges addressed in the paper, is not discussed in the context of related work. This omission overlooks a pertinent area that should be considered for a comprehensive literature review.\n\n4- The presentation of trends in Table 1 could be clearer. \n\nMinor:\nIsn't section 4.3 part of ablations?"
                },
                "questions": {
                    "value": "1- Regarding equation 1, could the model be adapted to incorporate supervised contrastive learning by including text prototypes in relation to class names?\n\n2-In reference to equation 2, what would be the outcome if, instead of aggregating all positive examples for each modality, we evaluated them individually and then combined the losses? Could this approach address the issue where high textual similarity might permit visual embeddings to remain distant from one another?\n\n3-Is it possible to weigh some words more favorably, for instance, the corresponding attributes for each image in the CUB or aircraft datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805763585,
            "cdate": 1698805763585,
            "tmdate": 1699636039859,
            "mdate": 1699636039859,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "ZMHAM9o2oQ",
            "forum": "G7waGZjsdt",
            "replyto": "G7waGZjsdt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1136/Reviewer_4NxT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1136/Reviewer_4NxT"
            ],
            "content": {
                "summary": {
                    "value": "Generalized Category Discovery problem is studied in this paper, with the aim to discover new classes in a dataset and classify the corresponding instances. This paper finds building an intermediate text representation (referred to as \"text anchor\") is able to model semantic similarity between images better than visual features. Such text anchor feature is built by retrieval the k-nearest words in a large word corpus (Google Conceptual Cpations 12M dataset) in CLIP space. With the proposed text anchor feature, a contrastive learning framework is designed to preserve image-image, image-text consensus. Experimental results are promissing."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well written. It is easy to follow.\n2. Resorting to CLIP to search for k most related word from a large corpus for each image is interesting idea."
                },
                "weaknesses": {
                    "value": "1. The main concern of this paper is that the proposed methods and experiments do not fit into the conventional setup of Generalized Category Discovery (GCD), which assumes a finite labeled set. In this context, besides the limited known class labels, one should avoid introducing additional label information from external data or models. For instance, in the original paper by \"Vaze et al., Generalized Category Discovery, CVPR 2022,\" the authors deliberately chose to initialize the image encoder with self-supervised model weights from ImageNet, rather than using the supervised counterpart. This approach ensures that knowledge from ImageNet labels does not leak into their proposed model or framework. However, this paper heavily relies on the CLIP model, and its knowledge, particularly a weak form of external labels, can be easily transferred to the proposed method. Therefore, the experimental setup in this paper differs from the original work in its design.\n\nFurthermore, it is probable that the \"text memory\" employed in this paper already encompasses both the known and unknown class names in GCD. Consequently, this paper is more like performing image tagging first, which is highly likely to have already identified all unknown classes and achieved a precise classification. It harnesses the capabilities of CLIP without necessitating additional learning.\n\n2. The use of different names to refer to the same term is causing some confusion. For example, \"prototype\" and \"anchor\" as well as \"text memory\" and \"work memory.\"\n\n3. Regarding the source for building the \"text memory,\" it is repeatedly mentioned that any \"arbitrary word corpus\" will work. Unfortunately, this claim is unsupported, as no ablation study has been conducted. In fact, the \"Google Conceptual Captions 12M dataset\" is the only source used for this purpose.\n\n4. Some related work is missing. When it comes to searching for k-nearest neighbor-related words to form the text prototype feature, it shares similarities with a line of early work that searches for k-nearest neighbors in the visual space and propagates associated tags to the query image. An example of this work is \"Li et al., Learning social tag relevance by neighbor voting, TMM 2009.\""
                },
                "questions": {
                    "value": "1. Could authors explain what is being memorized by \"text memory\"?  The reviewer's understanding is that it is fixed during the training/test.\n\n2. Does the proposed frame generate for each image a pair of image (visual) prototype feature and a text prototype feature?  Or visual prototypes and text prototypes are a set of learned representatives w.r.t. the entire dataset with fixed number?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817597850,
            "cdate": 1698817597850,
            "tmdate": 1699636039717,
            "mdate": 1699636039717,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "XyZbe64ao5",
            "forum": "G7waGZjsdt",
            "replyto": "G7waGZjsdt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1136/Reviewer_H3av"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1136/Reviewer_H3av"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a language-guided clustering method for generalized category discovery (GCD). The proposed approach pairs each image with a corresponding text embedding by retrieving k-nearest-neighbor words from a text corpus and aggregates them through cross-attention. The paper proposes also to perform clustering without knowing a priori the total number of classes.\n\nThe contributions of this paper are as follows:\n- a language guided image clustering is proposed\n- Clustering images into classes without knowing a priori the total number of classes.\n- The proposed model achieves the state-of-the-art performance on the public GCD benchmark"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed model achieves the state-of-the-art performance on the public GCD benchmark\n\n- extensive evaluation of the model and the different technical decision\n\n- Clustering images into classes without knowing a priori the total number of classes"
                },
                "weaknesses": {
                    "value": "The paper is hard to read and could use some rewriting. \n\n\nSome part of the model are not very intuitive and not well described, for instance:\n\n- How the linkage matrix is used for clustering images without requiring the number of classes?\n\n- if the number of classes is not known, therefore some labels should be missing (otherwise, the number of labels should represent the number of classes). How the model is then able to classify images into these classes and associate a label with them?"
                },
                "questions": {
                    "value": "- How the linkage matrix is used for clustering images without requiring the number of classes?\n\n- if the number of classes is not known, therefore some labels should be missing (otherwise, the number of labels should represent the number of classes). How the model is then able to classify images into these classes and associate a label with them?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1136/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820105154,
            "cdate": 1698820105154,
            "tmdate": 1699636039627,
            "mdate": 1699636039627,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]