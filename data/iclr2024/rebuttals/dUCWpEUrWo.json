[
    {
        "title": "Asynchronous Graph Generators"
    },
    {
        "review": {
            "id": "ErgcVWwbb2",
            "forum": "dUCWpEUrWo",
            "replyto": "dUCWpEUrWo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission933/Reviewer_QMJ3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission933/Reviewer_QMJ3"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the Asynchronous Graph Generator (AGG), a new graph neural network architecture designed for multi-channel time series data. AGG represents observations as nodes within a dynamic graph, enabling it to perform data imputation through transductive node generation without relying on recurrent components or assumptions about temporal regularity. It incorporates measurements, timestamps, and metadata as learnable embeddings within the nodes and employs attention mechanisms to capture expressive relationships among these variables. AGG implicitly learns a causal graph representation of sensor measurements, which can be used to predict new measurements based on unseen timestamps and metadata. The text discusses comparisons with previous work and the positive impact of data augmentation, highlighting AGG's state-of-the-art performance in time series data imputation, classification, and prediction across benchmark datasets like Beijing Air Quality, PhysioNet Challenge 2012, and UCI localisation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper's attempt to address the data imputation problem from the perspective of an asynchronous graph is somewhat novel."
                },
                "weaknesses": {
                    "value": "My main concern is that the proposed AGG architecture appears to be a fusion of various components, including transformers and graph encoders, but the paper lacks a detailed justification for why these specific combinations were chosen.\nThe experimental results presented in the paper appear to be somewhat limited and not entirely convincing. While AGG is shown to outperform existing methods in certain scenarios, a more comprehensive evaluation, including a broader range of datasets and potentially varying levels of data complexity, would bolster the paper's credibility. Additionally, insights into the computational resources required for AGG and any potential scalability challenges should be addressed. A more detailed discussion of the trade-offs between model complexity and performance gains would also be valuable for readers seeking to understand the practical implications of implementing AGG in real-world applications. Overall, expanding the experimental analysis and providing a more nuanced discussion of the results would further strengthen the paper's contributions and impact.\nIn addition, there are opportunities to enhance the paper's presentation. For instance, there could be clearer explanations of the acquisition system's workings, the data preparation process, and the interpretation of colors (e.g., green and yellow) in Figure 2."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission933/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698105424209,
            "cdate": 1698105424209,
            "tmdate": 1699636020079,
            "mdate": 1699636020079,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SblpaOxAn8",
                "forum": "dUCWpEUrWo",
                "replyto": "ErgcVWwbb2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission933/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission933/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QMJ3 (Weaknesses part)"
                    },
                    "comment": {
                        "value": "**The proposed AGG architecture is a fusion of various components, including transformers and graph encoders, lacking justification for their choice.** Justification of the modules chosen by AGG has by no means been arbitrary, we tried to be as clear as possible regarding their choice throughout the paper. We succinctly refer to this here: \n\n- First 4 paragraphs of Sec 3 justify the use of an asynchronous continuous-time dynamic graph\n- Sec 3.1 presents the problem formulation (the input/output variables) and the role of the components chosen\n- Sec 3.2 presents the embeddings used: \n    - The choice of learnable Fourier-based temporal embeddings is justified from the point of view of (Fourier) spectral analysis, meaning that these embeddings will capture oscillatory information in the data\n\t- The high-dimensional (common) metadata and value embedding follow the rationale of using a homogeneous graph instead of a heterogeneous one \n- Secs 3.3 and 3.4 present the self-attention and cross-attention stages. Here, we clarified that the objective is to produce a data representation shaped to address the imputation problem. \n- Additionally, Secs 6 and 7 emphasise that inspiration for the attention stages come from successful transformer models in both language and vision applications\n\n**More experiments** As stated to Reviewer cjtX, we acknowledge that the more experiments the better. However, we have considered exactly the same setup of the SOTA and replicated their experiments in addition to perform an ablation study for data augmentation in Sec 6. _See the list of SOTA works in the response to Reviewer cjtX._\n\n**Computational resources & scalability.** We have described in detail the computational resources used at the beginning of Sec 5 under the title **Infrastructure**. Furthermore, the proposed AGG, which has a quadratic cost depending on the context length (in our case $L=100$), which results from the dot product attention computing the $L \\times L$ weighted graph, it only features 378k learnable parameters and thus can run in most consumer hardware\n\n**Acquisition system and data preparation.** Thanks for this question: AGG is can deal with the data _as is_ and one of its main features is its asynchronicity: there is no need of conditioning or repairing the dataset before use. Conceptual dataset preparation (for training) is presented in detail in Sec 3.1."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission933/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699997425516,
                "cdate": 1699997425516,
                "tmdate": 1699997425516,
                "mdate": 1699997425516,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YC4hXkYLL9",
            "forum": "dUCWpEUrWo",
            "replyto": "dUCWpEUrWo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission933/Reviewer_cjtX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission933/Reviewer_cjtX"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an asynchronous graph generator (AGG) for data imputation, classification, and prediction that can leverage knowledge learned from the observed asynchronous time-stamped data. Specifically, AGG predicts new sensor measurements conditioned on timestamps and metadata by adding new nodes to the learned graph."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. Overall, the main idea of AGG is well-delivered. The problem is well-defined, and the model architectures are clearly presented.\n2. The problem that the paper aims to work on of modeling asynchronous events on graph is fascinating and important."
                },
                "weaknesses": {
                    "value": "1. Although the problem itself is interesting, the contribution of the paper seems to be incremental. For instance, if I understand right, the model can only predict/impute measurement (i.e., $y$). However, an essential question in asynchronous event modeling is how to effectively predict the time/attribute of the next or future events. \n2. The model architecture is not novel. Most of the paper discusses embedding and attention structure, which have been well-studied in previous works. I don't think the contribution \n3. More numerical experiments should be expected to highlight the effectiveness/characteristics/benefits of the model, e.g., simulation or ablation studies. The current results seem to be a black box."
                },
                "questions": {
                    "value": "1. Only data imputation is illustrated in Fig. 2, and based on that, it is a bit hard to imagine the situation of prediction (i.e., new nodes come after known temporal embedding). For example, if $L=4$ in Fig. 2, which batch should nodes 5 and 6 go to?\n2. Is there any real example in the experiments about prediction?\n3. It seems that there is no generative mechanism going on in the model? Please clarify, if possible, the meaning of \"generator\" in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission933/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722739918,
            "cdate": 1698722739918,
            "tmdate": 1699636019933,
            "mdate": 1699636019933,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xgcguP3nxF",
                "forum": "dUCWpEUrWo",
                "replyto": "YC4hXkYLL9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission933/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission933/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cjtX 1/2 (Weaknesses part)"
                    },
                    "comment": {
                        "value": "**Contribution is incremental:** \n\nWe respectfully disagree with this statement. Our work presents a thorough revision of the SOTA on graph NNs for time series and position our contribution very clearly. We summarise our contributions again here: \n\n - The interpretation of time series as a directed graph which is free from assumptions of temporal regularity that typically constrain prior architectures such as RNNs---see **BRITS**: _Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. Brits: Bidirectional recurrent imputation for time series. NeurIPS 31. Curran Associates, Inc., 2018._)\n - The use of this geometric interpretation (inspired by _Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veli\u010dkovi\u0107. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges._) to derive a novel class of architectures that leverages this asynchronous graph through learnt graph attention (see _Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael Bronstein. Temporal graph networks for deep learning on dynamic graphs. arXiv preprint arXiv:2006.10637, 2020._)\n- The application of this graph to multichannel time series, in a computationally efficient manner, by casting the problem from a heterogenous graph to a homogeneous graph via novel channel embedding (referred to as metadata embedding in the paper in Sec. 3)\n- The extension of SOTA (see _Petar Veli\u010dkovi\u0107. Everything is connected: Graph neural networks. Current opinion in structural biology, 79:102538, April 2023. ISSN 0959-440X. doi: 10.1016/j.sbi.2023.102538._) equating transformers to GNNs and applying this to the asynchronous representation via a novel causal temporal masking of dot product self attention\n- A novel methodology of transductive node generation via cross attention using a learnable representation of metadata and temporal embeddings. Time and channel can be chosen arbitrarily to conditionally _generate any_ node in the asynchronous graph allowing a flexible methodology for imputation and prediction (simply choose a time that succeeds the final measurement to cater for prediction)  \n\n**The model can only impute measurement but not predict time/attribute future events.** Perhaps the aim of our work was not clear to the Reviewer: As stated in the first lines of the paper, our work focuses on data imputation and not on predicting when the next measurement will occur. This setup is not unique to our work but shared by all methods considered in the paper and the SOTA against which we compare:\n\n- SSGAN: _Xiaoye Miao, Yangyang Wu, Jun Wang, Yunjun Gao, Xudong Mao, and Jianwei Yin. Generative\nsemi-supervised learning for multivariate time series imputation. In AAAI, volume 35, pp. 8983\u20138991, 2021._ \n- BRITS: _Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. Brits: Bidirectional recurrent imputation for time series. In NeurIPS 31. Curran Associates, Inc., 2018._\n\n\n**Architecture is not novel, embedding and attention have been well-studied in previous works.** We would to respectfully invite the Reviewer to be more open-minded about what a contribution to ML research is. Currently, most neural architectures are based on the attention mechanism, which has enabled clear advantages in many applications. Saying that using attention is not novel is equivalent to saying that any architecture using convolutional layers, skip connections or recurrences are invalid since those components are known. Furthermore, our work is not aimed at *studying attention and embeddings*, but to design and extend specific instantiations of these concepts that are useful to our setup. For instance: i) the temporal (Fourier-based) embedding aims to extract spectral information, ii) the metadata embedding allows flexible (homogeneous) representations of heterogeneous graphs (those with multiple sources), and iii) the attention heads aim to learn the graph dependencies among the nodes values, timestamps and metadata  (see Sec 3). \n\n**More experiments expected such as simulation or ablation studies. The current results seem to be a black box.** We acknowledge that the more experiments the better, but we do not understand what model features need to be clarified via more experiments in the view of the Reviewer, as they were not specified. We have considered exactly the same setup of all the SOTA benchmarks considered (listed above) and replicated their experiments, while also performing an ablation study for data augmentation in Sec 6. Lastly, we are unsure how to take the comment of our model being a _black box_: we have detailed as much as possible the architecture and training procedure. Perhaps our model is a _black box_ as much as any other neural network model."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission933/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699997276248,
                "cdate": 1699997276248,
                "tmdate": 1699997276248,
                "mdate": 1699997276248,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XODIrJkQGE",
                "forum": "dUCWpEUrWo",
                "replyto": "YC4hXkYLL9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission933/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission933/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cjtX 2/2 (Questions part)"
                    },
                    "comment": {
                        "value": "**How to perform prediction.** The AGG performs prediction/imputation as transductive node generation; this is achieved by conditioning the cross-attention block on an arbitrary selection (user-defined) of the metadata and time. The selections chosen by the user are encoded via the learned embedding representation $(t2v(\\tau), embed(m_t))$ (eq 8) and then used in the cross attention (Sec 3.4) AGG block along with the embeddings of the input graph to compute (predict) the value of the new node in the asynchronous graph. Furthermore, as stated in 3rd paragraph of Sec 3 in the paper: _\"...If the encoding places the new node within the temporal \u201cneighbourhood\u201d of the other nodes in the graph, we refer to data imputation, whereas if the new node comes after the known temporal encodings we refer to prediction\"_. Explicitly, if a time value is chosen that _succeeds_ the inputs, we perform prediction. Following the question of the Reviewer, we have included a prediction example on the Beijing dataset in Appendix A.4, emphasising that this required no modification of the existing architecture. \n\nRegarding the Reviewers question about context length in Figure 2, extending the context length to $L=4$ would use nodes $0,1,3,4$ in the input. Also, generating node 2 would be considered _imputation_, while generating nodes 5 and 6 would be considered _prediction_. Alternatively, nodes 7,8,10,11 could be used to generate both nodes 5 and 6, in which case we would refer to _imputation_. Note that the flexible representation of the conditional generation allows this type of arbitrary generation of nodes (which represent new samples). Note the context length of $L=3$ is simply demonstrative, the context length is a hyperparameter and is typically chosen based on the computational resources available ($L=100$ was used in our experiments). \n\n**Real world examples.** All our data come from the real datasets Beijing Air Quality, PhysioNet Challenge 2012, and UCI localisation. The reason we considered those datasets is because all the SOTA benchmarks considered are tested on them and thus we can directly report objective quantitative comparisons of AGG. See:\n\n\n- SSGAN: _Xiaoye Miao, Yangyang Wu, Jun Wang, Yunjun Gao, Xudong Mao, and Jianwei Yin. Generative\nsemi-supervised learning for multivariate time series imputation. In AAAI, volume 35, pp. 8983\u20138991, 2021._ \n- BRITS: _Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. Brits: Bidirectional recurrent imputation for time series. In NeurIPS 31. Curran Associates, Inc., 2018._\n\n**Clarification of generative property.** Thank you for this question. The generative feature of the AGG refers to the way it performs transductive node generation (which is used for imputation in this specific case) via conditioning the cross attention block; we detail this in an earlier response to \"how to perform prediction\". More details regarding the generation can be found in Sec 3.2 (for embedding representation). See Figure 3 for a pictorial representation of how these embeddings are used to condition the generation, and Sec 3.4 for the algorithmic implementation of conditional cross attention. Also, we refer the Reviewer to Figure 1b), and c) for a conceptual pictographic demonstration of the node generation, where Figure 1b) demonstrates how the time series data is encoded into a graph which is then use to generate a new node in the graph ($h_6$, missing data). The image denotes new node in red implying it was from the \"red\" channel and at time 4, these would be encoded via the learned embedding representation of metadata and time2vec respectively and then used in the cross attention block (eq 8) along with the input embedding to generate the new node $h_6$ in this image."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission933/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699997374856,
                "cdate": 1699997374856,
                "tmdate": 1699997374856,
                "mdate": 1699997374856,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EWeTA4A9FD",
            "forum": "dUCWpEUrWo",
            "replyto": "dUCWpEUrWo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission933/Reviewer_qi5c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission933/Reviewer_qi5c"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a graph-based approach for temporal data imputation using cross multi-head attention. The model assumes input of time-stamped observations with metadata and value of interest for imputation and prediction. Imputation requires target timestamps and metadata (in addition to the graph context of the input data including values) to predict the output value for the targets. They demonstrate superior performance when compared to baselines, particularly those based on RNNs and assuming discrete-time observations. The main advantage of their model in the demonstrated experiments seem due to the application of multi-head attention to construct a graph representation of the time series."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors clearly outline their modeling approach, referencing the relevant literature for embedding of temporal data, metadata, and the prediction values. The transformer encoding is intuitive and powerful, naturally inducing a continuous-time graph representation of the time series.\n2. The flexibility of the approach is demonstrated methodologically in Section 4 and empirically in Section 6. The advantage of an attention-based inter-observation influence mechanism is clearly outlined as compared to RNN-type methods. The authors connect to the vision literature when investigating the data augmentation in Section 6, which is insightful."
                },
                "weaknesses": {
                    "value": "1. As the key advantage of this work (when compared to the current baselines) is the use of attention, I think it is problematic that the authors have not compared extensively to attention-based temporal data imputation techniques. Particularly, [1] seems to be a very similar model and work, as they also evaluate on the same datasets. See their Table 1; AGG outperforms theirs in 10% and 50% missing data for PhysioNet, but they claim superior performance for 90%. The Beijing Air Quality data seem to be scaled differently, but you should also compare to their result.\n\nIn summary, a comparison should be made empirically and the differences in the methodology should be highlighted to other attention-based works for data imputation in temporal data.\n\n\n2. The ability to impute continuous-time data is emphasized early in the paper, but then is not exploited in the experiments. While it is clear that this method outperforms discrete-time methods, I think a comparison to continuous-time imputation techniques would be insightful. For example, it seems GP-VAE does not use RNNs, and may also not require time discretization. You could hypothetically decode the Gaussian process to arbitrarily-time outputs, and compare more directly to your approach. Similarly, while there is not extensive literature on point process models for data imputation, they represent another class of continuous-time model which could be used to impute missing temporal data. For example, [2] introduces the PILES model for such a task. See also [3] Section 5.4.\n\nIn summary, I think an emphasis on the ability of your method to address continuous-time data imputation could address some limitations and similarities between other attention-based approaches. Additional experiments could highlight the true novelty of this approach, which I believe to be an attention-based method for **continuous-time** data imputation.\n\n\n3. It seems a strong assumption that the time and (especially) metadata of unknown targets are known - can you demonstrate the ability to predict the metadata, at least? Regardless, I think it is a bit of a limitation that the timestamp of the imputed observation must be provided.\n\n\n4. A minor comment: perhaps outlining the benefits/properties of each baseline would be helpful.\n\n\n\n[1] Y\u0131ld\u0131z, A. Yark\u0131n, Emirhan Ko\u00e7, and Aykut Ko\u00e7. \"Multivariate time series imputation with transformers.\" IEEE Signal Processing Letters 29 (2022): 2517-2521.\n\n[2] Chen, Jiadong, et al. \"An Adaptive Data-Driven Imputation Model for Incomplete Event Series.\" International Conference on Advanced Data Mining and Applications. Cham: Springer Nature Switzerland, 2023.\n\n[3] Shchur, Oleksandr, Marin Bilo\u0161, and Stephan G\u00fcnnemann. \"Intensity-free learning of temporal point processes.\" arXiv preprint arXiv:1909.12127 (2019)."
                },
                "questions": {
                    "value": "As above:\n\n1. Please comment on the difference between your approach and other attention-based approaches for temporal data imputation, and compare to these apparently strong baselines.\n\n2. Consider continuous-time models, such as a small variation of the GP-VAE baseline and perhaps a point process type of imputation model.\n\n3. Demonstrate that you can also predict metadata given only the timestamp of a new observation.\n\nSmall questions/comments:\n\n4. Outline the purpose and details of the baselines.\n\n5. Line above Eq (3) \u201cAGG is a heterogeneous graph\u201d but it is actually homogeneous?\n\n6. Does the MLP hidden layer need to have $l \\times d_{\\rm encode}$ nodes, or is this an arbitrary choice?\n\n7. After cross multi-head attention in Figure 3, a residual connection is shown between the output of CrossMultiHead and $h_l$. However, Eq (10) shows the residual connection between the output of CrossMultiHead and $g_0$.\n\n8. In implementation details \u201cthe input layer of dimension 5 \\times 16\u201d where does the 5 come from in this case?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission933/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission933/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission933/Reviewer_qi5c"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission933/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700148531327,
            "cdate": 1700148531327,
            "tmdate": 1700148531327,
            "mdate": 1700148531327,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NpTK7Zpvqj",
                "forum": "dUCWpEUrWo",
                "replyto": "EWeTA4A9FD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission933/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission933/Authors"
                ],
                "content": {
                    "title": {
                        "value": "AGG as a heterogeneous graph and metadata"
                    },
                    "comment": {
                        "value": "From the reviewers comments we can see that we failed to explain how exactly the metadata is obtained and why it is valid assumption that this is known for the prediction step, this is also related to why we refer to the graph as heterogeneous and by including the metadata as an embedding we can cast the problem to a heterogeneous one. We believe a comprehensive example will help to clarify this point. The Beijing data set, for example consists of 12 weather stations seperated geographically:\n$$ station \\in \\\\{ Aotizhongxin,Changping,Dingling, Dongsi, Guanyuan,Gucheng, \\ldots, Wanshouxigong \\\\} $$\n\nand for each of these stations they provided different sensor measurements (from different types of sensors often simultaneously, although often their values are missing):\n$$ sensor \\in \\\\{\n    PM2.5,\n    PM10,\n    SO2,\n    NO2,\n    CO,\n    O3,\n    TEMP,\n    PRES,\n    DEWP,\n    RAIN,\n    WSPM\n\\\\} $$\n There is also additional categorical metadata for each station, this is the $WindDirection$ which is the cardinal directions \n $$ WindDirection \\in \\\\{NNW, E, NW, WNW, N, ENE,\n    NNE,\n    W,\n    NE,\n    SSW,\n    ESE,\n    SE,\n    S,\n    SSE,\n    SW,\n    WSW,\n    None\n\\\\}\n$$ The features $y$ are the measurements of any $sensor$ at any location $station$. One way to express the interconnection between physical locations and sensors is to use a heterogeneous graph, where the types of nodes are the different $station$ and $sensor$ combinations and the edges are the measurements $y$. \n\nThe $WindDirection$ is a categorical feature of each node and is used to extend the information of the features through a categorical embedding. The metadata is known for all stations and sensors (as it is implicit characteristics of each node), but the measurements $y$ are not known for all sensors at all stations (as they have temporal, spatial, type dependence) and are the values we want to predict, but we need to know which $station$ and $sensor$ in order to do so, which makes sense intuitively as at time $t$ how could the model know for which $sensor$ and at which $station$ we wish to predict its value, logically the value of $PM2.5$ will vary across physical locations that are separated by many kms and many different sensor give values at the same time, thus it makes sense to _condition_ the generation step. \n\nFinally, the $timestamp$ is an independent value that is simply the time of the measurement relative to the start of the batch, there is no restriction in its value thus we can select this arbitrarily, and it is then encoded through a learnt `time2vec` embedding module (to capture frequency dynamics). Of course, we choose time values for which we have measurements so that we can train the model but generation is not bound by such restrictions.\n\nWe hope this example clarifies the assumptions we make and why we refer to the graph as heterogeneous. The problem of modelling a heterogeneous graph is that we would require a different set of weights for each node type (we induces data sparsity problems), which is why we embed the node metadata and instead treat it as a homogeneous graph where the sensors type and the geographical location are encoded in the node embedding and concatenated along with the measurement $y$ (this alows the model to leverage redundancies between different node types under a common set of graphs or \"attention heads\"). This is also why \u201cthe input layer of dimension $5 \\times 16$\u201d, the 5 is the number of separate embedding types the feature, station, type, time and categorical (wind direction in this case) data and the 16 is the dimension of the resulting embedding. To ease the notational burden (perhaps to the detriment of clarity) we referred to station, type and categorical data collectively as \"metadata\".\n\nThe generation step simply consists on selecting of which \"metadata\" we want to condition on in order to predict the $y$ value, and the model will output the generated value, there is no rational for predicting the metadata as it is simply which part of the \"heterogeneous graph\" we want to generate."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission933/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700569792946,
                "cdate": 1700569792946,
                "tmdate": 1700569792946,
                "mdate": 1700569792946,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ve6Ob3wNBe",
                "forum": "dUCWpEUrWo",
                "replyto": "EWeTA4A9FD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission933/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission933/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Weaknesses and Questions"
                    },
                    "comment": {
                        "value": "### Weaknesses \n1. We thank the reviewer for connecting our work with [1], as we were not aware of this work. We will endeavor added a comparison with [1] in the revised manuscript. Your suggestion of including a more focused comparison with attention-based methods is also well taken.\n2. We thank the reviewer for the suggestion of comparing with continuous-time imputation techniques and agree that this is a rather unexplored area of research. We will endeavor to include a comparison with GP-VAE and PILES in the revised manuscript. We also agree that the ability to impute continuous-time data is a key advantage of our method and will endeavor to highlight this in the revised manuscript, we appreciate the insight of the reviewer on this point.\n3. Regarding the assumption that the time and metadata of unknown targets are known, perhaps we were not clear enough in the manuscript. We have provided a detailed example of how the metadata is obtained and why it is valid assumption that this is known for the prediction step in the previous section. The time of the unknown targets is simply the time of the prediction, which is arbitrary and can be selected by the user. We will endeavor to clarify this in the revised manuscript.\n\n### Questions\n1. Thank you for your suggestion we agree that a more comprehensive comparison is needed and will endeavor to include this in the revised manuscript.\n2. We agree that the power of the model is in its ability to impute continuous-time data and will endeavor to highlight this in the revised manuscript.\n3. Please refer to the previous section for a detailed example of how the metadata is obtained and why it is valid assumption that this is known for the prediction step. It makes no sense to predict the metadata as it is an implicit part of the \"heterogeneous graph\" we want to generate.\n4. We will endeavor to clarify the purpose and details of the baselines in the revised manuscript.\n5. Please refer to our response detailing the heterogeneous graph assumptions and why they are valid, we will endeavor to clarify this in the revised manuscript but we maintain that this is in fact a heterogeneous graph problem to which we cast to a homogeneous graph problem by embedding the metadata.\n6. The MLP hidden layer does not need to have $l\\times d_{\\text{encode}}$ nodes, this is an arbitrary choice, we simply followed the prior work of \"Attention is all you need\" amoung others that expanding the hidden layer by a factor of $l$ is a good choice in order to improve the compuational capacity of the model.\n7. Thank you for pointing this out, yes the figure is incorrect and the residual connection should be between the output of CrossMultiHead and $g_0$ as shown in Eq (10). We will endeavor to correct this in the revised manuscript.\n8. Please refer to our detailed heterogeneous graph example for a detailed clarification. Briefly the input layer of dimension $5 \\times 16$ is the number of separate embedding types the feature, station, type, time and categorical (wind direction in this case) data and the 16 is the dimension of the resulting embedding."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission933/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570084091,
                "cdate": 1700570084091,
                "tmdate": 1700570084091,
                "mdate": 1700570084091,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dBR2gDI0nI",
                "forum": "dUCWpEUrWo",
                "replyto": "ve6Ob3wNBe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission933/Reviewer_qi5c"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission933/Reviewer_qi5c"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the thorough response and a more clear description of the metadata. I now understand why, under this framework, prediction of metadata is not relevant.\n\nHowever, regarding a comparison to continuous-time data imputation techniques, I would say that predicting metadata may be more important. For example, one could predict all three: (1) the time of the missing observation, (2) the node on the graph at which the observation would occur, and (3) the value of the observation. I understand that this is a very different setting than the one you are considering now, but it might be an applicable area which better demonstrates the novelty of your approach, assuming you would outperform other continuous-time imputation techniques in this setting as well.\n\nThank you again for the response, but I will keep my score as it is."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission933/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656253648,
                "cdate": 1700656253648,
                "tmdate": 1700656253648,
                "mdate": 1700656253648,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8cVqzJfJVL",
            "forum": "dUCWpEUrWo",
            "replyto": "dUCWpEUrWo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission933/Reviewer_mCDq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission933/Reviewer_mCDq"
            ],
            "content": {
                "summary": {
                    "value": "The work proposes a graph neural network architecture for multi-channel time-series imputation, upon leveraging embeddings and attention mechanisms. The proposed method reaches satisfactory performance on various real-data examples against baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed AGG architecture is novel and useful for the imputation, classification, and regression tasks.\n- The proposed method is thoroughly explained in Section 3."
                },
                "weaknesses": {
                    "value": "1. Literature review: Section 2 seems to ignore more recent developments on time-series imputation in the field. See a couple mentioned in this survey paper (https://arxiv.org/abs/2307.03759) and more below. In particular, the development in the field since 2022 is barely discussed.\n\n2. Problem setup: \n\n- Section 3.1 introduces the problem formulation, which however is unclear to the reader. For example, is the imputation task focusing on predicting $y$? This seems to be the case as shown in Eq (12). If so, how does this differ from a standard prediction task? \n- As the authors claim \"no previous GNN-based method approaches the imputation problem\nfrom the perspective of an asynchronous graph\", it is important to separate alone a section explaining the formal mathematical setup of the problem, which at least contains (1) the imputation problem (2) how this is asynchronous (3) why the problem is challenging/unique that others have not proposed ways to solve it. The current Section 3.1 is highly insufficient.\n\n3. Experiments (existing results):\n- I find it strange to say \"a common AGG architecture was implemented without hyper-parameter tuning for all datasets\". Does this mean your method can always work without any tuning, even for learning rate/batch size, etc.? If not, it would be important to say clearly the implication behind this.\n- Related to the first point, how does your method perform under various hyper-parameters, if they are actually tuned? Would it be significantly improve over current results?\n- How does your model capacity compare to those of the baselines? Your model has 378k trainable parameters. How about others? What architecture of theirs is adopted in the comparison?\n- The appendix should contain a table highlighting the data specifics (e.g., number of observations, number of time-series, feature dimension, etc.), as it is hard to infer these values from looking at Appendix A.1. I would suggest the authors to list these numbers in accordance with notations in previous sections. Similar thing can be done when explaining the AGG architecture.\n\n4. Experiments (new ones currently lacking):\n- The most recent baseline is SSGAN (Miao et al., 2021). However, many works have followed theirs; a quick search reveals [1-5] for the imputation task, while I believe more are existing. I thus find it unrealistic that the SSGAN is still the \"state-of-the-art\" method after two years. \n- How does the method perform on other time-series datasets? The current experiments closely follow SSGAN, but it would be important to examine beyond that setting established more than 2 years ago.\n\n(Incomplete) list of related papers\n- [1] Miao et al., 2021: Efficient and effective data imputation with influence functions\n- [2] Cini et al., 2022: Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks\n- [3] Alcaraz et al., 2023: Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models\n- [4] Liu et al., 2023: PriSTI: A Conditional Diffusion Framework for Spatiotemporal Imputation\n- [5] Wu et al., 2023: Jointly Imputing Multi-View Data with Optimal Transport"
                },
                "questions": {
                    "value": "Questions are summarized in the weakness section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission933/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission933/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission933/Reviewer_mCDq"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission933/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700158771229,
            "cdate": 1700158771229,
            "tmdate": 1700158874904,
            "mdate": 1700158874904,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "N5ZrzQNnwD",
                "forum": "dUCWpEUrWo",
                "replyto": "8cVqzJfJVL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission933/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission933/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewers insights into our manuscript, we will certainly take this into account for the next iteration of the article. We thank you for taking the time to produce a thoughtful response to our work and providing valuable references some of which we had not considered."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission933/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570328279,
                "cdate": 1700570328279,
                "tmdate": 1700570328279,
                "mdate": 1700570328279,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YfC91K44MS",
            "forum": "dUCWpEUrWo",
            "replyto": "dUCWpEUrWo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission933/Reviewer_TnYt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission933/Reviewer_TnYt"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the challenge of analyzing multi-channel time series data, particularly focusing on issues like irregular time intervals and complex spatial-temporal relationships. It proposes a novel approach with the Asynchronous Graph Generator (AGG), a graph neural network architecture that models time series observations as nodes on a dynamic graph, facilitating data imputation and prediction. AGG's unique feature is its ability to directly embed measurements, timestamps, and metadata into nodes, using attention mechanisms to discern intricate relationships among variables \u2014 which can hardly convince the reviewer \u2014 and as claimed in the paper, this method stands out from existing models by bypassing the limitations of recurrent neural networks and conventional time series models that often assume temporal regularity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The reviewer can hardly say that they can understand the paper's method. But the idea that representing a multivariate time series as nodes in a graph is truly interesting. Even though the reviewer is not familiar with the baselines in this field, the experimental results seem relatively comprehensive and convincing."
                },
                "weaknesses": {
                    "value": "The reviewer can only offer some general suggestions:\n\n1. The reviewer believes a good research paper should be educative to general audience; they did look at Cao's work on RNN for time series imputation and find their problem set-up and  proposed approach easy to understand. Unfortunately, the current form of this paper makes it really difficult for readers without certain background knowledge to understand the setting and the contribution.\n\n2. Given that this paper is purely empirical, the numerical experiments are the most important part to verify the performance. Table 1 may benefit from including uncertainty quantification (in the meanwhile, the reviewer acknowledges that the improvement is quite significant).\n\n3. Terminology should be used more carefully: the term \"causal\" is used multiple times (and perhaps that is the reason why the reviewer gets invited to review this paper); However, it seems that \"causal\" merely refers to temporal order, which is \"Granger causal\" and means correlation from past to the future. The reviewer suggests using simple terms like temporal order directly.\n\n4. Scaling might be one major issue when the dimension is high and the time horizon is long (since there must be a really huge graph to represent the multivariate time series)."
                },
                "questions": {
                    "value": "There are two major concerns that make the reviewer lean towards rejection of this work:\n\n1. In Fig. 1 (c), the proposed method used future to predict past. The reviewer is fine with using the expressive power of neural networks to learn a latent causal representation, but a structure shown in Fig. 1 (c) seems to imply that the latent data generation mechanism depends on the future which is clearly wrong. Can the authors justify that?\n\n(PS: in granger causal literature, one famous example is that \"buying Christmas tree\" Granger causes \"Christmas\". However, this example cannot justify the proposed structure since there should be a latent variable \"knowing Christmas will be on 12/25\" captured by the latent data generating process).\n\n2. Another reason why the reviewer gets invited to reviewer this paper might be the use of Physionet dataset \u2014 the are a lot of lab tests in that dataset where missing values themselves mean that the clinician is not suspect of related dysfunction/disease at all. That is why there are so many missing values in that dataset \u2014  a single patient cannot be suspected to have all diseases \u2014 and the missingness carries meaning. Can authors justify on why imputing this dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission933/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission933/Reviewer_TnYt",
                        "ICLR.cc/2024/Conference/Submission933/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission933/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700190779596,
            "cdate": 1700190779596,
            "tmdate": 1700191042940,
            "mdate": 1700191042940,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nQF9O48uHr",
                "forum": "dUCWpEUrWo",
                "replyto": "YfC91K44MS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission933/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission933/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer taking the time to review our work and providing a valuable general perspective, we will endeavour to include the reviewers suggestions in our next iteration of the manuscript \n## Weaknesses\n1.  We will endeavour to make the next iteration of this manuscript more accessible to the general audience.\n2. Thank you for the insight we agree uncertainty quantification would improve the imputation section, for the post imputation classification and regression we performed a 5 fold cross validation which indeed quantifies the uncertainty so some degree.\n3.  We appreciate the connection to \"Granger\" causality, we had not considered this definition, and the reviewer is correct the \"causal\" refers to the \"temporal order\", although the underlying design is to capture (learn through graph attention) the phase dynamics between different measurements i.e. that the measurement from sensor $y_k$ at time $t-2$ is related to sensor $y_l$ at time $t$ by some hidden dynamics (which are related by some phase shift in the signals) of which we wish to learn through graph attention.\n4. We will endeavour to explore the limitations of architecture wrt to high dimensionality and longer time horizons, thank you for the suggestion.\n## Questions\n1. Figure 1b) shows the temporal ordering of the masking of the graph, which respects the ordering of the signals, this input graph is then encoded along with the signals through the AGG which results on a set of encoded signals $h$ which a used to either impute or predict any other node. In the case of figure 1c) this is demonstrating imputation (filling in missing values) which indeed uses the future value to fill in the missing sensor measurements. This is justified as the purpose is to use all available information to infer the missing value. In the prediction case this indeed would be a violation but would not occur as the generated node would be after the encoded inputs in order to be prediction. We appreciate the reviewers question.\n2. Simply this dataset has been a benchmark for imputation for many state of the art papers in the past, including all the papers to which we compare our results, most notably SSGAN by Miao et al., so it seemed natural to apply our paper to this benchmark in order to show fair comparisons. That being said the imputation could have many benefits for this type of scenario, it may be informative to estimate for example the blood pressure of a patient at any point of time when examining the patients history in the ICU that eventually led to their death, this is obviously infeasible that these measurements were taken constantly and impossible to retroactively know the ground truth of this and other indicators. With this type of model under the imputation set up we have shown we can reasonably estimate these unknown values for various levels of sparsity"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission933/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572367653,
                "cdate": 1700572367653,
                "tmdate": 1700572367653,
                "mdate": 1700572367653,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PUgqJpCyDl",
                "forum": "dUCWpEUrWo",
                "replyto": "nQF9O48uHr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission933/Reviewer_TnYt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission933/Reviewer_TnYt"
                ],
                "content": {
                    "comment": {
                        "value": "The reviewer has read the response and decided to keep the score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission933/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631338491,
                "cdate": 1700631338491,
                "tmdate": 1700631338491,
                "mdate": 1700631338491,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]