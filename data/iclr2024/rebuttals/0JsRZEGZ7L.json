[
    {
        "title": "From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module"
    },
    {
        "review": {
            "id": "7ylg8ZyQ99",
            "forum": "0JsRZEGZ7L",
            "replyto": "0JsRZEGZ7L",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5956/Reviewer_EucF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5956/Reviewer_EucF"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a novel Latent Topology Inference (LTI) method that enables the learning of non-regular topologies based on their novel Differentiable Cell Complex Module (DCM). DCM is designed to compute cell probabilities within the complex, thus enhancing downstream tasks. They show how to integrate DCM with cell complex message-passing network layers and train it in an end-to-end fashion, offering significant improvements, especially in cases where an input graph is not provided. The paper demonstrates the effectiveness of the proposed approach on both homophilic and heterophilic graph datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The novel $\\alpha$-DGM demonstrates remarkable efficacy in characterizing the 1-skeleton of the latent cell complex.\n- The paper substantiates its claims with extensive experimental results, both in the main body and the appendix, providing a thorough evaluation of the proposed method.\n- The \"Limitations\" section is thoughtfully composed, addressing potential constraints and challenges of the approach."
                },
                "weaknesses": {
                    "value": "- Section 3.1 initially introduces the $\\alpha$-DGM; however, the subsequent description within this section appears to be inconsistent with the concept of $\\alpha$-DGM.\n- Section 3 would benefit from a reorganization to enhance clarity and coherence. Its current form significantly hinders the understanding of the proposed method. Please reorganize it for readers to follow the description.\n- Notably, there is no dedicated \"Reproducibility Statement\" section in the paper, which hinders providing clear instructions for reproducing the results. The inclusion of such a section would enhance the paper's accessibility and reproducibility."
                },
                "questions": {
                    "value": "- Same as I stated in the section of weakness, on of my major concerns it the organization of Section 3. More efforts are needed in this part. The following few questions might help the authors to refine the section.\n- In the paragraph above 'Remark 2', the layer normalization $\\mathcal{LN}$ is employed. If I understand correctly, $\\mathcal{LN}$ actually works on vectors of similarities. Thus is it really precise to call it as layer normalization?\n- In the same paragraph, the threshold of assigning an edge is set as $0$. Is there any ablation study on the selection of this threshold?\n- In the same paragraph, what is 'parameter $\\alpha$'? Is it mentioned before?\n- In Eqn. (5), how to get $\\mathcal{B}(\\sigma_i^1)$?\n- Similarly, in Eqn. (9),  how to get $\\mathcal{CB}(\\sigma_i^0)$?\n- As shown in Table 2, the performance of DCM is higher without an initial graph than with an initial graph. What could be the possible reason behind this observation?\n- How to modify the proposed method to make it work for directed complexes?\n\nI will raise my score if my concerns are correctly addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5956/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5956/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5956/Reviewer_EucF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5956/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697731841280,
            "cdate": 1697731841280,
            "tmdate": 1700496294457,
            "mdate": 1700496294457,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nFLjfNuVT9",
                "forum": "0JsRZEGZ7L",
                "replyto": "7ylg8ZyQ99",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5956/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5956/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Weaknesses\n\n1. In our paper, DGM refers to the original Differentiable Graph Module of Kazi et al. (2022). Section 3.1 introduces $\\alpha$-DGM, which is our prosed variation of DGM that exploits the entmax function instead of Gumbel-Softmax to sample a potentially irregular graph topology. $\\alpha$-DGM is the variant we use in our Differentiable Cell Module (DCM), as shown e.g. in Figure 3. This is also reflected in the experiments, where we use DGM only as a baseline model to refer to the original variant of Kazi et al. (2022). We clarified this point in the revised version of the paper. \n\n2. We modified the Section based on the reviewers\u2019 suggestions (please also see our other answers). The organization is as follows: we provide a general overview of the method (Fig. 2), before describing each component in turn, (i) the graph sampling procedure (Section 3.1), the lifting procedure (Section 3.2), our novel polygon sampling method (Section 3.3, Fig. 3), and the final downlifting procedure (Section 3.4). Reviewer XizW suggested this to be the most natural exposition, however, we would be glad to do additional rewriting or reorganization if the reviewer has further suggestions.\n\n3. We added a reproducibility statement immediately after the references and before Appendix A. Please note that we provide a link to an anonymous repository with code allowing to reproduce our experiments.\n\n## Questions\n\n1. See Weaknesses 1-2.\n\n2. At the graph level, this operation is applied to a matrix containing row-wise the probability vectors $\\mathbf{p}_i$. Each row is normalized based on its empirically computed mean and variance. Hence, this is equivalent to LN as generically used for sequences or sets in transformers. This is also matched in the code (see our previous answer), where we applied the default LN implementation from PyTorch.\n\n3. We set it to $0$ because a similar effect to modifying the threshold can be obtained in the $\\alpha$-DGM module by varying the $\\alpha$ parameter, which is optimized by gradient descent and does not require manual fine-tuning (which may be time-consuming and dataset-dependent). Nevertheless, we tested two additional values for the threshold (0.1 and 0.01) on a subset of the datasets obtaining similar results, as shown in the following Table.  \n\n|                   |          | Th = 0   | Th = 0.01 | Th = 0.1  |\n|-------------------|----------|----------|-----------|-----------|\n| **Citeseer**      | w graph  | 78.72%   | 78.88%    | 78.54%    |\n|                   | w/o graph| 76.47%   | 76.16%    | 76.32%    |\n| **Cora**          | w graph  | 85.78%   | 86.33%    | 85.41%    |\n|                   | w/o graph| 78.80%   | 78.77%    | 78.47%    |\n| **Texas**         | w graph  | 84.87%   | 87.13%    | 83.60%    |\n|                   | w/o graph| 85.71%   | 85.48%    | 84.65%    |\n| **Wisconsin**     | w graph  | 86.33%   | 86.10%    | 86.11%    |\n|                   | w/o graph| 87.49%   | 85.95%    | 87.10%    |\n\n\n4. We describe the meaning and the usage of $\\alpha$ below Eq. (5). In the revised paper, we made it clear that the Tsallis $\\alpha$-entropy is formally a class of functions parametrized by a parameter $\\alpha \\geq 1$.\n\n5. The boundary of an edge $i$ is simply made by its endpoints node $j$ and $v$. We clarified it below Eq. (5) in the revised paper.\n\n6. The coboundary of a node $i$ is simply made by all the edges for which node $i$ is an endpoint. We clarified it below Eq. (9) in the revised paper.\n\n7. Table 2 is about heterophilic datasets. We expect the DCM to achieve better performance when the graph is not provided. This is because, in the heterophilic setting, the input graph and data structure are in contradiction with the homophily assumption on which most of the well-known Graph and Topological NNs hinge, i.e. connected nodes have similar labels. Indeed, our method achieves top performance without an input graph for the heterophilic datasets and with an input graph for the homophilic datasets, as one could expect. However, even observing the results across both homophilic and heterophilic datasets when the input graph is provided, we can appreciate how DCM exploits the available \"good'' graphs in the homophilic case while being less sensitive to the \"wrong'' ones in the heterophilic case.\n\n8. Implementing Latent Topology Inference for directed complexes is one of the future directions we are definitely most interested in. However, it is not trivial, because fundamental literature about how to theoretically characterize directed regular cell complexes is missing. Few works analyzed directed simplicial complexes (very loosely speaking, a particular case of cell complexes), however, they could be pretty limited as objects to leverage in the context of Latent Topology Inference."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5956/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258710787,
                "cdate": 1700258710787,
                "tmdate": 1700258710787,
                "mdate": 1700258710787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lmbtAbO5rc",
                "forum": "0JsRZEGZ7L",
                "replyto": "nFLjfNuVT9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5956/Reviewer_EucF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5956/Reviewer_EucF"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for your rebuttal and the revisions made to the paper. Your responses have effectively addressed my concerns, leading me to consider a higher review score.\n\nCheers,\n\nReviewer EucF"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5956/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496277963,
                "cdate": 1700496277963,
                "tmdate": 1700496277963,
                "mdate": 1700496277963,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "t2NhhNjcX2",
            "forum": "0JsRZEGZ7L",
            "replyto": "0JsRZEGZ7L",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5956/Reviewer_XizW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5956/Reviewer_XizW"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to learn \"latent topology\" in ML tasks, extending Latent Graph Inference. The idea is to learn a cell complex during training. Their method extends the Differentiable Graph Module (DGM) (Kazi 2022) to Diff. Cell Module (DCM). They also use $\\alpha$-maxent instead of gumball softmax. As I understand it, the procedure involves: \n1. Get auxiliary node features $x_{0,aux}= \\nu (x_{0,in})$ from inputs $x_{0,in}$, (via GNN, if a graph is given, or MLP otherwise).\n2. Use $\\alpha$-maxent to infer a graph (1-simplex). \n3. Build higher order cells (e.g. faces, volumes etc) using message passing (MP) on the inferred lower-order cells (e.g. edges) \n4. Do MP for inference using cell complex conv nets (CCCN). \n\nIn the paper, they mostly only discuss up to 2-simplexes, with the Polygon Inference Module (PIM), and not higher simplexes.  \n\nThey conduct a set of experiments on the usual graph benchmarks and show superior performance against a few baselines, including DGM. Most notably, they point out that most graph based methods (if the graph is similarity based) do not perform well on heterophilic datasets. They find that their method (using 2-simplexes) usually outperforms other methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of inferring cell complexes is a nice and natural extension of learning graphs. \n2. Although the number of higher-order cells can grow and quickly become intractable, they seem to use methods that makes this inference manageable (Appendix B and sec. 3.3). \n3. It outperforms others on heterophilic datasets (Table 2)\n4. Extensive tests and ablation studies.  \n5. Well-written with detailed appendix and discussion of limitations"
                },
                "weaknesses": {
                    "value": "1. In PIM, eq (6) restricts the types of polygons that can be inferred (see questions)\n2. Limiting PIM to small polygons may fail to capture long-range dependencies\n3. When a graph is given, the experiments are inconclusive. I appreciate reporting the negative results. But a discussion of the failure cases would be beneficial."
                },
                "questions": {
                    "value": "1. How efficient is PIM in practice? How does the training time compare with DGM or other baselines? I know you discuss the time complexity in App B, but I'm curious about the wall-clock time. \n2. For eq (6), is there a reasoning saying higher order correlations would be weaker, e.g. randomness of $x_{1,int}$? In principle, you could also consider any contraction, including products of all three $ x(i),x(j),x(v)$. \n3. Table 1, with graph, in about half of the cases GCN outperforms all higher order methods. Any intuition on why? When should we expect higher topology to matter?   \n4. Is the \"sim\" function cosine similarity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5956/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786631633,
            "cdate": 1698786631633,
            "tmdate": 1699636635405,
            "mdate": 1699636635405,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BBoTKTO8kS",
                "forum": "0JsRZEGZ7L",
                "replyto": "t2NhhNjcX2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5956/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5956/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Weaknesses\n\n1. (Includes also the reply to Question 2.) Eq. (6) can be used for polygons of arbitrary size $k$. We initially presented it for the triangle case ($k=3$) just to provide a simple example. We made the choice to decompose the \u201c$k$-th order\u201d similarity as a sum of pairwise similarities to enhance the scalability of our method (trivially, a sum can be parallelized). However, it can be arbitrarily designed, as long as it is a similarity measure, i.e. it is higher if the involved edge features are similar. We rewrote Eq. (6) in its general form for a polygon of arbitrary size $k$ and included the above considerations in the revised paper.\n\n2. The reviewer is right, indeed we conducted extensive ablation studies on the maximum inferrable polygon size $K_{max}$ (Tables 8 and 9  in the Appendix of the revised paper). We performed experiments using values for $K_{max} > 5$ but did not observe improvements. We also observed that very few times the inferred 1-skeletons presented cycles of lengths greater than 4 or 5. Although it has been shown that MP at the polygon level mitigates the GNNs\u2019 limitation in handling long-range interactions (see e.g. Bodnar et al., Giusti et al.), we agree that future research on Latent Topology Inference could explicitly focus on this issue, given the promising results of this work.\n\n 3. It is true that, on the homophilic datasets, the results of the DCM with input graph are slightly worse than the other well-known baselines. We argue this is an expected behavior, because those models inherently rely on the homophily assumption. Our results are also consistent with previous literature on LGI (e.g. DGM). On the other hand, DCM consistently shows superior performance in the heterophilic settings when the graph is provided, whereas the performance of the other \u201chomophily-based\u201d methods tends to drop. When the input graph is not provided, the performance of the DCM is consistently superior across all the tested datasets. Finally, please note that DCM is a general framework allowing to employ other more complex GNNs and TopologicalNNs as backbones at the node and edge levels instead of the simpler GCN and CCCN we used in our paper for the sake of simplicity. This feature represents an exciting future direction. \n\n## Questions\n\n1. In Table 3 of Appendix B of the revised paper (reported also in the reply to Reviewer ims6), we show a breakdown of the inference execution times on two reference datasets, Cora and Texas, for GCN, GCN+CCCN, DGM, and DCM,  based on the (macro-)operations they require. As we can see from this empirical analysis and as we could expect from the complexity analysis we report in Appendix B, the lifting operation (computing the cycles and lifting the node embeddings) and cell sampling (computing the similarities among edge embeddings and applying the $\\alpha$-entmax) are the main computational bottlenecks. The lifting operation needs to be performed in every complex-based architecture (we show DCM and GCN+CCCN, but for CWN would be the same); architectures that do not perform LTI (CWN or GCN+CCCN) can mitigate this problem (only) on transductive tasks by computing the cycles offline. Additional solutions w.r.t. the ones presented in point (b) of Appendix B could be accumulating the gradients and inferring the graph/cell-complexes once every $t$ iteration rather than after every optimization step. The cell sampling bottleneck, unlike the lifting, is not given by any technical requirement, but it is just related to our actual implementation. To keep the code readable and reproducible for (mainly) academic purposes, we use basic data structures and functions, e.g. we store the cells in lists that we parse. However, the bottleneck could be mitigated by optimizing the code. We are currently working on this and we will soon update our repo, hopefully before the end of the rebuttal period. \n\n2. Please see Weakness 1.\n\n3. About the \"why\", as explained in detail above,  we expect the results of DCM to be comparable to the other baselines on the homophilic datasets. About the \u201cwhen\u201d, several works tried to give a technical answer. To name a few,  some works showed that working on these spaces enhances the expressivity and the handling of long-range interactions (Bodnar et al., Giusti et al.), while some other works leveraged a Signal Processing perspective (Barbarossa et al., Yang et al., Calmon et al.) to prove that working on these spaces provides more versatile and sophisticated frequency-based tools. In general, as it often happens with DL, there is no unique answer. As long as multiway (group, high order, beyond pairwise) interactions play a crucial role in the adopted data, then higher topology will significantly help.\n\n4. The \u201csim\u201d function is any (pseudo-)similarity measure. Cosine similarity could be employed as well. In our experiments, we use minus the square distance among the embeddings. We clarified this in the revised paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5956/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258668492,
                "cdate": 1700258668492,
                "tmdate": 1700258668492,
                "mdate": 1700258668492,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3Wr6UGxJX7",
                "forum": "0JsRZEGZ7L",
                "replyto": "BBoTKTO8kS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5956/Reviewer_XizW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5956/Reviewer_XizW"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the revisions and explanation. The run times in Table 3 are very good. The paragraph on complexity is also helpful. And thanks for providing the code. I have no further questions at this point."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5956/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581482587,
                "cdate": 1700581482587,
                "tmdate": 1700581482587,
                "mdate": 1700581482587,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "m5nqCSO4Ui",
            "forum": "0JsRZEGZ7L",
            "replyto": "0JsRZEGZ7L",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5956/Reviewer_ims6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5956/Reviewer_ims6"
            ],
            "content": {
                "summary": {
                    "value": "This paper deals with the learning of high-order cell complexes with sparse and not regular graph topology.\n\nThe key is to introduce a learnable function that computes cell probabilities in the complex and integrate with cell complex message-passing network layers in a scalable way. The model achieved improve test accuracy in both homophilic and heterophilic graph node classification benchmarks. However, the improvements seem incremental."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The key is to introduce a learnable function that computes cell probabilities in the complex and integrate with cell complex message-passing network layers in a scalable way. The model achieved improve test accuracy in both homophilic and heterophilic graph node classification benchmarks."
                },
                "weaknesses": {
                    "value": "In empirical studies, the improvements on accuracy seem incremental."
                },
                "questions": {
                    "value": "What is the time efficiency of DCM, comparing with the baselines?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5956/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699116800246,
            "cdate": 1699116800246,
            "tmdate": 1699636635303,
            "mdate": 1699636635303,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZR3JvtkFzP",
                "forum": "0JsRZEGZ7L",
                "replyto": "m5nqCSO4Ui",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5956/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5956/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Weaknesses\n\n 1. It is true that, on the homophilic datasets, the results of the DCM with provided graph are slightly worse than the presented well-known graph-based and complex-based architectures. We argue this is an expected behavior, because those architectures inherently rely on the homophily assumption, i.e. they employ diffusion/message-passing schemes that work better when connected nodes have similar labels. Our results are also consistent with previous literature on latent graph inference (e.g. DGM). On the other hand, DCM consistently shows superior performance in the heterophilic settings when the graph is provided, whereas the performance of the other \u201chomophily-based\u201d methods tends to drop. When the input graph is not provided, the performance of the DCM is consistently superior across all the tested datasets. To conclude, we want to stress that one should not consider DCM only as a novel effective GNN architecture. The main novelty of DCM is in it being (to our knowledge) the first model capable of Latent Topology Inference, i.e. learning of higher-order latent interactions modeled as combinatorial topological spaces (regular cell complexes, in our case). DCM is able to overcome many of the Latent Graph Inference's (and Topological Deep Learning's) limitations and achieve SOTA or near-SOTA results across multiple datasets. In addition, please note that DCM is a general framework allowing to employ other more complex GraphNN and TopologicalNN architectures as backbones at the node and edge levels instead of the simpler GCN and CCCN we used in our paper for the sake of simplicity (Remark 5). This feature represents an exciting direction for future work. \n\n## Questions\n\n1. In Table 3 of Appendix B of the revised paper, reported below, we show a breakdown of the inference execution times (in seconds) on two reference datasets, Cora and Texas, for GCN, GCN+CCCN, DGM, and DCM,  based on the (macro-)operations they require. As we can see from this empirical analysis and as we could expect from the complexity analysis we report in Appendix B, the lifting operation (computing the cycles and lifting the node embeddings) and cell sampling (computing the similarities among edge embeddings and applying the $\\alpha$-entmax) are the main computational bottlenecks. The lifting operation needs to be performed in every complex-based architecture (we show DCM and GCN+CCCN, but for CWN would be the same); architectures that do not perform LTI (CWN or GCN+CCCN) can mitigate this problem (only) on transductive tasks by computing the cycles offline. Additional solutions w.r.t. the ones presented in point (b) of Appendix B could be accumulating the gradients and inferring the graph/cell-complexes once every $t$ iteration rather than after every optimization step. The cell sampling bottleneck, unlike the lifting, is not given by any technical requirement, but it is just related to our actual implementation. To keep the code readable and reproducible for (mainly) academic purposes, we use basic data structures and functions, e.g. we store the cells in lists that we parse. However, the bottleneck could be mitigated by optimizing the code. We are currently working on this and we will soon update our repo, hopefully before the end of the rebuttal period. \n\n|                       |          | Graph MP | Graph Sampling | Lifting | Cell Sampling | Cell MP |\n|-----------------------|----------|----------|----------------|---------|---------------|---------|\n| **Cora**              | GCN      | 5e^-5    | -              | -       | -             | -       |\n|                       | GCN+CCCN | -        | -              | 2e^-1   | -             | 2e^-4   |\n|                       | DGM      | 6e^-4    | 4e^-3          | -       | -             | -       |\n|                       | DCM      | 1e^-5    | 5e^-3          | 1e^-1   | 2e^-2         | 5e^-4   |\n| **Texas**             | GCN      | 2e^-5    | -              | -       | -             | -       |\n|                       | GCN+CCCN | -        | -              | 1e^-2   | -             | 2e^-4   |\n|                       | DGM      | 3e^-5    | 9e^-4          | -       | -             | -       |\n|                       | DCM      | 1e^-5    | 9e^-4          | 3e^-2   | 7e^-2         | 4e^-5   |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5956/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258651979,
                "cdate": 1700258651979,
                "tmdate": 1700258651979,
                "mdate": 1700258651979,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]