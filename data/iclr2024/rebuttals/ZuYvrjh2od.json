[
    {
        "title": "ReForm-Eval: Evaluating Large Vision Language Models via Unified Re-Formulation of Task-Oriented Benchmarks"
    },
    {
        "review": {
            "id": "CJDtHBIVpk",
            "forum": "ZuYvrjh2od",
            "replyto": "ZuYvrjh2od",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3580/Reviewer_J51n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3580/Reviewer_J51n"
            ],
            "content": {
                "summary": {
                    "value": "This paper contributes a new benchmark for evaluating the large vision-language models (LVLMs) comprehensively. The benchmark re-formulates 61 benchmark datasets based on existing data resources and evaluate the models with both black-box and white-box methods. The authors also conduct extensive experiments to analyze the strengths and weaknesses of existing LVLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The scale of benchmark is large, including 61 datasets and is 100 times the size of MMBench.\n2. The evaluation dimensions are comprehensive, including many perception and cognition sub-tasks.\n3. Some insightful conclusions are required from the experiments on proposed benchmark."
                },
                "weaknesses": {
                    "value": "Limitations in novelty to some extent: the formulation of multiple-choice is widely used in previous work like MMBench [1]; the proposed generation and likelihood evaluation method are also used in previous benchmarks, like VisualGPTScore [2] proposed using likelihood of generating references conditioned on images and prompts to do multiple-choices tasks. \n\n[1] Liu, Yuan, et al. \"MMBench: Is Your Multi-modal Model an All-around Player?.\" arXiv preprint arXiv:2307.06281 (2023).\n[2] Lin, Zhiqiu, et al. \"VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores.\" arXiv preprint arXiv:2306.01879 (2023)."
                },
                "questions": {
                    "value": "1. Are the formulation of current datasets used in the benchmark modified manually?\n2. Should we use generation metric, likelihood metric, or both of them when using the benchmark? \n3. Are there any human validation or other validations to show the superiority over other benchmarks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Considering that this benchmark contains 61 existing datasets, are there any copyright issues involved in re-formulating them?"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3580/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3580/Reviewer_J51n",
                        "ICLR.cc/2024/Conference/Submission3580/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3580/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698251741466,
            "cdate": 1698251741466,
            "tmdate": 1700622698020,
            "mdate": 1700622698020,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rEtcEA2rnk",
                "forum": "ZuYvrjh2od",
                "replyto": "CJDtHBIVpk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review of Submission3580 by Reviewer J51n (Part 1 of 2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable advice. Here is our response to your concerns and questions:\n\n**Weakness 1: Limitation in novelty. The format of multiple-choice questions and the likelihood-based evaluation method have been explored.**\n\nResponse:  **The core contribution of ReForm-Eval is a strategy for re-formulating existing task-oriented datasets to suit the evaluation of LVLMs**. The reason for choosing multiple-choice questions is that this format is in line with the evaluation needs of LVLMs to output in free-form texts. At the same time, based on the findings that current LVLMs have weak capability in following multiple-choice instructions, we further assist the evaluation through both black-box (text-only in-context samples) and white-box (likelihood) methods. Thus, **ReForm-Eval exhibits a versatile and efficient way to evaluate LVLMs by combining these individual components in a reasonable way.** \n\n------------\n\n**Q1: Are the formulation of current datasets used in the benchmark modified manually?**\n\nResponse:  In ReForm-Eval, we propose a re-formulation framework for existing benchmark datasets. For each dataset, we conduct thorough evaluation and analysis, based on which the target problem format of re-formulation is determined. Subsequently, the data is transformed with the corresponding **automatic pipeline** **where no human annotation is required**.\n\n-----------\n\n**Q2: Should we use the generation metric, likelihood metric, or both of them when using the benchmark?**\n\nResponse:  If possible, **we recommend using both approaches for comparison.** The **black-box strategy is more versatile** but requires higher adherence to instructions by models; some models (like BLIVA) require the **white-box method to better reflect their multimodal capabilities**, and it offers higher stability and computational efficiency. At the same time, comparing the performance differences between the two strategies can also reveal the instruction-following abilities between models."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202325246,
                "cdate": 1700202325246,
                "tmdate": 1700202325246,
                "mdate": 1700202325246,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y4yeYSy0Gj",
                "forum": "ZuYvrjh2od",
                "replyto": "CJDtHBIVpk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review of Submission3580 by Reviewer J51n (Part 2 of 2)"
                    },
                    "comment": {
                        "value": "**Q3: Are there any human validation or other validations to show the superiority over other benchmarks?**\n\nResponse:  \n\nThanks for your advice. **We compare ReForm-Eval with other LVLM benchmarks from five perspectives: (1) Dataset size; (2) Annotation; (3) Evaluation; (4) Instability and the measurement; (5) Scalability.** As demonstrated in Rebuttal Table 7, ReForm-Eval has a large scale and wide coverage of data; it requires no manual efforts during the annotation stage and has strong scalability; based on the unified evaluation format, it does not require external services to assist the evaluation, making it simple and easy to use; it considers the instability of models to prompts and provides quantitative measurements, making it comprehensive and reliable. **We believe that ReForm-Eval has clear superiority over previous benchmarks.** \n\nRebuttal Table 7: Comparison between benchmarks for LVLMs.\n\n| Benchmark   | Size  | Annotation   |              | Scalability | Evaluation   |                  | Instability     |                 |                  | Instability Measure  |\n| ----------- | ----- | ------------ | ------------ | ----------- | ------------ | ---------------- | --------------- | --------------- | ---------------- | ------------ |\n|             |       | **Human**    | **ChatGPT**  |             | **ChatGPT**  | **Unified Form** | **Instruction** | **Option Mark** | **Option Order** |  |\n| LAMM        | 186k  |              | $\\checkmark$ | high        | $\\checkmark$ |                  |                 |                 |                  | None         |\n| MME         | 2.4k  | $\\checkmark$ |              | low         |              | $\\checkmark$     |                 |                 |                  | None         |\n| LVLM-ehub   | 1243k |              |              | high        | $\\checkmark$ |                  | $\\checkmark$    |                 |                  | None         |\n| MMBench     | 3.0k  | $\\checkmark$ |              | low         | $\\checkmark$ | $\\checkmark$     |                 |                 | $\\checkmark$     | $\\Delta acc$ |\n| ReForm-Eval | 521k  |              | $\\checkmark$ | high        |              | $\\checkmark$     | $\\checkmark$    | $\\checkmark$    | $\\checkmark$     | entropy      |\n\n\nFurthermore, we show that **an obvious advantage of Reform-Eval is its large volume of data, providing a more stable estimation of model capabilities.** We perform bootstrap sampling on the data and measure the stability in assessing model capabilities by calculating the variance of the bootstrap estimator. As presented in Rebuttal Table 8, ReForm-Eval provides a more stable evaluation.\n\nRebuttal Table 8: Variance of bootstrap estimators on VQAv2 dataset from ReForm-Eval and on fine-grained perception (single-instance) dataset from MMBench. For each, we repeatedly draw 20% of the data with replacement for 10 times. \n| **Benchmark**   | **BLIP-2** | **LLaVA_L2** | **mPLUG-Owl** | **PandaGPT** | **ImageBindLLM** | **mmGPT** | **Shikra** | **Cheetor_V** |\n| --------------- | ---------- | ------------ | ------------- | ------------ | ---------------- | --------- | ---------- | ------------- |\n| **MMBench**     | 6.7        | 7.1          | 3.28          | 8.08         | 4.79             | 8.72      | 14.25      | 5.94          |\n| **Reform-Eval** | 0.3303     | 0.2316       | 0.0658        | 0.1403       | 0.3683           | 0.1587    | 0.2341     | 0.292         |\n\n-----------\n\n**Q4: Will there be copyright issues?**\n\nResponse: \n\nDuring the construction of ReForm-Eval benchmark, **we only use the open-source datasets. The usage of these data for academic purposes is permitted.** The purpose of ReForm-Eval is to assist in the academic research of LVLMs, so we believe there should be no related issues. Furthermore, we will supplement the copyright requirements of the corresponding datasets in the usage instructions of ReForm-Eval."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202453658,
                "cdate": 1700202453658,
                "tmdate": 1700202673427,
                "mdate": 1700202673427,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bVxVt0iBKj",
                "forum": "ZuYvrjh2od",
                "replyto": "Y4yeYSy0Gj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3580/Reviewer_J51n"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3580/Reviewer_J51n"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank the authors for response. The response has addressed my questions to some extent. After consideration, I decide to raise the score to 6."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622669283,
                "cdate": 1700622669283,
                "tmdate": 1700622669283,
                "mdate": 1700622669283,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QoKvMajAVt",
            "forum": "ZuYvrjh2od",
            "replyto": "ZuYvrjh2od",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3580/Reviewer_3n83"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3580/Reviewer_3n83"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new strategy to benchmark large vision language models.\nIt reformulates 61 existing benchmarks into multiple-choice problems or specialized text generation problems,\ntest existing LVLMs with the ReForm-Eval, and report the accuracy and CIDEr for two types of problems, respectively. \nWith ReForm-Eval, the authors benchmarked multiple LVLMs and studied the effect of model backbone, connection module, pre-training data, instruction-tuning data. \nFurthermore, the paper also discussed the effect of in-context sample, the difference between generation and likelihood based evaluation, and the instability during evaluation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper provides a practical approach to unify existing computer vision benchmarks under a unified formulation. It converts 61 existing datasets to multi-choice problems and specialized text generation problems and provides extensive evaluation results."
                },
                "weaknesses": {
                    "value": "1. Some findings presented in this paper are not original findings. For example, the effect of in-context examples and the instability of existing LVLMs have already been discussed in MMBench. \n2. The core contribution of this work is to propose an approach to convert existing benchmarks to a unified formulation. However, the authors used many pages to present and discuss the evaluation results, rather than delving deeper into the reformulation methodologies. In fact, many aspects can be explored during the reformulation:\n    1. In general, one need to use some distractors as the negative options when building multi-choice problems. There exists multiple ways to obtain these distractors (as mentioned in this paper): 1. find the negative classes with the highest confidence; 2. find some semantically related but not synonymous answers; 3. LLM-based hard-negative generation. Besides, another baseline is to randomly pick incorrect class labels as negative options. How can the use of those distractors quantitatively affect the evaluation results?\n    2. Fine-grained recognition, which is a substantial component of Fine-grained perception,  is not included in the fine-grained perception tasks."
                },
                "questions": {
                    "value": "1. For Figure 10, why the proportions of 'A' and 'B' in Ground-truth Option Distribution are not the same? Does that mean there exists questions with only one option?\n2. Typo in Page.8 Line 1, MSOCO\n3. It would be better if the authors can provide more ablation study for the reformulation process, on factors including: 1. the methods to add the distractors, 2. the number of options. \n4. Reform-Eval is a large dataset contains over 500,000 evaluation instances, when doing the sub-sampling, do the authors do it uniformly or sample evaluation instances from each benchmark with different probability to improves the data balance? Besides, have the authors studied if the shrink of the dataset size will change the evaluation results? Can we use a even smaller subset for evaluation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3580/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3580/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3580/Reviewer_3n83"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3580/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698483739913,
            "cdate": 1698483739913,
            "tmdate": 1699636313222,
            "mdate": 1699636313222,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HI07rfhQl1",
                "forum": "ZuYvrjh2od",
                "replyto": "QoKvMajAVt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review of Submission3580 by Reviewer 3n83 (Part 1 of 2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable advice. Here is our response to your concerns:\n\n**Weakness 1: Some findings presented in this paper are not original findings, namely the effect of in-context samples and the instability of LVLMs.**\n\nResponse: \n\n1. In-context samples have been widely used during the model training stage to enhance the instruction following ability of models, i.e. MIMIC-IT dataset used in Otter. **To the best of our knowledge,** **ReForm-Eval is the first attempt to utilize in-context samples to** **assist the** **evaluation of LVLMs****.** In this paper, **our novel findings is that** complete image-text pairs are not necessary as in-context samples; **text-only in-context samples can provide effective guidance**, helping LVLMs to answer multiple-choice questions in the correct format.\n2. The analysis of instability in MMBench is limited to the impact of option order perturbations. **In ReForm-Eval, we comprehensively consider various perturbations****:** not only shuffling option order, but also applying different instructions and option markers. Furthermore, we **quantitatively measure the instability of models in the form of entropy**. Above are the original contributions of ReForm-Eval in terms of instability. \n\n-------------\n\n**Weakness 2.1: Lacking analysis and experiments on the re-formulation process, such as the quantity and sources of distractors.**\n\nResponse: Please refer to our response to Q3 in the common response.\n\n-------------\n\n**Weakness 2.2: Fine-grained recognition tasks are not included in ReForm-Eval.**\n\nResponse: \n\nWe believe this is a misunderstanding due to the different interpretations of term \"fine-grained\". **In ReForm-Eval, fine-grained perception refers to the perception of semantic information at the level of local objects.** On the other hand, fine-grained image classification tasks (**also known as fine-grained recognition**) focus on distinguishing fine-grained categories, which involve perception at the image level. Therefore, in ReForm-Eval, **such tasks are categorized as image classification tasks in coarse-grained perception**, such as Flowers102 and Pets37."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700201466014,
                "cdate": 1700201466014,
                "tmdate": 1700201466014,
                "mdate": 1700201466014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eg7kkytSp9",
                "forum": "ZuYvrjh2od",
                "replyto": "QoKvMajAVt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review of Submission3580 by Reviewer 3n83 (Part 2 of 2)"
                    },
                    "comment": {
                        "value": "Here is our response to your questions:\n\n**Q1: For Figure 10, why the proportions of 'A' and 'B' in Ground-truth Option Distribution are not the same?**\n\nResponse:  In this dataset, there are no questions with only one option. The distribution of correct answers has a certain level of randomness due to the shuffle, so the proportions of A and B are not exactly the same.\n\n---------\n\n**Q2: Typo in Page.8 Line 1**\n\nResponse: Thank you for pointing out the error. We have corrected this typo.\n\n---------\n\n**Q3: It would be better if the authors could provide more ablation studies for the reformulation process**\n\nResponse: Please refer to our response to Q3 in the common response.\n\n---------\n\n**Q4: Explanation and analysis of the sampling method.**\n\nResponse: \n\n- **Detailed explanation of our sampling method:** Since the Reform-Eval covers 61 benchmarks and contains more than 500,000 evaluation samples, we employ ***a balanced sampling strategy*** to enhance the robustness of the evaluation. Within each benchmark, we sample 10% data based on the distribution of the original data **except for three cases**: (1) when the size of the original benchmark is less than 1000, we keep the whole benchmark for a stable evaluation; (2) when the original benchmark has more than 10,000 evaluation samples, we filter the data and then conduct the sampling process; (3) for benchmarks used in Multi-turn Dialogue dimension, we retain all evaluation samples as only two datasets are included and the total sample volume is moderate in this dimension (~3000).\n\n- **Why we sample at a rate** **of** **10%**: We analyze whether the shrink or expansion of the dataset size will change the evaluation results, by conducting several experiments on the VQAv2 benchmark and Flowers102 benchmark (the evaluation sample size is 21441 and 818, respectively). Rebuttal Table 5 demonstrates that the more data sampled, the more consistent they are with the evaluation on the complete dataset. **A 10% sampling ratio can achieve a good balance between the evaluation efficiency and consistency.** \n\n- **Rationality of our sampling method**: As shown in Rebuttal Table 6, for larger datasets, the sampling ratio has less impact on the results; for smaller datasets, different sampling ratios cause the results to fluctuate  dramatically. **To ensure the stability of the evaluation, we generally perform balanced sampling for larger datasets and retain the entire dataset for smaller datasets.** \n\nRebuttal Table 5: Evaluation results under different sampling ratios on the VQAv2 and Flowers102 benchmark. We derive the results of different models on test sub-benchmarks under each sampling ratio, and calculate the correlation coefficient $\\rho$ and average absolute deviation $\\bar{d}$ of these results compared with the results on the complete test benchmark.\n\n| **Dataset**    | **Metric**  | **Generation** |        |         |         |          | **Likelihood** |         |         |         |          |\n| -------------- | ----------- | -------------- | ------ | ------- | ------- | -------- | -------------- | ------- | ------- | ------- | -------- |\n|                |             | **1%**         | **2%** | **10%** | **20%** | **100%** | **1%**         | **2%**  | **10%** | **20%** | **100%** |\n| **VQAv2**      | $\\rho$    | 0.9861         | 0.9948 | 0.9989  | 0.9996  | 1        | 0.9689         | 0.9857  | 0.9970  | 0.9991  | 1        |\n|                | $\\bar{d}$ | 3.1483         | 1.7075 | 0.5550  | 0.3658  | 0        | 2.6450         | 2.1167  | 0.8725  | 0.4958  | 0        |\n| **Flowers102** | $\\rho$    | 0.9575         | 0.9559 | 0.9794  | 0.9336  | 1        | 0.7984         | 0.7861  | 0.9131  | 0.9727  | 1        |\n|                | $\\bar{d}$ | 8.5738         | 9.0256 | 4.3775  | 1.6994  | 0        | 12.1756        | 10.6850 | 3.2488  | 2.9275  | 0        |\n\nRebuttal Table 6: Variance of accuracy(%) under different sampling ratios. We repeatedly sample a certain percentage of the data for 10 times. We derive the accuracy each time and then compute the variance for each model. The final variance value is averaged across the models.\n\n| **Dataset**    | **Size** | **Generation** |        |         |         |          | **Likelihood** |        |         |         |          |\n| -------------- | -------- | -------------- | ------ | ------- | ------- | -------- | -------------- | ------ | ------- | ------- | -------- |\n|                |          | **1%**         | **2%** | **10%** | **20%** | **100%** | **1%**         | **2%** | **10%** | **20%** | **100%** |\n| **VQAv2**      | 21441    | 5.22           | 2.90   | 0.44    | 0.19    | 0        | 8.63           | 5.06   | 0.81    | 0.26    | 0        |\n| **Flowers102** | 818      | 169.79         | 85.60  | 18.83   | 6.62    | 0        | 243.01         | 174.59 | 17.79   | 10.66   | 0        |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700201585389,
                "cdate": 1700201585389,
                "tmdate": 1700205294116,
                "mdate": 1700205294116,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HYNm4W64VC",
            "forum": "ZuYvrjh2od",
            "replyto": "ZuYvrjh2od",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3580/Reviewer_mogP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3580/Reviewer_mogP"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a novel benchmark called ReForm-Eval for assessing large vision-language models. The underlying approach of ReForm-Eval involves transforming several publicly available VQA datasets into a multiple-choice format."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This work proposes a novel benchmark, namely ReForm-Eval. Reformatting the current VQA dataset into multiple-choice questions partially alleviates a problem in using the current metric in the VQA dataset to evaluate generative VLMs. The problem is the exact matching between the prediction and the reference target, which leads to potential limitations.\n2. Some insights are good, such as \"FlanT5-based models\" performing well on the multiple-choice tasks, which aligns with findings in the NLP domain."
                },
                "weaknesses": {
                    "value": "1. The very impressive ability lies in the large language models (GPT-4, ChatGPT, Llama, etc.) and the popular vision-language model (CLIP) is their powerful zero-shot learning ability. One important way to evaluate the zero-shot learning of models is to ensure there is no dataset overlap between the evaluating data and the training data, such as the evaluating strategy in CLIP[1]. However, ReForm-Eval includes many datasets that are trained in the evaluated VLM. This might incur two issues: (1) it is unfair to compare models that were trained by datasets evaluated in ReForm-Eval with models that have not been trained on any datasets in ReForm-Eval. (2) Ultimately, ReForm-Eval can only evaluate the \"supervised learning\" ability instead of the \"zero-shot learning.\"\n\n2. Some insights might not be solid. For example, (1) when discussing which connection module is more suitable for which visual backbone, this work should ensure that other influential factors are the same between compared models, such as training data and language model. (2) The grouping of high-quality data and without high-quality data might be cherry-picked, as some models (Lynx) in the high-quality group also use \"data filtered on rules or CLIP.\" (3) The variance in Fig. 4 (c) is so large that it is difficult to conclude that \"more instructional data leads to better performance.\"\n\n3. Using CIDEr to evaluate visual descriptions is not optimal, especially for models that intend not to generate concise descriptions, such as LLaVA, and it benefits models that are tuned by dataset, such as coco-caption, whose ground truth descriptions are in a shorter format.\n\n[1] Radford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" ICML 2021."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3580/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634422896,
            "cdate": 1698634422896,
            "tmdate": 1699636313002,
            "mdate": 1699636313002,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gpVYq7Gpy9",
                "forum": "ZuYvrjh2od",
                "replyto": "HYNm4W64VC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review of Submission3580 by Reviewer mogP"
                    },
                    "comment": {
                        "value": "Thanks for your valuable advice. Here is our response to your concerns:\n\n**Q1: Some models might have been trained on datasets included in ReForm-Eval, potentially leading to unfair comparisons and challenges in measuring zero-shot capabilities of the models.**\n\nResponse: Please refer to our response to Q1 in the common response.\n\n--------\n\n**Q2: Questions about the soundness of insights gleaned in this paper. (1) Issues regarding controlling variables; (2) Criteria for grouping models in Figure 4 (b); (3) Excessive variance in the curve fitted in Figure 4 (c).**\n\nResponse: Please refer to our response to Q2 in the common response. \n\n--------\n\n**Q3: CIDEr may not be suitable as a metric for visual description tasks.**\n\nResponse: \n\nWe chose CIDEr as an automated evaluation metric following BLIP-2. In the updated version of our paper (see Table 20), we have supplemented other generation metrics including BLEU-4, Meteor and Rouge-L for a comprehensive evaluation. Here is a quick look:\n\nRebuttal Table 4: Evaluation results on visual description based on CIDEr, BLEU-4, Meteor and Rouge-L. The evaluation dataset is NoCaps for zero-shot evaluation.\n| **Model**          | **CIDEr** | **BLEU-4** | **Meteor** | **Rouge-L** |\n| ------------------ | --------- | ---------- | ---------- | ----------- |\n| **BLIP-2**         | 83.57     | 37.18      | 27.09      | 49.34       |\n| **Lynx**           | 77.31     | 30.52      | 26.26      | 47.30       |\n| **MiniGPT4**       | 58.71     | 22.84      | 24.99      | 40.83       |\n| **mPLUG-Owl**      | 48.43     | 17.35      | 21.68      | 37.66       |\n| **LLaVA_V**        | 42.43     | 17.39      | 21.67      | 36.27       |\n| **InstructBLIP_V** | 30.18     | 7.71       | 13.48      | 25.13       |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700201198304,
                "cdate": 1700201198304,
                "tmdate": 1700202504890,
                "mdate": 1700202504890,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ILkKBRawWH",
                "forum": "ZuYvrjh2od",
                "replyto": "gpVYq7Gpy9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3580/Reviewer_mogP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3580/Reviewer_mogP"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. \n\nHowever, the primary concern remains: Given that current LVLM training tends to utilize all available data, my concern is that ReForm-Eval may ultimately assess only 'supervised learning' capabilities, rather than 'zero-shot learning.' This concern could significantly influence the long-term contribution of this work to the community.\n\nFurthermore, I acknowledge the difficulty in fairly evaluating current LMMs. Therefore, I suggest presenting only results that can be scientifically validated."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536401733,
                "cdate": 1700536401733,
                "tmdate": 1700536401733,
                "mdate": 1700536401733,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HNYrl30IOl",
            "forum": "ZuYvrjh2od",
            "replyto": "ZuYvrjh2od",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3580/Reviewer_cerW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3580/Reviewer_cerW"
            ],
            "content": {
                "summary": {
                    "value": "This paper claims that the capabilities of LVLMs have not been comprehensively and quantitatively evaluated. Accordingly, it proposes a ReForm-Eval benchmark, which re-formulates existing task-oriented benchmarks into unified LVLM-compatible formats. Based on ReForm-Eval, it conducts extensive experiments, thoroughly analyzes the strengths and weaknesses of existing LVLMs, and try to reveal insights behind LVLMs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. ReForm-Eval benchmark re-formulates 61 benchmark datasets based on existing data resources, including visual perception to high-level visual reasoning and dialog. \n2. ReForm-Eval has a large scale."
                },
                "weaknesses": {
                    "value": "1. ReForm-Eval is not suitable for a fair comparison of capability dimensions among different LVLMs. It is composed of 61 existing datasets. A number of these datasets are widely used in training data for LVLMs, e.g., VQA, VQAv2, GQA, OK-VQA, TextVQA, OCR-VQA, Text caps, Flickr30K, and so on. Different LVLMs will choose different training datasets but only cover some of them. However, ReForm-Eval merges both trained and reserved datasets into ability dimensions, leading to unfair comparison. \n2. This paper tries to reveal insights into model architecture and training datasets. However, since different LVLMs have different model architectures (Vision Encoders, connection modules, LLMs) and training datasets (pretraining and instruction tuning), the summarized insights in this paper are not convincing. \n3. The methods to generate appropriate negative options in visually grounded reasoning and multi-turn dialogue may not be reliable. The information of question and answer may not be sufficient to generate reasonable negative options with ChatGPT."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3580/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699125317574,
            "cdate": 1699125317574,
            "tmdate": 1699636312797,
            "mdate": 1699636312797,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sf4qRR73YT",
                "forum": "ZuYvrjh2od",
                "replyto": "HNYrl30IOl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review of Submission3580 by Reviewer cerW"
                    },
                    "comment": {
                        "value": "Thank you for your valuable advice. Here is our response to your questions: \n\n**Q1: Some models might have been trained on datasets included in ReForm-Eval, potentially leading to unfair comparisons.**\n\nResponse:  Please refer to our response to Q1 in the common response. \n\n---------\n\n**Q2: Questions about the soundness of insights gleaned in this paper, namely the issues regarding controlling variables.**\n\nResponse:  Please refer to our response to Q2.1 in the common response. \n\n---------\n\n**Q3: Lack of rational analysis of using ChatGPT to generate distractors based on textual QA pairs.**\n\nResponse:  Thank you for the feedback. Here, we assess the rationality of distractors on two levels:\n\n* **Reliability**: whether distractors face false-negative issues; \n* **Difficulty**: whether negative options are challenging and not overly naive. \n\nAccording to the Rebuttal Table 3 in the common response, **ChatGPT-generated distractors have a low false-negative rate in terms of reliability**. Regarding difficulty, we find that the performance of all models is not satisfactory (the best model achieves an accuracy of 55%), indicating that **ChatGPT-generated distractors are challenging**. Introducing additional image-related information, such as captions and objects, would require extra information or the use of detection models, incurring higher costs. Therefore, we believe the current approach is relatively reliable and efficient."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700200553092,
                "cdate": 1700200553092,
                "tmdate": 1700200927939,
                "mdate": 1700200927939,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4QBe41I41Y",
                "forum": "ZuYvrjh2od",
                "replyto": "HNYrl30IOl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Hope to receive feedback from reviewer cerW"
                    },
                    "comment": {
                        "value": "Dear Reviewer cerW:\n\nTonight is the rebuttal deadline. We really want to receive your further feedback, as in response to your questions, we have supplemented relevant experimental results and provided explanations. \nWe hope our responses can address your concerns. We will be very happy to clarify any further questions.\n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733737937,
                "cdate": 1700733737937,
                "tmdate": 1700733737937,
                "mdate": 1700733737937,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]