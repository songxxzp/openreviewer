[
    {
        "title": "Where is the Invisible: Spatial-Temporal Reasoning with Object Permanence"
    },
    {
        "review": {
            "id": "O2CZksCTNA",
            "forum": "eQcVfCK5cO",
            "replyto": "eQcVfCK5cO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9071/Reviewer_QThc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9071/Reviewer_QThc"
            ],
            "content": {
                "summary": {
                    "value": "- This work proposes a novel method for tracking invisible/occluded objects in videos using object permanence from developmental psychology. \n- The proposed method consists of three modules, namely, a Visual perception Module (VM), a qualitative spatial relation reasoner (SRR) and a quantitative relation-conditioned spatio-temporal relation analyst (SRA). \n- The SRR module infers the spatial relationship between all the objects in a frame, i.e. whether the target object is occluded, using past and current information. The SRA module, modeled using a conditioned diffusion model, uses this information to predict the possible location of the target object in the future frame.\n- Authors perform experiments on a synthetic and two real datasets and show competitive performance against contemporary methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The use of diffusion models for tracking is interesting and useful to the community."
                },
                "weaknesses": {
                    "value": "- Looking at a high level, the current method mimics the classical Kalman and Particle filter very closely (predict future location based on past observations and update the posterior based on current observations). Given the similarities, I believe comparing to such classical methods is necessary. I'm sure with the power of deep networks, the proposed method can outperform such classical methods but it is essential to know the gap in performance. Are the deep networks even necessary or does Kalman filter just solve the synthetic dataset?\n- Authors demonstrate results on two real world benchmarks but I believe some more experiments are necessary to properly understand the contributions. RAM (Tokmakov et.al.) show results on KITTI benchmark. The ID-Switch problem in Multi-Object Tracking (MOT) is a result of models failing to understand object permanence. If authors claim their method is good at reasoning object permanence, it is essential to report results on these tracking datasets. I don't expect the authors to show state-of-the-art performance on Tracking/MOT but showing that this work improves some metric, like reducing the number of ID switches, is a good indicator of this method working on real world data. I strongly recommend authors to perform these experiments and report results on these tasks and datasets.\n- Also consider reporting results on Occluded Video Instance Segmentation (OVIS) dataset [1] which deals with segmenting occluded objects in videos. \n- A few design decisions haven't been ablated to understand their significance. Why did authors choose to go with diffusion models for object tracking? Why not go with the well tested trackers like SORT[2] or DeepSORT[3]? Or some recent state of the art MOT trackers [4]? I think this experiment is necessary to justify the use of diffusion models.\n[1] Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang Bai, Serge Belongie, Alan Yuille, Philip H.S. Torr, Song Bai, Occluded Video Instance Segmentation: A Benchmark.\n[2] Bewley, Alex and Ge, Zongyuan and Ott, Lionel and Ramos, Fabio and Upcroft, Ben, Simple online and realtime tracking.\n[3] Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich, Simple Online and Realtime Tracking with a Deep Association Metric."
                },
                "questions": {
                    "value": "**Kindly address the concerns mentioned in the Weakness section for me to improve my rating**\n- Not a major issue but make sure the tables and figures in the paper appear at the top of the page. This is probably my own preference but I would like the authors to consider this, to make the paper look more professional."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I do not foresee any immediate ethical concerns with this work."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9071/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779481063,
            "cdate": 1698779481063,
            "tmdate": 1699637142189,
            "mdate": 1699637142189,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rDRTwE22rL",
                "forum": "eQcVfCK5cO",
                "replyto": "O2CZksCTNA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9071/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9071/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer QThc"
                    },
                    "comment": {
                        "value": "We wish to thank the reviewer for this helpful and comprehensive referee report, and we are glad to see your positive assessment of our methodology and presentation. Below we answer the questions and explain how the revised manuscript accommodates this detailed and helpful feedback.\n\n> *Q1. Looking at a high level, the current method mimics the classical Kalman and Particle filter very closely...*\n\nWe applied the traditional Kalman filter method to conduct experiments both on the LA-CATER and iVOT datasets, and the experimental results showed that the traditional filter method cannot effectively understand the spatial temporal relationship under uncertain noise distribution and nonlinear system. That is the Kalman filter only predicts the short-term trajectories, and can not conditioned on the object-object relationship. However, the objects in containment may disappear in the long term.\n\n- Comparison of the traditional Kalman filter method and our proposed method on the LA-CATER dataset and the iVOT dataset using mean IOU.\n\n|                     | LA-CATER | iVOT |\n|---------------------|---------------------|-----------------|\n| Kalman filter               | 0.6978               | 0.374           |\n| QQ-STR(Ours)        | 0.8278               | 0.583           |\n\n> *Q2. Authors demonstrate results on two real-world benchmarks but I believe some more experiments are necessary to properly understand the contributions.*\n\nWe agree that the evaluation of more real-world datasets is necessary. However, we notice that the object-object relationship in most existing real-world datasets is too simple to evaluate the object permanence reasoning. The invisible objects usually are just occluded by other objects or out of view, hardly contained by other objects. As a result, the invisible objects usually are shortly occluded by other objects, and the reasoning is unnecessary in most cases. However, in this paper, we focus on the ability to reason the localization of invisible objects, partially in some complex relationships, e.g., co-occurring containments and occlusions. Hence, this motivated us to collect new datasets in our daily activities that contain complex transitions of the object relationship. We argue that it will be our future work to collect more diverse videos with complex object relationships to evaluate the object permanence, rather than on those existing video tracking datasets, as the focus of the two domains are different.\n\n> *Q3. A few design decisions haven't been ablated to understand their significance. Why did the authors choose to go with diffusion models for object tracking? Why not go with the well tested trackers like SORT[2] or DeepSORT[3]? Or some recent state of the art MOT trackers [4]? I think this experiment is necessary to justify the use of diffusion models.*\n\nThe main purpose of using the diffusion model is to predict the trajectories of invisible objects under certain spatial constraints rather than serving as a tracker itself. Under high uncertainty, the possible trajectories of the invisible target can be viewed as a noisy Gaussian distribution that represents a blurred area controlled by the container trajectory. As the uncertainty decreases, the distribution gradually approximates the ground truth distribution for generating the correct trajectory. Therefore, we regard the trajectory prediction task as a condition-based generative task and apply the diffusion model. The relationship experiments in Section 4.5 demonstrate the effectiveness of diffusion models for trajectory prediction. \n\nThe MOT trackers also apply a similar tracking-by-detection framework. They focus on solving the data association problem for multi-object identification, where the objects may be of similar appearance and will shortly occluded by others. In this case, most MOT trackers assume the object keeps the linear dynamic (they usually track multiple pedestrians), and only apply some simple models to predict the short-term trajectory when the object is occluded by another. However, such short-term occlusion is a simple case in the study of object permanence. \nWe find that the trajectory of the object highly depends on the relationship to other objects. If the object is only occluded by others, it will keep its own movements. In contrast, if it is contained in some object, its trajectory will depend on the transition of the container.\nThat is why we need to build a relation-conditioned trajectory prediction for tracking invisible objects. The results of the Kalman Filter-based tracker also show that the simple predictor can not handle the complex relationship and long-term invisibility.\n\n> *Q4. Not a major issue but make sure the tables and figures in the paper appear at the top of the page.*\n> \nThanks to the reviewer for valuable and helpful suggestions. We have revised the manuscript to make the Tables and Figures appear at the top of the page."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9071/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681842135,
                "cdate": 1700681842135,
                "tmdate": 1700681842135,
                "mdate": 1700681842135,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7u2RXorqct",
            "forum": "eQcVfCK5cO",
            "replyto": "eQcVfCK5cO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9071/Reviewer_xz37"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9071/Reviewer_xz37"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a qualitative-quantitative reasoning framework for tracking invisible objects. The proposed method consists of three main modules, where a visual perception module is used to embed visual frames, SRR module is used to generate spatial relations between different objects, and SRA module predicts the location of the object based on the inferred relationships and a diffusion model. Experiments are performed on both synthetic and real-world datasets. Besides, this paper proposes a real-world RGB-D dataset, containing various scene categories, longer sequences, and more complex spatial relations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper proposes a real-world RGB-D dataset, which might be beneficial for future research on the related task.\n2. This paper proposes a novel qualitative-quantitative reasoning framework, separating the explicit qualitative reasoning analysis and the quantitative location prediction, which might provide a new sight to solve object permanence."
                },
                "weaknesses": {
                    "value": "1. The different modules in the proposed framework don\u2019t look compact, but independent. The framework integrates many separate models to address separate problems, like an object detector, a human pose estimation, the correction module, and a diffusion-model-based trajectory predictor. These different models are also trained separately, but not in an end-to-end manner. Compared with SOTA methods, results in Table2 are also not evident enough to show the advantages brought by this kind of complex design. Besides, the framework seems to bring many hyperparameters, I think it's necessary to explain these hyperparameter settings, and also perform corresponding experiments to show the robustness of the proposed framework on different hyperparameter settings.\n\n2. Some detailed designs are not explained clearly. For example:\n(1) For the object detection model, the classification of the general detection task is to identify different categories of objects, while I think the proposed method needs an object-level classification, the paper didn't mention much about this.\n(2) For the shiver error in the correction module, what if the size of the object itself changes in adjacent frames, like aspect ratio change or scale change?\n\n3. One of the main contributions of this paper is to propose a new dataset, however, there are very few descriptions of this new dataset. Besides, there is also a lack of experimental results of SOTA methods on the proposed dataset iVOT (Table3).\n\n4. The paper is a bit hard to read since:\n(1) the definition and use of some variables and formulas seem a bit complicated and have some typos. For example, the definition of hand positions H_i, the use of t/T/k, inconsistent use of superscript and subscript on different formulas, and so on.\n(2) Fig1&2 are not connected tightly with text. The cases shown in the figures are not well explained in the text."
                },
                "questions": {
                    "value": "Please find my concerns in the above \"Weaknesses\".\nI am concerned most about issues 1,3 and 2 in order. I'll consider changing my rate if the authors explain them well."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9071/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805239501,
            "cdate": 1698805239501,
            "tmdate": 1699637142074,
            "mdate": 1699637142074,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bwUUW0sgJh",
                "forum": "eQcVfCK5cO",
                "replyto": "7u2RXorqct",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9071/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9071/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer xz37"
                    },
                    "comment": {
                        "value": "Thanks for your helpful and constructive suggestions. Below we answer the questions in detail.\n\n> *Q1: The different modules in the proposed framework don\u2019t look compact, but independent. The framework integrates many separate models to address separate problems, like an object detector, a human pose estimation, the correction module, and a diffusion-model-based trajectory predictor. These different models are also trained separately, but not in an end-to-end manner. Compared with SOTA methods, the results in Table 2 are also not evident enough to show the advantages brought by this kind of complex design.*\n\nA1: We argue that the overall framework is well-organized and comprehensive, rather than independent. All the modules we introduced are essential for the reasoning of object permanence. The object detector localizes the visible objects, and the human pose estimation is to recognize the human-object interaction for identifying the change in the object-object relationship (as most object states are changed by human actions in the real world). To localize the invisible object, the trajectory prediction module proposes multiple possible trajectories conditioned on the reasoned spatial relationship. The correction module checks the reasonableness of the proposed trajectory based on the state of the visible objects. Each module is interdependent and indispensable for localizing the invisible objects, as shown in the ablation study.\n\nCompared to the end-to-end methods, our method has the following advantages: \n1) **Generalizability.** The pre-trained model can provide generalizable state representation to make our framework easily transferable to unseen datasets. We have reported the cross-domain generalization of our method compared with the baselines, shown in the following Table. The models are trained on LA-CATER and tested on iVOT. Our approach demonstrates superior generalization capabilities compared to OPNet.\n\n</table>\n|                     | Trained on LA-CATER | Trained on iVOT |\n|---------------------|---------------------|-----------------|\n| AutoMatch            | 0.491               | 0.491           |\n| OPNet              | 0.476               | 0.535           |\n| QQ-STR(Ours)        | 0.554               | 0.583          |\n\n2) **Modularity.** Each module (e.g., object detection, action recognition, trajectory prediction) can be easily replaced by state-of-the-art methods, thus our model can benefit from the advances of each component. \n3) **Interpretability.** Our framework consists of modular components that can be individually analyzed and evaluated. For example, we can visualize the relationship graph and the forecasted trajectory produced by each module, which can help us identify the potential weaknesses and limitations of our approach. \n\n> *Q2: Besides, the framework seems to bring many hyperparameters, I think it's necessary to explain these hyperparameter settings, and also perform corresponding experiments to show the robustness of the proposed framework on different hyperparameter settings.*\n\nThanks for your suggestions. We have provided the details about the hyperparameter in the Appendix. For convenience, we listed the main hyperparameters used in our method and conducted experiments to further analyze the effect of each hyperparameter in the LA-CATER. In the ablation experiment, we fixed other hyperparameters and varied only one hyperparameter to test the impact of different hyperparameters on performance stability. As is shown in the results, our framework can be robust to the change of hyperparameters. We have added this analysis in Section 4.3 and Table 2 of the revised manuscript. \n\n- Experimental results of our method on the LA-CATER dataset with different hyperparameters using mean IoU. We highlight the default value in bold.\n\n| Hyperparameters | Value | mIoU  |\n|-----------------|-------|-------|\n|                 | 10    | 82.10 |\n| $L$: The length of the trajectories input into the predictor            | **20**    | 82.78 |\n|                 | 40    | 81.78 |\n|                 |   |  |\n|                 | 1     | 82.17 |\n| $K$: The maximum number of spatial relation graph candidates              | **16**    | 82.78 |\n|                 | 64    | 82.78 |\n|                 |   |  |\n|                 | 100   | 82.63 |\n| $N$: The steps of diffusion models             | **200**   | 82.78 |\n|                 | 1000  | 82.65 |\n|                 |   |  |\n|                 | 0.1   | 82.54 |\n| $\\lambda_{reappear}$: The threshold confidence of reappearance| **0.2**   | 82.78 |\n|                 | 0.3   | 82.19 |\n|                 |   |  |\n|                 | 0.2   | 82.78 |\n|$\\lambda_{disappear}$: The threshold confidence of disappearance| **0.4**   | 82.78 |\n|                 | 0.6   | 82.18 |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9071/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681379081,
                "cdate": 1700681379081,
                "tmdate": 1700719123237,
                "mdate": 1700719123237,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2z9vFT2pei",
                "forum": "eQcVfCK5cO",
                "replyto": "7u2RXorqct",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9071/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9071/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer xz37"
                    },
                    "comment": {
                        "value": "> *Q4: Some detailed designs are not explained clearly...* \n> \nA4: We are sorry that we did not describe the details exhaustively and caused your misunderstanding. We have revised the manuscript to explain more details. For convenience, we introduce the details in the following: \n\n*Object Detection*: For the experiments on the CATER and LA-CATER datasets, due to the limited types of objects included in the dataset, the vision module can be implemented by an object detector. For a fair comparison, we use the same object detector as previous works (OPNet[1], AAPA[2], and RAM[3]), a Faster RCNN network pre-trained on the COCO dataset, which is introduced in Section 4.2 of the paper. For the experiments in real scenes, we apply AutoMatch[4] as the object detector. During tracking, the search area in the next frame depends on the estimated target location (detection position or predicted position) in the current frame. This rule is widely used in visual tracking models. In this case, the adjustment of the search area will affect the performance of the tracker once it disappears.\nHence, in our framework, once the confidence of the detection result is lower than a threshold of $\\lambda_{disappear}$, we will assume the target has disappeared and predict the trajectory of the invisible target to guide the adjustment of the search area synchronously. In the same way, when the confidence of an object that originally disappeared is greater than $\\lambda_{reappear}$, we will think that the object appears again. The proposed method focuses on predicting the spatial and motion relationships of objects in the invisible state, rather than vision-based object tracking.\n\n*Correction Module*:  Thanks for pointing this out. Since the frame rates of the LA-CATER synthetic dataset and the iVOT real-world dataset used are both 30fps, the scale of objects in adjacent frames changed slightly in most cases. If the scale changes dramatically, the correction module may result in unreasonable modifications. It will be our future work to extend our framework on videos with violent shaking.\n\n> Q5: *One of the main contributions of this paper is to propose a new dataset, however, there are very few descriptions of this new dataset. Besides, there is also a lack of experimental results of SOTA methods on the proposed dataset iVOT (Table 3).*\n> \nA5: We appreciate your suggestions. We have added more details and results about the new dataset. To be specific, the proposed iVOT(invisible Object Tracking) dataset is collected by Intel Realsense D435i which has an RGB frame resolution of 1920\\*1080, a depth output resolution of 1280\\*720, and a frame rate of 30. The data are recorded in indoor rooms (e.g., bedroom and living room), including diverse human-object interaction activities, where the objects frequently become invisible in the video. These cases are normal in our daily activities but are challenging to the state-of-the-art methods. For example, a man picks up a toy into a box and moves the box to another place, where the model needs to reason the object-object relation to localize the target object. More details on this dataset are elaborated in Appendix A.5. We also add more results of SOTA methods and visualizations in Appendix A.6.\n\n- Comparison of SOTA methods testing on the iVOT dataset using mean IOU.\n\n| Method       | mIoU  |\n|--------------|-------|\n| AutoMatch[4]    | 0.491 |\n| RTS[5]          | 0.513 |\n| ToMP-50[6]      | 0.527 |\n| ToMP-100[6]     | 0.532 |\n| QQ-STR(Ours) | 0.583 |\n\n> *Q6: The paper is a bit hard to read...*\n> \nA6: Thanks for your valuable and helpful suggestions. We have corrected relevant ambiguities or inconsistencies in the revision. We highlight the main update in blue."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9071/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681620128,
                "cdate": 1700681620128,
                "tmdate": 1700719328494,
                "mdate": 1700719328494,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "r5zJGSkvEw",
            "forum": "eQcVfCK5cO",
            "replyto": "eQcVfCK5cO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9071/Reviewer_4XQC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9071/Reviewer_4XQC"
            ],
            "content": {
                "summary": {
                    "value": "- The paper focuses on the problem of 2D bounding box-based object tracking under occlusion and containment.\nThe proposed approach is named QQ-STR, Qualitative-Quantitative Spatial-Temporal Reasoning. It has three components.\n- a) Visual perception: Per frame object detection and human pose estimation using off-shelf methods. \n- b) Qualitative spatial relation reasoning: Predicts the spatial relationship between objects in a frame and considers multiple possible object relationships as a graph by maintaining potential candidates. \n- c) Quantitative relation-conditioned spatial-temporal relation analyst: Brings time into consideration. Error corrects and analyses the trajectory of the object. Helpful in tracking completely invisible objects.\n- Evaluation is done on three datasets, LA-CATER, Liang et al. (2018), iVOT (collected by the authors).\n- iVOT is RGB-D, 49 videos, 0.5 to 1.5 minute long - 12 scenes, 31k frames and 171 annotated trajectories.\n- Baselines: OPNet (Shamsian et al. 2020), PA (Liang et al. 2021), RAM (Tokmakov et al. 2022), AAPA (Liang et al. 2021).\n- The proposed method QQ-STR outperforms baselines on both synthetic and real datasets (mIoU metric)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-organized and easy to follow.\n- The fundamental idea of using the graph structure for occlusion/containment reasoning for multi-object tracking is technically novel. The proposed modules make sense, and it is interesting to see the combination of the two components (qualitative and quantitative) working towards the state estimation of invisible objects.\n- The experiments are done over multiple datasets, synthetic and real. The authors also contribute a dataset iVOT in this work, which is much appreciated. The scale of the dataset is also reasonably large compared to existing datasets focused on the same tasks.\n- Ablations are done in Table. 4 to provide insights into the effectiveness of each components, SRR and SRA."
                },
                "weaknesses": {
                    "value": "- No qualitative results: There is no single image in the main paper visualizing the tracking results of the proposed method. \nPlease consider showing multiple frames (which can be manually selected) of an object sequence and the tracked bounding boxes for QQ-STR and the closest baseline method (say RAM). This will tell us the improvements QQ-STR brings over the existing methods. If space permits, consider adding a failure case visualization highlighting the limitations. Note that the supplemental consists of two short videos (3 secs from Liang et al and 5 secs from LA-CATER) showing the results of the proposed method but no comparison to the baseline. It is a missed opportunity to not show video results on the collected iVOT dataset in the main paper or supplementary.\n\n- Evaluation on the CATER dataset: Sec. 2 argues that the CATER dataset was not used in the evaluation as it only has classification and relation labels. However, in comparison to LA-CATER (the substitute), the CATER dataset is more widely used by current methods and has a more evolved list of quantitative performance of related methods. The results of the proposed method QQ-STR can still be evaluated on the CATER dataset, similar to the  CATER-Snitch localization task Table 1 of [1].\n\n- Effect of number of objects on performance/Generalization: The proposed method considers all possible relations between the objects; the complexity of such a graph will increase when more objects come into the picture. This questions the ability of the method to generalize to conditions beyond five objects (which is the maximum used to show results). Testing on real-world tracking datasets like KITTIT (similar to RAM paper's Table. 2) would be a great way to showcase in-the-wild generalization of the proposed method.\n\n- More descriptive method figures: Fig.1 and Fig.2 treat the proposed modules, VM, SRR, and SRA, as black boxes and tell nothing about the method. Please consider adding details about these components and provide insights into the inner workings of these blocks. Method figures are excellent visual tools to quickly convey the key ideas to the reader, which takes a while if it is only text-based (which is the case currently).\n\n[1] LEARNING WHAT AND WHERE: DISENTANGLING LOCATION AND IDENTITY TRACKING WITHOUT SUPERVISION, ICLR 2023."
                },
                "questions": {
                    "value": "As listed above,\n1. Visual comparision of the QQ-STR and baseline.\n2. CATER evaluation.\n3. Going beyond the toy-object datasets, testing in-the-wild generalization.\n4. Improved paper presentation (specific focus on the method figure)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9071/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698884549708,
            "cdate": 1698884549708,
            "tmdate": 1699637141947,
            "mdate": 1699637141947,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ONEmBR8VKn",
                "forum": "eQcVfCK5cO",
                "replyto": "r5zJGSkvEw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9071/Reviewer_4XQC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9071/Reviewer_4XQC"
                ],
                "content": {
                    "comment": {
                        "value": "No reply from the authors. Keeping my original rating for \"5: marginally below the acceptance threshold\"."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9071/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673031166,
                "cdate": 1700673031166,
                "tmdate": 1700673031166,
                "mdate": 1700673031166,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fDIVRX1atC",
                "forum": "eQcVfCK5cO",
                "replyto": "r5zJGSkvEw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9071/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9071/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 4XQC"
                    },
                    "comment": {
                        "value": "We sincerely apologize for our delayed reply. We hope you will kindly reconsider your rating after reading our response. We appreciate your patience and understanding. We thank you for your helpful and comprehensive review. We have carefully addressed your questions and incorporated your feedback into the revised manuscript. The followings are the detailed response: \n\n> *Q1: No qualitative results...*\n\nA1: We agree with your comment that qualitative results are important to demonstrate the effectiveness of our method. We have added representative sequences on the iVOT datasets in the manuscript, as shown in Appendix A.6. These sequences compare the qualitative results of different methods and illustrate the advantages of our approach. \n\n> *Q2: Evaluation on the CATER dataset...*\n\nA2: We apologize for the confusion caused by our argument. We have evaluated our model on both the LA-CATER and the CATER datasets. Due to space limitations, we reported the results on the CATER dataset in Appendix A.3. We have clarified this point in the manuscript. In summary, our method outperforms most of the baselines in the Top 1 Accuracy and achieves comparable results to [1] (slightly lower in the Top 1 Accuracy but higher in the Top 5 Accuracy).\n\n> *Q3: Effect of number of objects on performance/generalization...*\n\nA3: We believe that this problem can be solved to a certain extent by increasing the size of the candidate relationship sets ($K$) or through appropriate pruning methods. In fact, in the LA-CATER and iVOT datasets of our experiments, the maximum number of objects is $15$ (more details are shown in Appendix A.2 of our paper), and on this scale of numbers, our proposed method shows satisfying performance. Finding a way to efficiently establish spatio-temporal relationships in crowded scenes is one of our future research directions.\n\nWe also agree that the evaluation of more real-world datasets is necessary. However, we observe that most existing real-world datasets have simple object-object relationships that are not suitable for evaluating object permanence reasoning. As a result, the invisible objects are usually briefly occluded by other objects, and the relation reasoning is trivial in most cases. In contrast, in this paper, we focus on the ability to reason about the localization of invisible objects, especially in some complex relationships, such as co-occurring containments and occlusions. Therefore, we collected new datasets from our daily activities that contain complex transitions of object relationships. We argue that it is more meaningful to evaluate the object permanence on these new datasets, rather than on the existing video tracking datasets, as the two domains have different focuses.\n\n> *Q4: More descriptive method figures...*\n>\nA4: We appreciate your valuable suggestions. We have revised our manuscript to provide more details about each module of our method. Please refer to Appendix A.1 (Fig.4, Fig.5, Fig.6) for the detailed illustrations and explanations of our method.\n\n[1] LEARNING WHAT AND WHERE: DISENTANGLING LOCATION AND IDENTITY TRACKING WITHOUT SUPERVISION, ICLR 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9071/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677414215,
                "cdate": 1700677414215,
                "tmdate": 1700678290091,
                "mdate": 1700678290091,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "60Yk1y8cPN",
            "forum": "eQcVfCK5cO",
            "replyto": "eQcVfCK5cO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9071/Reviewer_Z8r6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9071/Reviewer_Z8r6"
            ],
            "content": {
                "summary": {
                    "value": "NOTE: My review is fully re-edited on Nov.10 because what was written before is about another paper.\n\nThe paper proposed a new method, QQ-STR, for invisible object tracking.\nQQ-STR mainly contains of 3 modules:\n- A Visual Module (VM) consisting of an object detector and a human pose estimator for visual perception from input video frames.\n- A Spatial Relation Reasoner (SRR) that generates possible spatial relationship graph for objects in each frame at every timestamp\n- A Spatial-temporal Relation Analyst (SRA) that predicts possible trajectories and select the best one as the prediction\n\nExperiments are done on 3 datasets including one proposed by the authors themselves and the results suggest QQ-STR can achieve better or comparable performance as previous state-of-the-arts. Ablation studies support some important design choices."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Experiments are done on 3 datasets and show seemingly competitive results\n- Adapt a diffusion model for trajectory generation looks interesting\n- Some ablation studies are provided to support a few important design choices."
                },
                "weaknesses": {
                    "value": "- Many important details are not well explained or missing. For example, hand positions are extracted as in Sec. 3.3, but how it is being used is never mentioned in latter sections.\n\n- Some parts don't make much sense. For example:\n   - When generating spatial relationship graphs, do you distinguish occlusion and containment? If not, why?\n   - Also, when generating the graphs, why do you only use object trajectories without identity information? The object characteristic should affect occlusion and containment, in my opinion.\n\n- The method needs to \"enumerate all possible spatial relations in the first frame and form the candidates PG1.\" This may cause some problems when there are too many objects.\n\n- Despite a relative straightforward main idea of estimating occlusion / containment status based on past trajectories, there are a lot of twists and tweaks involved, for example, lots of hyperparameters and those correction stages, making the whole system over-complicated and vague to understand. The effects of most of them are unclear, and may hinder the generalizability to other datasets.\n\n- The presentation needs to be improved. The whole idea is acutally Also, please double check inconsistent or incorrect notions, for example, \"H_i = {(pl_i^t, pr_i^t), (pl_i^t, pr_i^t), . . . , (pl_i^t, pr_i^t)}\" should be H_i = {(pl_i^1, pr_i^1), (pl_i^2, pr_i^2), . . . , (pl_i^t, pr_i^t)}"
                },
                "questions": {
                    "value": "Please address my concern according to the Weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9071/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9071/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9071/Reviewer_Z8r6"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9071/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699523440979,
            "cdate": 1699523440979,
            "tmdate": 1699665523814,
            "mdate": 1699665523814,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0KqhkosXxf",
                "forum": "eQcVfCK5cO",
                "replyto": "60Yk1y8cPN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9071/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9071/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Z8r6"
                    },
                    "comment": {
                        "value": "Thanks for the helpful and comprehensive review, and we are glad to see your positive assessment of our methodology (\"Adapt a diffusion model for trajectory generation looks interesting\") and experiments. Below we answer the questions and explain how the revised manuscript accommodates this detailed and helpful feedback.\n\n> *Q1: Many important details are not well explained or are missing.*\n \nWe apologize for the lack of clarity and detail in our original manuscript. We have improved our manuscript to provide more details about our method. In particular, we have explained how the hand positions are used in the Spatial Relation Predictor (Section 3.4). Empirically, we regard the hand as a special object that never be contained by other objects in our framework. If we treat the hand as an ordinary object, it may cause ambiguity between containments and occlusions.\n\n> *Q2: When generating spatial relationship graphs, do you distinguish occlusion and containment? If not, why?*\n\n When generating a spatial relation graph, we distinguish three types of spatial relationships: occlusion, direct containment, and no direct relationship. We use different edge categories to represent these relationships.\n\n> *Q3: when generating the graphs, why do you only use object trajectories without identity information? The object characteristic should affect occlusion and containment, in my opinion.*\n\nThe visual module in our framework has already obtained the trajectories of visible objects by ID matching (using the same Faster RCNN network as OPNet and RAM on the LA-CATER dataset, and the AutoMatch tracker on real-world datasets). In this paper, the relation graphs focus on the relationship between invisible objects and visible objects. However, there is no identity information for invisible objects, so our method only uses historical trajectories without identity information when generating spatial relation graphs. The main goal of our work is to infer the trajectories of invisible objects by constructing the spatial relations of both visible and invisible objects, rather than using visual features to track visible objects.\n\n> *Q4: The method needs to \"enumerate all possible spatial relations in the first frame and form the candidates PG1.\" This may cause some problems when there are too many objects.*\n\nWe agree that the number of candidate relationship graphs may grow exponentially, so we only keep the most likely $K$ spatial relation graphs at each time step, as described in the Evaluation Module in Section 3.5. In practice, our method achieves an average processing speed of $100ms$ per frame when the size of the candidate set K=16. We have also tested the cases where the size of candidate sets is larger or smaller, and the processing speed of our method is about $30ms$ per frame (K=1) and $250ms$ per frame (K=64). We have updated the manuscript accordingly."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9071/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680454158,
                "cdate": 1700680454158,
                "tmdate": 1700680454158,
                "mdate": 1700680454158,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JAQvnagdSg",
                "forum": "eQcVfCK5cO",
                "replyto": "60Yk1y8cPN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9071/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9071/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to  Reviewer Z8r6."
                    },
                    "comment": {
                        "value": "> *Q5: Despite a relatively straightforward main idea of estimating occlusion /containment status based on past trajectories, there are a lot of twists and tweaks involved...*\n\nCompared to the end-to-end methods, our method has the following advantages: \n1) Generalizability. The pre-trained model can provide generalizable state representation to make our framework easily transferable to unseen datasets. We have reported the cross-domain generalization of our method compared with the baselines. The models are trained on LA-CATER and tested on iVOT. Our approach demonstrates superior generalization capabilities compared to OPNet. \n2) Modularity. Each module (e.g., object detection, action recognition, trajectory prediction) can be easily replaced by state-of-the-art methods, thus our model can benefit from the advances of each component. \n3) Interpretability. Our framework consists of modular components that can be individually analyzed and evaluated. For example, we can visualize the relationship graph and the forecasted trajectory produced by each module, which can help us identify the potential weaknesses and limitations of our approach. \n \nTo evaluate the generalization ability of our method to different datasets, we performed cross-dataset experiments, where we trained our model on the synthetic LA-CATER dataset and tested it on the real-world iVOT dataset. In this experiment, we use OPNet for comparison. The AutoMatch model is a part of the vision module for localizing the visible objects. Results show that our proposed method has stronger generalization ability and performance than OPNet.\n\n- Comparison of the cross-domain generalization using mean mIoU. We train the models on the LA-CATER dataset and evaluate the performance on the iVOT dataset. We also report the results of OPNet as a reference.\n\n\n|                     | Trained on LA-CATER | Trained on iVOT |\n|---------------------|---------------------|-----------------|\n| AutoMatch[1] | 0.491               | 0.491           |\n| OPNet[2]               | 0.476               | 0.535           |\n| QQ-STR(Ours)        | 0.554               | 0.583          |\n\n> *Q6: The presentation needs to be improved.*\n\nThanks for your valuable and helpful suggestions. We have revised relevant ambiguities or inconsistencies in the paper.\n\n[1]. Zhang, Zhipeng, et al. \"Learn to match: Automatic matching network design for visual tracking.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n\n[2]. Shamsian, Aviv, et al. \"Learning object permanence from video.\" Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XVI 16. Springer International Publishing, 2020."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9071/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680507595,
                "cdate": 1700680507595,
                "tmdate": 1700719113893,
                "mdate": 1700719113893,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]