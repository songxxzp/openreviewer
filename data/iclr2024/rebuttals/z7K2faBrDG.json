[
    {
        "title": "Perceptual Measurements, Distances and Metrics"
    },
    {
        "review": {
            "id": "tWMlD0xAYd",
            "forum": "z7K2faBrDG",
            "replyto": "z7K2faBrDG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2733/Reviewer_qN8s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2733/Reviewer_qN8s"
            ],
            "content": {
                "summary": {
                    "value": "The authors derive nonlinear relations between certain univariate image features and the perceptual scale (human response) to these features using Fisher information. \nWhile the response for frequency and orientation of Gaussian Random Fields proves to be quite consistent with actual human responses derived from similarity judgements, the situation is more complicated in naturalistic textures.\nFor Gaussian fields the correspondence is good because (1) these images are easy to characterize with those parameters and hence, the Fisher information of those parameters captures all the information of the multidimensional objects, and (2) the discriminability is actually related to the image information. However, for the responses to interpolation between naturalistic textures, it is not clear which features to consider to compute the Fisher information. In this case, the authors try different features (pixel values, wavelet responses, VGG19 responses, and the power spectrum), and the best agreement is found using the power spectrum."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors address an interesting issue (the nonlinearity of the human image representation), and they show that Fisher information may be a driving factor of discriminability in simple textures defined by band-pass power spectra. \nThe provided expressions of Fisher information have technical interest."
                },
                "weaknesses": {
                    "value": "It is not clear how to generalize their approach to more complicated textures, where it is not obvious the features to describe them and how to compute Fisher information, J, from those. \n\nIn the more complicated scenario the authors only try a limited (non conclusive) set of features on some so-called naturalistic textures which are not very general either. All this reduces the strength of the experiments and the possible conclusions. Moreover, in some of the cases (e.g. pixel and wavelet features) they do not explain how to compute J.\n    \nGiven the normalization of the response axes, the derived nonlinear functions cannot be used as a metric to compute differences between textures."
                },
                "questions": {
                    "value": "MAJOR QUESTIONS:\n\n* Please give examples interpolating between more general textures (e.g. a brick wall, and a flower field, or a pile of fruits -see examples of natural textures in Portilla & Simoncelli IJCV 2000-). Do you get similar results (theoretical predictions and human responses) in those cases?\n\n* The description of the method is confusing. For example, how do you get the nonlinear response in Fig. 1? I guess first you use the expressions in appendix A, then, you use \\Psi in Eq. 6 and then you integrate, right?. This is not clear in the text. Similarly, how do you get the predictions in Fig. 2?. For the VGG response you assume they are Gaussian vectors, then you apply Proposition 4, and then you integrate Eq. 6?.  \n\n* Appendices give explicit expressions for the Fisher information for the frequency and orientation of Gaussian fields and of Gaussian vectors for the activations of VGG-19 (if they were Gaussian), but how do you compute J for the pixel representation? (just apply an FFT and then use that estimation of the spectrum and the formula in Preposition 3?). \nHow do you do it for wavelets?... What is the wavelet decomposition you used? There are many of them!\n\n* The normalization of the response axes imply that all modifications of the stimuli lead to the same perceptual distortion. Simple visual inspection of the pairs in Fig. 2 shows that this is not correct. If the authors do not propose a way to give an absolute scaling in these different dimensions they should not claim that they are giving a metric of the image space.\nWhat they just provide is a nonlinear (up to a scalar) relation between displacements in certain directions and variations in the inner representation, but this is not a metric.\n\n* Following the above comment, the proposed method would be unable to give a measure to predict the Mean Opinion Score (MOS) on distortion in databases such as TID [Ponomarenko et al. 13] or KADID [Lin et al. 19]. If this is not the case, the authors should mention how to infer this MOS. \n\n* The title is too vague, it does not reflect the content of the paper, and actually overstresses \"distances\" and \"metrics\" not quite addressed in the work. What about changing the title by something like: \"Nonlinear image representations in humans from Fisher information\".\n\nMINOR ISSUES:\n\n* In the abstract authors say \"we demonstrate that it is related to the Fisher information of the generative model that underlies perception\", while it should say \"we demonstrate that it is related to the Fisher information of the generative model that underlies the stimuli\"\n\n* In the first paragraph of page 6 the authors say \"We will see that in both cases...\" ... Where do the authors show this? (this is related to the confusing description of the method stated above).\n\n* Typos:  \nfirst paragraph of page 8 \"VGG-19 to another (bottom-right of Fig. 2)\" ---> \"VGG-19 to another (bottom-left of Fig. 2)\"\nsecond paragraph page 8 \"frequency mode (Proposition 2 and Proposition 2)\" ---> ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2733/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2733/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2733/Reviewer_qN8s"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2733/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698652787949,
            "cdate": 1698652787949,
            "tmdate": 1700757531155,
            "mdate": 1700757531155,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oT71cvAX9O",
                "forum": "z7K2faBrDG",
                "replyto": "tWMlD0xAYd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "response 1"
                    },
                    "comment": {
                        "value": "Thank you for reading our paper and for your detailled feedback. This very useful to us. We appreciate that you consider the question of non-linear representation of image interesting and that you are considering our results technically useful.\n\n**Response to weaknesses**\n\n> It is not clear how to generalize their approach to more complicated textures, where it is not obvious the features to describe them and how to compute Fisher information, J, from those. \n\nParts of our results are generic in the sense our theoretical results hold as long as there is a generative model of your the image that is sufficient to compute the Fisher information (or estimate it numerically). We focus on the texture model that is capable of synthesizing a wide range of natural textures and that is often used in experimental vision studies (Gatys 2015, Wallis 2017, Vacher 2020, Ziemba 2021). The features we are testing are also the standard ones that are tested in vision studies. \n\n> In the more complicated scenario the authors only try a limited (non conclusive) set of features on some so-called naturalistic textures which are not very general either. All this reduces the strength of the experiments and the possible conclusions. Moreover, in some of the cases (e.g. pixel and wavelet features) they do not explain how to compute J.\n\nWe acknowledge that we only run experiment with few textures. Though the goal of the current paper is to provide the technical results. We are already working in more general settings : generating image interpolations using deep generative models (eg VAEs) and evaluate the corresponding perceptual scale. So we expect to further extend the experimental work but this is beyond the scope of this paper. See the response to questions below for the calculation of Fisher information.\n\n> Given the normalization of the response axes, the derived nonlinear functions cannot be used as a metric to compute differences between textures.\n\nThe reviewer is correct, the measured scale is relative and cannot be used to measure distances between textures. However, this is not our goal here. Yet, we explain that we can measure the discrepancy between model predictions and empirical perceptual scales using the AMS score introduced in section 3.2. Such a score is a measure of how much a model is aligned to perception.\n\n**Response to questions**\n\n> Please give examples interpolating between more general textures (e.g. a brick wall, and a flower field, or a pile of fruits -see examples of natural textures in Portilla & Simoncelli IJCV 2000-). Do you get similar results (theoretical predictions and human responses) in those cases?\n\n In the supplementary material of Vacher *et al.* 2020, you can see that indeed a wide range of textures can be synthesized. We can definitely run a few more experiments before the camera ready deadline if our work were to be accepted (before the review period ends is not possible because of administrative delays). \n \n > The description of the method is confusing. For example, how do you get the nonlinear response in Fig. 1? I guess first you use the expressions in appendix A, then, you use \u03a8 in Eq. 6 and then you integrate, right?. This is not clear in the text. Similarly, how do you get the predictions in Fig. 2?. For the VGG response you assume they are Gaussian vectors, then you apply Proposition 4, and then you integrate Eq. 6?. \n\nYou are correct on everything. For Fig 1, it's Appendix A + Eq. 6 and for Fig 2, it's Proposition 4 + Eq. 6. We apologize if it's not clear we have clarified this in the Figures' caption and main text.\n\n> Appendices give explicit expressions for the Fisher information for the frequency and orientation of Gaussian fields and of Gaussian vectors for the activations of VGG-19 (if they were Gaussian), but how do you compute J for the pixel representation? (just apply an FFT and then use that estimation of the spectrum and the formula in Preposition 3?). How do you do it for wavelets?... What is the wavelet decomposition you used? There are many of them!\n\nWe acknowledge that we did not give the details of the computation for all models. In every case, we use the Gaussian assumption so it's either Proposition 3 or 4. For the pixel measurement, we are looking at the pixels distribution (histogram). For the image assumption, it's in fact the GRFs assumption so here we compute modulus of the Fourier transform and Prop 3. For the wavelet, we use the steerable pyramid (Portilla & Simoncelli 2000).Then, it is similar to VGG, we consider that the vector of wavelet activations at each pixel location is a sample of multivariate Gaussian distribution so we compute the empirical mean and covariance.  We have added a section in the appendix with those details and point to this section in the Figure caption."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700342894797,
                "cdate": 1700342894797,
                "tmdate": 1700342894797,
                "mdate": 1700342894797,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ccrz8OKWLU",
                "forum": "z7K2faBrDG",
                "replyto": "tWMlD0xAYd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2733/Reviewer_qN8s"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2733/Reviewer_qN8s"
                ],
                "content": {
                    "title": {
                        "value": "I find the replies ok. I rise the score to 8 given the interest of the subject for ICLR (learning representations!) and quality of info-theoretic results"
                    },
                    "comment": {
                        "value": "After rising my score to 8 (due to the replies, quality of the info-theoretic results, and interest of the subject for the ICLR community -learning representations!-), just a comment on related literature. Following this statement done in the repy to other reviewer: \"To this date and up to our knowledge, we are not aware of any alternative (normative) models that provide predictions of the perceptual scales.\"... The authors may have missed other (normative) approaches based on manifold equalization (that includes goals such information maximization and error minimization) that lead to similar/equivalent nonlinearities. \nThis concept dates back (in the univariate case) to:\n* Laughlin Phys. Biol. Proc. Im., Springer 1983 (for the luminance-brightness nonlinearity)\n* Twer & MacLeod Network 2001 (for nonlinear color responses)\n\nAnd has been extended to multivariate cases is:\n* Malo & Gutierrez Network 2006 (for nonlinear response to contrast of ICA-like patterns)\n* Laparra & Malo Front. Human Neurosci. 2015 (for color / contrast of PCA-like textures / energy of moving patterns)\n\nCheck those and consider including references to these other normative/functional explanations of the nonlinearities. I think it would be nice to connect both methods in the future."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699095509,
                "cdate": 1700699095509,
                "tmdate": 1700700083891,
                "mdate": 1700700083891,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v7ZlFuKKBQ",
                "forum": "z7K2faBrDG",
                "replyto": "O8c1NcoAGN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2733/Reviewer_qN8s"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2733/Reviewer_qN8s"
                ],
                "content": {
                    "title": {
                        "value": "Quick comment on univariate vs multivariate methods"
                    },
                    "comment": {
                        "value": "I see your point, but note that multivariate methods do not mean that you cannot compute non-linearities in unidimensional (univariate) directions. Actually, the comparisons with experiments are done in \"unidimensional\" examples."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735260854,
                "cdate": 1700735260854,
                "tmdate": 1700735260854,
                "mdate": 1700735260854,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "myzWIj5jlM",
                "forum": "z7K2faBrDG",
                "replyto": "tWMlD0xAYd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2733/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Right, we will need to go into the details !\n\nPS : do not forget to update your score in the main review (if you still wish to do so ofc)."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738573445,
                "cdate": 1700738573445,
                "tmdate": 1700738641303,
                "mdate": 1700738641303,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "E4pkG5yw0c",
            "forum": "z7K2faBrDG",
            "replyto": "z7K2faBrDG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2733/Reviewer_wfqs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2733/Reviewer_wfqs"
            ],
            "content": {
                "summary": {
                    "value": "The paper seems to measure the perceptual scale of spatial frequency, orientation, and synthetic texture interpolation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problem is of fundamental importance.\n\n2. The reviewer likes reading the introduction, especially the literature review."
                },
                "weaknesses": {
                    "value": "1. The reviewer has read the paper multiple times but fails to comprehend the main contributions. \n\n2. The authors claim to have a convergence theorem, but what are the implications and practical relevance? Specifically, a Gaussian field is assumed, but high-dimensional natural images are highly non-Gaussian.\n\n3. The function $\\psi$ in Eq. (2) as the perceptual scale is tested in a very constrained scenario, without comparing to competing methods. For example, a recent computational model of the contrast sensitivity function considers spatio-temporal frequency, eccentricity, luminance, and area [C1].\n\n4. Moreover, the experimental results regarding texture interpolation are performed in a discriminative setting, not from a generative perspective (i.e., texture synthesis).\n\n\n[C1] stelaCSF: A unified model of contrast sensitivity as the function of Spatio-Temporal frequency, Eccentricity, Luminance and Area,\n SIGGRAPH 2022."
                },
                "questions": {
                    "value": "1. The goals of the paper should be more precise.\n\n2. The comparison to previous methods should be performed, and performed in a comprehensive way.\n\n3. The authors may want to test their models on natural photographic texture images, besides the synthetic and simplistic ones (as shown in Figs. 1 and 2)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N.A."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2733/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722785345,
            "cdate": 1698722785345,
            "tmdate": 1699636215758,
            "mdate": 1699636215758,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mG5qnQsCq4",
                "forum": "z7K2faBrDG",
                "replyto": "E4pkG5yw0c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "response 1"
                    },
                    "comment": {
                        "value": "Thank you for reading our manuscript and for your feedback. We appreciate that you liked the introduction and that you have a good opinion about the importance of our work. \n\n**Response to weaknesses**\n\n> The reviewer has read the paper multiple times but fails to comprehend the main contributions. \n\nOur work has three main contributions that are detailed in the last section of the introduction. We have updated the section to clarify our contributions. Here is a reformulation of these contributions :\n\n1. Proposition 1 together with Proposition 3 allow to justify that the common assumption behind Bayesian modeling in psychophysics, that is the observer has a univariate representation of the distribution of the parameter of interest (*e.g.* spatial frequency), is compatible with the assumption that an observer is measuring spectral energy distribution of the image. To clarify, these assumptions correspond to a decision about the nature of measurement $M$ in the model given by Equation (2). \n\n2. In the model given by Equation (2), the function $\\psi$ is in fact measurable using the MLDS experiment. This has not been previously used in the context of this model so we are providing a clear link between theory and experiment. From the theory and with extra assumptions about what are the measurements $M$ we can derive predictions for the perceptual scale $\\psi$. Thanks to contribution (i), in the case of GRF stimuli, assuming that $M$ are the whole image $F$ or the random variables $Z$ or $\\Theta$ lead to the same prediction.\n\n3. We propose to go further than just perceptual distances by comparing the model prediction of perceptual scale (which can be obtained as long as you have a generative model) to the empirical perceptual scales. This is deeper than distances because we are comparing the model behavior along a geodesic in the space of images to the perception of this geodesic. The AMS score introduced in Section 3.2 could serve as a measure of how well a model is aligned to visual perception.\n\n> The authors claim to have a convergence theorem, but what are the implications and practical relevance? Specifically, a Gaussian field is assumed, but high-dimensional natural images are highly non-Gaussian.\n\nThe convergence theorem has been previously proven already. Here we explain how this result justifies the compatibility between different hypotheses made in Bayesian modeling of perception about an observer's measurement. See contribution (i) above. \nThe result hold for stationary Gaussian images only. The question remains open for more complex images. Yet, we show that when this result does not hold it is necessary to make extra assumption about the observer's measurements and without the mathematical result, different assumptions could lead to different predictions  (Figure 6). \n\n> The function in Eq. (2) as the perceptual scale is tested in a very constrained scenario, without comparing to competing methods. For example, a recent computational model of the contrast sensitivity function considers spatio-temporal frequency, eccentricity, luminance, and area [C1].\n\nThank you for pointing to stelaCSF model. We will cite it as an alternative line of work. The work is interesting, yet the goal is totally different from our. The stelaCSF model is a descriptive model (*i.e.* it does not provide an explanation for why the contrast functions are like they are) aiming at better detecting visual artifacts in computer vision applications such as Virtual and Augmented Reality. In contrast, our aim is to provide and test a normative model of perception (*i.e.* we assume that perception is driven by the inference of parameters from a generative model). To this date and up to our knowledge, we are not aware of any alternative (normative) models that provide predictions of the perceptual scales. Though, we are comparing different sub-assumption of our model about the nature of measurement $M$ (Figure 6b). In addition, the CSF is a measure of the limit of the perceptual system (between seen and not seen) while the perceptual scale is a measure of perceptual sensitive inside the window of visibility (Watson 1986)\n\n\n> Moreover, the experimental results regarding texture interpolation are performed in a discriminative setting, not from a generative perspective (i.e., texture synthesis).\n\nTo be precise, we are actually synthesizing interpolated textures using a generative model (Vacher *et al.* 2020) and the generated textures are used in the psychophysical experiment."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700342795832,
                "cdate": 1700342795832,
                "tmdate": 1700342795832,
                "mdate": 1700342795832,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "83CRHq9rTj",
            "forum": "z7K2faBrDG",
            "replyto": "z7K2faBrDG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2733/Reviewer_AMLy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2733/Reviewer_AMLy"
            ],
            "content": {
                "summary": {
                    "value": "This paper shows that the assumption that an observer has an internal representation of univariate parameters such as spatial frequency or orientation while stimuli are high-dimensional does not lead to contradictory predictions when following a theoretical framework. The perceptual scale is found to correspond to the transduction function in this framework and is related to the Fisher information of the generative model underlying perception. The research suggests that the stimulus power spectrum largely influences the perceptual scale. Furthermore, the study proposes that measuring the perceptual scale can help estimate the perceptual geometry of images, going beyond simple distance measurements to understand the path between images."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- A theoretical analysis on the perceptual scale in the case of GRFs was performed.\n- Different scaling experiments involving GRF and naturalistic textures were conducted."
                },
                "weaknesses": {
                    "value": "- The theoretical analysis is performed for the case of GRFs, which does not apply to naturalistic textures (Note that Gaussian textures are a very limited class of images). Most of the Propositions are special cases of previous work, so what is the theoretical contribution of this work?\n- The different scaling experiments only involve a small set of pairs and 5 naive participants. There may be insufficient data for detailed analysis, and actually results were presented in Sec. 3 without any analysis.\n- The paper appears to be hastily written with many typos (e.g., page 5: e have -> we have; Fig. 2: the colors are wrong, and the caption's description conflicts with the main body, Page 8: Proposition 2 and Proposition 2; ...) and symbols are not always well-defined or explained."
                },
                "questions": {
                    "value": "This looks like careful and sophisticated work at first glance. I did not notice major defects in the paper, to my knowledge. However, the paper is difficult to follow and would benefit from careful editing.\nSince I do not have a solid background in this area, I cannot confidently evaluate the significance here.\nIt may be better if the authors can write their manuscript from the point of view of a general researcher in ICLR.\n\nAdditional question: Can the authors explain more on how measuring the perceptual scale helps estimate the perceptual geometry of images? This is claimed in the Abstract but rarely mentioned in the main body."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2733/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2733/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2733/Reviewer_AMLy"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2733/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698738927950,
            "cdate": 1698738927950,
            "tmdate": 1699636215653,
            "mdate": 1699636215653,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wmx5lbP2GV",
                "forum": "z7K2faBrDG",
                "replyto": "83CRHq9rTj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2733/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thanks you for reading our manuscript and for your feedback. We appreciate that you liked our theoretical approach.\n\n**Response to weaknesses**\n\n> The theoretical analysis is performed for the case of GRFs, which does not apply to naturalistic textures (Note that Gaussian textures are a very limited class of images). \n\nWe acknowledge that from the point of view of computer vision, Gaussian textures allow to synthesized a very limited class of images, mostly what is known as micro-textures (see Galerne 2011). Yet, in vision study most experiments are performed using simple and/or deterministic stimuli such as drifting grating, moving dots, moving bars, sparse Gabor functions... Moving toward naturalistic stimuli or even better natural images is still out of reach as we are not yet able to make sense of how to control the generation of those images. We have tool to do so thanks to deep generative models but they work as a black boxes which is not satisfactory for vision experiment purposes. \n\n> Most of the Propositions are special cases of previous work, so what is the theoretical contribution of this work?\n\nWe have made no new theoretical contributions to the fields of information theory (or more generally to mathematics) though we apply existing results to the theory of perception. In particular the consequence of Proposition 1 have never been explained previously. In the context of our paper, this shows that different hypothesis about the internal representation used by an observer leads to similar results though this is not always the case (*e.g.* see Vacher *et al.* 2018) \n\n> The different scaling experiments only involve a small set of pairs and 5 naive participants. There may be insufficient data for detailed analysis, and actually results were presented in Sec. 3 without any analysis.\n\nThe number of participants is enough for this type of experiments (Devinck & Knoblauch 2012). We tested 15 pairs (3 in Fig.3, 10 in Fig. 4 and Fig. 5) which is a good sample size to start testing our model. The current work is not enough in term of experiment and we expect to test it further in a future work that will be more focused on experiments.\n\n> The paper appears to be hastily written with many typos (e.g., page 5: e have -> we have; Fig. 2: the colors are wrong, and the caption's description conflicts with the main body, Page 8: Proposition 2 and Proposition 2; ...) and symbols are not always well-defined or explained.\n\nWe apologize for the typos and the wrong color in the captions. The second proposition 2 was refering to the one you can find in the appendix. This has been corrected, thank you ! Reviewer gWW2 has also highlighted that $s$ wasn't defined in Equation (1). This has also been corrected.\n\n**Response to questions**\n\n1. ICLR is a wide and diverse community in which there is a subset that is interested in perception and is familiar with the cited literature. The goal of mentioning perceptual distances and geometry is to attract other people from the community who are interested in that topic but we would like to show the existence of tools to actually measure those distances (but we are doing more than that here) and that it is not sufficient to claim that a distance is a perceptual one. Too many papers are doing so without perceptual experiments.\n\n2. Using texture interpolations (Vacher *et al.* 2020) we are controlling the change of the stimulus along a geodedic between two textures. Therefore, we are exploring a line on the manifold of natural textures (characterized by VGG feature statistics). The measure of perceptual scale reflects the sensitivity to this change. If we were to find a metric to interpolate between textures that leads to a linear perceptual scale, this would mean that we have found a representation of textures and a metric that is accounting for human perception (not the distance between textures but the sensitivity along the path joining those textures). \n\n**Extra references**\n\nDevinck, F., \\& Knoblauch, K. (2012). A common signal detection model accounts for both perception and discrimination of the watercolor effect. Journal of Vision, 12(3), 19-19"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700342661635,
                "cdate": 1700342661635,
                "tmdate": 1700415275777,
                "mdate": 1700415275777,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jvdmh42HEU",
                "forum": "z7K2faBrDG",
                "replyto": "wmx5lbP2GV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2733/Reviewer_AMLy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2733/Reviewer_AMLy"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634730051,
                "cdate": 1700634730051,
                "tmdate": 1700634730051,
                "mdate": 1700634730051,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wd0L81rW2q",
            "forum": "z7K2faBrDG",
            "replyto": "z7K2faBrDG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2733/Reviewer_gWW2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2733/Reviewer_gWW2"
            ],
            "content": {
                "summary": {
                    "value": "The paper concerns itself with perceptual measures, in particular the perceptual distance between images showing what to me look like \"noise\" images each with a dominant spatial frequency. A theoiry is developed, and then tested experimentally."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I very much approve of the style of the experiment.\nAnd I applaud all efforts to measure human-based distances."
                },
                "weaknesses": {
                    "value": "I am not fully convinced by the model - the departure from human measures is significant.\n\nThe stimulli are very limited - gray scale images of textures that look to me like band-limited noise.  I am not at all sure how I should generlise any reults in this paper to general images. That is, it is not clear the model is general. \n\nI suppose what I would really like to see would be something akin to a just-noticable difference, and then to build a measure in frequency space based on jnds - by analogy to jnd in colour space.\n\n--\n\nNot all equations are not numbered, which is a presentation error because it makes discussion hard."
                },
                "questions": {
                    "value": "What does the parameter \"s\" mean?\n\nI do not understand the experiment. Three stimulli were presented with s1 < s2 < s3.  I suppose this means the parameter s somehow orders the stimulli, but I don't know how.  Then participants are required to pair either (s1,s2) or (s2,s3). Why remove the (s1,s3) option? (It's removal may bias results.)\n\nOverall, I get the impression the the authors have conducted perceptually experiments (on a very low number of participants) and that this paper is probably better suited to one of the perceptual psychology forums.  But, I found it hard to read, so I could very easily be wrong."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2733/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2733/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2733/Reviewer_gWW2"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2733/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698954486599,
            "cdate": 1698954486599,
            "tmdate": 1699636215596,
            "mdate": 1699636215596,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wrKaZgqv4c",
                "forum": "z7K2faBrDG",
                "replyto": "wd0L81rW2q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "We would like to thank you for reading our manuscript and for your feedback. We appreciate that you liked the efforts we put in collecting human perceptual data.\n\n**Response to weaknesses**\n\n> I am not fully convinced by the model - the departure from human measures is significant.\n\n The core of the model is Equation (2) (see paragraph Fisher information in neural populations). The most recent development of this model has been done by Wei and Stocker (2017) who provide several correct predictions of previous psychometric measures. Our contribution is to use this model to predict the results of MLDS experiments which has been proposed back in 2003 by Maloney and Yang. The model qualitatively explains part of the observation (positive score in Fig. 6). Yet, there are indeed quantitative discrepancies between model and data (Fig. 4) that need to be addressed in future work. Note in addition, that our model has nothing to account for individual variability (see individual results that we added in the supplementary material)\n\n\n> The stimulli are very limited - gray scale images of textures that look to me like band-limited noise. I am not at all sure how I should generlise any reults in this paper to general images. That is, it is not clear the model is general. \n\n We are using two types of texture stimuli.\n\nFirst, we are indeed using band limited noise stimuli (see Figure 1). Here, we know what to expect by changing the spatial frequency mode. Though, bandwidth parameters ($b_z$, $\\sigma_\\theta$) have never been tested before so even if the stimuli are somehow simple the experiments are new.\n\nSecond, the textures used in the rest of our experiments are naturalistic textures synthesized by state-of-the-art texture synthesis algorithms (Vacher *et al.* 2020). This type of texture synthesis algorithms have been been widely used in vision studies (Freeman *et al* 2011, Wallis *et al* 2017, Vacher *et al* 2020, Ziemba *et al* 2021).\n\n\n> I suppose what I would really like to see would be something akin to a just-noticable difference, and then to build a measure in frequency space based on jnds - by analogy to jnd in colour space.\n\nMeasuring a perceptual scale with MLDS is different from measuring JND (with other methods like staircase of 2AFC experiment). Yet, it provides similar but more detailed and precise information as the derivative of the perceptual scale is a measure of sensitivity (see Aguilar *et al.* 2017). One of our motivation for conducting this work is that we believe the MLDS method is powerful in providing complementary data that are important to depict the full relationship between images and human percepts.\n\n\n**Response to questions** \n\n1. We apologize for not numbering all equations. We have updated the manuscript accordingly.\n\n2. We apologize for introducing the parameter $s$ in Equation (1) without explaining what is is. The parameter $s$ is the stimuli. It was stated just before Equation (2). We have moved this description right after Equation (1).\n\n3. We use the method of triads described in Knoblauch and Maloney (2008). The stimulus $s_2$ plays the role of a reference stimulus and the method does not introduce bias because all triplets (except non-informative ones) are tested in an experiment. We are not giving a lot of details about the experiment because we believe this is not the most interesting aspect of our paper as we are using a previously described protocol.\n\n4. As few as 5 participants is considered appropriate for this type of psychophysical experiments (Devinck & Knoblauch 2012) . We provided the results for each individual in the supplementary material to illustrate inter-participant variability. \n\n**Extra References**\n\nDevinck, F., \\& Knoblauch, K. (2012). A common signal detection model accounts for both perception and discrimination of the watercolor effect. Journal of Vision, 12(3), 19-19\n\nWallis, Thomas SA, et al. \"A parametric texture model based on deep convolutional features closely matches texture appearance for humans.\" Journal of vision 17.12 (2017): 5-5.\n\nZiemba, Corey M., and Eero P. Simoncelli. \"Opposing effects of selectivity and invariance in peripheral vision.\" Nature communications 12.1 (2021): 4597."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700342571662,
                "cdate": 1700342571662,
                "tmdate": 1700342571662,
                "mdate": 1700342571662,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Fh1hRKzdcz",
                "forum": "z7K2faBrDG",
                "replyto": "X8YEzTkUIi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2733/Reviewer_gWW2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2733/Reviewer_gWW2"
                ],
                "content": {
                    "title": {
                        "value": "Not moved me much"
                    },
                    "comment": {
                        "value": "I thank you for your repsonses.\nFor me though, it has not much moved my opinion.\nThe model fit is not good, even to the very limited stimulii you have used.\nThis raises considerable doubt over the ability to generalise.\nWhile I appreciate your comments regarding JND, I continue to argue that a perceptual measure in \"texture space\" should be premised on JND, because JND measures provide the equivalent of a unit ruler. Unit rulers are the basis of measurements.\n\nPersonally I think perceprtual and subjective distances are under explored in general, but are very important.\nSo although I think the work - or at least its description - falls short of publishable standard on this occasion,\nI wish you well and hope to read material from you in the near future."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668576022,
                "cdate": 1700668576022,
                "tmdate": 1700668576022,
                "mdate": 1700668576022,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]