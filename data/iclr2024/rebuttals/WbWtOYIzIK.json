[
    {
        "title": "Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models"
    },
    {
        "review": {
            "id": "XXHlBvxLUN",
            "forum": "WbWtOYIzIK",
            "replyto": "WbWtOYIzIK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4686/Reviewer_HvjK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4686/Reviewer_HvjK"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces \"Knowledge Card\", a modular framework designed to augment large language models (LLMs) with up-to-date, factual, and relevant knowledge. Knowledge cards are trained on specific domains and sources. These knowledge cards act as parametric repositories and are selected during inference to provide background knowledge to the base LLM. The paper claims state-of-the-art performance on six benchmark datasets, demonstrating the effectiveness of the Knowledge Card framework in dynamically synthesizing and updating knowledge across various domains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. As an alternative for retrieval based method, knwoledge card allows for the dynamic synthesis and updating of knowledge from various domains, which is a significant advancement over static general-purpose LLMs.\n2. The method demonstrates state-of-the-art performance on six benchmark datasets. The results indicate that it is beneficial for numerous knowledge-intensive tasks, especially in situations that require the latest and accurate information.\n3. Designing the relevance selector, pruning selector, and factuality selector to evaluate dimensions of relevance, brevity, and factuality encompasses a broader and more extensive range than considered in previous methods."
                },
                "weaknesses": {
                    "value": "The benchmarks and datasets tested in the paper primarily focus on natural language understanding tasks, lacking more results on generative tasks."
                },
                "questions": {
                    "value": "The bottom-up approach and top-down approach mentioned in the text each have their own advantages and disadvantages. Is it possible to combine the two?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4686/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4686/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4686/Reviewer_HvjK"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4686/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698404274454,
            "cdate": 1698404274454,
            "tmdate": 1699636449952,
            "mdate": 1699636449952,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sy762HxmIV",
                "forum": "WbWtOYIzIK",
                "replyto": "XXHlBvxLUN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4686/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4686/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their thoughtful comments and feedback.\n\n> The benchmarks and datasets tested in the paper primarily focus on natural language understanding tasks, lacking more results on generative tasks.\n\nThe three tasks and six datasets in this work focus on classification and open-domain QA tasks (while open-domain QA might be considered as a generation task). While we did not identify good resources to test out LLM knowledge abilities in a long-form generation setting that\u2019s fitting in this work, we believe that Knowledge Card would be better at knowledge generation tasks as well thanks to the modular knowledge cards and the two integration approaches. Our goal is to demonstrate the modular Knowledge Card approach works for knowledge-intensive understanding tasks, while future work can explore expanding it to generative tasks.\n\n> The bottom-up approach and top-down approach mentioned in the text each have their own advantages and disadvantages. Is it possible to combine the two?\n\nOne straightforward way to combine them would be: in each step of top-down, the LLM proposes multiple knowledge cards as candidates, then employ the bottom-up approach with the pool of these knowledge cards for knowledge generation. We conducted a quick exploration with this using the gpt-3.5-turbo LLM, finding out that \u201chaving LLMs propose multiple knowledge card candidates\u201d would need more than just zero-shot prompting. Thank you for proposing this interesting idea. We would love to explore this in our future work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4686/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699944123045,
                "cdate": 1699944123045,
                "tmdate": 1700188435422,
                "mdate": 1700188435422,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "81O2YPpesS",
            "forum": "WbWtOYIzIK",
            "replyto": "WbWtOYIzIK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4686/Reviewer_fqxe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4686/Reviewer_fqxe"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a modular framework augmented with a domain-specific knowledge module called Knowledge Card. They introduce two scenarios (top-down and bottom-up) for knowledge integration. They demonstrate consistent performance improvement across multiple datasets compared to existing retrieval-augmented language models and generated knowledge prompting approaches. They also provide an analysis of each proposed module."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The authors demonstrate improvement across various benchmarks such as general-purpose knowledge QA, misinformation detection, and midterm QA compared to existing retrieval-augmented language models and generated knowledge prompting approaches.\n- Through an ablation study, the authors demonstrate the effectiveness of each module and conduct an analysis for each module."
                },
                "weaknesses": {
                    "value": "- Language model based modules appear to entail potential risks. For example, knowledge cards based on a language model necessitate a retrieval-based factuality selector, and inaccuracies can arise in LLM-based yes/no decisions during the top-down knowledge integration process.\n- One of the major differences between this work and existing work based on retrieval or generation is the utilization of knowledge cards from multiple domains. Therefore, it would be beneficial to demonstrate performance trends based on the gradual accumulation of knowledge cards or the level of granularity of knowledge cards."
                },
                "questions": {
                    "value": "In Table 1, 2, and 3, there are different performance trends among the three Knowledge Card variations. Do the authors speculate about the potential reasons behind these results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4686/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4686/Reviewer_fqxe",
                        "ICLR.cc/2024/Conference/Submission4686/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4686/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698735328561,
            "cdate": 1698735328561,
            "tmdate": 1700456845505,
            "mdate": 1700456845505,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "k2c2JkJnar",
                "forum": "WbWtOYIzIK",
                "replyto": "81O2YPpesS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4686/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4686/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their thoughtful comments and feedback.\n\n> Language model based modules appear to entail potential risks. For example, knowledge cards based on a language model necessitate a retrieval-based factuality selector, and inaccuracies can arise in LLM-based yes/no decisions during the top-down knowledge integration process.\n\nWe agree that each step in Knowledge Card is not, and will never be, 100% accurate. However, we argue that the modular components in Knowledge Card could be seamlessly integrated with the future state-of-the-art and continuously improve. Thanks to the modularity of Knowledge Card, these errors and risks could be continuously mitigated with new factuality evaluation models, strategies for LLMs to abstain and seek information, and more.\n\n> One of the major differences between this work and existing work based on retrieval or generation is the utilization of knowledge cards from multiple domains. Therefore, it would be beneficial to demonstrate performance trends based on the gradual accumulation of knowledge cards or the level of granularity of knowledge cards.\n\nWe conduct new experiments to test out knowledge card accumulation with the misinformation dataset, 2-way setting, bottom-up approach, and the ChatGPT model. Starting from 0 knowledge cards (vanilla LLM), we add one knowledge card at a time and reevaluate performance.\n\n|        | vanilla | + PubMed | + IMDB | + BookCorpus | + News | + Wikipedia |\n|:------:|:-------:|:--------:|:------:|:------------:|:------:|:-----------:|\n| # card |    0    |     1    |    2   |       3      |    4   |      5      |\n|  BAcc  |   80.1  |   80.7   |  80.6  |     82.3     |  85.7  |     86.5    |\n|   MaF  |   70.5  |   70.6   |  71.2  |     72.9     |  73.1  |     75.3    |\n\nIt is demonstrated that the addition of knowledge cards, especially in-domain ones (News in this case), is helpful in improving the base large language model.\n\n> In Table 1, 2, and 3, there are different performance trends among the three Knowledge Card variations. Do the authors speculate about the potential reasons behind these results?\n\nWe argue that the bottom-up and top-down settings of Knowledge Card have their respective pros and cons. While bottom-up is especially good at multi-domain knowledge synthesis and better works with documents spanning multiple knowledge domains, top-down is better at iterative and selective knowledge solicitation. Specifically:\n\nTask 1, MMLU: \u201ctop-down generally outperforms bottom-up likely because MMLU contains math-related questions that do not necessitate external knowledge. This observation suggests that top-down\napproaches are better at tasks where external knowledge is not always necessary.\u201d\n\nTask 2, misinformation: \u201cbottom-up outperforms both variants of top-down, thanks to its methodology to jointly activate knowledge cards from various domains and enable multi-domain knowledge synthesis.\u201d\n\nTask 3, MidtermQA: when there is a specific in-domain knowledge card for a given task, the top-down approach is better as it could accurately pinpoint which knowledge card is most needed.\n\nWe have included some of the explanations in the current paper and will include these empirical evidence of bottom-up and top-down\u2019s pros and cons in Section 4 of the final version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4686/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699944017448,
                "cdate": 1699944017448,
                "tmdate": 1700430446275,
                "mdate": 1700430446275,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hDaCDm39pf",
                "forum": "WbWtOYIzIK",
                "replyto": "k2c2JkJnar",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4686/Reviewer_fqxe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4686/Reviewer_fqxe"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the additional experimental results and explanation. I revised the score because I believe the response resolved my curiosity."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4686/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700457071303,
                "cdate": 1700457071303,
                "tmdate": 1700457071303,
                "mdate": 1700457071303,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Nq7aNSXR2T",
            "forum": "WbWtOYIzIK",
            "replyto": "WbWtOYIzIK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4686/Reviewer_PFBW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4686/Reviewer_PFBW"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes knowledge cards which are essentially language models finetuned on specific domains. These knowledge cards can be probed to generate or recite information the domain-specific LMs have memorized (for a given question) without having to explicitly store encoded representations for each document separately. The authors also propose three knowledge selectors which heuristically govern how to aggregate knowledge from different knowledge cards to generate a final answer. The three knowledge selectors include: 1.) relevance selector chooses relevant (generated) documents given a query 2.) pruning selector to summarize generated documents to fir in the given context length and 3.) factuality selector is used to filter out hallucinating documents based on some entailment score. The authors test their system on several tasks including MMLU, MidtermQA and LUN for hallucination detection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.) The idea to train individual language models seems novel. Modular knowldge organization can be helpful for making progress on continual learning."
                },
                "weaknesses": {
                    "value": "1.) The paper is missing many details and is hard to follow at times, especially in Section 2.1 and 2.3. (specific issues in questions section)\n\n2.) Existing models that employ similar ideas of modular knowledge organization https://arxiv.org/pdf/2108.05036.pdf, https://arxiv.org/pdf/2203.06311.pdf and datasets that test temporal aspects https://arxiv.org/pdf/2110.03215.pdf are not compared."
                },
                "questions": {
                    "value": "1.) It is unclear how information will be stored in a knowledge card for unseen questions at test time\n\n2.) It appears that an entailment classifier is being used to obtain factuality score, can you elaborate how it is trained?\n\n3.) Can you elaborate more on how the MidtermQA dataset was curated? \n\n4.) In bottom up approach (Figure 1) how would a passage about \"San Mateo's senior senator\" be looked up on prompting with just the query \"Who is the senior senator of Tom Brady\u2019s birth place?\". The term \"senior senator\" will not be sufficient as that would yield a very large number of documents and \"San Mateo\" term will only be obtained after first step in multi-hop retrieval.\n\n5.) Unlike bottom-up, top-down approach seems iterative where the classifier \"Do you need more information?\" is used to stop iteration. What does the prompt for this look like? How does it perform on a held-out set.\n\n6.) How is it ensured that all the knowledge cards are being useful? It has been shown that wikipedia can often answer simple questions from other domains, which MMLU dataset often tests.\n\nTypos:\nhandful of knowledge cardds ->  handful of knowledge cards"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4686/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4686/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4686/Reviewer_PFBW"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4686/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818182094,
            "cdate": 1698818182094,
            "tmdate": 1700625550931,
            "mdate": 1700625550931,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1HcOY5bMrj",
                "forum": "WbWtOYIzIK",
                "replyto": "Nq7aNSXR2T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4686/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4686/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (1/2)"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their thoughtful comments and feedback.\n\n> Existing models that employ similar ideas of modular knowledge organization https://arxiv.org/pdf/2108.05036.pdf, https://arxiv.org/pdf/2203.06311.pdf and datasets that test temporal aspects https://arxiv.org/pdf/2110.03215.pdf are not compared.\n\nWe would like to thank the reviewer for valuable pointers to related literature.\n\nSince Knowledge Card \u201cspecifically focuses on augmenting **black-box** LLMs to enrich their knowledge capabilities\u201d (Section 1, page 2), while the suggested Demix and ELLE, along with other modular works such as BTM [1] and ColD Fusion [2], operates with **white-box** access to the language model for pretraining and fine-tuning or requiring token probabilities, we argue that they are not comparable. By focusing specifically on the black-box setting, Knowledge Card is compatible with the state-of-the-art proprietary models behind API calls while these approaches are not.\n\nThe suggested dataset CKL features a collection of datasets (cc-news, LAMA, invariant-LAMA, etc.) that are curated before 2022, while the knowledge cutoff of our base LLMs (Codex, GPT-3.5, ChatGPT) is after that. We did look at other temporal datasets such as TempLAMA [3] and RealTimeQA [4]. While none of them is especially suitable for creating a temporal misalignment in our work, we propose the MidtermQA dataset, focusing on events that happened in late 2022 and early 2023 to investigate Knowledge Card\u2019s ability to incorporate new and emerging events through a domain-specific knowledge card.\n\nWe agree that these suggested works and works on LLM continual learning [5-8] are indeed relevant and we will add them to the related works to cite and discuss these works, better positioning Knowledge Card in their context.\n\n[1] Li, Margaret, et al. \"Branch-train-merge: Embarrassingly parallel training of expert language models.\" arxiv 2022.\n\n[2] Don-Yehiya, Shachar, et al. \"ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning.\" ACL 2023.\n\n[3] Dhingra, Bhuwan, et al. \"Time-aware language models as temporal knowledge bases.\" TACL 2022.\n\n[4] Kasai, Jungo, et al. \"RealTime QA: What's the Answer Right Now?.\" arxiv 2022.\n\n[5] Qin, Yujia, et al. \"ELLE: Efficient Lifelong Pre-training for Emerging Data.\" ACL 2022, Findings.\n\n[6] Jang, Joel, et al. \"Towards Continual Knowledge Learning of Language Models.\" ICLR 2022.\n\n[7] Qin, Yujia, et al. \"Recyclable Tuning for Continual Pre-training.\" arxiv 2023.\n\n[8] Ke, Zixuan, et al. \"Continual Pre-training of Language Models.\" ICLR 2023.\n\n> It is unclear how information will be stored in a knowledge card for unseen questions at test time.\n\nFor unseen questions from new and emerging knowledge domains, we incorporate a newly trained knowledge card into the framework to expand its access to new knowledge. For example, the MidtermQA dataset focuses on the news event in late 2022 while LLMs\u2019 knowledge cutoff is earlier than that. We demonstrate that an additional knowledge card trained on midterm election news coverage could significantly improve performance and expand LLM knowledge. (Table 7, Section 4)\n\n> It appears that an entailment classifier is being used to obtain factuality score, can you elaborate how it is trained and if it is\n\nTwo entailment classifiers are adopted to obtain the factuality scores.\n\nFor **summarization factuality**, we use the state-of-the-art FactKB [1] to evaluate whether the condensed summary accurately reflects the knowledge document generated by knowledge cards. We directly use the publicly available checkpoint of FactKB, which was pretrained on knowledge base data and fine-tuned on summarization factuality datasets.\n\nFor **retrieval-augmented factuality**, we use the VitaminC [2] entailment classifier to evaluate whether the generated knowledge is supported by retrieved evidence from Wikipedia. We directly use the publicly available checkpoint of VitaminC, which was trained with contrastive evidence and a spectrum of fact-checking tasks with varying granularity to enhance robustness.\n\nWe refer the reviewers to [1-2] for full training details. The review text seems to be cut off, we would be happy to answer the latter half of the question.\n\n[1] Feng, Shangbin, et al. \"Factkb: Generalizable factuality evaluation using language models enhanced with factual knowledge.\" EMNLP 2023.\n\n[2] Schuster, Tal, et al. \"Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence.\" NAACL 2021."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4686/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699943344357,
                "cdate": 1699943344357,
                "tmdate": 1700188412740,
                "mdate": 1700188412740,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mBohZ9K8yd",
                "forum": "WbWtOYIzIK",
                "replyto": "Nq7aNSXR2T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4686/Reviewer_PFBW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4686/Reviewer_PFBW"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for answering my questions. I have raised the scores accordingly."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4686/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625576613,
                "cdate": 1700625576613,
                "tmdate": 1700625576613,
                "mdate": 1700625576613,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]