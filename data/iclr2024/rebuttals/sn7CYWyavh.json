[
    {
        "title": "Whole-song Hierarchical Generation of Symbolic Music Using Cascaded Diffusion Models"
    },
    {
        "review": {
            "id": "QVOWwMUusP",
            "forum": "sn7CYWyavh",
            "replyto": "sn7CYWyavh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3963/Reviewer_EK1V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3963/Reviewer_EK1V"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a hierarchical strategy for symbolic music generation. Specifically, their approach involves a cascade of conditional diffusion models which iteratively generate a series of interpretable representations in a coarse-to-fine fashion. The authors compare their proposed approach to strong baselines through both quantitative metrics and a qualitative user study, demonstrating promising performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is a nice paper overall. Among its virtues are (1) **the quality of the results**, (2) **simplicity of the approach**, (3) **usefulness for controllable generation**, and (4) **clarity of the writing**.\n\n**Result quality**. The proposed method achieves impressive results in both the quantitative evaluation and subjective tests, especially compared to strong baselines. Moreover, the included sound examples are quite compelling, and the contribution of the proposed hierarchical approach to the final outputs is immediately apparent.\n\n**Simplicity**. While the design of the hierarchical approach is somewhat complex, the proposed generative modeling approach is satisfyingly simple, with each stage using the same basic setup despite their structural differences, and later stages adding in well-motivated mechanisms to address clear issues (e.g., autoregressive component to generate locally but with global consistency).\n\n**Usefulness for control**. The proposed hierarchy (both in the data representation and modeling) is helpful for enabling long-form generation w/ global structure. However, it has an additional benefit of enabling interpretable manipulation of intermediate representations. While the authors don\u2019t specifically explore interaction, it is clear that this aspect of the approach could be very powerful for users.\n\n**Well-written**. This paper is extremely clear and well-written, especially relative to the median paper on music generation. Symbolic music generation, especially work that focuses on interpretability, tends to be a very messy subject with lots of in-the-weeds details that often manifest as confusing and poorly-written papers. All symbolic music gen papers tend to require substantial music expertise to fully understand, but this paper does a fantastic job of both minimizing the expertise needed and being exceptionally clear in overall formulation."
                },
                "weaknesses": {
                    "value": "There are two primary weaknesses with this work: (1) **unclear if model is copying**, and (2) **impact is limited by data availability**\n\n**Unclear if model is copying**. This model is trained on a very small amount of data, just 909 songs. Despite this, subjectively speaking, the results from the proposed model are quite good. It seems quite plausible that the model is overfit to the training data and producing copies or near-copies. Can the authors provide any evidence that this is not happening?\n\n**Impact limited by data availability**. A broader issue is that extracting the proposed hierarchical representation requires rich annotations aligned with the raw notes: chords, melody, key, phrase boundaries, etc. Some of this the authors extract (e.g. key / phrase boundaries) and some of this comes from human labels (e.g. chords and melody). This limits the overall applicability of the approach to music datasets with such rich labels (POP909), and prevents application to much larger symbolic music datasets (Lakh) which cover more styles.\n\nLow-level comments:\n\n- Table 1 could describe M, \u03b3, \u03b4 in the caption\n- Table 1 would be well-complemented by a Figure 1 showing the languages visually and their relationships to music scores / one another\n- Sound examples page could have qualitative examples from baselines\n- It\u2019s a shame that there is no \u201coverall structure\u201d music in the whole-song subjective study, so users can rate if models produce music with clear long-form structure."
                },
                "questions": {
                    "value": "Can the authors provide quantitative or qualitative evidence that the model is not overfit to the training data? If so, I would consider raising my score from a 6 to an 8."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3963/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3963/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3963/Reviewer_EK1V"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3963/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698724233971,
            "cdate": 1698724233971,
            "tmdate": 1700585707447,
            "mdate": 1700585707447,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7WmWxTit1e",
                "forum": "sn7CYWyavh",
                "replyto": "QVOWwMUusP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EK1V"
                    },
                    "comment": {
                        "value": "Thank you for your careful review and acknowledging our contributions. In this rebuttal, we will respond to the weakness and questions: 1) plagiarism and overfitting issue and 2) dataset limitation. We also thank you for leaving some low-level comments which we will consider in paper revision.\n\n### **Plagiarism issue**\n\nThank you for pointing out this issue! Since this is really important and is also mentioned by other reviewers, we write a response to all reviewers to answer the question. Please kindly check the **response to all reviewers** for more detail. In short, the current results show that our model does not suffer from plagiarism problem. \n\n### **Dataset limitation**\n\nCurrently, we use POP909 to study whole-song generation because our current focus is on pop songs and the dataset contains relatively accurate annotation. With the help of current MIR algorithms, we can achieve not only phrase structure analysis, but also score quantization, melody extraction and so on. (Actually most of the annotation from the POP909 dataset is extracted using algorithms.) In our ongoing project, we are extracting annotations from larger MIDI datasets and train our model on these datasets. We expect the performance will be better with more training data.\n\nWe admit there is a limitation for our current methods that our hierarchical language only works for pop songs (e.g., not for classical) and does not directly work for multi-track music. In our future work, we hope to define more general hierarchical languages to model those types of music."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411229887,
                "cdate": 1700411229887,
                "tmdate": 1700411229887,
                "mdate": 1700411229887,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m2EOJBMmst",
                "forum": "sn7CYWyavh",
                "replyto": "7WmWxTit1e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3963/Reviewer_EK1V"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3963/Reviewer_EK1V"
                ],
                "content": {
                    "title": {
                        "value": "Changed score"
                    },
                    "comment": {
                        "value": "Thank you for the response. I am reasonably satisfied by the initial plagiarism analysis and explanation about applicability to larger datasets, and have increased my score. I remain surprised that results can be this strong given the small size of the dataset, and encourage the authors to eventually add a more detailed plagiarism analysis to their paper as promised."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585827081,
                "cdate": 1700585827081,
                "tmdate": 1700585827081,
                "mdate": 1700585827081,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bhcmrZPmop",
            "forum": "sn7CYWyavh",
            "replyto": "sn7CYWyavh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3963/Reviewer_JPv8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3963/Reviewer_JPv8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to learn to generate a full pop song with piano accompaniment as a hierarchical generation process. This paper defines a music language or representation of 4 levels. The generation process is then defined into four stages: form, counterpoint, lead sheet and accompaniment. In each stage, a diffusion model is used as the backbone generative model, generating the image-like representation at that level."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper proposes a novel hierarchical representation of symbolic music.\n2. This paper proposes a novel task formulation of generating full-song symbolic music and has a good qualitative result and offers a wide range of controlability."
                },
                "weaknesses": {
                    "value": "The system, including its condition input, appears complex. The reliance on multiple pretrained models (as referenced in Section 3.3) might be cumbersome. It would enhance the paper's credibility if the authors could provide ablation studies both on the model's architecture and the efficacy of the control input."
                },
                "questions": {
                    "value": "I wonder how this method applies to a more general MIDI-like dataset. Or does it rely heavily on the POP909 dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3963/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3963/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3963/Reviewer_JPv8"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3963/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762485958,
            "cdate": 1698762485958,
            "tmdate": 1699636357288,
            "mdate": 1699636357288,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4mu9NG7flP",
                "forum": "sn7CYWyavh",
                "replyto": "bhcmrZPmop",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JPv8"
                    },
                    "comment": {
                        "value": "Thank you for your review and we are glad to see the strengths of the paper is well-summarized. In this rebuttal, we mainly respond to the weakness and questions.\n\n### **System complexity**\n\nWe admit our system is indeed complex. In this paper, we believe the r*ealization of compositional hierarchy* (the main contribution of our work) is the \u201cskeleton\u201d of music composition; and on top of it we add *autoregressive conditions* and *external conditions* as \u201cplugins\u201d. these \u201cplugins\u201d are necessary and currently they are modeled in a simple way. In the future, we hope *autoregressive condition* can be extended to represent motif development and *external condition* can be developed into a hierarchical cross-modal control interface.\n\n### **Ablation studies on model\u2019s architecture**\n\nIn our experiments, we ablated the assumption whether compositional hierarchy is necessary. We compared our proposed model with two types of baselines: \n\n1. Models without compositional hierarchy at all. This corresponds to the baselines: Polyff. and TFxl(REMI).\n2. Models with very simple compositional hierarchy (i.e., only phrase label as high-level language). This corresponds to the baselines: Polyff.+ph.l. and TFxl(REMI)+ph.l.\n\nBoth assumptions are implemented with diffusion models (more suitable to the current task) and Transformers (more conventional). \n\nMoreover, we ablate \u201cautoregressive condition\u201d in our preliminary experiments. We trained a version with only compositional hierarchy but no autoregressive condition. The generated result is promising but only within 128 beats (the time scope of stage 2). For example, if two verse sections are far apart (e.g., 40 measures apart), the realization of the Counterpoint will be very different and so the melody lacks coherency. That\u2019s why we incorporate autoregressive condition in our model architecture.\n\nWe see that there are other possible ways to define compositional hierarchy and it is important for this study to try more settings to ablate. Please let us know if there are other ablations you suggest and we can try to conduct the experiments as soon as possible.\n\n### **Ablation studies on the efficacy of the control input**\n\nWe train our model under both settings: 1) with external condition (under classifier-free guidance) and 2) without external condition. The performance of both models are similar when doing unconditional generation (without specifying latent codes of external conditions). The fact is also shown in the original classifier-free guidance paper [1]. \n\nThe effectiveness of the external condition is studied and shown in generation example in section A.4 (Figure 5 (g)(h), Figure 6 (h)(i), and Figure 7(e)). In the Polyffusion paper [2], the authors already conducted systematic experiments to show the control efficacy in diffusion-based music generation models. If we have more time, we can do systematic experiments for whole-song generation.\n\n### **Generalizability to other MIDI datasets**\n\nCurrently, we use POP909 to study whole-song generation because our current focus is on pop songs and the dataset contains relatively accurate annotation. With the help of current MIR algorithms, it is possible to annotate larger MIDI datasets and train our model on these datasets, which is actually our ongoing project. We expect the performance will be better with more training data.\n\nWe admit there is a limitation for our current methods that our hierarchical language only works for pop songs (e.g., not for classical) and does not directly work for multi-track music. In our future work, we hope to define more general hierarchical languages to model those types of music.\n\n[1] Ho J, Salimans T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598. 2022 Jul 26.\n\n[2] Lejun Min, Junyan Jiang, Gus Xia, Jingwei Zhao: Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls. ISMIR 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411146038,
                "cdate": 1700411146038,
                "tmdate": 1700411146038,
                "mdate": 1700411146038,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HIFlPOgmvx",
                "forum": "sn7CYWyavh",
                "replyto": "4mu9NG7flP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3963/Reviewer_JPv8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3963/Reviewer_JPv8"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Sorry for the late reply. Thank you for the response and explanation. For the efficacy of control: Thanks for pointing out the experiment results in section A.4. What I referred to was conducting experiments on getting quantitative statistics of the qualitative results in the comparisons in section A.4. That could be more convincing in showing the efficacy of the control."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623561673,
                "cdate": 1700623561673,
                "tmdate": 1700623561673,
                "mdate": 1700623561673,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "R8CnjLBgSP",
            "forum": "sn7CYWyavh",
            "replyto": "sn7CYWyavh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3963/Reviewer_JZXS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3963/Reviewer_JZXS"
            ],
            "content": {
                "summary": {
                    "value": "The authors showcase a system to generate complete pop songs in pianoroll format. The approach consists in splitting the generation into a hierarchy of four stages. The representation for each of this stage can be computed directly from the original pianoroll and a bespoke algorithm termed Tonal Reduction Algorithm is introduced to compute the so-called \"Conterpoint\" representation.\nThe different level of the hierarchy are then generated iteratively by conditioning on the preceding stages using a diffusion model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This article features a very well-engineered system. The generated pop songs are convincing and seem to capture well the style of the POP909 dataset.\nThe presentation website features lots of interesting examples that sound well.\nIt shows that this model is able to cover many use cases beyond generation from-scratch: from accompaniment generation based on texture to leadsheet generation."
                },
                "weaknesses": {
                    "value": "The main weaknesses seem the lack of details concerning the diffusion model and the sampling procedure.\nThis is even more problematic as it seems that the modeling process is not standard, with the diffusion model used to generate chunks of music in an autoregressive manner. \n\nIt is also unclear how the data is represented as it seems at first sight that the diffusion model used would be a Discrete diffusion model.\nAs such, it is not very accessible for people knowing the standard literature on diffusion models.\n\"We represent key by K \u2208 R2\u00d7M\u00d712, where tonic information and scale information are stored on the two channels\"\nIt may be interesting to emphasize on the non standard points .\n\n\nVery custom Tonal Reduction Algorithm. Seems very close from Polyffusion.\nThe results shown here may not be of interest for the broad ICLR community as the main contributions are mostly about symbolic music generation in a pianoroll format."
                },
                "questions": {
                    "value": "The diffusion model used auto regressively\n\nWhat are the pretrained models for chord progression, rhythmic pattern, accompaniment texture?\n\n\"Conterpoint\" term for the second stage is not appropriate as this stage describes the sketch of the melody and harmony t. \"Draft\" or \"Sketch\" stage may be more suitable?\n\nLeadsheet encoded as melody and chords? Are chords in string format?\n\nWhat is the sampling time for a whole song?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3963/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831412801,
            "cdate": 1698831412801,
            "tmdate": 1699636357200,
            "mdate": 1699636357200,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4u5qpHF8ZP",
                "forum": "sn7CYWyavh",
                "replyto": "R8CnjLBgSP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JZXS (part one)"
                    },
                    "comment": {
                        "value": "Thank you for your review and insight. In this rebuttal, we address the weaknesses in detail (in part one) and respond to the questions (in part two).\n\n### **Weakness 1: non-standard diffusion model and unclear data representation**\n\nThe diffusion model used in our method is the standard DDPM [1]. Polyffusion [2] has shown such method is an effective way to generate 8-measure music by regarding piano-roll as real-valued image data. (The training process is the same as DDPM. In inference the output of the generation will be quantized to discrete values and empirically such quantization operation does not yield syntax error [2].) In this study, we use the same method to represent music and its high-level languages. Data representation is discussed in section 3.1-3.2 and also in A.2. If there is other details about the model and data representation you think is missing, please let us know. We are happy to explain and make it clearer in the revised version.\n\n### **Weakness 2: Limited contribution for ICLR community**\n\nHierarchical music generation is the core problem and we propose to bring innovative methodology to solve it by 1) defining a music compositional hierarchy, and 2) training a corresponding cascaded generative model. The methodology is efficient in data usage and computational resource, and is more controllable and interpretable compared to other existing music generation methods. In this paper, we see Polyffusion as a tool to realize this goal. The Tonal Reduction Algorithm is a small but important design to make sure the hierarchical model is functioning. \n\nFor the ICLR community, hierarchical symbolic music generation, as a sequence generation problem, has a unique significance. As stated by Prof. LeCun [3], learning multiple levels of abstraction and hierarchical prediction is one of the main challenges to address for today\u2019s AI research. Symbolic music has a rich and ambiguous structure, and the current research in symbolic music can shed light on learning hierarchical structure in natural language, image, as well as in art and real-world long-term planning in general."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411451858,
                "cdate": 1700411451858,
                "tmdate": 1700418758749,
                "mdate": 1700418758749,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mVgxBLcyN4",
                "forum": "sn7CYWyavh",
                "replyto": "R8CnjLBgSP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JZXS (part two)"
                    },
                    "comment": {
                        "value": "### **Questions**\n\n**Q1: The diffusion model used auto regressively**\n\nAutoregressively applying diffusion model is indeed not a standard way. The method is the same as image inpainting and is first introduced in Polyffusion [2] as a way to generate coherent sequences. Such method is necessary for whole-song generation because as we generate lower level languages, the sequence becomes longer.\n\nThe autoregressive generation process is introduced in Algorithm 1. Here we try to explain it more intuitively. For example, a diffusion model can generate a sequence of fixed length T, equal to the image width.  In the autoregressive process, the model first generates [0, T]. Then the model shifts right by T // 2 and inpainst [T, T + T//2] conditioning on the generated part [T//2, T], and so on so forth.  \n\n**Q2: What are the pretrained models for chord progression, rhythmic pattern, accompaniment texture?**\n\nFor both latent codes of chord progression and accompaniment texture, we use the pre-trained encoders in [4]. For latent code of rhythmic pattern, we use the pre-trained encoder in [5]. These are introduced in A.4 and we will make it clearer in our revision.\n\n**Q3: \"Conterpoint\" term for the second stage is not appropriate as this stage describes the sketch of the melody and harmony t. \"Draft\" or \"Sketch\" stage may be more suitable?**\n\nThank you for the suggestion. We realize that the \u201cCounterpoint\u201d might not be precise. We will consider to use \u201cDraft\u201d or \u201cSketch\u201d. And please let us know if you have other idea since we want the name to be more specific.\n\nIn fact, \u201cCounterpoint\u201d is designed to emphasize the idea that the intermediate level music flow is represented by the interplay of melodic and harmonic development. From a Schenkerian perspective, we can regard melody and bass of the chord as a two-part counterpoint and the chord notes as \u201cfigured bass\u201d.  This is what we mean by \u201cCounterpoint\u201d here.\n\n**Q4: Leadsheet encoded as melody and chords? Are chords in string format?**\n\nThe *Lead sheet* is encoded as melody and chords. The chords are represented as piano-roll as discussed in section 3.2. Section A.2 shows an example of such data representation in piano-roll format.\n\n**Q5: What is the sampling time for a whole song?**\n\nWe train and inference our model on GeForce RTX 2080 Ti. It takes several minutes (5 - 20 minutes) to generate a song depending on the length. For example, it takes approximately 15 minutes to generate eight pieces (batch size=8) of 40-measure songs. On more powerful computing resources, we expect better performance.\n\n### **References**\n\n[1]  Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models. Advances in neural information processing systems. 2020;33:6840-51.\n\n[2] Lejun Min, Junyan Jiang, Gus Xia, Jingwei Zhao: Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls. ISMIR 2023.\n\n[3] LeCun Y. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review. 2022 Jun 27;62.\n\n[4] Ziyu Wang, Dingsu Wang, Yixiao Zhang, Gus Xia: Learning Interpretable Representation for Controllable Polyphonic Music Generation . ISMIR 2020: 662-669\n\n[5] Ruihan Yang, Dingsu Wang, Ziyu Wang, Tianyao Chen, Junyan Jiang, Gus Xia: Deep Music Analogy Via Latent Representation Disentanglement. ISMIR 2019: 596-603"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411506940,
                "cdate": 1700411506940,
                "tmdate": 1700411506940,
                "mdate": 1700411506940,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IeRA4KeNfx",
            "forum": "sn7CYWyavh",
            "replyto": "sn7CYWyavh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3963/Reviewer_QD5q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3963/Reviewer_QD5q"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new symbolic music generation system that can generate a full pop song. By defining four levels of music representations, the system generates piano roll from coarse to fine using a cascade diffusion model. The system is trained on POP909 and evaluated using objective metrics and a subjective listening test. The objective metrics show that the system can generate music with better long-term structural consistence. The listening test results show that the proposed system generates music of better quality."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper is a timely and significant contribution to the field of symbolic music generation. As far as I know, this is the first deep music generation model that can generate a full structural song given high-level structural hints.\n- The demo is impressive! Some of the samples are too good that I wonder if there is some overfitting issue (some nearest neighbor analysis might help clear these doubts).\n- The proposed model is clever. Figure 4 clearly shows how the musical compositional hierarchy imposes a proper inductive bias to the system."
                },
                "weaknesses": {
                    "value": "- A discussion on the limitations of the proposed system is missing. I see two main limitations: First, the musical compositional hierarchy adopted here is constrained to pop music. Second, the high-level form and structures still need to be provided.\n- One thing missing in the evaluation is some nearest neighbor analysis to check if the model is returning part of the training data directly.\n- The evaluation doesn't really measure the capability of whole-song generation, but there is no proper baselines as far as I know, so it's fine."
                },
                "questions": {
                    "value": "- (Section 1) \"and therefore we need to organize various music representations in a structured way.\" -> I cannot understand this sentence. Why do you mean by \"organize representations\"?\n- (Section 3.2) \"continuous\" -> I'm not sure what \"continuous\" means here. Are you using binary or real-valued piano rolls?\n- (Section 3.2) \"Both melody reduction and simplified chord progression ...\" -> How were these achieved? A pointer to the Appendix would be helpful here.\n- (Section 3.2) \"13 times\" -> Why 13 times? Isn't it 128/12 = 11 times?\n- (Section 3.3) \"We select Sk relevant music segments\nprior to t based on a defined similarity metric on X<k.\" -> Is this X^k or X^<k? The descriptions in this paragraph are somewhat confusing. From Figure 1, it seems like it's both  X^k and X^<k. Please clarify this.\n- (Algorithm 1) Isn't the song length M also an input?\n- (Section 4) \"40 measures\" -> How do you determine the number of measures to be generated?\n- (Section 5.1) \"... and segment them into 8-bar musical segments with a 1-bar hopping size.\" -> What is this segmentation step for?\n- (Section 5.3) \"Using pre-trained VAEs from Yang et al. (2019) and Wang et al. (2020).\" -> Are these the models you used to extract autoregressive controls?\n- (Section 5.3) How did you select the test inputs for the evaluation?\n\nHere are some other comments and suggestions:\n\n- (Section 1) \"(typically ranging from a measure up to a phrase)\" -> I don't think \"phrase\" has a strict definition.\n- (Section 2.3) It's good to scope this section properly as only symbolic music models are discussed here.\n- (Section 3.1) The musical compositional hierarchy discussed in this paper seems to be constrained to pop music. It would be help to discuss this limitation somewhere in the main text.\n- (Section 3.1) \"counterpoint\" -> I personally find this term confusing as we have melody and harmony here -- it sounds more like a \"reduced/simplified lead sheet\" to me.\n- (Section 3.3) \"The time scopes (image widths) of these diffusion models are more or less the same\" -> What do you mean by \"more or less\" the same? Please avoid such wordings.\n- (Section 4) Having this qualitative analysis before introducing the dataset is somewhat misleading. I don't even know what I should expect. Please consider rearranging the sections.\n- (Section 4) \"In Appendix A.4, ...\" -> I would love to see Figure 4 in the main text rather than Figure 2. The sheet music might be hard to understand for ICLR readers.\n- (Section 4) \"long (32 measures)\" -> This is still far from whole song evaluation.\n- (Section 5.3) \"whole-song (32 measures)\" -> I think 32 measures is still far from whole song generation.\n- (Figure 3) It would be great to also include the ground truth for 32-measure generation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Discussions on the copyright of the dataset is missing. Also, a nearest neighbor analysis is missing, which is important to check if the model is returning part of the training data directly."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3963/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3963/Reviewer_QD5q",
                        "ICLR.cc/2024/Conference/Submission3963/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3963/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699078392491,
            "cdate": 1699078392491,
            "tmdate": 1700619464974,
            "mdate": 1700619464974,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Wvqig1l3Mo",
                "forum": "sn7CYWyavh",
                "replyto": "IeRA4KeNfx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QD5q (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your careful review and very detailed feedback. In this rebuttal, we first respond to the weaknesses (in part one) and then answer each of the questions (in part one and two).\n\n### **Weakness 1: discussion of limitation is missing.**\n\nThank you for pointing out our current limitation. We admit the current work is limited to pop music and we will put it in our revision. \n\nOn the other hand, high-level form does not need to be given. In the training of stage one, the samples of *Form* are zero-padded to 256 measures. So in inference, the model can generate *Form* containing 256 measures and we derive the song length M by finding the first time step that contains all-zero entries. In section 4, we show music piece of 40 measures under given form for better visualization and space-saving purpose.\n\n### **Weakness 2: Evaluation of overfitting.**\n\nWe indeed see the evaluation of overfitting is crucial. We are conducting a systematic evaluation on the issue. Please see the **response to all reviewers** for more detail. In short, the current results show that our model does not suffer from plagiarism problem.\n\n### **Weakness 3: No evaluation method of whole-song generation.**\n\nDirectly evaluating whole-song generation is hard because 1) the evaluation of longer segments is more subjective; and 2) long music segments take a longer time to listen to and is hard to be memorized. In this paper, we propose to effectively evaluate whole-song generation by decomposing the evaluation problem into structure and quality evaluation:\n\n- Structure: a good structure usually implies clear phrase boundary, similarity between, e.g., two verses, and dissimilarity between, e.g., verse and chorus. These are evaluated by ILS in objective evaluation (Table 2) and \u201cBoundary clarity\u201d & \u201cPhrase similarity\u201d in subjective evaluation (Figure 3c&d).\n- Quality: we consider music quality in both short-term (8 measures) and long-term (32 measures) using criteria \u201cNaturalness\u201d, \u201cCreativity\u201d and \u201cOverall musicality\u201d (see Figure 3).\n\nAlso, in the evaluation, whole songs are generated under several condensed music form such that the length of the piece is balanced: on one hand, the generated pieces won\u2019t be too long (i.e., within 1\u201930\u2019\u2019); on the other hand, the music contains all necessary sections with repetition of verses or choruses.\n\n### **Questions**\n\n- **(Section 1) \"and therefore we need to organize various music representations in a structured way.\" -> I cannot understand this sentence. Why do you mean by \"organize representations\"?**\n    \n    The common sense of music hierarchies are ambiguous. There are a lot of possible music hierarchies (e.g., pitch contour, tonal structure or metrical structure), but these hierarchies are usually not compatible with one another and do not serve for generation purpose. In this paper, we rely on these concepts to define music compositional hierarchy that can directly used in music generation.\n    \n- **(Section 3.2) \"continuous\" -> I'm not sure what \"continuous\" means here. Are you using binary or real-valued piano rolls?**\n    \n    The data are represented as real-valued piano-rolls and the generated real-valued piano-rolls will be quantized. The process is the same as Polyffusion [1], where the authors show such quantization operation has almost zero syntax error in practice.\n    \n- **(Section 3.2) \"Both melody reduction and simplified chord progression ...\" -> How were these achieved? A pointer to the Appendix would be helpful here.**\n    \n    These are computed using Tonal Reduction Algorithm discussed in A. 1. We will add the pointer here.\n    \n- **(Section 3.2) \"13 times\" -> Why 13 times? Isn't it 128/12 = 11 times?**\n    \n    It should be 11 times. We will fix the typo in our revision.\n    \n- **(Section 3.3) \"We select Sk relevant music segments prior to t based on a defined similarity metric on X<k.\" -> Is this X^k or X^<k? The descriptions in this paragraph are somewhat confusing. From Figure 1, it seems like it's both X^k and X^<k. Please clarify this.**\n    \n    Sorry for the confusion. The term $X^{<k}$ here is correct. We use $X^{<k}$ as the criteria to select segments but embed the corresponding $X^{\\leq k}$ as condition. This is not clearly addressed in the submission and we will discuss the procedure in detail in our revision.\n\n- **(Algorithm 1) Isn't the song length M also an input?**\n    \n    No, it\u2019s not. In the quantization step discussed above, we quantize the generated real-valued piano-rolls. In the training of stage one, the samples of *Form* are zero-padded to 256 measures. So in inference, the model outputs *Form* containing 256 measures and length M is determined by finding the first time step that contains all-zero entries. We will make it clearer in the revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409967185,
                "cdate": 1700409967185,
                "tmdate": 1700411028550,
                "mdate": 1700411028550,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dDxnW74XJk",
                "forum": "sn7CYWyavh",
                "replyto": "IeRA4KeNfx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QD5q (Part 2)"
                    },
                    "comment": {
                        "value": "- **(Section 4) \"40 measures\" -> How do you determine the number of measures to be generated?**\n    \n    Here the first-level language *Form* is given and so the length of the piece is given. We select a given 40-measure *Form* for better illustration and space-saving purpose.\n    \n- **(Section 5.1) \"... and segment them into 8-bar musical segments with a 1-bar hopping size.\" -> What is this segmentation step for?**\n    \n    We apologize for the unclarity. It should be \u201cWe segment pieces into 32-bar segments to train the second stage of the model, and segment pieces into 8-bar segments to train the third and forth stage of the model\u201d. We will revise the paper accordingly.\n    \n- **(Section 5.3) \"Using pre-trained VAEs from Yang et al. (2019) and Wang et al. (2020).\" -> Are these the models you used to extract autoregressive controls?**\n    \n    No. Unlike external condition, for autoregressive condition, we use a trainable encoder consisting of convolutional layers. We will make these details clearer in the revision.\n    \n- **(Section 5.3) How did you select the test inputs for the evaluation?**\n    \n    All the samples are randomly selected. Samples that have obvious failure will be discarded for both proposed and baseline methods. Such failure happens rarely, e.g., blank for one measure). For 8-measure subjective evaluation, the first two measures are given, and for 32-measure subjective evaluation, the first 4 measures are given. This setting reduce the possible variance in unconditional generation evaluation.\n    \n    The reviewer also asks why not include ground truth samples in 32-measure subjective evaluation. The reason is that we regard 32-measure as a condensed pop song form (see response to Weakness 3). Such ground truth does not exist in the dataset. In the future, we should consider rewriting a ground truth piece into shorter piece for comparison.\n    \n\nWe are thankful to the reviewer to leave other comments and suggestions regarding organization of the paper and detailed wording choice. We agree with the reviewer in general and will consider revising the paper accordingly. (Note that some of the comments are addressed in detail in the above paragraphs.)\n\nFinally, as a response to the ethics flag, the arrangements in the POP909 dataset (the MIDIs and not the original audio) do not have copyright issue because it is made to the public by the authors of the POP909. Also, with our evaluation of plagiarism, we conclude that the plagiarism in our generation is unlikely, and if it happens, it can be detected. \n\n### **References**\n\n[1] Lejun Min, Junyan Jiang, Gus Xia, Jingwei Zhao: Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls. ISMIR 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410328831,
                "cdate": 1700410328831,
                "tmdate": 1700410328831,
                "mdate": 1700410328831,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VI27TjsjcN",
                "forum": "sn7CYWyavh",
                "replyto": "dDxnW74XJk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3963/Reviewer_QD5q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3963/Reviewer_QD5q"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the detailed rebuttal. The revisions look good to me. I've increased my score to 8.\n\n- Re song length M, thank you for the clarification. The added explanation in Section is helpful.\n- Re overfitting and plagiarism concerns, the new experiments and the added appendix are great.\n- Re the term \"continuous\" in Section 3.2, then maybe just use \"real-valued\" for clarity? (or something like \"e.g., real-valued\") Also, it's still unclear to me if you consider \"velocity\" or it's just relaxing {0, 1} to [0, 1].\n- Re \"For 8-measure subjective evaluation, the first two measures are given, and for 32-measure subjective evaluation, the first 4 measures are given.\", this information of your experimental setup is important. Please add this to Section 5.3."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619443367,
                "cdate": 1700619443367,
                "tmdate": 1700619443367,
                "mdate": 1700619443367,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]