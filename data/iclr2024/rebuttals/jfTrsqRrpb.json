[
    {
        "title": "Open-world Instance Segmentation: Top-down Learning with Bottom-up Supervision"
    },
    {
        "review": {
            "id": "sZxOXR3X9j",
            "forum": "jfTrsqRrpb",
            "replyto": "jfTrsqRrpb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2753/Reviewer_bvBC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2753/Reviewer_bvBC"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the limitations of top-down instance segmentation architectures in open-world scenarios, where predefined closed-world taxonomies may not be sufficient. To overcome this challenge, the authors propose a novel approach called Bottom-Up and Top-Down Open-world Segmentation (UDOS).\n\nUDOS combines classical bottom-up segmentation methods within a top-down learning framework. It utilizes a top-down network trained with weak supervision derived from class-agnostic bottom-up segmentation to predict object parts. These part-masks are then refined through affinity-based grouping to generate precise instance-level segmentations.\n\nThe key advantage of UDOS is its ability to balance the efficiency of top-down architectures with the capacity to handle unseen categories by leveraging bottom-up supervision. By incorporating both approaches, UDOS achieves superior performance over state-of-the-art methods in cross-category and cross-dataset transfer tasks. The authors validate their approach on challenging datasets such as MS-COCO, LVIS, ADE20k, UVO, and OpenImages."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper demonstrates a high level of originality in several aspects. Firstly, it introduces the concept of combining classical bottom-up segmentation methods with a top-down learning framework to address the limitations of predefined taxonomies in open-world scenarios.\n\n+ The use of weak supervision derived from class-agnostic bottom-up segmentation to predict object parts contributes to the originality of the proposed method."
                },
                "weaknesses": {
                    "value": "- While the Multiscale Combinatorial Grouping (MCG) approach was proposed in 2016, it might be beneficial to consider the use of more recent methods, such as the Segmentation Attention Module (SAM), to enhance the generation of higher-quality masks for this problem. The integration of SAM into the existing framework could potentially improve the performance and accuracy of mask generation.\n\n- In order to provide a comprehensive evaluation of the proposed approach, it would be valuable to compare it with relevant open-world panoptic segmentation techniques, such as ODISE (Open-vocabulary DIffusion-based panoptic SEgmentation). The inclusion of a comparative analysis with ODISE would enable a thorough assessment of the strengths and weaknesses of the proposed method and offer insights into its effectiveness in handling open-world scenarios."
                },
                "questions": {
                    "value": "Please refer to paper Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698044354555,
            "cdate": 1698044354555,
            "tmdate": 1699636217940,
            "mdate": 1699636217940,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lemgHfISJc",
                "forum": "jfTrsqRrpb",
                "replyto": "sZxOXR3X9j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback and highly relevant comments! We address each of them below.\n\n> **Using advanced super-pixel methods**\n\nThank you for raising this important point. We use MCG as it is fast and efficiently scalable to generate initial masks for the datasets we used in our paper. Note that despite employing a basic superpixel method like MCG, our UDOS framework already outperforms the next best prior method by 2% maskAR100 for cross-category (Tab 2) and upto 5% maskAR100 in cross-dataset settings (Tab 4). We will update the paper with this discussion.\n\n> **Incorporating Segmentation Attention Module**\n\nWe thank the reviewer for posing this idea. We unfortunately couldn\u2019t find a canonical reference for the suggested \u201csegmentation attention module\u201d, closest works are [1] and [2] - both of which are trained on supervised, closed-world annotated images and hence not applicable to generate part-masks in an open-world setting.\n\nIn case you meant Segment Anything Model (SAM)[3], we note that foundational models like SAM are reliant on millions of supervised images during training, making direct comparison of their open-world capabilities with ours difficult. Particularly, SAM's training data lacks category labels, making it infeasible to construct an evaluation set for categories not included in SAM's training data. However, UDOS is perfectly compatible to incorporate advances like SAM by replacing the initial super-pixel supervision with that generated by SAM, opening up exciting possibilities for enhanced open-world segmentation that builds upon UDOS in the future.\n\n[1] Jiang, Junzhe, et al. \"DSA: Deformable Segmentation Attention for Multi-Scale Fisheye Image Segmentation.\" Electronics 12.19 (2023): 4059.\n\n[2] Gou, Yuchuan, et al. \"Segattngan: Text to image generation with segmentation attention.\" arXiv preprint arXiv:2005.12444 (2020).\n\n[3] Kirillov, Alexander, et al. \"Segment anything.\" ICCV  (2023).\n\n\n> **Comparison with ODISE**\n\nThank you for bringing this prior work to our notice. We note that ODISE and UDOS are fundamentally different tasks. While ODISE is designed for open-vocabulary panoptic segmentation which requires labeling new classes in-the-wild, UDOS is focused on open-world instance segmentation which aims at detecting categories unseen during training (that is, objects with no masks during train time). Further, ODISE still employs seen and annotated instances for training the mask module using binary classification loss, which we already demonstrate does not generalize as well to open-classes as UDOS (MaskRCNN baseline in Tab 2 and Tab 4). We will include a discussion around this difference with ODISE in the updated version of the paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700274936,
                "cdate": 1700700274936,
                "tmdate": 1700732167347,
                "mdate": 1700732167347,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ligVzKmnB9",
            "forum": "jfTrsqRrpb",
            "replyto": "jfTrsqRrpb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2753/Reviewer_QToW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2753/Reviewer_QToW"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes bottom-Up and top-Down Open-world Segmentation (UDOS), a novel approach that combines classical bottom-up segmentation methods within a top-down learning framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This method is reasonable and novel. Combining bottom-up and top-down is an interesting idea."
                },
                "weaknesses": {
                    "value": "1. This paper generate candidate object regions through unsupervised segmentation methods. However, it cannot be guaranteed that these unsupervised methods can generate object regions that cover all regions. Especially when the number of categories increases, I question the performance of the unsupervised segmentation methods. The author should provide :1) the specific performance of the unsupervised segmentation methods, 2) experimental comparison with existing methods when categories are more, like COCO to LVIS.\n2. The author should provide more result metrics with previous methods. For example, LDET also provides AP, AR10. The author should provide related performance comparisons to provide more comprehensive results.\n3. [A] also proproses a CLN (region proposal generation algorithm). What's about performance comparision with this work.\n4. What's about the details about Refinement module? I feel that this is all about previous methods, no matter the objectness ranking and inference.\n\n[A] Detecting everything in the open world: Towards universal object detection. CVPR 2023"
                },
                "questions": {
                    "value": "Please refer to the weakness part. I will adjust the rating based on the author's feedback."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2753/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2753/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2753/Reviewer_QToW"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753635099,
            "cdate": 1698753635099,
            "tmdate": 1699636217854,
            "mdate": 1699636217854,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u8KjkzZHz8",
                "forum": "jfTrsqRrpb",
                "replyto": "ligVzKmnB9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2753/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their highly insightful comments, for appreciating the novelty in UDOS and finding the work interesting! We answer the specific questions posed below.\n\n> **specific performance of the unsupervised segmentation methods**\n\nThanks for raising this pertinent question! We show the segmentation performance of using only the unsupervised segmentation methods in Tab 2 of our paper. Specifically, the most competitive unsupervised method MCG yields 23.6 box AR100 compared to the most competitive MaskRCNN baseline of 25.6, indicating its effectiveness in detecting open-set instances.\n\nAlso, we do agree that the unsupervised methods do not necessarily cover all the objects in the region, but as shown in our qualitative illustrations in Fig 5 in main paper and Fig 3,4 in the supplementary, UDOS does manage to detect many objects missed by MaskRCNN. This property is also reflected in the SOTA performances achieved by UDOS across the board (Tab 2 and Tab 4), with upto 5% improvements in maskAR100 compared to next best method.\n\n> **Effectiveness of UDOS on many-class settings**\n\nWe note that the UVO dataset used in our paper is exhaustively annotated to cover every object in the image across several types of categories and scenes on a large-scale. It is notable that many of these categories are not even covered in the 1.2k sized LVIS taxonomy at all, as shown in [1]. On this dataset, UDOS achieves SOTA results outperforming all prior methods (COCO -> UVO, Table 4), effectively demonstrating UDOS's capabilities in handling open-world scenarios where a wide range of unseen objects may be present.\n\n> **more result metrics**\n\nUsing AP/AR10/AR100, UDOS gives 2.8/15.0/31.6 while LDET gives 5.0/16.3/27.4 on the VOC to NonVOC setting. However, note that on COCO with non-exhaustive annotations, evaluating precision (AP) or low AR@K (like AR10), may unfairly punish detecting valid, yet un-annotated objects. We remark that other prior works in open-world segmentation (including OLN and GGN) also often choose not to report AP on cross-category generalization for this reason. We will add this point to the paper, along with the AP result.\n\n> **Comparison with CLN**\n\nWe thank the reviewer for bringing this paper to our attention. While the region proposal generation algorithm in CLN employs label fusion from multiple closed-world datasets to mimic an open-world setting, this approach may not guarantee true open-world capabilities, as classes not included in any training datasets could still be overlooked. In contrast, our annotation-free and vocabulary-free segmentation method utilizes bottom-up low-level grouping to achieve state-of-the-art open-world capabilities with a remarkably simple framework. We will add this citation and relevant discussion to the paper.\n\n> **Details about the refinement module.**\n\nOur refinement method follows the mask and box prediction heads from previous methods only in design, but significantly differs in the purpose served. Specifically, prior methods feed the region proposals for mask prediction, while we feed the grouped part-masks as input to the refinement module for correcting noisy part-mask groupings. The significance of our refinement module is highlighted qualitatively in Tab.5a (+1% improvement in AR100) and qualitatively in Fig. 1 and Fig 2 in the supplementary through several visualizations.  It is also notable that just adding this refinement head to baseline MaskRCNN (_MaskRCNN_sc_ in Tab 2 and Tab 4) without the bottom-up grouping significantly underperforms UDOS by atleast 8% in AR value.\n\n[1] Wang, Weiyao, et al. \"Unidentified video objects: A benchmark for dense, open-world segmentation.\" _Proceedings of the IEEE/CVF International Conference on Computer Vision_. 2021."
                    },
                    "title": {
                        "value": "Response to the reviewer"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703664398,
                "cdate": 1700703664398,
                "tmdate": 1700732038161,
                "mdate": 1700732038161,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hcrW3djYrJ",
            "forum": "jfTrsqRrpb",
            "replyto": "jfTrsqRrpb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2753/Reviewer_Rvq9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2753/Reviewer_Rvq9"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed the UDOS for open-world instance segmentation that combines bottom-up unsupervised grouping with top-down learning. This model designed a grouping module and refinement method to achieve SOTA performance on multiple datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The group-parts-to-whole strategy for segmentation is interesting. \nExperiments on multiples datasets verify the effectiveness of the proposed methods.\nThe paper writing and organization are good and clear."
                },
                "weaknesses": {
                    "value": "Question: \n1. Is there any time-consuming experiments on the cluster in the grouping module? Because the similarity is calculated two-by-two.\n2. I am interested in the AP performance if adding the classification head in cross-datasets and cross-category setting. I know the task is category-free and different from open-vocabulary segmentation task, but I wander the segmentation performance with higher recall.\n3. As we know, the segment anything (SAM[1]) has high generalizability in category-free segmentation task. It is a foundation model pretrained in many data, but its zero-shot ability is strong without fine-tune in specific datasets in category-free segmentation task, so I think the comparison is necessary. Can this have higher recall that SAM? If not, please discuss on the contribution.\n4. Why exclude part masks from U that overlap with any ground truth mask in S with an IoU greater than 0.9? Please discuss on it with experiments.\n5. How about the grouping result on these situations: two same-category instances are close (or overlap), two instance with similar color, two hierarchical categories (e.g. clothes and person).\n[1] Segment Anything, ICCV2023."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2753/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2753/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2753/Reviewer_Rvq9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837691521,
            "cdate": 1698837691521,
            "tmdate": 1699636217774,
            "mdate": 1699636217774,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1Lj65fBsar",
                "forum": "jfTrsqRrpb",
                "replyto": "hcrW3djYrJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their very useful feedback and raising several pertinent questions! We address each of them below.\n\n> **Training and inference times for the grouping module.**\n\nOur UDOS framework utilizes a fast and efficient agglomerative clustering algorithm from scikit-learn, *ensuring minimal time and memory overhead* even when handling hundreds of part-masks. While standard MaskRCNN training on 8 Nvidia A10 GPUs takes about 22 hours, UDOS with its grouping and refinement module takes less than 24 hours. As already noted in the paper (sec 4.4), UDOS takes 0.13sec/image (7 FPS) during inference compared to MaskRCNN's 0.09 sec/image (11 FPS), while delivering remarkably higher accuracy. \n\n> **AP Values**\n\nUDOS gives an AP value of 2.8 while LDET gives 5.0 on the VOC to COCO setting on the binary mask classification. However, evaluating precision using AP on COCO with non-exhaustive annotations may unfairly punish detecting valid, yet un-annotated objects. We remark that other prior works in open-world segmentation (including OLN and GGN) also often choose not to report AP on cross-category generalization for this reason. We will add this point to the paper, along with the AP result.\n\n> **Comparison with Segment Anything (SAM)**\n\nThis is a great question, thanks for raising this comment! Although foundational models like SAM expand the potential for segmentation, their reliance on vast amounts of supervised images during training impedes a fair comparison of their open-world capabilities with ours. Particularly, SAM's training data lacks category labels, making it infeasible to construct an evaluation set for categories not included in SAM's training data. However, note that UDOS is perfectly compatible to incorporate advances like SAM by replacing the initial super-pixel supervision with that generated by SAM, opening up exciting possibilities for enhanced open-world segmentation. We will add a citation to SAM, along with this discussion, in the final version.\n\n> **Excluding masks in U that overlap with S.**\n\nWe remove part masks from U with significant overlap with those in the annotated set S to avoid possibly redundant supervision. Removing these masks using a threshold of 0.9 made no change to the resulting AR values, hence is optional. We will add this remark to the paper. \n\n> **UDOS capabilities in various situations.**\n\nThis is a great point raised by the reviewer! We thank you for suggesting a novel protocol to construct evaluations, we believe that following these suggestions and conducting large-scale studies on failure modes of existing systems in the open-world can be a new contribution in itself! In current setting, despite the difficulty in quantitative evaluations, we still aim to provide understanding of these cases qualitatively in the main paper and the supplementary through several visualizations where UDOS shows its prominence in all those cases, such as:\n\n- _Similar category objects close to each other_: The glasses on the table (Fig 5 in main paper) and the people on the elephant (Fig 1 in the supplementary) are segmented with separate masks even when they are close together.\n- _Similar colored instances_: The two cabs (Fig 1 in the supplementary) are segmented separately.\n- _Hierarchical categorization_: The dress and the boy (Fig 1 in the supplementary), the uniform and the baseball player (Fig 5 in main paper) as well as the truck and its wheel (Fig 1 in the supplementary) are segmented together. Note that it is in general difficult to resolve this ambiguity in an open-world setting without the knowledge of real world categorization of the class hierarchies."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699063404,
                "cdate": 1700699063404,
                "tmdate": 1700732173319,
                "mdate": 1700732173319,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Y767YqrrHN",
            "forum": "jfTrsqRrpb",
            "replyto": "jfTrsqRrpb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2753/Reviewer_jfRf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2753/Reviewer_jfRf"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a top-down bottom-up approach for open-set instance segmentation. The bottom-up segmentation module is used to predict object parts, and then the authors use a clustering/group method to assemble parts into objects.  The main point that the authors try to argue is that, this bottom-up module somehow fits well in the open-world (open-set) instance segmentation scenario."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "originality: The approach involves quite a few components. To me the authors build a quite complex system and it's unclear to me what the motivation and which component is the main reason which contributes to the good performance.\n\nquality: borderline\n\nclarity: The idea is clear and the paper is easy to follow\n\nsignificance: The task per se is quite important. However I do not think the system presented in this paper is good enough to have an impact on open-world instance segmentation."
                },
                "weaknesses": {
                    "value": "1) The bottom-up module is quite complex, involving a few components. I do see the authors did ablation experiments to justify some design choices, it is not clear why  part-segmentation and grouping work better than other baseline approaches.  Part-segmentation + grouping appeared in the literature long time ago and researchers abandoned this idea.  Current experiments in this paper do not convince me that this is actually a better idea for open-world segmentation.  A simple baseline will be to train a class-agnostic instance segmentation using, e.g. COCO annotations.  Papers already showed that a  class-agnostic model works better for open-world problems.\n\n2) The compared methods are very old. For example, authors choose Mask RCNN and MCG as the baseline methods. These two methods are very old. The authors will need to consider recent methods. Even for top-down methods, Mask2former etc. will be a much better choice. I see that the authors might argue that the proposed method can use any other top-down method to replace Mask RCNN. But still why choose MaskRCNN in the first place. Using a more recent method will make the experiment results more convincing."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698929354124,
            "cdate": 1698929354124,
            "tmdate": 1699636217713,
            "mdate": 1699636217713,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nhVk8Siiiz",
                "forum": "jfTrsqRrpb",
                "replyto": "Y767YqrrHN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reviewer"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback on our paper! We address each of their questions below!\n\n> **why should part-segmentation and grouping be preferred compared to class-agnostic training?**\n\nFirstly, the suggested class-agnostic instance segmentation is *already included as one of our baselines* (named _MaskRCNN_, and related variants), for cross-category setting (using VOC-COCO annotations) in Table 2 and cross-dataset setting (using all COCO annotations) in Table 4. Despite being a strong baseline, the class-agnostic models still suffered from weaker performance in open-world instance segmentation literature as also shown by other works in this field in OLN, LDET and GGN. We emphasize that UDOS *outperforms this exact baseline suggested by the reviewer by 8.4%* in cross-category and *6% in cross-dataset* settings. We refer the reviewer to Table 2, Table 4 and section 4.1 where we explicitly describe the baselines, including the class-agnostic model, a few improved variants of it as well as the state-of-the-art methods from recent literature.\n\n> **Prior works in part-segmentation + grouping**\n1. Reviewer mentions that part-segmentation and grouping methods appeared in literature a long time ago but failed to provide any specific references. Thus, we cannot provide a detailed comparison. We do, however, like to refer reviewers to our related work section on how our work is related to prior works for segmentation.\n2. In addition, the reviewer mentions that the idea of part-segmentation + grouping has been attempted before and has been abandoned. We respectfully disagree that this is a weakness and a justification for the low rating of our work. Firstly, we emphasize that our approach stands out from prior works due to two novel design choices: (i) *employing weak supervision from bottom-up part segments* for handling unseen instances and (ii) *integrating a refinement module* within an end-to-end framework for improving noisy part-mask groupings. These contributions *are validated both quantitatively (Table 5a and 5c) and qualitatively (Figure 5 and supplementary Figures 3-4)* in our paper. Second, we do not believe that unsuccessful prior attempts with conceptually similar ideas diminishes the value of new works. In fact, our strong empirical performance suggests that combining part segmentation and grouping indeed achieves strong performance in the open-world. \n\n> **Current experiment setting is not convincing**\n\nWe thank the reviewer for noting their concern. We note that the current experiments and baselines in the paper are very comprehensive, covering diverse datasets (COCO, LVIS, UVO, ADE and OpenImages) and transfer settings (cross-category and cross-dataset), which were also used in all prior works in open-world instance segmentation (LDET, OLN, GGN). *UDOS comprehensively outperforms all these methods on all the settings*, thus highlighting the applicability of our approach for open world segmentation. In addition, the ablations clearly justify several of our design choices, such as the use of part-mask supervision or the grouping and refinement modules. \n\n> **Choice of MaskRCNN as the backbone**\n\nWe employ MaskRCNN to enable fair comparisons with prior works such as LDET, GGN and OLN - all of which use MaskRCNN for open-world segmentation. We also highlight that despite employing a basic top-down architecture like MaskRCNN and superpixel method like MCG, our *UDOS framework already outperforms the best prior method* by 2% maskAR100 for cross-category (Tab 2) and upto 5% maskAR100 in cross-dataset settings (Tab 4). Moreover, as rightly noted, UDOS framework is designed to integrate seamlessly with any segmentation architecture, enabling it to incorporate recent and future advancements to potentially improve results further."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696851096,
                "cdate": 1700696851096,
                "tmdate": 1700731469595,
                "mdate": 1700731469595,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]