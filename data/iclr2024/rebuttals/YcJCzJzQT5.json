[
    {
        "title": "DipDNN: Decomposed Invertible Pathway Deep Neural Networks"
    },
    {
        "review": {
            "id": "dNcdwrwrjN",
            "forum": "YcJCzJzQT5",
            "replyto": "YcJCzJzQT5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3006/Reviewer_zeZn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3006/Reviewer_zeZn"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a novel model DipDNN for the intent of solving bi-directional inference problems. The model enforces bijectivity by using coupling layers, inspired by the work of  Dinh et al., 2014, to construct an analytical inverse, which is easy to compute and does not restrict the model's capacity shown in theorem 1. In addition, the proposed method reduces computation time by utilizing upper and lower triangular weight matrices in the model, which is demonstrated in Figure 2. Lastly, the proposed method is empirically validated for a variety of inverse problems, including image construction competition and system identification-based state estimation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1). Diversity in types of experiments, the authors evaluated the proposed wide range of applications.\n\n2). The results of the proposed method are empirically promising in the system identification-based state estimation experiments."
                },
                "weaknesses": {
                    "value": "1). The paper lacks some important details that make the paper challenging to evaluate. \n*  The training objective of the proposed method is not articulated clearly mathematically. The paper should clearly state the objective \n    function and what parameters are being optimized. \n* Objective function defined in section 2.1 the set g is an element of is not defined. \n* Figures 1 and 2 lack a legend/an explanation of the operations. What are the blue lines and red dots?\n* Similar to training the objective function for inference is not stated. How are the inference problems being solved? What is the mathematical objective?\n\n2). Authors should provide an explanation of why they chose an additive coupling layer over coupling methods, i.e. Affine or spline coupling layers. Normalizing flow models have made significant progress in the area of coupling layers, so the authors should justify why they're choosing additive over another type. \n\n3). Novelty of theoretical contribution\n* The results of Theorem 1 are heavily dependent on the contribution of Duan et al., 2023.\n* Coupling layers have been proven to be universal approximations [Coupling-based Invertible Neural Networks Are\nUniversal Diffeomorphism Approximators](https://proceedings.neurips.cc/paper/2020/file/2290a7385ed77cc5592dc2153229f082-Paper.pdf) Nuerips 2020\n\n\n4). Image Construction and Face Completion experiments\n* Figure 6 Based on the eye test it is challenging to see the improvements in the DipDNN over the Additive Coupling\n* Based on Figure 7 Results B i-Resnet outperforms the proposed method, but the authors do not make a comment about this result in the paper."
                },
                "questions": {
                    "value": "Questions are addressed in the weakness section.\n\nMain questions:\n* Could the authors state the objective functions for both training and inference of the proposed method?\n* Could authors please state the theoretical contributions and the insight they provide to the paper?\n* Please provide justification or explanation on i-Resnet outperforming the proposed method in Figure 7 b."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3006/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3006/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3006/Reviewer_zeZn"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3006/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698607413445,
            "cdate": 1698607413445,
            "tmdate": 1699636245118,
            "mdate": 1699636245118,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vS5YiAJZ09",
                "forum": "YcJCzJzQT5",
                "replyto": "dNcdwrwrjN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3006/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3006/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer zeZn:"
                    },
                    "comment": {
                        "value": "We would like to thank you for your time and effort in reviewing our paper. We are pleased you recognize the proposed method's applicability to diverse inverse problems. The suggestions you provided for clarifying the paper's target and contribution are very useful, and we will use them for revision. Below, please find our responses to the questions.\n\n**Specify objectives of the inverse problem:**\n\n*Define the inverse problem:*\n\nFor system identification, the target is to learn a mapping $f: \\mathcal{X} \\rightarrow  \\mathcal{Y}$ from data for estimating $y$ given $x$. The inverse problem is an opposite direction of system identification to infer states $x$ given $y$. Therefore, our target is first to learn a forward mapping and then derive the inverse counterpart. Mathematically, the objective function includes 1) a regular supervised learning error term of the forward learning and 2) a reconstruction error term of using the inverse mapping to recover back to $x$.\n\n*Clarify the mathematical objectives:*\n\nThanks for pointing out the typo in the math expression. We checked it and found that the previous objective and description may cause confusion. We rewrite it here.\n\n\nAs computing the inverse solution is the final target, we define the objective using the inverse mapping $g: \\mathcal{Y} \\rightarrow  \\mathcal{X}$. \n\n$$ {g}^* =   \\operatorname*{argmin}_{g \\in \\mathcal{G} } \\sum_{i=1}^N \\ell_1\\left({g_{\\theta}}^{-1}\\left(\\bm{x}_i\\right),\\bm{y}_i\\right) + \\sum_{i=1}^N \\ell_2\\left(\\bm{x}_i, g_{\\theta}\\left({g_{\\theta}}^{-1}\\left(\\bm{x}_i\\right)\\right)\\right)$$\n\nThe first term is the supervised learning loss to minimize the mismatch in forward model recovery, where $\\ell_1(\\cdot)$ denotes the mean square error we used in this paper. The second term is the reconstruction error to make the inverse consistent with the forward.\n$\\theta$ are neural network parameters of the forward model, which is optimized during training. In our proposed method, we design analytical invertibility in the forward NN model such that the inverse counterpart shares the parameters $\\theta$.\n\nTherefore, once the forward model is well-trained, we directly use the invertible NN to derive the inverse model and use it for inference of $x$. No more training is needed. This is also why an analytical inverse form is essential; otherwise, the inverse computation is challenging.\n\n*Clarify components in Figures 1 and 2:*\n\nWe use Figure 1 to illustrate the architecture limitation and representation power of existing invertible networks. They motivate us to design the proposed DipDNN in Figure 2. Figure 1(a) is a basic layer of the classic NICE model [1]. The blue component denotes the nonlinear NN correlation, which is only between $x_{I_1}$ and $y_{I_2}$. ${x}_{I_1}$ and ${y}_{I_1}$, ${x}_{I_2}$ and ${y}_{I_2}$ are linearly correlated and $x_{I_2}$ and $y_{I_1}$ are not correlated. Therefore, at least three such layers are required to enable full nonlinear coupling of all inputs with all outputs in Figure 1(b). Regular NN takes one layer for full coupling, so we try to compress Figure 1(b) with an equivalent representation and compare the network connection with a regular NN. Red dots denote Leaky ReLU activation, providing nonlinearity on the linear paths in the reduced architecture. Leaky ReLU activation is a strictly monotone function customized from ReLU, which preserves invertibility in nonlinear mapping.\n\nFigure 2 presents the proposed decomposed-invertible-pathway DNN (DipDNN) model. The blue lines indicate connections in NN, and the red dots still denote Leaky ReLU. Here, we first maintain the dense NN architecture and then enforce one-to-one correspondence by decomposing the nested layer of NICE into a basic invertible unit, as shown in the top right corner of Figure 2. Subsequently, we extend the invertible unit to wider and deeper NN."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3006/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702142971,
                "cdate": 1700702142971,
                "tmdate": 1700702142971,
                "mdate": 1700702142971,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Iu8q9HZoJ0",
            "forum": "YcJCzJzQT5",
            "replyto": "YcJCzJzQT5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3006/Reviewer_uKYL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3006/Reviewer_uKYL"
            ],
            "content": {
                "summary": {
                    "value": "Targeted on the bi-directional inference demands, such as state estimation, signal recovery, privacy preservation, and reasoning, the paper presents a novel Deep Neural Network design, decomposed-invertible-pathway DNNs (DipDNN), that decomposes the nested structure to avoid the inapplicable requirement of splitting input/output equally, while ensures strict invertibility and minimum computational redundancy without hurting the universal approximation.\n\nIn general, the problem this article focuses on,  the inconsistency in bi-directional inferences, is important. Due to the inherent irreversibility of DNNs, the problem is quite challenging; the paper presents a well-designed invertible DNN architecture with rigorous theoretical analysis. However, since the datasets in the experiments are relatively small, as well as the model architecture; the scalability of the proposed method is not well illustrated."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, this paper is well structured, and the idea has been explained clearly.\n\nThis paper focuses on a problem that has important practical implications but lacks effective approaches in the deep learning field. It designs a novel methodology that maintains the consistent and analytical forms of inverse solutions with the nested DNNs while reducing the computational expense and relaxing the restrictions on invertible architecture.\n\nThe paper presents a rigorous theoretical analysis to prove the strict invertibility of the DipDNN model without hurting the universal approximation. This theoretical grounding provides a strong foundation for the model's validity.\n\nThe paper conducts numerical experiments on several practical applications, including image construction and face completion, power system (PS) state estimation, etc., which show that DipDNN can recover the input exactly and quickly in diverse systems."
                },
                "weaknesses": {
                    "value": "Although the proposed method appears to be significantly superior to the methods compared, I found that the baseline methods seem somehow outdated (the latest one, i-ResNet, was proposed in 2019). \n\nThe datasets in the experiments are pretty small; the DNN is also simple. For image datasets, I believe that providing some results on CIFAR10/100 (ImageNet may be impossible) and comparing them with i-ResNet can better illustrate the effectiveness of the proposed method."
                },
                "questions": {
                    "value": "1. How large can this model structure scale to, and how will DipDNN with deeper and wider layers perform on complex data sets such as CIFAR?\n\n2. As for experiments, are all the DNN architectures the same through different tasks, i.e., the DNNs on Image Construction and Face Completion and System Identification-based State Estimation?\n\n3. In which part of the experiment the effectiveness of the proposed parallel structure for physical regularization over DipDNN are tested? Is the model performance sensitive to the hyperparameters, i.e., \u03bb_Phy and \u03bb_DipDNN?\n\n4. As for the parallel structure for physical regularization over DipDNN, for physical systems with unknown priors, is the model such as equation learner jointly trained together with DipDNN?\n\nMaybe a typo: the line below the Figure3: f(x) = \u03bb_Phy f1(x) + \u03bb_DipDNN f2(x), should be  f(x) = \u03bb_DiPDNN f1(x) + \u03bb_Phy f2(x)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3006/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3006/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3006/Reviewer_uKYL"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3006/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698914610802,
            "cdate": 1698914610802,
            "tmdate": 1699636245039,
            "mdate": 1699636245039,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s62xtHLKFN",
                "forum": "YcJCzJzQT5",
                "replyto": "Iu8q9HZoJ0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3006/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3006/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer uKYL:"
                    },
                    "comment": {
                        "value": "First, we would like to thank the reviewer for the positive review. We\u2019re pleased you found our work meaningful and well-designed and our theoretical analysis solid. The suggestions are very valuable, and we\u2019ll use them for revision. Below, we provide responses to the questions.\n\n**Regarding baseline methods**\n\nThe reviewer mentioned a very important aspect of choosing baselines to compare with. The related work is quite limited because the target problem is inverse point estimates, especially in providing analytical inverse. Most work focuses on a generative setting for inverse problems in image-related tasks, e.g., flow models. We select [1] for the classic bijective function using affine coupling. Its latest variation is [2] with applications on discriminative learning tasks, and we refer to their training details in experiments. I-Resnet [3], mentioned by the reviewer, is another typical design of invertible NN with Lipschitz regularization. To the best of our knowledge, the following work has not changed the core invertibility designs of [1] and [2] in NN architectures, which is our focus. So, we choose them as the baselines. We would really appreciate it if the reviewer had any recommendations on an up-to-date baseline method.\n\n**Considering complex dataset and model scalability**\n\nWe add some experiments to test the model on CIFAR-10 and compare it with i-Resnet. We test a CIFAR-10 classification task and note that the forward-inverse mapping is built between inputs to the last feature extraction layer, where a linear classification layer is used on top of the invertible blocks. Due to the time limit, we reduce the hidden layers and control all the models to have the same number of layers (modes using MLP and having the dimension be 3072) for a fair comparison.\n\n| Model                        | i-Resnet(Conv)                                  | i-Resnet(MLP)          | NICE(MLP)          | DipDNN            |\n|------------------------------|-------------------------------------------------|------------------------|--------------------|-------------------|\n| Classification Acc. (Forward)| 74.85% (93.07% in [3] using full model)        | 69.73%                | 57.31%             | 65.23%            |\n| Running time (Forward Training)| 26.27 Sec/Epoch                                | 14.13 Sec/Epoch       | 47.93 Sec/Epoch    | 12.46 Sec/Epoch   |\n| Reconstruction Err. (Inverse)| 6.95e-5                                         | 8.32e-4               | 4.98e-3             | 3.27e-10          |\n| Running time (Inverse Testing)| 45.81 Sec                                      | 37.64 Sec             | 5.23 Sec            | 2.46 Sec          |\n\n*Clarify NN architectures:*\n\nFirst, we modify the baseline models for each testing case to control all the NN architectures to be as similar as possible for fair comparison. For example, NICE has a nested DNN in the affine coupling layer, and we use the same MLP for the nested DNN as our DipDNN. Then, the numbers of hidden layers differ from case to case, e.g., 3 layers for face completion task and 6-8 layers for MNIST based on cross-validation. For state estimation, it depends on the problem size, e.g., 3 layers for IEEE 8-bus network and 6-10 layers for IEEE 123-bus network based on cross-validation.\n\n*Insignts on the scalability:*\n\nFor the scalability of the model, we agree with the reviewer that more practical large-scale use cases will be helpful. In existing work, the invertible model is often used for density estimation, for which image-related tasks are usually large-scale. Our work focuses on general inverse problems in physical systems, for which the scale varies from case to case. Unlike generative learning in density estimation, the inverse problem in physics targets the point estimates and usually has a critical requirement for accuracy. Thus, the emphasis of this paper is to obtain an accurate inverse solution. The current model has been tested on small synthetic examples (2-9 variables) cases as large as a 123-bus power system for physical system state estimation and MNIST data set (28$\\times$28 = 784) for image construction. During our experiments, we have some observations regarding scalability. In inverse problems, a typical scalability challenge is that, with a larger problem size and larger model, the numerical errors will aggregate and lead to numerical non-invertibility. We show partial results in Figure 4 for the error propagation with respect to increasing problem dimension and nonlinearity and increasing model depth. The errors increase on an exponential scale. For this challenge, we analyze with details in Appendix A.4. While large-scale problems may aggregate errors, we can regularize bi-Lipschitz continuity to enhance inverse stability."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3006/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554454611,
                "cdate": 1700554454611,
                "tmdate": 1700554454611,
                "mdate": 1700554454611,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7WZgukDNdX",
            "forum": "YcJCzJzQT5",
            "replyto": "YcJCzJzQT5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3006/Reviewer_RyQp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3006/Reviewer_RyQp"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the problem of designing deep neural network (DNN) strctures that is invertible. As a main contribution, the decomposed-invertible-pathway DNN structure is designed to mitigate the computational redundancy of existing invertible DNN designs. Besides, a parallel DNN structure is introduced to add regularization to the invertible DNN to improve the prediction accuracy. Simulations on over a number of test cases, inlcuding the image processing and state estimation in power systems, are conducted to show the effectiveness of the proposed DNN design."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problem investigated in the paper is important and interesting.\n2. The idea of paper for contructing an invertible DNN is novel."
                },
                "weaknesses": {
                    "value": "1. The paper is not easy to follow. See the comments below.\n\n2. The contribution of the paper is not clear.\n\n3. The theoretical analysis in the paper is not sufficient. See the comments below."
                },
                "questions": {
                    "value": "1. The paper is hard to follow. The authors are suggested to re-organize the contents and polish the expressions in order to make it easier to read. \n\n2. The contribution of the paper is vague. The authors are suggested to explain the advantage of the proposed approach as compared to state-of-the-art invertible DNN designs clearly. If the contribution of the proposed approach is having lower complexity, a run-time complexity analysis should be given in the paper.\n\n3. In paragraph 3 of Sec. 3.1, the authors mention \"Although the nonlinear DNN is nested in the middle, some interconnections among variables are eliminated due to the separated input/output groups, for which the comparison with regular NN is in Appendix A.1..\" This is not easy to follow. The authors are suggested to explain why some interconnections among variables are eliminated in a more clear way.\n\n4. It is not clear why fixed spliting the input and output is a important disadvantage of eisting invertible DNN designs. The authors are suggested to give some illustration and toy examples to show it."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3006/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698917631802,
            "cdate": 1698917631802,
            "tmdate": 1699636244959,
            "mdate": 1699636244959,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b8tfz1VOkh",
                "forum": "YcJCzJzQT5",
                "replyto": "7WZgukDNdX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3006/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3006/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer RyQp:"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review and provide valuable comments on our work. We\u2019re pleased to see that you found our work novel, and we appreciate your suggestions on the presentation of the paper's contents and contributions. We\u2019ll include them in our revision. Below, please find our response to the questions.\n\n\n\n**Clarify the contributions of our work.**\n\n\n\n\nTo clarify the paper's contributions, we will divide the explanation into the drawbacks of existing methods and designs to address them.\n\n*General motivations from existing work:*\n\nThe neural network is not invertible in nature because the multi-layers of interconnections break a strict one-to-one correspondence. One group of previous methods approximates the numerical inverse, i.e., minimizing reconstruction error. It is difficult to ensure an accurate inverse, especially for extrapolation. For analytical inverse, several methods try to embed invertibility into the NNs, and we analyze two typical methods. One is i-ResNet [1], which has proved invertibility with regularized weights, but there is no analytical inverse form. Still, we\u2019ll refer to its regularization to address numerical issues. The other is the NICE model [2], where the affine coupling in layers (and also the exponential nonlinearity designed later in [3]) is strictly bijective and has an easy-to-derive inverse form. We started with applying this model to the inverse problem in physical systems and found poor performance in that 1) the maximize likelihood loss function doesn\u2019t apply as our task is discriminative learning and the data is not generated from a Gaussian distribution, 2) the random splitting of image pixels is not fair for variables with physical meaning as different splitting groups lead to different results, 3) each layer ensures the invertibility via linear paths and bypasses nonlinear mappings.\n\n*Illustrate the drawbacks of nested design and fixed input/output splitting:*\n\nPoints 2) and 3) are related to the architecture limitation and representation power of existing invertible networks. To illustrate with an intuitive example, we would like to point the reviewer to Appendix A.1, where Page 14 includes three figures to compare the network architectures. Figure 9(a) is a basic layer of NICE. The nonlinear NN correlation is only between $x_{I_1}$ and $y_{I_2}$. $x_{I_1}$ and $y_{I_1}$, $x_{I_2}$ and $y_{I_2}$ are linearly correlated and $x_{I_2}$ and $y_{I_1}$ are not correlated. Therefore, at least three such layers are required to enable full coupling of all inputs with all outputs in Figure 9(b) (this can be inferred from Jacobian derivation on Page 13). Regular NN takes one layer for full coupling, so we compress Figure 9(b) into Figure 9(d) with an equivalent representation and compare the network connection with a regular NN. In Figure 11, we can observe that, while all the dimensions are coupled, the nonlinear mapping is separated into groups and isolated within each group coulping, i.e., from $x_{I_1}$ to $y_{I_1}$, from $x_{I_2}$ to $y_{I_2}$, from $x_{I_1}$ to $y_{I_2}$, and from $x_{I_2}$ to $y_{I_1}$. Many interconnections are eliminated, like nonlinear couplings of variables in $x_{I_1}$ and variables in $x_{I_2}$.\n\nTherefore, we see that the nested design in the NICE model ensures invertibility at the cost of $\\sim 1.5\\times$ computation complexity (i.e., at least three layers for full coupling) and representation power (i.e., an equivalent sparse representation of regular NN). This is why we are motivated to design an invertible model that relaxes the fixed splitting of variables and maintains regular NN\u2019s representation power as much as possible.\n\n*Proposed designs to address the drawbacks:*\n\nIn the proposed decomposed-invertible-pathway DNN (DipDNN) model, we first maintain the dense NN architecture and then enforce one-to-one correspondence by decomposing the nested layer of NICE into a basic invertible unit, as shown in the top right corner of Figure 2. The adoption of Leaky ReLU activation maintains invertibility in nonlinear correlation. In this way, we could prove the preserved approximation capability using the LU decomposition of weights and the latest theory of narrow deep networks [4]."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3006/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552740003,
                "cdate": 1700552740003,
                "tmdate": 1700700777662,
                "mdate": 1700700777662,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]