[
    {
        "title": "Towards Reliable Evaluation and Fast Training of Robust Semantic Segmentation Models"
    },
    {
        "review": {
            "id": "r6QDYE92ZT",
            "forum": "aM7US5jKCd",
            "replyto": "aM7US5jKCd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7859/Reviewer_CxEd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7859/Reviewer_CxEd"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes several losses for attacking the semantic segmentation models with adversarial training and an evaluation protocol for benchmarking the adversarial robustness of the segmentation models. This paper also proposes to adopt adversarially pretrained models for segmentation models' initialization. Extensive experiments with various segmentation networks present the effectiveness of the proposed  methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation of this paper for the proposed method is clear and the proposed method is consistent to the motivations.\n2. The paper proposes new losses for attacking, evaluating and training semantic segmentation models. The proposed loss and the evaluation protocol could become great baselines for the further works.\n3. The proposed method is verified on two popular segmentation networks and two datasets and it presents great insights in this field."
                },
                "weaknesses": {
                    "value": "1. The advantages of the proposed three losses are not well depicted. It would be better if the author could discuss under different scenarios which proposed loss is the best for attacking. Some visual examples would be better.\n2. The organization of this paper is confusing. The introduction of some existing works like APGD should be put in the related works. An overview of the method's structure could be added to the beginning of section 2. As section 2 mentions PIR-AT s many times, a short description about PIR-AT model is also necessary.\n3. As many related works and ablation studies are mixed in the method part, it is difficult to distinguish the contributions of this work from previous works.\n4. The SEA is not presented clearly. Why four losses perform worse than all six losses is not analyzed. How the current four losses are selected is not mentioned. And SEA doesn't discuss how to balance different losses.\n5. Previous works[1] have proven that using a better robust initialization model could improve the task model's robustness. How PIR-AT is different from the existing practices is not well presented.\n\n[1]Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang. Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning. CVPR 2020"
                },
                "questions": {
                    "value": "1. Will AT obtain the same performance as PIR-AT if sufficient training time is given?\n2. PIR-AT suggests using $L_{\\infty}$-robust ImageNet model for initialization. How much computational resources are required to train this model compared to the normal ImageNet model with the same parameters?\n3. How the image-wise worst case over all losses in Table 2 is calculated? A short description is expected.\n4. Does the conclusion in Figure 2 also generalize to clean models?\n5. What is the result of AT with 32 epoch in Figure 5?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7859/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7859/Reviewer_CxEd",
                        "ICLR.cc/2024/Conference/Submission7859/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7859/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719706586,
            "cdate": 1698719706586,
            "tmdate": 1700735701324,
            "mdate": 1700735701324,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iwEyeLPqEu",
                "forum": "aM7US5jKCd",
                "replyto": "r6QDYE92ZT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7859/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7859/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer CxEd (part 1)"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for the positive and detailed feedback. In the following we address the weaknesses and questions raised in the review.\n\n&nbsp;\n\n**Q: The advantages of the proposed three losses are not well depicted. It would be better if the author could discuss under different scenarios which proposed loss is the best for attacking. Some visual examples would be better.**\n\nThe (theoretical) discussion of benefits and weaknesses for each loss in Sec. 2.2 suggests that one main difference among losses is how they balance the weight of different pixels in the objective function. On one extreme, the plain cross-entropy maximizes the loss for all pixels independently of whether they are misclassified, and assign them the same importance. Conversely, the masked losses exclude (via the mask) the misclassified pixels from the objective function, with the danger of reverting back the successful perturbations. As middle ground, losses like the JS divergence assign a weight to each pixel based on how \u201cconfidently\u201d they are misclassified. We conjecture that for radii where robustness is low, masked losses help focusing on the remaining pixels, and already misclassified pixels are hardly reverted since they are far from the decision boundary. Conversely, at smaller radii achieving confident misclassification is harder (since the perturbations are smaller), and most pixels are still correctly classified or misclassified but close to the decision boundary: then it becomes more important to balance all of them in the loss, hence losses like JS divergence are more effective. This hypothesis is in line with the empirical results in Table 2. We are happy to add this more detailed discussion to the paper.\n\nFinally, if the Reviewer could clarify what they mean for \u201cvisual examples\u201d, we will address that point directly.\n\n&nbsp;\n\n**Q: The organization of this paper is confusing. The introduction of some existing works like APGD should be put in the related works. An overview of the method's structure could be added to the beginning of section 2. As section 2 mentions PIR-AT s many times, a short description about PIR-AT model is also necessary.**\\\n**Q: As many related works and ablation studies are mixed in the method part, it is difficult to distinguish the contributions of this work from previous works.**\n\nThanks for the suggestions about the presentation. We agree with the Reviewer that the definition of PIR-AT comes late in the paper, we will add a short description earlier on in the revision. We will also add more detail of SEA at the start of Sec. 2, and discuss APGD in the related works.\n\n&nbsp;\n\n**Q: The SEA is not presented clearly. Why four losses perform worse than all six losses is not analyzed. How the current four losses are selected is not mentioned. And SEA doesn't discuss how to balance different losses.**\n\nWe introduce the details of SEA in Sec. 2.4, combining the loss functions analyzed in Sec. 2.2 and the optimization algorithm from Sec. 2.3. SEA is based on a worst-case evaluation: this means that, for each image, the strongest attack among those obtained running APGD on the different losses is considered. Therefore, having more losses can only improve its performance (since it has a strictly larger set of attacks to choose among). However, each additional loss requires additional computational cost. Then, as mentioned in Sec. 2.4, we select 4 losses which allow our ensemble SEA to perform already on par with using all 6 losses, while saving \u2153 of runtime. This is further illustrated in one of the ablation studies in App. C.2, which shows that the improvement given by 6 over 4 losses is minimal (<0.1% in robust accuracy). Finally, since the losses are used for independent runs, it is not necessary to balance them.\n\n&nbsp;\n\n**Previous works[1] have proven that using a better robust initialization model could improve the task model's robustness. How PIR-AT is different from the existing practices is not well presented.**\n\n[1] studies the effect of self-supervised pre-training for robustness of image classifiers: they show that adversarial pre-training has no significant advantage over clean pre-training to provide best robust accuracy on CIFAR-10 when full adversarial fine-tuning is used (i.e. all network parameters are updated with adversarial training), as illustrated by Table 3 in [1]. In our case, we show that leveraging pre-trained image classifiers which are publicly available, e.g. in RobustBench, we can largely improve the performance of adversarial training on semantic segmentation tasks. This means that in this scenario, unlike in [1], using a robust backbone has a significant impact on the resulting robustness in the target task."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700319722806,
                "cdate": 1700319722806,
                "tmdate": 1700319722806,
                "mdate": 1700319722806,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W4VYeCnmV9",
                "forum": "aM7US5jKCd",
                "replyto": "r6QDYE92ZT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7859/Reviewer_CxEd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7859/Reviewer_CxEd"
                ],
                "content": {
                    "comment": {
                        "value": "I would highly appreciate the efforts the authors have put for rebuttal. Most of my concerns are addressed. I appreciate the workload of this paper for developing segmentation model evaluation protocol and the extensive experiments with various losses. However, as the proposed losses are all from existing works, SEA seems to be running the models with various losses combinations, and PIR-AT is just a common practice to apply an adversarially pre-trained model, I agree with the point of Reviewer pqj9 that the current presentation would fit for a workshop paper. Maybe after improving the writing this paper could be accepted by other top-level conferences, I believe the current format is not suitable for acceptance for ICLR. I would modify my rating to 5 to illustrate my point.\n\nBTW, after reading the rebuttal and other reviewers' comments, I raise more concerns about SEA. Table 8 is only verified on Pascal-VOC with one model, which somehow harm the reliability of the claim that four losses perform on par with all six losses for a generalized setting. Table 9 also indicates that on different dataset and models, different loss combinations may result in different conclusion(subset C performs best on UPerNet while subset A performs best on Segmentor for 12/255).\n\nFor W1's *visual examples* I mean the visual images after the perturbation.\n\nFor Q5's *result of AT with 32 epoch*, I made a mistake and I consider it would be better to add results of AT with 32 epoch for ADE20K, UPerNet with ConvNeXt-T backbone in Table 4 so that it would be more complete."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735669950,
                "cdate": 1700735669950,
                "tmdate": 1700736055361,
                "mdate": 1700736055361,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fqY2YnyyiU",
            "forum": "aM7US5jKCd",
            "replyto": "aM7US5jKCd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7859/Reviewer_tMpk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7859/Reviewer_tMpk"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the l_{\\limit} white-box adversarial attacks for semantic segmentation model. By discussing the loss functions used in semantic segmentation, i.e., pixel-level cross entropy loss, this paper shows the difficulty of adversarial attacks for semantic segmentation, than image classification model. Besides, this paper also proposes and compares 4 loss functions for semantic segmentation. As a result, to achieve higher attack performance, the proposed method combines 4 loss functions as SEA attack. Finally, this paper also studies the defense techniques for the above attacks. Comparing the proposed method with SegPGD and CosPGD, this work shows stronger attack performance won ADE20K and Pascal VOC. Finally, this paper also presents the comparison between the proposed defense PIR-AT with AT on several network architectures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The overall work is solid, that the proposed method starts from the analysis of loss functions for semantic segmentation. Besides, 4 different loss functions are compared, and then Semantic Ensemble Attack (SEA) is proposed, which is interesting.\n\n+ The evaluation is conducted under different attack strengths and network architectures.\n\n+ Different optimization methods are discussed for adversarial attacks, that fewer computation costs are needed."
                },
                "weaknesses": {
                    "value": "- This paper needs to discuss more about PIR-AT. It is only mentioned that this paper proposes Pre-trained ImageNet Robust Models. What is this method in detail? \n\n- What is the motivation to limit this paper to focus on l_{\\limit} threat model?"
                },
                "questions": {
                    "value": "See weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7859/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813291424,
            "cdate": 1698813291424,
            "tmdate": 1699636963375,
            "mdate": 1699636963375,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1IMDt9e8Lt",
                "forum": "aM7US5jKCd",
                "replyto": "fqY2YnyyiU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7859/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7859/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer tMpk"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for the positive feedback. In the following we address the weaknesses pointed out in the review.\n\n&nbsp;\n\n**Q: This paper needs to discuss more about PIR-AT. It is only mentioned that this paper proposes Pre-trained ImageNet Robust Models. What is this method in detail?**\n\nWith PIR-AT (as described in Sec. 3.1) we propose to use a robust image classifier, i.e. obtained with adversarial training, on ImageNet as initialization of the backbone of the segmentation model. Such initialization allows adversarial training on the segmentation task to achieve higher robustness than existing methods, even with lower computational cost. As shown in Table 3 and Table 4, our PIR-AT models outperform DDC-AT (Xu et al., 2021), SegPGD-AT (Gu et al., 2022), and match or improve over AT from clean initialization with 4-6x fewer training epochs.\n\n&nbsp;\n\n**Q: What is the motivation to limit this paper to focus on l_{\\limit} threat model?**\n\nWe focus on the $\\ell_\\infty$-threat model since this is the most popular in the literature: in fact, for image classification, the large majority of defenses reported in [RobustBench](https://robustbench.github.io/#div_imagenet_Linf_heading) are for $\\ell_\\infty$, and all of those on ImageNet (which we use as initialization of the model backbone in PIR-AT). Similarly, the closest prior work for robust semantic segmentation, SegPGD (Gu et al., 2022), focuses on $\\ell_\\infty$-bounded attacks. However, we think that SEA might be easily extended to other $\\ell_p$-threat models since APGD provides versions for such cases, which could be an interesting direction to explore for future work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700319202708,
                "cdate": 1700319202708,
                "tmdate": 1700319202708,
                "mdate": 1700319202708,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Dd9krWmr2g",
            "forum": "aM7US5jKCd",
            "replyto": "aM7US5jKCd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7859/Reviewer_pqj9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7859/Reviewer_pqj9"
            ],
            "content": {
                "summary": {
                    "value": "The paper focus on making robust evaluation of semantic segmentation models against L-inf adversarial attacks and show a straightforward approach to train robust segmentation models faster. Authors review existing measures like Jensen-Shannnon divergence, Masked cross-entropy and Masked spherical loss for their applicability as adversarial objectives for semantic segmentation task. They show that these objectives serve as better robustness evaluators than previously utilized objectives in the literature. They make three optimization related decisions: 1) replacing PGD with APGD, 2) progressively reduce attack radius and, 3) train for more iterations. Finally, they propose Segmentation Ensemble Attack (SEA) to evaluate models with different losses utilizing APGD and optimize for more iterations. Furthermore, to improve speed and efficiency of adversarial training, they initialize backbone of semantic segmentation models with ImageNet Robust Models and show a significant improvement on the adversarial robustness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-\tPaper is well written and easy to understand.\n\n-\tMotivation is clearly delivered.\n\n-\tDiscussed and reviewed different measures for their suitability for adversarial loss.\n\n-\tResults show that the candidate losses mentioned in the paper attack the model better.\n\n-\tBackbones initialized with ImageNet Robust Models provide higher adversarial training robust accuracy than using standard ImageNet model."
                },
                "weaknesses": {
                    "value": "My major concern is the lack of originality and novelty in the paper. All the losses, optimizations tricks, and robust models utilized in the paper are obtained from the existing literature (authors have cited the prior works sufficiently). There are no novel methodological contributions presented in the paper. Paper detail as different existing components collectively utilized to obtain better results. The findings shown in the paper are interesting. However, I believe that they alone do not support to meet the standards for accepting at a conference. Novel methodological contribution regarding losses or obtained robust models would be appreciated."
                },
                "questions": {
                    "value": "My concern mainly targets the core essence of the paper i.e. lack of originality and technical novelty. I appreciate the authors for conducting this study. I believe this paper would fit for a workshop submission."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7859/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7859/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7859/Reviewer_pqj9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7859/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836028645,
            "cdate": 1698836028645,
            "tmdate": 1700707862889,
            "mdate": 1700707862889,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FkrjYAYz8S",
                "forum": "aM7US5jKCd",
                "replyto": "Dd9krWmr2g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7859/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7859/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer pqj9"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for the feedback. Below we address their concern about novelty raised in the review.\n\n&nbsp;\n\n**Q: My major concern is the lack of originality and novelty in the paper. All the losses, optimizations tricks, and robust models utilized in the paper are obtained from the existing literature (authors have cited the prior works sufficiently). There are no novel methodological contributions presented in the paper. Paper detail as different existing components collectively utilized to obtain better results. The findings shown in the paper are interesting. However, I believe that they alone do not support to meet the standards for accepting at a conference. Novel methodological contribution regarding losses or obtained robust models would be appreciated.**\n\nWe appreciate that the Reviewer finds our results interesting and our paper well written. However, we think that the contributions of this paper are more than sufficient - in fact this paper sets completely new standards regarding robust semantic segmentation: \n\n1. on the level of robustness evaluation, our attack ensemble SEA shows that even the previously best attack SegPGD overestimates robustness by more than 17% (Table 1, PIR-AT model, evaluation at 12/255). As already observed with AutoAttack, a good and reliable attack for robustness evaluation enables better comparisons of new defense techniques and thus faster progress in the field.\n\n2. we show how to train robust semantic segmentation models and improve by more than 20 points in mIoU compared to the SegPGD-AT paper (their reported numbers are even for a weaker attack than ours). In the previous paper DDC-AT (Xu et al., 2021), a sophisticated method was presented, which unfortunately proved to be completely unrobust. Therefore, we believe that it is an important contribution of this work to show how to train robust semantic segmentation models using pre-trained robust ImageNet backbones, and even save computational time. Also, this shows that seemingly strong methodological contributions can end up being completely useless.\n\n3. we make all our code and models available (in contrast to SegPGD-AT) to foster research in this area as well as reproducibility.\n\nAdditionally to these points, we have clear methodological contributions as we analyze why the plain cross-entropy loss, one of the standard losses for attacks for image classification, does not work well for semantic segmentation. At the same time we show that the Jensen-Shannon divergence, a loss not used for adversarial attacks before, has exactly the properties needed for semantic segmentation (the gradient vanishes as the pixel is maximally misclassified) and thus does not require any masking. Thus while the losses themselves are not novel, there is a clear novelty in our analysis and in the justification why these losses are good for semantic segmentation."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700319021618,
                "cdate": 1700319021618,
                "tmdate": 1700319021618,
                "mdate": 1700319021618,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3viYmmrETP",
                "forum": "aM7US5jKCd",
                "replyto": "FkrjYAYz8S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7859/Reviewer_pqj9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7859/Reviewer_pqj9"
                ],
                "content": {
                    "comment": {
                        "value": "I have read all the reviews and authors response. I understand that this work show overestimation of robustness by prior works and suggest a robust ImageNet backbone for training. I agree on authors point regarding usage of Jensen-Shannon divergence for semantic segmentation. However, as I find the results interesting, but they are obtained from collective utilization of prior methods (as in conducting ablation study of different methods) without additional novel insights and contributions. After reconsidering authors response for all the reviewers, I increase my rating to 5 but not confident to consider it to be a potential contribution to the conference."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707850107,
                "cdate": 1700707850107,
                "tmdate": 1700707850107,
                "mdate": 1700707850107,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XbzirDJWjB",
            "forum": "aM7US5jKCd",
            "replyto": "aM7US5jKCd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7859/Reviewer_hjSQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7859/Reviewer_hjSQ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose an ensemble of adversarial attacks,  like in the AutoAttack framework, containing attacks with different loss functions for the task of semantic segmentation. In particular, they empirically show that existing loss functions for the task of semantic segmentation overestimate the confidence of robust models. Furthermore, they also train a robust model by utilizing the robust backbones from image classification literature, which significantly boosts performance while saving computing power"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The presentation of the paper is good \n- The empirical boost in performance is consistent across the board for different models\n- It is interesting to see the benefit that comes with the robust initialization using pre-trained Imagnet models"
                },
                "weaknesses": {
                    "value": "- My major concern is the limited novelty, as the explored loss functions are not new. Although JS divergence, masked CE loss, and masked spherical loss have not been commonly used in the context of segmentation attacks, in my view, this appears to be a simple 'plug and play' of loss functions\n\n\n- The conducted attacks are white-box, and the absence of black-box evaluation is a significant limitation\n\n- The paper only considers untargeted attacks, and it would be useful to extend the analysis to targeted attacks to showcase the strength of the proposed attack method.\n\n\n- The authors could conduct experiments to evaluate the transferability of the proposed attack to other models and compare it against the baseline PGD/CosPGD/SegPGD attacks."
                },
                "questions": {
                    "value": "- Please see my comments in the Weakness section.\n\n- Why is AT in Section 3 performed with the PGD attack baseline?  It would be interesting to use stronger attacks during the AT to develop even stronger robust models\n\n- How did you choose the budget scheme of 3:3:4 in the progressive reduction of epsilon approach?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7859/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698853838917,
            "cdate": 1698853838917,
            "tmdate": 1699636963139,
            "mdate": 1699636963139,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K2EgrFccCj",
                "forum": "aM7US5jKCd",
                "replyto": "XbzirDJWjB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7859/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7859/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer hjSQ (part 1)"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for the detailed feedback. In the following we address the weaknesses and questions raised in the review.\n&nbsp;\n\n**Q: My major concern is the limited novelty, as the explored loss functions are not new. Although JS divergence, masked CE loss, and masked spherical loss have not been commonly used in the context of segmentation attacks, in my view, this appears to be a simple 'plug and play' of loss functions**\n\nIn the discussion around the usage of different losses, we think we have clear methodological contributions: in fact, we analyze why the standard cross-entropy loss, one of the standard losses for attacks for image classification, does not work well for semantic segmentation. At the same time we show that the Jensen-Shannon divergence, a loss not used for adversarial attacks before, has instead the properties needed for semantic segmentation (the gradient vanishes as the pixel is maximally misclassified) and then does not require any masking. Thus, while the losses themselves are not novel, there is a clear novelty in our analysis and in the justification why these losses are good for semantic segmentation.\n\nMoreover, we uncover the complementarity of different losses, which is key, together with the improvements in the optimization algorithm, for our ensemble attack SEA to outperform the existing SOTA attacks by more than 17% (Table 1, PIR-AT model, evaluation at 12/255).\n\n&nbsp;\n\n**Q: The conducted attacks are white-box, and the absence of black-box evaluation is a significant limitation**\n\nTo extend the evaluation to black-box methods, we tried to adapt Square Attack [(Andriushchenko et al., 2020)](https://arxiv.org/abs/1912.00049), a SOTA score-based black-box attack for image classification, by using its random search-based algorithm to optimize our JS loss. However, the resulting attack has poor performance even on clean models. We hypothesize that the localized updates in Square Attack are not sufficiently effective to optimize pixelwise losses. Similarly we tested using gradient estimation via finite difference [(Ilyas et al., 2018)](https://arxiv.org/abs/1804.08598), but could not get performance close to that of white-box methods. We showed in the paper that balancing gradients of different pixels is already difficult with exact gradients, then we argue that having only approximated gradients is expected to make optimization even more challenging. This shows that finding an effective black-box attack for semantic segmentation requires designing task-specific algorithms, which could be the topic of an independent paper.\n\nAs additional black-box evaluation, we tested transfer attacks from less robust models to the PIR-AT. In particular, we run APGD on the Masked-CE loss on Segmenter models obtained with either clean training or AT (5 steps) on ADE-20k. We then transfer the found perturbations to our PIR-AT (5 steps, 128 epochs), and report robust accuracy and mIoU in Table A, together with the results of the white-box SEA on the same model (from Table 3 of the paper) as baseline. We observe that the transfer attacks are far from the performance of SEA, which further supports the robustness of the PIR-AT models. We are happy to add this evaluation (potentially expanded to include more models) in the revision of the paper.\n\n**Table A:** Robust average accuracy and mIoU given by transfer attacks on ADE-20k dataset. In boldface the white-box baseline by SEA.\n\n|source model | target model     | 0    |        | 4/255 |        | 8/255 |       | 12/255 |       |\n|:--------------|:-----------|-----------:|--------:|-------:|--------:|-------:|-------:|--------:|-------:|\n| | |aAcc | (mIou)| aAcc | (mIou)| aAcc | (mIou)|aAcc | (mIoU)|\n| Segmenter clean | Segmenter PIR-AT | 69.1 | (28.7) | 68.8 | (28.3) | 68.6 | (28.0)| 68.3 | (27.8) |\n| Segmenter AT | Segmenter PIR-AT | 69.1 | (28.7) | 66.3 | (26.0) | 63.1 | (23.8) |  57.4 | (19.9) |\n| **Segmenter PIR-AT** | **Segmenter PIR-AT** | 69.1 | (28.7) | 54.5 | (16.1) | 32.8 | (7.1) | 8.6 | (1.8) |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7859/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700318596214,
                "cdate": 1700318596214,
                "tmdate": 1700318596214,
                "mdate": 1700318596214,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]