[
    {
        "title": "Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo"
    },
    {
        "review": {
            "id": "9REDoQRQwx",
            "forum": "nfIAEJFiBZ",
            "replyto": "nfIAEJFiBZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5234/Reviewer_dAXr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5234/Reviewer_dAXr"
            ],
            "content": {
                "summary": {
                    "value": "The paper is about online RL algorithm design and theoretical analysis. Different from most previous RL theory works which lack deep RL demonstrations, this work proposes a both practical (scalable to deep RL domains) and provably efficient online RL algorithm (LMC-LSVI) based on the celebrated Langevin Monte Carlo algorithm. Theoretically, it proves that with linear function approximation LMC-LSVI achieves a $\\widetilde{\\mathcal{O}}(d^{3/2}H^{5/2}T^{1/2})$-online regret. On the practical side, LMC-LSVI is further extended to the Adam LMCDQN algorithm which performs similarly or even better than SOTA explorative deep RL algorithms in some challenging RL domains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Bridging the gap between RL theory and practice is of great importance to the advance of RL research. This work gives a possible and positive answer to this question in the specific setting of online RL where exploration-exploitation balance is a key problem. \n2. The proposed  Langevin Monte Carlo Least-Squares Value Iteration (LMC-LSVI) algorithm turns out to have a quite clean form which simply adds a noise term to the gradient descent update of the Bellman error (Line 9 of Algorithm 1) to incentivize exploration. This advantage thus allows for a deep RL extension where Adam-based adaptive SGLD is further applied.\n3. The proposed algorithm enjoys theoretical guarantees (in the linear function approximation setting) which is missing in most previous deep RL exploration methods even for LFAs."
                },
                "weaknesses": {
                    "value": "The rate of the online regret in linear function approximation setting is far from tight compared with known lower bounds. But from my view this is understandable given that a new approach is derived whose practicability is of higher importance."
                },
                "questions": {
                    "value": "1. Regarding the theoretical analysis (Theorem 4.2), I am curious why the failure probability $\\delta$ must be larger than a certain quantity, say $1/(2\\sqrt{2e\\pi})$? Is this inevitable for a sampling-stype algorithm and analysis? This will narrow the applicability of the theory since for frequentist regret analysis we always hope that the regret bound can hold for arbitrarily small fail probability.\n2. The authors say in the contribution part that \"unlike any other provably efficient algorithms for linear MDPs, it can easily be extended to deep RL settings\",..., \"such unification of theory and practice is unique in the current literature of both theoretical RL and deep RL\", which to my knowledge is overclaimed. Even though the perspective of Langevin dynamic are less explored in this regime, which is done by this paper, there do exist other works trying to achieve sample efficiency while being compatible with practical deep RL methods, e.g., [1, 2, 3]. So it seems improper to describe the work as unique given this line of research.\n\n**References:**\n\n[1] Feng, Fei, et al. \"Provably Correct Optimization and Exploration with Non-linear Policies.\" *International Conference on Machine Learning*. PMLR, 2021.\n\n[2] Kitamura, Toshinori, et al. \"Regularization and Variance-Weighted Regression Achieves Minimax Optimality in Linear MDPs: Theory and Practice.\" *International Conference on Machine Learning*. PMLR, 2023.\n\n[3] Liu, Zhihan, et al. \"One Objective to Rule Them All: A Maximization Objective Fusing Estimation and Planning for Exploration.\" *arXiv preprint arXiv:2305.18258* (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5234/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698461086390,
            "cdate": 1698461086390,
            "tmdate": 1699636521906,
            "mdate": 1699636521906,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OsyV9ZP1Ug",
                "forum": "nfIAEJFiBZ",
                "replyto": "9REDoQRQwx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5234/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5234/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dAXr"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable time and effort in providing detailed feedback on our work. We hope our response will fully address all of the reviewer's points. \n\n---\n### Tightness of regret bound.\n\nWe thank the reviewer for pointing this out. After the submission, we realized that a minimal change in the proof of Lemma B.4 (Lemma B.5 in the original submission) results in improving the bound by a factor of $H$. We refer the reviewer to our **Overall Response** for more detail on this improvement. Regarding the dependency on $d$, as mentioned in the main paper, *\u201dthe gap of $\\sqrt{d}$ in worst-case regret between UCB and TS-based methods is a long standing open problem, even in a simpler setting of linear bandit.*\u201d due to Hamidi and Bayati (2020) and is yet to be addressed. In the revised version, LMC-LSVI achieves a $\\tilde{O}(d^{3/2}H^{3/2}\\sqrt{T})$ online regret which matches **the best known** bound for any randomized algorithm for linear MDP.\n\n---\n### Why must the failure probability $\\delta$ be larger than a certain quantity?\n \nIndeed, for the frequentist regret analysis we would ideally hope that the regret bound can hold for a small failure probability. This mentioned interval arises from Lemma B.7 (Lemma B.8 in the original submission), where we get an optimistic estimation with a constant probability of $\\frac{1}{2\\sqrt{2e\\pi}}$. This is addressed and improved by using the **optimistic reward sampling** scheme proposed in [4].  Concretely, we can generate $M$ estimates for Q function $\\\\{Q\\_h^{k, m}\\\\}\\_{m \\\\in[M]}$ through maintaining $M$ samples of $w$: $\\\\{w\\_h^{k, J\\_k, m}\\\\}\\_{m \\\\in [M]}$ throughout the LMC. Then, we can make an optimistic estimate of Q function by setting $Q\\_h^{k}(\\\\cdot, \\\\cdot)= \\\\min\\\\{\\\\max\\_{m \\\\in [M]}\\\\{Q\\_h^{k, m}(\\\\cdot, \\\\cdot)\\\\}, H-h+1\\\\}.$ However, for the simplicity of the algorithm design we keep the current algorithm. We added a discussion of this in the revised version. Please refer to Remark 4.3 on page 5.\n\n---\n### There are other works trying to achieve sample efficiency while being compatible with practical deep RL methods.\n \nWe thank the reviewer for bringing these works to our attention, as they were inadvertently overlooked in our literature survey. We have cited them in our revision and slightly revised our discussion accordingly. The revised paragraph looks as follows:\n\n```\nUnlike many other provably efficient algorithms with linear function approximations (Yang and Wang, 2020; Cai et al., 2020; Zanette et al., 2020a; Ayoub et al., 2020; Zanette et al., 2020b; Zhou et al., 2021; He et al., 2023), LMC-LSVI can easily be extended to deep RL settings (Adam LMCDQN). We emphasize that such unification of theory and practice is rare (Feng et al., 2021; Kitamura et al., 2023; Liu et al., 2023) in the current literature of both theoretical RL and deep RL.\n```\n\nWe hope we have addressed all of your questions/concerns. If you have any further questions, we would be more than happy to answer them. Finally, we are grateful for the reviewer\u2019s positive support on our paper. Given our updated regret bound and comment regarding how to address the failure probability constraint, if the reviewer thinks the work is worthy of a higher score to be highlighted, we would be immensely grateful.\n\n\n[1] Feng, Fei, et al. \"Provably Correct Optimization and Exploration with Non-linear Policies.\" International Conference on Machine Learning. PMLR, 2021.\n\n[2] Kitamura, Toshinori, et al. \"Regularization and Variance-Weighted Regression Achieves Minimax Optimality in Linear MDPs: Theory and Practice.\" International Conference on Machine Learning. PMLR, 2023.\n\n[3] Liu, Zhihan, et al. \"One Objective to Rule Them All: A Maximization Objective Fusing Estimation and Planning for Exploration.\" arXiv preprint arXiv:2305.18258 (2023).\n\n[4] Haque Ishfaq, Qiwen Cui, Viet Nguyen, Alex Ayoub, Zhuoran Yang, Zhaoran Wang, Doina Precup, and Lin Yang. Randomized exploration in reinforcement learning with general value function approximation. In International Conference on Machine Learning, 2021."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5234/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700070141946,
                "cdate": 1700070141946,
                "tmdate": 1700111581097,
                "mdate": 1700111581097,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q5uasCDS98",
                "forum": "nfIAEJFiBZ",
                "replyto": "9vlD3UImfV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5234/Reviewer_dAXr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5234/Reviewer_dAXr"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks the authors for the very detailed response to my questions and concerns, and I am glad to see that the regret of the algorithm has been improved to the best known bound. I have no further concerns and I will keep my rate as 8."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5234/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682688962,
                "cdate": 1700682688962,
                "tmdate": 1700682688962,
                "mdate": 1700682688962,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Q8k4x1yIS2",
            "forum": "nfIAEJFiBZ",
            "replyto": "nfIAEJFiBZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5234/Reviewer_SrMF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5234/Reviewer_SrMF"
            ],
            "content": {
                "summary": {
                    "value": "They have introduced online RL algorithms incorporating Langevin Monte Carlo. \n\n   *They have established the regret bound for Q-learning with Langevin Monte Carlo under linear MDPs. \n\n   *They have adapted the aforementioned algorithm to use the ADAM optimizer, demonstrating its favorable empirical performance through experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "While the literature on Bayesian model-free algorithms is extensive, it is indeed lacking in works that offer both robust theoretical guarantees and practical viability. In my view, this paper effectively bridges this gap. \n\n* The paper presents a good comparison, highlighting noteworthy contributions compared to existing works.\n\n* Section 3 offers a non-trivial and novel analysis that significantly enhances the paper.\n\n* The practical version's experimental results in Section 5 demonstrate promising performance."
                },
                "weaknesses": {
                    "value": "* It looks Algorithms they use in practice (Algorithm 2) are not analyzed. So, this algorithm is \"practical\", but I am not sure it is fair to say this is \"provable.\" \n\n* Some aspects of the comparisons appear to lack complete fairness.\n\n   * In Table 1, it is unclear what precisely the author intends to convey with \"computational traceability\" and \"scalability.\" These terms should be defined more formally in the caption of the table. \n\n   * Furthermore, it may not be entirely fair to say that OPT-RLSVI and LSVI-PHE lack scalability in Table 1. While I acknolwedge that the proposed algorithms (OPT-RLSVI and LSVI-PHE) may not perform well in practice, one could argue that they could exhibit scability with the incorporation of certain simple heuristics, even if formal guarantees are absent. Indeed, the LSVI-PHE paper includes moderate experiments. The author has similarly extended theoretically justified algorithms to practical versions without formal backing as they did. So, in this sense, I am not sure why it is reasonable to say the author's algorithm is scalable, but their algorithms are not scalable."
                },
                "questions": {
                    "value": "I raised several concerns for the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5234/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5234/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5234/Reviewer_SrMF"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5234/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813908649,
            "cdate": 1698813908649,
            "tmdate": 1699636521810,
            "mdate": 1699636521810,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3z3sNWbKF5",
                "forum": "nfIAEJFiBZ",
                "replyto": "Q8k4x1yIS2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5234/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5234/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SrMF"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable time and effort in providing detailed feedback on our work. We hope our response will fully address all of the reviewer's points. \n\n---\n### 1. Why do you say the method is provable and practical?\n\nWe would like to clarify that in our submission, we claim LMC-LSVI (Algorithm 1) as provable and propose Adam LMCDQN (Algorithm 2) as a *\u201dpractical variant\u201d* of LMC-LSVI which is more well-suited for deep RL due to its usage of Adam based adaptive SGLD. Moreover, the proposed two algorithms can be viewed as a whole framework, which shows us how to provably conduct randomized exploration (under linear function approximation), and is easily generalized to practical implementation. \n\nWhile it\u2019s interesting to have convergence and regret guarantee for Adam LMCDQN (Algorithm 2), as shown in [1], Adam has issues in convergence and theoretically it has been shown that it may not converge to an optimal solution. We propose the Adam based extension of LMC-LSVI due to its many observed empirical advantages, and leave the theoretical convergence of it for future study.\n\n[1] Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. In Proceedings of the 6th International Conference on Learning Representations, 2018.\n\n---\n### 2. What are the definitions of computational tractability and scalability in Table 1? \n\nThanks a lot for your suggestion. By computational tractability in Table 1, we meant whether the algorithm is \u201cimplementable\u201d. By \u201cscalability\u201d we mean whether the algorithm can be easily extended to deep RL setup. In the revision, we precisely defined these terms more formally in the caption of Table 1. The caption currently says the following:\n\n```\nRegret upper bound for episodic, non-stationary, linear MDPs. Here, computational tractability refers to the ability of a computational problem to be solved in a reasonable amount of time using a feasible amount of computational resources. An algorithm is scalable if it can be easily extended to deep RL setup.\n```\n\n---\n### 3. Why do you say OPT-RLSVI and LSVI-PHE lack scalability in Table 1?\n\nThanks for asking the questions and providing your insight on the comparison. We added more discussion in Remark 6.1 and the caption of Table 1 to explain our comparison. \n\nNote that the proposed LMC-LSVI algorithm can be easily extended to deep RL by plugging in neural network function approximation of the Q-function, without any change of the algorithm framework. In contrast, one major disadvantage of LSVI-PHE is that it needs to maintain many ensembles of Q-function approximators that are fitted using independent copies of perturbed datasets at each round. This is computationally heavy and memory-wise extremely burdensome. Without changing the algorithm framework of LSVI-PHE, it will be computationally impractical for environments like Atari, for which we may require many ensemble models. Regarding OPT-RLSVI, we believe even with heuristics, it is not clear how to provide its practical implementation for the reasons outlined in Remark 6.1. Concretely, OPT-RLSVI relies on the feature norm with respect to the inverse covariance matrix. When the features are updated over iterations in deep RL, the computational complexity becomes unbearable as it needs to compute the feature covariance matrix repeatedly. \n\n---\nWe hope we have addressed all of your questions/concerns. If you have any further questions, we would be more than happy to answer them and if you don\u2019t, would you kindly consider increasing your score?"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5234/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700070012302,
                "cdate": 1700070012302,
                "tmdate": 1700070012302,
                "mdate": 1700070012302,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4CrOeR4en4",
                "forum": "nfIAEJFiBZ",
                "replyto": "mlPyM57Ir6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5234/Reviewer_SrMF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5234/Reviewer_SrMF"
                ],
                "content": {
                    "comment": {
                        "value": "I checked the response.  \n\n>> We would like to clarify that in our submission, we claim LMC-LSVI (Algorithm 1) as provable and propose Adam LMCDQN (Algorithm 2) as a \u201dpractical variant\u201d of LMC-LSVI which is more well-suited for deep RL due to its usage of Adam based adaptive SGLD.\n\nThen, the current title might be provable \"or\" practical. But, not \"and\". Usually. when we use \"and\", we would think the same algorithm can achieve both properties.  A current title sounds overclaiming.  \n\n>> By \u201cscalability\u201d we mean whether the algorithm can be easily extended to deep RL setup.\"\n\nI generally recommend that you remove the \"scalability\" column after reading your response again. We can take many arbitrary interpretations for \"easily extended\". This is perceived as an unfair comparison for many readers.  If you want to convey this kind of stuff, the author can just claim in a sentence that we implement our algorithm in these complicated deep environments, where it is unclear whether existing works (cite) work, or something.  But I think this simple table is biased and misleading. \n\n>> Answer to \"Why do you say OPT-RLSVI and LSVI-PHE lack scalability in Table 1?\" \n\nI am still not convinced. Did you check your statement with the authors?  I think they would claim their algorithms can be \"easily\" extended. For example, this sounds like a biased statement. \n\n* Note that the proposed LMC-LSVI algorithm can be easily extended to deep RL by plugging in neural network function approximation of the Q-function, without any change of the algorithm framework.  ---> But, you changed anyway as well. We can consider many ways to plug in. So, I am not sure this is treated as \"easily extended\". Whether it is easy or not is more subjective. \n\n*  \"Without changing the algorithm framework of LSVI-PHE, it will be computationally impractical for environments like Atari, for which we may require many ensemble models\".  ---> Did you check with them? Or did you run experiments?"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5234/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696900579,
                "cdate": 1700696900579,
                "tmdate": 1700696900579,
                "mdate": 1700696900579,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "djsa05WDMT",
            "forum": "nfIAEJFiBZ",
            "replyto": "nfIAEJFiBZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5234/Reviewer_yHqj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5234/Reviewer_yHqj"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies usefulness of LMC algorithms for MDPs. It first shows an LMC algorithm deployed with LSVI for linear MDPs and upper bounds the corresponding regret. It also proposes versions of LMC used with Adam and DQN for practical purposes. The performance of the practical algorithm is illustrated on multiple atari games."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper proposes a regret upper bound for applying LMC with LSVI for linear MDPs.\n2. The paper proposes a practical version of LMC to be applied with DQN. It works in practice as shown through multiple experiments."
                },
                "weaknesses": {
                    "value": "1. The theoretical analysis is shown for linear MDPs and the practical algorithm is applied with DQN. Thus, the theory and applications serve two different purposes and does not compliment each other. It makes the paper look like an assortment of results for different settings than a coherent study.\n2. If the aim is to design a practical algorithm, why analysing it for linear MDPs, which is known to be unfit for practice (the version stated in the paper) [1]? Why not analysing it for more practical models like [1], [2], [3]?\n3. The regret bound for LMC lsvi is loose in terms of both d and H. Why is it so? Is it due to any fundamental hardness in analysis or inherent to LMC algorithms or just a shortcoming of the analysis done in the paper? Can you explain this?\n4. The practical version is claimed to be better than the existing LMC algorithm for RL cause the proposal has theoretical guarantees and also employs better practical techniques. The first is not completely valid as the setting plus algorithm for analysis and practice are really different. The second is also not clear as the experimental results leave the question of performance improvement statistically inconclusive . Can you provide a reason where adam LMC DQN would work better than langevin DQN and where it would be worse?\n\n[1] Zhang, Tianjun, Tongzheng Ren, Mengjiao Yang, Joseph Gonzalez, Dale Schuurmans, and Bo Dai. \"Making linear mdps practical via contrastive representation learning.\" In International Conference on Machine Learning, pp. 26447-26466. PMLR, 2022. \n[2] Ouhamma, Reda, Debabrota Basu, and Odalric Maillard. \"Bilinear exponential family of MDPs: frequentist regret bound with tractable exploration & planning.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, no. 8, pp. 9336-9344. 2023.\n[3] Weisz, Gell\u00e9rt, Andr\u00e1s Gy\u00f6rgy, and Csaba Szepesv\u00e1ri. \"Online RL in Linearly $ q^\\pi $-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore.\" arXiv preprint arXiv:2310.07811 (2023)."
                },
                "questions": {
                    "value": "Please check the weaknesses for questions ."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5234/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5234/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5234/Reviewer_yHqj"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5234/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699273729259,
            "cdate": 1699273729259,
            "tmdate": 1700750364046,
            "mdate": 1700750364046,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "y6jNcdZH30",
                "forum": "nfIAEJFiBZ",
                "replyto": "djsa05WDMT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5234/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5234/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yHqj (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable time and effort in providing detailed feedback on our work. We hope our response will fully address all of the reviewer's points. \n\n---\n### 1. The theory and applications seem to serve two different purposes?\n\nIn this paper, along with providing theoretical analysis of LMC-LSVI (Algorithm 1) under linear MDP, we also provide experimental results for it in Appendix E.1 in simulated linear MDPs and the riverswim environment. We later show that we can readily extend the proposed algorithm to deep RL. This is a favorable property of the proposed algorithms that is inspired by stochastic gradient descent and can be generalized to deep learning settings. The theoretical analysis serves as an inspiration to develop the practical methods.\n\nConcretely, the proposed Adam LMCDQN (Algorithm 2) is an instantiation of LMC-LSVI in deep RL by using Adam based adaptive SGLD with DQN as a backbone. DQN being a canonical value based learning method, it makes sense to incorporate Langevin monte carlo based exploration with it when instantiating LMC-LSVI for deep RL setting. \n\n---\n### 2. Why analyzing it for linear MDPs instead of other models?\n\nFor the study in the paper, we choose linear MDP for its relative generality and potential to provide insights for practical algorithms. We agree that the mentioned variants are more general; however, they also lack the generality needed for real application. As the reviewer knows, there is always a more general setting to be studied. We would also like to highlight that the theoretical analysis of RL algorithms in the linear MDP setting is itself a challenging task, especially for randomized exploration based algorithms such as LMC-LSVI. Nevertheless, analyzing LMC based approaches in newer models like [1], [2], [3] is certainly an interesting direction for future extensions and we have included them in our revised conclusion (Page 9, Section 8) . We provide a copy of the modified version here:\n\n\n```\nWe proposed the LMC-LSVI algorithm for reinforcement learning that uses Langevin Monte Carlo to directly sample a Q function from the posterior distribution with arbitrary precision. LMC-LSVI achieves the best-available regret bound for randomized algorithms in the linear MDP setting. Furthermore, we proposed Adam LMCDQN, a practical variant of LMC-LSVI, that demonstrates competitive empirical performance in challenging exploration tasks. There are several avenues for future research. It would be interesting to explore if one can improve the suboptimal dependence on H for randomized algorithms. Extending the current results to more practical and general settings (Zhang et al., 2022; Ouhamma et al., 2023; Weisz et al., 2023) is also an exciting future direction.\n```\n\n---\n### 3. Why is the regret of LMC-LSVI loose in terms of both $d$ and $H$?\n\nWe thank the reviewer for pointing this out. After the submission, we realized that a minimal change in the proof of Lemma B.4 (Lemma B.5 in the original submission) results in improving the bound by a factor of $H$. We refer the reviewer to our **Overall Response** for more details on this improvement. Regarding the dependency on $d$, as mentioned in the main paper (Section 4, page 5), *\u201dthe gap of $\\sqrt{d}$ in worst-case regret between UCB and TS-based methods is a long standing open problem, even in a simpler setting of linear bandit.*\u201d due to Hamidi and Bayati (2020) and is yet to be addressed. In the revised version, LMC-LSVI achieves a $\\tilde{O}(d^{3/2}H^{3/2}\\sqrt{T})$ online regret, which matches **the best known** bound for any randomized algorithm for linear MDP.\n\nHere we provide the updated comment from the revised paper (Section 4, page 5)\n\n\u201cWe compare the regret bound of our algorithm with the state-of-the-art results in the literature of theoretical reinforcement learning in Table 1. Compared to the lower bound $\\Omega(dH\\sqrt{T})$ proved in Zhou et al. (2021), our regret bound is worse off by a factor of $\\sqrt{dH}$ under the linear MDP setting. However, the gap of $\\sqrt{d}$ in worst-case regret between UCB and TS-based methods is a long standing open problem, even in a simpler setting of linear bandit (Hamidi and Bayati 2020). When converted to linear bandits by setting $H = 1$, our regret bound matches that of LMCTS (Xu et al., 2022) and the best-known regret upper bound for LinTS from Agrawal and Goyal (2013) and Abeille et al. (2017).\u201d"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5234/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069775718,
                "cdate": 1700069775718,
                "tmdate": 1700111421662,
                "mdate": 1700111421662,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]