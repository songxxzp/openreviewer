[
    {
        "title": "Sparse Weight Averaging with Multiple Particles for Iterative Magnitude Pruning"
    },
    {
        "review": {
            "id": "o3PsZEWatp",
            "forum": "Y9t7MqZtCR",
            "replyto": "Y9t7MqZtCR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4640/Reviewer_Eb3S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4640/Reviewer_Eb3S"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents SWAMP, a novel method using the average of multiple particles' stochastic weight averaging (SWA) to achieve improved model performance. The authors have tested SWAMP\u2019s effectiveness across various tasks, including vision models (CNNs) and language models (RoBerTa finetuning), providing a comprehensive evaluation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written, and easy-to-follow.\n2. The authors have conducted extensive experiments, covering vision tasks and language model fine-tuning. Additional studies such as mask analysis and efficient implementation SWAMP+ are also provided."
                },
                "weaknesses": {
                    "value": "While the authors address the computational efficiency of SWAMP+ in section 4.3, stating that it can utilize a single particle for the first few iterations, this claim seem to work because the networks are already very sparse. My concern lies in the computational cost of SWAMP+ at lower sparsity levels. Furthermore, did the authors test SWAMP+ on ImageNet?\n\nSee also my questions.\n\nI am willing to adjust my rating if my questions are addressed."
                },
                "questions": {
                    "value": "1. The results in Figure 4 show that interpolated weights yield even lower errors compared to IMP weights. Could the authors provide a detailed explanation or hypothesis as to why this is the case?\n2. Appendix B states, \"The learning rate for this phase (SWA phase) is set to a constant value of 0.05.\" Does this imply that the minimum learning rate is set at 0.05 for SWAMP, and for other baselines such as IMP?\n3. Do IMP and SWAMP use the same epoch T_0 to rewind weights?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4640/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4640/Reviewer_Eb3S",
                        "ICLR.cc/2024/Conference/Submission4640/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719614368,
            "cdate": 1698719614368,
            "tmdate": 1700596291240,
            "mdate": 1700596291240,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1XIsquN6h8",
                "forum": "Y9t7MqZtCR",
                "replyto": "o3PsZEWatp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4640/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "__[W1] *(Re. SWAMP+ in low sparsity regime):*__ Thank you for pointing out the effectiveness of SWAMP+ in low sparsity regimes. We agree with the reviewer that the effectiveness of SWAMP+ fades under low sparsity regimes. That said, IMP is a SOTA algorithm for extremely high sparse networks (sparsity over 90%). We thus suggest two alternatives instead of SWAMP+ for low sparsity regimes. Firstly, dynamic sparse methods (e.g., RigL) can also benefit from SWAMP as shown in Table 9 which exhibits significantly lower training costs than IMP. Secondly, leveraging the robustness of SWAMP (refer to Table 7), one can employ higher pruning ratio which can still boost the training speed even in low sparsity regimes. We believe the general response could further resolve your concerns regarding the training cost of the algorithm. Additionally, as per your great suggestion, we hereby provide the results of IMP+ on ImageNet in Table R.10. Please note that the results in Table R.10 differ from Table 3, as the experiment was conducted earlier with a different set of hyperparameters.\n\n__[Q1] *(Re. superior performance of interpolated networks in Fig. 4):*__ We appreciate the reviewer for highlighting empirical analysis in Figure 4. Following [Paul et al., 2023], Figure 4 illustrates that there exists a linear path on the loss landscape connecting two successive IMP/SWAMP solutions. In case of both IMP/SWAMP, as the reviewer pointed out, the interpolated weights exhibit lower loss compared to obtained solutions which is also corroborated by several prior works [Garipov et al., 2018; Wortsman et al., 2021; Nam et al., 2022; Paul et al., 2023]. This finding suggests that one can obtain a better solution interpolating successive solutions which is the main motivation for Lottery Pools [Yin et al., 2023]. Further, while we only highlight the existence of linear connectors in the paper, we here provide additional experimental results in Tables R.11 and R.12 suggesting that SWAMP can also benefit from such a phenomenon, i.e., combining SWAMP and Lottery Pools leads to further performance enhancement.\n\n__[Q2] *(Clarification: learning rate setting):*__ Thank you for the attentive review. In our IMP experiments, we explored two learning rate schedules: cosine annealing (initiating from 0.1 and decaying to 0.001) and step learning rate (adopted from SWA; dropping to a constant value of 0.05). The cosine annealing schedule turned out to be the best for IMP, while SWAMP exhibited better performance with step lr schedule. In short, we employed the cosine annealing schedule for IMP and the step lr schedule for SWAMP. \n\n__[Q3] *(Clarification: rewinding epochs):*__ Yes, indeed. We set the same matching epoch for both IMP and SWAMP. \n\n\\\n__Table R.10.__ Results of SWAMP+ on ImageNet. \n| Method | 86.4% | 89.1% | 91.2% | 92.9% | 94.3% |\n| :-     | :-    | :-    | :-    | :-    | :-    |\n| IMP    | 76.15 | 76.02 | 75.48 | 75.16 | 74.43 |\n| SWAMP+ | 76.55 | 76.33 | 76.06 | 75.45 | 74.84 |\n\n\\\n__Table R.11.__ Further comparison between (a) IMP, (b) SWAMP, (c) Lottery Pools, and (d) SWAMP + Lottery Pools algorithm on CIFAR-10.\n|     | Sparsity 50% | Sparsity 75% | Sparsity 90% | Sparsity 95% |\n| :-  | :-:          | :-:          | :-:          | :-:          |\n| (a) | 93.97 \u00b1 0.16 | 94.02 \u00b1 0.23 | 93.90 \u00b1 0.15 | 93.58 \u00b1 0.09 |\n| (b) | 94.74 \u00b1 0.04 | 94.88 \u00b1 0.09 | 94.73 \u00b1 0.10 | 94.23 \u00b1 0.11 |\n| (c) | 94.39 \u00b1 0.16 | 94.28 \u00b1 0.14 | 94.16 \u00b1 0.11 | 93.43 \u00b1 0.23 |\n| (d) | 94.65 \u00b1 0.14 | 94.70 \u00b1 0.25 | 94.52 \u00b1 0.25 | 94.31 \u00b1 0.28 |\n\n\\\n__Table R.12.__ Further comparison between (a) IMP, (b) SWAMP, (c) Lottery Pools, and (d) SWAMP + Lottery Pools algorithm on CIFAR-100.\n|     | Sparsity 50% | Sparsity 75% | Sparsity 90% | Sparsity 95% |\n| :-  | :-:          | :-:          | :-:          | :-:          |\n| (a) | 75.40 \u00b1 0.23 | 75.72 \u00b1 0.41 | 75.24 \u00b1 0.25 | 74.60 \u00b1 0.37 |\n| (b) | 77.29 \u00b1 0.53 | 77.35 \u00b1 0.39 | 77.14 \u00b1 0.33 | 76.48 \u00b1 0.73 |\n| (c) | 76.31 \u00b1 0.51 | 76.17 \u00b1 1.03 | 75.84 \u00b1 0.67 | 75.14 \u00b1 0.49 |\n| (d) | 77.52 \u00b1 0.46 | 77.45 \u00b1 0.53 | 77.36 \u00b1 0.61 | 77.10 \u00b1 0.54 |\n\n\n\\\n__References__\n- Garipov et al., \u201cLoss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs.\u201d NeurIPS 2018. \n- Wortsman et al., \u201cLearning Neural Network Subspaces.\u201d ICML 2021.  \n- Nam et al., \u201cImproving Ensemble Distillation With Weight Averaging and Diversifying Perturbation.\u201d ICML 2022. \n- Paul et al., \u201cUnmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask?\u201d ICLR 2023. \n- Yin et al., \u201cLottery Pools: Winning More by Interpolating Ticket without Increasing Training or Inference Cost.\u201d AAAI 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699969938023,
                "cdate": 1699969938023,
                "tmdate": 1699972726336,
                "mdate": 1699972726336,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l67PehJvCL",
                "forum": "Y9t7MqZtCR",
                "replyto": "1XIsquN6h8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4640/Reviewer_Eb3S"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4640/Reviewer_Eb3S"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "I thank the authors for their responses. I have the following additional questions.\n\n### 1. W1 and training costs.\n\n**No additional training costs of parallelization?** While I understand your perspective, I maintain a different view regarding the impact of parallelization. Parallelization transforms time costs into the costs of using multiple GPUs, which should be accounted for. \n\n**SWAMP+.** Regarding SWAMP+, the addition of results in Table R.10 is appreciated. Could the authors possibly provide a comparison of FLOPs between IMP and SWAMP+ in this table? \n\n### 2. Q2 and unfair comparison\n\nThank you for your response to Q2. I appreciate the information shared. However, to address concerns about potentially unfair comparisons, could the authors provide the results of IMP with the step learning rate, such as in CIFAR-10/100?\n\n### 3. Q1 and Q3\n\nThank you for your detailed clarification."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510131085,
                "cdate": 1700510131085,
                "tmdate": 1700510131085,
                "mdate": 1700510131085,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9mof0LB1wI",
                "forum": "Y9t7MqZtCR",
                "replyto": "cekIrWzkMG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4640/Reviewer_Eb3S"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4640/Reviewer_Eb3S"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your reply"
                    },
                    "comment": {
                        "value": "Thank you for your clarification. Most of my concerns are addressed and I have adjusted my rating from 5 to 6."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596277737,
                "cdate": 1700596277737,
                "tmdate": 1700596277737,
                "mdate": 1700596277737,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XaCITnjJeD",
            "forum": "Y9t7MqZtCR",
            "replyto": "Y9t7MqZtCR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4640/Reviewer_1f6B"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4640/Reviewer_1f6B"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends the Iterative Magnitude Pruning (IMP) technique by proposing an approach called SWAMP that trains multiple sparse models called particles in each magnitude pruning iteration of IMP using Stochastic Weighted Averaging (SWA) optimization. The particles in each pruning iteration exhibit the same matching ticket and their diversity is achieved through different batch orders. The trained particle masks are combined in the weighted average fashion to get the single mask of a given pruning iteration. This process of training multiple particles followed by the weighted average of their mask is repeated until desired sparsity or pruning iteration is achieved. The experimentation is conducted on multiple datasets along with different architectures to showcase the effectiveness of the proposed SWAMP model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The authors have done a great job in terms of summarizing their contributions compared to the IMP technique in Section 3.2. \n* The paper is very easy to read and the proposed contribution can be easily understood through Algorithm 1.\n* Extensive experimentation is conducted on multiple tasks (vision and language), multiple datasets, and multiple architectures. \n* A very comprehensive ablation study is conducted to showcase the effectiveness of the proposed components in the paper. For example, Table  5 clearly shows the importance of the SWA optimization along with the weighted average mechanism of the particles to enhance the performance."
                },
                "weaknesses": {
                    "value": "* In terms of methodology, the proposed technique provides an empirically guided straightforward extension over the IMP technique. The proposed SWAMP therefore has a trivial contribution and therefore lacks novelty.\n* In terms of experimentation, the performance gain over other techniques seems to be marginal and  reduces the significance of their proposed methodology. \n* In Figure 3, for relatively lower sparsity (e.g., sparsity of 20%), the proposed Weighted Average (WA) technique seems to underperform the individual particle performance. Does this mean, the proposed technique  harm the performance on the lower sparsity? The authors may need to provide more extensive justification to explain this phenomenon."
                },
                "questions": {
                    "value": "In Figure 3, why does the proposed technique have a lower performance compared to individual particles in the lower network sparsity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4640/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4640/Reviewer_1f6B",
                        "ICLR.cc/2024/Conference/Submission4640/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764650558,
            "cdate": 1698764650558,
            "tmdate": 1700681841259,
            "mdate": 1700681841259,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mmHOWOJhTn",
                "forum": "Y9t7MqZtCR",
                "replyto": "XaCITnjJeD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4640/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "__[W1] *(Re. lack of novelty):*__ Our primary contribution extends beyond the development of an algorithm; it involves empirically demonstrating that sparse networks trained with varying SGD noise can benefit from weight-averaging, especially as they become sparser \u2013 a finding that is not immediately obvious and has been acknowledged by Reviewer-tk5N. Moreover, as Reviewer-MGna highlighted, our work is inspired by the theoretical framework of [Paul et. al, 2023] which enabled us to empirically validate whether SWAMP retains the key property of IMP; we kindly refer the reviewer to Section 3.3.\n\n__[W2] *(Marginal improvement of the algorithm):*__ Please refer to General Response B. \n\n__[W3, Q1] *(Analysis under lower sparsity regime):*__ Thank you for highlighting the failure of sparse weight averaging under low sparsity regimes. We can see that weight averaging fails at the earlier stages of IMP due to the highly non-convex nature of the landscape. However, as sparsity increases, particles tend to locate in the same wide basin which enables weight-averaging. Such a finding is in line with [Frankle et al., 2020; Paul et al., 2023] demonstrating the ease of finding a low-loss curve with a smaller network compared to a larger one, i.e., a sparse network tends to be more stable. Additionally, it further supports that our algorithm benefits more with sparser networks.\n\n\\\n__References__\n- Frankle et al., \u201cLinear mode connectivity and the lottery ticket hypothesis.\u201d ICML 2020. \n- Paul et al., \u201cUnmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask?\u201d ICLR 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699973426512,
                "cdate": 1699973426512,
                "tmdate": 1699973426512,
                "mdate": 1699973426512,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "um2kMBAQMc",
                "forum": "Y9t7MqZtCR",
                "replyto": "dsyMNAmatM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4640/Reviewer_1f6B"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4640/Reviewer_1f6B"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Rebuttal"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for the clarification.  While I maintain my view that the proposed technique has limited novelty, I commend the authors for effectively justifying the empirical results during the rebuttal phase. Therefore, I am inclined to increase the score from 5 to 6."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681939348,
                "cdate": 1700681939348,
                "tmdate": 1700681939348,
                "mdate": 1700681939348,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HyJB2CPuy6",
            "forum": "Y9t7MqZtCR",
            "replyto": "Y9t7MqZtCR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4640/Reviewer_YhLg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4640/Reviewer_YhLg"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a modification to the Iterative Magnitude Pruning algorithm, SWAMP. The basis of this algorithm is the empirical evidence that different models trained from the same matching tickets can be weight averaged without encountering a loss barrier post certain sparsity levels. SWAMP obtains marginal accuracy improvements with respect to the baselines used."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. The manuscript is well written\n\nS2. The method is empirically sound and arguments are well made.\n\nS3. Extensive empirical is provided to justify the merit in this approach."
                },
                "weaknesses": {
                    "value": "W1. The authors have not empirically justified their choice of using Stochastic Weight Averaging (SWA) as opposed to SGD in the manuscript. It would be important to understand the impact of SWA on the proposed approach by demonstrating two things.\n1. How does IMP perform when it uses SWA as opposed to SGD.\n2. How does SWAMP perform when it uses SGD as opposed to SWA.\n\nW2. Multiple instances of imprecise statements. For example, \"As illustrated in Figure 1, our algorithm achieves superior performance, which is on par with that of an ensemble consisting of two sparse networks.\" It is not clear with respect to what are the authors claiming superior performance? Because in Figure 1, IMP-3 outperforms SWAMP in terms of accuracy."
                },
                "questions": {
                    "value": "Q1. I would like to understand why is it that the authors choose to average the weights in SWAMP? As demonstrated in Figure 1, there might be individual IMP runs that outperform SWAMP. Why not take the best of multiple pruned weights?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4640/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4640/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4640/Reviewer_YhLg"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807778517,
            "cdate": 1698807778517,
            "tmdate": 1699636443821,
            "mdate": 1699636443821,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nm3sQjDFV3",
                "forum": "Y9t7MqZtCR",
                "replyto": "HyJB2CPuy6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4640/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "__[W1] *(Ablation study w.r.t. SWA & multiple particles):*__ Thank you for pointing out the issue. We indeed provided an ablation study regarding SWA in Table 5. In detail, the second and the third row each correspond to SWAMP without SWA and IMP with SWA, respectively. We can readily find that each component, SWA and multi-particles, contributes similarly to the performance gain, and the optimal performance is attained when used in combination.\n\n__[W2] *(Clarity: performance comparison to ensembles):*__ We greatly appreciate the reviewer for pointing out unclear points. In Figure 1, we show that SWAMP matches the performance of an ensemble of two sparse networks but not surpassing that of an ensemble of three networks, which is still a significant performance gain considering impressive performance of Deep Ensembles [Lakshminarayanan et al., 2017; Fort et al., 2019]. Further, in the right side of Figure 1, we demonstrate how SWAMP offers advantages over ensembles in terms of memory costs.\n\n__[Q1] *(Re. best seed performance):*__   As per your insightful suggestion, we provide the performance of an IMP ensemble, wherein each member is chosen as the best-performing model from multiple random seeds. Tables R.8 and R.9 below verify that exploring more random seeds does not result in additional improvements.\n\n\\\n__Table R.8.__ Further comparison between (a) IMP, (b) IMP selected from best seed, and (c) SWAMP  algorithm on CIFAR-10.\n|     | Sparsity 50% | Sparsity 75% | Sparsity 90% | Sparsity 95% |\n| :-  | :-:          | :-:          | :-:          | :-:          |\n| (a) | 93.97 \u00b1 0.16 | 94.02 \u00b1 0.23 | 93.90 \u00b1 0.15 | 93.58 \u00b1 0.09 |\n| (b) | 93.91 \u00b1 0.23 | 94.14 \u00b1 0.13 | 93.81 \u00b1 0.14 | 93.72 \u00b1 0.27 |\n| (c) | 94.74 \u00b1 0.04 | 94.88 \u00b1 0.09 | 94.73 \u00b1 0.10 | 94.23 \u00b1 0.11 |\n\n__Table R.9.__ Further comparison between (a) IMP, (b) IMP selected from best seed, and (c) SWAMP  algorithm on CIFAR-100.\n|     | Sparsity 50% | Sparsity 75% | Sparsity 90% | Sparsity 95% |\n| :-  | :-:          | :-:          | :-:          | :-:          |\n| (a) | 75.40 \u00b1 0.23 | 75.72 \u00b1 0.41 | 75.24 \u00b1 0.25 | 74.60 \u00b1 0.37 |\n| (b) | 75.79 \u00b1 0.53 | 75.05 \u00b1 0.39 | 74.92 \u00b1 0.58 | 74.35 \u00b1 0.43 |\n| (c) | 77.29 \u00b1 0.53 | 77.35 \u00b1 0.39 | 77.14 \u00b1 0.33 | 76.48 \u00b1 0.73 |\n\n\n\\\n__References__\n- Lakshminarayanan et al., \u201cSimple and Scalable Predictive Uncertainty Estimation using Deep Ensembles.\u201d NeurIPS 2017. \n- Fort et al., \u201cDeep Ensembles: A Loss Landscape Perspective.\u201d arXiv preprint, 2019."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699973550275,
                "cdate": 1699973550275,
                "tmdate": 1699973635919,
                "mdate": 1699973635919,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ehor07AkZL",
                "forum": "Y9t7MqZtCR",
                "replyto": "HyJB2CPuy6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4640/Reviewer_YhLg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4640/Reviewer_YhLg"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your rebuttal. My concerns have been addressed. I request the authors to fix imprecise statements in the manuscript such as that mentioned W2. I also request the authors to provide recent and relevant citations and a more precise replacement for their statement on IMP being state-of-the-art in their abstract (eg. IMP is SoTA in unstructured pruning at high-sparsity). Because this is not true in the structured sparsity regime [a].\n\nI will maintain my score.\n\n[a] Neural Pruning via Growing Regularization, Wang et al. ICLR 2021"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679755646,
                "cdate": 1700679755646,
                "tmdate": 1700679795827,
                "mdate": 1700679795827,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hYmJHT5a9x",
            "forum": "Y9t7MqZtCR",
            "replyto": "Y9t7MqZtCR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4640/Reviewer_tk5N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4640/Reviewer_tk5N"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose weight averaging of sparse models trained from a checkpoint of a single model, in many ways \"model soups\" for Iterative Maginitude Pruning (IMP). The authors motivate the method for IMP as as model soups in the dense context are, with the loss landscape perspective: we know that LTs lie within the same loss basin, and might expect that weight averaging would find a better generalizing solution. Experiments demonstrate that the approach identifies solutions within a flatter region of the loss basin, and improved generalization over IMP and many other sparse training methods for CIFAR-10/100 and ImageNet models."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper is overall well-written, with a good organization, clear writing for the most part, and a clear methodology.\n* The experimental analysis is appropriate, using reasonable datasets and models (except VGG), and demonstrates clear knowledge of the sparse training literature appropriate to the methodology.\n* The paper has a clear and well defined motivation: the method is motivated as cheaper than ensembles, much along the same lines of the model soups paper and how it is motivated for dense training.\n* The loss landscape analysis also originally used in model soups is clearly applicable to the sparse domain, especially since much of the linear-mode connectivity methodology comes from the sparse literature to begin with.\n* Hessian Trace analysis also provides some signal that the loss-landscape motivation for weight averaging holds in the sparse realm."
                },
                "weaknesses": {
                    "value": "* The method comes down to applying the model soup paper to sparse training/IMP. I believe there is sufficient novelty in applying a method only shown on dense training and not necessarily repeatable in the sparse training context, never mind the extensive analysis shown by the authors in this work. Saying that, it's also not the most novel research direction out there compared to many papers.\n* As presented in the main body of the paper, SWAMP is *much* more expensive than most of the compared sparse training methods in e.g. Table 2 at *training time*. This is because IMP with weight rewinding is extraordinarily expensive in practice. However, the authors do demonstrate that the SWAMP methodology applies to other much more efficient sparse training methods in the appendix, notably RiGL, a state-of-the-art sparse training method, and one that is reasonably efficient. I believe the authors should focus their method as being widely applicable to sparse training methods in the main body of the paper, rather than focusing on IMP however - this is especially important given the motivation that SWAMP is better than training an ensemble (which is in fact likely cheaper than SWAMP when using more practical sparse training methods than IMP!).\n* While CIFAR-10/100 results are relatively strong, the ImageNet results (Table 3) are relatively quite weak and not as obviously significant."
                },
                "questions": {
                    "value": "* While the paper is motivated by comparing the generalization of a SWAMP to an ensemble of two IMP solutions, what is the comparison in generalization when using other sparse training methods, e.g. RiGL, given that these other methods often generalize better than IMP?\n* Is there any reason to believe SWAMP is not a general method that applies to any sparse training method? If so what? If not, why focus on IMP?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819183253,
            "cdate": 1698819183253,
            "tmdate": 1699636443737,
            "mdate": 1699636443737,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JIgFMry8cG",
                "forum": "Y9t7MqZtCR",
                "replyto": "hYmJHT5a9x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4640/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "__[W1] *(Extension to different hyperparameters):*__ We thank the reviewer for recognizing the novelty of our work, where our main objective is to empirically validate sparse weight averaging technique and whether it preserves the characteristics of IMP rooted in the theoretical framework of [Paul et al., 2023]. As the reviewer pointed out the work of model soups [Wortsman et al., 2022], we hereby provide extended results of SWAMP with multiple particles obtained from various hyperparameters settings (weight decaying rate, learning rate) beyond random seeds in Table R.5. Our findings indicate that weight averaging with particles perturbed by means other than random seeds do not markedly improve performance. We acknowledge that further optimization of hyperparameters, given enough time, could potentially boost SWAMP's performance making it an interesting direction for future research. That said, one should also consider whether the improvements of SWAMP variants justify the additional computational cost they demand.\n\n__[W2, Q2] *(Heavy training cost & extension to other pruning methods):*__ As per your constructive feedback, we will revise the final manuscripts to include the SWAMP-extended dynamic pruning results in the main body of the paper, not the appendix. As shown in Table 9, SWAMP is indeed applicable to other sparse training methods. During the rebuttal phase, as time permits, we aim to investigate whether dynamic pruning methods, aside from RigL, can also benefit from SWAMP. Moreover, we choose to focus on IMP in order to verify whether SWAMP preserves the theoretical characteristics outlined in [Paul et al., 2023] as discussed in Section 3.3. Also, we believe that General Response A could further resolve your concerns regarding the training cost of the algorithm.\n\n__[W3] *(Marginal gain in ImageNet experiments):*__ Please refer to General Response B. \n\n__[Q1] *(Re. ensemble of dynamic pruning methods):*__  As per your detailed feedback, we hereby provide additional results on the ensemble of RigL and DST solutions. Tables R.6 and R.7 provide clear evidence that SWAMP exhibits comparable generalization to test sets when compared to ensembles of other baseline methods.\n\n\\\n__Table R.5.__ Further comparison on Cifar-10 with ResNet-20 between (a) SWAMP particles with varying weight decaying rates, and (b) SWAMP particles with varying learning rates. Reported values are averaged over three random seeds.\n|     | Sparsity 50% | Sparsity 75% | Sparsity 90% | Sparsity 95% |\n| :-  | :-:          | :-:          | :-:          | :-:          |\n| IMP | 91.57 \u00b1 0.08 | 91.51 \u00b1 0.08 | 90.44 \u00b1 0.16 | 89.07 \u00b1 0.33 |\n| SWAMP | 92.68 \u00b1 0.04 | 92.59 \u00b1 0.20 | 92.03 \u00b1 0.05 | 90.62 \u00b1 0.06 |\n| (a) | 91.33 \u00b1 0.29 | 91.84 \u00b1 0.22 | 91.41 \u00b1 0.22 | 90.17 \u00b1 0.11 |\n| (b) | 92.48 \u00b1 0.19 | 92.49 \u00b1 0.12 | 91.82 \u00b1 0.17 | 90.52 \u00b1 0.09 |\n\n__Table R.6.__ Additional accuracy results on the ensemble of two particles of (a) VGG13 with RigL, (b) VGG13 with DST on CIFAR-10. And (c) is the performance of a single SWAMP solution. Reported values are averaged over three random seeds.\n|     | Sparsity 50% | Sparsity 75% | Sparsity 90% | Sparsity 95% |\n| :-  | :-:          | :-:          | :-:          | :-:          |\n| (a) | 94.34 \u00b1 0.13 | 94.12 \u00b1 0.15 | 94.15 \u00b1 0.22 | 93.33 \u00b1 0.09 |\n| (b) | 94.54 \u00b1 0.09 | 94.48 \u00b1 0.19 | 94.29 \u00b1 0.14 | 94.11 \u00b1 0.02 |\n| (c) | 94.14 \u00b1 0.08 | 94.39 \u00b1 0.15 | 94.40 \u00b1 0.16 | 94.34 \u00b1 0.11 |\n\n__Table R.7.__ Additional accuracy results on the ensemble of two particles of (a) VGG16 with RigL, (b) VGG16 with DST on CIFAR-100. And (c) is the performance of a single SWAMP solution. Reported values are averaged over three random seeds.\n|     | Sparsity 50% | Sparsity 75% | Sparsity 90% | Sparsity 95% |\n| :-  | :-:          | :-:          | :-:          | :-:          |\n| (a) | 74.43 \u00b1 0.21 | 73.66 \u00b1 0.20 | 73.15 \u00b1 0.15 | 71.91 \u00b1 0.09 |\n| (b) | 75.06 \u00b1 0.14 | 74.87 \u00b1 0.21 | 74.46 \u00b1 0.16 | 73.67 \u00b1 0.16 |\n| (c) | 73.27 \u00b1 0.26 | 73.54 \u00b1 0.36 | 73.40 \u00b1 0.33 | 73.53 \u00b1 0.32 |\n\n\\\n__References__\n- Wortsman et al., \u201cModel soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.\u201d ICML 2022. \n- Paul et al., \u201cUnmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask?\u201d ICLR 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977690687,
                "cdate": 1699977690687,
                "tmdate": 1700014097921,
                "mdate": 1700014097921,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Dend36OryM",
                "forum": "Y9t7MqZtCR",
                "replyto": "hYmJHT5a9x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4640/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer-tk5N,\n\nWe sincerely appreciate your time and the constructive feedback. With the Author/Reviewer discussion deadline approaching, we would be grateful if you let us know whether our responses have addressed your concerns. This will greatly assist us in enhancing our work. For any further clarification, we are more than happy to answer.\n\nWarm regards, \\\nAuthors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544813297,
                "cdate": 1700544813297,
                "tmdate": 1700544931745,
                "mdate": 1700544931745,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aodZabaEvJ",
                "forum": "Y9t7MqZtCR",
                "replyto": "JIgFMry8cG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4640/Reviewer_tk5N"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4640/Reviewer_tk5N"
                ],
                "content": {
                    "comment": {
                        "value": "I'd like to thank the authors for their rebuttal, and apologize for my late participation in the rebuttal period - this was due to exceptional circumstances.\n\nI believe the authors have addressed most of my questions/concerns, and hopefully in incorporating the feedback they did, this has also strengthened the paper significantly."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680947230,
                "cdate": 1700680947230,
                "tmdate": 1700680947230,
                "mdate": 1700680947230,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AgLefLDqdr",
            "forum": "Y9t7MqZtCR",
            "replyto": "Y9t7MqZtCR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4640/Reviewer_MGna"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4640/Reviewer_MGna"
            ],
            "content": {
                "summary": {
                    "value": "SWAMP (Sparse Weight Averaging with Multiple Particles) is a new pruning method that enhances the performance of sparse neural networks by averaging multiple models trained with different stochastic gradients but sharing an identical sparse structure, known as a \"matching ticket.\" This process results in improved generalization due to the creation of flat minima and maintains the important linear connectivity between successive solutions, a key strength of the traditional Iterative Magnitude Pruning (IMP) method. SWAMP has demonstrated its ability to outperform other pruning baselines across various datasets and network structures. The technique's success invites further theoretical investigation into why the convex hull of the weight space of these averaged models forms a beneficial low-loss subspace, which could provide deeper insights into the algorithm's effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The motivation behind SWAMP is firmly rooted in robust theoretical frameworks, notably the lottery ticket hypothesis and the concept of linear mode connectivity.\n- The visualization of the loss landscape in Figure 2 provides a clear illustration of the methodology and supports the validation of the claims made.\n- It is clear from the evidence presented in Table 4 that SWAMP is adept at identifying more effective pruning masks.\n- Table 2 and 3 demonstrate that SWAMP achieves superior classification accuracy for a designated target sparsity level."
                },
                "weaknesses": {
                    "value": "- The study's reliance on demonstrating the process primarily through wide networks such as WRN and VGG-19, which are not the most parameter-efficient architectures, raises questions about the choice of models. An explanation of why these particular, potentially less efficient, models were selected for this research is needed.\n\n- The improvement in accuracy provided by SWAMP over IMP is modest, as shown in Tables 2 and 3, and this increment is even less pronounced for the ResNet model as evidenced in Table 3. This calls for a discussion on the significance of the marginal gains achieved by SWAMP, particularly when benchmarked against other models.\n\n- The feasibility of achieving an optimal sparse structure with SWAMP, especially for pre-trained models which are commonplace, may entail significant computational costs. It is imperative that the authors address the computational overhead, both in terms of space and time complexity, and the practical constraints when applied to large models, including Transformers. A comprehensive discussion on the limitations is warranted, given that IMP\u2014the foundation of SWAMP\u2014may have its own constraints with larger models.\n\n- The applicability of the proposed method to architectures like Transformers needs clarification. In Table 8, the RoBERTa model exhibits a noticeable performance drop even with less than 50% sparsity. The question arises as to whether this decline is attributed to the inherent limitations of IMP, on which SWAMP is based, or if it pertains to the broader challenges of applying pruning techniques to RoBERTa. Additionally, it would be beneficial to understand whether the principles behind SWAMP remain valid for other models, such as GPT-like architectures, and how they compare with alternative pruning strategies for these models."
                },
                "questions": {
                    "value": "Please refer to Weakness comments"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4640/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4640/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4640/Reviewer_MGna"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4640/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698918316275,
            "cdate": 1698918316275,
            "tmdate": 1700562825007,
            "mdate": 1700562825007,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AXKi5AAoV4",
                "forum": "Y9t7MqZtCR",
                "replyto": "AgLefLDqdr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4640/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "__[W1] *(The choice of model architectures):*__ We appreciate your attentive review. Throughout the experiments, we aim to verify the effectiveness of SWAMP via various architectures including ResNet (Tiny-ImageNet, ImageNet), Wide-ResNet (Cifar-10, Cifar-100), VGG (Cifar-10, Cifar-100), RoBERTa (GLUE). As per your great suggestion, in Tables R.3 and R.4, we additionally provide experimental results of ResNet-20 on Cifar10 and ResNet-18 on Cifar100, which is a frequently employed experimental setup in prior works [Paul et al., 2023; Yin et al., 2023]. We can readily find that SWAMP works well on not only wide-networks but also on parameter-efficient architectures which highlights that overparameterization is not a crucial factor for the success of SWAMP. \n\n__[W2] *(Marginal gain in Tables 2 and 3):*__ Please refer to General Response B. \n\n__[W3] *(Re. heavy computational costs):*__ Please refer to General Response A. \n\n__[W4] *(Re. transformer-based architectures):*__ Thank you for highlighting the observed performance drop of RoBERTa models beyond 50% sparsity. As shown in Table 8, while both IMP and SWAMP struggle to recover the matching performance, SWAMP still surpasses IMP in terms of accuracy throughout all sparsity levels. In other words, such performance drop stems from inherent limitations of IMP, not a drawback of SWAMP. Moreover, [Chen et al., 2020] supports our findings by empirically confirming that BERT networks suffer to find winning tickets compared to vision task models (See Table 3 in [Chen et al., 2020]). Also, we certainly agree with the reviewer that exploring IMP on GPT-like architectures would be a valuable addition to the community. However, as the focus of our work lie in exploring sparse weight averaging techniques along with empirical verification of the theoretical framework of IMP, we leave it as a future research. \n\n\\\n__Table R.3.__ Additional results on Cifar-10 with ResNet-20. Reported values are averaged over three random seeds.\n|     | Sparsity 50% | Sparsity 75% | Sparsity 90% | Sparsity 95% |\n| :-  | :-:          | :-:          | :-:          | :-:          |\n| IMP | 91.57 \u00b1 0.08 | 91.51 \u00b1 0.08 | 90.44 \u00b1 0.16 | 89.07 \u00b1 0.33 |\n| SWAMP | 92.68 \u00b1 0.04 | 92.59 \u00b1 0.20 | 92.03 \u00b1 0.05 | 90.62 \u00b1 0.06 |\n\n__Table R.4.__ Additional results on Cifar-100 with ResNet-18. Reported values are averaged over three random seeds.\n|     | Sparsity 50% | Sparsity 75% | Sparsity 90% | Sparsity 95% |\n| :-  | :-:          | :-:          | :-:          | :-:          |\n| IMP | 75.24 \u00b1 0.51 | 75.38 \u00b1 0.33 | 75.15 \u00b1 0.35 | 74.51 \u00b1 0.45 |\n| SWAMP | 76.25 \u00b1 0.39 | 76.52 \u00b1 0.08 | 76.70 \u00b1 0.03 | 76.43 \u00b1 0.10 |\n\n\\\n__References__\n- Chen et al., \u201cThe Lottery Ticket Hypothesis for Pre-trained BERT Networks\u201d, NeurIPS 2020. \n- Paul et al., \u201cUnmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask?\u201d ICLR 2023. \n- Yin et al., \u201cLottery Pools: Winning More by Interpolating Ticket without Increasing Training or Inference Cost.\u201d AAAI 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699973994860,
                "cdate": 1699973994860,
                "tmdate": 1700023970465,
                "mdate": 1700023970465,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3RTD04JIcK",
                "forum": "Y9t7MqZtCR",
                "replyto": "AgLefLDqdr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4640/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4640/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder"
                    },
                    "comment": {
                        "value": "Dear reviewer-MGna,\n\nWe sincerely appreciate your time and the constructive feedback. With the Author/Reviewer discussion deadline approaching, we would be grateful if you let us know whether our responses have addressed your concerns. This will greatly assist us in enhancing our work. For any further clarification, we are more than happy to answer.\n\nWarm regards, \\\nAuthors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544781756,
                "cdate": 1700544781756,
                "tmdate": 1700544922467,
                "mdate": 1700544922467,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SQPHwSoMao",
                "forum": "Y9t7MqZtCR",
                "replyto": "AXKi5AAoV4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4640/Reviewer_MGna"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4640/Reviewer_MGna"
                ],
                "content": {
                    "title": {
                        "value": "Response from Reviewer MGna"
                    },
                    "comment": {
                        "value": "Thank you for thorough comments and the additional experimental results. However, this reviewer has a couple of lingering questions:\n1. Given that unstructured pruning hasn't been widely implemented due to the current limitations in hardware support, I'm curious about the practical benefits of Iterative Magnitude Pruning (IMP) in today's inference context. Why focus on IMP rather than exploring hardware-friendly pruning techniques, which might be more immediately applicable?\n2. In what scenarios do you anticipate a significant divergence between the performances of IMP and SWAP? Particularly, I'm interested in knowing if this gap becomes more pronounced with larger models, such as Transformers with over 1 billion parameters. This aspect is crucial, as demonstrating the effectiveness of your proposed technique on such large-scale models, including vision transformers or large language models (LLMs), would significantly elevate the impact of your research. Unfortunately, I couldn't find convincing evidence in the current submission that supports the utility of your technique for these large models.\n\nNevertheless, I acknowledge that your proposed method seems to outperform IMP, as evidenced by the empirical results and the supporting theories you've presented. In light of this, I have decided to increase my score from 5 to 6."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4640/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562807421,
                "cdate": 1700562807421,
                "tmdate": 1700562807421,
                "mdate": 1700562807421,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]