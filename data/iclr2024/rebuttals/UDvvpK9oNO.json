[
    {
        "title": "Dual Diffusion Model for One-Shot High-Fidelity Talking Head Generation"
    },
    {
        "review": {
            "id": "lf1Cu0iYet",
            "forum": "UDvvpK9oNO",
            "replyto": "UDvvpK9oNO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2624/Reviewer_FJie"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2624/Reviewer_FJie"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a diffusion-based talking head generator.  The model first generates a sequence of landmarks from the audio, then these landmarks and an identity image are used with a conditional diffusion model to create the resulting image sequence.  To ensure consistency in the generated image sequence, blocks of image frames are generated together rather than frames generated individually."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Talking head generation is an important and challenging problem as viewers are highly sensitive to inconsistencies in lip motion.\n+ Open datasets are used.\n+ The approach seems sensible.  Controlling lip motion independently of non-speech facial motion allows the network to focus on what is important for speech, and helps avoid spurious correlations that might otherwise be captured.  Also, the use of 3D landmarks as an intermediate representation allows for effective pose normalization."
                },
                "weaknesses": {
                    "value": "- The main weakness of the paper is missing important details about the architecture, the training, and any hyperparameters, which makes reproducing the work impossible.\n- The paper states this is the first time one-shot diffusion models have been used for modeling talking faces.  However, see this work:  Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation.\n- The abstract reads like the approach requires a single image of a person and some accompanying audio to generate talking head sequences.  However, the method also requires a sequence of landmarks to drive the pose of the talking head, as the speech motion generation is all aligned to the mean of the 3DMM.\n- There is no accompanying video, which is the only way to gauge the quality of a talking head sequence.  The static frames in Figure 4 are insufficient \u2014 and this figure suggests problems with the generated lip shapes to me.  For the speaker in the lefthand images:  the third frame for your method has the lips too rounded, and the lip pursing in the fourth frame is under articulated.  For righthand speaker:  the inner mouth is wrong in the second frame, and the lip closure is missed in the fourth frame.\n- There is no subjective / perceptual study to qualitatively gauge the quality of the generated lip motions."
                },
                "questions": {
                    "value": "- If you wish to generate speech for a specific speaker in a one-shot fashion, how do you know how to generate the correct inner mouth if the reference frame has the mouth closed?  In addition, how does your model know how to capture the idiosyncrasies for a given speaker if you have not seen their specific speaking style before?  For example, in the extreme case that a speaker speaks out of one side of their mouth?\n- Why are only 68 landmarks extracted from the broader 3DMM?  Why not use the full mesh?\n- In equation (2), the difference between a mesh and the mean of the model does not transform \u201cthe posed face keypoints into a frontal face keypoints\u201d \u2014 to do this you need to properly align (using rotations and scaling).  Also, is i in this sequence a frame index?\n- In the description of the Audio-to-Motion Generation, you refer to a feature-wise linear modulation block (FiLM).  This does not appear to be labelled in the architecture diagrams, and there is no detail specifically about this.\n- Sequences of frames are generated simultaneously to ensure that identity is consistent across frames \u2014 you still need to be concerned with identity across blocks of frames though.  This is not discussed.\n- A driving video is required from which the pose is extracted for the generated sequence.  Is there any consideration for the consistency for the head pose that accompanies the speech?  Viewer can be sensitive to mismatches in head pose and speech (e.g., timing, rhythm, etc.).\n- For the conditional video diffusion, three 2D latents are introduced \u2014 what are these representing?\n- The latents zx represent S frames, but zr a single frame.  How is this mismatched handled?\n- None of the loss terms related to speech information specifically.  This is surprising.  For example, Equation (5) is encouraging the images to be similar, but in a global sense.  Errors in the cheeks (or planar regions) are less significant the say lips almost being closed when they should actually be closed.\n- Why was a user study not run to measure the quality of the speech generation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2624/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698255391904,
            "cdate": 1698255391904,
            "tmdate": 1699636201783,
            "mdate": 1699636201783,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "MamOcvGujV",
            "forum": "UDvvpK9oNO",
            "replyto": "UDvvpK9oNO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2624/Reviewer_KsG2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2624/Reviewer_KsG2"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a two-stage diffusion model for one-shot talking head generation. They use two diffusion models sequentially: (1) a HuBERT-conditioned DDPM for audio-to-motion prediction; (2) a landmark-conditioned PVDM for motion-to-video rendering. It contributions are twofolds: (1) it is the first work that utilize diffusion model for facial motion prediction; (2) it improves the temporal smoothness and system efficiency of the diffusion model for talking head generation. Compared with previous diffusion-based method, DiffTalk, it reduces the training time from 120h to 48h, and inference time from 875s to 125s. The paper writing is clear and is easy to follow. \n\nHowever, the novelty is relatively limited, and the system efficiency still seems unpractical in real-world applications. Besides, there is demo video available in the initial submission, which is necessary to assess the quality of a video generation paper. In summary, I tend to give a rate of 5."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- It well utilizes two different diffusion models, DDPM and PVDM for talking face generation. \n- It is the first work that utilizes diffusion model for facial motion prediction; \n- By using PVDM, it improves the temporal smoothness and system efficiency of the diffusion model for talking head generation."
                },
                "weaknesses": {
                    "value": "- The novelty is limited, both of DDPM used in audio-to-motion stage and PVDM used in motion-conditioned video generation stage are well-known models. \n- No demo videos are available."
                },
                "questions": {
                    "value": "- In Figure 3, it seems that several input frames are required for video rendering. This raises a concern that is it really a one-shot method? \n- The author could provide demo videos for qualitative evaluation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2624/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698661090112,
            "cdate": 1698661090112,
            "tmdate": 1699636201583,
            "mdate": 1699636201583,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "sbJXYlCwIZ",
            "forum": "UDvvpK9oNO",
            "replyto": "UDvvpK9oNO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2624/Reviewer_qQ9o"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2624/Reviewer_qQ9o"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a diffusion model based approach to synthesize full-face lip motion video conditioned on audio and a single facial image. The proposed method consists of two major components. The first one called AToM-Net is an audio-to-motion generation network that utilize HuBERT model and 3DMM to generate 3D facial landmarks given audio signals. The second one called MC-VDM takes the generated 3D landmarks and reference facial image to generate a sequence of frames with the same identity as the reference image and lip motion encoded by the landmarks. Experimental evaluation on benchmark dataset shows the proposed method has competitive temporal consistency and video quality compared to other state-of-the-art methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper proposed a novel approach to synthesize facial video with realistic lip motion conditioned on audio signal and a reference face image. The method has the advantage of diffusion model in high realism while keeping the temporal and identity consistency within a sequence of images. \n2. The proposed synthesis framework has improved efficiency in both training and inference time compared to most recent diffusion model based approach.\n3. Both quantitative and qualitative results shown the proposed method have competitive performance compared to state-of-the-art talking head generation methods."
                },
                "weaknesses": {
                    "value": "1. Although the proposed method is intuitive and conceptually appealing in the sense that facial landmarks are used to drive the facial and lip motion, the quantitative results seem to send a mixed signal. In Table 1, LSE-C and LSE-D are only better than MakeItTalk. On CSIM, Wav2Lip is the best instead of the proposed approach. CPBD is also worse than a few state-of-the-arts. On FID, the proposed approach achieved the best score but only marginally better than Wave2Lip. On qualitative results, there are only a handful of examples. It would be more convincing to provide additional examples or even better, to provide videos to show the temporal evolution. Therefore, it is hard to say the results achieved by proposed method is substantially better than state-of-the-arts.\n2. The ablation study on proposed approach can also be strengthened to demonstrate the impact of key design choice. For example, on AtoM-Net, the use of local attention on split of facial landmarks was claimed to be an important factor of improving performance. But there is study on how the specific split of landmarks (e.g. number of split, composition of each split) affect the performance. There is no ablation study on MC-VDM. One factor that is worth analyzing is the impact of different loss term used to train the model.\n3. It was not clearly explained why proposed method can achieve faster training and inference time."
                },
                "questions": {
                    "value": "1. In MC-VDM architecture illustration Figure 3, does the video encoder takes landmark and image frames jointly as input? Or is it the video encoder will encode landmark and image sequence separately? If encoding was done separately, are we using the same encoder for two types of input?\n2. According to the paper, it seems AtoM-Net and MC-VDM are trained separately. Have the authors considered joint training of the two modules? Do we have any expectation or known challenging in doing so?\n3. Some notation was not specified correctly or consistently. Can the authors clarify? For example, in the last line of page 5, $l\\in \\mathbb{R}^{S\\times H \\times w}$ should be $l\\in \\mathbb{R}^{S\\times H \\times W}$. The second line of page 6, should $zr^s$ be $zr^{xy}$? What is the $\\lambda$ value used in defining $\\mathbb{E}_{\\epsilon, t}$ and how is it determined?\n4. Finally, there are some typos, for example, in page 8, sentence 'Lip-sync metrics even though we. ' seems to be broken from the context. The authors should proofread the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2624/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821746631,
            "cdate": 1698821746631,
            "tmdate": 1699636201498,
            "mdate": 1699636201498,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "mbiLC9LJaJ",
            "forum": "UDvvpK9oNO",
            "replyto": "UDvvpK9oNO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2624/Reviewer_4W8w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2624/Reviewer_4W8w"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a two stage approach for generating single image talking head videos, based on diffusion networks.  This two stage process consists of: a transformer-based diffusion model that generates facial landmarks from audio (using 3DMMs for frontalization of landmarks), and in the second stage a diffusion net uses the landmarks and a reference image to genrate a naturalistic talking head video.  The authors propose using different attention networks for lip and non-lip face regions, which aims to address the fact that lip variability has less independence from audio, with different dynamics.  The results are promising over prior diffusion models, and this model is more optimized / faster to train than the ones compared with."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of separate lip/non-lip attention is valuable and works well, although different variation of head parts could emerge from the design of the architecture rather than a-priori\n\nThe results are shown to be state of the art, and the model is efficient"
                },
                "weaknesses": {
                    "value": "limitations are not adequately addressed\n\nevaluation metrics are lacking - perhaps not only for this paper but for the problem at hand (E.g. see below)\n\nAblation studies are focused on AToM-NET but not much on MC-VDM\n\nComparisons in speed only with Difftalk in table 2?"
                },
                "questions": {
                    "value": "the limitations of the method are not adequately mentioned. also, what are the limitations of the metrics employed? most measure lip to audio similarity, \n\nmethods that change only lips excel in video quality; but how can one measure that generating the video is in more agreement with the audio rather than simply adapting mouth? (I can see intuitively why this should be more reasonable of course)\n\nwhat happens when 3dmm fitting fails? how does this affect generalization to more diverse expressions?\nr"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2624/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698869316744,
            "cdate": 1698869316744,
            "tmdate": 1699636201393,
            "mdate": 1699636201393,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]