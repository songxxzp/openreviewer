[
    {
        "title": "Alignment as Reward-Guided Search"
    },
    {
        "review": {
            "id": "SbkrFOF8G5",
            "forum": "shgx0eqdw6",
            "replyto": "shgx0eqdw6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2603/Reviewer_6nE9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2603/Reviewer_6nE9"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces ARGS, a decoding framework that enhances the alignment of generated text with human preferences. It achieves this by employing a reward mechanism that guides the text-generation process of a language model. The method consists of reward-guided scoring and token selection. The goal is to generate text that is both coherent and contextually relevant while satisfying specific alignment criteria or objectives. The method improves the average reward compared to standard decoding and demonstrates better lexical diversity without compromising contextual consistency. The experiments validate the effectiveness of ARGS in aligning the generated text with human preference."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Authors aim to resolve an important problem in the reasearch area. \n\n- The problem is interesting. \n\n- Good discussion on broader impacts."
                },
                "weaknesses": {
                    "value": "- There are some serious issues in citation. \"Decoupled weight decay regularization\" is an ICLR-19 paper, not an arxiv. Please refer to https://openreview.net/forum?id=Bkg6RiCqY7. The authors should list all wrong citations and revise them. I will check the similar issues in all citations one by one. \n\n- Qualitative results are limited. I suggest that the authors provide more results to support the claims. It is hard for me to have a clear understanding of the improvements. If an anonymous web demo or a code link is provided, I will revise my rating. \n\n- In Figure 2, different types of lines are not shown in the figure. It is not very clear. \n\n- Compared with classical decoding methods, ARGS has higher time complexity. Although the $k$ can be small, it also has a higher complexity. Besides, small $k$ is not good for performance. \n\n- No period in Equation $T_{ARGS}(n, m, k)$. \n\n- The latest baseline method is a paper published in 2022. More baselines should be compared. \n\n- I am concerned about the technical novelty of the paper. The idea of reward-guided search has been proposed in SCST (Self-Critical Sequence Training). Besides, the technical contribution compared with the baseline is a tricky implementation, which is marginal. \n\n- The user study is needed for evaluation. \n\n- Missing discussion on limitation. \n\nOverall, the writing of this submission is unprofessional and the technical contribution is marginal. I provide a reject rating here and I will revise the rating according to the authors; rebuttal and other reviews."
                },
                "questions": {
                    "value": "See weakness.\n\n---\n\nRevise rating from 3 to 5, 5->6."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2603/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2603/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2603/Reviewer_6nE9"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2603/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697857713505,
            "cdate": 1697857713505,
            "tmdate": 1700615518957,
            "mdate": 1700615518957,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A8ex2RrZy2",
                "forum": "shgx0eqdw6",
                "replyto": "SbkrFOF8G5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2603/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2603/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6nE9 - Part I"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the detailed review, which has helped us strengthen the manuscript. Below we address your comments and suggestions:\n\n> **Citation format**\n\nWe appreciate your attention to detail. During the manuscript preparation, we made a conscientious effort to adhere to the proceeding format when citing relevant papers. However, we acknowledge the possibility of oversights during the editing phase. As suggested, we conducted a comprehensive check and revised several citations, including the one you highlighted.\n\nIt is important to note that some of the cited papers currently exist solely on arXiv without an associated proceeding venue, primarily due to their recent release, especially for works originating in 2023. For these instances, we have retained the citations in their current form and will update them when they are formally published.\n\n> **Qualitative results and code link**\n\nIn Section 3.3, we conducted qualitative evaluations across 300 randomly sampled prompts from the test set of HH-RLHF. Table 2 presents the qualitative evaluation results, measured by the percentage of win-ties of our method over the alternative decoding strategies. During rebuttal, we have updated our draft by incorporating more qualitative examples. For details, we direct the reviewer to **Appendix D**.\n\nTo facilitate further scrutiny of our work, we have included an anonymous link to our code (in the global response to all reviewers). This link will provide you with access to the experiments and implementation details.\n\n> **Figure 2**\n\nThat's a great catch. For clarity, we have revised the figure by changing the legend (ARGS-greedy 10 and 40) to a rectangle instead of a line.\n\n> **Time complexity**\n\nWe would like to point out that employing small $k$ yields similar performance as large $k$. This has been empirically validated in **Section 3.2**. Specifically, Figure 2 illustrates that the performance variation between $k=40$ (darker color bars) and $k=10$ (light color bars) is slight, suggesting that a large number of candidates may not be essential for producing aligned generations.\n\nEmpirically, we find that the time complexity overhead can be considerably small. For example, using the OPT-2.7b base model, the generation time per response increases only by 1.9 times when comparing ARGS ($k=10$) with conventional greedy decoding. Despite the slight increase in processing time, there was a notable enhancement in reward performance by $\\uparrow$ 6.8%, demonstrating the existence of a reasonable tradeoff between reward optimization and inference speed. The gap can be further reduced by employing a smaller reward model, or parallelizing the reward computation across $k$ candidates. \n\nOur discussion on time complexity has also been endorsed by reviewer Mqii, who commented: \n> _\"The authors do discuss the extra computation added at inference time and show its feasibility. I believe that **such a method is interesting and useful for the literature even with this extra weight at inference**. For example, it can be used to iterate over different reward models before running only one finetuning, or used directly if we have a small enough and good reward model.\"_\n\n\n> **More baselines**\n\nWe have added a latest baseline DPO (Direct Preference Optimization) [1] from NeurIPS 2023. The results have been added to **Section 4**. \n\n> **Human evaluation**\n\nWe employ a GPT-4-based evaluation approach to assess the quality of responses. Research, as discussed in [2], has shown that using a GPT-4 proxy aligns with human evaluations in over 80% of cases, providing a scalable method to approximate human preferences. This evaluation method is prevalent in recent literature, as evidenced by studies such as [1, 3].\n\nDue to practical considerations, our Institutional Review Board (IRB) protocol for conducting human evaluations is currently undergoing review at our institution. We fully intend to incorporate human evaluations into our study as soon as our IRB protocol receives approval."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2603/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075693522,
                "cdate": 1700075693522,
                "tmdate": 1700690122049,
                "mdate": 1700690122049,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IVaHuOnwnq",
                "forum": "shgx0eqdw6",
                "replyto": "2W1H3FZShG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2603/Reviewer_6nE9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2603/Reviewer_6nE9"
                ],
                "content": {
                    "title": {
                        "value": "RE: Response to Reviewer 6nE9"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for the authors' reply. Up to now, my concern has been partially resolved. (1) The remaining concern is about the time complexity. I will discuss this with reviewer Mqii. (2) Besides, I suggest discussing SCST in the paper. (3) For the provided codes, I will have a try in the following days. \n\nI enjoy the process of polishing a paper with authors. If all three can be resolved in the following days, I will revise my rating. And I will also follow other reviews. \n\nBest,\n\nReviewer"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2603/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700104066464,
                "cdate": 1700104066464,
                "tmdate": 1700104066464,
                "mdate": 1700104066464,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6hzq9oahcX",
                "forum": "shgx0eqdw6",
                "replyto": "2W1H3FZShG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2603/Reviewer_6nE9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2603/Reviewer_6nE9"
                ],
                "content": {
                    "title": {
                        "value": "Reminder"
                    },
                    "comment": {
                        "value": "The revision seems to have exceeded the space limit, please be attentive. Try to avoid factors that may cause desk rejection."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2603/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700104915162,
                "cdate": 1700104915162,
                "tmdate": 1700104915162,
                "mdate": 1700104915162,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "duQewzHUTq",
                "forum": "shgx0eqdw6",
                "replyto": "s8gITl67SF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2603/Reviewer_6nE9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2603/Reviewer_6nE9"
                ],
                "content": {
                    "title": {
                        "value": "RE: Response"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for the clarification on space limits. Accodring to review guidance, my discussion with reviewer Mqii will be after the author-reviewer discussion phase. However, I will run your codes to decide whether to revise my rating. If I do not reply before Nov. 20th 23:59 (AoE), please write an official comment to remind me. If you have any concerns, feel free to tell me. Thanks. \n\nBest,\n\nReviewer"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2603/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700208466399,
                "cdate": 1700208466399,
                "tmdate": 1700208466399,
                "mdate": 1700208466399,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hJpRVwxVRo",
                "forum": "shgx0eqdw6",
                "replyto": "SbkrFOF8G5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2603/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2603/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional resources: Model checkpoints"
                    },
                    "comment": {
                        "value": "We thank you for the follow-up and for engaging actively in the discussion! \n\nIn case it is helpful, we have also included the checkpoints of SFT and reward models. This allows you to perform decoding based on our approach directly. Please check them out in the revised README.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2603/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295524304,
                "cdate": 1700295524304,
                "tmdate": 1700690050752,
                "mdate": 1700690050752,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KJNbHL1HQk",
                "forum": "shgx0eqdw6",
                "replyto": "hJpRVwxVRo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2603/Reviewer_6nE9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2603/Reviewer_6nE9"
                ],
                "content": {
                    "title": {
                        "value": "RE: Additional resources: Model checkpoints"
                    },
                    "comment": {
                        "value": "Thanks for the more details. It may help me to check the codes and conduct testing. I will do it ASAP. \n\nBest,\n\nReviewer"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2603/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700296977354,
                "cdate": 1700296977354,
                "tmdate": 1700296977354,
                "mdate": 1700296977354,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6Q9dXqKKo7",
                "forum": "shgx0eqdw6",
                "replyto": "KJNbHL1HQk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2603/Reviewer_6nE9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2603/Reviewer_6nE9"
                ],
                "content": {
                    "title": {
                        "value": "Revising rating"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for providing the codes. I have checked the codes you provided. I would like to revise my rating from 3 to 5 now. My remaining concern is only about the time complexity. I will discuss this with reviewer Mqii in the next session. This determines whether I will continue to increase the rating. Besides, will you provide the web demo if accepted? If yes, I will increase my rating, too. \n\nBest,\n\nReviewer"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2603/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567630756,
                "cdate": 1700567630756,
                "tmdate": 1700567630756,
                "mdate": 1700567630756,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ipSZd6pLUg",
                "forum": "shgx0eqdw6",
                "replyto": "G03roqvsDr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2603/Reviewer_6nE9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2603/Reviewer_6nE9"
                ],
                "content": {
                    "title": {
                        "value": "RE: RE: Revising rating"
                    },
                    "comment": {
                        "value": "I revise my rating from 5 to 6. Authors should do as promised."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2603/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615596474,
                "cdate": 1700615596474,
                "tmdate": 1700615596474,
                "mdate": 1700615596474,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "l1IF6n8YBj",
            "forum": "shgx0eqdw6",
            "replyto": "shgx0eqdw6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2603/Reviewer_Mqii"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2603/Reviewer_Mqii"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces ARGS, a new framework for aligning LLMs with human preferences without the expensive RL training (i.e., RLHF). To this end, ARGS aligns the LLM with human preferences during the decoding step. Through a set of experiments, the authors show that ARGS leads to better alignment and diversity than the non-aligned baselines while preserving good coherence."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper introduces ARGS; a simple decoding-based model for LLMs alignment with reward models. In particular, ARGS only introduces an additional hyper-parameter \"w\" to tune at inference time.\n\nThis method is simple and leads to competitive results compared to PPO while not requiring any finetuning step. The authors do discuss the extra computation added at inference time and show its feasibility. I believe that such a method is interesting and useful for the literature even with this extra weight at inference. For example, it can be used to iterate over different reward models before running only one finetuning, or used directly if we have a small enough and good reward model."
                },
                "weaknesses": {
                    "value": "This paper aims to replace the RL step for human alignment with a more lightweight, only decoding-based, process. This means that PPO (as noted in Table 4) is the main baseline for ARGS. However, this comparison is not elaborated enough in the paper. This work focuses instead on other decoding-based baselines that do not aim for human alignment. For example, it would be interesting to add the Win-Tie(%) results for ARGS vs. PPO in Table 2 and discuss the low \"Diversity\" numbers for PPO in Table 4 (is it an issue of the KL penalty, was it hard to cross-validate this term?) These points would be an interesting addition to this paper."
                },
                "questions": {
                    "value": "."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2603/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698666352880,
            "cdate": 1698666352880,
            "tmdate": 1699636198939,
            "mdate": 1699636198939,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sKpsYqQho6",
                "forum": "shgx0eqdw6",
                "replyto": "l1IF6n8YBj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2603/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2603/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Mqii"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the positive and constructive review. We are very grateful that you find our paper useful for the literature. We address your feedback below!\n\n> **GPT-4 Evaluation of ARGS and baseline PPO**\n\nFollowing your suggestion, we have conducted an additional GPT-4 evaluation, comparing ARGS with the baseline PPO (on OPT models from Section 4). This evaluation strictly follows the protocol detailed in Section 3.3. The table below illustrates the completed GPT-4 evaluation results, measured by the percentage of win-ties of our method. Overall our method ARGS produces more favorable responses, achieving a win-tie rate of 72.33%.\n\n|  | Method | Win-Tie (%) |\n| ---- | --------------- | ---------------------- |\n| ARGS | PPO             | 72.33                  |\n\n> **Low diversity for PPO**\n\nThat's a great point. In the table below, we include the results of vanilla greedy decoding on the finetuned OPT-1.3b (SFT) which was the base model for the PPO model to validate the low diversity of PPO. The relatively low diversity of the outputs produced by the PPO model can be potentially related to the low diversity of the SFT model, as a result of the KL penalty.\n\n| Method      | Average Reward | Diversity | Coherence |\n| ----------- | -------------- | --------- | --------- |\n| ARGS-Greedy | 5.98           | 0.322     | 0.390     |\n| PPO-Greedy  | 5.88           | 0.247     | 0.264     |\n| SFT-Greedy  | 5.35           | 0.236     | 0.280     |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2603/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075568667,
                "cdate": 1700075568667,
                "tmdate": 1700075568667,
                "mdate": 1700075568667,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OSkbkd5zIS",
            "forum": "shgx0eqdw6",
            "replyto": "shgx0eqdw6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2603/Reviewer_2sRW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2603/Reviewer_2sRW"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel framework called ARGS (Alignment with Reward-Guided Sampling) for aligning language models with human preferences. The framework offers a flexible and efficient solution that eliminates the need for expensive RL training. With ARGS, you can generate texts with semantic diversity while being aligned with human objectives."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Resource-efficient: The ARGS framework is designed to be resource-efficient, making it an ideal solution for smaller institutions or businesses without the capacity for large-scale training. This can potentially level the playing field, allowing those with limited computational resources to benefit from state-of-the-art models without incurring significant costs.\n\n2. Broader applicability: The compatibility of the ARGS framework with different reward models extends its applicability across various domains and industries. This can accelerate the adoption of machine learning solutions in fields where resource constraints or rapidly changing data are prevalent.\n\n3. Easy to integrate: The ARGS framework is easy to integrate into existing language models, making it a practical solution for aligning language models with human preferences. The authors provide a detailed explanation of how to integrate ARGS into a pre-trained GPT-3 model, making it accessible to a wider range of users."
                },
                "weaknesses": {
                    "value": "1. Limited evaluation: The evaluation of the ARGS framework is limited to a few specific tasks (e.g., harmfulness), and it is unclear how well the framework would perform on other tasks (more complex ones like multi-step reasoning), especially when a good reward model is not easy to train. This may limit its applicability in certain domains.\n\n2. Unfair evaluation: The evaluation of the ARGS framework is evaluated on the score from reward model. However, there are some limitations: (1) ARGS will certainly achieve higher scores since the RM is integrated during the decoding process. Essentially, applying any RM constraints during the decoding stage will result in higher scores when being evaluated by that RM. (2) The calibration of RM remains unclear -- does a higher reward score certainly lead to a better response, especially the reward score difference is less than 1? \n\nOverall, I think it not very fair if the authors use the same RM in their methods and evaluation."
                },
                "questions": {
                    "value": "1. A good reward model is vital in ARGS, have you ever tried to enhance the RM? The paper does not show the relation between **RM quality** and **Effectiveness of ARGS**. For example, you might use some techniques from [a,b,c] to strengthen your RM.\n\n(a) The Trickle-down Impact of Reward (In-)consistency on RLHF 2023\n\n(b) Aligning Language Models with Preferences through f-divergence Minimization 2023\n\n(c) Fine-Grained Human Feedback Gives Better Rewards for Language Model Training 2023\n\n2. How would you evaluate the instruction-following ability of model based on ARGS? Since Diversity/Coherence are measuring the naturalness of model responses, there is a blank space for quantifying whether model completes the instruction (in your case, it is ethical property)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2603/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2603/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2603/Reviewer_2sRW"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2603/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698721697766,
            "cdate": 1698721697766,
            "tmdate": 1700547186667,
            "mdate": 1700547186667,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Cbgedk1Vym",
                "forum": "shgx0eqdw6",
                "replyto": "OSkbkd5zIS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2603/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2603/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2sRW - Part I"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the detailed and constructive review. We are also grateful that you find our paper novel and useful for the broader community. We address your feedback below!\n\n> **Evaluation on other tasks**\n\nYou raised an excellent point. For our current evaluations, we follow the standard and commonly used benchmarks in alignment literature. In particular, HH-RLHF from Anthropic and Stanford Human Preferences (SHP) are among the largest and publicly available datasets for alignment research. These tasks allow us to draw comparisons with existing approaches more easily and directly. As the first study to introduce the decoding-time alignment approach, we believe it's important to understand the potential of our approach more deeply in the existing realm of alignment tasks, before we expand to other tasks. \n\nWith that being said, we do agree that evaluating on more complex tasks such as multi-step reasoning would be valuable. During rebuttal, we looked further into this, however, we could not locate large-scale human preference datasets on multi-step reasoning, which hinders the feasibility of training the reward model. We have revised our draft to acknowledge this limitation and remain interested in delving deeper into more complex tasks as part of our future work.\n\n> **Evaluation metrics**\n\nWe understand your concern out of the Goodhart's Law :) We share the same and thus have taken the following additional steps to ensure our evaluations are comprehensive:\n\n- We performed an evaluation where the evaluating reward model is _different_ from the one we used in training. In Section 3.4, we conducted experiments where we employ fined-tuned OPT-1.3b and OPT-2.7b for decoding, and OPT-125m as reward models. We evaluate the models on random 1,000 samples of the test set from the Stanford Human Preferences (SHP) dataset, and the average reward is calculated by the OPT-350m reward model. As shown in Figure 3, ARGS consistently outperforms the greedy baseline.\n- To address the nuanced aspects of language quality that the standard metrics (diversity, coherence, reward) may not comprehensively capture, we also adopt a GPT-4-based evaluation approach for comparing the quality of responses. Table 2 presents the GPT-4 evaluation results, measured by the percentage of win-ties of our method over the alternative decoding strategies. A higher percentage indicates that our proposed method is more proficient in generating responses that exhibit not only contextual relevance and accuracy but also helpfulness and harmlessness. This observation is consistent with the outcomes of the reward-based evaluation discussed in Section 3.2.\n\n> **Relation between RM quality and effectiveness of ARGS**\n\nAnother great point raised! \n\nIndeed, our experimental results in Section 3.4 reveal such a connection between RM quality and the effectiveness of ARGS. In particular, the RM quality can be modulated by the model capacity. We experimented with a smaller capacity model OPT-125M, along with a larger capacity model OPT-350M. The reward modeling accuracy, as well as the ARGS performance (average reward, diversity, coherence), is summarized in the table below. We observe that a higher RM accuracy in general leads to stronger decoding performance by ARGS.\n\n**Base model**: OPT-1.3b\n\n| Reward Model | Evaluation Accuracy (%) | Average Reward | Diversity | Coherence |\n| ------------ | ----------------------- | -------------- | --------- | --------- |\n| OPT-125m     | 52.62                   | 5.698          | **0.322** | 0.376     |\n| OPT-350m     | **53.16**               | **5.979**      | **0.322** | **0.389** |\n\n**Base model**: OPT-2.7b\n\n| Reward Model | Evaluation Accuracy (%) | Average Reward | Diversity | Coherence |\n| ------------ | ----------------------- | -------------- | --------- | --------- |\n| OPT-125m     | 52.62                   | 5.71           | 0.369     | 0.431     |\n| OPT-350m     | **53.16**               | **5.929**      | **0.380** | **0.435** |\n\nThis experiment also informs us about a meaningful future direction to explore ARGS decoding from an RM modeling perspective. The papers you recommended are excellent starting points for this. We hypothesize that coupling advanced RM modeling objectives (beyond pairwise ranking loss) with ARGS can help further enhance the generation quality. We have added those discussions in our updated manuscript accordingly."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2603/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075405705,
                "cdate": 1700075405705,
                "tmdate": 1700075405705,
                "mdate": 1700075405705,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eH2oWWOaTK",
                "forum": "shgx0eqdw6",
                "replyto": "Cbgedk1Vym",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2603/Reviewer_2sRW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2603/Reviewer_2sRW"
                ],
                "content": {
                    "comment": {
                        "value": "Hi! Thanks for your response! It resolves most of my concerns. Still, I feel that only scaling up the RM model size is incremental to enhance the capability of RM. It would be interesting to see what will happen IN ARGS if advanced techniques are applied to strengthen the RM or if the RM has some obvious issues like inconsistency [a].\n\nOverall, I believe the authors have resolved most of my concerns. Thus, I decide to increase the score to 6.\n\n\n(a) The Trickle-down Impact of Reward (In-)consistency on RLHF 2023"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2603/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547170353,
                "cdate": 1700547170353,
                "tmdate": 1700547170353,
                "mdate": 1700547170353,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HFcb1nBGPn",
            "forum": "shgx0eqdw6",
            "replyto": "shgx0eqdw6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2603/Reviewer_vytn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2603/Reviewer_vytn"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the question: Do we really have to only have one model for sampling language? Recent work distills or amortizes reward models into language models via PPO or DPO, so that sampling is simple. This paper proposes a method for avoiding the distillation step. They instead perform decoding with the product of experts obtained by combining a language model with a reward model. The method does not require training; it directly uses the reward model, trained only on scoring complete sequences, to score incomplete prefixes during search.\n\nExperiments show that, compared to the SFT baseline, taking a product of experts ensemble of the language and reward models on incomplete prefixes during search results in better average rewards overall."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "There are two steps in this paper:\n1. Decoupling reward and language models as a product of experts (PoE)\n2. Using the reward model, unmodified, on prefixes\n\nThe first idea is the primary focus of the paper, and the second idea is not discussed. The second idea is just as, if not more important than the first. The reason PPO or DPO is used in the first place is that the reward model is an energy-based model that scores complete sequences, which can be amortized into a left-to-right autoregressive policy. This work bypasses that issue by directly applying the reward model to incomplete prefixes. The application of the reward model to prefixes instead of complete sequences requires experimental justification -- more on this in the weaknesses.\n\nOther than that, the originality, clarity, and significance were good."
                },
                "weaknesses": {
                    "value": "The decision to directly apply the reward model on prefixes should be justified experimentally, and separately from the decision to decouple the language and reward models. The main question I am interested in is: What is the performance loss from using the reward function on incomplete prefixes? Secondly, when is the predicted reward from the reward function most unreliable (likely on sequences further from completion)?\n\nSeparately, I understand that DPO [1] could maybe be considered concurrent work, but there should be comparisons against it.\n\n[1] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. May 2023"
                },
                "questions": {
                    "value": "## Questions and comments\n1. Can you add a sentence in section 3.1 stating that SFT on HH-RLHF means fine-tuning on the winning responses, if that is what was done.\n2. In section 3.2, can you say a relative improvement of 19.56% *in average rewards*.\n3. Is PPO the most widely used training-time alignment approach, or DPO?\n4. The contributions I would like to see are 1. Decouple reward and LM as product of experts, 2. Show that reward models can be reasonably applied on prefixes, and 3. Experimental validation. \n\n## Experimental ideas to strengthen the paper\n1. You could combine multiple reward functions pretty easily.\n2. It would be interesting to see if other reward models return sensible rewards on prefixes."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2603/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2603/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2603/Reviewer_vytn"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2603/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698874569923,
            "cdate": 1698874569923,
            "tmdate": 1700150264680,
            "mdate": 1700150264680,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7iukJjqMiQ",
                "forum": "shgx0eqdw6",
                "replyto": "HFcb1nBGPn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2603/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2603/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank the reviewer for the positive and constructive feedback! We are encouraged that you recognize the originality, clarity, and significance of the work. We address your comments below in detail.\n\n> **Further analysis of using reward model on prefixes**\n\nYou raise a very insightful question. To better understand this, we analyzed the average reward w.r.t. different locations. Specifically, for a given $\\texttt{prompt}$, we denote the sequence of generated tokens as $x_1$, $x_2$,.., $x_t$, and so on. The average reward at $t$-th position is calculated as $r([\\texttt{prompt},x_{\\le t}])$, averaged over the entire test set of HH-RLHF. We use the SFT-ed Llama-7B model for decoding, and report the statistics below for your reference.\n\n| Predicted token index $t$ | Average Reward |\n| ------------------------- | -------------- |\n| 10th                      | 5.38           |\n| 50th                      | 6.42           |\n| 100th                     | 6.80           |\n\n\nAs ARGS predicts more tokens, it can be seen that the average reward monotonically increases. This indicates that using the reward model on partial prefixes can steer the search toward generating sequences with higher overall reward scores. \n\n\n\n> **Comparison with DPO**\n\nAs suggested, we additionally conduct comparisons with the latest approach DPO [1] and report the result in the table below. The comparison and full training configurations have also been added to our manuscript (see **Section 4** and **Table 8**). Our method overall achieves a higher average reward. During experimentation, we notice the tendency for DPO to generate sometimes repetitive words, which potentially leads to low diversity. In contrast, our method more stably generates diverse responses. \n\n| Method | Category       | Average Reward | Diversity | Coherence |\n| ------ | -------------- | -------------- | --------- | --------- |\n| ARGS   | Decoding-based | 5.98           | 0.322     | 0.390     |\n| PPO    | Training-based | 5.88           | 0.247     | 0.264     |\n| DPO    | Training-based | 5.65 | 0.096 | 0.372  |\n\n[1] Rafailov, Rafael, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. NeurIPS 2023.\n\n> **Combining multiple reward functions**\n\nThat's a really interesting suggestion. As you recognized, our approach indeed offers the flexibility to integrate multiple reward signals in decoding-time, without having to re-train the PPO model. In our current exploration, we focus on using a single reward function, which allows us to draw comparisons with existing approaches more easily and directly. As the first study to introduce a decoding-time alignment approach, we believe it's important to understand the potential of our approach more deeply in the existing realm of alignment tasks, before we expand to more complex settings. \n\nWith that being said, we do agree that evaluating more complex tasks, such as multi-step reasoning, would be valuable. To do so, one needs more thoughtful and meaningful construction of the tasks, as well as the availability of diverse human preference datasets to train different reward functions. For this reason, we would like to spend more time diving deeper into this aspect in our future work, and hopefully present more conclusive findings after. \n\n> **Writing suggestions** \n\nAll fixed - the changes have been marked in red in our updated manuscript. Thank you for the careful read!"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2603/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075284921,
                "cdate": 1700075284921,
                "tmdate": 1700075284921,
                "mdate": 1700075284921,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EGZkvzP8MG",
                "forum": "shgx0eqdw6",
                "replyto": "HFcb1nBGPn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2603/Reviewer_vytn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2603/Reviewer_vytn"
                ],
                "content": {
                    "comment": {
                        "value": "I have updated my score from 6 to 8.\n\nFor a future version, could you also report the accuracies of the reward model on classifying the winner given prefixes of various lengths vs the full sequence on HH-RLHF? This would be a nice appendix experiment."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2603/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700150558850,
                "cdate": 1700150558850,
                "tmdate": 1700150657078,
                "mdate": 1700150657078,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]