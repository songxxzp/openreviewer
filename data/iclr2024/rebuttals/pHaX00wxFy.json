[
    {
        "title": "Reward-Free Exploration by Conditional Divergence Maximization"
    },
    {
        "review": {
            "id": "mnGnXUA8JR",
            "forum": "pHaX00wxFy",
            "replyto": "pHaX00wxFy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1795/Reviewer_XX6U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1795/Reviewer_XX6U"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a  model-free exploration strategy called Maximum Conditional Divergence (MaxCondDiv) for reinforcement learning. MaxCondDiv leverages the Cauchy-Schwarz (CS) divergence to estimate the disparity in state transition probabilities of two trajectory segments. The author further demonstrates that maximizing the CS divergence corresponds to optimizing the 2nd order Renyi entropy, which establishes a connection to the broader framework of maximum entropy approaches. The empirical evidence presented in this study shows that CS divergence is more stable than KL divergence in the Mujoco environments. Additionally, MaxCondDiv is able to explore more \nfrequently on far away states from the initial state than a maximum entropy baseline."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The author is able to demonstrate clearly the proposed method through figures and explanations. The proposed approach is well-experienced and analyzed with other reward-free exploration strategies."
                },
                "weaknesses": {
                    "value": "While the comparison with KL alternatives and max entropy based methods are well analyzed, there is a question mark on the novelty of the proposed work. In my opinion, the main contribution to the community would be utilizing CS divergence for reward-free exploration (Prop. 2) which looks similar to (Prop. 2) in [Yu et al., 2023]."
                },
                "questions": {
                    "value": "I am not sure how relevant experiment 4.1 is on MaxJDiv, especially given the conclusion that maximizing the joint is better than maximizing the conditional but the proposed approach is to maximize the conditional. \n\nHow does MaxCondDiv compare to other baselines (like the KL divergence alternatives) in terms of computational cost given the CS divergence computational complexity is O($N^2$)?\n\nAppendix C.5, in  Table 4, what about the KL divergence-based MaxCondDiv? Is it worse or better than CS divergence for downstream tasks?  \n\nIn the comparison of stability between KL and CS divergence, I would suggest also reporting the variance as the Y axis is different between Figure 12 and 13."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1795/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1795/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1795/Reviewer_XX6U"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1795/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698629205444,
            "cdate": 1698629205444,
            "tmdate": 1699636109050,
            "mdate": 1699636109050,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "mEGwI0z3kf",
            "forum": "pHaX00wxFy",
            "replyto": "pHaX00wxFy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1795/Reviewer_hR4c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1795/Reviewer_hR4c"
            ],
            "content": {
                "summary": {
                    "value": "In Reward-Free Exploration by Conditional Divergence Maximization, the author(s) introduce a novel exploration bonus, which they say is based on the \u201cConditional Cauchy-Schwarz Divergence\u201d. In their implementation, this divergence is between two estimated versions of the state transition distribution, which they estimate in a manner which they refer to as \u201cmodel-free\u201d. Using a procedure from a 2023 arXiv preprint by Yu et al., estimate the Conditional Cauchy-Schwarz Divergence and train agents in several environments, including a maze environment, MountainCar, and several MuJoCo environments. Their results demonstrate that their method is able to achieve remarkably higher state coverage than other exploration methods with similar goals."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper has many significant strengths - it introduces a novel measurement of divergence to the literature in Reinforcement Learning and demonstrates very significant gains in a particular measure of exploration, as well as impressive qualitative results. The paper also provides a high-quality review of the existing literature and a very nice introduction to the subject of R\u00e9nyi alpha-entropy. Further, the paper claims the ability accomplish exploration based on differences in transition probabilities without training an explicit model, which is laudable and could open a range of new options for existing methods."
                },
                "weaknesses": {
                    "value": "This paper has several important weaknesses, but one towers above the others: the central aspect of this work is its estimator of the CS divergence, which is neither introduced in this work nor is it verified in any previous peer-reviewed publication. Without the ability to rely on the claims made about the divergence estimator, it is impossible to recommend this paper for acceptance. The proof provided in that earlier work appears also to be present in the appendix of the presented paper, but this does not appear to be introduced as novel work, and as such I do not review it.\n\nSeveral points in the paper have unclear and uncited statements, such as 1. \u201cHowever, the utilization\u2026 leading to longer training time\u201d, 2. \u201cthey never explicitly estimate the true divergence\u2026 in the trajectory\u201d, 3. \u201cour framework estimates an intrinsic reward as defined by the divergence\u2026\u201d, 4. \u201cThe estimator exhibits low computational complexity\u2026\u201d, 5. \u201cTo maximize the divergence\u2026\u201d. These must be clarified.\n\nAt several points in the paper, the notion is invoked that there is some \u201ctrue\u201d method of estimating transition probabilities based upon different samples of transition triplets. The paper never explains what this \u201ctrue\u201d method is, nor what the \u201ctrue\u201d estimate would be, if these are different. This makes the central conceptual argument of the paper somewhat muddled, though the intention is able to break through well and the overall argument would make sense if this could be explained."
                },
                "questions": {
                    "value": "What is meant by the idea, repeated many times in the text and summarized in the abstract: \u201cthe divergence between the agent\u2019s estimation of the transition probability between the next state given current state-action pairs \u2026 in two adjacent trajectory fractions\u201d? Is a particular method of estimating these assumed, and is there any reason to think that this method would be efficacious? The paper cites a very similar method where this estimation relies on counting (Storck et al., 1995)\u2014that method seems to be on more stable footing, or at least more obviously stable footing. What is the basis in this work for such an estimator? I would like for these questions to be addressed also for the similar matter of \u201cthe true divergence of transition probability \u2026 from observations \u2026 in the trajectory\u201d. This is most severe in section 3.1.\n\nIn Section 4.1, the paper states that \u201cTo maximize the divergence, the agent needs to move to (100, 102) in the next step.\u201d. This is confusing, as it seems to me that transitioning up (i.e. incrementing the y value) again is more similar to transitioning up from a lower state than transitioning to the right or left (i.e. incrementing or decrementing the x value) would be. Is this claim made on a theoretical or empirical basis?\n\nIn Section 4.2, you state that you record trajectories for 50,000 steps in the maze environment, but reset the agent\u2019s position every 1000 steps\u2014that notion of trajectory changes very significantly the notion, discussed earlier in the paper, of basing transition probability estimates on different sections of a trajectory. This procedure is (to my mind, at least) much more akin to basing those estimates on entire trajectories (or, in fact, sets of up to 49 such trajectories). Can you explain this choice?\n\nHow many samples were used to generate the figures in the paper? In particular, how many were used for figure 5, and what do the error bars in that figure indicate?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The authors ask us to review a work whose basis has not itself been subject to peer review, and which they do not ask us to review here. This is not exactly plagiarism or dual submission, but it is harmful and the work obviously is not yet fit for publication. If they had asked us to review that material, it would be a different matter entirely, but that was not the job of the reviewers here, and the authors seem to be attempting to slip their unreviewed theoretical materials past us using an arXiv citation."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1795/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1795/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1795/Reviewer_hR4c"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1795/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807092867,
            "cdate": 1698807092867,
            "tmdate": 1699636108937,
            "mdate": 1699636108937,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "uDzXl2NYmT",
            "forum": "pHaX00wxFy",
            "replyto": "pHaX00wxFy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1795/Reviewer_ovbq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1795/Reviewer_ovbq"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a new curiosity-driven exploration method called maximum conditional divergence (MaxCondDiv). They define curiosity as trajectory-wise prediction errors. In addition to some theoretical analysis of the method, some experiments show that MaxCondDiv outperforms other baselines to a degree."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The method is sound and valuable.\n\n(2) The mathematical analysis is correct. \n\n(3) The modeling of the conditional distributions makes learning more efficient without trained dynamics models."
                },
                "weaknesses": {
                    "value": "(1) The authors claim that \"it reduces internal model selection bias\". However, there are no experiments or visualizations towards this conclusion.\n\n(2) The experiments are not sufficient to support the SoTA results, as some hard exploration tasks are not chosen, such as  Montezuma in atari and DMControl suite (not OpenAI gym, which is much easier)."
                },
                "questions": {
                    "value": "(1) How this method is sensitive to $\\tau$? (about the computational costs and performance)\n\n(2) If training the dynamics function to model the distributions, how would MaxCondDiv perform? No ablations for this, and how less computationally expensive is the MaxCondDiv?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1795/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1795/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1795/Reviewer_ovbq"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1795/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807473455,
            "cdate": 1698807473455,
            "tmdate": 1699636108828,
            "mdate": 1699636108828,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "xe8AyESjzS",
            "forum": "pHaX00wxFy",
            "replyto": "pHaX00wxFy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1795/Reviewer_3dXY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1795/Reviewer_3dXY"
            ],
            "content": {
                "summary": {
                    "value": "The paper focusses on developing a model-free curiosity-driven exploration in reward-free environments by maximizing an intrinsic bonus derived from the Cauchy-Schwarz divergence between the model estimation of two adjacent trajectory segments. The performance is validated with improved performance on several Mujoco benchmarks and shows to learn new skills in reward-free settings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Intrinsic curiosity-driven rewards to learn under sparse rewards have been leveraged in several past research and one of the primary novelties of the work lies in formulating curiosity in a model-free manner, which is computationally less extensive than the model-based as in several prior research. The research also introduces condition CS divergence as a reward function b/w the transition of adjacent trajectory segments which is interesting, unlike prior methods primarily relying on f-divergences. The experimental analysis shows improvement over baselines for reward-free exploration."
                },
                "weaknesses": {
                    "value": "The paper proposes an interesting heuristic in a reward-free exploration scenario, but the mathematical formulation is not extremely clear and needs more clarity. It\u2019s not explicitly clear why the formulation in (9) can probably lead to better exploration which has been explained intuitively with experiments. For example, the recent work in [1] has shown mutual information as a provable source of exploration and demonstrated improved performance under sparsity with information-directed regret analysis. However, current work lacks any such metric to characterize the exploration induced by the divergence. Additionally, the computational tractability of Cauchy Schwartz divergence is not very clear which requires the computation of several Gram matrices resulting in O(n^2) complexity, which is marginally better than modeling the dynamics with Gaussian process which requires a complexity of O(n^3) which can be improved easily with sparse GP or compression[2].  Then the model-free curiosity with less computational complexity is not very appropriate. Hence to clear it, will be helpful to discuss non-parametric transition estimation methods and this approach. The most important aspect is that with GPs, we can estimate exact epistemic uncertainty (which provably induces exploration as intrinsic rewards, since is connected to information gain), however how the proposed CS divergence can help and a precise connection to epistemic uncertainty is not clear (Equation 11 is not sufficient for that or needs more explaination). Additionally, it has been shown in several recent works that GPs [2, 3, 4] or ensembles are sufficient in learning dynamics in Mujoco tasks. Hence, will be helpful to provide a discussion around the same and compare tasks with more complex dynamics to understand the effect.\n\n1. Botao Hao,\u00a0Tor Lattimore, Regret Bounds for Information-Directed Reinforcement Learning, https://arxiv.org/abs/2206.04640\n2. Souradip Chakraborty, Amrit Singh Bedi, Alec Koppel, Brian M. Sadler, Furong Huang, Pratap Tokekar, Dinesh Manocha Posterior Coreset Construction with Kernelized Stein Discrepancy for Model-Based Reinforcement Learning https://arxiv.org/abs/2206.01162\n3. Ying Fan,\u00a0Yifei Ming Model-based Reinforcement Learning for Continuous Control with Posterior Sampling https://arxiv.org/abs/2012.09613\n4. Kurtland Chua, Roberto Calandra, Rowan McAllister, Sergey Levine  Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models https://arxiv.org/abs/1805.1211"
                },
                "questions": {
                    "value": "1. Why MaxDiv learns Skill without Using Extrinsic Rewards is not extremely clear and will be helpful to provide a detailed explanation \n2. Currently it focuses on max entropy objectives which is nice, but the validation of reward-free exploration in certain downstream tasks or some sparse reward tasks will be helpful to understand the efficacy of the proposed exploration method, as well as in some prior research.\n3. How does the proposed method compare with scenarios when we predict the dynamics with GPs and leverage the posterior uncertainty be helpful? A comparison with PSRL or Thompson sampling and information-directed approaches in the context will be helpful [1].\n\n\n\n[1]. Christoph Dann,\u00a0Mehryar Mohri,\u00a0Tong Zhang,\u00a0Julian Zimmert A Provably Efficient Model-Free Posterior Sampling Method for Episodic Reinforcement Learning https://arxiv.org/abs/2208.10904"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1795/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824168326,
            "cdate": 1698824168326,
            "tmdate": 1699636108759,
            "mdate": 1699636108759,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]