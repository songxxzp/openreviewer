[
    {
        "title": "LNL+K: Enhancing Learning with Noisy Labels Through Noise Source Knowledge Integration"
    },
    {
        "review": {
            "id": "apoejwwX6o",
            "forum": "MDXfiEpEEP",
            "replyto": "MDXfiEpEEP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5761/Reviewer_D5C9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5761/Reviewer_D5C9"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a solution to enhance learning with noisy labels using the knowledge of noise source. It presents a few extensions from current solutions to add the noise source knowledge.  Basically, if the sample is more likely to belong to a noise source, then the label can be considered a noisy label. Experimental results show the method works to some extent."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper presents a simple and straightforward method to detect noisy label under a very strong and unrealistic assumption. Multiple datasets are evaluated to conduct the experiments to show the idea work."
                },
                "weaknesses": {
                    "value": "There are several major limitations of the paper.\n\n(1) The novelty of the paper is very limited. The idea is established under a very strong assumption that the noise source is known. Moreover, mathematically, the method is also unjustified.\n\n(2) The depth of the paper is not strong enough. There is no theory to support the claims. The method also looks ad-hoc. The level of depth is too far away from ICLR paper.\n\n(3) The paper is also not well-written in the sense that the motivation of the paper is not clear."
                },
                "questions": {
                    "value": "In addition to very limited novelty and not enough technical depth, it is also unclear that how is the probability that labels is clean computed and how the noise source is being identified.\n\nFor instance, the noise label created by different people with different levels of experience can be mixed without the knowledge of source. It is a more general case. It is important to solve a more general more instead of an edge case."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5761/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5761/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5761/Reviewer_D5C9"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5761/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697888460915,
            "cdate": 1697888460915,
            "tmdate": 1699636605006,
            "mdate": 1699636605006,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fR3wUhgxur",
                "forum": "MDXfiEpEEP",
                "replyto": "apoejwwX6o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5761/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5761/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your efforts in reviewing our paper, and we will use your comments to improve our paper.  We have answered your questions below, and asked for additional clarifications for questions that were unclear to us.\n>(1) The novelty of the paper is very limited. The idea is established under a very strong assumption that the noise source is known.\n\nWe will begin by noting that the reviewer's comment provides no justification for their assertions.  We are happy to discuss these questions, and will give a general response, but we ask that the reviewer provide comparisons to existing work in order to justify their claims.\n\nThat said, we will begin our response by separating two concepts that the reviewer appears to be conflating, namely that of novelty and impact.  Novelty typically refers to offering a new idea not previously explored.  In our paper, we do indeed propose exploring cases where the noise source is known.  While this is a simple idea, this has not been rigorously explored previously.  In fact, we provide dozens of examples of papers that explored LNL tasks, but did not explore our setting.  The closest work to ours, Han et al, 2018a, also makes a similar argument to ours, but did so in a limited setting that only considered classifier-consistent methods that are unreliable when there are errors in the noise sources.  However, our paper provides a more thorough introduction to this topic including proposing a framework for addressing this task as well as experiments on several real-world datasets.  Of particular note are our cell datasets, which have additional sources of noise that we are not given (discussed further in our response to Reviewer 3qaZ), yet our approach is still able to boost performance in this challenging and realistic setting.\n\nNow, one could also consider the impact as well, which is affected by the assumptions that we make.  Our primary goal is to better support datasets where noise source knowledge is known.  As our experiments in Table 3 demonstrate, existing LNL methods often do not generalize to some settings, such as scientific datasets like CHAMMI-CP and BBBC036 where the noise source knowledge is always known.  Specifically, no method we explore was able to outperform standard training on both datasets.  In fact, the more recent methods actually performed worse than some of the simpler approaches.  This is likely due, in part, to the fact that these datasets are very challenging, with many types of noise and sometimes having high-ratios of noise.  However, as our experiments show, this is where having noise source knowledge is key: as they always perform on par or better than the adaptations that did not provide this information.\n\nFinally, we note that we did also explore settings where the noise source is not known and must be estimated in Table 4.  Specifically, we found that using estimated noise combined with our LNL+K adaptations boosted performance over the methods proposed by prior work.  As such, our work not only explored a setting that prior work failed to generalize to (i.e., they performed worse than standard training), but we also demonstrated that exploring our setting could provide benefits to other datasets where noise source is not known.  Thus, one can only conclude that our work is both novel and makes a significant contribution over the existing literature.\n\n>Moreover, mathematically, the method is also unjustified.\n\nFirst, we begin by noting that our primary contribution is a study in a largely unexplored topic, namely, where noise source knowledge is known.  Second, we mathematically define this problem thoroughly as well as the guiding principles that we use to adapt existing work to our problem in Section 3.1.  This would seemingly directly contradict the reviewer\u2019s comment, so we request for justification as to why they feel this is insufficient or what questions the reviewer had so that we could use these comments to improve our paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5761/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977430839,
                "cdate": 1699977430839,
                "tmdate": 1699977430839,
                "mdate": 1699977430839,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Up6U36u05V",
            "forum": "MDXfiEpEEP",
            "replyto": "MDXfiEpEEP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5761/Reviewer_3qaZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5761/Reviewer_3qaZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores an overlooked task of learning with noisy labels utilizing noise source knowledge. The paper proposes a simple wrapper method that can be used on top of existing methods for noise labels. Overall, the paper studies an interesting setup and the proposed method is practical. However, there are some concerns on the applicability of the method and also the experiment evaluation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Intuitively, using noise source knowledge can indeed be helpful.\n2. The paper proposes a simple method that can be plugged into existing base models.\n3. Experiments shows improvement from the proposed method."
                },
                "weaknesses": {
                    "value": "1. Writing can be improved. There are simple errors that should have been avoided by proof reading. For example. \u201cwhen considering the presence of the noise source yellow class, it becomes evident that these noisy samples are closer to their true label class.\u201d Where is \u201cyellow\u201d class in the figure? This causes confusion as \u201cyellow\u201d class is also referred in Equation 2. \n2. The applicability of the method should be made more clear and more intuition should be provided. \n3. Experiment setup can be improved to better justify the claims."
                },
                "questions": {
                    "value": "1. It seems the method works in the scenaio where there are confusing class pairs on which examples are miss labeled. However, in real-world, there can be many more noise label patterns in a same dataset, for example random white noise. How does the method work with the coexistence of other noise patterns?\n2. It is mentioned that the proposed method \"are effective on datasets where noise represents the majority of samples\". Given a new dataset at hand with only noisy ground-truth labels, how should one decide whether to use the proposed method? and how would one know whether noise represents the majority of samples?\n3. How often and also when is equation 2 different from selecting the highest probability class? Can you provide some numbers and examples from experiments? This can be helpful to understand the method better.\n4. How is the model trained after detecting the noise examples? Do you just drop the noise examples?\n5. It would be the best to also consider baselines that uses robust loss functions/regularization/etc"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5761/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698601285417,
            "cdate": 1698601285417,
            "tmdate": 1699636604901,
            "mdate": 1699636604901,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cccSRqGGru",
                "forum": "MDXfiEpEEP",
                "replyto": "Up6U36u05V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5761/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5761/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your efforts in reviewing our paper, and we will use your comments to improve our paper.  We have answered your questions below, and asked for additional clarifications for questions that were unclear to us.\n\n>W1. Writing can be improved. There are simple errors that should have been avoided by proof reading. For example. \u201cwhen considering the presence of the noise source yellow class, it becomes evident that these noisy samples are closer to their true label class.\u201d Where is \u201cyellow\u201d class in the figure? This causes confusion as \u201cyellow\u201d class is also referred in Equation 2.\n\nApologies, the color change was a last-minute change to make them easier to see.  Where we refer to \u201cyellow\u201d it should be \u201cblue\u201d in the current figure.  We will use these comments to adjust our paper\n\n>W2. The applicability of the method should be made more clear and more intuition should be provided.\n\n>W3. Experiment setup can be improved to better justify the claims.\n\nWe responded to your questions related to these comments below.  Please let us know if there are additional questions you have to address these comments.\n\n>Q1. It seems the method works in the scenaio where there are confusing class pairs on which examples are miss labeled. However, in real-world, there can be many more noise label patterns in a same dataset, for example random white noise. How does the method work with the coexistence of other noise patterns?\n\nThank you for the opportunity to discuss this point! The existence of other source of noise (e.g., non-label noise) arises in both cell datasets we explore.  Specifically, each compound being tested on the cell datasets are collected over multiple experiments.  Basically- the same experiment is conducted multiple times, and there may be some small variations in exactly how each was performed.  In addition, there may also be differences in microscopes or the stain used to image the cells.  These differences are commonly referred to as \u201cbatch effects\u201d and also make learning on these datasets challenging. Thus, our results on these cell datasets also demonstrate that we can achieve performance gains even when only some potential sources of noise (i.e., label noise) are known.\n>Q2. It is mentioned that the proposed method \"are effective on datasets where noise represents the majority of samples\". Given a new dataset at hand with only noisy ground-truth labels, how should one decide whether to use the proposed method? and how would one know whether noise represents the majority of samples?\n\nIt's crucial to emphasize that our knowledge-integrated methods exhibit effectiveness not only in high noise ratios but also provide benefits across a broad spectrum of noise ratios. When dealing with a new dataset, if there exists available knowledge about the noise sources, such as information on confusing pairs in the metadata, it is worthwhile to consider employing LNL+K methods.\n>Q3. How often and also when is equation 2 different from selecting the highest probability class? Can you provide some numbers and examples from experiments? This can be helpful to understand the method better.\n\nThank you for highlighting this. A potential experimental validation can be found by examining the precision and recall rates of clean the sample selection. We present statistics from CIFAR10 with a dominant noise ratio of 0.8 as an illustrative example:\n\n| Method  | precision | recall | classification accuracy |\n|---------|-----------|--------|-------------------------|\n| CRUST   | 72.29     | 36.15  | 65.79                   |\n| CRUST+k | 87.67     | 99.04  | 80.54                   |\n| FINE    | 88.53     | 61.89  | 75.45                   |\n| FINE+k  | 89.64     | 99.61  | 80.52                   |\n| SFT     | 97.27     | 94.37  | 75.43                   |\n| SFT+k   | 98.99     | 94.95  | 76.78                   |\n\nClearly, both precision and recall rates for sample selection witness an increase with the adoption of the LNL+K method. The substantial improvement in recall rate underscores the superior ability of LNL+K in retaining hard negative samples for training."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5761/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699977130861,
                "cdate": 1699977130861,
                "tmdate": 1699977404978,
                "mdate": 1699977404978,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lLx11mxu6T",
            "forum": "MDXfiEpEEP",
            "replyto": "MDXfiEpEEP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5761/Reviewer_X4gK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5761/Reviewer_X4gK"
            ],
            "content": {
                "summary": {
                    "value": "The work focuses on a very practical problem - learning with label noise. It introduces the concept of LNL+K, which incorporates additional 'noise source knowledge' into existing sample selection methods. Specifically, this work utilizes additional 'noise source knowledge' to identify potentially confusing noise classes $D_{c-ns}$. Instead of considering the probability of the annotated class being clean~($P_{c}$), the proposed approach compares the probability of a given label being clean with that of the confusing classes. The proposed strategy is combined with various existing sample selection methods and evaluated on several datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The presentation is clear.\n2. Multiple existing sample selection methods are considered for evaluation."
                },
                "weaknesses": {
                    "value": "1. The novelty is somewhat limited. As quoted in the paper - \"The selected sample\u2019s probability may not be the highest\", indeed current sample selection methods, such as the widely-applied GMM style, possibly lead to false positives (hard negatives), as they only consider whether the annotated label is 'clean enough or not' while ignoring its relative 'cleanness' versus other classes. Upon this, though introduced as 'noise source knowledge', the core idea of this work is to compare the 'cleanness' of the annotated class versus other classes, which is straightforward and trivial in existing sample selection heuristics, and been applied already ('consistency measure in [1], probablity difference in [2], there should be more work in major venues). This may sounds a bit stringent, but I expect more insights rather than rephrasement, especially for venues like ICLR.\n\n2. I expect more 'real noise source knowledge' to be considered, rather than current ones (transition matrix, etc.), which still rely on the noisy labels and current in-training models, leading to self-confirmation again. \n\n3. The confusion class set \\(D_{c-ns}\\) induced by 'noise source knowledge' involves new hyperparameters. More ablations are necessary. \n\n4. For a sample selection strategy, there always exists a dilemma of precision and recall, especially when extra hyperparameters are involved. This requires more detailed analysis.\n\n5. The considered real-world noisy datasets lack persuasiveness. Experiments should be conducted on at least Clothing1m and WebVision. \n\n[1] SSR: An Efficient and Robust Framework for Learning with Unknown Label Noise, BMVC2022\n[2] P-DIFF: Learning Classifier with Noisy Labels based on Probability Difference Distributions, ICPR2020"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5761/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676279087,
            "cdate": 1698676279087,
            "tmdate": 1699636604798,
            "mdate": 1699636604798,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wSloopv1Yv",
                "forum": "MDXfiEpEEP",
                "replyto": "lLx11mxu6T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5761/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5761/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your efforts in reviewing our paper, and we will use your comments to improve our paper.  We have answered your questions below.\n\n>1. The novelty is somewhat limited. The core idea of this work is to compare the 'cleanness' of the annotated class versus other classes, which is straightforward and trivial in existing sample selection heuristics, and been applied already ('consistency measure in [C1], probablity difference in [C2], there should be more work in major venues). This may sounds a bit stringent, but I expect more insights rather than rephrasement, especially for venues like ICLR.\n\nWe will note that the consistency measure and probability differences in [C1] and [C2] are not consistent with our introduced noise source knowledge formulation, and thus, it is not a simple rephasement of these methods.  To illustrate this point, let us begin by defining these criteria:\n\n[C1] defines consistency measure as \u201ca high consistency measure ci at a sample xi means that its neighbours agree with its current label l^r_i \u2014 this indicates that l^r_i is likely to be correct. By setting a threshold \u03b8s to ci, a clean subset (Xc, Yrc) can be extracted.\u201d We note that this consistency measure is very similar to that used by CRUST, which uses gradients rather than predicted labels to measure similarity between samples (which is also outperformed by LNL+K).  \n\n[C2] defines probability difference as \u201c\u03b4 = py \u2212 pn where pn is the largest component except p.\u201d\n\nNow let\u2019s consider the three following samples from the same neighborhood (ie., they are the samples used by [C1]) with predicted class probabilities for classes [cat, dog, bird]:\n\nSample A: [0.20, 0.10, 0.60] with given and true labels cat\n\nSample B: [0.25, 0.15, 0.60] with given and true labels bird\n\nSample C: [0.15, 0.15, 0.70] with given and true labels bird\n\nNow, when deciding whether to label Sample A as noisy, [C1] would refer to its neighbors, and since a majority of labels of its neighbors disagree, it would identify it as a noisy sample, but may retain Sample B and C depending on the exact threshold used.  [C2] would compare the differences in probabilities, and would result in the same selection as [C1] (identifying A as noise, but possibly retain B and C).  However, in LNL+K, if the noise sources for \u201ccat\u201d is only \u201cdog\u201d and not \u201cbird, then we would retain all three samples as clean.  This is because of the fact that our criteria would effectively ignore the high-confidence prediction for bird made for Sample A because we know that a cat would not be mislabeled as a bird sample.  Instead, it indicates this is likely a hard negative, perhaps due to similar backgrounds (e.g., many bird images may be in wooded areas, and this particular cat sample may also be taken in a wooded area).\n\nWhile we agree that this premise is a simple idea, we note that the majority of work in LNL either ignores the noise sources entirely (such as [C1] and C[2]), leading to failures like the one above, or focuses on trying to estimate the noise.  This illustrates that this idea is only obvious in hindsight.  In addition, we note that noise estimation methods are not beneficial for datasets where the noise is always given, such as in the tasks addressed in CHAMMI and BBBC036- they effectively attempt to just predict a known entity.  Also, the focus of noise estimation methods is typically, as the name suggests, on how to more effectively estimate the noise, but not necessarily on good strategies to use the noise source knowledge once it is estimated.  \n\nIn fact, an inherent issue with studying methods on how to effectively use the noise when exploring noise estimation methods is that it conflates errors due to issues with noise estimation with effective use of these methods.  In other words, some methods may be able to effectively use noise sources, but only when those are accurate representations of the data, and may underperform when the estimates are noisy.  This is where our work provides a contribution, as it can fairly evaluate such methods.  Our work also highlights the need for determining how to more effectively use noise sources either when they are known, such as for CHAMMI, BBBC036, and Animal10N.  Finally, we also demonstrate that our work can be beneficial when the noise sources are not known and instead have to be estimated, as shown in our experiments in Table 4.  Thus, our work provides novel insights into the LNL problem not found in prior work.\n\n[C1] SSR: An Efficient and Robust Framework for Learning with Unknown Label Noise, BMVC2022\n\n[C2] P-DIFF: Learning Classifier with Noisy Labels based on Probability Difference Distributions, ICPR2020"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5761/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699976082452,
                "cdate": 1699976082452,
                "tmdate": 1699976082452,
                "mdate": 1699976082452,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IiDN2ip48W",
            "forum": "MDXfiEpEEP",
            "replyto": "MDXfiEpEEP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5761/Reviewer_kCaM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5761/Reviewer_kCaM"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed incorporating noise source knowledge into some sample selection methods by comparing the confidence of noisy labels and noise source label. The proposed method is simple and easy to be integrated into multiple existing LNL methods. Experiments confirm the effectiveness of the proposed method in certain cases."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The studied problem that how to exploit noise source knowledge in noisy label learning is novel and interesting.\n2. The proposed method is very simple but reasonable, which can be integrated into many methods.\n3. The datasets include cell datasets which shows the potential of the proposed method in scientific research."
                },
                "weaknesses": {
                    "value": "1. As shown in the experiments, in some cases, the proposed method will lead to a decrease in performance. The authors should offer a deeper analysis about the reason of the decrease in performance and when could the performance gain be ensured.\n2. The real-world datasets are small-scale and special. It would be better to test the performance of the proposed methods with estimated noise source knowledge in more large and general benchmarks, e.g., Clothing1M and WebVision.\n3. Some writing need further clarification. First, how to generate dominant noise is still unclear.  Are there multiple noise sources for each recessive label. And why is the dataset still balanced after mislabeling in these cases? It seems that the number of examples labeled by dominant classes is less than recessive classes. Second, how to use DualT to estimate noise source knowledge is not clear.\n4. (Minor) The related works can include more recent classifier-consistent methods, e.g. [1,2,3].\n5. (Minor) What does the \"noise supervision\" mean?\n\n[1] Estimating Noise Transition Matrix with Label Correlations for Noisy Multi-Label Learning. NeurIPS 2022\n\n[2] A holistic view of noise transition matrix in deep learning and beyond. ICLR 2023\n\n[3] Identifiability of label noise transition matrix. ICML 2023"
                },
                "questions": {
                    "value": "See above weaknesses.  I am happy to increase my score if my concerns are addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5761/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698750442287,
            "cdate": 1698750442287,
            "tmdate": 1699636604709,
            "mdate": 1699636604709,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1SnXuJFQme",
                "forum": "MDXfiEpEEP",
                "replyto": "IiDN2ip48W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5761/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5761/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your efforts in reviewing our paper, and we will use your comments to improve our paper.  We have answered your questions below.\n\n>1. As shown in the experiments, in some cases, the proposed method will lead to a decrease in performance. The authors should offer a deeper analysis about the reason of the decrease in performance and when could the performance gain be ensured.\n\nThank you for your attention to these results. As discussed in section 4.3, we term the various performance improvements as the \"knowledge absorption rate,\" a metric relevant to noise type, noise ratio, and the base LNL methods. Among all our findings, only a few exhibited a performance decrease (8 settings out of 52), and the majority of those (5 settings out of 8) decreased by less than 0.5%. The decline in CRUST^{+k} performance is likely attributable to the need for careful tuning of a hyperparameter controlling the size of the clean sample selection, especially in the case of CRUST^{+k}. SFT^{+k} encounters challenges at exceptionally high noise ratios, suggesting that knowledge about the noise source is particularly beneficial for feature-based clean sample detection methods in such conditions.\n\n>2. The real-world datasets are small-scale and special. It would be better to test the performance of the proposed methods with estimated noise source knowledge in more large and general benchmarks, e.g., Clothing1M and WebVision.\n\nWe will note that the real world datasets are evaluated on are of similar sizes as other common fine-tuning benchmarks.  For example, BBBC036 contains 156K images, making it on par with datasets like COCO and Visual Genome.  We note that by definition, the task we are exploring is one that requires labeled samples.  Labels are expensive, and as such, collecting very large datasets can be prohibitively expensive for many applications.  This is especially true in scientific research settings like those analyzing the cell datasets where LNL+K is a natural fit as these datasets can require significant and very expensive expertise to annotate. Thus, settings where data and labels are *very* plentiful, like clothing Clothing1M and WebVision, actually represent very narrow application areas of LNL and related tasks.\n\nMore specifically, let us consider different axes by which we could consider how well Clothing1M and WebVision generalize.  In terms of image distribution, In addition, Clothing1M and WebVision are limited in that they use the same type of imaging (RGB), represent only two domains of data, and also only contain internet-collected samples that represent very biased datasets as we will expand on later.  In contrast, we explore three different datasets of varying imaging types, RGB images in Animals10N, which cover similar applications as the datasets suggested by the reviewer, as well as cell microscopy images. Additionally, CHAMMI-CP is a combination of samples from two different experiment types, one measuring the response of cells to different drug treatments and the other testing gene over-expression experiments, which is important for evaluating drug diversity projects. This already demonstrates that our study is far more general than those suggested by the reviewer and used in prior work (e.g., [1,2]) as they cover a broader range of tasks, applications, domains, and imaging types.\n\nIn their comment, the reviewer also seems to suggest that exploring Clothing1M and WebVision has greater potential impact, but we strongly disagree with these sentiments.  We generalize to cell data, which understanding and analysis of this type of data is an entire field of study on its own.  Thus,  exploring methods that can benefit these applications has far more potential for high impact than simply boosting performance on Clothing1M and WebVision. The cell datasets represent a variety of applications and generalization settings, including drug discovery.  In other words- these datasets represent tasks that can directly affect healthcare. Although one could argue that RGB images contain an important domain, we do already explore this setting with the Animals10N dataset.  Thus, we argue that the datasets already explored in our paper represent greater potential for impact than those suggested by the reviewer, and are clearly well above the bar for publication.\n\n[1] Yi, Li, et al. \"On learning contrastive representations for learning with noisy labels.\" CVPR. 2022.\n\n[2] Qi Wei, Haoliang Sun, Xiankai Lu, and Yilong Yin. Self-filtering: A noise-aware sample selection for label noise with confidence penalization. ECCV, 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5761/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699975649975,
                "cdate": 1699975649975,
                "tmdate": 1699976889708,
                "mdate": 1699976889708,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tjNxz11pfc",
            "forum": "MDXfiEpEEP",
            "replyto": "MDXfiEpEEP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5761/Reviewer_unqu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5761/Reviewer_unqu"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new task and method for the task of Noise Source Knowledge Integration. They assume that a set of possibly confusing classes for a given other class is made available e.g. the knowledge that trucks and automobiles are easily confused with each other. They integrate this knowledge in the learning process and demonstrate improved results on synthetic and real world datasets when their method is combined with various selected methods from the noisy labels literature which currently do not integrate noise source knowledge."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors report broadly positive results with improvements on most datasets in most settings.\n\n- The authors demonstrate that their method can be successfully combined with a range noisy labels learning methods.\n\n- The use of the CIFAR-10 and CIFAR-100 synthetic noise datasets is relatively standard in the noisy labels literature, and the authors introduce some non-standard evaluation datasets with interesting scientific applications (BBBC036, CHAMMI-CP)."
                },
                "weaknesses": {
                    "value": "- I found the presentation extremely hard to follow, with many terms' definitions unclear to me and/or errors in the notation and presentation that made following the paper very difficult. I have listed what I regard as errors below and have deferred to the questions section various unclear terms for clarification. I am open to substantially improving my score if it is clear I have misunderstood the paper and it is clarified to me, but at present given my careful reading of the presentation I find the entire method definition and hence its evaluation unconvincing as I cannot understand it.\n    - There are repeated references to a yellow class in Figure 1 e.g. \"the noise source yellow class\", \"p(yellow | x_i) > p(red ? x_i)\", there is no yellow class as far as I can see.\n    - The output of algorithm 1 is denoted P(X) = {p(\\tilde{y}_i | x_i)}, this neither seems to be the correct definition of P(X) or the algorithm output.\n\n\nSmall points:\n- $\\tilde{Y}$ is used in the literature and in section 2 to signify noisy labels, yet in paragraph 2 of section 3, this notation is flipped and now $\\tilde{y}$ are the \"true labels\"."
                },
                "questions": {
                    "value": "- The definition of noise sources is unclear to me. Can you please confirm if my understanding based on the definition at the end of the second paragraph in section 3 is correct: noise sources for a given class c, is the set of all other classes such that there is a non-zero probability of a data point that is truly in class c being mislabeled as that class?\n    - If my understanding is correct, then I find it hard to understand how this information is useful to the extent demonstrated in the experiments. Firstly, strictly based on this definition, all classes should be in the noise source set as there is a non-zero probability that a class c is mislabelled to any other class. However it seems to be the case that the definition of this set is in fact more loosely applied in the experimental section of the paper where the noise sources set for class c are really classes that have a relatively high probability of being confused for class c. Nonetheless I struggle to see how this information would be as useful as some of the experimental results seem to show, as essentially this additional information would merely say on a dataset level which classes are reasonably likely to be confused for each other. This is not per sample/input dependent nor does it convey as much information as the true transition matrix which would contain transition probabilities and not merely binary values for each class pair. Could the authors please comment?\n\n- Figure 1 is very unclear to me, based on my understanding of noise sources above, then noise sources are not data points but classes, yet Figure 1 presents new data points as noise sources? In addition could the authors please clarify the meaning of red, blue, circles and triangles in the figure. For example, what is the meaning of a blue triangle?\n\n- I do not understand the arguments leading to equation 2 in section 3.1. In particular:\n    - \"Fig. 1 has a high probability of belonging to the red class, i.e., p(red|xi) > \u03b4, then it is detected as a clean sample in LNL. However, compared to the probability of belonging to the noise source yellow class, p(yellow|xi) > p(red|xi), so the red triangle is detected as a noisy sample in LNL+K.\" As in this paper, noisy labels methods are usually evaluated on multi-class classification tasks. Hence I do not understand how a standard LNL method would fail in this case when p(yellow|xi) > p(red|xi), then while p(red|xi) > \u03b4 it must be the case that p(yellow|xi) >> \u03b4 and hence the sample would be labelled as yellow by the standard LNL method. Fundamentally I fail to see how standard LNL methods would not satisfy equation 2, please explain?\n\n- How the method is incorporated into the various LNL methods is unclear to me. I can understand how the various methods currently identify clean labels. But a very short description is given of the integration in each of the 5 cases. I do not think the level of detail would be sufficient to replicate your results or to combine the method with another LNL method. For example: \"To estimate the likelihood of a sample label being clean in CRUST+k, we mix this sample with all other noise source class samples and apply CRUST to the combined set. If the sample is selected as part of the noise source class cluster, we assume its label is noisy.\" What does it mean to mix the \"samples\"? What are the samples in this case? The noise sources as defined above are simply a set of classes, they are not input dependent nor can I see how they can be mixed with training examples?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5761/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792814944,
            "cdate": 1698792814944,
            "tmdate": 1699636604605,
            "mdate": 1699636604605,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dALGDL5gjc",
                "forum": "MDXfiEpEEP",
                "replyto": "tjNxz11pfc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5761/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5761/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your efforts in reviewing our paper, and we will use your comments to improve our paper.  We have answered your questions below, and asked for additional clarifications for questions that were unclear to us.\n\n>There are repeated references to a yellow class in Figure 1 e.g. \"the noise source yellow class\", \"p(yellow | x_i) > p(red | x_i)\", there is no yellow class as far as I can see.\n\nApologies for the confusion, where \u201cyellow\u201d is referenced it should be \u201cblue.\u201d We have adjusted our discussions accordingly.\n\n>The output of algorithm 1 is denoted P(X) = {p(\\tilde{y}_i | x_i)}, this neither seems to be the correct definition of P(X) or the algorithm output.\n\nThe algorithm output P is a list of length n, where n is the number of samples in the inputs X. P[i] denotes the probability of sample $x_i$ being clean, i.e. p(\\widetilde{y_i}|x_i). This comment is confusing to us, we have defined P(X) ourselves in the algorithm, P(X) = {p(\\widetilde{y_i}|x_i)}_{i=1}^n, so it can only be \u201ccorrect\u201d according to our definition.  As such,  we would appreciate some clarity on your comment.\n\n\n>Y is used in the literature and in section 2 to signify noisy labels, yet in paragraph 2 of section 3, this notation is flipped and now y are the \"true labels\"\n\nThank you for your comment, we will ensure that the notation is consistent.\n>The definition of noise sources is unclear to me. Can you please confirm if my understanding based on the definition at the end of the second paragraph in section 3 is correct: noise sources for a given class c, is the set of all other classes such that there is a non-zero probability of a data point that is truly in class c being mislabeled as that class?\n\nTo clarify, noise sources are defined on a category level.  So let\u2019s say for category *c* we are told its noise sources are categories *d* and *e*, it means that for any image for category *c* that is mislabeled, we know that its true label must either be an image category *d* or an image of category *e*.\n\n> Firstly, strictly based on this definition, all classes should be in the noise source set as there is a non-zero probability that a class c is mislabelled to any other class. However it seems to be the case that the definition of this set is in fact more loosely applied in the experimental section of the paper where the noise sources set for class c are really classes that have a relatively high probability of being confused for class c. Nonetheless I struggle to see how this information would be as useful as some of the experimental results seem to show, as essentially this additional information would merely say on a dataset level which classes are reasonably likely to be confused for each other. This is not per sample/input dependent nor does it convey as much information as the true transition matrix which would contain transition probabilities and not merely binary values for each class pair. Could the authors please comment?\n\nOur understanding of this comment is that the reviewer wishes to know 1. The identification of noise sources, 2. Knowing why categories that may often be confused with each other would be useful, as it conveys information at a dataset level and not on a per-sample level.  \n\nFor 1, in theory, we define those noise sources with \u201csignificantly large\u201d non-zero probabilities. In practice, for the real-world dataset, the noise source information is collected from the dataset metadata, e.g. confusing pairs from Animal10N dataset and control group in cell datasets.\n\nRegarding 2, we note that two images may be considered hard-negatives with each other not because they contain the same category, but because there are other correlated attributes.  For example, let\u2019s \u201ccat\u201d image x_1 is in a wooded environment and the dataset also contains many \u201cbird\u201d images that are also in similar wooded environments.  As such, although a human annotator may not confuse the cat for a bird, i.e., they are not noise sources for each other, a neural network may find them more challenging to distinguish between each other as the background would be the same.  Prior work may disregard these samples as they are not confident in their true label due to the high confidence prediction in \u201cbird.\u201d  However, in our work, we would retain all such samples.  Thus, by considering such information, we would retain more hard-negative samples than prior work."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5761/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699974922103,
                "cdate": 1699974922103,
                "tmdate": 1699974922103,
                "mdate": 1699974922103,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zSDdY2WBe7",
                "forum": "MDXfiEpEEP",
                "replyto": "tjNxz11pfc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5761/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5761/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply (part 2)"
                    },
                    "comment": {
                        "value": ">Figure 1 is very unclear to me, based on my understanding of noise sources above, then noise sources are not data points but classes, yet Figure 1 presents new data points as noise sources? \n\nNoise sources are retained from classes, however, Figure 1 represents how data selection methods may differ in our setting and prior work.  In the example given, we show that in cases where many samples may be mislabeled, then the model may begin to model the noise as the true class.  However, in our work, any sample that follows the same distribution as the noise sources would be removed (represented as the noise source data points).  We will clarify this point in a new figure.\n>In addition could the authors please clarify the meaning of red, blue, circles and triangles in the figure. For example, what is the meaning of a blue triangle?\n\nThese are defined at the bottom of the figure.  Colors refer to the noisy labels, whereas shapes refer to the true labels. Red circles and blue triangles are clean samples, and red triangles are noisy samples whose labels are red but true labels are blue.\n>\"Fig. 1 has a high probability of belonging to the red class, i.e., p(red|xi) > \u03b4, then it is detected as a clean sample in LNL. However, compared to the probability of belonging to the noise source blue class, p(blue|xi) > p(red|xi), so the red triangle is detected as a noisy sample in LNL+K.\" As in this paper, noisy labels methods are usually evaluated on multi-class classification tasks. Hence I do not understand how a standard LNL method would fail in this case when p(blue|xi) > p(red|xi), then while p(red|xi) > \u03b4 it must be the case that p(blue|xi) >> \u03b4 and hence the sample would be labelled as blue by the standard LNL method. Fundamentally I fail to see how standard LNL methods would not satisfy equation 2, please explain?\n\nNote that as mentioned in Figure 1 and reiterated above, colors are noisy labels, whereas shapes are true labels.  In other words, red circles are correctly labeled, whereas red triangles are incorrectly labeled.  That said, to answer your question, a fundamental difference between LNL methods and LNL+K is that they use different criteria to select samples.  **For an LNL method, it does not matter if a sample would satisfy Eq. 2 since this *is not the LNL criteria.***  Instead, LNL methods use the criteria p(red|xi) > \u03b4.   In other words, LNL methods often ask the question, what samples am I most confident are of the correct class (red in this case)?  In contrast, LNL+K asks, what samples am I most confident are not noise? These may seem like a subtle difference, but in practice it can have significant ramifications as demonstrated in our experiments. Effectively, the LNL criteria only requires that samples have a high likelihood of being a clean label *relative to other samples*, but all samples may have *low absolute likelihood* of being a clean label.  As such, LNL+K criteria is more suitable, as it does not measure the likelihood of being clean, but rather likelihood of not being noise.\n\n>How the method is incorporated into the various LNL methods is unclear to me. I can understand how the various methods currently identify clean labels. But a very short description is given of the integration in each of the 5 cases. I do not think the level of detail would be sufficient to replicate your results or to combine the method with another LNL method.\n\nWe apologize for the confusion.  As noted in Section 3.2, we provide a high-level summary of each method in the main paper, but a detailed description of how we modify each method for LNL+K is provided in Appendix A. We shall clarify these methods in a new version.\n\n> What does it mean to mix the \"samples\" in CRUST+k? What are the samples in this case? \n\nThe answers for these questions can be inferred from the earlier portion of this description in Section 3.2. I.e.,CRUST works by performing pairwise gradient distance within a class for clean sample selection, and selects those with the most similar gradients.  Thus, the \u201csamples\u201d are the data points that are labeled for a category (e.g., the red data points in Figure 1), and mixing them means that we would add the data samples for the noise source to this set (which would be the blue samples in Figure 1b).  This is further discussed in Appendix A.\n\n>The noise sources as defined above are simply a set of classes, they are not input dependent nor can I see how they can be mixed with training examples?\n\nThe noise sources are a set of classes, but each class must have their own labeled samples.  Thus, we can mix the samples of these categories together.  In other words, if \u201cdog\u201d is given as a noise source for \u201ccat\u201d images, then we must have labeled images of \u201cdogs\u201d in the training set for this noise source to be informative (i.e., we have to have some way of understanding what a \u201cdog\u201d is)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5761/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699975032489,
                "cdate": 1699975032489,
                "tmdate": 1699975112835,
                "mdate": 1699975112835,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]