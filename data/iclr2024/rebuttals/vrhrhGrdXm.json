[
    {
        "title": "KBFormer: A Transformer-based Diffusion Model of Structured Entities with Heterogeneous Properties"
    },
    {
        "review": {
            "id": "EaKInUJ74w",
            "forum": "vrhrhGrdXm",
            "replyto": "vrhrhGrdXm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8454/Reviewer_b6La"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8454/Reviewer_b6La"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a generative attention-based architecture, named KBFormer, for modeling structured entities consisting of different property types. The model is able to perform joint modeling of an entity's properties due to the hybrid diffusion training approach. The experimental results on a device KB and a nuclear physical dataset demonstrate the model's capability in representation learning for entity completion in diverse settings. The model is demonstrated to share information across properties and have various downstream uses for scientific applications. The inherent probabilistic nature of the model enables predictions accompanied by uncertainties."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality: The paper uniquely contributes to the field by proposing a generative attention-based architecture which can handle heterogenous data types through a mixed continuous-discrete diffusion process over the properties. Such a method stands out among the popular autoregressive models. \n\nQuality: The paper carefully designs the experiments to support the idea and improve their illustrations with helpful visualizations.\n\nClarity: The paper effectively communicates its ideas and findings with clarity. The paper is well-written, and the logic is coherent. Necessary mathematical deductions are presented for better understanding of the diffusion process. \n\nSignificance: The model proposed in the paper successfully handles heterogeneous property types along with hierarchical encodings with semantic meaning for different property types, and demonstrates its potential in various downstream scientific tasks. Besides, the model has an edge over traditional autoregressive models due to its inherent probabilistic nature."
                },
                "weaknesses": {
                    "value": "1. Although the authors make clear introductions to the hybrid diffusion training paradigm, the explanation for the model architecture is not clear enough (sometimes even confusing). In fact, the first of my two questions is because of not understanding the architecture here. I suggest the authors can modify Figure 5 and 6 to make the pipeline transparent to readers and include more details in the texts in an ordered way.\n\n2. Although the inherent probabilistic nature of the model makes it suitable for prediction tasks for scientific scenario, I still believe that comparisons between the KBFormer and some regression model baselines should be included to demonstrate the effectiveness of the model. Besides, in order to illustrate the model's capability in scientific applications, more experiments on benchmarks from other disciplines like molecules, proteins, etc., are required."
                },
                "questions": {
                    "value": "1. In terms of the model architecture, I suppose the output from the encoder is $(3, feature size)$ for the example shown in Figure 5. If that is true, I'm wondering why it is necessary to use a transformer model with such a short \"sequence length\".\n\n2. Transformer encoders are used to encode text fields. However, if the text is simply a property value (e.g. \"iPhone\" in Figure 5), why not just use a pretrained word embedding?\n\n3. The diffusion training strategy is a promising solution for probabilistic-based generative models. However, is the diffusion paradigm the optimal method to do this? Is there any other generative model (or even regressive model) that does not rely on the noise and denoise process?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8454/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8454/Reviewer_b6La",
                        "ICLR.cc/2024/Conference/Submission8454/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8454/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698715306719,
            "cdate": 1698715306719,
            "tmdate": 1700428882503,
            "mdate": 1700428882503,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6TRUQAWSkN",
                "forum": "vrhrhGrdXm",
                "replyto": "EaKInUJ74w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8454/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8454/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thorough review and suggestions! \n\n> 1. Although the authors make clear introductions to the hybrid diffusion training paradigm, the explanation for the model architecture is not clear enough (sometimes even confusing). In fact, the first of my two questions is because of not understanding the architecture here. I suggest the authors can modify Figure 5 and 6\u2026\n\nWe have made significant strides in improving the clarity of the text and the model architecture in particular (see changes in blue). Following your suggestions, we completely reworked Figures 5 and 6 (now combined into Figure 4). \n\n> 2. Although the inherent probabilistic nature of the model makes it suitable for prediction tasks for scientific scenario, I still believe that comparisons between the KBFormer and some regression model baselines should be included to demonstrate the effectiveness of the model. Besides, in order to illustrate the model's capability in scientific applications, more experiments on benchmarks from other disciplines like molecules, proteins, etc., are required.\n\nThank you for the suggestions! We have now included an additional baseline: Gradient Boosted Decision Trees (GBDT)s, which offer state-of-the-art performance on tabular data for both regression and classification tasks. Our experiments show that we can perform competitively against specialized regression models even though our model is generative in nature. We also included additional experiments across 15 datasets and compared our results against other generative models spanning various tasks, including two scientific datasets (MiniBooNE and Higgs).\n\n> Q1. In terms of the model architecture, \u2026 why it is necessary to use a transformer model with such a short \"sequence length\".\n\nWe have updated this figure and added extensive details around it. The previous figure used a small example with three properties for simplicity, though in practice, we use many more (see Table 6 with a description of all the datasets we used and the number of properties in each one.) The updated figure should clarify that there are more properties than what is shown. The transformer aggregates information across all properties (e.g., our tabular data experiments have up to 50). In future work, we aim to scale to a much larger number of properties and knowledge bases, and the transformer architecture is a good candidate for this.\n\n> Q2. Transformer encoders are used to encode text fields. However, if the text is simply a property value (e.g. \"iPhone\" in Figure 5), why not just use a pretrained word embedding?\n\nFigure 5 (now 4) should no longer imply that text fields are single words. If this were the case, we could indeed use pre-trained word embeddings. However, the goal of the text generator is to fill text property values with a (possibly long) sequence of tokens.\n\n> Q3. The diffusion training strategy is a promising solution for probabilistic-based generative models. However, is the diffusion paradigm the optimal method to do this? Is there any other generative model (or even regressive model) that does not rely on the noise and denoise process?\n\nIn our latest round of experiments, we explored the performance of our diffusion-based approach against several other generative models including VAEs, GANs, and a different implementation of diffusion on tabular data (TabDDPM). We also compare against regression models for nuclear physics and the GSM phone dataset. Across all the experiments, KBFormer obtains favorable performance, which illustrates the strength of the approach. Please note that our diffusion process has \u201cless noise\u201d than expected from more standard approaches. When we make predictions autoregressively, noise is injected via the random order in which properties are unmasked."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8454/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174979575,
                "cdate": 1700174979575,
                "tmdate": 1700174979575,
                "mdate": 1700174979575,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ya0FitZifu",
                "forum": "vrhrhGrdXm",
                "replyto": "6TRUQAWSkN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8454/Reviewer_b6La"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8454/Reviewer_b6La"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's response to my questions and concerns. I really appreciate that you have helped me better understand your contributions. The responses have resolved my concerns, and the quality of the paper is significantly improved. I'll consider raising my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8454/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700428855224,
                "cdate": 1700428855224,
                "tmdate": 1700428855224,
                "mdate": 1700428855224,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yPe1IbirUi",
            "forum": "vrhrhGrdXm",
            "replyto": "vrhrhGrdXm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8454/Reviewer_QYBP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8454/Reviewer_QYBP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a generative attention-based architecture that models structured entities comprising different property types, with applications on KB entities and tabular data. A hybrid diffusion training paradigm is proposed to handle the modeling of heterogeneous properties."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-presented, with excellent visualizations and clear delivery of the model details and results.\n2. KBTransformer enjoys superior performance against baselines in terms of prediction accuracies on two real data sets."
                },
                "weaknesses": {
                    "value": "1. The paper's contribution seems to be a bit incremental, since the diffusion modeling over heterogeneous data in Section 3.2 follows the previous work [1]. It would be helpful if the authors could clarify the difference/contribution of the proposed method.\n2. In the experiments, only the baseline that always predicts the marginal mode/mean is compared in terms of the prediction accuracy with different unmasked rates. Are there any other baselines with regression/autoregression that can be compared? Also it would be helpful if the authors could elaborate on Section 3.1 about why other traditional models with regression and masked modeling will have less optimal performance.\n\n---\n[1] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981\u201317993, 2021."
                },
                "questions": {
                    "value": "Please see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8454/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815528721,
            "cdate": 1698815528721,
            "tmdate": 1699637054869,
            "mdate": 1699637054869,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I9RQCLSZoW",
                "forum": "vrhrhGrdXm",
                "replyto": "yPe1IbirUi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8454/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8454/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review! We uploaded a revised paper with additional clarifications, baselines, and experiments, which we hope address your concerns.\n\n> 1. The paper's contribution seems to be a bit incremental, since the diffusion modeling over heterogeneous data in Section 3.2 follows the previous work [1]. It would be helpful if the authors could clarify the difference/contribution of the proposed method.\n\nHere we summarize the differences between our work and [1]:\n\nD3PM [1] deals with discrete-state spaces in **discrete time.** Campbell\u2019s formulation [[2]](https://proceedings.neurips.cc/paper_files/paper/2022/file/b5b528767aa35f5b1a60fe0aaeca0563-Paper-Conference.pdf) (which itself is parallel to [1]) gives a very general case for **continuous-time** discrete-state models. Our derivation builds on the continuous-time approach and shows that for the absorbing-state case, the ELBO can be estimated using a single evaluation of the model (two are required in Campbell\u2019s formulation, or a biased approximation). Additionally, [1] and [2] deal with continuous state spaces via Gaussian discretization, whereas we propose using a Gaussian Mixture Model.\n\nMoreover, in addition to the diffusion paradigm we derive, we also propose an architecture and hierarchical encoding scheme that can handle structured data and obtain superior performance across most datasets.  Finally, note that in many of our new experiments, we compare against TabDDPM, which is partly an implementation of [1] to handle tabular datasets, and show that we obtain favorable performance in many cases.\n\n> In the experiments, only the baseline that always predicts the marginal mode/mean is compared in terms of the prediction accuracy with different unmasked rates. Are there any other baselines with regression/autoregression that can be compared? Also it would be helpful if the authors could elaborate on Section 3.1 about why other traditional models with regression and masked modeling will have less optimal performance.\n> \n\nYes! We added Gradient Boosted Decision Trees (GBDTs) as a new baseline in our reconstruction experiments. We train a GBDT on all other properties for each numerical and categorical type. This is a much stronger baseline than what we had before (see the updated section 5 and appendices E and F, which contain more training details). We also include experiments on 15 datasets with new baselines built on various generative frameworks (GAN, VAEs, DDPM, etc). \n\nAs for section 3.1, we argued that generating many properties at once can degrade performance. In our diffusion framework, that would be akin to skipping \u201ctime steps.\u201d We ran ablations on the 15 datasets to show that generating all properties at once can impact performance greatly (see Table 4 in Appendix B.1). We also show a few qualitative examples in Appendix B.1.2). In one of these examples, we generate MNIST digits in one time step, and one can visually inspect the samples to see they are a lot noisier compared to the autoregressive samples. To clarify further, suppose in a language modeling setting, we are interested in predicting a sequence of two tokens that answers a question positively. \u201c[Of] [course]\u201d and \u201c[No] [problem]\u201d could be equally likely candidates. However, sampling the two tokens simultaneously can result in answers like \u201c[No] [course]\u201d or \u201c[Of] [problem].\u201d We run exactly into this inconsistency issue if we try to sample all properties simultaneously. But it can be solved by conditioning the second token generated on the first token. This is why our autoregressive model performs much better than the single-step approach.\n\n[1] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981\u201317993, 2021.\n\n[2] Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. A continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35, 2022."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8454/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700271336905,
                "cdate": 1700271336905,
                "tmdate": 1700271336905,
                "mdate": 1700271336905,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7IIQDsgExt",
            "forum": "vrhrhGrdXm",
            "replyto": "vrhrhGrdXm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8454/Reviewer_yZ6H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8454/Reviewer_yZ6H"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents KBFormer, an innovative transformer-based diffusion model adept at managing structured entities featuring heterogeneous characteristics. This versatile model excels in accommodating entities with complex hierarchical attributes, making it particularly suited for structured knowledge bases (KBs) and tabular data. KBFormer is designed to learn representations that are effective for entity completion across a range of contexts, and its probabilistic approach allows for predictions that incorporate measures of uncertainty. The authors detail the model's training methodology, introducing a novel loss modification that reimagines the problem as a continuous-time diffusion process over discrete states with an absorbing state. The paper culminates with an exploration of KBFormer's applicability in downstream tasks, highlighting its capacity to model numerical properties with remarkable precision and to generate accurate predictions in specialized domains, including nuclear physics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- This work introduces KBFormer, a novel transformer-based diffusion model adept at managing structured entities with varied and complex properties.\n- The paper demonstrates the model's practical applications, particularly in high-accuracy numerical property modeling and precise prediction-making in fields like nuclear physics, highlighting its value to researchers.\n- The paper is well-placed in the literature, with the KBFormer framework being noted for its flexibility and extensiveness, marking a progression from previous models.\n- The paper is clearly written and accessible, with lucid explanations of the model's architecture, training processes, and experimental results, catering to a broad audience.\n- The submission includes supplementary materials for implementation, which enhances the paper's credibility and supports the reproducibility of the KBFormer model."
                },
                "weaknesses": {
                    "value": "- This paper lacks ablation studies. The paper does not include ablation studies to analyze the contribution of different components of the model to its overall performance. For example, it is mentioned in paragraph \"Encoding\" of section 4, that two alternatives for embedding numerical values, yet it lacks a quantitative performance comparison between these methods. Conducting such an analysis could shed light on the critical components of the model and direct future enhancements.\n- There is no discussion in the paper about the limitations or potential failure modes of the KBFormer method. Including this could be crucial for fully understanding where the model may fall short in practical applications and where further research and development could be most beneficial."
                },
                "questions": {
                    "value": "Besides the aspects mentioned in \"Weakness\", I have the following concerns:\n\n- I would like to suggest the authors confirm whether the abbreviations used throughout the paper, such as \u201cKB\u201d in the abstract, are consistently defined upon first use? A uniform approach to abbreviation would aid reader comprehension.\n- The introduction promises that the KBFormer model addresses several tasks: KB completion, entity linking, anomalous property detection, and enhancement of foundation models with learned representations. The experiments, however, seem to showcase a subset of these. Can the authors clarify the criteria for experiment selection and indicate if demonstrating the model's capabilities on the remaining tasks is within the scope of future work?\n- Would the authors consider revising the reference list to ensure that all citations are consistent and reflect the most current research where applicable? This would help maintain the paper\u2019s relevance and assist readers in locating the sources."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8454/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699283678216,
            "cdate": 1699283678216,
            "tmdate": 1699637054749,
            "mdate": 1699637054749,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nRrvWhGsyE",
                "forum": "vrhrhGrdXm",
                "replyto": "7IIQDsgExt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8454/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8454/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review and all the suggestions! We are currently working on implementing them. We would be happy to revise our references with additional material. Are there any specific references we are missing?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8454/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151347907,
                "cdate": 1700151347907,
                "tmdate": 1700151347907,
                "mdate": 1700151347907,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "41RzVSyFCF",
                "forum": "vrhrhGrdXm",
                "replyto": "nRrvWhGsyE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8454/Reviewer_yZ6H"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8454/Reviewer_yZ6H"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for addressing my query. While I am satisfied with the overall selection of relevant literature, I would like to recommend a minor yet important refinement in the consistency of the reference list. Specifically, I've noticed some discrepancies in how papers from the 'Advances in Neural Information Processing Systems' are cited. For instance, some entries include page numbers, while others do not, and there are a few that even have links to the papers. Uniformity in these citations would greatly enhance the clarity and professionalism of the reference list. This suggestion, although seemingly minor, reflects a commitment to meticulous academic standards. (Yeah well that might be my OCD.)\n\nWarm regards,\nReviewer yZ6H"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8454/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153364161,
                "cdate": 1700153364161,
                "tmdate": 1700153364161,
                "mdate": 1700153364161,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bpxKDIHhhS",
                "forum": "vrhrhGrdXm",
                "replyto": "7IIQDsgExt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8454/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8454/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review and the great suggestions. The new version of the paper reflects the changes mentioned below.\n\n> \u2022 This paper lacks ablation studies. The paper does not include ablation studies to analyze the contribution of different components of the model to its overall performance. For example, it is mentioned in paragraph \"Encoding\" of section 4, that two alternatives for embedding numerical values, yet it lacks a quantitative performance comparison between these methods. Conducting such an analysis could shed light on the critical components of the model and direct future enhancements. \n\nYou are absolutely right! We have added ablations in Appendix B for the following:\n\n- Single-step masked modeling vs. absorbing state (autoregressive) diffusion.\n- DICE vs. Periodic Encoding for the numerical embeddings.\n- GMM vs. MSE for numerical quantities. (Using GMM with 1 mixture and unit variance is equivalent to training with MSE.)\n- Tying numerical embeddings across properties.\n\n> \u2022 There is no discussion in the paper about the limitations or potential failure modes of the KBFormer method. Including this could be crucial for fully understanding where the model may fall short in practical applications and where further research and development could be most beneficial.\n\nWhen mentioning future work, we allude to scaling to larger datasets and performing pre-training across multiple domains. That is our current main limitation. We made it more explicit in the paper and added a longer limitations section in Appendix G.\n\n> \u2022 I would like to suggest the authors confirm whether the abbreviations used throughout the paper, such as \u201cKB\u201d in the abstract, are consistently defined upon first use? A uniform approach to abbreviation would aid reader comprehension.\n\n\nThis should be resolved in our latest update of the paper. Apologies for missing some definitions in the first version!\n\n> \u2022 The introduction promises that the KBFormer model addresses several tasks: KB completion, entity linking, anomalous property detection, and enhancement of foundation models with learned representations. The experiments, however, seem to showcase a subset of these. Can the authors clarify the criteria for experiment selection and indicate if demonstrating the model's capabilities on the remaining tasks is within the scope of future work?\n\nIn the introduction, we list some applications to motivate developing our approach, though it is true we do not tackle all tasks we use for motivation. However, we are working on extensions to some of the other tasks in future work.\n\n> \u2022 Would the authors consider revising the reference list to ensure that all citations are consistent and reflect the most current research where applicable? This would help maintain the paper\u2019s relevance and assist readers in locating the sources.\n\nWe updated our references for improved consistency across references from the same source.\n\nCheers! The authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8454/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700270689279,
                "cdate": 1700270689279,
                "tmdate": 1700270740975,
                "mdate": 1700270740975,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TGSzMePHLZ",
                "forum": "vrhrhGrdXm",
                "replyto": "bpxKDIHhhS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8454/Reviewer_yZ6H"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8454/Reviewer_yZ6H"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for addressing the concerns raised by me. The updates to the paper with so many details on ablation studies are quite impressive, leading me to consider raising my review score. I will pay close attention to other reviewers' responses. I appreciate your responsiveness to the feedback.\n\nBest regards,\nReviewer yZ6H"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8454/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493830663,
                "cdate": 1700493830663,
                "tmdate": 1700493830663,
                "mdate": 1700493830663,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "miopZhwLZP",
            "forum": "vrhrhGrdXm",
            "replyto": "vrhrhGrdXm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8454/Reviewer_vWit"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8454/Reviewer_vWit"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a diffusion-based generative model to study the structured entities, with various property types such as numerical, categorical, strings. This work should address an interesting topic. However, I am not an expert in this area, and I am getting quite confused about the whole procedure and mechanism after several rounds of reading. I have a quick look at the code and believe the author should implement correctly. Maybe the authors can provide more details about KBformer, and re-organize the presentation to make it easy for understanding."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**1** The problems that this work tries to address are interesting and important. Also, the performance is promising.\n\n**1** The usage of diffusion process for entities is well-explored."
                },
                "weaknesses": {
                    "value": "However, I hope the authors can improve the paper through the following aspects:\n\n**1** the presentation of the paper is quite poor. For example, I hope the authors can make it comprehensive for Figure 5, many parts in this figure is unexplained. Maybe since I am not an expert in this topic, I find it quite confused and have many unclear parts for the KBFomer architecture."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8454/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699408478774,
            "cdate": 1699408478774,
            "tmdate": 1699637054647,
            "mdate": 1699637054647,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aqHP7iusM5",
                "forum": "vrhrhGrdXm",
                "replyto": "miopZhwLZP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8454/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8454/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review! We elaborated extensively in the paper (see changes in blue). We hope you find the updated Figure 5 (now Figure 4) to be clearer. If not, please let us know what points remain unclear so we can clarify them both in the paper and our final response."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8454/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700167756473,
                "cdate": 1700167756473,
                "tmdate": 1700167756473,
                "mdate": 1700167756473,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sMJLSwnfoo",
            "forum": "vrhrhGrdXm",
            "replyto": "vrhrhGrdXm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8454/Reviewer_EEBS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8454/Reviewer_EEBS"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a diffusion model for generating and demasking structured entities. KBFormer uses a mixed continuous-discrete diffusion process to generate different data types, such as text, numerical, categorical, and ordinal, and a transformer module to model cross-property relations. The paper demonstrates that KBFormer can outperform large language models on entity completion tasks and provide uncertainty estimation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed architecture with much smaller model parameters  can outperform LLMs which highlights the importance of modeling structure-aware inductive biases.\n\n- It can perform entity completion tasks with high accuracy and provides uncertainty estimation which is very useful for science applications that require confidence and reliability\n\n- It serves as an interpretable multi-modal foundation model for structured data and can augment LLMs with structure-aware inductive biases."
                },
                "weaknesses": {
                    "value": "1) The paper does not discuss or compare with other methods that can handle discrete data with continuous state, such as [1].  Moreover, the paper only compares with LLaMA2 for the first experiment, but it would be interesting to see how the proposed model performs against other knowledge masking strategies, such as [2, 3].\n\n2) The section on \u201cContinuous Relaxation of Discrete State Diffusion\u201d is not well explained. It is unclear what its objective is;  is the goal to learn bin centers, and how they are used in demasking? Is the discretization with 256 bins and learned bin centers similar to GMM with 256 mixtures? The paper also introduces some terms without proper definitions, such as \u201can infinite bin limit approximation\u201d and \u201cdiscretization with a large but finite bin density\u201d. It would be helpful to provide more details and intuition behind these concepts.\n\n3) The paper needs to improve its writing quality and clarity. Some specific issues are:\nProposition 1: The font of the proposition should be consistent and italicized. Is a proof provided  in the appendix?\nPage 6: The phrase \u201c\u2026 see Section 3.2.\u201d should be enclosed in parentheses, as it is not part of the main sentence.\n\nReferences:\n\n[1] Chen, T., Zhang, R. and Hinton, G., 2022. Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202.\n\n[2] Sun, Yu, et al. \"Ernie: Enhanced representation through knowledge integration.\" arXiv preprint arXiv:1904.09223 (2019).\n\n[3] Wang, Ruize, et al. \"K-adapter: Infusing knowledge into pre-trained models with adapters.\" arXiv preprint arXiv:2002.01808 (2020)."
                },
                "questions": {
                    "value": "1) Which type of encoder and decoder did you use for different property types: i)- conditioning on the property itself or ii) disjoint encoders for each property ?\n\n2) In page 6, it is stated that \u201c \u2018year\u2019 has the same representation in different contexts \u2026\u201d but the RNN encoder outputs context-aware representation. Can you clarify?  \n\n3) For experiments in section 5.2, what is the evaluation method? It is also worth comparing it with other models such as fine-tuned LLaMA2.  Also, for the evaluation of experiment in 5.1, please provide more clarification or examples for rotating each dictionary\u2019s fields D times and predicting only the last value."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8454/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699482593117,
            "cdate": 1699482593117,
            "tmdate": 1699637054519,
            "mdate": 1699637054519,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bcvxczB99T",
                "forum": "vrhrhGrdXm",
                "replyto": "sMJLSwnfoo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8454/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8454/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your thorough feedback!\n\n> 1. The paper does not discuss or compare with other methods that can handle discrete \u2026\n\nThank you for bringing this literature to our attention. In this submission, we focus on handling different property types in a generative framework; in future work, we will focus on knowledge-intensive tasks, making these comparisons very important.\n\nIn the latest submission, we conducted additional experiments across 15 datasets, comparing KBFormer against several other generative models specializing in structured data. The additional results showing favorable performance on most datasets against CTABGAN, TVAE, and TabDDPM are presented in Table 3. We hope that these new experiments satisfy your request for more baselines.\n\n> 2. The section on \u201cContinuous Relaxation of Discrete State Diffusion\u201d is not well explained\u2026\n\nWe have changed the contents of this section and added significantly more details to improve the clarity and the general flow (see the new text in blue). \n\nTo summarize here, the objective of this section is to give some intuition on how one might utilize the formulation of discrete space diffusion for continuous properties. Naively, if one wants to model a continuous property as a multi-class prediction problem, one would discretize the space with many bins (to maintain good precision), but the softmax computation can become quite expensive. This would also discard ordinality. However, under the simplifying assumption of Gaussian properties, we can formulate the problem as a mean-squared error (MSE) and maintain ordinality and good precision. This assumption does not usually hold, so we must use something more expressive, like a Gaussian Mixture Model (GMM), which can be seen as a generalization of the MSE approach. This section aims to develop intuition to justify using GMMs to model numerical properties within the full framework. \n\nIn practice, we completely drop any discretization and use the GMM parameters to sample numerical properties instead of logits. The number of mixtures we choose is simply a hyperparameter.\n\nPlease let us know if anything remains unclear so we can elaborate in further responses and the paper.\n\n> 3. The paper needs to improve its writing quality and clarity. Some specific issues are: Proposition 1: \u2026\n\n\nWe expanded our explanations and improved the general flow (see changes in blue). We now clearly point to the proof of Proposition 1 in the appendix, and we fixed all the typos/issues we found.\n\n> Q1. Which type of encoder and decoder did you use for different property types: i)- conditioning on the property itself or ii) disjoint encoders for each property?\n\nOne can do both. On a larger scale, we expect conditioning to enable dealing with unseen properties, but for the experiments in this paper, we used disjoint encoders/decoders. We ablate performance when sharing numerical embeddings in the current framing on GSM, which also seems to work.\n\n> Q2. In page 6, it is stated that \u201c \u2018year\u2019 has the same representation in different contexts \u2026\n\nYes, that was unclear. Apologies for the confusion. The GSM dataset has the property \"Phone.Launch.Year\". The RNN will process this as a sequence, RNN([\"Phone\", \"Launch\", \"Year\"]). Thus, if the token \"Year\" exists in another context (let's say \"Phone.Discontinued.Year\"), the RNN representations will contain information on the similarity of both keys.\n\n> Q3. For experiments in section 5.2, what is the evaluation method? It is also worth\u2026\n\nWe added a strong baseline across both sections referenced. In (prior) section 5.2, we evaluated the model's predictions on a given property when conditioned on all other available properties (this dataset is sparse). \n\nIn (prior) 5.1, we explained why we rotate the inputs as an augmentation. To answer the question here, since LLaMA is sequence-autoregressive, we have to rotate the string representation of the dictionary such that each field/property can appear in the last position. This ensures we can predict a value for the property while conditioning on all other properties.\n\nMore generally, causal models can perform worse when predicting properties [[1]](https://arxiv.org/pdf/2309.14402.pdf), even when they are in the training set. So, we ensure that the model has seen enough permutations of the properties to be able to make predictions while conditioning on any subset.\n\n[1] Physics of Language Models: Part 3.2, Knowledge Manipulation"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8454/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700167814279,
                "cdate": 1700167814279,
                "tmdate": 1700175231023,
                "mdate": 1700175231023,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]