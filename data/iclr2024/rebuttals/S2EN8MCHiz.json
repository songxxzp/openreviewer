[
    {
        "title": "Understanding Vision and Language Representations under the Lens of Intrinsic Dimension"
    },
    {
        "review": {
            "id": "LOA815RomI",
            "forum": "S2EN8MCHiz",
            "replyto": "S2EN8MCHiz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4796/Reviewer_sCrT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4796/Reviewer_sCrT"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an empirical investigation into the intrinsic dimension (ID) of multi-modal models. Traditionally, ID has been analyzed in uni-modal models to measure the utilization of a $D$-dimensional representation space. This study extends such analysis to multi-modal contexts, and analyzes the BLIP visual language model on the MS-COCO dataset. The paper finds that 1) higher IDs are observed in the visual modality compared to the language modality; 2) cross-modal attention struggles to align low-dimensional language representations with their visual counterparts; 3) IDs can be useful indicators of weight importance during model pruning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- This is a pioneering study on intrinsic dimensions within multi-modal models, potentially offering valuable theoretical insights.\n- The findings are interesting, particularly in highlighting the higher intrinsic dimension of the visual modality compared to language, the usefulness of ID in determining layer significance, and the greater sensitivity of the visual modality to pruning."
                },
                "weaknesses": {
                    "value": "- The study is limited to one specific vision-language model (BLIP), raising concerns about the generalizability of the conclusions. It remains unclear if these findings hold across diverse multi-modal models with different training objectives, such as discriminative contrastive models like CLIP, generative models with trainable text decoders like LLaVA/OpenFlamingo, or models involving more modalities like ImageBind.\n\n- The paper's presentation is poor regarding writing style and organization. It frequently introduces terms and acronyms without sufficient explanation (e.g., intrinsic dimension, TwoNN algorithm, BLIP, Magnitude/Sensitivity pruning), potentially confusing readers less familiar with the subject. The relevance of certain sections, such as the comparison of intrinsic dimensions between Transformers and CNNs in Section 3.1, is out of scope. Additionally, some sections are purely hypothetical without empirical support, and the overall presentation lacks a cohesive message.\n\n- The implications of the findings on the advancement of multi-modal training techniques are not discussed, lacking a broader context that could enhance the paper's impact."
                },
                "questions": {
                    "value": "- The paper often inconsistently uses \\citet and \\citep. \n- In Figure 1, it's unclear which parts represent the vision modality and the language modality.\n- The meaning of \"im\" and \"op\" in Figure 3 is unclear. Could you define these terms?\n- On Page 7, the statement \"ID values have a positive correlation with model performance but not in direct proportion\" requires more detail and further elaboration."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4796/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4796/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4796/Reviewer_sCrT"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4796/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698651354248,
            "cdate": 1698651354248,
            "tmdate": 1699636462313,
            "mdate": 1699636462313,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FNpt9xfYv1",
                "forum": "S2EN8MCHiz",
                "replyto": "LOA815RomI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4796/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4796/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer sCrT"
                    },
                    "comment": {
                        "value": "Thank you for your review. We appreciate your feedback. Here is our detailed response, which we hope will address your concerns.\n\n**Q1: How generalizable are the findings across different multi-modal models?**\n\nA1: Our research is indeed based on only one specific visual-language model (BLIP). However, BLIP is a universal and unified multi-modal pretraining that is trained with three objectives: Image-Text Contrastive Loss (used by CLIP), Image-Text Matching Loss, and Language Modeling Loss (used by LLaVA). We believe that our findings are still valuable and insightful, as BLIP is a representative and widely used model for general vision-language tasks. Also, our method of applying ID to multimodal scenarios is model-agnostic which can be applied to other models with different training objectives and modalities.\n\n**Q2: The presentation issues.**\n\nA2: \n- All terms and acronyms are defined with citations.\n\n    - Intrinsic dimension (ID) is introduced in the last line on Page 1 and further introduces its applications in subsequent paragraphs;\n\n    - TwoNN algorithm is explained in Sections 2.2 and 2.3;\n\n    - BLIP is explained in Section 2;\n    - Magnitude/Sensitivity pruning is explained in the second paragraph of Section 4.2.\n - In Section 3.1, we compare the ID of the Transformer and CNN models to demonstrate the generalizability of our findings. So we think it is necessary to keep it.\n - In response to the concern that \"some sections are purely hypothetical without empirical support\" we would like to clarify. All assumptions and conclusions in our paper are grounded in theoretical analysis and experimental validation. If the reviewer could indicate the specific sections that are problematic, we would greatly appreciate it and will make every effort to clarify and enhance them.\n\n**Q3: The implications of the findings on the advancement of multi-modal training techniques are not discussed, lacking a broader context that could enhance the paper's impact.**\n\nA3: Thanks for your insightful comment. We agree that ID has a lot of potential in multi-modal training techniques. Our study shows that ID can prune multimodal models efficiently and effectively. This suggests that ID can benefit model structure and hyperparameter optimization for multimodal training. However, it needs more experiments, which could be future research.\n\n**Q4: Inconsistently use \\citet and \\citep.**\n\nA4: Thank you for pointing out the typo. We will revise it in the revision.\n\n**Q5: How to distinguish the vision modality and the language modality in Figure 1?**\n\nA5: We marked the background color with red for vision and blue for language in all figures present IDs, to make it easier to distinguish the modalities.\n\n**Q6: What does \u201cim\u201d and \u201cop\u201d mean in Figure 3?**\n\nA6: \u201cim\u201d and \u201cop\u201d denote the intermediate layer and output layer, respectively. They are defined from the officially released BLIP network architecture. We denote these terms in the caption of Figure 3.\n\n**Q7: On Page 7, the statement \"ID values have a positive correlation with model performance but not in direct proportion\" requires more detail and further elaboration.**\n\nA7: We give an example to show this: in Figure 4, Sens and Full BLIP had a similar ID value gap as Mag and Mag w/o ft, but their performance gap (using CIDEr metric) is 9 and 177, which were not proportional. This shows that ID values are correlated with model performance, but not enough to evaluate model quality."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4796/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700420996859,
                "cdate": 1700420996859,
                "tmdate": 1700420996859,
                "mdate": 1700420996859,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "koBKblJK5Z",
                "forum": "S2EN8MCHiz",
                "replyto": "FNpt9xfYv1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4796/Reviewer_sCrT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4796/Reviewer_sCrT"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response. However, as most of my concerns remain, such as the generalizability of findings on different models and the lack of implications of these findings, I would retain my score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4796/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641649459,
                "cdate": 1700641649459,
                "tmdate": 1700641649459,
                "mdate": 1700641649459,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Q6ZcOKHYr6",
            "forum": "S2EN8MCHiz",
            "replyto": "S2EN8MCHiz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4796/Reviewer_Vq7s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4796/Reviewer_Vq7s"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the intrinsic dimension (ID) of a large-scale vision-language pre-training model and explores the relationships between ID, modality, and prunability. The authors find that the geometric characteristics of visual and language representations differ significantly, resulting in distinct prunability for each modality. They propose an importance metric based on ID for multimodal model pruning, which yields superior performance. The experimental results show that visual representations are more sensitive to pruning, while language representations are more robust."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors propose to investigate the intrinsic dimension (ID) of a large-scale vision-language pre-training model called BLIP and explore the relationships between ID, modality, and prunability.\n2. This paper applies ID to multimodal scenarios(i.e., language and vision) and propose an importance metric based on ID for multimodal model pruning, which yields superior performance.\n3. This article conducts detailed experiments and the experimental results support their claims."
                },
                "weaknesses": {
                    "value": "1. In Figure 2 1), the VLP is a transformer-based model, but its hunchback-shaped profiles is not significant, the author should explain the reason.\n2. Is w/o retrain in Figure 4 w/o finetuning? Why not just use w/o finetuning?\n3. This paper argues that the maximum ID is a more critical indicator for performance prediction, which is against the observation that the ID of the last latent layer indicates model performance, but from Figure 4, the ID of the last latent layer can also indicate model performance.\n4. In Figure 5, why the multiplication of IDs lead to the IDs of pure language representations decrease?"
                },
                "questions": {
                    "value": "See Weaknesses Part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4796/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674502100,
            "cdate": 1698674502100,
            "tmdate": 1699636462199,
            "mdate": 1699636462199,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aksbEjCI14",
                "forum": "S2EN8MCHiz",
                "replyto": "Q6ZcOKHYr6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4796/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4796/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Vq7s"
                    },
                    "comment": {
                        "value": "We value your comments and we hope that our reply will address your concerns and clear up some confusion.\n\n**Q1: Why the hunchback-shaped profiles of VLP is not significant?**\n\nA2: We think the most plausible reason is the input of VLP is the fused multimodal representations rather than pure visual modality. As we explained at the bottom of Figure 2. \u201cVLP\u2019s hunchback is positioned further to the right side, \u2026 The delayed peak and lower value of Transformer-based models\u2019 IDs can be explained by integrating textual features, particularly in the single-stream VLP model which fuses visual and language at the beginning. As described in Section 3.2, language representations have consistent and lower IDs.\u201d\n\n**Q2: What does \u2018w/o retrain\u2019 mean in Figure 4?**\n\nA2: Yes, \u2018w/o retrain\u2019 is \u2018w/o finetuning\u2019. Thanks for pointing out the typo, we will revise it in the revision.\n\n**Q3: How is the maximum ID a better predictor of performance than the last layer ID?**\n\nA3: Table 1 shows that model performance is ranked as Full BLIP>Sens>Mag>Mag w/o ft. According to [1], a smaller last layer ID indicates better performance, suggesting a ranking of Mag w/o ft>Mag>Sens>BLIP. However, our experimental results (Figure 5) show a different order: Sens>Mag>Mag w/o ft>BLIP. This demonstrates that the maximum ID is a more reliable performance predictor than the last layer ID. We believe this is an intriguing finding that challenges previous assumptions.\n\n**Q4: Why the multiplication of IDs lead to the decrease of language IDs in Figure 5?**\n\nA4: We think that it is because the language weights are less important, and *ID effectively estimates this weight importance. This leads to more pruning on pure language weights, which results in the decrease of language IDs. This result also illustrates the correlation between ID values and weight importance from another perspective."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4796/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700419623114,
                "cdate": 1700419623114,
                "tmdate": 1700419623114,
                "mdate": 1700419623114,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fo4JZFaf6I",
            "forum": "S2EN8MCHiz",
            "replyto": "S2EN8MCHiz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4796/Reviewer_RpzU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4796/Reviewer_RpzU"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the cooperation and the utility of each modality in multimodal representation learning, specifically the intrinsic dimension (ID) of a large-scale vision-language pre-training model BLIP and its implications on layer importance, modality importance, and prunability.  Several new ideas are proposed based on this framework including identifying shortcomings of embedding modalities into the same low-dimensional manifold, studying the contribution of different modalities, predicting model performance, and a new method for multimodal model pruning (for which some experimental results are presented)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problem of better understanding multimodal models, particularly vision-language models, is important to this generally empirical field. This paper makes some nice contributions to this study.\n2. The idea of ID is interesting and the implications also have potential for analyzing and improving multimodal models.\n3. There are some experiments on pruning multimodal models which can be quite useful."
                },
                "weaknesses": {
                    "value": "1. The biggest issue with this paper is that it tries to do too much and ends up overclaiming on many fronts. Dissecting each of the claims in the abstract:\n\n---- Empirical study of ID: There is a good amount of discussion and experiments for this, which is good. But it is mostly about applying TWONN method for computing ID. Also, how do you know that the IDs computed are accurate? Is there an evaluation metric for the quality of ID?\n\n---- Studying modality contribution: I do not see this experiment at all, only some anecdotal statistics in section 3.\n\n---- Predicting model performance using ID values: I do not see this experiment at all, nor is this mentioned subsequently in the paper.\n\n---- Better pruning: There are experiments for this in tables 1 and 2, but there are no comparisons to established weight pruning methods, only 1 from Sens Zhang et al. (2022). More comparisons are needed to really prove the efficacy of this part.\n\nOverall, the paper would be well-suited from reducing the number of sub-claims/sub-applications and just focus on doing 1 or 2 really well.\n\n2. Section 3 needs work - there are some interesting results but it can be better phrased as well-motivated research questions. Taking '3.3 INTERPRETING CROSS-MODAL ATTENTION VIA IDS' as an example, why is interpreting cross-modal important? Why should I use ID to do it when other people have used attention weights etc.? What insights does using ID to interpret cross-modal tell me, can I use it to better train or debug models? See https://arxiv.org/abs/2207.00056 for an example of setting up what to interpret in multimodal models, and using rigorous human user-studies to validate each of the findings.\n\n3. It would be good to have an overall plot of performance vs parameters, with 1 line being your pruning method and other lines for other pruning baselines, and the line that pareto dominants would be best."
                },
                "questions": {
                    "value": "see weaknesses above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4796/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811355819,
            "cdate": 1698811355819,
            "tmdate": 1699636462078,
            "mdate": 1699636462078,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KGEyqn0RUK",
                "forum": "S2EN8MCHiz",
                "replyto": "fo4JZFaf6I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4796/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4796/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer RpzU"
                    },
                    "comment": {
                        "value": "Thank you for your comment. We appreciate your suggestions and we have carefully considered your review. We hope that our reply addresses your concerns and clears up some confusions.\n\n**Q1: The quality of ID**\n    \nA1: ID is an idealized concept that only synthetic datasets have accurate IDs. For real-world datasets, ID is an estimated value that varies with the algorithm and the sample size. Common methods like MLE and TWONN often underestimate ID. Based on [1], ID becomes stable when the number of samples exceeds 1000. So we set the number of samples as 2000 for efficiency and accuracy.\n    \n**Q2: Studying modality contribution**\n    \nA2: In Section 4.3 DO VISION AND LANGUAGE CONTRIBUTE DIFFERENT PRUNABILITY?, we analyzed how different weights affect the model performance. We found that the language modality can be pruned more than the vision modality, and that the cross-modal attention layers are the least prunable. This suggests that the language modality has more redundancy and the vision modality is essential for multimodal learning.\n    \n**Q3: Predicting model performance using ID**\n    \nA3: In the third paragraph of Section 4.1 DOES PRUNING REDUCE OR INCREASE THE ID, we compared three models with different performance and their ID changes. We concluded that \u201cID values correlate positively with model performance but not linearly. We argue that the maximum ID is a better predictor of performance, which contradicts the observation that the ID of the last latent layer indicates model performance Ansuini et al. (2019).\u201d\n    \nMoreover, in Section 4.2 CAN ID PREDICT LAYER IMPORTANCE?, we further examine the relationship between the ID changes of different modalities/layers and the overall performance: Figure 5 shows that Sens*ID increases the IDs of all visual representations, including all layers in the visual model and the k and v layers in the language model. Conversely, the IDs of pure language representations decrease due to the multiplication of IDs.\n    \n**Q4: Better pruning**\n    \nA4: The most recent and only paper on multimodal pruning is Upop[4]. However, Upop proposes a structured pruning method while ours is unstructured, which makes the comparison unfair. Our main contribution as an unstructured pruning method is an improved weight importance metric. To further validate the efficiency of ID, we conducted experiments using the leading importance metrics: gradient[2] and magnitude[3] methods at a 95% pruning ratio (20x compression). The results are as follows:\n\n|    Metric    | CIDEr | BLEU4 |\n|:------------:|:-----:|:-----:|\n| magnitude[3] | 17.56 | 10.28 |\n| magnitude*ID | 26.46 | 12.92 |\n|  gradient[2] | 20.80 | 11.17 |\n|  gradient*ID | 24.85 | 12.99 |\n\n**Q5: Why use ID to interpret cross-modal? How does it compare to attention? What insights can we get?**\n\nA5: We believe that ID and attention are complementary methods, each offering unique insights into multimodal representations. Attention provides an intuitive understanding of the local alignment between vision and text. On the other hand, ID is more objective, exposing the training status of hierarchical representations, such as whether the most abstract representations can be achieved with minimal parameters. Each method has different research goals and targets, so they don't compete with each other.\n\nIn section 3, we conduct an empirical study that reveals significant differences in the IDs of vision and language modalities. This suggests that they possess different geometric characteristics and complexities. In section 4, we use the difference in ID as a guide for multimodal pruning, further demonstrating its effectiveness.\n\n**Q6: Can you provide an overall plot showing performance versus parameters?**\n\nA6: That is a useful suggestion. However, in Figure 6, we have presented a similar comparison. The baseline method, labeled as \"Sensitivity\", is represented by a dark-colored line, while our method (sensitivity*ID) is represented by a light-colored line. All the light lines dominate the dark ones, except when pruning both vision and language at a 95% pruning ratio. We will expand the comparisons involving magnitude and gradient (as shown in the above tables) and add them to Figure 6.\n\n[1] Amsaleg, Laurent, et al. \"Estimating local intrinsic dimensionality.\"\u00a0*SIGKDD*. 2015.\n\n[2] Molchanov, Pavlo, et al. \u201cPruning Convolutional Neural Networks for Resource Efficient Inference.\u201d\u00a0*ICLR*. 2017.\n\n[3] Zhu M H, Gupta S. \u201cTo Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression.\u201d *ICLR* *workshop.* 2018.\n\n[4] Shi D, Tao C, Jin Y, et al. \u201cUPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers.\u201d *ICML*. 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4796/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700419374209,
                "cdate": 1700419374209,
                "tmdate": 1700419374209,
                "mdate": 1700419374209,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GvLARY5ftB",
                "forum": "S2EN8MCHiz",
                "replyto": "KGEyqn0RUK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4796/Reviewer_RpzU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4796/Reviewer_RpzU"
                ],
                "content": {
                    "title": {
                        "value": "thank you for your response"
                    },
                    "comment": {
                        "value": "Thank you authors for your responses. Although they have clarified some of my comments (regarding pruning performance and comparisons), the majority of my comments remain unaddressed. Specifically, I feel there needs to be significant effort into understanding the quality of ID prediction, which is the key quantity the paper aims to estimate. It is not clear that the quality of ID is good given the lack of evaluation. Furthermore, the findings regarding modality contribution and predicting model performance remain weak, with only some anecdotes (eg correlation, looking at performance drops) that are not rigorous enough - ideally you would have a formal setup for evaluating modality contribution/model performance, other relevant baselines, exhaustive qualitative and quantitative analysis, etc. I suggest the authors consider these rigorous experiments in future versions of the paper, and I keep my rating for now."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4796/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715209033,
                "cdate": 1700715209033,
                "tmdate": 1700715209033,
                "mdate": 1700715209033,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "m7yCh6RGJk",
            "forum": "S2EN8MCHiz",
            "replyto": "S2EN8MCHiz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4796/Reviewer_r1cw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4796/Reviewer_r1cw"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the intrinsic dimension (ID) of a large-scale vision-language pre-training model BLIP and explore the relationships among intrinsic dimension, modality, and prunability, and show that, the ID geometric characteristics of visual and language representations differ significantly in terms of range and shape, resulting in distinct prunability for each modality."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper first presents the empirical study into the ID of a large-scale multimodal pre-training model.\n2. It explains how visual and language modalities align and change IDs in cross-modal attention mechanisms, and show the visual and language representations do not lie on the same low-dimensional manifold.\n3. This paper alsos shows the correlation between IDs and layer-wise importance for multimodal pruning."
                },
                "weaknesses": {
                    "value": "I wonder why BLIP is chosen for this study, instead of more recnet multimodal models? Any explanations on this? Also, I wonder if the observations based on BLIP can be extended to other multimodal models, and how? If not, I'd suggest experiments with more multimodal models to validate the generality of the observations."
                },
                "questions": {
                    "value": "please see Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4796/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817082632,
            "cdate": 1698817082632,
            "tmdate": 1699636461958,
            "mdate": 1699636461958,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8RSSgTG0sY",
                "forum": "S2EN8MCHiz",
                "replyto": "m7yCh6RGJk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4796/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4796/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer r1cw"
                    },
                    "comment": {
                        "value": "Thank you for your feedback. Here is our detailed response to your review comments. We hope it addresses your concerns.\n\n**Q1: Why BLIP?**\n\nA1: We initially chose CLIP as our model since it is a well-known multimodal pretraining. However, upon further analysis, we realized that BLIP, proposed in 2022, offers more comprehensive multimodal representations. BLIP considers both multimodal generation and understanding, which are critical for practical applications. It is trained with three loss functions: Image-Text Contrastive Loss, Image-Text Matching Loss, and Language Modeling Loss, to obtain rich and diverse multimodal representations. Therefore, we think that BLIP is a superior choice for investigating the intrinsic dimension of multimodal pre-training models, as it considers both the generalizability and diversity of multimodal data.\n\n**Q2: How to extend?**\n\nA2: ID estimation is model-agnostic, making it simple and universally applicable. Any representation can have an intrinsic dimension estimated, which can then be used for weight pruning by multiplying the ID with the importance score (typically the weight's gradient[1] or magnitude[2]). This method is straightforward, effective, and easily extendable to any multimodal model. We further verify its efficiency with gradient[1] and magnitude[2] importance metrics at a 95% pruning ratio. The results are shown below. We believe ID is a good indicator to probe the state of multimodal representations.\n|    Metric    | CIDEr | BLEU4 |\n|:------------:|:-----:|:-----:|\n| magnitude[2] | 17.56 | 10.28 |\n| magnitude*ID | 26.46 | 12.92 |\n|  gradient[1] | 20.80 | 11.17 |\n|  gradient*ID | 24.85 | 12.99 |\n\n\n[1] Molchanov, Pavlo, et al. \u201cPruning Convolutional Neural Networks for Resource Efficient Inference.\u201d\u00a0*ICLR*. 2017.\n\n[2] Zhu M H, Gupta S. \u201cTo Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression.\u201d *ICLR* *workshop.* 2018."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4796/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700418844384,
                "cdate": 1700418844384,
                "tmdate": 1700422739913,
                "mdate": 1700422739913,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]