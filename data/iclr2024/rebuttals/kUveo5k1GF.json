[
    {
        "title": "Improving equilibrium propagation without weight symmetry through Jacobian homeostasis"
    },
    {
        "review": {
            "id": "UG8gXbu1KG",
            "forum": "kUveo5k1GF",
            "replyto": "kUveo5k1GF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5357/Reviewer_2L2m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5357/Reviewer_2L2m"
            ],
            "content": {
                "summary": {
                    "value": "The authors suggest a modification to the holomorphic equilibrium propagation algorithm, that helps it deal with cases where the Jacobian is not symmetric, by adding a term which penalizes asymmetry of the Jacobian. They show this approach outperforms standard holomorphic equilibrium propagation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The method improves over previous holomorphic equilibrium propagation in the applications tested. \n\nAs someone who is not in this field, I found section 2 to be informative introduction to the area."
                },
                "weaknesses": {
                    "value": "It was not entirely clear to me what the motivation for this work is.  While the authors suggest that hEP is potentially biologically plausible, this seems like a stretch.  Not only does it require neurons to do computations with complex numbers, it also seems to require that the network settle to equilibrium at multiple phases of an ongoing oscillation.  This does not seem likely in the brain: the period of the gamma oscillation is tens of milliseconds, which certainly would not allow enough time, and even the theta oscillation seems to fast for this. It also was not clear what computational advantages this might lead to in purely artificial systems for which biological plausibility was not important."
                },
                "questions": {
                    "value": "Most important for me is to explain how this could be helpful in artificial learning systems, or why the problem of settling time does not apply to biological systems (which seems unlikely to be honest)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5357/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698503265286,
            "cdate": 1698503265286,
            "tmdate": 1699636540197,
            "mdate": 1699636540197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fp0OkQAcip",
                "forum": "kUveo5k1GF",
                "replyto": "UG8gXbu1KG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5357/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Individual response to reviewer 2L2m"
                    },
                    "comment": {
                        "value": "> *\u201cAs someone who is not in this field, I found section 2 to be informative introduction to the area.\u201d*\n\nWe thank reviewer 2L2m for the positive comment.\n___\n> *\u201cWhile the authors suggest that hEP is potentially biologically plausible, this seems like a stretch. [...] it also seems to require that the network settle to equilibrium at multiple phases of an ongoing oscillation\u201d*\n\nWe agree that for now the question of bio-plausibility is debatable. But here are our arguments. \n\nOn a conceptual level, the brain does use a hierarchy of oscillations spanning a whole range of frequency, as reviewer 2L2m pointed out, so the fact that holomorphic EP introduces oscillation in a principled way to do learning is a very promising similarity in our opinion. Yet, answering the question of plausibility for hEP will require investigating potential *mappings* from the complex units to neurons or microcircuits in the brain. For instance, ReLU activation is often interpreted as a firing rate, and a mapping to spiking network can be made based on that. For complex-valued activation, one can imagine that it encodes not only a firing rate, but also a phase with respect to an underlying rhythm. Another promising advantage that makes hEP plausible shown in the original paper (Laborieux & Zenke 2022) is the robustness to noise in the dynamics, which is not only required for any theory of learning in the brain, but also useful for neuromorphics artificial learning systems.\n\nAbout the question on whether the settling time is an issue for biological systems, we want to mention that neurons could use a mechanism called \u201cprospective dynamics\u201d, which means they try to anticipate their future activity, and it allows them to tightly track a moving equilibrium, such that the lag is reduced (see [r1], [r2] and references therein), this mechanism is compatible with our theory where neurons could track the oscillating teaching signal and reduce the time needed to learn.\n___\n> *\u201cMost important for me is to explain how this could be helpful in artificial learning systems\u201d*\n\nWe list below some of the reasons why EP, and in particular the holomorphic extension could be helpful in artificial learning systems such as neuromorphic hardware [r3]:\n1. The system only needs to handle one type of computation: the neural dynamics, and it need not handle a separate linear computation to derive gradients (as in BP). Designing a system that has to handle both computation with the same memory devices is more difficult than just one computation.\n2. The learning rule in hEP only involves the activity of pre and post neurons, and therefore is local, which is a decisive advantage for building an efficient artificial learning system.\n3. The robustness to noise already mentioned in the previous point. Not only is it bio-plausible, but also useful for noisy neuromorphic hardware.\n4. Another advantage is that the error is encoded as an integral of significantly varying (non-vanishing) neuronal oscillations, rather than a finite difference between two nearby states, which is difficult to sense in noisy substrates. In short, the gradient in hEP is computed accurately despite the network oscillating in a nonlinear way, while other approaches typically rely on a linear expansion around the free fixed point.\n5. In addition, the error as an integral can be accumulated in continuous time, removing the need to access the gradient at a precise oscillation period.\n6. Last but not least, robustness to asymmetric weights in the extent we describe in the paper. \n\nFinally, hardware realization leveraging EP suggests that it can dramatically reduce the energy cost of AI training by several orders of magnitude [r3]. Taken together, we hope that these reasons will convince reviewer 2L2m that our work is also useful for artificial systems.\n___\n**References**\n\n[r1] Haider, Paul, et al. \"Latent equilibrium: A unified learning theory for arbitrarily fast computation with arbitrarily slow neurons.\" Advances in Neural Information Processing Systems 34 (2021): 17839-17851.\n\n[r2] Senn, Walter, et al. \"A neuronal least-action principle for real-time learning in cortical circuits.\" bioRxiv (2023): 2023-03.\n\n[r3] Yi, Su-in, et al. \"Activity-difference training of deep neural networks using memristor crossbars.\" Nature Electronics 6.1 (2023): 45-51."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699995797368,
                "cdate": 1699995797368,
                "tmdate": 1699995797368,
                "mdate": 1699995797368,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZLGSh6xx2q",
                "forum": "kUveo5k1GF",
                "replyto": "fp0OkQAcip",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5357/Reviewer_2L2m"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5357/Reviewer_2L2m"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response.  I remain unconvinced about bioplausibility.  Adding the comments on artificial systems would improve the manuscript, but not enough to warrant a change in the review scores."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649811845,
                "cdate": 1700649811845,
                "tmdate": 1700649811845,
                "mdate": 1700649811845,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Cxi3iZGbVq",
            "forum": "kUveo5k1GF",
            "replyto": "kUveo5k1GF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5357/Reviewer_nv6j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5357/Reviewer_nv6j"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied the weight asymmetry in the weight transport problem extending the Holomorphic Equilibrium Propagation algorithm (hEP), through investigations about the Jacobian of the network, with experiments and theoretically analysed the reasons for the experimental results. Based on the analysis results, some new features were proposed to enable the algorithm to evade the need for perfect weight symmetry without affecting the functional symmetry. A new form of Jacobian homeostasis was introduced to maintain functional symmetry, without directly addressing weight symmetry. Finally, several experiments, including the investigation of weight symmetry evolution during training and comparative experiments, were conducted to verify the effectiveness of the proposed algorithm, and the work\u2019s performance exceeded the networks with Recurrent Backpropagation (RBP), even on larger datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "*Relatively new perspective\n\nThis paper shows a new perspective on studying the weight transport problem with the Jacobian of the network. It explains the connections between weight symmetry, functional symmetry and Jacobian homeostasis.\n\n*Relatively rigorous analysis and persuasive experiments\n\nThis paper shows the efforts in investigating the weight symmetry evolution during training in section 4. If the observations could be discussed more deeply with the figure would be better. \n\n*The part about \u2018Jacobian homeostasis improves functional symmetry\u2019 in section 4 is very detailed and well analysed."
                },
                "weaknesses": {
                    "value": "*Some expressions with flaws\n\nAlthough \u2018hEP\u2019 is the abbreviation of \u2018holomorphic Equilibrium propagation\u2019 can be understood after reading. However, this abbreviation has not been indicated in parenthesis when mentioning its full term for the first time.\nThe last sentence \u2018It is worth nothing that\u2026\u2019, in Definition 1 of section 2.2, might be miswritten, which should be \u2018It is worth noting that\u2026\u2019 based on the context. Also in this paragraph, the sentence after Eq. (6) \u2018Importantly, the quantities \u2026, which applies only to EBMs.\u2019, is a little longer and complex. It might confuse readers, for convenience of understanding, it would be better to split it into two simpler sentences.\n\n*Some disadvantages in the layout\n\nIn section 3.2, the paragraph with Eq. (9) is intersected by Figure 2, which could be rearranged to provide enhanced readability. The same disadvantages happen in Figure 3 and Table 1.\n\n*The performance of the proposed method is relatively not so advanced. And the method for comparison is relatively not so new, making the work of this article not very convincing."
                },
                "questions": {
                    "value": "Please refer to the above weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5357/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760327351,
            "cdate": 1698760327351,
            "tmdate": 1699636540056,
            "mdate": 1699636540056,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "twJWLMIjnT",
                "forum": "kUveo5k1GF",
                "replyto": "Cxi3iZGbVq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5357/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Individual response to reviewer nv6j"
                    },
                    "comment": {
                        "value": "We thank reviewer nv6j for the positive feedback and comments.\n___\n> *\u201cThis paper shows the efforts in investigating the weight symmetry evolution during training in section 4. If the observations could be discussed more deeply with the figure would be better.\u201d*\n\nWe understand that reviewer nv6j means that we should discuss more in detail Fig. 4 c,g,k. For Fig 4 c,g, we record a metric quantifying the degree of symmetry of the Jacobian $J$. By expressing the Jacobian as a sum of a symmetric matrix and an antisymmetric matrix $J = S + A$, we record the quantity $\\frac{\\| S \\|}{\\|S\\| + \\|A \\|}$ which varies from 0 to 1 where 0 means fully antisymmetric (provided $J \\neq 0$), and 1 means fully symmetric. In Fig 4c we see that the metric goes very close to one because the connections are reciprocal between layers (Fig 4a). For Fig 4g, the metric increases but stays far from 1 because the connections are not reciprocal (Fig 4e), so perfect symmetry is unachievable. For Fig 4k, because we use a larger network (Fig 4i), computing the symmetry metric as in Fig. 4 c,g is not possible (the jacobian is too large a matrix), so instead we record the angle between forward and feedback weights as a replacement, for which 0 now means perfect symmetry. We see that the homeostatic loss makes the angle smaller and therefore improves symmetry, while still being significantly different from perfect symmetry. If reviewer nv6j is happy with these explanations, we will update Section 4 accordingly.\n___\n> *\u201cAlthough \u2018hEP\u2019 is the abbreviation of \u2018holomorphic Equilibrium propagation\u2019 can be understood after reading. However, this abbreviation has not been indicated in parenthesis when mentioning its full term for the first time.\u201d*\n\nThanks for catching this important detail, it has now been fixed.\n___\n> *\u201cThe last sentence \u2018It is worth nothing that\u2026\u2019, in Definition 1 of section 2.2, might be miswritten\u201d* \n\nThanks for reading carefully, the typo has been fixed. \n___\n> _\u201cthe sentence after Eq. (6) \u2018Importantly, the quantities \u2026, which applies only to EBMs.\u2019, is a little longer and complex. It might confuse readers, for convenience of understanding, it would be better to split it into two simpler sentences.\u201d_\n\nWe acknowledge that the sentence was a bit heavy, so we have separated it into two simpler sentences.\n___\n> *\u201cthe paragraph with Eq. (9) is intersected by Figure 2, which could be rearranged to provide enhanced readability. The same disadvantages happen in Figure 3 and Table 1.\u201d*\n\nWe thank reviewer nv6j for pointing these out, we have changed the position of the Figure 2 one page earlier so that it does not intersect the paragraph with Eq. (9). We also move Figure 3 and Table 1 so that it does not intersect the paragraph.\n___\n> *\u201cThe performance of the proposed method is relatively not so advanced.\u201d*\n\nWe agree that our method is tailored to EP and dynamical networks, for which the performance is behind state-of-the-art approaches that use other architectures. We reworked our introduction (in blue in the updated pdf) to justify our approach of focusing on EP primarily because it is a promising algorithm for efficient learning hardware. Despite lower performance at the moment, we believe that the results we present are necessary to further improve the performance of EP approaches in the future."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699995642914,
                "cdate": 1699995642914,
                "tmdate": 1699995861821,
                "mdate": 1699995861821,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HpCiizyzcQ",
            "forum": "kUveo5k1GF",
            "replyto": "kUveo5k1GF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5357/Reviewer_8uWs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5357/Reviewer_8uWs"
            ],
            "content": {
                "summary": {
                    "value": "_Disclaimer: I have reviewed this paper recently again at a different venue. It has since been revised slightly. Some of my comments still apply to this version and are copied here verbatim from my previous review._\n\nThe paper focuses on Equilbrium Propagation (EP), i.e. a learning algorithm for neural networks that was introduced relatively recently and aspires to be more biologically plausible and more suitable for analog hardware than back-propagation (BP). The theoretical version of EP relies on an infinitesimal perturbation (which is not feasible to achieve in physical settings) and on symmetric weights (which are also unrealistic in biology and constrain hardware design). The paper aims to understand the impact of each of these two issues separately. It achieves to do so in the \"holomorphic EP\" (hEP) setting, i.e. under the assumption of complex-valued neurons. The authors achieve this by extending the theory of hEP to asymmetric connectivity, where they show that it is possible to obtain exact estimates of the gradient despite finite perturbations, therefore suggesting that the bigger issue is weight asymmetry. They then proceed to tackle the problem of asymmetry by introducing a loss term that penalizes it directly, and they show experimentally that this term improves the performance of hEP significantly in asymmetrically-initialized networks, in certain visual tasks using a 4-layer convolutional network."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is mostly nicely written and clear. The theoretical contributions of the paper are not trivial, as they require a deep understanding of a very specific algorithm i.e. the holomorphic version of EP, as well as a degree of comfort with certain mathematical concepts that is rare among neural network practitioners and possibly even theoreticians. More generally, the paper aims to contribute to an area that is of broad interest, as it relates to machine learning, neuromorphic engineering, and theoretical neuroscience. Furthermore, it truly advances the empirical results of the EP-related literature."
                },
                "weaknesses": {
                    "value": "While this is obviously a valuable piece of work in certain respects, I believe it is also significantly limited in other key aspects.\n\nSignificance: The work ultimately aspires to improve aspects of backpropagation that are indeed important and relevant to multiple large disciplines (ML, neuromorphic hardware, neuroscience), however, concretely, the resulting contribution is very narrow. Namely, it improves the performance and the theoretical understanding of hEP somewhat, specifically in the case of asymmetric weights, but it doesn't resolve the problem of asymmetry completely, as can be seen in the performance comparison to symmetric weights (Table 2). Moreover, hEP is a specialized version of EP that makes additional assumptions for complex-valued networks, which limits the applicability and generality of the algorithm. Furthermore, EP itself more broadly is interesting but is a rather limited method in terms of achieving its goals of good performance, efficiency, useful hardware demonstrations, or deep learning, in comparison with other methods that have similar goals that have been more successful. Its biological compatibility could also be debated, given its requirement for multiple network-wide iterations before a weight update. Therefore, it could be argued that the significance of the results only relates to a very narrow subfield. Furthermore, the theoretical advance separating the impact of the finite nudge from that of the asymmetry in hEP is not trivial, but I wonder how useful it is to the ICLR community, beyond the very narrow sub-community that specializes in hEP. In the broader picture, and given the already existing better-working alternatives, the progress made here towards biological plausibility or neuromorphic computation seems small, in my view.\n\nNovelty: The main novelty of the paper is in the theoretical results, since the empirical fact that improving performance by dealing with asymmetric weights is not a new result. Nevertheless, I suspect that to someone with a good understanding of the earlier mathematical work that introduced hEP (Laborieux & Zenke, NeurIPS 2022) the new theoretical results might not be very surprising. However, it should be noted that I am not in a position to judge this fully. In any case, the insight from the theory is the impact of asymmetrical weights, relative to that of finite nudge, which seems to be also covered by the empirical result, therefore the added value from the theory in this case might not be substantial. Regarding the paper's empirical or practical contributions, some novelty exists in the objective function that penalizes asymmetries in the weight matrix. However, the novelty of this is limited because much older learning rules that achieve weight symmetry do exist (Kolen & Polak, IJCNN 1994; see also Payeur et al., Nat. Neurosci. 2021). In fact, these rules do not rely on global iterative equilibria, so the present paper's implementation of learnable weight symmetry could be characterized as a step-back in this regard. It should be recognize though that the new method is applicable when the connectivity is not reciprocal, whereas previous ones probably were not.\n\nContextualization in the literature: The paper does (now) cite some of the works that had similar aims, but only in passing, only in the discussion as opposed to the motivation section, discounts the better empirical results that the other works achieved, attributing this only to the lower simulation cost of the alternatives, and does not mention that some of these methods not only perform better in classification benchmarks, but also require fewer assumptions for compatibility with biology and for efficiency in learning hardware, e.g. by circumventing the need for backward passes of information completely.\nSome examples are: Payeur et al., Nat. Neurosci. 2021; Greedy et al., NeurIPS 2022; Mengye Ren et al., ICLR 2023; Journ\u00e9 et al., ICLR 2023.\n\nIt would be very helpful to the readers and the targeted research communities if the authors motivated their choice to focus specifically on EP as opposed to alternative methods that have similar goals, but don't have the same limitations. This could be a way to mitigate the weaknesses of the paper to some extent.\n\nTo be clear, some of the algorithm's limitations in comparison to alternatives are: reliance on complex-valued networks; constraints in network architecture and depth, expensive simulations for the equilibrium dynamics (e.g. a multi-GPU cluster was used according to the supplementary material, despite the small networks and simple tasks); questionable bio-plausibility of the necessity for equilibrium; hardware implementations of EP are mostly theoretical. To their credit, the authors have mentioned some of these limitations in the discussion. They also provided some solutions or counterarguments, however these are largely theoretical, vague, or speculative.\n\nAll in all, I believe that the work's value might be able to increase if the manuscript could explain its motivations and its contributions in a context broader than the EP literature. At present, this is not clear enough or supported well enough, in my view."
                },
                "questions": {
                    "value": "The discussion suggests that \"analog substrates could achieve this relaxation \u201cfor free\u201d through device physics (Yi et al., 2023; Kendall et al., 2020)\". Could the authors please clarify, does any analog substrate achieve the relaxation \"for free\", or what are the requirements, more specifically? For example, the Kendall et al. reference seems to rely on a very peculiar and ad hoc hardware implementation.\n\nAlso, in the most efficient hypothetical hardware implementation, I suspect that \"for free\" is far from the truth, as, even there, the relaxation phase would include multiple weight updates, which consume power; in fact commonly more power than weight reads. Wouldn't alternative algorithms that do not have this relaxation phase be significantly more efficient? The provided references do not seem to disagree with this, so in which sense do the references support the \"free\" claim?\n\nI would appreciate the authors' insight in these questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5357/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5357/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5357/Reviewer_8uWs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5357/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785056676,
            "cdate": 1698785056676,
            "tmdate": 1700651748821,
            "mdate": 1700651748821,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Tus8g1VL59",
                "forum": "kUveo5k1GF",
                "replyto": "HpCiizyzcQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5357/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Individual response to reviewer 8uWs (1/2)"
                    },
                    "comment": {
                        "value": "We thank reviewer 8uWs for having enough interest in our work to review it for the second time. We are also thankful for the positive comments about the writing, the high mathematical standards, as well as the value of our work. Below we reply point by point to the points raised by reviewer 8uWs: \n___\n> \u201c_[...] the resulting contribution is very narrow. [...] hEP is a specialized version of EP that makes additional assumptions for complex-valued networks [...]  the results only relates to a very narrow subfield [...] I wonder how useful it is to the ICLR community, beyond the very narrow sub-community that specializes in hEP_\u201d\n\nWe still respectfully disagree, and believe that our results are of interest for all the broader community studying alternative learning algorithms. For instance, we see at least three subfields different from EP, and represented at ICLR and similar conferences, for which our results are interesting:\n1. Predictive coding and predictive coding networks, which are another kind of EBM often trained with algorithms related to EP (e.g. Millidge et al ICLR 2023)\n2. Deep equilibrium models, which also rely on iterative equilibria for different purposes as EP, but with transferable insights (e.g. Bai, Koltun & Kolter ICLR 2021)\n3. Target propagation, which is interested in mechanisms for improving symmetry (Meuleumans et al NeurIPS 2020, 2021, 2022)\n___\n> *\u201cIt would be very helpful to the readers and the targeted research communities if the authors motivated their choice to focus specifically on EP as opposed to alternative methods that have similar goals, but don't have the same limitations. [...] I believe that the work's value might be able to increase if the manuscript could explain its motivations and its contributions in a context broader than the EP literature.\u201d*\n\nWe focus on EP for two main reasons:\n\n1. Because of its potential to reduce the energy cost of AI training when running on a specific hardware substrate (such as already demonstrated in Yi et al 2023), which is a broader subject than the EP literature. \n2. Because EP is also a promising model for modeling learning in the brain (local learning rule, oscillations), though we do concede that a concrete mapping is still missing to make this statement compelling, which we acknowledge in the discussion.\n\nFor this reason, we choose to focus on EP and reduce the impact of asymmetry and finite nudge on its performance, rather than systematically comparing it to other methods. We believe it is fair since we do demonstrate non trivial performance compared to BP for similar architectures, and on challenging tasks such as ImageNet 32.\n\n**we have updated the introduction to highlight that EP running on dedicated hardware, as demonstrated in Yi et al 2023, can reduce the training cost of AI training by four orders of magnitude**. We believe that this statement places our work in the context of reducing AI\u2019s energy footprint, which is broader than EP itself, and also constitutes the main reason why we choose to focus on EP.\n___\n> *\u201cThe paper does (now) cite some of the works that had similar aims, but only in passing, only in the discussion as opposed to the motivation section, discounts the better empirical results that the other works achieved, attributing this only to the lower simulation cost of the alternatives, and does not mention that some of these methods not only perform better in classification benchmarks, but also require fewer assumptions for compatibility with biology and for efficiency in learning hardware, e.g. by circumventing the need for backward passes of information completely.\u201d*\n\nApology, by no means did we mean to discount other methods with our explanation for EP\u2019s lower performance, and are sorry if it was interpreted this way. Our reasoning was that since EP has a theoretical link with gradients computed by BP, it makes little doubt that higher performances with EP can be achieved with better engineering, which is corroborated by this very recent paper [r1] that demonstrated better runtime and performance for EP. **We have now removed this argument, and updated the introduction to mention explicitly that other bio plausible alternatives well-suited for learning hardware, do perform better than EP**. \n\n[r1] Scellier et al *\u201cEnergy-based learning algorithms for analog computing: a comparative study\u201d* NeurIPS 2023"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699995371082,
                "cdate": 1699995371082,
                "tmdate": 1699995371082,
                "mdate": 1699995371082,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lVFbyOOea6",
                "forum": "kUveo5k1GF",
                "replyto": "HpCiizyzcQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5357/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Individual response to reviewer 8uWs (2/2)"
                    },
                    "comment": {
                        "value": "> *\u201c[...] it doesn't resolve the problem of asymmetry completely [...] the empirical fact that improving performance by dealing with asymmetric weights is not a new result. [...] the new theoretical results might not be very surprising [...] It should be recognized though that the new method is applicable when the connectivity is not reciprocal, whereas previous ones probably were not.\u201d*\n\nWe stress again that our claim is to mitigate the effect of asymmetry in the context of EP, rather than solving it completely. We still do believe that our way of improving symmetry for EP is novel and surprising, since it does not simply amount to making forward and backward weights identical, like in earlier solutions such as Kolen & Polack. We thank reviewer 8uWs for acknowledging that. Finally, because our new method is general, it can transfer to other subfields represented at ICLR (enumerated above).\n___\n> *\u201cCould the authors please clarify, does any analog substrate achieve the relaxation \"for free\", or what are the requirements, more specifically? For example, the Kendall et al. reference seems to rely on a very peculiar and ad hoc hardware implementation. [...] even there, the relaxation phase would include multiple weight updates\u201d*\n\nSorry for the confusion. By \u201cfor free\u201d, we meant that the laws of physics bring the substrate to equilibrium, such that the computation need not be implemented explicitly as in traditional von Neumann hardware. In Kendall et al, this is done with a nonlinear resistive network as a physical substrate, and the corresponding laws of physics responsible for doing the computation are the Kirchhoff\u2019s laws. In particular, no weight update is required for the relaxation phase (see paragraph \u201cPerforming inference\u201d in Kendall et al). The fact that their hardware implementation is very peculiar and hoc is on purpose. The idea is to co-design a hardware dedicated to a specific training algorithm (EP), and not a general purpose hardware. We regret the poor choice of words \u201cfor free\u201d and **have removed it from the discussion.**\n___\n> *\u201cWouldn't alternative algorithms that do not have this relaxation phase be significantly more efficient?\u201d*\n\nGood question. The question of efficiency is not only a question of algorithm, but also of hardware substrate on which it runs. We are not saying that EP will be more efficient than other algorithms on conventional computers, we are saying that EP is well suited to be implemented on a dedicated hardware substrate, such as Yi et al 2023, because computation can be carried out efficiently with physical laws, unlike alternative methods, to the best of our knowledge. Owing to these advantages, it can be more efficient than other algorithms running on conventional computers (cf Yi et al 2023), but we agree that this prospect is somewhat remote for now. Nonetheless, it is still a research direction worth pursuing and being represented at ICLR.\n___\n> *\u201cTo be clear, some of the algorithm's limitations in comparison to alternatives are: [...] multi-GPU cluster was used according to the supplementary material, despite the small networks and simple tasks [...] To their credit, the authors have mentioned some of these limitations in the discussion. They also provided some solutions or counterarguments, however these are largely theoretical, vague, or speculative.\u201d*\n\nWe are sorry that reviewer 8uWs is not convinced by our arguments. However, we do mention most of these limitations in the discussion: \n- Complex numbers: \u201coscillate in the complex plane, whose physical interpretation is not straightforward in neurobiology\u201d\n- Iterative equilibria: \u201cAnother limitation of EP is that it requires convergence to an equilibrium.\u201d\n- Costly simulations: \u201cThis requirement makes EP costly on digital computers.\u201d\n\nAs for the network depth, and unless we are mistaken, it is also not significantly shallower than Journ\u00e9 et al and Payeur et al who test on five or so layers CNNs.\n\nAbout task simplicity, we want to stress again that we do test on ImageNet 32, which does not qualify as a \u201csimple task\u201d, and is not significantly less complex than standard ImageNet, since only the resolution changes, not the number of datapoints or classes."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699995542722,
                "cdate": 1699995542722,
                "tmdate": 1699995542722,
                "mdate": 1699995542722,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PgGGEnleym",
                "forum": "kUveo5k1GF",
                "replyto": "Tus8g1VL59",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5357/Reviewer_8uWs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5357/Reviewer_8uWs"
                ],
                "content": {
                    "comment": {
                        "value": "> our results are of interest for all the broader community studying alternative learning algorithms\n\nMy understanding is that the results don't even hold for EP itself, but rather only for the complex-number-reliant hEP algorithm. Is that so? In that case, how do the results help with even more distant algorithms? Can this contribution be concretized? Do this manuscript's theoretical results help in any way there? Has the empirical method for mitigating weight asymmetry been validated with any other algorithm than hEP?\n\n\n> We focus on EP for two main reasons: [...] 1. reduce the energy cost of AI training [...] 2. for modeling learning in the brain\n\nThat is very clear. What is not clear is why these reasons led the authors to hEP and not to any of the other algorithms that are equally well - if not better - suited for aims 1. and 2. I am not necessarily suggesting that this cannot be justified. However, I am suggesting that this is not made clear at all in the paper, even though proper contextualization and motivation would demand it."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700428224786,
                "cdate": 1700428224786,
                "tmdate": 1700428224786,
                "mdate": 1700428224786,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v5MkUWWaeI",
                "forum": "kUveo5k1GF",
                "replyto": "lVFbyOOea6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5357/Reviewer_8uWs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5357/Reviewer_8uWs"
                ],
                "content": {
                    "comment": {
                        "value": "> Good question\n\nI would like to clarify that my question was indeed about dedicated hardware, where computation is carried out by physical laws, i.e. it can be complemente as follows: \u201cWouldn't alternative algorithms that do not have this relaxation phase be significantly more efficient [even on dedicated hardware]?\u201d"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700428684621,
                "cdate": 1700428684621,
                "tmdate": 1700428684621,
                "mdate": 1700428684621,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hdpDOvu7LI",
                "forum": "kUveo5k1GF",
                "replyto": "HpCiizyzcQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5357/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> *Wouldn't alternative algorithms that do not have this relaxation phase be significantly more efficient [even on dedicated hardware]?*\n\nIt is difficult to give a definite answer without further details on the specific algorithm and hardware. \n\nFor the sake of example, let's consider a crossbar hardware architecture which can do matrix multiplication with Kirchhoff's law, and compare both BP and EP. \n\nImplementing a traditional ANN with BP does not require a relaxation phase, but requires a record of the operations done in the forward pass to compute the backward pass, moreover, the backward pass in BP transmits the $\\delta$ directly, which is easily disrupted by the noise of the hardware due to its scale (see paragraph \"backpropagation or errors\" in [r1]). \n\nOn the other hand, EP performs implicit differentiation through the nudge, the details of the operation during the relaxation phase are not needed, because $\\delta$ would not be transmitted explicitly as in BP, but implicitly through the change of neural activity, which is agnostic to the hardware imperfections since the same dynamics is used.\n\nSo in this case, even though an algorithm such as BP has no relaxation phase, the details of the hardware and the requirement of the algorithm itself make it less suited than EP despite the relaxation phase [r2]. \n\nWe hope this example can convince reviewer 8uWs about the validity of researching EP-related algorithms.\n___\n**References**  \n[r1] Narayanan, Pritish, et al. \"Toward on-chip acceleration of the backpropagation algorithm using nonvolatile memory.\" IBM Journal of Research and Development 61.4/5 (2017): 11-1.  \n[r2] Yi, Su-in, et al. \"Activity-difference training of deep neural networks using memristor crossbars.\" Nature Electronics 6.1 (2023): 45-51."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700479268962,
                "cdate": 1700479268962,
                "tmdate": 1700479369768,
                "mdate": 1700479369768,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IXJKbwPKH5",
                "forum": "kUveo5k1GF",
                "replyto": "HpCiizyzcQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5357/Reviewer_8uWs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5357/Reviewer_8uWs"
                ],
                "content": {
                    "comment": {
                        "value": "In my perception, these responses evade my actual questions.\n\n- The authors still have not explained how the *theoretical* results can be extended to anything other than hEP, even EP.\nIf that is not possible, then my judgment remains that the theoretical contribution is very narrow. For completeness, I think that the *practical* result of partly dealing with asymmetry is somewhat useful but not particularly surprising, or especially impactful in light of prior methods that deal with asymmetry. The promised added experiments would add some value.\n\n- The authors do not seem to be able to motivate researchers of biological learning or of neuromorphic hardware to study EP as opposed to other alternatives to backpropagation.\n\nThe above are my main criticisms of the paper.\n\nThe below is merely a question seeking clarification, but the authors also evaded it.\n\n- The last reply by the authors compared with BP even though the question was about hardware-oriented *alternatives* to BP that do not have backward passes of any sort and do not have multiple iterations per training example. Again, it appears to me that such alternatives would be more efficient than EP, which requires multiple forward and backward passes and multiple weight update iterations for each training example."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575365860,
                "cdate": 1700575365860,
                "tmdate": 1700575608868,
                "mdate": 1700575608868,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Zk6HSPwlrg",
                "forum": "kUveo5k1GF",
                "replyto": "HpCiizyzcQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5357/Reviewer_8uWs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5357/Reviewer_8uWs"
                ],
                "content": {
                    "comment": {
                        "value": "- The clarification that Sections 3.1, 3.2, 3.3 could also apply to classic EP, not only hEP, is very helpful.\nIt is not clear in the paper that this is the case, or how this is the case, as these sections seem to only mention hEP.\nCan this be mitigated?\n\n- In a further attempt to help the authors motivate the manuscript, is any of those listed nice features of EP unique to EP? I do not think so. There are certainly unique disadvantages, as it has been agreed on in this discussion. Again, are there any unique advantages to EP (already listed or not)? If that can be specified, it would be very helpful.\n\n- Efficient Hebbian learning on chip has been demonstrated to a much larger extent than EP, in multiple occasions, and in multiplle renditions, for decades now. Here is an example from 1992: https://ieeexplore.ieee.org/document/173117\nHere is another example in a spiking implementation that also has a power estimate (4mW) https://www.frontiersin.org/articles/10.3389/fnins.2015.00141/full\nHere is an example with a memristive implementation https://www.nature.com/articles/s41467-018-04933-y\nOther local learning algorithms too. Here is a chip for e-prop consuming 50 \u00b5W https://ieeexplore.ieee.org/document/9731734\nThese are only examples, there are possibly hundreds of such publications.\nAgain, it appears that the authors have focused on EP without acknowledging its context."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582950091,
                "cdate": 1700582950091,
                "tmdate": 1700583015651,
                "mdate": 1700583015651,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W77j6uTTXj",
                "forum": "kUveo5k1GF",
                "replyto": "HpCiizyzcQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5357/Reviewer_8uWs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5357/Reviewer_8uWs"
                ],
                "content": {
                    "comment": {
                        "value": "> In particular, the Cauchy integral formulation of the error vector we present offers the potential to compute the error through nonlinear oscillations. We are not aware of any other algorithm that can compute a theoretically sound error (that is, linked to an actual loss gradient), by integrating nonlinear dynamics. Typically, many other approaches like classic EP, predictive coding networks, target prop use finite differences to compute the error, which means they need that the network varies only linearly with respect to its prediction. In hEP, the oscillations can vary significantly around the free fixed point, such that the nonlinearity of the network is harnessed rather than fought against, which is an important feature for dedicated hardware.\n\nThis is indeed an important advantage of hEP over other approaches that also rely on oscillations.\n\nI am now convinced that the paper is a useful contribution to the literature of oscillation-based algorithms, which is itself important.\nI strongly recommend that the paper's introduction and conclusions make clear that the motivation, the focus and the contributions are in this subfield, which is narrower than the broader field of biologically plausible learning theory and of neuromorphic learning algorithms. I have not been convinced that the broader disciplines have advanced significantly in this paper.\n\nCould this improvement be made?\nIf so, then along with the other improvements that the authors have committed to, I would raise my score.\nCould the authors please briefly recap the changes they will make?\n\n\n\nSecondarily, and for the completeness of our discussion:\n> While some of these features taken individually are maybe not unique to EP, we believe that the accumulation of all those features into a single algorithm is unique, to the best of our knowledge.\n\nThat is only true if the comparison is limited among oscillation-based networks. Again, other algorithms that do not have this requirement for multiple iterations have all these advantages and more as has been mentioned a few times during this discussion.\n\n> We understand that the work mentioned before as those outperforming EP (Payeur et al., Greedy et al., Mengye Ren et al. ; Journ\u00e9 et al.) have not been implemented on dedicated hardware yet, unlike EP (Yi et al).\n\nYi et al's implementation is memristor activity-difference energy minimization (MADEM) which is very closely related to EP, but not exactly the same. Similarly, among the multiple existing realizations of physical on-chip learning, some are very closely related to, for example, the Hebbian winner-take-all of Journ\u00e9 et al. that the authors just mentioned. E.g. Kreiser et al.'s https://ieeexplore.ieee.org/document/8325168 and https://www.nature.com/articles/s41467-018-04933-y."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595446585,
                "cdate": 1700595446585,
                "tmdate": 1700595616426,
                "mdate": 1700595616426,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UBweUQIt3f",
                "forum": "kUveo5k1GF",
                "replyto": "kUTSZsl9FP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5357/Reviewer_8uWs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5357/Reviewer_8uWs"
                ],
                "content": {
                    "title": {
                        "value": "Raising my score"
                    },
                    "comment": {
                        "value": "With these changes, assuming they are implemented properly, this will be one of the few papers in this subfield that argue entirely rigorously and honestly among many that often make very weakly supported strong claims about biology and AI hardware. This on its own makes the paper valuable.\n\nMoreover, the paper does offer interesting theoretical insight and useful practical results to the community of oscillation-based algorithms.\n\nBased on these, I have raised my score."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651991768,
                "cdate": 1700651991768,
                "tmdate": 1700651991768,
                "mdate": 1700651991768,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mbGGskEW3A",
            "forum": "kUveo5k1GF",
            "replyto": "kUveo5k1GF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5357/Reviewer_EQJi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5357/Reviewer_EQJi"
            ],
            "content": {
                "summary": {
                    "value": "Equilibrium propagation, an alternative to backprop requires weight symmetry and nudges to yield unbiased gradient estimates. Generalization of Equilibrium proportion to non-symmetric dynamical system exists but shown to work only for simple problems.  \nThe paper proposes an extension of holomorphic EP to non-symmetrical dynamical systems and shows good results across different vision benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Analysis of bias in gradient estimation is helpful to the reader. \n- Incorporating functional symmetry through the use of matching jacobians is an interesting idea. It\u2019s similar to reconstruction error term used in methods like Target Propagation. Here, authors optimize the homeostatic loss with respect to all the weights as compared to using only feedback weights."
                },
                "weaknesses": {
                    "value": "- The paper is generally well written, though introduction is a bit complex if the reader is not aware of previous work (holomorphic EP)."
                },
                "questions": {
                    "value": "No further questions as such. The paper has good results across different vision benchmarks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5357/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699166366582,
            "cdate": 1699166366582,
            "tmdate": 1699636539720,
            "mdate": 1699636539720,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sZzWHxVrGe",
                "forum": "kUveo5k1GF",
                "replyto": "mbGGskEW3A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5357/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Individual response to reviewer EQJi"
                    },
                    "comment": {
                        "value": "> *\u201cintroduction is a bit complex if the reader is not aware of previous work (holomorphic EP).\u201d*\n\nThanks for the feedback, we rewrote the sentence introducing holomorphic EP in the introduction to make it easier for the reader. It is highlighted in blue in the revised version.\n\n> *\u201cNo further questions as such. The paper has good results across different vision benchmarks.\u201d*\n\nWe thank reviewer EQJi for the positive comment."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699995359435,
                "cdate": 1699995359435,
                "tmdate": 1699995359435,
                "mdate": 1699995359435,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]