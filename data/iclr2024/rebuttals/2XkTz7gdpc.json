[
    {
        "title": "Efficient and Scalable Graph Generation by Spectrum Preserving Local Expansion"
    },
    {
        "review": {
            "id": "pZoCk53mY6",
            "forum": "2XkTz7gdpc",
            "replyto": "2XkTz7gdpc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3733/Reviewer_38QF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3733/Reviewer_38QF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a graph-generative model based on (1) iteratively expanding the coarse graph and (2) spectral conditioning. The authors demonstrate performance improvement over several baselines (GraphRNN GRAN, GDSS, Digress) on planar, SBM, tree, protein, and point cloud graphs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposes a new method for scalable graph generation, which is an important problem. The idea is sound and the empirical results are solid."
                },
                "weaknesses": {
                    "value": "### Comparison to existing scalable graph generative model (BiGG, GraphGen)\n\nThe authors do not compare their results with significant baselines for graph generation. Since the proposed method is designed for scalable graph generation, scalable baselines should be prioritized over others. \n\n\n[Dai et al., 2020] Scalable Deep Generative Modeling for Sparse Graphs\n\n[Goyal et al., 2022] GraphGen: A Scalable Approach to Domain-agnostic Labeled\n\n[Diamant et al., 2023] Improving Graph Generation by Restricting Graph Bandwidth\n\n### Novelty compared to coarsening-based graph generation. \nThere exists a work based on generating graphs in a coarse-graining manner. The authors should compare with such a work. \n\n[Guo et al., 2022] An Unpooling Layer for Graph Generation\n\n### Lack of graph visualization and additional metrics for extrapolation\nThe authors do not visualize the generated graph, which is quite important for verifying the validity of the proposed method, e.g., well-generated graphs are usually connected and indistinguishable from training data from the human eyes. \n\n\nFurthermore, the authors only check a fraction of valid and unique ratios for interpolation and extrapolation tasks. I suggest reporting additional MMD-based metrics. \n\n### Lack of real-world experiments \n\nAll the datasets considered in the experiments are (perhaps arguably) synthetic. For the completeness of the experiment, the authors could consider real-world experiments like molecule generation."
                },
                "questions": {
                    "value": "- I do not understand why the authors argue the V.U.N. metric is the most important metric. Could the authors elaborate on this point?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3733/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3733/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3733/Reviewer_38QF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3733/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698733567744,
            "cdate": 1698733567744,
            "tmdate": 1700738822495,
            "mdate": 1700738822495,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F61BvvHWvY",
                "forum": "2XkTz7gdpc",
                "replyto": "pZoCk53mY6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to reviewer 38QF"
                    },
                    "comment": {
                        "value": "Thank you for the time and effort you have invested in reviewing our work. We appreciate your constructive feedback and have revised the manuscript in response to your comments. In the following, we would like address the concerns and questions you have raised.\n\n\n## Regarding weaknesses 1 and 2:\nThank you for highlighting these references. We have expanded the related work section of our manuscript to include a discussion on the following publications:\n- Scalable Deep Generative Modeling for Sparse Graphs\n- Improving Graph Generation by Restricting Graph Bandwidth\n- GraphGen: A Scalable Approach to Domain-agnostic Labeled\n- An Unpooling Layer for Graph Generation\n\nFor the first two papers listed, we have carried out experiments using our datasets under consideration and have included the findings in the updated manuscript. We recognize the value of conducting similar experiments for the last two papers mentioned and plan to incorporate these results into the final draft of our paper.\n\n\n## Regarding weakness 3:\nGraph visualizations have been included in the supplementary material to demonstrate the quality of generation (Figures 7,8,9,10,11,12). As the MMD-based metrics correlate with our reported validity metrics, we have chosen to present only the latter for a more concise presentation.\n\n\n## Regarding weakness 4:\nProtein and point cloud graphs represent real-world structures by encoding neighborhood information similar to molecular graphs. Therefore, we consider them to be non-synthetic datasets\nThe reason we excluded molecular datasets from our evaluation is that they consist of smaller graphs with just a few dozen nodes, which do not align with our method's focus on efficiently generating larger graphs.\nWe are optimistic that larger datasets with more nodes per graph will become available, providing further opportunities to evaluate our method on larger graphs.\n\n\n## Regarding question 1:\nThe validity metric is essential as it signifies whether the model has successfully captured the defining attributes of the target graph category. A low score in uniqueness and novelty can be indicative of overfitting, which is particularly important to consider since some models may display low MMD metrics, potentially giving a false impression of high performance. In reality, these models might be replicating the training dataset too closely, failing to generate new, valid structures, which is arguably the main task of generative models. Consequently, we emphasize the V.U.N. (Validity, Uniqueness, Novelty) metric to ensure that our model not only excels statistically but also produces graphs that are novel and characteristic of the intended class. We appreciate you bringing up this issue. To address this, we have included an explanatory note in the manuscript."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602859127,
                "cdate": 1700602859127,
                "tmdate": 1700602859127,
                "mdate": 1700602859127,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Om8Q4AY1WB",
                "forum": "2XkTz7gdpc",
                "replyto": "F61BvvHWvY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3733/Reviewer_38QF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3733/Reviewer_38QF"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for incorporating my comments. I think the paper is better now. I am not entirely satisfied with the rebuttal. \nHowever, I believe that the authors are planning to further improve the paper once accepted. Hence, I am raising my score but decreasing my confidence (since my assessment is borderline).\n\nWeakness 1 & 2: \nI think the incorporation of the first two baselines (BiGG and BwR) is great, and implementing two additional ones is of less priority (although it would be better).\n\nHowever, it seems that the performance of BiGG is worse than GRAN for degree, cluster, and orbit MMD metrics for the proteins and the point cloud dataset. This is against the large performance improvement reported in the BiGG paper and it is a bit hard for me to believe that the authors correctly implemented BiGG. Furthermore, in Figure 6 of the appendix, it seems that the proposed method is slower than BiGG.\n\nWeakness 3: \nThank you for the visualization. I think it would be better to include comparison to the existing baselines too.\n\nWeakness 4: \nI do not think the proteins and the point clouds dataset sufficiently represent real-world problems since they are non-attributed graphs that simplify the real-world attributed objects. The authors could consider polymer datasets similar to HierVAE proposed by Jin et al., 2020."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738799473,
                "cdate": 1700738799473,
                "tmdate": 1700738799473,
                "mdate": 1700738799473,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1YLfHZYQyp",
                "forum": "2XkTz7gdpc",
                "replyto": "pZoCk53mY6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further clarification"
                    },
                    "comment": {
                        "value": "Thank you for your response and additional suggestions.\n\nIt is true that the performance of BiGG is slightly worse than the performance indicated in the original paper. We utilized the official implementation (https://github.com/google-research/google-research/tree/master/bigg), and for both the protein and point cloud datasets, we applied the same hyperparameters as those documented in the original study. The models were trained until the validation loss stabilized, after which we reported the performance based on the test set. For the final version of the paper we will test what happens when we train them longer. We used a different training split though, as described in the paper, which may explain the difference in performance\nOur method is slower than BiGG by a constant factor. The primary objective of the comparison was to demonstrate the asymptotic scaling behavior to support our theoretical analysis, wherein our method exhibits similarities to BiGG. As we aim to improve diffusion models, which provide state-of-the-art generation quality for smaller graphs, but have very poor asymptotics that prevents their use for larger graphs. We successfully remedy this. \n\nIt is indeed correct that the protein and point cloud datasets do not constitute attributed graphs. Our current methodology is not equipped to generate attributed graphs; rather, we focus solely on constructing the topological structure. We appreciate your suggestion to test our model on polymers. This would certainly be a valuable extension."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740816165,
                "cdate": 1700740816165,
                "tmdate": 1700741666121,
                "mdate": 1700741666121,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dFHwVGGBOa",
            "forum": "2XkTz7gdpc",
            "replyto": "2XkTz7gdpc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3733/Reviewer_HbqV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3733/Reviewer_HbqV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel graph generation method via sequentially expansion. To achieve this, the authors sequentially coarse a graph via graph partition to obtain a sequence of intermediate graphs towards a single-node graph. The graph generation is basically reversing this process, starting from a single node graph and gradually do expansion and refinement towards the original full graph. What's more, each step of the graph refinement for reverting graph coarsen is modeled with denoising diffuison model, while all steps share the same parameters of denoising diffusion model. To establish and concretize this graph coarsen & graph expansion idea, the authors also study and optimize the orderings of coarsen and expansion (spectrum preserving graph coarsen), the underlying diffusion model used (SDE with pre-conditioning and self-conditioning), and also the network architecture (local PPGN to utilize sparsity while retain certain expressivity). The authors demonstrate the improved performance on several (small) plain-graph datasets, and two datasets with larger graphs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The designed method is novel. Graph generation via reverting graph coarsen is a promising and interesting approach.\n2. The development and concretization of the proposed idea is comprehensive. The authors discuss three additional design space around the idea: the graph coarsening ordering, the underlying diffusion model that utilizes newest improvement from literature, the local PPGN architecture to tradeoff expressivity with computational cost. \n3. The presentation in general is easy to follow, with minor problems that I will point out later. \n4. The authors designed an experiment of testing extrapolation and interpolation shows the great strength of sequentially expansion over one-shot generation, which is interesting and shows that the designed method is promising. \n5. The authors provide detailed configuration of hyperparameters, good for reader to check. \n6. I found that the spectrum preserving coarsen and spectrum guided generation are a good design, this constraints all intermediate graphs during the graph expansion process to be inside a family of graphs that having similar spectrum information. This probably leads to the improved extrapolation and interpolation result."
                },
                "weaknesses": {
                    "value": "1. While being comprehensive that touches many design spaces, I personally found that it's a bit harder to know where the improvement of performance comes from. It would be great if the author can provide more detailed ablation study to guide the audience the importance of each design choice. \n2. For designed method depends on random orderings of graph coarsen and the reverse ordering of graph expansion. One shortcoming is that the designed method can be sensitive to orderings. The second shortcoming is that the method may needs a longer training period to converge comparing with one-shot generation, and I wish the author to discuss this. \n3. To follow up the previous shortcoming, I have noticed that the author only test the designed method over datasets with really limited number of graphs. I concern whether the training time increased due to the sequential coarsen and randomness of coarsen ordering can be too large for a dataset with large number of graphs. For example, typical molecule datasets can have millions of graphs inside. If the designed method has too much improved training cost, then it is not applicable to these real-world cases. Hence, I strongly suggest the author introduce additional datasets instead of just these small datasets. (small = limited number of graphs in the dataset) \n4. Decomposition of joint distribution, cannot be sum over all conditional likelihood. Instead should sum over P(G|order) x P(order). Similar issue inside equation 2, equation 3. These parts need a revision. \n5. The equation after eq.1 combines two independent distributions together, which uses less information to predict v_l, and can lead to error. This is because that edges being refined can be very important for identifying which node is expected to expand. \n6. The equation above (3) is essentially importance sampling, which needs the condition that support of distribution is covered all possible ordering (prob >0 for all orderings)."
                },
                "questions": {
                    "value": "1. The definition of edge contraction and neighborhood contraction sets are unclear. How does each element inside the set translate to an order of coarsening? \n2. Can you explain how do you achieved batching of graphs when considering randomness of graph coarsening? And how do you batch different steps of graph expansion together? I'm concerning whether this leads to increased computation burden comparing with one-shot generation. \n3. When you use eigenvectors, why use SignNet? It is sign invariant representation, should be bad for edge prediction task. It should only be good for graph-level tasks. \n4. Can you provide more discussion over the choice of ordering of graph coarsen and graph expansion? From equation 1-3 they seem to play an important role. From your experimental result, how sensitive is the the ordering for generation quality? \n5. Can you provide result on molecular datasets? At least a relative small one QM9 should be considered.\n6. For one-shot generation, you have set the number of inference steps to be 1000, while for you each intermediate step needs 256 inference steps. Can you provide the total inference steps you used? Are this significantly larger than 1000? In the meantime, a fair comparison should use relatively same number of steps."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3733/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804866933,
            "cdate": 1698804866933,
            "tmdate": 1699636329427,
            "mdate": 1699636329427,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jliHqvRXnu",
                "forum": "2XkTz7gdpc",
                "replyto": "dFHwVGGBOa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer 1/2 to reviewer HbqV"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review and constructive feedback. We appreciate your time and effort in evaluating our work. \n\nWe would like to address your concerns and questions in the following:\n\n## Regarding weakness 1:\nIn response to the difficulty in discerning the source of performance improvement due to multiple design choices, we have included further commentary in the main text to elucidate the significance of each design choice.\nTo avoid misconceptions about the role of spectrum preservation, we will amend the title to \"Iterative Local Expansion\" in the camera-ready version (per ICLR guidelines). These changes aim to provide a clearer insight into the importance and contribution of individual design elements.\n\n\n## Regarding weaknesses 2 and 3 as well as questions 4 and 5:\nThank you for your attention to detail. Let us elaborate on the concerns mentioned, as this provides a good opportunity to clarify some aspects of our method and highlight its strengths.\n\nUtilizing a fixed random coarsening sequence for each graph in the dataset indeed introduces sensitivity to the chosen ordering, which can lead to overfitting when the training set is small. Consequently, we adopted a strategy of sampling different coarsening sequences during model training. This is reflected in the formulation of our method, where importance sampling is employed instead of maximizing the likelihood of a single coarsening sequence, in contrast to other hierarchical graph generation approaches.\n\nAlthough random coarsening can perform quite well, the spectrum-preserving coarsening proposed reduces the variability of the coarsening sequence and aids in increasing the convergence speed and generative performance.\n\nAs stated in the paper, we limited the maximum training time for all experiments to 48 hours. Typically, our method achieves faster convergence compared to baseline one-shot methods due to its efficiency.\n\nRegarding the concern about convergence speed on larger datasets, we experimented by expanding the synthetic planar dataset tenfold and observed no substantial change in the loss convergence rate. However, the generative performance improved, with all generated graphs being valid planar graphs. This improvement is likely because the training data in this case provides better coverage of the ground-truth distribution.\n\nThe robust performance on small synthetic datasets underscores the promise of our method. This is particularly noteworthy because several autoregressive methods tend to overfit these datasets and fail to generate valid new graphs.\n\nThe reason we excluded molecular datasets from our evaluation is that they consist of smaller graphs with just a few dozen nodes, which do not align with our method's focus on efficiently generating larger graphs.\nWe are optimistic that larger datasets with more nodes per graph will become available, providing further opportunities to evaluate our method on larger graphs.\n\n\n## Regarding weakness 4:\nYou are correct in your observation that the marginalization of the likelihood of a graph was incorrectly expressed. This has been corrected in the revised version. Thank you for pointing this out.\n\n\n## Regarding weakness 5:\nNode expansion indeed depends on the preceding refinement step; that is, $\\mathbf{v_l}$ is dependent on $\\mathbf{e_l}$. However, as we model a joint distribution $p(\\mathbf{v_l}, \\mathbf{e_l} | \\tilde{G}_l)$, this dependence is captured. The assumption we make is that $\\mathbf{v_l}$ is conditionally independent of the expanded graph $\\tilde{G}_l$ given the refined graph $G_l$.\n\n\n## Regarding weakness 6:\nYou are correct in your observation that the equality in the equation above (3) is essentially importance sampling and requires $q(\\pi|G)>0$ when $p(\\pi)>0$. In scenarios where a random cost function is employed during the sampling of coarsening sequences, it holds that the distribution $q$ supports all possible coarsening sequences. With a deterministic cost function, the introduction of the randomization parameter $\\lambda$ within the \"Randomized Greedy Min-Cost Partitioning\" algorithm is intended to ensure a similar support for $q$. However, our primary focus is on employing the variational interpretation to derive a valid lower bound for training purposes rather than evaluating likelihoods. Therefore, to remain general and keep the paper simple, we replace the equality in the equation above (3) with an inequality. This makes our lower bound applicable even in the absence of full support from $q$. Consequently, we have not included a formal argument and refrain from asserting the use of importance sampling in our methodology."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602687063,
                "cdate": 1700602687063,
                "tmdate": 1700688519590,
                "mdate": 1700688519590,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qCEbezv22u",
                "forum": "2XkTz7gdpc",
                "replyto": "yu7MAWnQtV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3733/Reviewer_HbqV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3733/Reviewer_HbqV"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the detailed response"
                    },
                    "comment": {
                        "value": "I thank the author's response. Some of my questions are partially cleared. However I still want to see more experimental result regarding ablation study and molecular datasets. At least this method should give comparable result and is runnable for these datasets."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682086820,
                "cdate": 1700682086820,
                "tmdate": 1700682086820,
                "mdate": 1700682086820,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cqTdJUua4Y",
            "forum": "2XkTz7gdpc",
            "replyto": "2XkTz7gdpc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3733/Reviewer_mLXw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3733/Reviewer_mLXw"
            ],
            "content": {
                "summary": {
                    "value": "The paper describes a generative method for graphs. The generative procedure is cast in terms of reversing a graph hierarchical coarsening algorithm that groups nodes of the original graph in a hierarchical way to approximately preserve the spectra of the graph.\nThe authors propose to model the un-coarsening as a sequence of expansion - in which each node is expanded to a clique of a sampled number of nodes - and refinement steps. This later is tasked to remove edges from the expanded graph.\nBoth expansion and refinement are modeled in a Bayesian framework, where the joint distribution of the number of expansion nodes (for the next step) and edges to keep are conditioned on the previous graph in the hierarchical un-coarsening process.\nSampling from this distribution is performed through a denoising diffusion algorithm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper provides a rigorous formulation (even if not always easily accessible) of graph generation in terms of graph uncoarsening in a probabilistic framework. The experimental section shows that the proposed method achieves results comparable (or better) to state-of-the-art graph generative models in standard datasets and shows that the method is able to deal with datasets with graphs of thousands of nodes."
                },
                "weaknesses": {
                    "value": "The main limitation of the problem is that it is not easy to follow. It somehow lacks a level of abstraction that allows the reader to understand the individual parts of the method in an \u201cintuitive\u201d way before going into the details. You really need to carefully read and understand each bit of the 3.1 and 3.2 to understand where the method is headed.\n\nThe second weakness is the description and discussion of the local spectral preservation. Being the main focus of the title, I was expecting this to be one of the most important contributions of the paper. Unfortunately, it is briefly described in one paragraph in the main paper, and a small ablation experiment on this component is only provided in the appendix. \nIn general, I think that an ablation study on the most important design (e.g.  Spectral Conditioning) choices performed by the authors is missing and does not allow one to understand their impact.\n\nI also missed the presence of a brief paragraph positioning the paper with respect of the SOTA and highlighting its contributions."
                },
                "questions": {
                    "value": "There are a few things that are not clear in the method description:\n- at the end of page 4 it is mentioned that you have to model a probability distribution over node and edge features. Why features? aren\u2019t v and e the number of nodes distribution and edge selection vector?\n- In the probabilistic model description, isn\u2019t a graph G unequivocally defined by an expansion sequence? That is, given an expansion sequence, shouldn\u2019t be the probability of a graph G either 1 or 0?\n- I would include some high-level details on how you model p(v_t,e_t | G_t) as a diffusion process. For instance, how are v and e modeled? Is e an adjacency matrix? Do you need to solve an inverse diffusion process for each expansion step?\n- Related to the previous point, if you need to sample through the denoising diffusion process, how does this impact the sampling time?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3733/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699044736133,
            "cdate": 1699044736133,
            "tmdate": 1699636329352,
            "mdate": 1699636329352,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PABKHivcVe",
                "forum": "2XkTz7gdpc",
                "replyto": "cqTdJUua4Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to reviewer mLXw"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review and constructive feedback. \nAlthough our paper provides a high-level summary and visual aids, we understand that technical details can be difficult to follow. We hope that the reworked algorithms in the appendix will help clarify the method. \n\nIn response to your comments on local spectral preservation, we concur that it does not represent the primary focus of our work. As such, we will amend the title to \"Iterative Local Expansion\" in the camera-ready version (per ICLR guidelines), which more accurately encapsulates the essence of our method. While spectrum-preserving graph coarsening does contribute to the method's enhanced performance, it is not a critical component.\n\nFurthermore, we have expanded the related work section to provide a clearer context within the current state-of-the-art landscape, thereby highlighting our contributions more effectively. We have also enriched the experimental section with additional comparisons to underscore the method's efficacy and impact.\n\n\nIn the following, we would like to give answers to your specific questions:\n\n## Regarding questions 1 and 3:\nThe expansion vector records the size of the expansion cluster for each node in the graph $\\tilde{G_t}$, whereas the refinement vector specifies whether each edge of this graph should be kept or removed in the refinement step. Consequently, we represent these two vectors as node and edge features of the graph $\\tilde{G_t}$. This is why we refer to them as features. We have clarified this in the text. Thank you for pointing this out.\n\n\n## Regarding question 2:\nThe marginalization of the likelihood of a graph has been corrected in the revised version. You are correct in your observation a graph is defined by an expansion sequence resulting in it. Thank you for raising this concern.\n\n\n## Regarding question 4:\nWe discretize the diffusion process into 256 steps. The second-order method employed necessitates two gradient evaluations per step, leading to a total of 512 model evaluations for each sample. Although this procedure multiplies the sampling time by a constant factor, it does not alter the asymptotic complexity.\nA comment stating this has been added to Section 3.3 in the revised manuscript."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601536694,
                "cdate": 1700601536694,
                "tmdate": 1700688338177,
                "mdate": 1700688338177,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TIsDQ9DcaZ",
                "forum": "2XkTz7gdpc",
                "replyto": "PABKHivcVe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3733/Reviewer_mLXw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3733/Reviewer_mLXw"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifications.\nI will have a closer look at the modification to the manuscript in the next few days."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652279717,
                "cdate": 1700652279717,
                "tmdate": 1700652279717,
                "mdate": 1700652279717,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gknJUDyMQt",
            "forum": "2XkTz7gdpc",
            "replyto": "2XkTz7gdpc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3733/Reviewer_sNQQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3733/Reviewer_sNQQ"
            ],
            "content": {
                "summary": {
                    "value": "In this study, the authors introduce a graph generative model that iteratively expands and refine a coarse graph into a larger graph. This model learns to invert a graph coarsening process by training a denoising diffusion model. To overcome the complexity problem of the available GNN models, a local GNN model based on the PPGN model is developed that concentrates on the local sub-graph expansion."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) Multi-scale graph generation from a coarse graph to fine details, adopted in this work, is important to learn the complex graphs which can also capture global structure and local details of the graphs. It is used as a key part to design efficient and scalable graph generative models. \n\n2) It designed an efficient local version of the PPGN model for parameterizing the local expansion and refinement model."
                },
                "weaknesses": {
                    "value": "1) While the paper introduces a hierarchical and multiscale graph generation approach, it fails to acknowledge and compare it to previous methods [1, 2, 6] with similar objectives. For instance, HiGen [2] employs a hierarchical generation scheme, iteratively un-coarsing graphs and generating subgraphs in a coarse-to-fine manner with state-of-the-art quality. A hierarchical normalizing flow model for molecular graphs is also ignored [1], where new molecular graphs are generated by inverting edge contraction coarsening and duplicating node attributes after each expansion.\n\t\t\t\t\n2) The method maintains a  constant  number of eigenvalues $k$ (principal Laplacian spectrum) in the expansion process, which means that a fixed global structure of rank $k$ is used while the graph size is growing. On the other hand, since a local PPGN focuses on the local dense subgraph, therefore such *spectrum preserving local expansion* method may not capture some details that lie in the gap between the principal Laplacian spectrum (fixed global structure of the graph) and the local subgraphs. This can limit the model expressivity for larger graphs where this gap is larger.\n\n3) It lacks a comprehensive complexity analysis, despite claiming sub-quadratic complexity. It is essential to compare the sampling times to recent scalable graph generation models. Furthermore, there is no visualization of the generated graphs, which is crucial for assessing the quality of the model's output.\n\n4) The paper falls short in evaluating the generation quality by not comparing it to recent methods such as HiGen [2], GraphARM [3], and EDGE [4]. In particular, the performance on the Tree dataset is only compared against two benchmarks. It is also recommended to include the evaluation of the proposed model using random GNNs based on [5] as an alternative evaluation method to provide a more comprehensive assessment.\n \n5) The paper's presentation requires improvement to enhance its readability. \n\n-  a) Notations can be confusing, with some instances of the same notation used for different terms. For instance, distinguishing the expansion vector $\\mathbf{v}$ from nodes using different notation and handling variations in symbols such as $\\mathit{\\Pi}$ and $\\Pi$ would be beneficial. Additionally, the dual use of $\\theta$ for representing both the distribution over coarsening sequences and model parameters in section 3.3 may lead to confusion. \n\n- b) Contraction family and contraction function are both denoted by $\\mathcal{F}(G)$ while the family need not be a function of input graphs. \n\n- c) The paper should be improved for self-containment and clarity. For example, \u201cwe found that the formulation proposed by Song et al. (2021), supplemented with enhancements from Karras et al. (2022), yielded the most promising results, similarly to the approach used in Yan et al. (2023)\u201d is very vague and one need to read the appendix. Also, a significant portion of the crucial model components is pushed to the appendix. \n\n- d) The paper employs the term \"autoregressive\" to describe the model, which might cause confusion. In section 2, it's mentioned that \"one-shot generative models have generally outperformed autoregressive ones.\" To avoid confusion, it's advisable to use more accurate terminology, such as \"recursive\" or \"iterative,\" to distinguish the model's nature.\n\n- e) The factorization mentioned at the bottom of page 4, based on the chain rule, should explicitly note the assumption of a Markov property in the expansion sequence, i.e., $P(G_{l-1} | G_l, G_{l+1}, \u2026, G_{L} ) = P(G_{l-1} | G_l)$. This additional clarification will help readers better understand the underlying assumptions.\n\n[1] Maksim Kuznetsov and Daniil Polykovskiy. Molgrow: A graph normalizing flow for hierarchical molecular generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 8226\u20138234, 2021.\n\n[2] M. Karami, \u201cHiGen: Hierarchical Graph Generative Networks\u201d, arXiv preprint arxiv:2305.19337 \n\n[3] Lingkai Kong, Jiaming Cui, Haotian Sun, Yuchen Zhuang, B. Aditya Prakash, and Chao Zhang. Autoregressive diffusion model for graph generation, 2023.\n\n[4] Xiaohui Chen, Jiaxing He, Xu Han, and Li-Ping Liu. Efficient and degree-guided graph generation via discrete diffusion modeling. arXiv preprint arXiv:2305.04111, 2023.\n\n[5] Rylee Thompson, Boris Knyazev, Elahe Ghalebi, Jungtaek Kim, and Graham W Taylor. On evaluation metrics for graph generative models. arXiv preprint arXiv:2201.09871, 2022.\n\n[6] Hamed Shirzad, Hossein Hajimirsadeghi, Amir H Abdi, and Greg Mori. Td-gen: Graph generation using tree decomposition. In International Conference on Artificial Intelligence and Statistics, pp. 5518\u20135537. PMLR, 2022."
                },
                "questions": {
                    "value": "1) It seems that the model is using two iterative processes: an outer process to expand a graph ($G_l -> G_{l-1}$) and an inner process that uses a denoising diffusion model at each $ l $? . It would be helpful to provide a clear explanation of these processes, especially in the context of using the one-shot method.\n\n2) Should the likelihood of a graph G presented before eq (1) be marginalized as:\n$P(G) = \\sum P(G|\\omega)p(\\omega)$? \n\nAlso, it seems more suitable to rather define it as marginalized over the coarsening sequences\n$P(G) = \\sum P(G|\\pi)p(\\pi)$\nAlso, in the first equation in variational interpretation, it is defined as marginalization over the a contraction family not all possible contraction sequences so it might be corrected by the upper bound:\n$P(G) \\ge \\sum_{\\pi in \\Pi_{F}} P(G|\\pi)p(\\pi)$\n\n3) Why do we need a multilevel coarsening scheme that ensures preservation of the Laplacian spectrum? Is this particular property a key to the model, or could other coarsening methods also be utilized which preserve global structures by explicitly preserving approximate Laplacian spectrum?\n\n4) In the footnote of page 5, an example is given for the expansion sequences that are not the reverse of any coarsening sequence. Are there other cases, or can the refinement step be restricted to ensure the refined graph remains connected?\n\n5) It would be valuable to elaborate on why this  added source of randomness in the expansion is beneficial to the model's performance.\n\n\n6) What was the range of $L$ in each dataset and what family of contractions did you use? Did you limit $L$? In the case $L=1$, is this method (one-shot) equivalent to one of the already available non-multiscale diffusion models in the literature such as Yan et al. (2023)?\n\n7) How the denoising process is focused on the local subgraphs details, did you only add noise to a subset of the edges at each diffusion step?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3733/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699050766027,
            "cdate": 1699050766027,
            "tmdate": 1699636329276,
            "mdate": 1699636329276,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LH2Frlum3N",
                "forum": "2XkTz7gdpc",
                "replyto": "gknJUDyMQt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer 1/2 to reviewer sNQQ"
                    },
                    "comment": {
                        "value": "Thank you for the time and effort you have invested in reviewing our work. We appreciate your constructive feedback and have revised the manuscript in response to your comments.\nIn the following, we would like to address the concerns and questions you have raised.\n\n\n## Regarding weaknesseses 1 and 4:\nWe appreciate your attention to these references. \nWe have revised our related work section to include a discussion of the cited publications, specifically addressing the following studies:\n- Molgrow: A graph normalizing flow for hierarchical molecular generation\n- HiGen: Hierarchical Graph Generative Networks\n- Autoregressive diffusion model for graph generation\n- Efficient and degree-guided graph generation via discrete diffusion modeling\n- Td-gen: Graph generation using tree decomposition\n\nFor the work \"Efficient and degree-guided graph generation via discrete diffusion modeling\", we have conducted experiments on the considered datasets and report the results in the revised manuscript. We recognize the value of conducting additional experiments on the other mentioned works and plan to incorporate these in the final version of the paper. Note that following the suggestion of Reviewer 38QF, we also conducted experiments with two other methods: \"Scalable Deep Generative Modeling for Sparse Graphs\" and \"Improving Graph Generation by Restricting Graph Bandwidth\".\n\nIn terms of employing random GNNs for evaluation, we recognize its potential significance. However, due to time limitations, we are currently unable to expand our experimental evaluations to include this measure. Nevertheless, we believe that the structural properties captured by the random GNN metric would likely show a correlation with the MMD values we have reported.\n\n\n## Regarding weaknesses 2:\nWe recognize your concerns regarding the potential limitations of our method, which arise from keeping the number of eigenvalues $k$ constant during the expansion process. However, it appears there might be a misunderstanding about how our model operates and the role of spectral conditioning.\n\nTo clarify, our Local PPGN model functions across the entire graph rather than on isolated local subgraphs, enabling it to capture global structural characteristics. The term \"local\" indicates that the model resembles the PPGN model within local dense subgraphs, which results in enhanced discriminative power in these areas compared to message-passing models. The expressiveness of our GNN at the global scale is at least the same as usual message-passing models. Spectral conditioning is not crucial for capturing global structural attributes; rather, its aim is to enhance the model's performance by providing useful node embeddings. We have revised the \"Spectral Conditioning\" section to more clearly explain this aspect.\n\n\n## Regarding weaknesses 3:\nIn response to your comment regarding the absence of a comprehensive complexity analysis, we have incorporated a new section in Appendix H. We clearly outline the assumptions that allow for the achievement of subquadratic complexity in relation to the number of nodes. This section also includes a new chart that illustrates empirical sampling speeds, benchmarked against alternative methods.\n\n\n## Regarding weaknesses 5:\nThank you for your detailed suggestions. We have revised the manuscript to address the concerns you highlighted.\nIn particular, we use \"q\" to denote the probability distribution over coarsening sequences, to avoid confusion with the model parameters.\n\nThe revised version avoids the term \"autoregressive\" to describe the method.\nFurthermore, it explicitly notes the Markov property assumption in the factorization of the joint distribution.\n\nWe have elucidated the representation of the expansion and refinement vectors $\\mathbf{v}_l,\\mathbf{e}_l$ as node and edge features, respectively, of the expanded graph $\\tilde{G}_l$, in response to the query raised by reviewer mLXw. Consequently, we maintain that boldface notation for these vectors is appropriate and have chosen to retain it.\n\nThe symbol $\\mathcal{F}(G)$ is exclusively used to denote the set of candidate contraction sets for a given graph $G$. While we recognize that this may not be the most conventional notation for contraction families, its meaning is unambiguous within the context provided.\n\nWe acknowledge that a considerable amount of methodological detail has been relegated to the appendix. Given the constraints of page limits, our intention was to succinctly present the core aspects of our methodology in the main text while offering comprehensive elaboration in the appendix."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601363440,
                "cdate": 1700601363440,
                "tmdate": 1700601363440,
                "mdate": 1700601363440,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QVwys62bZM",
                "forum": "2XkTz7gdpc",
                "replyto": "gknJUDyMQt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer 2/2 to reviewer sNQQ"
                    },
                    "comment": {
                        "value": "## Regarding question 1:\nYour observation regarding the model's use of two iterative processes is accurate. For each refinement and expansion step, we sample corresponding expansion and refinement vectors using denoising diffusion, which is an iterative process. We have updated Section 3.3 on denoising diffusion in the main text to more explicitly point this out.\n\n\n## Regarding question 2:\nYou are correct in your observation that the marginalization of the likelihood of a graph was incorrectly expressed. \nFurthermore, you are correct that the first equation under the variational interpretation section should be an inequality rather than an equality.\nBoth of these errors have been corrected in the revised version - thank you for pointing them out.\n\n\n## Regarding question 3\nOur method supports any coarsening scheme that operates by contracting connected subgraphs. \n\nCoarsening that preserves the Laplacian spectrum ensures that the resulting graphs share structural similarities with the original dataset graphs, which may ease the learning process. Our ablative studies indicate that such preservation can enhance model performance, when compared to random coarsening, albeit modestly. We invite further research to investigate alternative coarsening strategies within our framework.\n\n\n## Regarding question 4\nIndeed, there are additional scenarios. Consider the expansion of a triangular graph into a fully connected four-node graph, followed by an edge removal (refinement) that results in a line graph. This refined graph cannot revert to the original triangular structure through coarsening by contracting any connected subgraph. Generally, expansion and refinement may not be reversible when the refinement step eliminates edges in such a way that nodes originating from the same parent node no longer share a common candidate contraction set.\n\n\n## Regarding question 5\nThe incorporation of randomness in the expansion process primarily serves to mitigate overfitting, which is particularly advantageous for synthetic datasets comprising only 128 training samples. We added a comment to the \"Perturbed Expansion\" section to clarify this point.\n\n\n## Regarding question 6\nAs documented in the paper, edge contractions were utilized for all experiments. This approach limits the graph's maximum expansion factor to 2, which in turn means that L is upper bounded by \\log_2(n), where n represents the number of nodes in the graph. For the purpose of model training, coarsening sequences were constructed, with each step reducing the graph's size by a random fraction ranging from 10\\% to 30\\% (Details are provided in Algorithm 1). For instance, for the synthetic graphs of size 64, L ranges from 12 to 22.\n\nAddressing your question about the scenario in which the initial node is directly expanded to the target graph using a one-shot approach: Our method employs the same denoising diffusion framework as \"SwinGNN\" by Yan et al. However, while they apply a non-permutation invariant transformer architecture to graph edges, we utilize the introduced Local PPGN model\u2014which, in this case of a dense graph, would match the original PPGN model by Haggai et al.\n\n## Regarding question 7:\nIn the refinement step, we selectively remove some edges from the expanded graph $\\tilde{G}$. To do this, we sample a refinement vector using denoising diffusion. This vector is represented as edge features of the expanded graph $\\tilde{G}$, where each edge feature indicates whether the corresponding edge should be kept or removed during the refinement step. Consequently, noise is added to all edge features of the expanded graph during training."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601400015,
                "cdate": 1700601400015,
                "tmdate": 1700601400015,
                "mdate": 1700601400015,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9lpirihGaO",
                "forum": "2XkTz7gdpc",
                "replyto": "QVwys62bZM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3733/Reviewer_sNQQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3733/Reviewer_sNQQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response and clarifications. To help the reviewers' assessment, it would be beneficial to upload a diff version that highlights the changes with a different color in the supplement.\nI have a few minor concerns and suggestions:\n1. **Appendix, GRAPH NEURAL NETWORK ARCHITECTURE ABLATION STUDY:**\n   a. In comparing the performance of local PPGN against GINE, did you add spectral embedding as node embedding for GINE in a manner similar to local PPGN?\n   b. Considering the intention to update the paper title, is it necessary to refer to the GNN method as \"local PPGN\" when, as indicated in the appendix and your response, it functions as an \"edge-wise message-passing\" GNN? While it is mentioned that it is \"comparable to the PPGN architecture for a fully connected graph,\" this still requires further elaboration for readers.\n   c. Since there are other edge-wise message-passing GNNs in the literature, consider running an ablation study to compare against them. Additionally, a comparison with more powerful GNNs like transformer-based methods like GT (graph transformer models, as used in DiGress) or hybrid methods such as GraphGPS could further highlight the unique contributions of local PPGN.\n2. **Results of EDGE:**\n   The results for EDGE appear to be anomalous, showing very poor performance across all datasets. This raises skepticism about whether EDGE was trained and evaluated accurately during the rebuttal period.\n\nThe updates have notably improved the presentation and clarified certain aspects. I plan to read the revised version thoroughly and will consider updating my score after further discussion with other reviewers."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716482757,
                "cdate": 1700716482757,
                "tmdate": 1700716482757,
                "mdate": 1700716482757,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]