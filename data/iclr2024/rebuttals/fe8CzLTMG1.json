[
    {
        "title": "Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-Temporal Reasoning"
    },
    {
        "review": {
            "id": "IfdQxg1b4m",
            "forum": "fe8CzLTMG1",
            "replyto": "fe8CzLTMG1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6535/Reviewer_wB3J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6535/Reviewer_wB3J"
            ],
            "content": {
                "summary": {
                    "value": "Paper presents a new benchmark to evaluate the capacity of spatial-temporal reasoning of LLMs. The format is path planning, where given a 2D grid map with obstacles and targets (given as text), the LLM is required to produce a path plan that connects all the targets (optionally in a given order) and avoids all the obstacles. The LLM is also asked to predict whether the current path planning problem is unsolvable. The experiments cover an extensive range of topics including different LLMs, prompting methods, using in-context learning (zero-shot) vs, fine-tuning, IID vs. OOD tasks, etc."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "+The topic studied here is important, spatial-temporal reasoning is critical to more general intelligence, and as far as I know, there is not much evaluation on LLMs with a focus on this. I believe the research presented in this manuscript should be of interest to audiences not just from the LLM community, but reasoning and GOFAI as well.\n\n+The benchmark is well designed. It is simple and straightforward but gets right to the point of spatial-temporal reasoning(maybe a bit short on the temporal part, more on that lately).\n\n+The experiments are quite thorough, with rich numbers and details. From the LLM evaluation perspective, it covers most of the angles. I personally like the IID vs OOD part as it is less discussed before in the literature of LLM + reasoning and the results look quite promising as well."
                },
                "weaknesses": {
                    "value": "I have some concerns regarding the motivation, the results, and some technical details (which will be listed in the question section):\n\n-Can the author elaborate more on why the path planning task in this benchmark can be used to measure temporal reasoning? I understand there is a variant that requires reaching the goals in a pre-specified order, but this seems more of a spatial reasoning problem as the planned path ultimately unfolds into a 2D grid. How is this spatial-temporal? I need to admit I am not an expert it this but more of a curious reader and I am happy to learn more if there are kinds of literature/references about this.\n\n-As the results show, most of the models (both in-context learning and fine-tuned) are able to attain 75+ accuracy some many critical metrics, ex. success rate, optimality, feasibility, etc. If this is the case, it seems that LLMs have almost nailed this task, why is this benchmark still useful for building better LLMs? Maybe the unreachable accuracy is still quite challenging, but this is only a small portion of the proposed benchmark.\n\n-some references on LLM + planning are missing: [1-3]\n\n[1] DEPS: https://arxiv.org/abs/2302.01560\n\n[2] Plan4MC: https://arxiv.org/abs/2303.16563\n\n[3] GITM: https://arxiv.org/abs/2305.17144"
                },
                "questions": {
                    "value": "-In table 2, there are some results on OOD evaluation with in-context learning. Can the authors clarify the exact settings of this? What are the data used to fine-tune the models, and what are the in-context examples when performing the OOD evaluations?\n\n-What are the exact prompts used in the task? Specifically, what is the prompt for predicting whether the goal is reachable?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6535/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698592381100,
            "cdate": 1698592381100,
            "tmdate": 1699636735897,
            "mdate": 1699636735897,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RDAKiC3Mwb",
                "forum": "fe8CzLTMG1",
                "replyto": "IfdQxg1b4m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6535/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6535/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer wB3J"
                    },
                    "comment": {
                        "value": "We thank the reviewer for spending time reviewing our work and acknowledging our contributions to the discussion of IID vs. OOD!\n\nRegarding \u201cwhy the path planning task in this benchmark can be used to measure temporal reasoning\u201d and the challenge of our benchmark, please refer to \u201cGeneral Responses to All Reviewers\u201d.\n\n**Missing references**\nWe thank the reviewer for suggesting the missing references! We apologize for the oversight and missing these references; we will make sure to add them to our updated draft. \n\nLooking into the suggested papers, they all revolve around planning in a minecraft environment. Below, we briefly discuss their connect to our work. \n[1] proposed an interactive approach for open-world planning based on LLMs allowing error correction through self-explanation. This, indeed is relevant to our work as we find substantial improvements on our benchmark by incorporating \u201cinteractive\u201d environment feedback (i.e., the ReAct experiments).\n\n[2] relied on an approach where LLMs are used for skill planning. The proposed approach combines reinforcement learning with LLMs in order to decompose the planning task in a minecraft world to achieve long-term planning tasks. LLMs are used to build a skill graph by learning relationships between skills, while RL is used to plan over the learnt graph. This paper highlights another example of how LLMs can be useful tools to assist with long-term planning approaches. \n\n[3]  Used LLM to break down the goals and map them to structured actions for final control signals. The proposed LLM-based agent consists of an LLM Decomposer, an LLM Planner, and an LLM Interface. Similarly to [1], this work is very relevant to our work as it supports the conclusions reached through our ReAct agent. \n\n**Q1: Clarification on the OOD evaluation**\nFor the training data for the fine-tuned models, we use the \u201c#Train\u201d subsets from Table 1, which contains 16k instances in single-goal setting and 53k instances in multi-goal setting, all under 6x6 grids and with 1-5 obstacles. We point the reviewer to the first as well as the last two paragraphs of Section 3 for implementation details and experimental design. for more details about the different settings. Similarly, for in-context learning, we draw few-shot examples from the training sets used for finetuning and evaluate on instances drawn from the OOD test sets. \n\n**Q2: Clarification on the \u201cexact prompts used in the task\u201d**\nThe full prompts used for all in-context experiments are presented in Appendix B for reproducibility. The few shot exemplars are drawn from the training set. The prompt for predicting whether the goal is reachable is not a separate one, however, we provide few-shot exemplars that include this case, and we expect the LLMs to identify these cases based on these examples."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6535/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700417348610,
                "cdate": 1700417348610,
                "tmdate": 1700417348610,
                "mdate": 1700417348610,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gNcPKsg56q",
            "forum": "fe8CzLTMG1",
            "replyto": "fe8CzLTMG1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6535/Reviewer_yt2L"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6535/Reviewer_yt2L"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to investigate spatial-temporal reasoning and planning capabilities of SOTA LLMs. It proposed PPNL, a benchmark contains a set of 2D grid path planning problems, and conducted various experiments examining several LLMs's capabilities in path planning in a number of settings: in-distribution, out-of-distribution with varying grid size and number of obstacles, and multi-goal long-term planning settings. The results show that with appropriate prompting technique, LLMs can reason well in relatively simple settings, but struggles when it comes to long-term temporal reasoning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The direction of the paper is important: temporal and spatial reasoning capabilties are indeed crucial for LLMs and ultimately AGI systems\n- The experiments are well designed and conducted thoroughly\n- Writing and paper presentation are polished"
                },
                "weaknesses": {
                    "value": "- My biggest concern is that the experiment setting is a bit too simple: it's just a set of discrete 2D grid, which is far from ideal and realistic path planning setting: high DoF, 3D space, continuous action. I understand 2D grid is a good starting point, but still, it doesn't provide sufficient value for revealing deep enough insight into LLM's limits. for example, such experiments don't shed light on how current LLMs can reason in 3D space\n- This is a bit philosophical: spatial reasoning in a blind (pure language) space is, at least to me, not a well grounded request. I understand at the time of submitting, GPT-4v is not available yet, but there are also other large multimodal available, such as Bard. Maybe such experiments would be more justified if the reasoning is grounded with a vision input? After all, spatial path planning with only access to language description, even for humans, is not a very common task. I would like to see more insights on this from the authors."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6535/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6535/Reviewer_yt2L",
                        "ICLR.cc/2024/Conference/Submission6535/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6535/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777324071,
            "cdate": 1698777324071,
            "tmdate": 1700689880062,
            "mdate": 1700689880062,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BNH9LCwvkp",
                "forum": "fe8CzLTMG1",
                "replyto": "gNcPKsg56q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6535/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6535/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer yt2L"
                    },
                    "comment": {
                        "value": "We thank the reviewer for sharing the thoughtful comments on our work! We appreciate the reviewer\u2019s acknowledgement on the importance of our research topic (i.e., temporal and spatial reasoning). \n\nWe refer the reviewer to \u201cGeneral Responses to All Reviewers\u201d regarding the concern about \u201cthe experiment setting is a bit too simple\u201d and \u201cmultimodal\u201d modeling. We welcome further comments from the reviewer and will be happy to have deeper discussions!"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6535/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700417259435,
                "cdate": 1700417259435,
                "tmdate": 1700417259435,
                "mdate": 1700417259435,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0iYbY9dXwS",
                "forum": "fe8CzLTMG1",
                "replyto": "BNH9LCwvkp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6535/Reviewer_yt2L"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6535/Reviewer_yt2L"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thank you for your detailed response and additional experiments. My concerns have been partially addressed, but the simplicity of the task proposed still makes me not fully convinced by the significance of the contribution of the paper.\nI have raised my score to a positive one."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6535/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689975848,
                "cdate": 1700689975848,
                "tmdate": 1700689975848,
                "mdate": 1700689975848,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g6FcWnwnJD",
            "forum": "fe8CzLTMG1",
            "replyto": "fe8CzLTMG1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6535/Reviewer_w1hA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6535/Reviewer_w1hA"
            ],
            "content": {
                "summary": {
                    "value": "The paper examines the ability of Large Language Models (LLMs) to perform spatial-temporal reasoning with a focus on path planning. It presents findings on LLMs\u2019 proficiency in spatial reasoning when provided with spatial information and feedback from the environment. The paper highlights challenges LLMs face in scenarios requiring long-term planning and complex environments. The research introduces ReAct prompting and fine-tuned models' performances on newly proposed datasets, emphasizing their limitations and potentials in robotic applications."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strengths of the paper include a thorough analysis of LLMs' capabilities in spatial-temporal reasoning and path planning. The originality of the work is evident in the creation of a new dataset and the formulation of specific benchmarks for path planning. The quality of research seems robust, with significant clarity in presenting the challenges and potential of LLMs in complex tasks. The significance of the study is clear, as it informs the limitations of current models and outlines potential future work to improve LLMs' application in real-world tasks."
                },
                "weaknesses": {
                    "value": "One primary concern is what this paper brings to the community. The conclusion is stated at the end of the introduction, which basically matches what we would expect from other recent papers, especially considering the toy nature of the tasks. Additionally, the prompting itself, as the \"method\" section, is also using existing stand \"techniques\" and will not fundamentally solve the spatiotemporal reasoning + generalization problem.\n\nAnother one of the concerns about the paper is that the domain studied, such as 7x7 path planning, could be considered somewhat simplistic or \"toy-like.\" This raises questions about the extent to which the findings can be generalized to more complex, real-world scenarios. The use of small-scale environments may not adequately capture the challenges and nuances that would be present in larger, more intricate settings that LLMs might encounter in practical applications. If the benchmarking tasks do not accurately reflect the complexity of real-world tasks, it may limit the utility of the findings. To advance the field, it would be beneficial for future work to address scaling issues and test LLMs in more diverse and complex environments that better approximate actual use cases."
                },
                "questions": {
                    "value": "1. How do the authors justify the use of the 7x7 path planning environment as a valid proxy for evaluating LLMs' true planning performance?\n2. What are the authors' plans for testing LLMs in more complex and realistic environments to ensure the findings are scalable and applicable to real-world tasks?\n3. Could the authors comment on any additional metrics or methods that might be used to evaluate planning performance in more complex scenarios?\n4. How might advancements in LLMs impact the spatiotemporal reasoning capability?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6535/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699084282899,
            "cdate": 1699084282899,
            "tmdate": 1699636735648,
            "mdate": 1699636735648,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ImfQUNoMr1",
                "forum": "fe8CzLTMG1",
                "replyto": "g6FcWnwnJD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6535/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6535/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer w1hA"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the quality of our experimental design and the thoughtful comments they provided. \n\n**Clarification on the contribution of this work to the community**\nWe refer the reviewer to the general response for a detailed discussion on the contributions of our benchmark compared with prior work. \n\nHere, we further clarify that our work has provided many novel insights beyond \u201cwhat we would expect from other recent papers\u201d, particularly on the less studied topic of \u201cspatial-temporal reasoning\u201d. For example,  Patel et. al (2022) studied whether text-only LLMs can ground spatial concepts (e.g. direction, color, etc.) to a grid world. They find that these concepts are, indeed, encoded in the LLMs. However, the conclusions have been limited to only concept understanding but not \u201creasoning\u201d. Bubeck et. al (2023) evaluated LLMs ability to navigate a map of a house. They found that, while GPT-4 is not successful at exploring the whole house, it is able to accurately describe what it is exploring, despite being prompted solely through a text-based interface, highlighting LLMs' \"spatial awareness\" but also underscoring a shortcoming at planning; however the latter point was not explored in depth. Besides, as shown in the general response, most existing benchmarks do not support the assessment combining spatial and long-horizon temporal reasoning; as a result, not many insights can be obtained on this topic.\n\nOur work fills the gap by systematically evaluating a set of advanced LLMs in path planning, where both spatial and long-horizon temporal reasoning are needed. Our experiments were based on both fine-tuned LLMs and the state-of-the-art GPT-4, when it is augmented with the most advanced prompting methods (e.g., CoT, ReAct), which were not explored in prior work. Our experimental results showed that LLMs, with careful prompting, exhibit a certain level of spatial reasoning, but they still fall short in temporal reasoning.\n\n**Q1: How do the authors justify the use of the 7x7 path planning environment as a valid proxy & Q2: findings are scalable and applicable to real-world tasks**\n\nWe refer the reviewer to the general response for the discussion about PNNL\u2019s simplicity but that its findings can be generalized to more complex, real-world scenarios. \n\n**Q3: Could the authors comment on any additional metrics or methods that might be used to evaluate planning performance in more complex scenarios?**\n\nThe metrics used in this work (e.g., success rate) can still be generalizable to more complex scenarios. Another useful metric for motion and path planning in complex environments is Clearance from obstacles (Plaku, 2017), which measures the distance of the agent to nearby obstacles. This metric allows for better evaluation of the plans as, in some cases, paths having lower clearances might be harder to execute when taking dynamics into account. \n\n**Q4: How might advancements in LLMs impact the spatiotemporal reasoning capability?**\n\nA key takeaway from our conclusion is that LLMs are not able to perform any mature level if spatial-temporal reasoning. A future research question to be explored is: \u201cAre LLMs incapable of spatial-temporal reasoning, or do they just need to be prompted in a specific way to elicit such capability?\u201d. Our work shows that interactions with environment feedback idea is very promising in this regard, however, it remains impractical due to a high cost and slow inference. Therefore, an idea that can be explored is whether LLMs are able to self-correct intrinsically without need for external signals; recent work has shown that this skill is not present in current LLMs (https://arxiv.org/pdf/2310.01798.pdf); indicating that they lack the ability for true reasoning; but they seem to encode enormous knowledge which allows them to exhibit what seems to be concept commonsense understanding (which can be reduced to a memorization task). Hence, developing low-cost and efficient approaches (e.g. smaller fine-tuned models trained with carefully designed objective functions) can yield significant improvements."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6535/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700417176694,
                "cdate": 1700417176694,
                "tmdate": 1700417176694,
                "mdate": 1700417176694,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MlE9QuSJKh",
            "forum": "fe8CzLTMG1",
            "replyto": "fe8CzLTMG1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6535/Reviewer_T3Yg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6535/Reviewer_T3Yg"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a benchmark on the ability of LLMs to perform path planning (PPNL - Path Planning from Natural Language) and analyzes several language models on the benchmark including fine tuned models. The authors claim that results on this benchmark demonstrate that LLMs perform spatial reasoning which can be systematically measured and improved upon via evaluation on the benchmark. They find that LLMs do not succeed in path planning on out-of-distribution data and long horizon examples."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The systematic construction of examples on which to test path planning is nicely presented. The writing in the paper is clear, and the evaluation on the proposed benchmark is thorough. The formulation of this paper as an investigation into the true characteristics of a property which people are actively trying to leverage in their model development is a strong direction. If the claim of demonstrating spatial temporal reasoning in LLMs was established in this work (see weaknesses), it would be an interesting and novel result."
                },
                "weaknesses": {
                    "value": "It seems to me that the results here could be explained in an entirely different way.\n\nEven though there are systematically constructed evaluations of increasing complex path planning problems across different dimensions (length, number of obstacles, etc.), I do not see how the fact that LLMs fail on the more complex tasks does not just imply that the pattern based instruction imitation of LLMs (the alternative interpretation of LLM instruction success) does not just fall apart more quickly on more difficult tasks. Imitating textual examples of instructions that these LLMs have been trained on in a pattern based way would yield significant success in providing directional information, particularly over short horizons.  Difficult tasks (long horizon, etc.) which require spatial reasoning have a lower probability of accidentally being successful with imitation-based responses.\n\nIn fact, the fine-tuning results where improvement is found in distribution and fail on out of distribution examples seems to support this alternate interpretation - not the authors\u2019 interpretation.\n\nThis imitation based mimicry of problem solving in relation to the ability of LLMs to perform mathematical computations have been widely discussed in the past (Bubeck et al. 2023). Also, the referenced papers on spatio-temporal reasoning used on PPNL (CoT and ReAct) provide approaches to use LLMs for spatio-temporal reasoning which is fundamentally different from implying LLMs actually perform spatio-temporal reasoning. \n\nIn fact even in the spatial reasoning section of the related work, it does not appear that any prior work supports the idea that spatial reasoning can be performed by LLMs. There are 3 types of work cited by the authors which also represent my understanding of the community's view of this problem: \n\n1. LLMs can be used via methodological automated prompting to develop spatial plans (the prompting method + LLM executes planning which has varying success in accomplishing the task), \n2. LLMs have some level of spatial understanding (textual request for code to make images has significant success, etc.), \n3. LLMs do not perform reasoning for math / planning / etc. problems - just quite good mimicry.\n\nIf the authors can explain and convince me that these results show LLMs actually perform reasoning over spatial information during the rebuttal, I would be willing to significantly increase my score."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6535/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6535/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6535/Reviewer_T3Yg"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6535/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699100506399,
            "cdate": 1699100506399,
            "tmdate": 1700742860763,
            "mdate": 1700742860763,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ScsLCl974Q",
                "forum": "fe8CzLTMG1",
                "replyto": "MlE9QuSJKh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6535/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6535/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer T3Yg"
                    },
                    "comment": {
                        "value": "We are grateful to the reviewer for their insightful comment on the state of research on LLM spatial reasoning! \n\nWe noticed a misunderstanding on our experimental results. Therefore, we would first like to clarify our major observation. That is, based on our results, we believe that LLMs do exhibit __some__ ability for reasoning over spatial information. However, they are still __not perfectly__ solving spatial reasoning, and they have failed in long-horizon temporal reasoning. \n\nBelow, we clarify how we reached the conclusion that \u201cLLMs do exhibit __some__ ability for spatial reasoning\u201d. We believe that the observed success of LLMs is not solely \u201cpattern/instruction imitation\u201d. Whether LLMs can reason or not is still a debatable question and our work does not aim to resolve the argument (instead, we provided a benchmark to facilitate it). In our humble opinion, pattern imitation happens when the same or similar patterns have been provided to the LLM, either during its pre-training or in-context learning, so that the LLM may succeed by memorizing the patterns from the data and then adapting the language to the specific test input, rather than reasoning about the test problem from scratch. However, it is important to note that the path planning task we have focused on in this work is very different from the commonly seen math reasoning tasks (e.g., GSM8k, MATH) in the following ways:\n\nFirst of all, path planning itself is an optimization problem. As elaborated in Appendix A.2, a path planning task essentially projects to the A* search (single-goal setting) or combining A* search with the Traveling Salesman (multi-goal setting) problems. This is very different from math problem solving where an LLM could cheat by imitating the pattern of how numbers given in the condition can be combined into an equation and eventually led to a numerical answer. Due to its optimization nature, path planning cannot be achieved with trivial pattern imitation.\n\nWhile math problems are often framed in daily scenarios (such that an LLM may leverage the commonsense pattern it has learned to play \u201cshortcuts\u201d), the path planning problems in PNNL is highly symbolic (e.g., using p0 and p1 rather than the specific city names as locations, and indicating all obstacles using x-y coordinates). So playing \u201cshortcuts\u201d is not very likely.\nWe want to clarify that our prompts to LLMs do not include the exact test environments, and can even include completely OOD ones, so there does not exist any potential \u201cpattern\u201d for imitation. Notably, GPT-4 with CoT obtains stable generalization to smaller or larger grid environments (success rate of 0.787 -> 0.763 and 0.836). While it suffers from degradation in the obstacle OOD evaluation, the success rate is still decent (0.544). To further uncover its underlying mechanism, we added one experiment for 168 20x20 grid environments, each consisting of 40 obstacles. We still observed a success and optimal rates of 0.279 from GPT-4 AE and 0.351 from GPT-4 CoT. This implied that LLMs do reason about their optimization strategy in path planning, although the strategy is not very generalizable. We would like to denote a distinction between (a) whether LLMs can reason and (b) where their reasoning outcome (e.g., their path planning strategy) is generalizable to the more complicated scenarios. Intuitively, even for humans (e.g., human soldiers), when they have learned skills exclusively only in a small-size practice room, they would have difficulty when entering a real-size battlefield, but this does not imply their lack of reasoning or learning during the practice.\nLastly, while similar math problems may have potentially been included the pre-training data of LLMs, this is much less likely to be true for path planning, because intuitively people do not commonly verbalize a symbolic task on the Internet data. \n\nAs discussed in our experimental results section, LLMs still cannot \u201cperfectly\u201d reason about the spatial information. This again can be evidenced by the degradation of LLMs in OOD evaluation. They also fail to perform long-horizon, temporal reasoning; for example, we observed that the success of ReAct compared with CoT is attributed to the way how it receives environment feedback and decomposes a long-horizon task into multiple short-horizon ones. Another result highlighting LLMs inability for temporal planning is the low ability to find the optimal path, particularly for the multi-goal setting; We particularly probe the LLM ability to perform optimization in Appendix D, and present the results in Table 10, which clearly shows that GPT-4 falls short on this task."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6535/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700416863927,
                "cdate": 1700416863927,
                "tmdate": 1700416957329,
                "mdate": 1700416957329,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lKxATTjj55",
                "forum": "fe8CzLTMG1",
                "replyto": "MlE9QuSJKh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6535/Reviewer_T3Yg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6535/Reviewer_T3Yg"
                ],
                "content": {
                    "title": {
                        "value": "Reply to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you very much for the thorough rebuttal. I have thought carefully about the rebuttal points and am currently keep my score. \n\nSome remarks:\n\n- \"playing shortcuts is not very likely\" seems like a conjecture, and I do not know that there is evidence to back up this claim. Again, I will say that it seems to me that evidence of the solution being beyond imitation is the ability to execute the same logic on out of distribution or long horizon tasks (where the models tend to fail) since these cases cannot be imitated. In terms of \"number of examples\" being limited, the authors fine-tune the LLMs so the models do receive examples of this type of input/output combination or execute an explicit reasoning policy on top of the LLM .\n\n- Similar and the same are quite different. I would argue that providing fine-tuned information to an LLM of 6x6 grid path planning problems and then testing on different 6x6 grid path planning problems are \"similar\". I would expect a method which actually performs reasoning to fairly easily generalize to 7x7 grids. I think it is a substantially overstated analogy by the authors to make the comparison to this small degree of grid size generalization with human soldiers \"learn[ing] skills exclusively only in a small-size practice room, [...] hav[ing] difficulty when entering a real-size battlefield.\"\n\n- Regarding the simplicity of the environment which several other reviewers brought up, I understand the point that the environment is intentionally simple to probe the capability very directly for the intended planning task. I think this is a good design choice. However, a red flag regarding this proposed benchmark is the comment of the following comment from the authors combined with the fairly high performance results of the LLMs:\n\n\"A key takeaway from our conclusion is that LLMs are not able to perform any mature level if spatial-temporal reasoning. A future research question to be explored is: \u201cAre LLMs incapable of spatial-temporal reasoning, or do they just need to be prompted in a specific way to elicit such capability?\u201d.\"\n\nIf LLMs are not able to perform any mature level of spatial-temporal reasoning, why should we use a benchmark to measure spatial-temporal reasoning where 90+% success metrics reported in results tables for models that cannot do this reasoning? See Table 3 in the paper."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6535/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668167882,
                "cdate": 1700668167882,
                "tmdate": 1700668248841,
                "mdate": 1700668248841,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]