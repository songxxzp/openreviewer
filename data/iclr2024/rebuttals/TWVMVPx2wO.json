[
    {
        "title": "Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning"
    },
    {
        "review": {
            "id": "8nhjXEjkf6",
            "forum": "TWVMVPx2wO",
            "replyto": "TWVMVPx2wO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5583/Reviewer_1Fvy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5583/Reviewer_1Fvy"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of adapting pre-trained models for Deep Metric Learning (DML) tasks while preserving prior knowledge. Existing solutions often fine-tune models on standard image datasets, making it difficult to apply them to local data domains. The paper introduces a novel approach, the Visual Prompts (VPT) framework, which augments the conventional proxy-based DML method. VPT optimizes visual prompts for each class by integrating semantic information from input images and Vision Transformers (ViT). The experimental results demonstrate that this approach outperforms existing methods in DML benchmarks, achieving comparable or better performance with minimal parameter fine-tuning. This parameter-efficient technique offers a promising avenue for improving DML without requiring extensive model retraining."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper introduces a novel approach to Deep Metric Learning (DML) by proposing the Visual Prompts (VPT) framework. This framework addresses the challenge of fine-tuning pre-trained models for DML while incorporating semantic information and optimizing visual prompts. This approach represents an innovative combination of existing ideas, extending the traditional proxy-based DML paradigm with an efficient and effective method.\n\n- The paper is well-written and clearly articulates the motivation, methodology, and experimental results. The authors effectively communicate their approach, making it accessible to a broad audience of readers, including those familiar with DML and those new to the field. The paper's clarity enhances its potential for adoption and understanding by the research community.\n\n- The proposed framework has the potential to streamline and improve DML tasks, as it achieves comparable or better performance compared to full fine-tuning approaches, all while adjusting only a small percentage of total parameters."
                },
                "weaknesses": {
                    "value": "- The paper lacks a comprehensive comparison with existing DML methods, especially those using transformer based backbone. Additionally, it would be beneficial to compare the proposed Visual Prompts (VPT) framework not only against full fine-tuning but also against other state-of-the-art parameter-efficient methods. This would provide a more complete assessment of its effectiveness and novelty."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5583/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676261444,
            "cdate": 1698676261444,
            "tmdate": 1699636575016,
            "mdate": 1699636575016,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QQwEB1oCne",
                "forum": "TWVMVPx2wO",
                "replyto": "8nhjXEjkf6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5583/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5583/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We thank you for your review"
                    },
                    "comment": {
                        "value": "We are deeply grateful for the time and effort you invested in reviewing our manuscript. Your insightful comments and the connection you drew between our methods and existing DML ideas are highly appreciated.\n\n**Addressing the Need for Comprehensive Comparison with Existing DML Methods**\n\nIn alignment with our response to **Reviewer 1 (hvFX)**, we wish to further clarify the baseline methodologies used in our study. The \"full fine-tuning\" baseline, as discussed in both our main paper and Appendix, is based on the proxy-anchor approach [1], which is recognized as one of the state-of-the-art methods in DML. Additionally, we have conducted comparisons with other leading approaches, such as Hyp-ViT [2] and RS@k [3], which are also based on ViT architectures and considered state-of-the-art.\n\nIt is important to note that all these state-of-the-art methods [1][2][3], while varying in their optimization loss, share a commonality in their computational costs, as they are all fundamentally full fine-tuning methods. To provide a more detailed comparison, we have included an additional analysis in Table 6 in the Appendix. This table compares our methods with those mentioned and other existing DML methods, offering a comprehensive comparative view.\n\nFor an in-depth understanding of these comparisons and the rationale behind our approach, we kindly refer you to our detailed response to **Reviewer 1 (hvFX)**, where we elaborate on these points further.\n\n\n**Addressing the comparison with other parameter-efficienct fine-tuning methods**\n\nWe would like to draw your attention to our comprehensive analysis comparing our VPT method with several leading parameter-efficient techniques in the DML benchmark. This detailed comparison is outlined in Section 5.1.2 of our paper. The results of this comparative study are showcased in Table 1 of the main manuscript, where we assess and compare the performance of each method in terms of DML metrics (specifically R@1 and MAP@R), the number of adjustable parameters, and memory usage.\n\nTo ensure a fair and unbiased comparison, as elaborated in Section 5.1.3, we conducted an extensive hyperparameter search for each method. This rigorous approach ensures that our comparative analysis is not only comprehensive but also equitable, taking into account the varied characteristics and capabilities of each parameter-efficient method under consideration.\n\nWe trust that these clarifications address your concerns regarding the comparative analysis of our methods with other DML approaches. Your constructive feedback is invaluable in our pursuit of continual improvement of our manuscript.\n\n\n[1] Kim, Sungyeon, et al. \"Proxy anchor loss for deep metric learning.\" CVPR 2020. \n\n[2] Ermolov, Aleksandr, et al. \"Hyperbolic vision transformers: Combining improvements in metric learning.\" CVPR 2022.\n\n[3] Patel, Yash, Giorgos Tolias, and Ji\u0159\u00ed Matas. \"Recall@ k surrogate loss with large batches and similarity mixup.\" CVPR. 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5583/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678575844,
                "cdate": 1700678575844,
                "tmdate": 1700678575844,
                "mdate": 1700678575844,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IGXEsoEuWT",
            "forum": "TWVMVPx2wO",
            "replyto": "TWVMVPx2wO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5583/Reviewer_HvDC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5583/Reviewer_HvDC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an efficient method for fine-tuning the pre-trained models for the DML image retrieval tasks. The authors propose a method based on visual prompts (VPT) to partially fine-tune the model instead of tuning all parameters. Based on the proxy-based DML methods, they also initial the proxies with input images and visual prompts based on classes."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1 The contribution is solid. As far as I know, this may be the first work to explore the parameter-efficient (PEFT) methods specifically for deep metric learning.\n\n2 Novelty is fine. Although the VPT fine-tuning is not something new, the local design to improve it specifically for DML is interesting and novel.\n\n3 The writing is clear and easy to follow.\n\n4 Experiments is well organized and convincible. \n\nThe authors evaluate their method widely on popular DML datasets, and the results seem to be strong and solid. They also comprehensively compare other PEFT methods on DML datasets, which might be interesting to the community."
                },
                "weaknesses": {
                    "value": "I didn't see significant weakness. Based on the limited results on some datasets compared with SOTA, I hope the authors can provide more analysis and possible ideas to improve it.\n\nIt seems the results are good on large pre-training datasets but fair on small pre-training. Could you explain more about this phenomenon? \n\nIt is suggested to move some results in the Appendix, like results on other datasets and pre-trained models, to the main paper. \n\nMinor:\nSome typo or grammar errors:\n\"which includes a random horizontal flip, cropping at random...\"\n\"we found that it linear probing...\"\n\nSome recent works missing: \n\nWang et al. \"Deep Factorized Metric Learning.\" CVPR 2023\nKim et al. \"HIER: Metric Learning Beyond Class Labels via Hierarchical Regularization\" CVPR 2023\nKotovenko et al. \"Cross-Image-Attention for Conditional Embeddings in Deep Metric Learning\", CVPR 2023"
                },
                "questions": {
                    "value": "Please see the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5583/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802191308,
            "cdate": 1698802191308,
            "tmdate": 1699636574920,
            "mdate": 1699636574920,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ruv6TrtLDH",
                "forum": "TWVMVPx2wO",
                "replyto": "IGXEsoEuWT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5583/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5583/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Expression of Gratitude for Your Detailed Review"
                    },
                    "comment": {
                        "value": "We are immensely grateful for the time and effort you have devoted to reviewing our manuscript. Your acknowledgment of our paper's contribution and novelty, coupled with your valuable insights and suggestions, is highly appreciated.\n\n**Addressing Limited Results on Some Datasets Compared with SOTA**\n\nIn Section 5 of the Appendix, we have openly discussed the limitations of our proposed VPT methods. The suboptimal performance on the SOP and InShop datasets, which feature a higher number of classes and proxies, is a focus of our ongoing research and improvement. This admission underlines our commitment to transparency and the continuous development of our work.\n\n**Performance on Large vs. Small Pre-training Datasets**\n\nAs elaborated in the Experimental section, we posit that the performance of VPT and other parameter efficiency methods is influenced by the distributional variances between pre-training and fine-tuning datasets. Smaller pre-training datasets like ImageNet-1K often display a more significant variance in distribution compared to target datasets. This observation is consistent with prior findings in VPT research [1], lending support to our analysis.\n\n\n**Reorganization of Results and Appendix Content**\n\nIn response to your suggestion, we have included the results from the iNaturalist dataset in the main paper (now in Table 3). Due to space and format constraints, we are unable to relocate more results from the Appendix to the main text. We believe that the current structure of the manuscript strikes a balance between detail and conciseness, effectively presenting our findings.\n\n**Attention to Minor Details**\n\nAll minor points raised in your review have been meticulously addressed and corrected. We believe these revisions significantly enhance the clarity and accuracy of our manuscript.\n\nWe believe that our responses and revisions comprehensively address your concerns and contribute to the refinement of our paper. We are sincerely thankful for the opportunity to improve our manuscript based on your constructive feedback and eagerly await any further guidance you might offer.\n\n\n\n[1] Jia, Menglin, et al. \"Visual prompt tuning.\" ECCV, 2022"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5583/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678474786,
                "cdate": 1700678474786,
                "tmdate": 1700678474786,
                "mdate": 1700678474786,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NkhMKGbI2x",
                "forum": "TWVMVPx2wO",
                "replyto": "ruv6TrtLDH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5583/Reviewer_HvDC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5583/Reviewer_HvDC"
                ],
                "content": {
                    "title": {
                        "value": "Keeping my rating for acceptance"
                    },
                    "comment": {
                        "value": "Thanks for your detailed response. I appreciate the efforts and clarification. I will keep my rating as accept."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5583/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715733238,
                "cdate": 1700715733238,
                "tmdate": 1700715733238,
                "mdate": 1700715733238,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "knNEXukVvK",
            "forum": "TWVMVPx2wO",
            "replyto": "TWVMVPx2wO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5583/Reviewer_hvFX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5583/Reviewer_hvFX"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces visual prompt tuning (VPT) to improve proxy-based deep metric learning. As VPT could generate and integrate semantic proxies to improve the representations in deep metric learning, the proxies generated by VPT is considered to be better than random proxy which is generally used in proxy-based deep metric learning. The experiments are conducted several classification benchmark datasets including CUB-200-2011 (CUB200), CARS196, and retrieval benchmark datasets including Stanford Online Products (SOP), In-shop Clothes Retrieval (In-Shop). The proposed method is compared and shown to outperform or perform on par with various parameter-efficient fine-tuning methods such as Adapter Fine Tuning, and Bitfit."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Although the proposed method is simple, the proposed idea of introducing visual prompt tuning to improve proxy-based deep metric learning is well motivated and valid."
                },
                "weaknesses": {
                    "value": "The main concern of this paper lies in the lack of evaluation/comparison:\n\n-- The paper claims the proposed method to be parameter-efficient for deep metric learning (DML) tasks, but there is not comparison in term of efficiency as compared to existing DML methods. \n\n-- The proposed method targets to solve deep metric learning (DML) tasks but the compared methods are mostly fine-tuning methods. More DML methods should be considered for comparison. \n\n-- A baseline using default/vanilla proxy-based deep metric learning (without visual prompt learning) is missing to convince the effectiveness of introducing visual prompt tuning."
                },
                "questions": {
                    "value": "What is the computational cost of the proposed method as compared to (1) vanilla proxy-based DML method, (2) other fine-tuning methods such as BitFit and Adapter? Please discuss the comparison on computation cost for model training and inference. \n\nHow does the proposed method compare to the state-of-the-art deep metric learning methods? \n\nHow does the proposed method compared to the vanilla proxy-based deep metric learning in terms of model performance and compute cost?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concern on Ethics."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5583/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820245511,
            "cdate": 1698820245511,
            "tmdate": 1699636574823,
            "mdate": 1699636574823,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Pi3pwjKejW",
                "forum": "TWVMVPx2wO",
                "replyto": "knNEXukVvK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5583/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5583/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gratitude for Your Insightful Review"
                    },
                    "comment": {
                        "value": "Dear Reviewers,\n\nWe extend our sincerest thanks for your insightful and constructive comments on our paper. Your feedback is invaluable in helping us clarify and address the questions you have raised.\n\n**Comparison in Terms of Efficiency with Existing DML Methods**\n\nAligning with the metrics in existing parameter-efficient works [1][2], we assess efficiency by comparing the number of tunable parameters in the model, which reflects computational costs per iteration. Most existing DML methods [4][5] rely on full fine-tuning optimization with varying loss functions or sampling strategies, making their efficiency comparable to our full fine-tuning baseline.\n\nFurthermore, we have compared actual running latencies in Section C and Table 8 of our Appendix (formerly Supplementary Materials). This table shows the average running time per iteration in identical environments across 100 trials. Our results show that our proposed VPT and other parameter-efficient methods significantly reduce latency compared to full fine-tuning approaches. It's important to note that running latency is subject to hardware and software configurations. Hence, parameter-efficient works [1][2] often use tunable parameters as a universal metric for evaluating fine-tuning efficiency.\n\n**Consideration of More Existing DML Methods and a Vanilla Proxy-Based Baseline**\n\nIn response to your concern, we have made the following clarifications and additions:\n1) Our full fine-tuning approach, detailed in the main paper and Appendix, is based on the vanilla proxy-anchor (PA) [3] methodology, enhanced with a ViT backbone and larger ImageNet-21k pre-training datasets. We have included comparisons with state-of-the-art methods like Hyp-ViT [4] and RS@k [5], which also utilize a ViT backbone. These comparisons are specific to ViT backbones trained on ImageNet-21k, as earlier works [3][6][7][8] with CNN backbones and ImageNet-1k training show significantly lower DML benchmark performance.\n2) Most current state-of-the-art methods, including Hyp-ViT [4] and RS@k [5], use full fine-tuning of the ViT with different losses or sampling strategies, similar to our PA baseline. These methods often require larger batch sizes (as indicated in Table 2), leading to higher latencies compared to our baseline. Therefore, their overall efficiency is close to or below our full fine-tuning PA baseline, while our VPT methods demonstrate higher DML performance, as shown in Table 2.\n3) Although it's not entirely fair to compare with earlier works using different backbones and pre-training models, we have included performance and tunable parameters for some earlier works [6][7][8] and the vanilla proxy-anchor [3] as references. This comparison is also added to our Appendix as Table 6.\n\n***\n|Method \t              | Arch | Parameters \t| R@1 | R@2| R@4| MAP@R | \n| :---------------------- | :------- | :----------------- | :------ | :------- | :----- | :------ |\n|ProxyNCA++[6]     | R50 | 26.5M \t\t| 69.0 | 79.8 | 87.3 | - |\n|NIR [7]\t              | R50 | 26.5M             | 69.1  | 79.6 | -    | -  |\n|XBM[8]                  | BN\t| 12M                | 65.8 | 75.9 | 84.0 |   - \t|\n|PA (vanilla)[3]       | R50 | 26.5M              | 69.7 | 80.0  | 87.0 | 26.5 |\n|Hyp[4] | ViT-S/16 | 30.2M\t\t\t| 85.6 | 91.4 | **94.8** | - |\n|Full fine-tuning (PA) | ViT-S/16 | 30.2M  |  85.1  | 91.1 | 94.0 | 51.1 |\n|VPTSP (ours) | ViT-S/16 | **1.56M**          | **86.6** | **91.7** | 94.7 | **52.0** |\n***"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5583/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678252450,
                "cdate": 1700678252450,
                "tmdate": 1700722834583,
                "mdate": 1700722834583,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zUrSu8tW3i",
                "forum": "TWVMVPx2wO",
                "replyto": "knNEXukVvK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5583/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5583/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gratitude for Your Insightful Review"
                    },
                    "comment": {
                        "value": "**Q1: What is the Computational Cost of the Proposed Method**\n\nAs mentioned, we evaluate our method's computational cost in two aspects:\n1) The total trainable parameters that are updated in each optimization step (shown in Table 1).\n\n2) The actual running latency per optimization step (shown in Table 8 of the Appendix).\n\nTable 1 in the main paper demonstrates that our method requires only about 5% of the trainable parameters needed for full fine-tuning methods. For running latency, our method significantly undercuts full fine-tuning methods, as listed in Table 8 of the Appendix.\n\n\n**Q2 and Q3: Comparison to State-of-the-Art DML Methods and Vanilla Proxy-Based DML**\n\nTo better represent our \"full fine-tuning method\" as a stronger baseline compared to the vanilla proxy-anchor, we have added explanatory text in our revision. We have also updated Table 1 in the main paper and Table 8 in the Appendix for greater clarity. Additionally, a new table (Table 6 in Appendix) comparing our proposed method with existing DML works using ViT backbones [4][5] and earlier DML methods with CNN backbones [3][6][7][8] has been added.\n\nWe hope these revisions and additional explanations comprehensively address your concerns, thereby enhancing the clarity and impact of our work. We are deeply appreciative of the opportunity to refine our manuscript based on your thoughtful feedback and eagerly await any further suggestions you may have.\n\n\n[1] Jia, Menglin, et al. \"Visual prompt tuning.\" ECCV, 2022\n\n[2] Chen et al. \"Adaptformer: Adapting vision transformers for scalable visual recognition.\" NeurIPS 2022\n\n[3] Kim, Sungyeon, et al. \"Proxy anchor loss for deep metric learning.\" CVPR 2020. \n\n[4] Ermolov, Aleksandr, et al. \"Hyperbolic vision transformers: Combining improvements in metric learning.\" CVPR 2022.\n\n[5] Patel, Yash, Giorgos Tolias, and Ji\u0159\u00ed Matas. \"Recall@ k surrogate loss with large batches and similarity mixup.\" CVPR. 2022.\n\n[6] Teh et al. \"Proxynca++: Revisiting and revitalizing proxy neighborhood component analysis.\" ECCV 2020\n\n[7] Roth et al. \"Non-isotropy regularization for proxy-based deep metric learning.\" CVPR 2022\n\n[8] Wang et al. \"Cross-batch memory for embedding learning.\", CVPR 2020"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5583/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678313420,
                "cdate": 1700678313420,
                "tmdate": 1700723276425,
                "mdate": 1700723276425,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]