[
    {
        "title": "Diff-Privacy: Diffusion-based Face Privacy Protection"
    },
    {
        "review": {
            "id": "YIfZnzxLrE",
            "forum": "anG2Y15mwc",
            "replyto": "anG2Y15mwc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1371/Reviewer_J6vY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1371/Reviewer_J6vY"
            ],
            "content": {
                "summary": {
                    "value": "This paper unifies the task of anonymization and visual identity information hiding and proposes a novel diffusion-based face privacy protection method. Specifically, it learns a set of SDM format conditional embeddings through the MSI module. Then, the authors designed corresponding embedding scheduling strategies and energy functions to guide the denoising process to achieve different privacy protection tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. This study analyzes the similarities and differences between anonymization and visual identity information hiding tasks and proposes the first work that can simultaneously achieve these two tasks.\n2. The authors utilize the powerful generation prior of the diffusion model and design reasonable guidance to help SDM complete privacy protection and identity recovery. It is a novel privacy protection framework with unique password and identity recovery patterns.\n3. This paper is well organized and written in general. Most of the claims are supported by ample experimental analysis."
                },
                "weaknesses": {
                    "value": "1. The author learns a set of conditional embeddings through a few images. When using the LFW dataset for training, do different images under the same identity need to be trained separately or can they be trained together to obtain a set of conditional embeddings for encryption and decryption?\n2. In Sec.2.2, the description of obtaining conditional embeddings seems unclear. Does each time step correspond to a unique time embedding?"
                },
                "questions": {
                    "value": "Please see the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1371/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698461892976,
            "cdate": 1698461892976,
            "tmdate": 1699636064757,
            "mdate": 1699636064757,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m3o651zJAT",
                "forum": "anG2Y15mwc",
                "replyto": "YIfZnzxLrE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1371/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1371/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer J6vY"
                    },
                    "comment": {
                        "value": "Thank you for your comments and feedback. We address your concerns here.\n\n>Q1: The author learns a set of conditional embeddings through a few images. When using the LFW dataset for training, do different images under the same identity need to be trained separately or can they be trained together to obtain a set of conditional embeddings for encryption and decryption?\n\nA1: Thank you for the opportunity to clarify. Our MSI module learns conditional embedding through a few images. When the training set is FFHQ or CelebA-HQ, each person has only one image, so we use a single image to learn conditional embedding. When the training set is an image from LFW, we use all images of the person to learn conditional embedding.\n\n>Q2: In Sec.2.2, the description of obtaining conditional embeddings seems unclear. Does each time step correspond to a unique time embedding?\n\nA2: Thank you for the opportunity to clarify. According to previous work [1,2,3], we know that diffusion models focus on different-level information at different time steps. Generally, the model tends to generate the overall layout at the initial stage of the denoising process (corresponding to a large time step), the structure and content at the intermediate stage, and the detailed texture at the final stage. Therefore, we designed an MSI module to learn a set of conditional embeddings as conditions for different periods in the diffusion model. Specifically, we divide the generation process (1000 time steps) into ten time periods, so the output dimension of the time embedding layer is also 10, with each time embedding corresponding to a period.\n\n**Reference**\n\n[1] Wang, Jianyi, et al. \"Exploiting Diffusion Prior for Real-World Image Super-Resolution.\" arXiv preprint arXiv:2305.07015 (2023).\n\n[2] Zhang, Yuxin, et al. \"ProSpect: Expanded Conditioning for the Personalization of Attribute-aware Image Generation.\" arXiv preprint arXiv:2305.16225 (2023).\n\n[3] Balaji, Yogesh, et al. \"ediffi: Text-to-image diffusion models with an ensemble of expert denoisers.\" arXiv preprint arXiv:2211.01324 (2022)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1371/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124580564,
                "cdate": 1700124580564,
                "tmdate": 1700124580564,
                "mdate": 1700124580564,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kouGBkoi7x",
            "forum": "anG2Y15mwc",
            "replyto": "anG2Y15mwc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1371/Reviewer_pv71"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1371/Reviewer_pv71"
            ],
            "content": {
                "summary": {
                    "value": "Overall this is an interesting paper that proposes a novel diffusion-based method for face privacy protection. The method is flexible and can achieve both anonymization and visual identity information hiding. The results demonstrate state-of-the-art performance."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "See the summary in detail."
                },
                "weaknesses": {
                    "value": "1. The network architecture shown in Fig.2 is not clear. As mentioned by the authors, the framework of the proposed method can be divided into three stages, which are not presented in Fig.2. Also, key-E, key-I, and the proposed energy function-based identity guidance module are not clearly shown in Fig.2. \n2. The energy function in Section 2.3.2 is introduced briefly - more details are needed on the formulation and how it enables identity guidance. In addition, the definition of $\\varepsilon$ which indicates energy function is not clearly defined.\n3. In the Require part of Alg.1, \"scheduling strategy for embedding $C_{T}$\" is confusing as the authors define $C_{T}$ as embedding used in time step t before. \n4. The writing should be improved. The manuscript is hard to read and there are occasional grammatical issues and unclear/repetitive statements."
                },
                "questions": {
                    "value": "See the summary in detail."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1371/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1371/Reviewer_pv71",
                        "ICLR.cc/2024/Conference/Submission1371/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1371/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698670186308,
            "cdate": 1698670186308,
            "tmdate": 1701049408647,
            "mdate": 1701049408647,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nqDdOvYWfM",
                "forum": "anG2Y15mwc",
                "replyto": "kouGBkoi7x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1371/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1371/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1 of Response to Reviewer pv71"
                    },
                    "comment": {
                        "value": "Thank you for your comments and feedback. We address your concerns here.\n\n>Q2: The energy function in Section 2.3.2 is introduced briefly - more details are needed on the formulation and how it enables identity guidance. In addition, the definition of $\\epsilon$ which indicates energy function is not clearly defined.\n\nA2: Thank you for your suggestion. We have provided more details about the formula below and added it to the revised manuscript.\n\nFirstly, we provide a concise overview of Score-based Diffusion Models. Score-based Diffusion Models (SBDMs) [1,2] are a category of diffusion model rooted in score theory. It illuminates that the essence of diffusion models is to estimate the score function $\\nabla_{x_t}\\log_{}{p(x_t)}$, where $x_t$ is noisy data. During the sampling process, SBDMs predict $x_{t\u22121}$ from $x_t$ using the estimated score step by step. Its sampling formula is as follows:\n\n$x_{t-1} = (1+\\frac{1}{2}\\beta_t)x_t+ \\beta_t \\nabla_{x_t}\\log_{}{p(x_t)}+\\sqrt \\beta_t \\epsilon, $ \n\nwhere $\\epsilon \\in N(0,I)$ is randomly sampled Gaussian noise and $\\beta_t \\in R$ is a pre-defined parameter. Given sufficient data and model capacity, the score function can be estimated by a score estimator $s(x_t, t)$, that is, $s(x_t, t) \\approx \\nabla_{x_t}\\log_{}{p(x_t)}$. Nevertheless, the original diffusion process is limited to functioning solely as an unconditional generator, yielding randomly synthesized outcomes. To achieve controllable generation, SDE [2] proposed to control the generated results with a given condition $c$ by modifying the score function as $\\nabla_{x_t}\\log_{}{p(x_t|c)}$. Using the Bayesian formula $p(x_t|c) = \\frac{p(c|x_t)p(x_t)}{p(c)}$, the conditional score function can be written as two terms: \n\n$\\nabla_{x_t}\\log_{}{(x_t|c)} = \\nabla_{x_t}\\log_{}{p(x_t)}+ \\nabla_{x_t}\\log_{}{(c|x_t)},$\n\nwhere the first term $\\nabla_{x_t}\\log_{}{p(x_t)}$ can be estimated using the pre-trained unconditional score estimator $s(\u00b7, t)$ and the second term $\\nabla_{x_t}\\log_{}{(c|x_t)}$ is the critical part of constructing conditional diffusion models. The second term can be interpreted as a **correction gradient** [3], directing $x_t$ towards a hyperplane in the data space where all data conform to the given condition $c$. Following [3], we used an energy function [4,5] to model the correction gradient:\n\n$p(c|x_t) = \\frac{\\exp\\left\\\\{-\\lambda \\varepsilon(c,x_t) \\right\\\\}}{Z},$\n\nwhere $\\lambda$ denotes the positive temperature coefficient and $Z>0$ denotes a normalizing constant, computed as $Z = \\int_{c \\in c_d} \\exp\\left\\\\{\u2212 \\lambda \\varepsilon (c, x_t) \\right\\\\}$ where $c_d$ denotes the domain of the given conditions. $\\varepsilon (c, x_t)$ is an **energy function** that measures the compatibility between the condition $c$ and the noisy image $x_t$. When $x_t$ aligns more closely with $c$, the value of $\\varepsilon (c, x_t)$ decreases. If $x_t$ satisfies the constraint of $c$ perfectly, the energy value should be zero. Any function satisfying the above property can serve as a feasible energy function, and we can adjust the coefficient $\\lambda$ to obtain $p(c|x_t)$.\n\nTherefore, the correction gradient $\\nabla_{x_t}\\log_{}{(c|x_t)}$ can be implemented with the following:\n\n$\\nabla_{x_t}\\log_{}{(c|x_t)} \\propto - \\nabla_{x_t}\\varepsilon(c,x_t).$ \n\nBased on the above theories and formulas, classifier guidance methods [4,6,7,8] trained a classifier by using noisy data to calculate the correction gradient for conditional guidance. Unlike the classifier guidance approach, we aim to use pre-trained models to calculate the correction gradient. Therefore, we need to estimate clean data $\\hat x_0$ from noisy data $x_t$. we use Eq.10 to estimate clean latent codes $\\hat z_0$ from noisy latent codes $z_t$ and decode $\\hat z_0$ to obtain clean image $\\hat x_0$. Once we have a clean image $\\hat x_0$, we can use existing facial recognition models to calculate the distance between the clean image and condition and construct an energy function.\n\n$\\varepsilon(c,x_t) \\approx D_\\theta(\\hat x_0, x),$\n\nwhere $D_\\theta$ is a distance measure function and $x$ is the actual condition (original image $x$).\n\nThen, we can perform gradient correction on the DDIM sampling process according to the above formula.\n\n$z_{t-1}= \\sqrt{\\frac{\\alpha_{t-1}}{\\alpha_t}}z_t + \\left(\\sqrt{\\frac{1}{\\alpha_{t-1}}-1}-\\sqrt{\\frac{1}{\\alpha_t}-1}\\right)\\cdot \\epsilon_\\theta\\left(z_t,t,C_t\\right),$\n\n$z_{t-1}' = z_{t-1} - \\lambda_t \\nabla_{\\hat{z}_0} D\\_\\theta(x,\\hat{x}_0),$\n\nwhere $z_{t-1}$ is obtained through the original ddim sampling (Eq.2). $z_{t-1}'$ is the result obtained after our gradient correction and $\\lambda_t$ is a scale factor."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1371/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700123863763,
                "cdate": 1700123863763,
                "tmdate": 1700124321520,
                "mdate": 1700124321520,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pjVZCaM2sU",
                "forum": "anG2Y15mwc",
                "replyto": "XKcbFbM03z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1371/Reviewer_pv71"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1371/Reviewer_pv71"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. The proposed method is solid and I prefer to raise my rating from 5 to 6."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1371/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538517331,
                "cdate": 1700538517331,
                "tmdate": 1700538517331,
                "mdate": 1700538517331,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wb34Xy6sYq",
                "forum": "anG2Y15mwc",
                "replyto": "kouGBkoi7x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1371/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1371/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response."
                    },
                    "comment": {
                        "value": "Thank you for carefully reviewing the discussions and deciding to raise the score. We are glad that we can modify the manuscript with your suggestion to make it easier to read and understand."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1371/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635169756,
                "cdate": 1700635169756,
                "tmdate": 1700635297548,
                "mdate": 1700635297548,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bHyJAG4n4n",
            "forum": "anG2Y15mwc",
            "replyto": "anG2Y15mwc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1371/Reviewer_4Vnn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1371/Reviewer_4Vnn"
            ],
            "content": {
                "summary": {
                    "value": "This paper develops a new paradigm for facial privacy protection based on the diffusion model. Once trained, this model can flexibly implement different facial privacy protection tasks during inference, such as anonymization and visual identity information hiding. Qualitative and quantitative experiments have demonstrated the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strength:\n++ The authors innovatively propose a new paradigm for facial privacy protection based on diffusion models. Quantitative and qualitative experiments have demonstrated the effectiveness of the proposed paradigm.\n++ The authors propose an MSI module to learn a set of SDM formats conditional embeddings of the original image and demonstrate that the embeddings extracted by this module have better editability and decoupling.\n++ The authors specially design an embedded scheduling strategy and an energy-based identity guidance module to guide the diffusion model, which makes the diffusion model effectively meet the needs of facial privacy protection tasks at the human perception level and machine perception level."
                },
                "weaknesses": {
                    "value": "Weakness:\n-- Although the author has significantly reduced the training cost of the model (including the need for high-quality facial datasets), the diffusion-based method for inference is still inefficient compared to the GAN-based method.\n-- I noticed that the author conducted an experiment on the security of keys. I think more interesting experiments can be conducted on key-I and key-E to explore their role in image generation."
                },
                "questions": {
                    "value": "Please find the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1371/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699154674490,
            "cdate": 1699154674490,
            "tmdate": 1699636064619,
            "mdate": 1699636064619,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x1ZrJubqcH",
                "forum": "anG2Y15mwc",
                "replyto": "bHyJAG4n4n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1371/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1371/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4Vnn"
                    },
                    "comment": {
                        "value": "Thank you for your comments and feedback. We address your concerns here.\n\n>Q1: Although the author has significantly reduced the training cost of the model (including the need for high-quality facial datasets), the diffusion-based method for inference is still inefficient compared to the GAN-based method.\n\nA1: Thank you for pointing out this issue. Our method inherits the diffusion model's intrinsic drawbacks, notably its sluggish inference speed. Nevertheless, noteworthy strides have been made in expediting sampling by constructing specialized samplers. This development aligns with our objectives, and we can leverage these advancements to enhance the sampling speed of our method in future implementations.\n\n>Q2: I noticed that the author conducted an experiment on the security of keys. I think more interesting experiments can be conducted on key-I and key-E to explore their role in image generation.\n\nA2: Thank you for your suggestion. We have followed your advice and conducted more experiments on the role of keys (as shown in Fig.7). While ensuring the correctness of key-I, we used the unconditional embedding of the diffusion model as the key-E for image recovery. The results indicate that the image can not be recovered correctly as long as there is a password error. Furthermore, key-I serves as the starting point for denoising, preserving some global information about the original image (such as background and human skin color). On the contrary, key-E contains more detailed information on the face."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1371/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700119395637,
                "cdate": 1700119395637,
                "tmdate": 1700119395637,
                "mdate": 1700119395637,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5tAKrbo4MZ",
            "forum": "anG2Y15mwc",
            "replyto": "anG2Y15mwc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1371/Reviewer_jBnK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1371/Reviewer_jBnK"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a face privacy-preservation method based on diffusion models that claims to achieve both anonymization and visual information hiding within a unified framework."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The paper addresses an important problem of enhancing privacy for face images shared on the Internet.\n2) The proposed solution appears to be somewhat novel and feasible, but it is hard to judge because sufficient details have not been provided."
                },
                "weaknesses": {
                    "value": "1) The paper is extremely hard to read and understand. A number of mathematical notations and terms (e.g., key-E, conditional embedding, etc.) are  not defined clearly, which makes the proposed approach hard to follow.\n\n2) First and foremost, the most basic requirement in a security/privacy paper is stating the threat model and assumptions explicitly. What information is being protected and from whom? What are the capabilities of the adversary? In this paper, no clear threat model has been presented. For example, the goals of anonymization and visual information hiding appear to be quite different. Is the proposed framework designed to meet both these objectives simultaneously? Or is the paper trying to propose a common framework that can achieve either one of these objectives at a time by slighting tweaking some parameter/loss?\n\n3) For anonymization (de-identification), it is not sufficient to show that \"anonymized\" faces cannot be recognized by a standard face recognition system. It is necessary to prove that the face cannot be \"deanonymized\" by a malicious adversary. In the proposed approach, it appears that complete recovery is possible provided the so-called \"keys\" are available. This makes the privacy dependent on the confidentiality of the \"key\". Even in the absence of the \"key\", it may be possible for an adversary to learn the inverse mapping between the original and anonymized faces if many training pairs of (original, anonymized) faces are available. Contrarily, if the face recognition system is finetuned with a few examples of anonymized faces as an augmentation, may be it will start recognizing anonymized faces correctly. It is important to discuss the feasibility of such attacks.\n\n4)  Similarly, in the case of identity recovery, it is not sufficient to show that reconstructed images have high perceptual similarity to the original images. It must shown that the reconstructed image matches with the original images with high probability using a face recognition system. \n\n5) The diversity component of the proposed solution is not well-motivated. What is the application need for ensuring diversity? If the diversity loss is removed completely, will the same facial identity be generated every time irrespective of the input noise pattern (added to the initial latent code). If yes, is the model learning a one-to-one mapping between the latent code and generated image irrespective of the noise?\n\n6) According to the introduction, \"face images processed by visual identity information hiding methods are unrecognizable to human observers but can be recognized by machine\". The images generated by the proposed method clearly shows the presence of a face image to a human observer. Only, the identity is partially hidden. In this case, how is this different from anonymization?\n\n7) What is meant by \"conditional embedding\" of an image? What is the role of the \"time modulation\" module, which has been prominently highlighted in Figure 2, but never described in the text?"
                },
                "questions": {
                    "value": "Please see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1371/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1371/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1371/Reviewer_jBnK"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1371/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699251724067,
            "cdate": 1699251724067,
            "tmdate": 1699636064563,
            "mdate": 1699636064563,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OlGv8Xyqv2",
                "forum": "anG2Y15mwc",
                "replyto": "5tAKrbo4MZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1371/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1371/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1 of Response to Reviewer jBnK"
                    },
                    "comment": {
                        "value": "Thank you for your comments and feedback. We address your concerns here.\n\n>Q1: The paper is extremely hard to read and understand. A number of mathematical notations and terms (e.g., key-E, conditional embedding, etc.) are not defined clearly, which makes the proposed approach hard to follow.\n\nA1: We have made our best efforts to explain mathematical notations and terms to make our manuscript easy to read and understand. In the revised manuscript, we have furnished a comprehensive introduction to the definition of conditional embeddings and the method for acquiring them in Sections 2.1 and 2.2. In brief, conditional embedding is the learnable vector in stable diffusion model's text conditional space, which can help the diffusion model reconstruct a given image. We designed the MSI module to learn the conditional embedding of the given image in the text conditional space (Eq.6 and Eq.7).\n\n>Q2: First and foremost, the most basic requirement in a security/privacy paper is stating the threat model and assumptions explicitly. What information is being protected and from whom? What are the capabilities of the adversary? In this paper, no clear threat model has been presented. For example, the goals of anonymization and visual information hiding appear to be quite different. Is the proposed framework designed to meet both these objectives simultaneously? Or is the paper trying to propose a common framework that can achieve either one of these objectives at a time by slighting tweaking some parameter/loss?\n\nA2: Thanks for your suggestion. Our privacy protection method aims to safeguard personal identity information and prevent its collection and misuse by threat models, such as illegal snoopers, unauthorized automatic recognition models, and malicious facial manipulation models. As you mentioned in your comment, there are notable distinctions between the objectives of anonymization and visual identity information hiding tasks. Consequently, existing methods can only accomplish one of these goals. In this paper, we propose a unified framework that can achieve either anonymization or visual identity information hiding tasks by slightly tweaking some parameters during inference.\n\n>Q3: For anonymization (de-identification), it is not sufficient to show that \"anonymized\" faces cannot be recognized by a standard face recognition system. It is necessary to prove that the face cannot be \"deanonymized\" by a malicious adversary. In the proposed approach, it appears that complete recovery is possible provided the so-called \"keys\" are available. This makes the privacy dependent on the confidentiality of the \"key\". Even in the absence of the \"key\", it may be possible for an adversary to learn the inverse mapping between the original and anonymized faces if many training pairs of (original, anonymized) faces are available. Contrarily, if the face recognition system is finetuned with a few examples of anonymized faces as an augmentation, may be it will start recognizing anonymized faces correctly. It is important to discuss the feasibility of such attacks.\n\nA3: Potential attackers lack the incentive to recover the original image as they are unaware that it is encrypted. This is especially true when the encrypted image closely resembles a real face image. Even if malicious adversaries are aware of encryption, they can not access paired data (only have the anonymized version). Only in the event of our model being fully leaked can these adversaries obtain paired data. However, it is widely agreed among all anonymization methods that encryption models remain invisible to attackers."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1371/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700115978833,
                "cdate": 1700115978833,
                "tmdate": 1700115978833,
                "mdate": 1700115978833,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VYDWjVn6yS",
                "forum": "anG2Y15mwc",
                "replyto": "5tAKrbo4MZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1371/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1371/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 2 of Response to Reviewer jBnK"
                    },
                    "comment": {
                        "value": ">Q4: Similarly, in the case of identity recovery, it is not sufficient to show that reconstructed images have high perceptual similarity to the original images. It must shown that the reconstructed image matches with the original images with high probability using a face recognition system.\n\nA4: We included the recognition rate of the recovered image in Table 1 of the previous manuscript, demonstrating that the recovered image generated by our method closely matches the original images when using the face recognition system. By speculating on the reviewer's intention, we have further calculated the perceptual similarity between the recovered image and the original image in the recognition model. Specifically, we input both images into the face recognition model to extract identity embedding and then calculate their Cosine similarity (Cos-IE). Cos-IE has higher precision than the recognition rate, which can better reflect the similarity between the recovered image and the original image in the embedding space of the recognition model. As shown in the table below, our method produces a high Cos-IE value compared to existing anonymization methods, indicating its superiority. In terms of visual identity information hiding task, our method achieves comparable performance in Cos-IE with AVIH. \n\nQuantitative comparison of recovered results. We compared our method with other anonymization methods (FIT, RIDDLE) and visual identity information hiding method (AVIH) in terms of image recovery performance. (F) and (A) represent that we use FaceNet and ArcFace as face recognition models, respectively. \n\n| Method     | FIT    | RIDDLE | Ours   | AVIH (F) | AVIH(A) | Ours   |\n|------------|--------|--------|---------|--------|----------|---------|\n| MSE        | 0.006  | 0.045   | **0.003**  | **0.003**    | 0.004  | 0.004  |\n| LPIPS      |0.051  | 0.192  | **0.037** | 0.216    | 0.109   | **0.059**  |\n| SSIM       | 0.762  | 0.494   | **0.854**  | 0.775    | 0.793   | **0.872**  |\n| PSNR       | 28.693 | 19.489 | **28.9**   | **32.306**   | 31.369   | 31.913 |\n| Cos-IE (F) | 0.896  | 0.774   | **0.956**  | 0.926    | 0.87   | **0.929**  |\n| Cos-IE (A) | 0.88   | 0.709 | **0.932**  | **0.919**    | 0.726   | 0.905  |\n\n\n>Q5: The diversity component of the proposed solution is not well-motivated. What is the application need for ensuring diversity? If the diversity loss is removed completely, will the same facial identity be generated every time irrespective of the input noise pattern (added to the initial latent code). If yes, is the model learning a one-to-one mapping between the latent code and generated image irrespective of the noise?\n\nA5: Diversity is crucial. If diversity is not emphasized, like some existing methods [1,2,3] do, the anonymous faces tend to look similar for different individuals. This makes it easy for an attacker to recognize that these faces are anonymized. Moreover, in certain situations, such as online conferences [4], the de-identified faces of participants should be distinct from one another. To ensure reliable identity protection, it is also important to consider variations in ethnicity, age, gender, and other facial features. Even after removing diversity loss (as shown in Fig.14), the model can still generate images with different identities because the noise added to the initial latent code varies. However, when a specific noisy latent code is used, the diversity of image identities generated decreases due to the identity dissimilarity loss guiding the anonymous image towards a subspace that is least similar to the original image."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1371/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700118975630,
                "cdate": 1700118975630,
                "tmdate": 1700137254154,
                "mdate": 1700137254154,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dz596snG9p",
                "forum": "anG2Y15mwc",
                "replyto": "CdACejuwzM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1371/Reviewer_jBnK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1371/Reviewer_jBnK"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. I have gone through it in detail, but I'm still not convinced about the threat model and assumptions. Hence, I would like to retain the original rating."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1371/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636124226,
                "cdate": 1700636124226,
                "tmdate": 1700636124226,
                "mdate": 1700636124226,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]