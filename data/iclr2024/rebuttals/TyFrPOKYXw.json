[
    {
        "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback"
    },
    {
        "review": {
            "id": "1T08ApXgir",
            "forum": "TyFrPOKYXw",
            "replyto": "TyFrPOKYXw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7372/Reviewer_NMqi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7372/Reviewer_NMqi"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel algorithm, Safe Reinforcement Learning from Human Feedback (Safe RLHF), to address the crucial challenge of balancing the performance and safety of large language models (LLMs). LLMs often face an inherent tension between the objectives of being helpful and harmless, which can confuse crowdworkers during training. Safe RLHF effectively decouples human preferences related to helpfulness and harmlessness, enabling separate reward and cost models. The safety concern of LLMs is formalized as an optimization task to maximize the reward function while satisfying specified cost constraints. Using the Lagrangian method, Safe RLHF dynamically adjusts the balance between these two objectives during fine-tuning. Experimental results demonstrate that three rounds of fine-tuning using Safe RLHF significantly improve the helpfulness and harmlessness of LLMs, surpassing existing value-aligned algorithms. This work is a significant contribution to enhancing the safety of AI systems based on LLMs by effectively addressing the tension between helpfulness and harmlessness during fine-tuning, offering a promising approach to mitigate harmful responses while improving model performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper's strength lies in its innovative approach, Safe Reinforcement Learning from Human Feedback (Safe RLHF), which addresses the critical challenge of striking a balance between helpfulness and harmlessness objectives in the training of large language models (LLMs). By decoupling human preferences from these objectives, the paper ensures unbiased feedback during data annotation and adaptively balances the trade-off between these inherently conflicting training goals using the Lagrangian method. Notably, Safe RLHF is the first integration of Safe RL and RLHF frameworks, incorporating a two-dimensional human annotation scheme and a safe training mechanism. Through three rounds of Safe RLHF fine-tuning, the paper effectively enhances the helpfulness of the base model while significantly reducing harmful responses, surpassing the performance of existing value-aligned algorithms. The release of all data and training codes enhances the paper's reproducibility and validates its findings, making it a valuable contribution to improving the safety and performance of AI systems based on LLMs."
                },
                "weaknesses": {
                    "value": "- Even though the paper is the first integration of Safe RL and RLHF frameworks, the contribution is limited in the case of Safe RL.\n- The comparison between the (reward and cost) model in Section 3.2 to the classic (reward and cost) signals in safe RL should be clarified more. \n- The convergence of the proposed methods may be hard to guarantee, and there are no related theoretical results."
                },
                "questions": {
                    "value": "- Figure 1 needs to be polished more, and the font there is a bit small.\n- Can we use any Lagrangian safe RL methods (TRPO-Lag, PPO-Lag) in Safe RLHF?\n- What is the difference between Safe RLHF and the off-policy Lagrangian safe RL methods, e.g., WCSAC (published in AAAI-21 and MLJ-23)?\n- With safe RLHF, how can we ensure the learning is stable and finally converge?\n- The stability of the reward and cost signals should be analyzed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This paper contains example data that may be offensive or harmful. But there are relevant prompts in the paper."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7372/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7372/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7372/Reviewer_NMqi"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7372/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698168174081,
            "cdate": 1698168174081,
            "tmdate": 1700324050730,
            "mdate": 1700324050730,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7UYEbffu4j",
                "forum": "TyFrPOKYXw",
                "replyto": "1T08ApXgir",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7372/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7372/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Reply to Reviewer NMqi (1/2)"
                    },
                    "comment": {
                        "value": "> **W1:** Even though the paper is the first integration of Safe RL and RLHF frameworks, the contribution is limited in the case of Safe RL.\n\n**Reply to W1:** Our primary goal is to propose **a new RLHF framework that addresses the challenging tension** between helpfulness and harmlessness goals in the field of LLM alignment, rather than introducing a new Safe RL algorithm. Instead, we aim to broaden the application scope of Safe RL algorithms, bringing new perspectives to both fields.\n\nHere, we reiterate our contributions:\n\n- Safe RLHF is **the first work to combine Safe RL with the RLHF framework**.  It is a comprehensive methodology that includes decoupled data collection, training of two different preference models, and fine-tuning through the integration of Safe RL. It formalizes safety as a constraint and dynamically adjusts it during training, aiming to navigate the inherent tension between helpfulness and harmlessness.\n- Our **extensive experiments** demonstrate the following conclusions:\n\n    - Fine-tuning with Safe RLHF framework effectively enhances the helpfulness and harmlessness of LLMs under human values (Section 5.2.1).\n    - Decoupling helpfulness from harmlessness effectively increases agreement among crowd workers when annotating preference data, reducing bias due to the contradiction (Section 5.2.2).\n    - Compared to a fixed ratio of optimizing for helpfulness and harmlessness, dynamically adjusting the trade-off between them during training more effectively guides the inherent tension between the two (Section 5.2.3).\n    - Providing preference-based scores, rather than just a binary safe/unsafe classifier, improves LLM safety more efficiently (Section 5.2.4).\n\n- We **release all the data and code** from our experiments in hopes of **contributing to the reduction of the high research costs in this field**. (Currently, they are in the supplementary materials.)\n\n**We sincerely hope that you could appreciate our paper and recognize our efforts and contributions.**\n\n---\n\n> **W2:** The comparison between the (reward and cost) model in Section 3.2 to the classic (reward and cost) signals in safe RL should be clarified more.\n\n**Reply to W2:** Compared to traditional Safe RL's reward and cost signals, Safe RLHF presents the following challenges:\n\n- **Human values are high-dimensional and complex, making it impossible to design reward and cost functions in a rule-based manner.** Instead, human preferences must be modeled from collected datasets. Thus, guiding crowd workers to provide high-quality annotations (Section 4.1), constructing prompt datasets for large language models (Section 5.1 Prompts and Red-teaming), controlling the distribution of prompt-response pairs in the dataset (Section 5.1 Preference Datasets), and training better preference models (Section 4.2) are among the issues that a comprehensive large language model alignment process must address.\n\n- The threshold for what is considered safe in human values cannot be formulaically defined. Therefore, **we propose a new training method for a Cost Model to model the safety threshold**, which is one of our contributions. Building on the Bradley-Terry model, we introduce a comparison loss between responses and a virtual response $y_0$ near the safety threshold, setting the cost of $y_0$ to zero. This design separates safe from unsafe responses at a cost boundary of zero (as shown in Figure 2(a)), making zero the safety threshold for an individual response."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7372/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700165903841,
                "cdate": 1700165903841,
                "tmdate": 1700165903841,
                "mdate": 1700165903841,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IQ5iOc2yXh",
                "forum": "TyFrPOKYXw",
                "replyto": "lC6RkLrqjf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7372/Reviewer_NMqi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7372/Reviewer_NMqi"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for their response.\n\nIn general, my concerns were addressed. I am happy to raise my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7372/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700324013444,
                "cdate": 1700324013444,
                "tmdate": 1700324013444,
                "mdate": 1700324013444,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KPQZcOiI9E",
            "forum": "TyFrPOKYXw",
            "replyto": "TyFrPOKYXw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7372/Reviewer_xNog"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7372/Reviewer_xNog"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm designed to address the challenge of balancing helpfulness and harmlessness in Large Language Models (LLMs). Safe RLHF decouples human preferences regarding helpfulness and harmlessness, allowing for separate training of reward and cost models. By leveraging the Lagrangian method, Safe RLHF dynamically adjusts the balance between these objectives during fine-tuning. Experimental results demonstrate significant improvements in both helpfulness and harmlessness compared to existing value-aligned algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-written and easy to follow. The authors present a well-defined methodology, including a clear description of the Safe RLHF pipeline, preference annotation process, and training algorithms for reward and cost models.\n\n2. Given the societal impact of LLMs, ensuring their safety and usefulness is of utmost importance. Safe RLHF presents a significant contribution by effectively aligning human values with model behavior, addressing an essential concern in AI research."
                },
                "weaknesses": {
                    "value": "1. The technique contributions seem incremental to me. The decoupling of rewards into rewards and costs is a standard formulation in CMDP, and the Lagrangian methods with RL are not new at all. \n\n2. Another concern in this paper is that I don't think there is an appropriate cost threshold and cost-reward trade-off in the LLM alignment settings. I think safety is cleary a priority when compared with preferences. So that being said, how do you define how much safety LLMs are to trade off the preference performance? If this is a super safe-critical scenario, the cost limit should be 0. That being said. A simple LLM finetuning reward function can be designed that A>B if (preference (A>B) and (A passes safety check threshold)). I do not see the necessity of using the cost-reward formulation of CMDP.\n\n3. Also, this concern is a follow-up to point 2. I think the authors failed to convince me that standard rewarding shaping is not good enough in this setting. Since Lagrangian methods need to try different threshold values, It is not enough to conclude Lagrangian methods is better than reward shaping for LLMs fine-tuning without trying different reward shaping coefficients."
                },
                "questions": {
                    "value": "1. Could you elaborate on the limitations and potential risks associated with Safe RLHF? For instance, are there scenarios where the algorithm might fail to balance helpfulness and harmlessness effectively? Understanding the limitations would provide a more nuanced perspective on the applicability of your approach.\n\n2. The paper lacks a discussion on the computational resources and time required for the Safe RLHF training process. Can you provide insights into the computational efficiency and scalability of your method, especially concerning large-scale LLMs?\n\n3. As for safe RL methods like Lagrangian methods, the cost threshold is super sensitive. A threshold that is too small might result in complete failures of policy training. I noticed that the authors selected negative cost thresholds for two experiments. How many values of cost threshold have you tried? And how robust is the Lagrangian method for RLHF to different threshold values?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7372/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7372/Reviewer_xNog",
                        "ICLR.cc/2024/Conference/Submission7372/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7372/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698714581434,
            "cdate": 1698714581434,
            "tmdate": 1700516811170,
            "mdate": 1700516811170,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EvkXgf8W8f",
                "forum": "TyFrPOKYXw",
                "replyto": "KPQZcOiI9E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7372/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7372/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Reply to Reviewer xNog (1/3)"
                    },
                    "comment": {
                        "value": "> **W1:** The technique contributions seem incremental to me. The decoupling of rewards into rewards and costs is a standard formulation in CMDP, and the Lagrangian methods with RL are not new at all.\n\n**Reply to W1:** We sincerely hope that reviewers will recognize that **our work is not incremental**, especially in **the domain of LLM safety**.\n\nLogically, we firstly decouple human value towards LLMs into two parts: helpfulness and harmlessness. After modeling harmlessness as a constraint that needs to be prioritized, we formalize the entire problem under the CMDP framework. Reversing this logical order is a disheartening accusation. It overshadows our efforts in\n\n- **guiding the crowdworkers to generate decoupling data** reflecting their preferences for helpfulness and harmlessness;\n- utilizing reward model and **our proposed novel cost model** to represent objective and constraint signals;\n- **integrating Safe RL with the training of LLMs.**\n\nStandard CMDPs indeed encompass both reward and cost signals. However, our innovation lies in how to **model, acquire, and utilize** such signals in the alignment of LLMs.\n\nFurthermore, we do not deny that the Lagrangian method is a mature technology in the field of traditional Safe RL. However, for our comprehensive Safe RLHF framework, it constitutes just one component. We do not claim the introduction of the Lagrangian method to solve Safe RL problem as our contribution. Here, we reiterate our contributions:\n\n- Safe RLHF is **the first work to combine Safe RL with the RLHF framework**.  It is a comprehensive methodology that includes decoupled data collection, training of two different preference models, and fine-tuning through the integration of Safe RL. It formalizes safety as a constraint and dynamically adjusts it during training, aiming to navigate the inherent tension between helpfulness and harmlessness.\n- Our **extensive experiments** demonstrate the following conclusions:\n\n    - Fine-tuning with Safe RLHF framework effectively enhances the helpfulness and harmlessness of LLMs under human values (Section 5.2.1).\n    - Decoupling helpfulness from harmlessness effectively increases agreement among crowd workers when annotating preference data, reducing bias due to the contradiction (Section 5.2.2).\n    - Compared to a fixed ratio of optimizing for helpfulness and harmlessness, dynamically adjusting the trade-off between them during training more effectively guides the inherent tension between the two (Section 5.2.3).\n    - Providing preference-based scores, rather than just a binary safe/unsafe classifier, improves LLM safety more efficiently (Section 5.2.4).\n\n- We **release all the data and code** from our experiments in hopes of **contributing to the reduction of the high research costs in this field**. (Currently, they are in the supplementary materials.)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7372/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700165774915,
                "cdate": 1700165774915,
                "tmdate": 1700165774915,
                "mdate": 1700165774915,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YlEM2ct05Y",
                "forum": "TyFrPOKYXw",
                "replyto": "9Mzz8tSgv4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7372/Reviewer_xNog"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7372/Reviewer_xNog"
                ],
                "content": {
                    "title": {
                        "value": "Appreciation and Further Clarifications on Technical Contributions"
                    },
                    "comment": {
                        "value": "I would like to acknowledge the diligent efforts undertaken to address some of my concerns, particularly pertaining to questions Q1, Q2, Q3, and Weakness3. However, I find it necessary to delve deeper into the discussion surrounding the novelty of Safe RLHF and the application of CMDP formulation, specifically addressing Weakness1 and Weakness2.\n\n- **The nolvety of Safe RLHF**: \n\nRegarding the novelty of Safe RLHF, the authors assert that their primary technical contributions lie in the *decoupling of human value towards LLMs into two distinct components: helpfulness and harmlessness, with a focus on guiding crowdworkers to generate decoupling data.* After revisiting the paper, I remain unconvinced of this being a significant technical contribution. Unlike other works in diverse domains, such as power grid systems [1], where specific challenges and domain requirements are met, I fail to identify a distinctive LLM-specific challenge necessitating innovative safe RL frameworks in this work.\n\nThe claimed innovation in *modeling, acquiring, and utilizing signals in the alignment of LLMs, specifically through CMDP formulation, guiding crowdworkers, and applying Lagrangian-based methods*, does not seem to present a compelling argument for an ICLR-level paper's technical contributions. While I appreciate the pioneering nature of this work in applying safe RL to LLMs, I believe it may be more suitable for the LLMs track rather than the safe RL track.\n\n\n\n[1] Glavic, M., Fonteneau, R., & Ernst, D. (2017). Reinforcement learning for electric power system decision and control: Past considerations and perspectives. IFAC-PapersOnLine, 50(1), 6918-6927.\n\n- **The necessity of using the cost-reward formulation of CMDP**: \n\nOn the matter of the necessity of using the cost-reward formulation of CMDP, I appreciate the additional clarifications provided. However, I wish to express further concerns regarding the simplicity of the proposed LLM finetuning reward function. I said in my review that *A simple LLM finetuning reward function can be designed that A>B if (preference (A>B) and (A passes safety check threshold))*. Actually, it might require a lot of efforts to make simple methods works (just like how RLHF worked for LLMs which required a lot of engineering and tuning). However, I understand that the authors could not possibly tune the other baseliens with that much effort. And I agree CMDP formulation is a good way to ease that difficulty.\n\n\nIn conclusion, while the authors have made commendable efforts to address some of my concerns, a few lingering reservations persist. I acknowledge the potential value this work holds for the LLMs community, given its pioneering application of safe RL to LLMs alignment, and have adjusted my overall assessment accordingly.\n\nThank you for your attention to these matters, and I look forward to any further clarifications or insights you may provide."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7372/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516792204,
                "cdate": 1700516792204,
                "tmdate": 1700516792204,
                "mdate": 1700516792204,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PFdXsk27jX",
            "forum": "TyFrPOKYXw",
            "replyto": "TyFrPOKYXw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7372/Reviewer_murU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7372/Reviewer_murU"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment in large language models (LLMs). Safe RLHF decouples human preferences regarding helpfulness and harmlessness, allowing separate training of reward and cost models. The safety concern of LLMs is formalized as an optimization task, and the balance between the two objectives is dynamically adjusted during fine-tuning. Through three rounds of fine-tuning using Safe RLHF, the paper demonstrates improved performance and harm mitigation compared to existing value-aligned algorithms."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. Separation of rewards and costs is an excellent idea that probably resolves the optimization contradiction in RLHF of LLM. \n2. The paper provides concrete experimental results demonstrating the effectiveness of Safe RLHF in enhancing model performance and reducing harmful responses."
                },
                "weaknesses": {
                    "value": "Minor suggestions in Questions."
                },
                "questions": {
                    "value": "1. Fig2.(a) the symbols of axis indexes are absent.\n2. Could you add the scatter point of Beaver-v2 and Beaver-v3 in Fig.6(a) and (b)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7372/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7372/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7372/Reviewer_murU"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7372/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769225323,
            "cdate": 1698769225323,
            "tmdate": 1699636881718,
            "mdate": 1699636881718,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZTL52pNIlI",
                "forum": "TyFrPOKYXw",
                "replyto": "PFdXsk27jX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7372/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7372/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Reply to Reviewer murU"
                    },
                    "comment": {
                        "value": "We greatly appreciate your valuable feedback and are encouraged by your recognition of the Safe RLHF framework as an excellent idea that probably resolves the optimization contradiction in the RLHF of LLM. It is gratifying to know that our efforts and dedication are acknowledged. Should you have any additional questions or suggestions, we warmly invite you to share your insights with us.\n\n---\n\n> **Q1:** Fig2.(a) the symbols of axis indexes are absent.\n\n**Reply to Q1:** We are very grateful to the reviewer for their comments on Figure 2(a). Indeed, we acknowledge that this figure lacks some necessary symbols and explanations, which we will add both here and in the newly submitted PDF version of the paper.\n\n**The red dashed line in Figure 2 represents the ambiguous boundary between safety and unsafety in human values.** By proposing a new training method for a preference model (cost model), we fit this ambiguous boundary while maintaining the original properties of the Bradley-Terry model. The introduction of this cost model is fundamental for integrating RLHF with Safe RL algorithms and is one of our main contributions. Specifically, building on the traditional preference model, we introduce a comparison loss between responses and a virtual response $y_0$ near the safety threshold, setting the cost of $y_0$ to zero (Equation 4).\n\n---\n\n> **Q2:** Could you add the scatter point of Beaver-v2 and Beaver-v3 in Fig.6(a) and (b)?\n\n**Reply to Q2:** As illustrated in the following figure, we added scatter plots for Beaver-v2 and Beaver-v3 on the basis of Fig.6(a) and Fig.6(b):\n\nhttps://anonymous.4open.science/r/Rebuttal-ICLR24/images/ablation.png\n\nHowever, it is important to note that Beaver-v2 and Beaver-v3 were initialized using Beaver-v1 and Beaver-v2 respectively. Given that these plots employ the win rate against Alpaca-7B as the axis, a direct comparison with other baselines in the figure is unfair. This is the primary reason they were omitted in the initial version of the paper. This is also why we chose to use the Elo score as the evaluation metric for Figure 5."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7372/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700165720798,
                "cdate": 1700165720798,
                "tmdate": 1700165720798,
                "mdate": 1700165720798,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1mO6jjKUVl",
            "forum": "TyFrPOKYXw",
            "replyto": "TyFrPOKYXw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7372/Reviewer_NDaV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7372/Reviewer_NDaV"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose safe rlhf, a framework to decouple helpfulness and harmfulness of RLHF model responses. They employ a dynamic \u03bb-trade-off to dual helpfulness and harmlessness objectives. They demonstrate that this approach also results in better inter-rater agreement thereby generating cleaner annotations for RLHF training. They define a cost model for harmlessness and pose the problem as a constrained optimization problem where they employ Lagrangian method to solve the same. Through three rounds of such safe RLHF iterations, they are able to demonstrate at 7B model scales that they are able to mitigate harmful responses while also improving model performance compared to standard value alignment algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "With safety being an important aspect in LLMs, this paper tackles an important question -- how to do value alignment under both the safety and usefulness axes. The paper is well written and explains the methodology involved clearly. Even if the techniques to accommodate safety costs into RLHF are simple and straightforward, the paper does a good job in explaining the motivation behind the choices and conducts careful ablations to demonstrate the motivations behind these choices. The evaluation methodology is also robust and the paper carefully evaluates the design choices. The paper also presents a clean framework to decouple different human values safety being one of them from the overall utility of the responses and thus can be extended to other desiderata easily."
                },
                "weaknesses": {
                    "value": "One thing that I feel the paper could do a better job of is to incorporate more safe RLHF baselines. For example, Constitutional AI [1] tackles a very similar problem balancing helpfulness and harmlessness. The only couple of ablations that I can see are of fixed lambda (reward shaping approach) and the approach used in Sparrow. I would have loved to see one or two more safe RLHF approaches that do not need to tow the lines of conventional RLHF exactly.\n\nThe improvement achieved in the RLHF stages over the base models is often a function of the SFT stage in between. Aspects of safety can also be incorporated in the SFT data and the paper uses Alpaca 7B off the shelf. If a fine-tuning stage could be done on the responses collected as part of the safety data (or special SFT data can be collected in this regard) and can be introduced as a step in between, then the gains of RLHF with the cost/preference models could be more clearly earmarked compared to the simpler SFT stages. This will help us understand how much value we get by framing this problem during the RL stage vs SFT stage vs both. The focus on the SFT aspect is one thing that find missing in this paper. \n\n\n1. Constitutional AI: Harmlessness from AI Feedback - Bai et al, Dec 2022."
                },
                "questions": {
                    "value": "1. I think you can do a slightly better job of rewriting the cost model explanation in Section 3.2. Particularly it was a bit time consuming to understand the motivations behind the loss formulation that had both the BT terms and also classification-like term. It would be nice to revisit this explanation in the final draft.\n\n2. More of a suggestion : I would recommend moving important things like Related Work to the main section of the paper instead of leaving them in Appendix. It would be important to position the work in the context of other relevant works. The page limitation could be adjusted by shortening and reformatting other sections or moving some of those to appendix.\n\n3. Were other safe RL baselines considered apart from Sparrow and reward shaping ?\n\n4. Is there any reason why stronger SFT models were not trained considering the fact that you gathered some safety related data before doing RLHF ?\n\n5. I am not sure whether there are clear explanations of how model selection was done both in the RM/Cost model training stage and RLHF training stage. Can you clarify ? Often one finds that model selection plays a much bigger role in final performance than even the training algorithm. Considering you do multiple RLHF iterations, I wanted to understand this aspect clearly. \n\n6. Again a minor suggestion : Some of the labels and text in the figures were hard to read at 100% resolution. Kindly resize them for appropriate reading."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7372/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7372/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7372/Reviewer_NDaV"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7372/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699356175922,
            "cdate": 1699356175922,
            "tmdate": 1700498504366,
            "mdate": 1700498504366,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vvKCl1leMj",
                "forum": "TyFrPOKYXw",
                "replyto": "1mO6jjKUVl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7372/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7372/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Reply to Reviewer NDaV (1/2)"
                    },
                    "comment": {
                        "value": "> **W1:** One thing that I feel the paper could do a better job of is to incorporate more safe RLHF baselines. For example, Constitutional AI tackles a very similar problem balancing helpfulness and harmlessness. The only couple of ablations that I can see are of fixed lambda (reward shaping approach) and the approach used in Sparrow. I would have loved to see one or two more safe RLHF approaches that do not need to tow the lines of conventional RLHF exactly.\n\n> **W2:** The improvement achieved in the RLHF stages over the base models is often a function of the SFT stage in between. Aspects of safety can also be incorporated in the SFT data and the paper uses Alpaca 7B off the shelf. If a fine-tuning stage could be done on the responses collected as part of the safety data (or special SFT data can be collected in this regard) and can be introduced as a step in between, then the gains of RLHF with the cost/preference models could be more clearly earmarked compared to the simpler SFT stages. This will help us understand how much value we get by framing this problem during the RL stage vs SFT stage vs both. The focus on the SFT aspect is one thing that find missing in this paper.\n\n> **Q3:** Were other safe RL baselines considered apart from Sparrow and reward shaping ?\n\n**Reply to W1W2Q3:** Thank you for your constructive feedback regarding our paper. We appreciate the opportunity to enhance our work based on your insightful suggestions. In response to your comments, we have conducted additional experiments with **Constitutional AI** and **Safety SFT** baselines to address both of the weaknesses you identified. Below is a summary of our response and findings:\n\nWin rates for three different methods compared to Aplaca-7B rated by GPT-4:\n| | Safe RLHF (ours; Beaver-v1) | Constitutional AI | Safety SFT |\n| :---: | :---: | :---: | :---: |\n| Helpfulness Win Rate | 71.8% | 40.2% | 53.6% |\n| Harmlessness Win Rate | 73.3% | 69.7% | 53.6% |\n\nOur experiments reveal interesting insights:\n\n**Helpfulness and Harmlessness Balance:** Neither Constitutional AI (40.2% helpfulness, 69.7% harmlessness) nor Safety SFT (53.6% for both) matched the performance of our Safe RLHF (Beaver-v1) approach, which achieved a helpfulness win rate of 71.8% and harmlessness win rate of 73.3%. This underscores **the challenge in balancing helpfulness and harmlessness** in large language models and highlights the efficacy of our approach.\nImpact of SFT on RLHF: Addressing your second concern, our findings suggest that while the SFT stage contributes to model safety, the gains from the RLHF stage, especially with advanced reward models like Beaver-v1, are more pronounced. **This distinction is crucial in understanding the relative impacts of SFT and RLHF stages on model performance.**\nOur additional experiments demonstrate that while incorporating safety in the SFT stage is beneficial, **the RLHF stage offers a more substantial improvement in balancing helpfulness and harmlessness**. The inclusion of new baselines provides a comprehensive view of the current landscape of our Safe RLHF method and validates the effectiveness of our approach.\n\nWe believe these enhancements address your concerns and significantly strengthen our paper's contributions to the field of large language models.\n\n**Implementation details of Constitutional AI and Safety SFT:**\n- **Constitutional AI:** We follow the Constitutional AI paper to revise the raw responses based on constitutional critiques. The Constitutional AI method requires a large model with the capability to follow complex instructions to generate critiques for the responses and then revise the responses. We use the GPT-3.5 model to revise the responses generated by Alpaca-7B using the same constitutional prompts from the Constitutional AI paper. We let the reward model tend to prefer the revised response over the original response. And apply the standard RLHF pipeline over the Alpaca-7B model.\n\n- **Safety SFT:** For the Safety SFT method, we first filter out preference pairs in our preference dataset if both responses are labeled unsafe. Then we finetune the Alpaca-7B model to fit the safer response in the preference dataset."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7372/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700165506914,
                "cdate": 1700165506914,
                "tmdate": 1700165506914,
                "mdate": 1700165506914,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tjRzXDN6oO",
                "forum": "TyFrPOKYXw",
                "replyto": "1mO6jjKUVl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7372/Reviewer_NDaV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7372/Reviewer_NDaV"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledgement of rebuttal"
                    },
                    "comment": {
                        "value": "Thanks I have read your rebuttal. I am happy with the additional baselines and the authors addressing other questions. Increasing my score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7372/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498346186,
                "cdate": 1700498346186,
                "tmdate": 1700498490835,
                "mdate": 1700498490835,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]