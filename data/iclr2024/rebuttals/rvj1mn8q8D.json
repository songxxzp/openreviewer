[
    {
        "title": "TextGenSHAP: Scalable Post-hoc Explanations in Text Generation with Long Documents"
    },
    {
        "review": {
            "id": "djUEAGOwNC",
            "forum": "rvj1mn8q8D",
            "replyto": "rvj1mn8q8D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2091/Reviewer_vxq9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2091/Reviewer_vxq9"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces \"TextGenSHAP\", a method designed to adapt the Shapley value for text generation tasks in Large Language Models (LLMs). TextGenSHAP addresses the scalability issues associated with traditional SHAP for LLMs with large inputs/parameters. The main contribution is its ability to provide real-time explanations for LLM outputs, especially in tasks like long-document question answering and document retrieval."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper addresses a timely issue and is well grounded in existing literature.\n- It covers a significant breadth: multiple steps such as speculative decoding and flash attention are considered to facilitate speed-ups.\n- The approach naturally allows flexible control over the runtime of the algorithm through its hyperparameters.\n- The use of (linked) interactive visualizations is commendable.\n- The adaptation of block sparsity and flash attention shows forward-thinking."
                },
                "weaknesses": {
                    "value": "- The main issue with this paper is its lack of readability:\n  - The description of the actual method is fairly sloppy, and there is no algorithm provided in the main text despite a reference to one (important parts of the paper are either not included or delegated to the appendix). There are few points throughout the paper that very clearly describe the process and contributions. As a result, the paper was quite difficult to read, despite the contributions made.\n  - No diagram or algorithm besides Figure 2, which is poorly captioned and ambiguous in places. The paper includes many details, but compelling and concise high level summaries are lacking.\n  - For instance, *\"In the traditional Shapley, log-probabilities are needed for every candidate output\"* should come at the start of section 3.1 and not the end (the start of the paragraph is less motivated and more confusing as a result).\n- The introduction of several techniques might complicate the implementation and debugging of the method. Some clarity on the ease of implementation or integration of these techniques would be valuable (the aforementioned speculative decoding, etc).\n- The improvements are less pronounced on the MIRACL dataset and performance appears varied."
                },
                "questions": {
                    "value": "1. The section discussing speculative decoding references grafting new answers onto a \"causal decoding tree\" and updating its attention matrix. Could you elaborate more on the nature, structure, and significance of the \"causal decoding tree\" mentioned?\n2. It would be interesting to know how TextGenSHAP performs with sampling techniques other than argmax.\n3. How closely do perturbed inputs need to resemble already decoded samples for speculative decoding to be efficient? Is there a threshold, and how was it determined?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2091/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2091/Reviewer_vxq9",
                        "ICLR.cc/2024/Conference/Submission2091/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698527966007,
            "cdate": 1698527966007,
            "tmdate": 1700671297851,
            "mdate": 1700671297851,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9snIfp9cIY",
                "forum": "rvj1mn8q8D",
                "replyto": "djUEAGOwNC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review and specific feedback"
                    },
                    "comment": {
                        "value": "> lack of readability\n\nWe deeply apologize for how the lack of readability significantly detracted from the contributions made by the work.  We have updated Section 3 to be much more friendly to readers without background knowledge in Shapley-based explanations and appreciate your specific feedback.  In particular, we have decided to make explicit the distinction between $v_\\ell$ and $v_p$, highlighting the log-probabilities earlier and more gradually motivating our contribution, among other changes.\n\n> No diagram or algorithm in the main text\n\nWe have updated the diagram for Figure 2 to focus on detailing the speculative decoding methodology we employ in our sampling algorithm.  We hope that this can give much greater insights into how the speculative decoding works, but also give some basic insights into how our sampling algorithm proceeds.  In particular, the algorithm proceeds by continuing to sample random subsets, compute the model output, and update the Shapley values.  This can be continued until convergence, but for our work we stop after a fixed number of samples.\n\n> elaborate more on the \u201ccausal decoding tree\u201d\n\nHopefully after the introduction of the new illustration in Figure 2, it is easier to understand how the `causal decoding tree\u2019 works.  Essentially, we are just constructed a data structure which holds on to all possible outputs we could want to generate (in a fashion similar to a trie) and are then also keeping track of all the necessary causal attention and position bias matrices needed to make this computation an exact copy of what would happen if we decoded autoregressively.\n\n> how closely do perturbed inputs need to resemble\u2026\n\nFrom this description, we can now say that: when we say that the \u201cperturbed inputs resemble one another\u201d, we mean that their generated outputs will be exactly the same.  We mean resemblance in the sense that (compared to existing literature applying speculative decoding) our application in explainability will have a much much higher likelihood that two inputs generate the same output than in other possible applications.\n\n> introduction of several techniques\n\nAn important detail of the several techniques which we implement is that they are fortunately decoupled from one another.  All of our other methods can be applied with or without Flash Attention and block sparsity.  Hopefully after our Figure 2 and responses, the speculative decoding method is clearer to understand, and it can also be seen how it is independent from everything else.  The in-place encoding is extremely simple to implement and FlashAttention is quickly becoming standard in a variety of machine learning packages.  We moreover plan to release the code for our work which should even further facilitate the ease of implementation.  \n\n> less pronounced on the MIRACL dataset\n\nHere, we actually believe that this does not represent a shortcoming of our method as discussed briefly in the results section.  As mentioned in the response to everyone, we have updated the appendix with specific MIRACL examples showing case scenarios where our method clearly answers correctly, but receives no AUC points because of the methodology used to construct the MIRACL dataset.  In particular, the MIRACL dataset human annotators only annotate 10 labels, specifically the top ten retrieved by an existing retrieval system. Accordingly, our method\u2019s ability to find a more diverse set of passages amongst extremely long contexts is not effectively captured by the \u201cMIRACL (Original)\u201d AUC metrics.  To address this, we explore using psuedolabels for the MIRACL dataset, and in this scenario our method consistently outperforms the baseline, emphasizing the effectiveness of our approach.\n\n> performs with sampling techniques other than argmax\n\nAssuming we are asked about the quantitative performance in terms of time, methods like K-beam will scale the memory linearly although the speculation aspect should not take any more time complexity.  If the question refers to the qualitative performance in terms of how the explanations will change, as we make the distribution less sharp by increasing K, we will consequently make the explanations less sharp as well.  We would be happy to try to answer further if you had a more specific interest.\n\nWe greatly appreciate your helpful and specific feedback and hope that these changes as well as our responses can help further clarify the contributions that we have made."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700013310536,
                "cdate": 1700013310536,
                "tmdate": 1700013310536,
                "mdate": 1700013310536,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "URNY1xwN2F",
                "forum": "rvj1mn8q8D",
                "replyto": "djUEAGOwNC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2091/Reviewer_vxq9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2091/Reviewer_vxq9"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the rebuttal"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for addressing my concerns, and for the updates made to the paper, in particular Figure 2.\n\nI have read the other reviews, and while I still think there are parts of the paper that could be improved e.g. the algorithm is still deferred to the appendix (I understand the difficulties with space constraints but this is not ideal in my opinion), overall I am raising my score to vote for acceptance of this paper (from 5 to 6)."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671266767,
                "cdate": 1700671266767,
                "tmdate": 1700671330618,
                "mdate": 1700671330618,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Rx2PNHM1D3",
            "forum": "rvj1mn8q8D",
            "replyto": "rvj1mn8q8D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2091/Reviewer_7NAw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2091/Reviewer_7NAw"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the post-hoc explanation of text generation with long documents. The key idea is to speed up the Shapley value explanation method with techniques such as speculative decoding. The experimental results show that the proposed method considerably speeds up the explanation and can be used as a document retrieval method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The problem of post-hoc explanation for text generation with long documents is interesting and timely.\n\n2. The paper used several speedup techniques to make the explanation faster and more affordable.\n\n3. The experiments show a practical use of the proposed method for document retrieval."
                },
                "weaknesses": {
                    "value": "1. Notations are not fully explained and are hard to parse. For example, in Notation paragraph, why is the input space of a language model defined as $\\mathbb{R}^d \\times \\mathcal{P}([d])$. What do these notations correspond to in plain English? In the same paragraph, $[.]_+$ is introduced but not used. It would be helpful if the authors could update the notation and check the consistency throughout the paper.\n\n2. The motivation for technical details is not fully explained. For example, in the current version, I do not understand why Eq (2) is better than Eq (1) in explaining text generation.\n\n3. Figure 4 shows that the proposed method is not always better than an extremely simple baseline (i.e., similarity score), especially on MIRACL.\n\n4. Lack of baselines. Although previous works on text generation explanation would take a long time to run, it would still be interesting to see their performances and time consumption. Currently, the paper does not include any baseline for explanation."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2091/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2091/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2091/Reviewer_7NAw"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634779496,
            "cdate": 1698634779496,
            "tmdate": 1700437174025,
            "mdate": 1700437174025,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OyYcLSJDBe",
                "forum": "rvj1mn8q8D",
                "replyto": "Rx2PNHM1D3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review and feedback"
                    },
                    "comment": {
                        "value": "> Notations are not fully explained and are hard to parse\n\nFirst, we hope that the updated Section 3 will be easier to parse than the previous version, but we still answer your specific questions directly here.\nThe input space of a masked text generation model defined as $\\mathbb{R}^d \\times \\mathcal{P}([d])$, is now written as $[V]^d \\times \\\\{0,1\\\\}^d$ which hopefully makes it clearer that ${x}\\in \\mathcal{X}=[V]^d$ represents the input sequence and ${s}\\in\\mathcal{M} = \\\\{ 0,1 \\\\}^d$ represents the binary input masks.  $[\\cdot]_+$  is now moved to be introduced right after it is used for the first and only time.\n\n> I do not understand why Eq (2) is better than Eq (1)\n\nWe have now further delineated Eq (1) and Eq (2) by forcing the different choice of $v_\\ell(\\cdot)$ and $v_p(\\cdot)$ respectively. The major difference is as follows. It is computationally infeasible to compute the full probability vector over the exponentially large space $[0,1]^{[V]^m}$, so we will instead turn to greedy decoding or K-beam decoding.  But using these algorithms will make the probability of generating certain outputs exactly zero, meaning we can no longer consider their log-probabilities and the entire Eq (1) becomes an ill-defined quantity. In contrast, Eq (2) can set some probabilities to zero without taking the logarithm, and the defined quantity is not only well-defined but should additionally be close to the Shapley-Shubik value as if we were to have the full exponentially-sized vector.\n\n>not always better than an extremely simple baseline (i.e., similarity score), especially on MIRACL\n\nIn the discussion of the results, we actually mention why we think this demonstrates the potential usefulness of our method, rather than a downside. Because the MIRACL dataset was constructed using the same similarity-score based approaches, there is a higher likelihood that a human annotator will give a positive label to the passages preferred by existing similarity-score approaches.  We have further added Section E to show exactly what we mean by this on specific examples from the MIRACL corpus and give further motivation for the pseudo-label approach as a fair comparison of the different methods.\n\n>Lack of baselines\n\nTo the best of our knowledge, there are no existing feature attribution methods which can be directly applied to seq-to-seq text generation.  The main category of explanations available for text-to-text generation are natural language explanations designed for end users.  We are now trying to adapt existing feature attribution methods to enable comparison and are currently running experiments to try to compare AUC results.  We hope to update you once they are completed."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700014229829,
                "cdate": 1700014229829,
                "tmdate": 1700014229829,
                "mdate": 1700014229829,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "apw6qChKBE",
                "forum": "rvj1mn8q8D",
                "replyto": "3Lzs8vzlxm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2091/Reviewer_7NAw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2091/Reviewer_7NAw"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response! I still have several questions.\n\n- What is the dimension of $\\psi_i$? If it is a high-dimensional vector, how do you convert it to a scalar as the explanation score?\n- Maybe the attention baseline can be updated in Table 1?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700426027791,
                "cdate": 1700426027791,
                "tmdate": 1700426027791,
                "mdate": 1700426027791,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LNkHQWxcfU",
                "forum": "rvj1mn8q8D",
                "replyto": "s2kSxJ6B9o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2091/Reviewer_7NAw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2091/Reviewer_7NAw"
                ],
                "content": {
                    "title": {
                        "value": "Raised my score"
                    },
                    "comment": {
                        "value": "Thank you for the clarification. My major concerns on the clarity are addressed, and I have raised my rating to 6."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700437476123,
                "cdate": 1700437476123,
                "tmdate": 1700437476123,
                "mdate": 1700437476123,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cYwGZ4UY36",
            "forum": "rvj1mn8q8D",
            "replyto": "rvj1mn8q8D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2091/Reviewer_VZk7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2091/Reviewer_VZk7"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces TextGen-SHAP, an innovative method that adapts the Shapley value for text generation tasks, ensuring faster computational speed especially tailored for large language models (LLMs). They prioritize the complex situation of providing explanations when employing lengthy input prompts, especially in tasks like abstractive question answering from extensive texts. Their approach is highlighted in three main areas: a) managing extended contexts with thousands of tokens, b) supporting vast models with billions of parameters, and c) promoting open-ended text generation, in contrast to tasks like classification. Additionally, the authors showcase how the explanations from TextGenSHAP can improve the efficiency of question answering from long documents."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper focuses on a significantly important question on how to explain LLMs' behaviors on text generation tasks.\n- The authors propose a straightforward and intuitive method for approximating and estimating the Shapley value on text generation task.\n- The paper is overall well-written and easy to read. The idea and method proposed in this paper are clearly illustrated and introduced, making the reader easy to understand."
                },
                "weaknesses": {
                    "value": "- On the token-wise explanations, Shapley-derived insights offer only superficial clarity on text generation tasks. The significant scores of tokens give minimal information to the audience. For instance, the tokens are sometimes not even readable words. Moreover, identical tokens in the starting prompt and following question context could have varying importance ratings, which can potentially confuse the audience.\n- The Shapley attributions on input tokens reveal unreliable explanations on text generation tasks, as the outputs of LLMs are usually uncertain. The output quality of LLMs usually highly depends on the instructions given to LLMs, which means the prediction changes may related to the changes in instructions rather than the input questions and other stuff.\n- TextGenSHAP claims to estimate Shapley Values. However, the proposed evaluation metrics are insufficient to support the claims. It is highly recommended to provide either theoretical analysis on the axiom of Shapley Value on the generated values from TextGenSHAP or empirical evidence with commonly used evaluation metrics (e.g., absolute error metrics or l2 error) that can directly reveal the values are similar to Shapley Values.\n- Some annotations are not stated clearly in the methodology section. For instance, $v(\\cdot)$ is only illustrated as a value function, but this is still vague under the settings of LLMs. According to the paper, the value function maps features to text outputs, which means the output of the value function is only the tokens but not the probability of each token. If this is the case, could the authors explain how the estimation of Shapley values in Eq. (2) has been processed?\n- The input capacity of T5 is limited to 512 tokens, which is considerably less than modern state-of-the-art LLMs like Vicuna that can handle around 2k tokens. When compared to other advanced LLMs, evaluations with an input size of 512 may be overclaimed as being effective for managing long inputs."
                },
                "questions": {
                    "value": "- The outputs produced by LLMs can be considerably influenced by various sampling strategies (e.g., temperature, search methods, etc.), which means the output probability can be very different even the LLMs receive the same input text. Are there specific approaches within your framework designed to address these challenges? Please correct any misinterpretation on my part. From my perspective, the mechanism proposed in this paper does not equip to tackle or circumvent these inherent difficulties when explaining the generative results of LLMs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2091/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2091/Reviewer_VZk7",
                        "ICLR.cc/2024/Conference/Submission2091/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828646351,
            "cdate": 1698828646351,
            "tmdate": 1700637085370,
            "mdate": 1700637085370,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BRO1zw7Jri",
                "forum": "rvj1mn8q8D",
                "replyto": "cYwGZ4UY36",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review and insights"
                    },
                    "comment": {
                        "value": "> output quality of LLMs usually highly depends on the instructions given to LLMs\n\n> identical tokens in the starting prompt \u2026 could have varying importance ratings\n\nWe apologize for the confusion regarding this.  Indeed, our method can easily be applied to the instructions or prompt of the LLM.  In our application, we are focusing on the explanations for the documents because it is meaningful for the QA task.  For future work, as you mention, an important aspect of LLMs to focus on is understanding the prompt.  For example, imagine an application where you are trying to design a good set of instructions.  Maybe you have five different prompts and they are all giving the same performance.  Using our method, you can see that prompt #1 is important for certain data samples while prompt #2 is important for other data samples.  Here, we believe it is a feature and not a bug that the importance rating will change for different data samples.  One can imagine that after seeing TextGenSHAP\u2019s explanation, you will be able to design a new prompt which has the important parts of prompt #1 and the important parts of prompt #2, or perhaps a pipeline which automatically selects the correct prompt to be used for each specific example.  (similar to how we automatically select the relevant documents in our application)\n\n> token-wise explanations \u2026 offer only superficial clarity\n\nAlthough we agree that tokens are sometimes not readable words, we believe that the tokens are a critical ingredient to providing an accurate explanation.  For example, the same English word can be tokenized in multiple different ways, and it is possible that the model prediction will change with different tokens, meaning our explanations should also change.  If your major issue is with tokens being too small and unreadable words, we think you will appreciate our hierarchy we used going from paragraphs \u2192 sentences \u2192 words, which puts less emphasis on small unreadable tokens.  We encourage you to check out our linked visualization.\n\n> TextGenSHAP claims to estimate Shapley Values. \n\nWe add some more detailed discussion on the Shapley value and typical approaches to the appendix, mentioning why this approximation is standard.  For greater details on this approximation and for L1 and L2 errors on toy datasets one can refer to [1,2].  We emphasize that such L2 metrics can only be calculated on toy datasets since for real datasets it is computationally infeasible to get the exact value.  Hopefully this can help explain why these sampling algorithms are standard in the first place.\n\n> Some annotations are not stated clearly\n\nHopefully our revamped notation and background section can alleviate these concerns.  The \u2018value function\u2019 terminology is standard for the Shapley value, but now we have changed from the one-hot output generation to writing $v(\\cdot)$ as a probability vector.  We have also delineated the log-likelihood value function and the probability value function to make the distinction clearer.  \n\n> T5 is limited to 512 tokens\n\nThe original T5 model was trained on 512 input tokens.  First, it should be mentioned that because of the relative position biases, it is possible to run the T5 model on any sized input text.  But moreover, the models which we used in our experiments are the T5-flan variants, which were trained on input contexts of 2048 tokens.  The vast majority of our experiments are run on 2K-4K tokens.  In any case, our major contributions also do not depend on a particular context length and can be applied regardless.\n\n> influenced by various samplings\n\nIndeed various search strategies can affect the output generations. Although temperature scaling will have a trivial linear effect on the output generations, other methods like K-beam and top-P sampling will affect the support of the generated output, consequently influencing the final output distribution.  We mention this briefly in Section 4.1, reiterating that our approach can easily be extended to handle any of these decoding methods, but we stick with greedy decoding which is standard for ODQA.\n\n\n[1] \u201cExplaining by Removing: A Unified Framework for Model Explanation\u201d, 2021. Covert et al.\n\n[2] \u201cSampling Permutations for Shapley Value Estimation\u201d, 2022. Mitchell et al."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700014589414,
                "cdate": 1700014589414,
                "tmdate": 1700014589414,
                "mdate": 1700014589414,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wvumgfm6sN",
                "forum": "rvj1mn8q8D",
                "replyto": "cYwGZ4UY36",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2091/Reviewer_VZk7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2091/Reviewer_VZk7"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your efforts."
                    },
                    "comment": {
                        "value": "I appreciate the authors for their thorough and detailed responses to my questions. While some of my concerns have been addressed, there remain aspects that are still unclear to me.\n\n- TextGenSHAP claims to estimate Shapley Values.\n\n  - The purpose of this question is to assess the faithfulness of the generated importance values and to understand how users can benefit from the highlighted heatmap of words or sentences. From my perspective, the case studies presented offer limited evidence to substantiate the high quality of the generated explanations from the proposed framework. Empirical studies focusing on both word-level and sentence-level explanations are strongly recommended. Such investigations would significantly enhance the framework's persuasiveness instead of purely providing case studies.\n\n  - If L2 metrics are not applicable in your scenario (possibly due to constraints from extended contexts), then considering the fidelity or faithfulness metric would be an excellent choice, as it effectively demonstrates the faithfulness of the explanations.\n\n- T5 is limited to 512 tokens\n  - Please provide the references or official documents of Flan-T5 that reveal Flan-T5 can handle up to 2048 context length. Please correct me if I am wrong. I did not find any comprehensive experiments or evidence in the original Flan-T5 paper that demonstrate its effectiveness with up to 2048 token contexts. Random case studies with few examples are not persuasive.\n  - How do the paper manage input sizes of 2k-4k tokens to produce valid outputs in Flan-T5? Considering that such token counts exceed Flan-T5's context length capacity (regardless of the length constraint being 512 or 2048 tokens), this excess can lead to inferior performance and cause invalid output values from LLMs [1,2]. Notably, the estimation of Shapley value in this work is based on the calculation of output values from Flan-T5. When the context length is exceeded, the output values from the value functions of Flan-T5 may become unreliable and inferior, consequently diminishing the reliability of the generated importance scores, which means the generated important scores are calculated based on random and chaotic outputs from Flan-T5. If the paper truncates the input context before processing it through Flan-T5, the resulting explanations could be even less faithful and reliable. This approach may risk eliminating key features or words that are essential for accurate explanations.\n\n[1] Press, Ofir, Noah A. Smith, and Mike Lewis. \"Train short, test long: Attention with linear biases enables input length extrapolation.\" ICLR 2022.\n\n[2] Chen, Shouyuan, et al. \"Extending context window of large language models via positional interpolation.\" arXiv preprint arXiv:2306.15595 (2023)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700440553235,
                "cdate": 1700440553235,
                "tmdate": 1700637721588,
                "mdate": 1700637721588,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lSyKX77IMc",
                "forum": "rvj1mn8q8D",
                "replyto": "cYwGZ4UY36",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2091/Reviewer_VZk7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2091/Reviewer_VZk7"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "I appreciate the authors' clarification and feedback, addressing some of my questions and concerns. However, I was not convinced by the statements from the authors to my follow-up questions. I will discuss these parts with other reviewers. \n\n- Metrics:\n\n  I think there may be some misunderstanding. Please refer to [1] for more introduction to fidelity and faithfulness metrics. From my understanding, fidelity or faithfulness metrics are precisely proposed for the situation when Shapley values or other explanation scores are not able to be calculated. Thus, fidelity and faithfulness metrics are applicable in the scenario of this paper. In my opinion, case studies are not convincing enough to prove the faithfulness of the generated explanation scores.\n\n- Exceed context length:\n\n  The consensus and observation from several advanced studies [2,3,4] are clear: LLMs are ineffective when inputs exceed their limited context length. The effective ways of solving this problem are neither the case I want to emphasize nor the case that this paper is focused on. I want to underscore the potential risks of directly using the value functions of LLMs, particularly in scenarios involving input lengths exceeding 2k tokens, as mentioned by the authors. The generated TextSHAP \"values\" may be severely impacted by LLMs' inferior and unreliable output value, leading to non-faithful explanation generation. Ensuring the faithfulness of generated Shapley-based \"values\" is the primary focus from my perspective.\n\n[1] Liu, Yang, et al. \"Synthetic benchmarks for scientific research in explainable machine learning.\" NeurIPS 2021.\n\n[2] Press, Ofir, Noah A. Smith, and Mike Lewis. \"Train short, test long: Attention with linear biases enables input length extrapolation.\" ICLR 2022.\n\n[3] Chen, Shouyuan, et al. \"Extending context window of large language models via positional interpolation.\" arXiv preprint arXiv:2306.15595 (2023).\n\n[4] Xiao, Guangxuan, et al. \"Efficient streaming language models with attention sinks.\" arXiv preprint arXiv:2309.17453 (2023).\n\nIn sum, despite not being completely convinced by the responses, the authors address some of my primary concerns, and I will increase my score accordingly. Please be sure to conclude the updated details in your latest draft, especially the misunderstanding on the context length limitation of Flan-T5."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637064289,
                "cdate": 1700637064289,
                "tmdate": 1700637944459,
                "mdate": 1700637944459,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "j6MNBvhMMY",
            "forum": "rvj1mn8q8D",
            "replyto": "rvj1mn8q8D",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2091/Reviewer_e5MV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2091/Reviewer_e5MV"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a very efficient method for calculating Shaply values for text generation with long documents. To speed up the computation, it proposes a set of techniques, including speculative decoding, flash attention and in-place encoding. Experiments show that the proposed method is applicable for long inputs, large models, and generative outputs, in the meantime, it decreases the computation time from hours to minutes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper provides us with an efficient method to calculate Shapley values for LLMs, solving the notorious problem of slowness in generating explanations. The improvement in the efficiency is impressive."
                },
                "weaknesses": {
                    "value": "1. Some essential details are omitted in the paper. For example, how to do speculative decoding is not clear to me. \n2. I'm not an expert in this field. I'm wondering whether the proposed decoding techniques lead to different generations.\n3. What's the limitation of the proposed method?"
                },
                "questions": {
                    "value": "Please see the weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699225458295,
            "cdate": 1699225458295,
            "tmdate": 1699636141451,
            "mdate": 1699636141451,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XjYoytC458",
                "forum": "rvj1mn8q8D",
                "replyto": "j6MNBvhMMY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for tending to accept this paper"
                    },
                    "comment": {
                        "value": ">What's the limitation of the proposed method?\n\nIt is first important to recognize that this work is seemingly the first work to yield feature-based explanations for a text-to-text generative model.  Accordingly, we cannot directly compare its limitations to other currently available feature attribution methods.  Nevertheless, the primary constraint of high-fidelity explanations like the Shapley value is usually their time complexity.  Accordingly, a limitation of the current work is the need to further verify on a wider variety of models and tasks.  In particular, multi-GPU and multi-TPU workbenches are left unexplored in the current work.\n\n>speculative decoding is not clear to me\n\nWe first emphasize that speculative decoding is an exact computation of the originally decoded likelihoods, but just decodes in a more efficient way.  We have now clarified this in the text and updated Figure 2 to focus on explaining how the speculative decoding method gradually constructs a tree of possible outputs which can then all be computed in a single pass of the decoder (instead of needing multiple autoregressive passes when decoding one-by-one).  We are happy to answer further details which are still unclear after our new figure."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700013521726,
                "cdate": 1700013521726,
                "tmdate": 1700013521726,
                "mdate": 1700013521726,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]