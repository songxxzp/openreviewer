[
    {
        "title": "SEABO: A Simple Search-Based Method for Offline Imitation Learning"
    },
    {
        "review": {
            "id": "3RTKcBiSPq",
            "forum": "MNyOI3C7YB",
            "replyto": "MNyOI3C7YB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1645/Reviewer_LUNW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1645/Reviewer_LUNW"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce an offline imitation learning algorithm called SEBO, which is centered on the task of learning a reward function from a dataset of expert demonstrations and applying it to unlabeled data to facilitate offline reinforcement learning.\n\nThe key innovation in SEBO lies in its use of a straightforward metric for generating the reward function, without the use of any neural networks. The algorithm employs search algorithms to determine the reward function. Specifically, SEBO constructs a KD-tree based on the expert demonstrations. For each unlabeled data point, the algorithm queries the KD-tree to identify its closest neighbor in the expert dataset and assesses the proximity between them. If the distance is minimal (indicating similarity to the expert trajectory), a high reward is assigned; conversely, if the distance is substantial (indicating deviation from the expert trajectory), a low reward is assigned.\n\nThe paper evaluates the SEBO algorithm across a range of MuJoCo environments and demonstrates its performance."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well written, and the algorithm is studied well on different MuJoCo environments. The authors also conduct a sensitivity analysis with respect to the two parameters $\\alpha$ and $\\beta$\n2. SEBO is efficient, and easy to implement. It costs a minimal overhead over existing Offline RL algorithms, and does not involve training of a new neural network.  \n3. The authors also evaluate it in a scenario where there is only access to observations, a case that might be of real world importance."
                },
                "weaknesses": {
                    "value": "**Evaluations restricted to deterministic environments**\n\nAll evaluations performed in this paper are conducted on deterministic environments (transitions in MuJoCo are completely deterministic), the claim that just one expert trajectory is sufficient might not be true if the environments are stochastic. For example, you may have a good (s,a,s\u2019) pair in the dataset, but you may assign a lower reward to it as its not present in the expert demonstration.  I think there is an inherent correlation between the number of expert trajectories needed and the stochasticity of the environment. \n\n**The paper lacks theoretical justifications, which makes understanding some parts a little difficult. For instance,**\n\nThis method might not work in situations where there are multiple ways to solve the same task. \nConsider the following example of a navigation problem, where the goal is to navigate to the destination from start position, and there are two ways to solve this task. One that goes left and reaches the goal, the other that goes right and reaches the goal.  Suppose the expert demonstrations takes the left path, and the unlabeled demonstrations (say from an expert policy as well) are from the right path, SEBO will assign low rewards to all transitions and not learn a good policy while BC will probably work."
                },
                "questions": {
                    "value": "1. Is there an inherent assumption being made between the distribution of state-action pairs in the expert demonstrations and the unlabeled dataset? \n2. Why are the experiments on D4RL restricted to medium level task and not conducted on expert and random versions of it? \n3. Does the stochasticity of environments affect the performance of SEBO? \n4. When using point wise matching to determine the reward are you making some inherent assumptions on the transition kernel?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1645/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1645/Reviewer_LUNW",
                        "ICLR.cc/2024/Conference/Submission1645/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1645/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698168636719,
            "cdate": 1698168636719,
            "tmdate": 1700499573227,
            "mdate": 1700499573227,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "93pLuCrgrw",
                "forum": "MNyOI3C7YB",
                "replyto": "3RTKcBiSPq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1645/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Responses to Reviewer LUNW (part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thoughtful reviews. We appreciate that the reviewer thinks that our paper is well-written and our method is simple yet effective. Below we try to address the concerns from the reviewer. If we are able to resolve some concerns, we hope that the reviewer will be willing to raise the score.\n\n**Concern 1: evaluations restricted to deterministic environments**\n\nWe agree that our evaluations are conducted in deterministic environments. We respectfully argue that most offline IL/offline RL algorithms are evaluated on deterministic environments, and the dynamics in many scenarios (e.g., robotic manipulation) are completely deterministic. Meanwhile, we do not claim that our method is all-around and can handle all scenarios (it is difficult for one single algorithm to do so). With only one expert trajectory, SEABO may fail to produce satisfying rewards for learning in stochastic environments. However, we do not think the *potential* incapability of handling stochastic environments downgrades the contribution of our work.\n\nWe agree that, intuitively, there may exist an inherent correlation between the number of expert trajectories needed and the stochasticity of the environment. We deem that it is interesting to check whether the stochasticity of environments affects the performance of SEABO. Nevertheless, as far as we can tell, there are few standard, widely adopted benchmarks with stochastic continuous control environments. Some prior work (e.g., stochastic D4RL Mujoco suite in Appendix C.2 in [1]) introduce stochasticity to the environment by modifying the reward in the dataset, which is somewhat incompatible with SEABO since there are no rewards in the dataset in our setting. We then introduce stochasticity by injecting Gaussian noises (with mean 0 and std $\\sigma$) to observations during evaluation, following prior work [2]. We try std=0.05 and std=0.1, respectively, and conduct experiments on MuJoCo datasets. We use IQL as the base algorithm and follow the same hyperparameter setup of SEABO in MuJoCo tasks. We summarize the results below.\n\n| Task Name | IQL (oracle, std=0.05) | IQL+SEABO (std=0.05) | IQL (oracle, std=0.1) | IQL+SEABO (std=0.1) |\n| ---- | :---: | :---: | :---: | :---: |\n| halfcheetah-medium-v2 | 40.9$\\pm$1.1 | 40.6$\\pm$0.6 | 31.8$\\pm$0.8 | 30.4$\\pm$0.2 |\n| hopper-medium-v2 | 45.2$\\pm$1.8 | 52.4$\\pm$3.1 | 33.5$\\pm$2.6 | 31.1$\\pm$2.8 |\n| walker2d-medium-v2 | 76.8$\\pm$4.0 | 80.2$\\pm$3.2 | 66.6$\\pm$2.2 | 74.3$\\pm$5.0 |\n| halfcheetah-medium-replay-v2 | 38.6$\\pm$0.9 | 39.5$\\pm$1.2 | 30.0$\\pm$1.6 | 26.4$\\pm$3.1 |\n| hopper-medium-replay-v2 | 51.8$\\pm$8.2 | 71.5$\\pm$7.9 | 31.7$\\pm$7.0 | 33.1$\\pm$4.1 |\n| walker2d-medium-replay-v2 | 61.4$\\pm$7.5 | 69.2$\\pm$4.3 | 51.8$\\pm$5.5 | 53.5$\\pm$7.3 |\n| halfcheetah-medium-expert-v2 | 36.5$\\pm$1.0 | 34.2$\\pm$2.9 | 26.8$\\pm$1.5 | 24.7$\\pm$1.0 |\n| hopper-medium-expert-v2 | 30.4$\\pm$5.3 | 35.8$\\pm$7.1 | 34.0$\\pm$4.8 | 29.5$\\pm$3.3 | \n| walker2d-medium-expert-v2 | 101.4$\\pm$4.6 | 107.6$\\pm$0.7 | 87.1$\\pm$9.1 | 90.5$\\pm$6.5 |\n\nTable 1. Experimental results in stochastic environments. Oracle means that raw rewards are used for training. The results are averaged over 5 runs.\n\nBased on the results, we find that under stochastic environments, the base algorithm itself fails to behave well (we observe performance collapse on many datasets like halfcheetah-medium-expert-v2). The performance of both IQL and IQL+SEABO decreases with larger noise std. Note that SEABO still outperforms vanilla rewards on many datasets under stochastic environments.\n\n[1] Ma, Y., Jayaraman, D., Bastani, O. Conservative offline distributional reinforcement learning. \n\n[2] Guo, Y., Oh, J., Singh, S., Lee, H. Generative adversarial self-imitation learning.\n\n**Concern 2: the paper lacks theoretical justifications**\n\nWe would like to clarify that the main contribution of this paper lies in the empirical side. Many previous offline IL papers like OTR [3] also do not include theoretical analysis. Our concern lies in the fact that theoretical analysis often relies heavily on assumptions that may be hard to satisfy in practice, resulting in a gap between theory and empirical results.\n\n[3] Luo, Y., Jiang, Z., Cohen, S., Grefenstette, E., Deisenroth, M. P. Optimal Transport for Offline Imitation Learning."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235881243,
                "cdate": 1700235881243,
                "tmdate": 1700235881243,
                "mdate": 1700235881243,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rL5yxXWrJg",
                "forum": "MNyOI3C7YB",
                "replyto": "SUbJSAQ4F6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1645/Reviewer_LUNW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1645/Reviewer_LUNW"
                ],
                "content": {
                    "title": {
                        "value": "Addressed all concerns"
                    },
                    "comment": {
                        "value": "I applaud the detailed rebuttal provided by the authors. The authors have addressed my concerns, and I have modified my score accordingly."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499711682,
                "cdate": 1700499711682,
                "tmdate": 1700499711682,
                "mdate": 1700499711682,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QrE7JPWxW5",
            "forum": "MNyOI3C7YB",
            "replyto": "MNyOI3C7YB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1645/Reviewer_u2Fn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1645/Reviewer_u2Fn"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the \"SEABO\" algorithm, which is an imitation learning algorithm that utilizes an expert dataset along with an unlabeled dataset. SEABO annotates the unlabeled dataset with rewards based on the distance between each state and the closest state in the expert dataset, and then runs an offline RL algorithm on the annotated dataset. The authors show improvements on the D4RL benchmark compared to prior baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper proposes a very simple algorithm that achieves good performance relative to more complex methods. I think this type of work has good value to the community in that it introduces easy-to-reproduce results and discourages over-engineering of methods.\n\n- The paper is clear in presentation and mostly well written."
                },
                "weaknesses": {
                    "value": "- There is only empirical analysis of the proposed method. I believe there are certain tasks where SEABO would perform poorly, such as a cliff-walking type of task where there is a precise boundary between what is accetable and what is a failure. \n\n- I hypothesize that the approach will also only work in lower dimensional control environments. This is because the method relies heavily on a  distance function, and this could suffer from the curse of dimensionality in more complex environments.\n\nMinor:\nSearch in the context of RL and planning typically has a slightly different connotation, which is using some type of tree-based or trajectory-shooting method that optimizes some cost function. In this work search is only used to find states close to an expert. I'm not sure what can be done with this in the writing, but I was expecting a method in the former category after reading the abstract. It may be better to replace \"search\" with \"nearest neighbors\" to be more specific."
                },
                "questions": {
                    "value": "\"we hypothesize that the transition is near-optimal if it lies close to the expert trajectory\" -> Is this strategy always good or does it have weaknesses? I would imagine that certain environments with discontinuities or discrete events (e.g. a car crash) would not favor this strategy.\n\nThere is another simple baseline commonly used in offline RL, which is sometimes referred to as percent-BC, which is to imitate the top N% of trajectories in the offline dataset (such as N=10 or N=25). As it is somewhat similar in spirit to SEABO, it would be good to see comparisons to this approach.\n\nIs there any effect of the \"curse of dimensionality\" for higher dimensional states?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1645/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698551986365,
            "cdate": 1698551986365,
            "tmdate": 1699636092658,
            "mdate": 1699636092658,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l7A0hvty2o",
                "forum": "MNyOI3C7YB",
                "replyto": "QrE7JPWxW5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1645/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Responses to Reviewer u2Fn (part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments. We provide point-to-point clarification to the concerns of the reviewer. Please check our responses below.\n\n**Concern 1: there are only empirical analysis and there are certain tasks where SEABO would perform poorly**\n\nWe have to admit that our paper mainly lies on the empirical side, mainly due to the fact that theoretical analysis often incurs a gap between theory and practical implementation or empirical results, and some assumptions made for the benefit of theoretical derivation may not necessarily hold in practice. We agree that a theoretical analysis can be a nice addition to our work, and can leave that for future exploration.\n\nIt is interesting to explore how our method behaves when there is a precise boundary between what is acceptable and what is a failure. To that end, we use the risky pointmass environment from [1] (please see the environment code in https://github.com/JasonMa2016/CODAC/blob/main/env/risky_pointmass.py and the details of risky pointmass environment in Appendix C.1 of the CODAC paper) and make some slight modifications. In the original pointmass environment, the agent gets a penalty with some probability. To make the boundary more precise, we directly assign the agent a penalty if it steps into the risky region (line 107-111). That is,\n```\nif not self.is_safe(self.state):\n    u = np.random.uniform(0, 1)\n    if u > self.risk_prob:\n        cost = 1\n        reward -= self.risk_penalty\n```\nis modified to\n```\nif not self.is_safe(self.state):\n    cost = 1\n    reward -= self.risk_penalty\n```\n\nOther codes are kept unchanged. We collect the offline dataset by training an SAC agent for 1M interactions. We choose the trajectory with the highest return as the expert trajectory. We train an IQL agent upon the collected dataset, and set the reward scale $\\alpha=1$ and coefficient $\\beta=0.5$ for SEABO. As the environment is simple, we train the IQL agent for 500K gradient steps (hyperparameters are set to be identical to MuJoCo tasks) and summarize the experimental results below. In the risky pointmass environment, some of the data in the expert trajectory can be close to the risky region, while it turns out that IQL+SEABO still beats IQL with vanilla reward.\n\n\n| Task name | $\\mu\\_{\\rm{max}}$ | $\\mu\\_{\\rm{min}}$ | IQL (oracle) | IQL+SEABO |\n| ---- | :---: | :---: | :---: | :---: |\n| risky pointmass | -1.1| -11083.3 | -149.3$\\pm$62.1 | **-98.3$\\pm$13.2** |\n\nTable 1. Results of SEABO in risky pointmass across 5 different random seeds. $\\mu\\_{\\rm{max}}$ denotes the highest return in the dataset and $\\mu\\_{\\rm{min}}$ denotes the smallest return in the dataset.\n\nNevertheless, we agree that under some extreme cases (e.g., the expert trajectory is very close to the risky boundary), our method may fail. We actually do not expect that our method can handle all possible situations (and it is hard to do so). If one is concerned with the existence of a precise boundary of failure, one can add some rules to post-process the rewards calculated by SEABO (e.g., penalize those query states that are too close to the boundary).\n\n\n[1] Ma, Y., Jayaraman, D., Bastani, O. Conservative offline distributional reinforcement learning.\n\n**Concern 2: SEABO will only work in lower dimensional control environments**\n\nIt is true that MuJoCo tasks are low-dimensional control tasks. However, in the main text, we also evaluate SEABO on Adroit tasks, which controls a 24-DoF simulated Shadow Hand robot to accomplish different tasks like hammering a nail. The observation dimension of `pen-human-v0` gives 45, and the observation dimension of `hammer-human-v0` gives 46. We further include experiments on kitchen datasets, whose observation space gives 60 (which we think can be regarded as high dimensional control tasks), and the results are shown below. Based on the results in Adroit tasks (Table 3 in the paper) and kitchen tasks, we believe SEABO can also work in high dimensional control environments (the curse of dimensionality does not seem to be an issue here).\n\n| Task name | BC | CQL | IQL (oracle) | IQL+SEABO |\n| ---- | :---: | :---: | :---: | :---: |\n| kitchen-complete-v0 | 65.0 | 43.8 | 62.5 | **67.5$\\pm$4.2** |\n| kitchen-partial-v0 | 38.0 | 49.8 | 46.3 | **71.0$\\pm$4.1** |\n| kitchen-mixed-v0 | 51.5 | 51.0 | 51.0 | **55.0$\\pm$3.5**|\n| average score | 51.5 | 48.2 | 53.3 | **64.5** |\n\nTable 2. Results on kitchen environments across 5 different random seeds.\n\nHowever, it may be challenging to apply SEABO on pixel-based tasks (which are of high dimension and SEABO may suffer from the curse of dimensionality), which we leave as future work since we mainly focus on state-based tasks in this paper. As we stated in Appendix E, one possible solution is to leverage a pre-trained encoder to convert the high-dimensional input into low-dimensional representations. Then, we can apply SEABO to acquire rewards."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234124397,
                "cdate": 1700234124397,
                "tmdate": 1700234124397,
                "mdate": 1700234124397,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wMNjyJF5T0",
                "forum": "MNyOI3C7YB",
                "replyto": "9GYtmhmKv5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1645/Reviewer_u2Fn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1645/Reviewer_u2Fn"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response."
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for the clarifications. On one hand I appreciate the improved experimental results, but also the same limitations that have been highlighted by the other reviewers (lack of a rigorous theoretical analysis, difficulty lack of scaling the distance metric into higher dimensional spaces). I think a score of weak accept is fair for this work, so I will be maintaining my score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668199372,
                "cdate": 1700668199372,
                "tmdate": 1700668199372,
                "mdate": 1700668199372,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qc7N4XfuXW",
            "forum": "MNyOI3C7YB",
            "replyto": "MNyOI3C7YB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1645/Reviewer_m3fB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1645/Reviewer_m3fB"
            ],
            "content": {
                "summary": {
                    "value": "The paper proses an approach for imitation learning using nearest-neighbor-based reward computation and offline RL. Given a dataset of unlabeled environment interactions and a single demonstration, they use a KD-tree to compute the distance of each transition to it's nearest neighbor in the demo (euclidean distance) and use this as a pseudo-reward which they can optimize with offline RL. The method is evaluated extensively in D4RL locomotion (and few manipulation) tasks and shows strong performance over prior works."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea is simple and well-explained in the paper. The empirical validation is thorough, with results across many D4RL locomotion tasks and a small number of manipulation tasks. A representative set of baselines is used and the method shows strong empirical performance. The simple proposed method beats more complex alternatives that need to use optimal transport etc."
                },
                "weaknesses": {
                    "value": "I don't have many issues with the paper. The method has some limitations (see below), but I don't think this invalidates the contributions of the current paper. \n\nOne minor weakness is that the approach is mostly evaluated on locomotion tasks for which precision is not the most critical. It could be great to evaluate it on a challenging, long-horizon manipulation task to test the limits of the method. For example the IKEA Furniture assembly benchmark could provide a nice test bed and I would be curious how well the proposed method performs.\n\nAnother minor point is that the paper lacks explanation how the euclidean distance is computed on the transition tuple. Do you compute euclidean distance on state, action and next state separately and then sum them? is there a weighting? Providing some details on this would be great!\n\n\n### Commentary on Limitations\n\nThe proposed method seems to have two limitations: \n\n(1) it's unclear how well it would work with visual observations where computing distances and nearest neighbors is a lot more challenging -- the authors mention this as a direction for future work and I agree that this is a limitation shared by many similar works so I wouldn't hold it against this paper. It should be mentioned though that the proposed approach with it's reliance on an analytic distance metric my be more troublesome in such scenarios than e.g. methods that use discriminator-based \"distances\".\n\n(2) it's unclear how well this would work in tasks that require very precise manipulation, since in such scenarios the proposed method would still assign high rewards to states that are only \"nearly\" demonstration states but may fail to perform the task in practice. It may be that methods that match trajectory segments instead of individual transitions fare better here?"
                },
                "questions": {
                    "value": "--\n\n# Post Rebuttal Comments\n\nThank you for answering my review.\n\nI appreciate the new experimental results -- performance on Kitchen seems strong and should be included in the paper.\n\nRegarding the furniture assembly benchmark: if offline imitation algorithms struggle in this benchmark, would it be feasible to apply your algorithm in an online context? Also, note that there is a new version of the benchmark (IKEA Furniture Bench, https://clvrai.github.io/furniture-bench/) -- while its focus is on real-world manipulation, it also comes with a simulated counterpart and offline dataset -- you could check that one out as well!\n\nIn any case, thank you for adding the experiments and I maintain my recommendation of acceptance. I also skimmed the other reviews and it seems that all reviewers are in agreement that the paper should be accepted."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "--"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1645/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1645/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1645/Reviewer_m3fB"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1645/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698625118704,
            "cdate": 1698625118704,
            "tmdate": 1701037312061,
            "mdate": 1701037312061,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BYCoizJHKQ",
                "forum": "MNyOI3C7YB",
                "replyto": "qc7N4XfuXW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1645/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Responses to Reviewer m3fB (part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful review. We appreciate that the reviewer thinks that our empirical validation is thorough and the performance of SEABO is strong. Please find our clarifications to the concerns below.\n\n**Concern 1: evaluation on a challenging, long-horizon manipulation task to test the limits of the method**\n\nWe thank the reviewer for recommending the IKEA Furniture assembly benchmark [1]. We think it is interesting to check how well our method behaves under manipulation tasks that require high precision. We adopt the skill-chaining [2] repository (https://github.com/clvrai/skill-chaining) from the IKEA benchmark author for running experiments, which contains the furniture benchmark. We implement the IQL algorithm on top of `sac_agent.py` in the `robot_learning` repository. We set the expectile=0.7, and tempreature=3.0 in IQL (default values for MuJoCo tasks). We set `control_type=ik` for demonstrate generation and RL training. We use the provided `furniture_sawyer_gen.py` script to generate 200 demonstrations. Though this benchmark supports IL algorithms, it seems to be prepared for *online IL algorithms*. Since no offline datasets are available, we train an SAC agent online for 1M steps and store its collected trajectories. We acquire the offline dataset by combining the demonstration data and the online interaction data. We randomly choose one demonstration from the generated demonstrations as the expert demonstration. We train the IQL agent offline for 1M gradient steps. Due to the fact that the whole procedure is time-consuming (collect online data, generate demonstration, train offline RL algorithm, evaluate offline policy), we only conduct experiments on two tasks from the IKEA benchmark, `chair_ingolf_0650` and `three_blocks`. We set `--num_connects=4, --max_episode_steps=800` for `chair_ingolf_0650` and `--num_connects=2, --max_episode_steps=400` for `three_blocks`. For SEABO, we set reward scale $\\alpha=1.0$, coefficient $\\beta=0.5$ for these tasks. We summarize the results below (as we are not familiar with this benchmark, we report some results based on the test logging information). We take the results of BC from [1].\n\n|  | BC | IQL (oracle) | IQL+SEABO |\n| ---- | :---: | :---: | :---: |\n| `chair_ingolf_0650` (phase completion) | 1.0 | 1.0$\\pm$0.0 | 1.0$\\pm$0.0 |\n| `chair_ingolf_0650` (episode return) | - | -5099.7$\\pm$371.4 | -4864.2$\\pm$768.9 |\n| `three_blocks` (phase completion) | 1.0 | 1.0$\\pm$0.0 | 1.0$\\pm$0.0 |\n| `three_blocks` (episode return) | - | -3385.6$\\pm$701.7 | -3346.18$\\pm$679.1 |\n\nTable 1. Results in IKEA furniture benchmark over 5 runs.\n\n[1] Lee, Y., Hu, E. S., Lim, J. J. IKEA furniture assembly environment for long-horizon complex manipulation tasks.\n\n[2] Lee, Y., Lim, J. J., Anandkumar, A., Zhu, Y. Adversarial skill chaining for long-horizon robot manipulation via terminal state regularization.\n\nNote that this benchmark is quite challenging, and requires very high precision manipulation. It turns out that behavior cloning methods generally fail (please see Table 1 in [1]), and IQL with vanilla rewards also fails, indicating that *the base algorithm counts in this benchmark*. Online methods like skill-chaining [2] can handle these tasks well, while it remains unclear whether offline RL algorithms can achieve meaningful performance in this benchmark, which we believe lie out of the scope of our paper.\n\nWe further examine how SEABO behaves on some less challenging long-horizon manipulation tasks. To that end, we evaluate SEABO in Kitchen datasets from the D4RL paper. The kitchen environment consists of a 9 DoF Franka robot interacting with a kitchen scene that includes an openable microwave, four turnable oven burners, an oven light switch, a freely movable kettle, two hinged cabinets, and a sliding cabinet door. In kitchen, the robot may need to manipulate different components, e.g., it may need to open the microwave, move the kettle, turn on the light, and slide open the cabinet (precision is required). Please see details of the kitchen environment in [3,4].\n\n[3] Gupta, A., Kumar, V., Lynch, C., Levine, S., Hausman, K. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning.\n\n[4] Fu, J., Kumar, A., Nachum, O., Tucker, G., Levine, S. D4rl: Datasets for deep data-driven reinforcement learning."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232529832,
                "cdate": 1700232529832,
                "tmdate": 1700232529832,
                "mdate": 1700232529832,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gmt4PrHTuS",
            "forum": "MNyOI3C7YB",
            "replyto": "MNyOI3C7YB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1645/Reviewer_KA4c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1645/Reviewer_KA4c"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for Offline Imitation Learning (IL) that defines a reward function based on the Euclidean distance to the nearest neighbor expert state. The method, called SEABO, uses a KD-tree to efficiently query expert states and compute rewards for all transitions. The resulting problem can then be optimized using an arbitrary offline RL algorithm. The experimental results demonstrated improved performance in several tasks of the D4RL benchmark."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed approach is both novel and simple, and its implementation is efficient due to the use of a KD-tree, without the need for training an extra discriminator.\n2. This paper focuses on the context of single-expert-demonstration IL tasks, which is an area of growing interest in the field.\n3. I find the discussion on using different search algorithms in Section 5.4 and Appendix Section C interesting.\n4. The Limitations section in the appendix is highly appreciated, as it provides valuable guidance on tuning hyperparameters and applying SEABO on visual input."
                },
                "weaknesses": {
                    "value": "1. I'm concerned about the use of Euclidean distance and would suggest that the authors include references justifying the use of this distance metric. This is crucial because there might be scenarios where states that are close in Euclidean distance are, in fact, far apart when accounting for the transitions within the Markov Decision Process (MDP). This particular challenge doesn't arise in discriminator-based methods, mainly due to the use of an additional neural network during training.\n2. I believe that \"(oracle)\" should be omitted from Table 1-3 and 6. For instance, consider \"IQL (oracle)\": it utilizes the ground truth reward but doesn't rely on expert demonstrations. Removing the ground truth reward and integrating an additional expert demonstration does not necessarily make the task more challenging.\n3. The experiments currently compare with only two Offline RL methods (IQL and TD3_BC). It would be better to include more recent baselines such as Trajectory Transformer [[1]], Diffuser [[2]], or other methods.\n\n[1]: https://arxiv.org/abs/2106.02039\n[2]: https://arxiv.org/abs/2205.09991"
                },
                "questions": {
                    "value": "1. Can SEABO utilize alternative distance metrics in place of the Euclidean distance? If so, how much modification is required?\n2. The comparison between ground-truth rewards and the rewards obtained by SEABO (as in Figures 2 and 12) is intriguing. Have you also conducted similar reward comparisons in additional environments, such as AntMaze-v0 and Adroit-v0?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1645/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1645/Reviewer_KA4c",
                        "ICLR.cc/2024/Conference/Submission1645/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1645/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698669045245,
            "cdate": 1698669045245,
            "tmdate": 1700709353902,
            "mdate": 1700709353902,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5DiSDiC2Cn",
                "forum": "MNyOI3C7YB",
                "replyto": "gmt4PrHTuS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1645/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Responses to Reviewer KA4c (part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging that our method is simple yet effective. Below we try to address the concerns of the reviewer. If we are able to resolve some concerns, we hope that the reviewer will be willing to raise the score.\n\n**Concern 1: on the use of Euclidean distance**\n\nThanks for the advice. Section 3.2 ***Reward Engineering*** paragraph in [1] lists several works that leverage Euclidean distance for reward engineering, which we think can be a nice reference here. We have included this paper in our revised manuscript (Section 4) for validating the choice of Euclidean distance. The reviewer is concerned that there may exist some scenarios where states that are close in Euclidean distance are, in fact, far apart when accounting for the transitions within the MDP. Based on our experiments, we do not find such phenomena (Euclidean distance can incur good results on most of the evaluated tasks). Suppose that there exist some cases where such a phenomenon occurs, we would suggest the user adopt another distance measurement that can better measure their deviation within the MDP.\n\n[1] Torabi, F., Warnell, G., Stone, P. Recent advances in imitation learning from observation\n\nIt is interesting to check how SEABO behaves with other distance measurements. We choose Manhattan distance ($D(x,y) = \\sum\\_{i}|x\\_i - y\\_i|$) and cosine distance ($D(x,y) = \\dfrac{x\\cdot y}{\\||x\\|| \\||y\\||}$) for evaluation and summarize the results below. Note that it is quite easy to replace Euclidean distance with other distance measurements (the modification is minor as we only need to replace the function that calculates Euclidean distance with another distance measurement function). Note that we keep hyperparameters of SEABO and IQL unchanged on the evaluated datasets across different distance measurements.\n\n| Task Name | IQL (orcale) | +SEABO(Euclidean) | +SEABO(Manhattan) | +SEABO(cosine) |\n| ---- | :---: | :---: | :---: | :---: |\n| halfcheetah-medium-v2 | **47.4$\\pm$0.2** | 44.8$\\pm$0.3 | 44.5$\\pm$0.1 | 42.6$\\pm$0.1 |\n| hopper-medium-v2 | 66.2$\\pm$5.7 | **80.9$\\pm$3.2** | 74.9$\\pm$2.7 | 80.9$\\pm$1.1 |\n| walker2d-medium-v2 | 78.3$\\pm$8.7 | 80.9$\\pm$0.6 | **81.1$\\pm$0.7** | 77.3$\\pm$0.7 |\n| halfcheetah-medium-replay-v2 | **44.2$\\pm$1.2** | 42.3$\\pm$0.1 | 42.1$\\pm$0.8 | 36.6$\\pm$1.3 |\n| hopper-medium-replay-v2 | 94.7$\\pm$8.6 | 92.7$\\pm$2.9 | **96.7$\\pm$3.4** | 69.3$\\pm$4.8 |\n| walker2d-medium-replay-v2 | 73.8$\\pm$7.1 | **74.0$\\pm$2.7** | 61.6$\\pm$22.0 | 63.0$\\pm$8.4 |\n| halfcheetah-medium-expert-v2 | 86.7$\\pm$5.3 | 89.3$\\pm$2.5 | **92.0$\\pm$0.4** | 71.9$\\pm$6.0 |\n| hopper-medium-expert-v2 | 91.5$\\pm$14.3 | 97.5$\\pm$5.8 | **98.0$\\pm$10.7** | 92.7$\\pm$10.1 |\n| walker2d-medium-expert-v2 | 109.6$\\pm$1.0 | **110.9$\\pm$0.2** | 109.1$\\pm$0.5 | 105.7$\\pm$5.2 |\n| pen-human-v0 | 70.7$\\pm$8.6 | 94.3$\\pm$12.0 | **96.6$\\pm$5.4** | 96.0$\\pm$13.8 |\n| pen-cloned-v0 | 37.2$\\pm$7.3 | 48.7$\\pm$15.3 | 52.3$\\pm$20.7 | **52.6$\\pm$17.7** |\n| door-human-v0 | 3.3$\\pm$1.3 | **5.1$\\pm$2.0** | 5.0$\\pm$3.5 | 2.7$\\pm$0.9 |\n| door-cloned-v0 | **1.6$\\pm$0.5** | 0.4$\\pm$0.8 | 0.0$\\pm$0.0 | 0.0$\\pm$0.1 |\n\nTable 1. Comparison of SEABO under different distance measurements. The results are averaged over 5 different random seeds.\n\nIt can be seen that Manhattan distance can also incur good performance on some datasets, while cosine distance seems to have unsatisfying performance on numerous datasets. The optimal distance measurement for different dataset may vary, while simply using Euclidean distance can already ensure a good performance.\n\n**Concern 2: (oracle) should be omitted from Table 1-3 and 6**\n\nBy specifying *oracle*, we would like to emphasize that the results are obtained by running the offline RL algorithms on datasets with ground-truth rewards. We can remove these if the reviewer deems it necessary."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231354108,
                "cdate": 1700231354108,
                "tmdate": 1700231354108,
                "mdate": 1700231354108,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RMCnDP8oqy",
                "forum": "MNyOI3C7YB",
                "replyto": "tJKAl9Ds61",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1645/Reviewer_KA4c"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1645/Reviewer_KA4c"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for the detailed responses and the additional experimental results.\n\nThe main weakness of this paper is the lack of theoretical justifications. Specifically, the reliance on Euclidean/Manhattan/cosine distances may be inadequate in certain instances of MDP, a concern also raised by other reviewers, including Limitation (2) by Reviewer m3fB, Question (1) by Reviewer u2Fn, and Question (4) by Reviewer LUNW.\n\nI also wish to reiterate that the use of \"(oracle)\" might be misleading. Offline RL algorithms trained with ground-truth rewards do not incorporate the additional single-expert demonstration and, therefore, cannot be considered true oracles. The true oracle would require modifying the offline RL algorithms to somehow utilize the additional expert demonstration.\n\nNevertheless, as acknowledged in the author responses to Reviewer LUNW, the authors do not claim that this method is universally applicable and capable of addressing all scenarios. I find the empirical perspective of this work interesting, demonstrating that simple methods can yield good performance. The inclusion of new experimental results has addressed some of my initial concerns, leading me to revise my score accordingly."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709499018,
                "cdate": 1700709499018,
                "tmdate": 1700709499018,
                "mdate": 1700709499018,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]