[
    {
        "title": "Efficient Model-Agnostic Multi-Group Equivariant Networks"
    },
    {
        "review": {
            "id": "vWHcNSHN2q",
            "forum": "O9gstAazBM",
            "replyto": "O9gstAazBM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8572/Reviewer_mvp1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8572/Reviewer_mvp1"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the problem of designing model-agnostic group-equivariant networks in an efficient manner. The authors consider two settings: (1) the network has multiple inputs each with a potentially different group acting on it, and (2) the network has a single input and the group acting on it is a large product group. For the former the authors consider linear formulations and characterize the entire space of linear equivariant layers. They then use the obtained equations to extend to non-linear models and show that there exist a design that is universal in approximating invariant-symmetric functions. For the second setting, they propose a method that is more efficient than existing works, at the cost of decreased expressivity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I think the motivation is strong and the theoretical results are valuable."
                },
                "weaknesses": {
                    "value": "I think the group-equivariant network that is designed for inputs having large product groups acting on them is claimed to be less expressive but more efficient than equitune (page 2). However the loss of expressivity is not discussed in section 3. I think this part is fundamental and should be stressed."
                },
                "questions": {
                    "value": "1. For the results in Table 1, why do you consider the input as ordered? What if instead we consider the input as a set of images, and apply Maron et al 2020? How would it compare?\n2. Related to 1, I think the multi-image task does not really test the first scenario, as the input groups are the same and therefore Maron et al 2020 can be directly applied."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8572/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698707272690,
            "cdate": 1698707272690,
            "tmdate": 1699637072261,
            "mdate": 1699637072261,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m9t6zrxJeE",
                "forum": "O9gstAazBM",
                "replyto": "vWHcNSHN2q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8572/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8572/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer mvp1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. We aim to address the weaknesses here.\n\n**Reviewer:** Weaknesses:\nI think the group-equivariant network that is designed for inputs having large product groups acting on them is claimed to be less expressive but more efficient than equitune (page 2). However the loss of expressivity is not discussed in section 3. I think this part is fundamental and should be stressed.\n\n**Response:** Thanks for this comment.  We have now added a discussion on the loss of expressivity in the newly added Sec. G in the appendix of the paper to provide an intuition describing where the trade-off between efficiency and performance comes from. \n\n**Reviewer:**\nQuestions:\nFor the results in Table 1, why do you consider the input as ordered? What if instead we consider the input as a set of images, and apply Maron et al 2020? How would it compare?\n\n**Response:** If we ensure that the fusion layers are identical and share across channels, then our framework directly extends for sets instead of sequences. But Maron et al. 2020 is not applicable here since they assume that the group elements acting on each image is the same. Thus, Maron et al. is not equivariant to the group actions considered here. Whereas, in Tab. 1, the group elements applied are chosen independently.\n\nIn the appendix of Maron et al. Sec. B, they also provide some results on applying independent group actions to each image/data point, but they assume the group actions are transitive. We make no such assumptions. In fact, the transitive assumption does not hold on the image classification tasks considered.\n\n**Reviewer:** Related to 1, I think the multi-image task does not really test the first scenario, as the input groups are the same and therefore Maron et al 2020 can be directly applied.\n\n**Response:** As discussed above, please note the framework of Maron et al. 2020 does not work in our problem setting."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8572/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667846023,
                "cdate": 1700667846023,
                "tmdate": 1700667846023,
                "mdate": 1700667846023,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zWYpE6DF7h",
            "forum": "O9gstAazBM",
            "replyto": "O9gstAazBM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8572/Reviewer_EpCD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8572/Reviewer_EpCD"
            ],
            "content": {
                "summary": {
                    "value": "This paper tries to address the computational problem of constructing model-agnostic group equivariant networks for large product groups, and provides efficient model-agnostic equivariant designs for two related problems with different input specifications. For different problems, this paper proposes new fusion layer designs, which can be extended beyond linear models, and model-agnostic equivariant designs for large product groups. Experimental results are provided for different applications, such as language compositionality, natural language generation, and zero-shot classification, showing high computational efficiency than the existing ones."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is well-organized and well-written. The motivation is stated in a clear way, and the objective is easy to follow.\n\nThe theoretical findings are organized in a proper way, and the proofs are given in a rigorous way.\n\nThe computational complexity could have been given in detail."
                },
                "weaknesses": {
                    "value": "The related work could be expanded to highlight the difference between this work and the existing ones. The novelty and contributions could have been highlighted.\n\nMore institutions and explanations of the theoretical findings could be provided after each theorem. It is not easy to figure out how important these theoretical results are and how they could be used to guide the practical designs.\n\nIn addition to the comparison of computational complexity, it is unclear how and to what extent the proposed designs are practically useful."
                },
                "questions": {
                    "value": "Add more intuitions and explanations for theoretical findings and possible insights to guide practical designs.\n\nAdd more meaningful experimental results to show the practical usefulness of the proposed designs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8572/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698851648195,
            "cdate": 1698851648195,
            "tmdate": 1699637072158,
            "mdate": 1699637072158,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pcj7O2ysIS",
                "forum": "O9gstAazBM",
                "replyto": "zWYpE6DF7h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8572/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8572/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer EpCD"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. We aim to address the weaknesses here.\n\n**Reviewer:** The related work could be expanded to highlight the difference between this work and the existing ones. The novelty and contributions could have been highlighted.\n\n**Response:** Thanks for the suggestion on emphasizing novelty and contributions, which makes it easier for the reader.  As we have now added in Sec. 2, our work follows the very recent trend of works in equivariant deep learning that focuses on making pretrained models equivariant [1-4]. \n\nSince it a new area of research, there are very few works in this direction. Most of the works described in Sec. 2 focus on training from scratch and are therefore very different from our setup.\n\nCompared to [1] that uses a simple averaging over the entire group to obtain symmetrization, we simply perform averaging over subgroups when the group can be decomposed as products. [2-4] use weighted averaging over group elements to obtain symmetrization, which are complementary to our work and can be used on top of our work for future work.\n\n[1] Basu et al. \"Equi-tuning: Group equivariant fine-tuning of pretrained models.\" AAAI 2023.\n\n[2] Basu et al. \"Equivariant few-shot learning from pretrained models.\" NeurIPS 2023\n\n[3] Kim et al. \"Learning probabilistic symmetrization for architecture agnostic equivariance.\" NeurIPS, 2023.\n\n[4] Mondal et al. \"Equivariant Adaptation of Large Pre-Trained Models.\" NeurIPS 2023.\n\n**Reviewer:** More institutions and explanations of the theoretical findings could be provided after each theorem. It is not easy to figure out how important these theoretical results are and how they could be used to guide the practical designs.\nIn addition to the comparison of computational complexity, it is unclear how and to what extent the proposed designs are practically useful.\n\n**Response:** We have now added further description in Sec. 3.2 about the connections between Thm. 1 and Thm. 2. Thm. 1 proves the equivariance of the construction in (1) and Thm. 2 shows that the construction covers the entire linear equivariant space. Hence, (1) characterizes the entire space of linear equivariant networks.\n\nThm. 3 proves the universality of the IS fusion layer and Thm. 4 proves the equivariance of the general non-linear construction in (6), as described in Sec. 3. \n\nThe main goal of the work is to provide computational efficiency over [1] with little loss in performance, as described above. Further details on the trade-off between efficiency and performance is described in the newly added Sec. G in the appendix."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8572/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667703136,
                "cdate": 1700667703136,
                "tmdate": 1700667703136,
                "mdate": 1700667703136,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tT9tUPtssZ",
            "forum": "O9gstAazBM",
            "replyto": "O9gstAazBM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8572/Reviewer_gLGd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8572/Reviewer_gLGd"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims at designing a model-agnostic group equivariant network for direct product groups."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The topic is interesting and important."
                },
                "weaknesses": {
                    "value": "It could be that I misunderstand something. But from what I understand from the construction, the model is vacuous, and can only apply for the trivial symmetry.\n\nIn page 2, the notion of \u201cinvariant-symmetric\u201d seems strange. Note that you simply define the identity action of $G_2$ on the range of $f$ in the space $Y$. Namely, for any $y=f(x)\\in Y$, you assume that $g_2y=y$. In what sense is this an interesting model of symmetries? It describes no symmetry, or more accurately, just the trivial symmetry.\n\nIn the construction in (1), from what I understand, for the  \u201cinvariant-symmetric\u201d property of\n\n $L^{IS}_{G_2,G_1}$ \n\nyou need to assume that $G_1$ acts trivially on $X_1$, namely, $gx=x$, and for the \u201cinvariant-symmetric\u201d property of $L^{IS}_{G_1,G_2}$ you need to assume that $G_2$ acts trivially on $X_2$, namely, $gy=y$. This means that your construction cannot describe any structure but the trivial symmetry. The proof of Theorem 1 in the appendix also shows that this is what you construct. Hence, the whole construction is vacuous. \n\nWhat am I missing?\n\nIf I misinterpreted the construction, please explain. I will be happy to change my score."
                },
                "questions": {
                    "value": "Some other problems that I found in the paper before realizing that the model (1) is problematic:\n\nThe term \u201cindependent groups\u201d, for example at the bottom of page 1, should be replaced by commuting subgroups, or by \u201call inputs are acted upon independently by separate groups\u201d.\n\nPage 2: \u201cinvariant-symmetric\u201d: Note that you simply define the identity action of $G_2$ on  the range of $f$ in $Y$. Namely, for any $y=f(x)\\in Y$, you assume that $g_2y=y$. In what sense is this an interesting model? It describes no symmetry.\n\nSection 3.1 Multiple Inputs - you mean that the sequence $(X_1,\\ldots,X_N)$ is the input, not that this is a set of inputs. You should also formulate the setting as follows: the direct product group $G=(G_1,\\ldots,G_N)$ acts on the input space $\\mathbb{R}^{d_1,\\ldots,d_N}$.\n\nLarge product groups : ``the subgroups $g_i$ act in the same order.\u2019\u2019 The order does not matter since $G$ is a direct product group. The subgroups $G_i$ commute.\n\n\u201c whereas for constructing G-invariant models we do not need commutativity\u201d It is not a matter of need. Since the subgroups $G_i$ commute by definition of direct product of groups, and by definition of group action, the action of the subgroups $G_i$ must commute.\n\nEquation (1): what is $L^{IS}_{G_2,G_1}$ \n\n?\n You did not define it. You also did not define $L_{G_1}^{Eq}$ and $L_{G_2}^{Eq}$. There is a problem with this construction as I wrote above.\n\nAt this point I have to admit that I stopped reading. If my assessment of (1) is correct, the paper should be rejected. If I misunderstood the construction I apologize."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8572/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8572/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8572/Reviewer_gLGd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8572/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699028890353,
            "cdate": 1699028890353,
            "tmdate": 1700033865167,
            "mdate": 1700033865167,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dYshAczKHC",
                "forum": "O9gstAazBM",
                "replyto": "tT9tUPtssZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8572/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8572/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer gLGd"
                    },
                    "comment": {
                        "value": "We clarify the weaknesses and questions raised by the reviewer. With these clarifications, we sincerely hope the reviewer will kindly reconsider reading through our paper. We strongly believe the reviewer will find it a compelling next step in designing efficient model agnostic equivariant networks, following the recent emergence of works in this area [1-4].\n\n[1] Basu et al. \"Equi-tuning: Group equivariant fine-tuning of pretrained models.\" AAAI 2023.\n\n[2] Basu et al. \"Equivariant few-shot learning from pretrained models.\" NeurIPS 2023\n\n[3] Kim et al. \"Learning probabilistic symmetrization for architecture agnostic equivariance.\" NeurIPS, 2023.\n\n[4] Mondal et al. \"Equivariant Adaptation of Large Pre-Trained Models.\" NeurIPS 2023.\n\n**Reviewer:** \"... the model is vacuous, and can only apply for the trivial symmetry.\"\n\n**Response:** Please note that this is a misunderstanding since we design group equivariant models (not invariant-symmetric) for product groups G1xG2 for any two groups G1 and G2. In Thm. 2, we prove that our model design for the linear case covers the entire linear equivariant function space corresponding to product groups. Hence, clearly, our model does not only apply to trivial symmetries. Further, the competitive performance of our model design while respecting all the required equivariances also implies its meaningfulness.\nPlease find more details on the nature of the invariant-symmetric fusion layers in the response below.\n\n**Reviewer:** \"In page 2, the notion of \u201cinvariant-symmetric\u201d seems strange.\"\n\n**Response:** Please note that we do not invent the notion of invariance-symmtery, we simply discover that it is present in linear equivariant layers of product groups and give it a name. This observation/discovery helps us design simpler networks going beyond the linear framework.\n\nAs we show in Thm. 2, it is naturally present in linear models of product groups. When a linear group equivariant model for the first design is constructed for multiple groups, this notion of invariance-symmetry naturally arises as a fusion layer between the different inputs. This is similar to different fusion layers with symmetries that arise in Deep Sets [5], and DSS [6]. Unlike Deep Sets and DSS, we do not stack up linear layers to construct our full model. Instead, we give a model-agnostic design, which goes beyond linear design and works even for pretrained models, leading to the methods discussed in Sec. 3.3, 3.4.\n\n[5] Manzil et al. \"Deep sets\", NeurIPS 2017\n[6] Maron et al. \"On learning sets of symmetric elements\", ICML 2020.\n\n**Reviewer:** \"Note that you simply define the identity action ... just the trivial symmetry.\"\n\n**Response:** \n\n- a) Please note that Pg. 2 only provides the def of invariance-symmetry (IS) that we discover in the **fusion layer of the equivariant network corresponding to product groups**. Thus **IS only represents a subcomponent of our model design**, it does not define the symmetry we are aiming for, which is group equivariance to product groups.\n\n- b) Please note that the reviewer seems to have misunderstood the definition of IS and confused it with trivial symmetry.  The two are different concepts as described below. Also, note that we make no assumptions on the IS function; instead, we provide functions f that precisely satisfy the IS constraint.\n    - trivial symmetry (https://en.wikipedia.org/wiki/Symmetry_group) means the group action corresponds to a group with one single element that does not perform any transformation to the object of the action.\n  - A function f: X -> Y has the property of IS with respect to ordered groups (G1, G2) where G1 acts on X and G2 acts on Y, if f(g1 x) = x (invariance) and g2 f(x) = f(x) (symmetry) for all x in X, g1 \\in G1, g2  \\in G2. From what we can gather, the reviewer may be confusing the notion of symmetry in our work f(x) = g2 f(x) to trivial symmetry group. Trivial symmetry group means a group with a single element which when acts on an object leaves the object unchanged. On the other hand, our notion of symmetric function simply means any function that remains unchanged with respect to group actions on the output of the function. This is very closely related to the notion of invariance (we call a function invariant when the output remains unchanged with respect to transformations on the input). On the other hand, we call a function symmetric when it remains unchanged with respect to a group transformation applied to the output. Consider the following example. Let f1: R^2-> R^2, f2: R^2->R^2 be functions defined as f1(x, y) = (1, 1), f2(x, y) = (1, 1) if (x, y) in {(0, 0), (0, 1), (-1, 0), (0, -1)}, else (0, 0). Here f1 is invariant whereas f2 is symmetric, with respect to 90 degree rotations of the input about the origin. Finally, we reemphasize we did not invent this notion of IS it naturally arises in equivariant network design for product groups, which we discover, name, and exploit for efficient model designs."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8572/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699892251161,
                "cdate": 1699892251161,
                "tmdate": 1699892251161,
                "mdate": 1699892251161,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "khB0KLUKdD",
                "forum": "O9gstAazBM",
                "replyto": "tT9tUPtssZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8572/Reviewer_gLGd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8572/Reviewer_gLGd"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for you response.\n\nLet me see if I can rephrase things more accurately so we can agree.\n\nFor $f$ to be IS, you assume that for every $y$ in the range of $f$, and every $g_2\\in G_2$, you have $y=g_2f$.\n\nThis means that the range of $f$ is a subset of the following set of points in $Y$: the set $S$ of all $y\\in Y$ such that $g_2y=y$ for every $g_2\\in G_2$.  So $G_2$ acts trivially on the range of $f$. Namely, the action of $G_2$ is the identity there, and the range of $f$ is called fixed under $G_2$. In $S$, the action of $G_2$ is the trivial action (this is what I called informally a trivial symmetry.).\n\nDo we agree now on that?\n\n**I do see now that it is meaningful to assume that the rage of $f$ is fixed under the action.** For example, under the reflection action on images (action of the group $\\{-1,1\\}$), the output of $f$ must be in the set of symmetric (or even) images.\n\n\nRegarding replacing direct product with semidirect product, the image application indeed has a semidirect product group structure.\n\nRegarding the other applications, I cannot localize in the paper the definitions of the semidirect product group and its action in these applications. Can you write these definitions explicitly so we see that these are indeed semidirect product groups?\n\n\nI apologize for not reading in detail the whole paper the first time. The inconsistencies in group theory (the fact that the theory was for direct product groups but you used non-commuting groups in applications), and the confusion about triviality of the action led me to immediately reject the paper.\n\nNow, I need to see the revised paper so I can re-evaluate it."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8572/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699901517723,
                "cdate": 1699901517723,
                "tmdate": 1699901612584,
                "mdate": 1699901612584,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rbPHaiMS7Q",
                "forum": "O9gstAazBM",
                "replyto": "tT9tUPtssZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8572/Reviewer_gLGd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8572/Reviewer_gLGd"
                ],
                "content": {
                    "title": {
                        "value": "Order of semidirect product"
                    },
                    "comment": {
                        "value": "Thank you. \nNote that semidirect product is not symmetric. One subgroup is normal and the other is not in general. So, if you define a nested semidirect product you need to use parenthesis that indicate in which order you apply the semidirect products.\n\nFor example, does $G=G_1 \\rtimes G_2 \\rtimes G_2$ mean $(G_1 \\rtimes G_2) \\rtimes G_3$, where $G_1 \\rtimes G_2$ is normal in $G$, and $G_1$ is normal in $G_1 \\rtimes G_2$, or does it mean $G_1 \\rtimes (G_2 \\rtimes G_3)$, where $G_1$ is normal in $G$ and $G_2$ is normal in $G_2 \\rtimes G_3$?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8572/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699968103840,
                "cdate": 1699968103840,
                "tmdate": 1699968747611,
                "mdate": 1699968747611,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cMJmEuAJFA",
                "forum": "O9gstAazBM",
                "replyto": "jklabsCr2R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8572/Reviewer_gLGd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8572/Reviewer_gLGd"
                ],
                "content": {
                    "title": {
                        "value": "Review of the revised paper"
                    },
                    "comment": {
                        "value": "**Motivation**\n\nIt is not clear how important the problem of taking a trained model and symmetrizing it. Usually, geometric deep learning is motivated by reducing the number of training parameters before training, not after training, to reduce the model complexity. This point should be discussed in the paper. What is the advantage and disadvantage in symmetrizing the model before vs after training. I think that this should be one main point in the paper.\n\n\n**Inconsistencies in group theory**\n\n\nThe proofs in this paper are specifically for subgroups of the symmetric group and their representations as permutations. However, the paper presents the theory as if it is for general group actions. See for example (3) and Lemma 1: \n\nIn Equation (3): what do you mean by \u201cpermutation group corresponding to $g\\in G$\u201d? In Maron et al the group was a subgroup of the permutation group, and $P(g)$ was the matrix representation of the permutation. You work with general actions until now. If you want to use this results you need to restrict the analysis to permutation representation.  This is also trie for Lemma 1. Note the difference between group action and group representation, and between general representation and permutation.\n\nAfter equation (5), fix to \u201cis invariant-symmetric with respect to (Gi, Gj).\u201d You also did not introduce the notation $M$.\n\nEquation (6): this formula also shows that your construction should be restricted to group representations and not general group actions. Without linearity of the action WRT the vector space on which it is defined the construction would not work.\n\n\nApplications:\n\nRegarding the language model application. As I explained above, the theory should be restricted to permutation representations on vector spaces. The language model application was described using a group action on a set which is not a vector space. You should write a paragraph, perhaps in the appendix, that explains how the input and output of the model in this case are in vector spaces and how the action is a permutation representation.\n\n\n**Suggestion**\n\nYou do not really need the group to be a nested semidirect product group for the construction to work, right? \nYou can formulate the setting as follows. Write that G is a set-direct product of subgroups $G_1,\\ldots,G_n$. Namely, as a set it is a direct product of sets, but not as a group. The subgroups need not commute. Then you can give two examples: group direct product and nested group semidirect product in any order of nesting. If you formulate the setting this way, take special care to stress that the direct product is in the set sense and not the group sense."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8572/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700033824832,
                "cdate": 1700033824832,
                "tmdate": 1700033824832,
                "mdate": 1700033824832,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iueQvuPP5C",
            "forum": "O9gstAazBM",
            "replyto": "O9gstAazBM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8572/Reviewer_ePZ4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8572/Reviewer_ePZ4"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of constructing equivariant networks in a model-agnostic manner. In particular, this paper tackles the setting in which the group can be decomposed as a product group. This leads to two principal tasks: 1.) the multi-input setting in which each group in the product acts on its respect input and 2.) the single input setting but the symmetry on this data type has a product structure. For the first setting, the authors propose IS fusion layer and characterize the entire space of linear equivariant functions with multiple inputs. They then extend this to the non-linear setting and prove universal approximation capabilities. Experiments are conducted in both settings and include multi-image classification and downstream applications of compositional generalization and language. The proposed approach is sometimes competitive with previous approaches but has the benefit of linear computational complexity w.r.t. to the number of groups."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper has a few strengths that I would like to highlight. First, the paper builds upon two recent papers by Basu et. al 2023 and Kim et. al 2023. and this allows the paper to lean on existing methodology. Thus the overall idea is relatively straightforward to understand. Moreover, the presented theory can be understood equally easily as it largely follows from the author's definitions and results in Maron et. al 2020."
                },
                "weaknesses": {
                    "value": "Despite the stated strengths above; I have strong reservations regarding this paper. The first one is on the motivation. It is unclear to me why we would want to make an existing trained model equivariant. This is certainly the assumption in the downstream language experiments, but this is not at all a convincing demonstration. The task is contrived and does not fit in the broader equivariant literature.\n\n**Large discrete product groups?**\n\nIn addition, to the lack of coherent motivation, I also found the claims in the paper to be unsupported. A large theme of this work is on building equivariance across **large discrete** product groups. In the multi-input experiments (Table 1) you have $N=4$ and the $C_4$ group. This is not a large product or large individual group. Similarly, in the compositional generalization experiment you write \"The product groups are made of three smaller each of size two, and the largest product group considered is of size eight\". Again this is not a large product group. A similar criticism can be attributed to the intersectional fairness experiments. So I find the entire claim and motivation for doing this work lacking. In fact, I would argue that you can just as easily do frame averaging and canonicalization in these settings. Thus at **minimum** these should be baselines. \n\n**Experimental design and results**\nThe entire choice of experiments in this paper leans heavily on Basu et. al 2023's results. But I don't think the authors thought that the experimental setup in that paper might also be problematic in terms of highlighting the claims and goals. First, there is really no standard equivariant benchmark from many of the seminal equivariant papers. For example, you could have considered molecular datasets where you have $S_n$ and $SE(3)$. $S_n$ would be a much larger discrete group and you could consider discrete subgroups of $SE(3)$. Given the large literature of equivariant models in this space, the lack of this benchmark is alarming. In addition, if you really want to show discrete product group structure then there is a large body of work on latent space disentanglement via Linear symmetries which started from the seminal work of (Higgins et. al 2018). Completely ignoring this line of work and its benchmarks which are almost tailor-made to the setting considered in this work is questionable. Finally, I find the choice of compositional generalization and intersectional fairness via group theoretic notions quite a contrived task that runs counter to the actual practical goal of this work which is to scale up equivariant models for product groups.\n\nWith regard to results, there are many areas of improvement. To start off, the proposed approach does worse than equitune in some experiments (e.g. Fig 2). The authors do not have a convincing argument on why this is acceptable outside that their proposed approach has better-scaling properties w.r.t. to the number of groups in the product. Unfortunately, as I stated above this is not a large product so this is weird. Secondly, obvious baselines are missing. These include having an actual equivariant (architecture) model that is trained (not fine-tuned) post hoc in this manner. Also, frame averaging and canonicalization should be included."
                },
                "questions": {
                    "value": "Please consider adding the following experiments.\n\n1.) An experiment where $S_n$ is in the product. This can be a molecular task or not, but the equivariant literature has many examples of benchmarks.\n\n2.) Adding an experiment with latent product symmetry. This would be a really convincing and better experiment in your setting.\n\n3.) Can you please add the baselines mentioned in the weakness section?\n\n4.) Can you please highlight the computational cost (iters/sec, flops, training time, inference time, etc...) of your approach versus frame averaging for your tasks. My guess is that it is quite similar given how small the group is."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8572/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699336046470,
            "cdate": 1699336046470,
            "tmdate": 1699637071930,
            "mdate": 1699637071930,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "37a5ZHiCNl",
                "forum": "O9gstAazBM",
                "replyto": "iueQvuPP5C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8572/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8572/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer ePZ4"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed feedback to improve the contributions of the paper. We aim to clarify the concerns raised by the reviewer.\n\n**Reviewer:** The first one is on the motivation. It is unclear to me why we would want to make an existing trained model equivariant.\n\n**Response:** Please note that making pretrained models equivariant is of crucial importance with the rise of large pretrained models. These large models are usually not trained with equivariances required for certain downstream tasks, but training them from scratch with group equivariance is extremely expensive and hence not often an option. Yet, using these large foundation models in downstream tasks shows excellent performance. Thus, the motivation of this work (focused on making pretrained model equivariant) is of great importance. Indeed, this research direction has gained recent importance as evident from several papers [1-4] in this direction. These are clearly mainstream papers on group equivariance published in respected conferences (NeurIPS and AAAI). Hence, we respectfully disagree that the considered task does not fit in the broader equivariant literature.\n\nFurther, we do not believe that the language task is contrived. The problem of addressing social biases in generated text from language models and the used framework [5] is widely studied. Further, addressing the issue of bias in natural language generation is not trivial, as shown in several works such as [6]. Taking inspiration from the formulation of [1] that shows that equivariance can provably debias natural language with respect to defined groups, we aim to extend these equivariance techniques to larger social groups much more efficiently to help safer deployments of language models. One can further note that intersectionality has been a pernicious problem in debiasing, and our mathematical framework is very natural therein. \n\n[1] Basu et al. \"Equi-tuning: Group equivariant fine-tuning of pretrained models.\" AAAI 2023.\n\n[2] Basu et al. \"Equivariant few-shot learning from pretrained models.\" NeurIPS 2023\n\n[3] Kim et al. \"Learning probabilistic symmetrization for architecture agnostic equivariance.\" NeurIPS, 2023.\n\n[4] Mondal et al. \"Equivariant Adaptation of Large Pre-Trained Models.\" NeurIPS 2023.\n\n[5] Sheng et al. \"The Woman Worked as a Babysitter: On Biases in Language Generation.\" EMNLP-IJCNLP 2019\n\n[6] Steed et al. \"Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models.\" ACL 2022\n\n**Reviewer:** Large discrete product groups?\nIn addition, to the lack of coherent motivation, I also found the claims in the paper to be unsupported. A large theme of this work is on building equivariance across large discrete product groups. In the multi-input experiments (Table 1) you have N=4 and the C_4 group. This is not a large product or large individual group. Similarly, in the compositional generalization experiment ... largest product group considered is of size eight\". Again this is not a large product group. A similar criticism can be attributed to the intersectional fairness experiments. So I find the entire claim and motivation for doing this work lacking. \n\n**Response:** Our main contribution is intended to be theoretical, where the experiments are given to validate our theoretical claims. Hence, we considered products of small groups rather than large discrete product groups, and show that the gap in computational complexity (in terms of memory consumption) is as comes from our theory. We are happy to rewrite the experiments section to indicate that the experiments are meant to validate the theoretical findings and that we did not work with very large product groups since that extra computational cost was unnecessary for theory validation."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8572/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666328288,
                "cdate": 1700666328288,
                "tmdate": 1700666328288,
                "mdate": 1700666328288,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0qS8vYYkfe",
                "forum": "O9gstAazBM",
                "replyto": "37a5ZHiCNl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8572/Reviewer_ePZ4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8572/Reviewer_ePZ4"
                ],
                "content": {
                    "title": {
                        "value": "Re: Part 1 of Response"
                    },
                    "comment": {
                        "value": "Thank you for responding to my review. I still disagree with your motivations. I believe the problem domains that you tackle are individually important (e.g. fairness, compositional generalization, equivariance), but the intersection of equivariance + X in this paper is not motivated / interesting enough. I have read the cited papers you mention in your response and I still maintain my position that---very respectfully---the language task you consider is very toy and contrived."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8572/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668997302,
                "cdate": 1700668997302,
                "tmdate": 1700668997302,
                "mdate": 1700668997302,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jo1Yx8rX6B",
                "forum": "O9gstAazBM",
                "replyto": "iueQvuPP5C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8572/Reviewer_ePZ4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8572/Reviewer_ePZ4"
                ],
                "content": {
                    "title": {
                        "value": "Re: Response part 2"
                    },
                    "comment": {
                        "value": "Thank you again for the second part of your response. I remain unconvinced as before and I will maintain my rating. I encourage the authors to think about my initial rebuttal---especially the experimental design---more carefully. I think the authors would greatly benefit from departing and leaning on the line of work related to Equitune."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8572/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669119049,
                "cdate": 1700669119049,
                "tmdate": 1700669146127,
                "mdate": 1700669146127,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]