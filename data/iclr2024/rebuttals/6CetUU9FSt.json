[
    {
        "title": "Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games"
    },
    {
        "review": {
            "id": "4YzQG1huOA",
            "forum": "6CetUU9FSt",
            "replyto": "6CetUU9FSt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5210/Reviewer_bAy4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5210/Reviewer_bAy4"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied different video encoders for imitation learning in modern video games. The motivation is that existing pre-trained models are usually trained on real-world images, while the impact of distributional shift on video-game images remains unknown. The paper conducted a systematic research that compared different pre-trained visual encoders and from-scratch trained visual encoders in three video games. The observations suggest that pre-trained self-supervised models are worth trying in video game agent development."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The writing is brilliant. The paper is very easy to follow.\n- The study is systematic and leads to some interesting observations. The paper also gives insightful analysis for these observations, which may shed some light on the research of video-game agent development.\n- The motivation is clear, and the identified problem (video-game image distribution is different from pre-training distribution) is meaningful for the community."
                },
                "weaknesses": {
                    "value": "- Although the paper offered many insights and potential analysis from the emprical observations, the paper lacks enough decisive conclusions. To be specific, I find that the following claims are not convincing:\n  - In the last sentence from section 5.1: \"while ViTs do not guarantee improvement over ResNets, they can provide significant improvement.\" This conclusion is drawn from the observation that ResNets are comparable with ViTs in Minecraft Dungeons, while are outperformed by ViTs for a large margin in Minecraft. However, the observation in Table 4 (the experiments in CS:GO) shows that ResNet outperforms ViT significantly. Therefore, it still remains unknown which of these two types of networks should be chosen as visual encoder.\n  - In section 5.3, \"This finding suggests that, if high-quality data is available for the specific task, it might be beneficial to consider training visual encoders end-to-end for BC agents, even in situations with less available data.\" As the finding shows the end-to-end encoders is comparable to pre-trained encoders, why do you say that end-to-end is beneficial? Moreover, as the pre-trained backbone is fixed during imitation learning in the paper, the trainable parameters for pre-trained settings are significantly less than end-to-end setting. I am curious if the performance will be better or worse if we don't fix the pre-trained visual encoders.\n  - In the last paragraph of section 5.5, it states that the pre-trained visual encoders fail to generalize when the input image size shifts. There are three related questions:\n    - The resize operation seems unreasonable. As the pre-trained encoder is fixed during BC training, the feature extracted from the image is fixed, which is distorted during resizing. What about padding the image to 280x280 and then resize it to 224x224?\n    - How about unfreezing the visual encoders during training? It may address the last point I raised as the visual encoder can adapt to new input size during fine-tuning.\n    - The conclusion is drawn from only one experiment, how about other cases that the pre-trained model fails when input image sizes are different?\n\n- The conclusions are drawn without controlling some critical variables. For example, the effect of network size is overlooked in the paper.\n- The experiments are constrained in a limited number of video game tasks. For example, there are thousands of tasks in Minecraft as shown by [1], and this paper only tested on the \"Treechop\" task. Also, the paper only studied the task-specific imitation learning, while large-scale pre-training adopted by VPT [2] or multi-task imitation learning [3] are not examined.\n- (minor) A key motivation of this paper is that, the images are often related to real-world scenes, which differs from video games. But there seems not enough suppotive evidence in the paper, and the readers usually don't know whether video game images are used during pre-training. Maybe a summary table that presents the pre-training sources of different models will be clear.\n\n \n\n[1] Minedojo: Building open-ended embodied agents with internet-scale knowledge. In NeurIPS, 2022.\n\n[2] Video pretraining (vpt): Learning to act by watching unlabeled online videos. In NeurIPS, 2022.\n\n[3] Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction. In CVPR 2022."
                },
                "questions": {
                    "value": "- How do you select the hyper-parameters for different experiments? \n- What are the original image sizes for Minecraft Dungeons and Minecraft?\n- Why does the paper use FocalNet as classification supervised pre-trained encoders? ImageNet pre-trained models such as ViT / DeiT are also popular, which share the same architectures as in the other categories (language contrastive / self-supervised pre-trained) and thus are easy to be compared."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5210/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5210/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5210/Reviewer_bAy4"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5210/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698405402719,
            "cdate": 1698405402719,
            "tmdate": 1700463429066,
            "mdate": 1700463429066,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4qyAKdk9ma",
                "forum": "6CetUU9FSt",
                "replyto": "4YzQG1huOA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5210/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5210/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Reviewer bAy4 (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and suggestions. Below, we will address the main questions and concerns raised by the reviewer.\n\n> How do you select the hyper-parameters for different experiments?\n\nWe refer the reviewer to Section \u201cHyperparameter Selection\u201d in our common response where we clarify how hyperparameters were selected.\n\n> What are the original image sizes for Minecraft Dungeons and Minecraft?\n\nOur collected data in Minecraft Dungeons and the filtered dataset in Minecraft contain images of resolutions 1280x720 and 640x360, respectively. We added this information to the environment descriptions in the paper.\n\n> Why does the paper use FocalNet as classification supervised pre-trained encoders? ImageNet pre-trained models such as ViT / DeiT are also popular, which share the same architectures as in the other categories (language contrastive / self-supervised pre-trained) and thus are easy to be compared.\n\nWe agree that there are popular and well-performing ViT (and ViT variations) models trained on ImageNet classification. While these models are architecturally more similar to other pre-trained and end-to-end trained models, we decided to instead include FocalNet since these models have recently been proposed and have been shown to outperform ViT-based models in the ImageNet benchmark [1].\n\n> Although the paper offered many insights and potential analysis from the emprical observations, the paper lacks enough decisive conclusions.\n\nWe thank the reviewer for their detailed thoughts. We revised the description of the discussed conclusions and point the reviewer to the colour-coded parts in the revised paper.\n\n> The conclusions are drawn without controlling some critical variables. For example, the effect of network size is overlooked in the paper.\n\nEnd-to-end visual encoders mostly have a comparable number of parameters and therefore do not allow clear conclusions about network size, but we evaluate three models of varying sizes for CLIP, DINOv2 and FocalNet. For the model size of these pre-trained encoders, we do not observe any clear relationship with online performance. While larger DINOv2 models perform best in Minecraft, the same trend does not hold for CLIP and FocalNet where encoders with fewer parameters perform better. Results for Minecraft Dungeons do not allow any clear conclusions either. The same inconclusive trend continues for pre-trained visual encoders when reducing the amount of training data. Therefore, at least for pre-trained visual encoders, we can confidently say that the number of parameters does not seem to be a clear indicator for online evaluation performance. We included a brief description of these conclusions to our revised paper (see colour-coded sentences).\n\n> How about unfreezing the visual encoders during training? It may address the last point I raised as the visual encoder can adapt to new input size during fine-tuning.\n\nWe refer the reviewer to Section \u201cFine-tuning of Pre-Trained Visual Encoders\u201d in our common response. In short, fine-tuning pre-trained visual encoders would come at a considerable computational cost that is not feasible at the breadth of our empirical study.\n\n> In the last paragraph of section 5.5, it states that the pre-trained visual encoders fail to generalize when the input image size shifts. \u2026\n\nWe agree with the reviewer that our experiments in CS:GO allow no clear conclusions as to why pre-trained visual encoders fail. To investigate our hypothesis that the image processing might be detrimental to DINOv2, we compare the performance of four pre-trained visual encoders (one for each of the four families CLIP, DINOv2, FocalNet, Stable Diffusion) with three different types of image processing in Minecraft. For details, we refer the reviewer to Appendix E which describes the conducted experiment and findings. In summary, we do not find any evidence that image processing, as applied in CS:GO, is detrimental to the performance of pre-trained visual encoders in Minecraft. This raises the open question of why pre-trained DINOv2 visual encoders are not effective for decision making in CS:GO, in contrast to Minecraft Dungeons and Minecraft. We hope this paper raises awareness of this unique challenge to the ICLR community and will open source all required code to reproduce and build on this result.\n\n> For example, there are thousands of tasks in Minecraft as shown by [1], and this paper only tested on the \"Treechop\" task.\n\nWe acknowledge the simplicity of the Treechop task in Minecraft but would like to highlight that the goal of our study is to cover a range of video games. We refer the reviewer to Section \u201cEvaluation Tasks\u201d in our common response where we discuss the selection of tasks more."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700157063307,
                "cdate": 1700157063307,
                "tmdate": 1700157106290,
                "mdate": 1700157106290,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l5HHOUzTzr",
                "forum": "6CetUU9FSt",
                "replyto": "7LrWfYnf1V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5210/Reviewer_bAy4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5210/Reviewer_bAy4"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for your comprehensive responses. However, I find the scope of the conclusions in this paper to be overly narrow, and the necessity to exclude numerous outlier scenarios is concerning. The conclusions drawn from the experiments lack universal applicability, which casts doubt on the broader relevance of the paper's contributions to the field.\n\nRegarding the conclusion section:\n\nThe claim that \"training visual encoders end-to-end on relatively small images can yield strong performance when using high-quality, representative data for the evaluation task, even in low-data regimes of few hours or minutes\" seems overly optimistic. This assertion appears valid only for simpler tasks that do not demand high-resolution perception. The study's focus on the TreeChop task in Minecraft fails to recognize the limitations of low-resolution visuals in more complex tasks, such as crafting items, where finer details are crucial for accurate item identification.\n\nThe statement \"DINOv2, trained with self-supervised objectives on diverse data, consistently outperformed other pre-trained visual encoders, indicating its generality and suitability for video games\" is problematic. For instance, DINOv2's performance in CS:GO was notably poor, a fact acknowledged by the authors. This inconsistency raises questions about the robustness of DINOv2 across different gaming environments.\n\nThe paper's limited focus on just three video games undermines the reliability of its conclusions. Specifically, the inconsistency observed in CS:GO challenges the generalizability of the findings to other video games. This limitation should be addressed to enhance the credibility and applicability of the research.\n\nIn summary, the paper's contributions seem constrained by its narrow focus and the exclusion of critical scenarios, diminishing its impact and relevance to the broader community. More comprehensive testing and analysis, particularly in diverse and challenging environments, would be necessary to strengthen the conclusions and enhance the paper's contribution to the field."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463387686,
                "cdate": 1700463387686,
                "tmdate": 1700463387686,
                "mdate": 1700463387686,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bm59eqDYKO",
            "forum": "6CetUU9FSt",
            "replyto": "6CetUU9FSt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5210/Reviewer_sq4s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5210/Reviewer_sq4s"
            ],
            "content": {
                "summary": {
                    "value": "This work compares using pretrained image encoders with learned end-to-end encoders trained with behavioral cloning. They consider a few variants of both ResNets and ViT end-to-end encoders with and without image augmentation. They compare these to encoders from language contrastive pretraining (CLIP), self-supervised pretraining (DINOv2), supervised pretraining (FocalNet), and reconstruction based pretraining (VAE). They compare this methods in 3 modern video game settings: Minecraft Dungeons, Minecraft, and Counter-Strike GO.\n\nThey find that image augmentation improves performance for end-to-end BC encoders in some cases, but in other cases it is better to train end-to-end. They first compare which end-to-end encoder is best and find ViT\u2019s to be the most performant. They find that amongst the considered pretrained encoders, DINOv2 performed best. They further compare these methods in more data limited regimes; surprisingly, results are mixed even in the data-limited regime where one would expect pretrained encoders to shine."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The authors provide a valuable datapoint to the community for which existing pretrained encoders they may want to initialize their experiments from (seemingly DINO)."
                },
                "weaknesses": {
                    "value": "Small scope and unsurprising results. This paper is more of a baselines paper comparing existing methods. For a baselines paper, I would expect far more extensive experiments across domains and methods.\n\nThe domains considered here, while they are \u201cmodern video games\u201d, are quite limited. E.g. for Minecraft they only consider the treechop task, which is the most basic thing one can do in Minecraft"
                },
                "questions": {
                    "value": "How do these methods compare in more domains? I would also expect experiments in simpler domains like e.g. atari, coinrun, maybe robotics environments.\n\nHow do the methods considered here compare to other common methods?\n\n- auxiliary objectives for representation learning are quite common in reinforcement learning. Given this paper is studying the efficacy of different image encoders, it would seem natural to me to also include auxiliary self-supervised objectives into the end-to-end experiments\n- the authors hold pretrained encoders fixed. It seems natural to also comp\n\nHow well tuned were the experiments for different encoders? It seems hyperparameters were held fixed across all architectures."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5210/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698555320721,
            "cdate": 1698555320721,
            "tmdate": 1699636518357,
            "mdate": 1699636518357,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eGMI7V0wtE",
                "forum": "6CetUU9FSt",
                "replyto": "bm59eqDYKO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5210/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5210/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Reviewer sq4s (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and suggestions. Below, we will address the main comments raised by the reviewer.\n\n> This paper is more of a baselines paper comparing existing methods. For a baselines paper, I would expect far more extensive experiments across domains and methods.\n\nWe would like to highlight that our experiments evaluate a total of 22 different visual encoder configurations across two tasks, each model being evaluated across three seeds in each task, with a partial set of visual encoders being considered in a third task. We believe our evaluation already covers a wide range of commonly used visual encoders. Furthermore, we outline our reasoning for the selection of existing tasks and their complexity in Section \u201cEvaluation Tasks\u201d of our common response.\n\n> The domains considered here, while they are \u201cmodern video games\u201d, are quite limited. E.g. for Minecraft they only consider the treechop task, which is the most basic thing one can do in Minecraft\n\nand\n\n> How do these methods compare in more domains? I would also expect experiments in simpler domains like e.g. atari, coinrun, maybe robotics environments.\n\nWe acknowledge the simplicity of the Treechop task in Minecraft but would like to highlight that the goal of our study is to cover a broad range of modern video games. We refer the reviewer to Section \u201cEvaluation Tasks\u201d in our common response where we discuss the selection of tasks more.\n\nRegarding the suggestion to run simpler domains like Atari or Coinrun, we would like to highlight that, first, the focus of our work is on modern video games that go beyond Atari (as stated in the abstract). Second, Atari and Coinrun both offer a programmatic interface and are therefore common environments for reinforcement learning. In contrast, our work focuses on imitation learning in video games where no such interface is available. Third, imitation learning in Atari is already a well-studied problem with a plethora of prior work [1-6]. For Coinrun, few work consider the application of imitation learning due to the lack of datasets of (human) demonstrations. Existing work therefore trains policies from demonstrations collected by reinforcement learning agents [7] which does not align with our focus on imitating human gameplay.\n\nLastly, our work already acknowledges and cites an existing empirical study on the application of imitation learning with varying visual encoders in robotics environments [8]. Given this study, we focus on the settings of video games which are meaningfully different from robotics since robotics tasks often represent the current state using images akin to the real world. In contrast, video games often have highly stylised visual representations (e.g. Minecraft and Minecraft Dungeons) which are notably different from the real-world images many pre-trained visual encoders are trained on. This represents a significant shift in distribution of encountered images which raised the research question whether visual encoders pre-trained on real-world images can be effective in video games. Our study clearly answers this question affirmatively and provides clear evidence as to which pre-trained visual encoders should be considered for application in complex video games.\n\n\n> auxiliary objectives for representation learning are quite common in reinforcement learning. Given this paper is studying the efficacy of different image encoders, it would seem natural to me to also include auxiliary self-supervised objectives into the end-to-end experiments\n\nAs the reviewer rightfully states, these auxiliary objectives are commonly applied in reinforcement learning. However, we train agents purely with imitation learning in domains where no reinforcement learning signal is available. In our evaluation domains, there exists no or only a limited programmatic interface which does not provide all necessary information used in many of these reinforcement learning auxiliary objectives so these approaches are not applicable to our setting or have not been established yet to serve as additional training objectives in our setting.\n\n> the authors hold pretrained encoders fixed. It seems natural to also comp\n\nWe infer that the reviewer meant to suggest to fine-tune pre-trained visual encoders. In this case, we refer the reviewer to Section \u201cFine-tuning of Pre-Trained Visual Encoders\u201d of our common response in which we illustrate why we have not considered fine-tuning pre-trained visual encoders in our study. We kindly ask the reviewer to clarify if they meant to make a different suggestion."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700156944761,
                "cdate": 1700156944761,
                "tmdate": 1700156944761,
                "mdate": 1700156944761,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5pXSB6IukR",
            "forum": "6CetUU9FSt",
            "replyto": "6CetUU9FSt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5210/Reviewer_JgYZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5210/Reviewer_JgYZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies an important problem: whether a pre-trained vision encoder can boost the performance of sequential decision-making models. The authors comprehensively study four primary encoder categories: self-supervised trained, supervised trained, contrastive-learning trained, and reconstruction trained, and draw several interesting conclusions. This will be meaningful for choosing backbones to design policy models in complicated environments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper is well-written and easy to follow. \n* This paper studies an important problem: the difference of vision encoders in building policy models for decision-making. \n* The selected environments are three modern video games, which are popular and challenging. To some degree, I believe the conclusions drawn from these environments can be generalized to real-world scenarios."
                },
                "weaknesses": {
                    "value": "* **Missing some details.** It is not clear what kinds of image augmentation tricks are used. Why the image augmentation method is specific to the game? Why a pre-trained model (DINOv2) is better than the others? It lacks deep discussions.\n\n* **Provides rollout videos for better understanding.** Rollout videos are very helpful for readers to understand the challenges of the environments and the effectiveness of the model. It is strongly recommended to include some videos in the supplementary materials. \n\n* **Insufficient evaluation tasks in the Minecraft domain.** In Minecraft, the \"Treechop\" task is the most basic and simple task. Although it is an important benchmark, however, conducting experiments solely on this task is not enough. It is better to include 2-3 extensive tasks, such as \"Hunt animals\", \"Craft crafting_tables\", and \"Mine ores\", to enhance the soundness. \n\n* **Concerns about the training data distribution of baselines.** \n\n* **Missing some baselines and references.** [1] proposed an important foundation model for decision-making in Minecraft, which was trained on large-scale YouTube gameplays with behavior cloning. It yields a good vision encoder that is specified in the Minecraft domain. Although it is cited in the paper, it does not participate in the comparison. I suggest the authors to compare VPT in the experiment. [2] is a large-scale pre-trained segmentation model, which has demonstrated strong cross-domain recognition capability. It should be included as a baseline. [3, 4, 5] are also imitation learning methods in the Minecraft domain, which are strongly related to this topic. I suggest the author reference these works and have necessary discussions. \n\n[1] \"Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos\", https://arxiv.org/abs/2206.11795\n\n[2] \"Segment anything\", https://arxiv.org/abs/2304.02643\n\n[3] \"STEVE-1: A Generative Model for Text-to-Behavior in Minecraft\", https://arxiv.org/abs/2310.08235\n\n[4] \"Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction\", https://arxiv.org/abs/2301.10034\n\n[5] \"GROOT: Learning to Follow Instructions by Watching Gameplay Videos\", https://arxiv.org/abs/2310.08235"
                },
                "questions": {
                    "value": "My questions are listed in the weakness part. \n\nI will consider improving the rating if the author adequately addresses my concerns."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5210/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5210/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5210/Reviewer_JgYZ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5210/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698644865248,
            "cdate": 1698644865248,
            "tmdate": 1700460737153,
            "mdate": 1700460737153,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5I3V6KSjZd",
                "forum": "6CetUU9FSt",
                "replyto": "5pXSB6IukR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5210/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5210/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Reviewer JgYZ (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and suggestions. Below, we will address the main comments raised by the reviewer.\n\n> It is not clear what kinds of image augmentation tricks are used\n\nAs stated in Section 3.2, we apply the same image augmentations also applied by VPT [1]. To further clarify, we added a description of the applied image augmentations in Appendix A.1 in the revised paper.\n\n> Why the image augmentation method is specific to the game?\n\nWe apply the same image augmentation in each game. We find image augmentations to be effective in Minecraft Dungeons and harmful to training in Minecraft which might be surprising but is consistent with prior findings where image augmentations can significantly improve the robustness of decision making agents [2, 3] but also harm performance if it perturbs images too much [4].\n\n> Why a pre-trained model (DINOv2) is better than the others? It lacks deep discussions.\n\nWe acknowledge that from our experiments, it is difficult to identify the reason why some pre-trained models work better than others. We hypothesise that the self-supervised training objective of DINOv2 leads to more general embeddings than reconstruction (tries to recreate all pixels rather than embed high-level information), classification (tries to only embed information related to predicted ImageNet classes), and language-contrastive learning (embed information correlated with language). This might be particularly important in domains, such as video games, which both visually and conceptually differ notably from the real-world images these models are typically trained on. Language descriptions, classification labels, and reconstruction might be too focused on embedding concepts important in the real world, self-supervised objectives might be more agnostic and, thus, effective in domains which differ from the real world such as video games.\n\n> Provides rollout videos for better understanding. Rollout videos are very helpful for readers to understand the challenges of the environments and the effectiveness of the model. It is strongly recommended to include some videos in the supplementary materials.\n\nWe thank the reviewer for the suggestion to release rollout videos of agents. These videos can now be accessed at the following anonymous repository: https://anonymous.4open.science/r/imitation_learning_in_modern_video_games_rollouts-4C48/README.md\n\n> Insufficient evaluation tasks in the Minecraft domain. In Minecraft, the \"Treechop\" task is the most basic and simple task. Although it is an important benchmark, however, conducting experiments solely on this task is not enough. It is better to include 2-3 extensive tasks, such as \"Hunt animals\", \"Craft crafting_tables\", and \"Mine ores\", to enhance the soundness.\n\nWe acknowledge the simplicity of the Treechop task in Minecraft but would like to highlight that 1. our goal is to cover a range of visually and conceptually diverse modern video games (breadth) rather than focusing on few challenging tasks in a single game (depth), and that 2. Minecraft Dungeons in particular represents a highly challenging decision making task. For a more detailed discussion of this topic, we refer the reviewer to the section \u201cEvaluation Tasks\u201d in our common response.\n\n> [1] (VPT) proposed an important foundation model for decision-making in Minecraft, which was trained on large-scale YouTube gameplays with behavior cloning. It yields a good vision encoder that is specified in the Minecraft domain. Although it is cited in the paper, it does not participate in the comparison. I suggest the authors to compare VPT in the experiment. [2] (SAM)  is a large-scale pre-trained segmentation model, which has demonstrated strong cross-domain recognition capability. It should be included as a baseline.\n\nWe refer the reader to the sections \u201cDiscussion of Related Work\u201d and \u201cSegment Anything Pre-Trained Visual Encoder\u201d in our common response where we contrast VPT to our evaluation setup in more detail and discuss the applicability of SAM to our setting. In short, we do not believe that comparisons to VPT would be fair or informative. For Segment Anything, we agree these models appear promising but unfortunately we found its inference cost to be too high for our tasks in which agents have to take actions in real-time."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700156862411,
                "cdate": 1700156862411,
                "tmdate": 1700156862411,
                "mdate": 1700156862411,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OnpztYShju",
                "forum": "6CetUU9FSt",
                "replyto": "5I3V6KSjZd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5210/Reviewer_JgYZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5210/Reviewer_JgYZ"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "+ In Appendix A.1, are you sure that VPT used image augmentation tricks? Can you specify which page contains the descriptions of augmentation tricks in VPT? \n\n+ Thank authors for providing rollout videos. I checked the rollout videos and found the performance really bad (did not chop any tree among the five videos). Such experiments will not convince me. \n\n+ Although the task of \"chop tree\" is not simple, it is not enough to just do this task for the Minecraft environment. It would be more persuasive to at least conduct experiments related to hunting animals as well.\n\n+ I did not ask the authors to directly compare their method with the original VPT in terms of success rate. However, researchers in the Minecraft community may be interested in how the video encoder of pre-trained VPT could be helpful for their tasks. As a research paper on which video encoder is better, it is meaningless to exclude the visual encoder of the most powerful model VPT.\n\n+ Since the author mentions that the SAM speed is particularly slow, this experiment can be skipped."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700456932261,
                "cdate": 1700456932261,
                "tmdate": 1700456932261,
                "mdate": 1700456932261,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CETfjSpWdX",
            "forum": "6CetUU9FSt",
            "replyto": "6CetUU9FSt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5210/Reviewer_yJPq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5210/Reviewer_yJPq"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on the challenge of training agents in modern video games, going beyond simpler games like those on Atari. The central research question is: How can images be encoded for data-efficient imitation learning in modern video games? To address this, the authors compare both end-to-end trained visual encoders and pre-trained visual encoders across three modern video games: Minecraft, Minecraft Dungeons, and Counter-Strike: Global Offensive.\n\nthe paper's main contributions can be summarized as follows:\n\n1. **Addressing a Gap**: The paper tackles the challenge of training agents in modern video games, which has traditionally been resource-intensive and costly.\n2. **Leveraging Large Vision Models**: It explores the potential of using publicly available large vision models to reduce costs and resource requirements, a pertinent issue given the current trend in machine learning towards larger models.\n3. **Comparative Study**: A systematic study is conducted to compare the performance of publicly available visual encoders with traditional, task-specific, end-to-end training approaches in the context of imitation learning.\n4. **Focus on Modern Games**: The study specifically targets modern video games, including Minecraft, Minecraft Dungeons, and Counter-Strike: Global Offensive, reflecting a move beyond simpler, classic game environments.\n5. **Human-like Gameplay**: The authors emphasize training agents to play games in a human-like manner, using behavior cloning and offline training with human gameplay data, which is a step towards creating AI that can interact in complex environments in a natural way."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors have selected a diverse set of modern video games, including Minecraft, Minecraft Dungeons, and Counter-Strike: Global Offensive, for their experimental studies. This choice reflects a significant step forward from the commonly used Atari games in previous research, providing a more realistic and challenging benchmark for evaluating imitation learning techniques.\n2. The paper introduces an innovative approach to imitation learning by leveraging publicly available large vision models. This strategy not only addresses the resource-intensive nature of training agents in modern video games but also democratizes access to high-quality training for smaller research groups or institutions.\n3. The writing is clear, concise, and well-structured."
                },
                "weaknesses": {
                    "value": "1. It seems that the task is very simple, such as chopping trees in Minecraft, which is a fairly straightforward task. The author's conclusion is that there is no significant difference between various visual encoders and input image resolutions. However, due to the simplicity of the task, this conclusion is unreliable. Evaluating models like CLIP and DINO on such simple tasks does not effectively demonstrate the differences between modern vision transformers and CNNs. I strongly recommend that the author choose more challenging tasks, such as `MineRLObtainDiamondShovel-v0` or `MineRLBasaltBuildVillageHouse-v0` etc.\n2. In time-series decision-making tasks, the memory of historical states is crucial for making decisions. For example, VPT uses a transformer to record a history state of 128 frames, while the paper only utilizes LSTM to capture a limited number of frames. This can have negative implications for completing long-horizon tasks using behavior cloning.\n3. Recently, the popular technique of Segment Anything has achieved better results in various visual tasks. The author can further compare this model to explore its potential.\n4. In the conclusions shown in Table2, the best model of tree pruning only has a success rate of 32%, much lower than VPT's nearly 100%. Does this imply that the visual encoder is not actually the most important module in game playing?\n\nIn conclusion, although the author has compared a considerable number of vision encoders in the game, the reliability of the results is compromised due to their choices in task setting and temporal transformer.\n\nSome relevant work has not been cited:\n1. Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction\n2. A generalist agent\n3. GROOT: Learning to Follow Instructions by Watching Gameplay Videos\n4. Learning to drive by watching youtube videos: Action-conditioned contrastive policy pretraining"
                },
                "questions": {
                    "value": "See in weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Null"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5210/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743290645,
            "cdate": 1698743290645,
            "tmdate": 1699636518161,
            "mdate": 1699636518161,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8Bxk2yuls9",
                "forum": "6CetUU9FSt",
                "replyto": "CETfjSpWdX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5210/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5210/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Reviewer yJPq (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and suggestions. Below, we will address the main comments raised by the reviewer.\n\n> It seems that the task is very simple, such as chopping trees in Minecraft, which is a fairly straightforward task. The author's conclusion is that there is no significant difference between various visual encoders and input image resolutions. However, due to the simplicity of the task, this conclusion is unreliable. Evaluating models like CLIP and DINO on such simple tasks does not effectively demonstrate the differences between modern vision transformers and CNNs. I strongly recommend that the author choose more challenging tasks, such as MineRLObtainDiamondShovel-v0 or MineRLBasaltBuildVillageHouse-v0 etc.\n\nWe acknowledge the simplicity of the Treechop task in Minecraft but would like to highlight that 1. our goal is to cover a range of visually and conceptually diverse modern video games (breadth) rather than focusing on few challenging tasks in a single game (depth), and that 2. Minecraft Dungeons in particular represents a highly challenging decision making task. For a more detailed discussion of this topic, we refer the reviewer to the section \u201cEvaluation Tasks\u201d in our common response.\n\n> In time-series decision-making tasks, the memory of historical states is crucial for making decisions. For example, VPT uses a transformer to record a history state of 128 frames, while the paper only utilizes LSTM to capture a limited number of frames. This can have negative implications for completing long-horizon tasks using behavior cloning.\n\nWe agree with the reviewer that memory of historical states can be crucial for decision making in some long-horizon tasks. However, we would like to disentangle the episodic length of tasks and the memory agents need to solve a task. Some long-horizon tasks require very limited or no memory to solve. For example, DQN was able to achieve superhuman performance in many Atari games without any memory, some of which feature relatively long episodes [1]. \n\nFurthermore, we disagree that LSTMs are only able to capture a limited number of frames since they can learn to keep information in its hidden state (memory) throughout an episode as long as it provides value. Impressive prior decision making achievements in long-horizon tasks have been achieved using LSTM models [2]. In contrast, a transformer is always limited to the context of its input frames (dependent on the used context length)\n\n\nTo further stress the relevance of agents maintaining history in our evaluation tasks, we highlight that the Minecraft Dungeons task requires agents to take up to 3,000 actions within a single episode (5mins at 10Hz). We believe that the ability of agents to effectively act in such a long-horizon task demonstrates that agents with LSTMs are sufficient to complete many long-horizon tasks in modern video games.\n\n> Recently, the popular technique of Segment Anything has achieved better results in various visual tasks. The author can further compare this model to explore its potential.\n\nWe agree that Segment Anything appears to be a promising pre-trained visual model. However, its inference cost makes it infeasible for our tasks in which agents have to take actions in real-time. We refer the reviewer to the section \u201cSegment Anything Pre-Trained Visual Encoder\u201d of our common response for more details.\n\n> In the conclusions shown in Table2, the best model of tree pruning only has a success rate of 32%, much lower than VPT's nearly 100%. Does this imply that the visual encoder is not actually the most important module in game playing?\n\nWe refer the reader to the section \u201cDiscussion of Related Work\u201d in our common response where we contrast VPT to our evaluation setup in more detail.\n\nFurthermore, we politely but strongly disagree that our best models performing notably worse than VPT implies that the visual encoder is not the most important module in game playing. There are many differences between our evaluation setup and VPT (the most notable ones being the scale of data and models, as well as agent architecture) which all confound the comparison, so no such conclusions can be drawn from the discrepancy in results in our paper and VPT.\n\n> Some relevant work has not been cited\n\nWe thank the reviewer for pointing out these recent, concurrent related works and included these works in Section 2 on related work. We further refer the reviewer to the section \u201cDiscussion of Related Work\u201d in our common response where we discuss the work on open-world multi-task control in more detail. Lastly, we would like to point out that the most recent version of 1. and the listed paper 3. were made publicly available on arXiv after the ICLR submission deadline."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700156745786,
                "cdate": 1700156745786,
                "tmdate": 1700156775617,
                "mdate": 1700156775617,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qhg6DNmKlZ",
                "forum": "6CetUU9FSt",
                "replyto": "pJk7x7WQa6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5210/Reviewer_yJPq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5210/Reviewer_yJPq"
                ],
                "content": {
                    "title": {
                        "value": "Comments on Rebuttal"
                    },
                    "comment": {
                        "value": "1. The author's response did not solve the core issue we raised, which is that the task chosen by the author is too simple.\n2. The author achieved only a 4% success rate using Impala CNN, while VPT with the same structure can almost complete this task with a 100% success rate. Maybe the authors should check the code or settings to avoid possible errors.\nAfter considering the author's rebuttal, I have decided to keep the score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700210205715,
                "cdate": 1700210205715,
                "tmdate": 1700210205715,
                "mdate": 1700210205715,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6P70iSTcrv",
                "forum": "6CetUU9FSt",
                "replyto": "CETfjSpWdX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5210/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5210/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal Comments of Reviewer yJPq"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their prompt response but, with respect, we firmly disagree with both of their criticisms for the following reasons.\n\n## Environment Selection\n\nWe notice that the reviewer\u2019s criticisms solely focus on our task selection in Minecraft and would like to highlight that we also evaluate in CS:GO and Minecraft Dungeons - the latter representing an entirely new environment for imitation learning. **Minecraft Dungeons is not a variation or task in Minecraft but a completely different game with distinct game mechanics** (focus on navigation and combat) **and visual perspective** (isometric top-down view in contrast to first-person view in Minecraft and CS:GO).\n\nTo visualise the uniqueness and complexity of Minecraft Dungeons as well as the Minecraft and CS:GO environments, we would like to point the reviewer to our **released rollout videos**: https://anonymous.4open.science/r/imitation_learning_in_modern_video_games_rollouts-4C48/README.md\n\n## Comparison to VPT\n\nWe strongly disagree with the reviewer\u2019s comparisons to VPT and their statement that VPT has the \u201csame structure\u201d. We would argue that adding comparisons to VPT, which would only be possible in Minecraft since VPT has been specifically designed and trained for this environment, would provide no additional value for our study. Given the many differences between VPT and our models, VPT outperforming our models in Minecraft provides no clear insight in addition to our findings.\n\nTo expand on our common response, we would like to highlight the following differences:\n\n- **>100x parameters**: VPT\u2019s main model has 500M parameters vs our model having ~4M (in the policy) + ~94K (in the referenced Impala CNN encoder) parameters\n- **~100,000x training data**: VPT uses ~70,000h of (filtered) training data vs ~45min training data for our Minecraft experiments\n- **>3,000x training budget**: VPT training uses 6480 GPU days (720 GPUs for 9 days) vs the training of the Impala CNN model using 2 GPU days (a single GPU for less than 2 days)\n\nEven if we focus on the ResNet architecture of the VPT policy model alone, the main VPT model\u2019s ResNet has 40M parameters (the smallest variation has 10M parameters in the ResNet). In comparison, the Impala CNN model highlighted by the reviewer has ~94K parameters. The ResNet encoders considered in our study, which are architecturally more comparable to VPT\u2019s ResNet, only have ~580K parameters which is still substantially less than the ResNet used in VPT.\n\n**The insights of our study are not conflicting or in competition with VPT, which serves as a foundation model for decision making in Minecraft, but provide additional and orthogonal findings about the impact of visual encoders for decision making in complex video games more broadly.**\n\n## Code Quality\n\nGiven the significant differences between VPT and our Impala CNN, we disagree with the assessment that the poor performance of the Impala CNN model suggests possible errors in our code. **We will open source the code to enable reproducibility** and assure the reviewer of the quality of the code."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5210/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243841839,
                "cdate": 1700243841839,
                "tmdate": 1700244052938,
                "mdate": 1700244052938,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]