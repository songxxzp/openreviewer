[
    {
        "title": "Eureka: Human-Level Reward Design via Coding Large Language Models"
    },
    {
        "review": {
            "id": "QOk8GxxwqW",
            "forum": "IEduRUO55F",
            "replyto": "IEduRUO55F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7937/Reviewer_JeMn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7937/Reviewer_JeMn"
            ],
            "content": {
                "summary": {
                    "value": "This paper purpose a novel pipeline to harness the LLM to design reward for using reinforcement learning to do diverse tasks without any task-specific template. The pipeline use environment code and reward feedback as input to LLM, and let LLM to continously update the reward function. The comprehensive experiments demonstrate the generalization ability of the LLM for reward design and the effectiveness of the reward designed by LLM(comparable to human)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper purpose a general pipline for reward designing of RL, which is indeed a long standing problem for RL researches, this kind of pipeline may save a lot of time for human to shape the reward.\n2. The pipeline requires no task-specifiction template for LLM, which shows a great generalization to different tasks.\n3. This paper demonstrate the LLM with evolutionary is a effective approach for using LLM, has potential for other area."
                },
                "weaknesses": {
                    "value": "Time cost and compute resource cost: Since each iteration, LLM will sample tens of reward sample, and we need to test all this reward function and get feedback, we need to simultanously run multiple experiments for each reward sample, it seems there will be a lot compute resource needed? And for each sampled reward, how many environments do you create to train? How do you decide when to terminate each running expertiment? Will it be possible that because the environment not create enough or did not train for a long time than miss a effective reward sample? What will be the total time of finding a good reward based on what kind of device?"
                },
                "questions": {
                    "value": "1. Can this method used for task that object is diverse?\n2. If the task require image as input, it hard to run many environments simutenously, can this method still work?\n3. Will only give success reward as inital reward, can make the LLM to find more optimal reward?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7937/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7937/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7937/Reviewer_JeMn"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7937/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698376134398,
            "cdate": 1698376134398,
            "tmdate": 1700604908184,
            "mdate": 1700604908184,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fVfmia0H9g",
                "forum": "IEduRUO55F",
                "replyto": "QOk8GxxwqW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7937/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7937/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer JeMn,\n\nThank you reviewer for your thoughtful comments and suggestions! Here, we respond to the specific questions and comments the reviewer raises. Please let us know if you have lingering questions and whether we can provide any additional clarifications during the discussion period to improve your rating of our paper. \n\n---\n**Question/Comment 1**: What is the computational cost of Eureka?\n\nWe have added the computation resources as well as run time of Eureka in Appendix D.4. In short, a single run of Eureka, with the default hyperparameters in our experiments, can be executed on 4 32GB memory GPUs (e.g. NVIDIA V100) and take less than one day of wall clock time even for the most complex environments (e.g. bimanual manipulation). Here, we also address some related questions from the reviewer: \n\n\n**(a)** For each sampled reward, how many environments do you create to train?\n\n**Response**: The number of environments created for training is determined by the task-specific hyperparameters set in the original benchmarks. For example, the number of environments is 2048 for Humanoid. We find the default number to work well for all our rewards, demonstrating the robustness of our Eureka approach.\n\n**(b)** How do you decide when to terminate each running experiment?\n\n**Response**: We do not perform early termination for RL runs. Fixing an environment, each RL run will execute for a fixed number of episodes according to the default task-specific hyperparameters provided in the benchmarks. That said, we believe that it is possible to use early termination to filter out unpromising reward candidates. This is an effective strategy for reducing the overall computational cost, and we leave it to future work for investigating this as computational efficiency is orthogonal and complementary to our core contribution of automated reward design.\n\n**(c)** Is it possible that RL training is not long enough so it will miss an effective reward sample?\n\n**Response**: This is certainly possible, but in practice, we find the benchmark default training hyperparameters are sufficient for discovering good reward functions across our tasks. With additional compute budget, we can also increase the training time to reduce this kind of false negative.\n\n**(d)** What will be the total time of finding a good reward based on what kind of device?\n\n**Response**: All our experiments took less than one day of wall clock time, and each individual experiment can be done on 4 V100 GPUs.\n\n---\n**Question/Comment 2**:  Can this method be used for tasks with diverse objects?\n\n**Response 2**: Yes, Eureka is a general reward design method that is not specific to any particular object type. As an example, we have demonstrated that Eureka can design effective reward functions for the novel task of pen re-orientation and spinning. The same procedure also discovers effective rewards for re-orienting a cube on a different robot hand (i.e., AllegroHand). In addition to existing evidence in the paper, we note that prior works on in-hand object re-orientation often uses the same reward function for diverse objects [1,2], and it is reasonable to assume that Eureka rewards, which work better than one such prior reward, can be applied across objects as well. \n\n---\n**Question/Comment 3**:  Can this method still work with tasks that require images as input?\n\n**Response 3**: Yes, we can infer state variables relevant to reward definition from vision. State estimation has been effectively applied in many prior works for vision-based robotic learning in real world scenarios [3,4]. While state estimation techniques may not always be accurate in complicated scenes, we believe that this is a challenge facing all real-world policy learning methods, and research progress on these fronts can readily benefit Eureka\u2019s application to real-world tasks. \nOn the other hand, if it is only the final policy that needs to act from visual inputs, we can use Eureka to first design the most effective reward function in simulation and then distill the state-based policy to a vision-based policy; this strategy has been effectively applied in many prior Sim2Real approaches [5]. \n\n---\n**Question/Comment 4**: Can the success/fitness function serve as initialization for Eureka reward search?\n\n**Response 4**: Yes, this is indeed possible. We have conducted a similar experiment in which we use a human-supplied reward function to initialize Eureka (Section 4.4). Note that supplying the fitness function, which is also the optimization objective for Eureka reward search, is likely to lead to reward functions that can better optimize it. For example, Eureka can choose to use the fitness function to create a success bonus in its reward generations.\n\nWe thank the reviewer again for their time and effort helping us improve our paper! Please let us know if we can provide additional clarifications to improve our score."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700008189076,
                "cdate": 1700008189076,
                "tmdate": 1700008189076,
                "mdate": 1700008189076,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xPd1Jo4IbH",
                "forum": "IEduRUO55F",
                "replyto": "cIxDK6BKI3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_JeMn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_JeMn"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response, most of my concerns have been addressed, but I still have two questions:\n1. For generalization on diverse objects, I mainly mean to generalize across different objects as in the example you provided of \"in-hand object re-orientation\". Although the example you show can generalize to different object instances with one reward, in-hand orientation actually has a small exploration space that is much easier to explore compared to the task in dexterity, in such case, it should be much harder to write a reward that can learn the general policy. For example in [1], the single PPO policy can not learn well for generalized grasping. Thus, I am curious whether your method is capable of writing a reward that improves these.\n2. For train with image as observation, I actually means by taking raw image as input for training, since in such case, it is hard to open many environments, due to GPU memory constraints. Or, if reduce the num of training environments, will the performance be influenced a lot?"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460673648,
                "cdate": 1700460673648,
                "tmdate": 1700460673648,
                "mdate": 1700460673648,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OlerVS5RPm",
                "forum": "IEduRUO55F",
                "replyto": "icHVwuH5jH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_JeMn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_JeMn"
                ],
                "content": {
                    "comment": {
                        "value": "Just want to double-check about the one final reward for each task, do the author just run once for each task using Eureka (with different seeds or the same seeds for different tasks?) and then use the final reward to run the subsequent 5 runs with different seeds?"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547663306,
                "cdate": 1700547663306,
                "tmdate": 1700574904747,
                "mdate": 1700574904747,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ro7vUmpmPL",
                "forum": "IEduRUO55F",
                "replyto": "y483EVHnAa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_JeMn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_JeMn"
                ],
                "content": {
                    "comment": {
                        "value": "To make sure I understand correctly, the results in Figure 4 are the averaged results of five PPO runs of the same final reward,  and the results in Figure 11 are the averaged results of 1(?) PPO runs of the five best rewards of each sub-run?"
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602216227,
                "cdate": 1700602216227,
                "tmdate": 1700602216227,
                "mdate": 1700602216227,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LI3FGFkyA8",
                "forum": "IEduRUO55F",
                "replyto": "QOk8GxxwqW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_JeMn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_JeMn"
                ],
                "content": {
                    "comment": {
                        "value": "From my perspective, tasks in these environments, usually require a human to take a lot of time and effort to shape a good reward, and when the human is designing the reward, the human has ultimate access to every part of the environment code, and even discuss with other people. Thus, Eureka can design a reward that can get such results is indeed useful. Compared to baseline, eureka also does not require task-specific prompts, which can be used in different tasks, this has already improved a lot for \"generality\". Even for a human to design a reward, humans need learn some specific task knowledge for different environments. And with more rigorous experiments have been provided,  I would like to raise my score."
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604885861,
                "cdate": 1700604885861,
                "tmdate": 1700605158800,
                "mdate": 1700605158800,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NvmUdouBuH",
                "forum": "IEduRUO55F",
                "replyto": "eRaYz1TAV9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_JeMn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_JeMn"
                ],
                "content": {
                    "comment": {
                        "value": "I am actually curious about the doubts of assumptions. From my perspective, for example, when a human decides to tackle a new task following a sim2real pipeline based on RL algorithm[1], the human might first establish an environment in simulation, in such case, the human also needs to write a reward function. And it seems reasonable for me to assume knowledge of the environment and sparse rewards.\n\n[1]Chen, Yuanpei, et al. \"Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation.\" 7th Annual Conference on Robot Learning. 2023."
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609736223,
                "cdate": 1700609736223,
                "tmdate": 1700610564563,
                "mdate": 1700610564563,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eRaYz1TAV9",
            "forum": "IEduRUO55F",
            "replyto": "IEduRUO55F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7937/Reviewer_XD65"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7937/Reviewer_XD65"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces EUREKA, an LLM-powered method for reward design for reinforcement learning tasks. The proposed method utilizes the zero-shot generation and in-context improvement of LLM, enabling evolutionary optimization over reward code without task-specific prompting or pre-defined reward templates. The paper showcases EUREKA's performance against human-engineered rewards in a diverse range of robot tasks. Furthermore, the algorithm is able to integrate with human feedback for further reward improvement, enhancing the quality and safety of generated rewards without the need for model updating."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "While the idea of this paper is rather simple, it yields a surprisingly good performance, which reflects a well-structured system. Being able to bring an easy idea to such a complete and well-considered system is commendable.\n\nMoreover, this work brings insight to the reward design community by removing the dependency on collecting expert demonstration data. The study suggests that Large Language Models (LLMs) can serve as an cheap alternative to human expert demonstrations for acquiring domain-specific task knowledge.\n\nThe paper's presentation is clear; the authors make the content easily comprehensible. Their responsible practice of providing all relevant prompts and code offers an added advantage."
                },
                "weaknesses": {
                    "value": "1. Unrealistic assumption of access to the environment source codes:\n\nThe reward code generation in this paper critically depends on having access to the source code of the MDP specification as context for the initial reward proposal. The authors have presented this as a benefit, allowing the LLM to exploit code structure to understand task environments. However, it makes an unrealistic assumption, as most reinforcement learning setups only require access to a black-box simulation. \n\nA significant limitation of this approach is that it may be infeasible for real-world robotic tasks where the transition function may either be unavailable or in different analytical forms. Given the heavy dependence on source code for task environment understanding, this method could be essentially restricted to simulated RL environments only.\n\n2. Strong assumption on the fitness function F(.)\n\nAnother weak point is the strong assumption on the fitness function F(.). The evolutionary search for the LLM generated reward function requires a fitness function capable of assessing the quality of each proposed reward function. In this work, the fitness function F(.) is implicitly assumed to have access to the ground truth reward function to evaluate the induced policies of the proposed reward functions. This limitation implies that the method is applicable only to tasks that come with known ground-truth reward functions. This weakens the method's practical applicability, restricting it mainly to idealized or known environments, hindering its usefulness for real-world, less predictable reinforcement learning tasks."
                },
                "questions": {
                    "value": "While the pen spinning demonstration is impressive, it remains uncertain what is driving the task's success. Is it due to an effective reward design, which is the paper's main focus, or is it a byproduct of unspecified engineering efforts? Section 4.3 is not very clear and leaves room for several pertinent questions:\n\n1. The paper does not detail how the pre-trained policy was obtained. The statement, \"Specifically, we first instruct EUREKA to generate a reward for ... Then, using this pre-trained policy,\" leaves the readers wondering what exactly \"this\" references to. \n\nThe application of curriculum learning here appears to break the policy training into two main stages - an initial 'pre-training' stage and a 'fine-tuning' stage with the pre-trained policy. If this interpretation is accurate, clarity around the following is crucial:\n\n2. Is the training process \u2013 both pre-training and fine-tuning stages \u2013 guided by the reward functions derived using the LLM-powered reward design method proposed in this paper? Are these reward functions identical?\n\n -  If they are, there needs to be a detailed explanation of the distinctiveness of 'pre-training' and 'fine-tuning' when it comes to the optimization of policies under the same reward function. The reason how a dual-stage policy optimization can notably enhance policy performance remains under-explained. Additionally, if this is the case, it appears that the success of the task may be due to the dual-stage policy optimization rather than the reward design, casting doubts about the central argument of the paper.\n\n -  If they're not, clarity is needed on how these varying reward functions are generated, and what differences exist between them. Furthermore, how does this dual-stage reward function model vary from the primary algorithm proposed? How to ascertain the number of reward functions sufficient for a given task?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7937/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730700335,
            "cdate": 1698730700335,
            "tmdate": 1699636974829,
            "mdate": 1699636974829,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Bm4C6Zd1G8",
                "forum": "IEduRUO55F",
                "replyto": "eRaYz1TAV9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7937/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7937/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer XD65,\n\nThank the reviewer for your thoughtful comments and suggestions! Here, we respond to the specific questions and comments the reviewer raises. Please let us know if you have lingering questions and whether we can provide any additional clarifications during the discussion period to improve your rating of our paper. \n\n---\n**Question/Comment 1**:  Unrealistic assumption of access to the environment source codes; reinforcement learning setups only require access to a black-box simulation.\n\n**Response 1**: We would like to clarify that the problem setting of our work is not reinforcement learning, but rather reward design. This is traditionally done by humans who write and refine reward function codes in trial-and-error fashion in order to induce the desired behavior; Eureka automates this at scale. Its outer loop is a code generation problem, and it is standard to assume access to parts of the source code in order to generate/complete other parts of the code [1]. Furthermore, as we have clarified in our updated manuscript, Eureka\u2019s \u201cenvironment as context\u201d procedure requires only the environment\u2019s state and action variables to be exposed. This information can be supplied, for example, via an API without requiring the full environment code; see Example 1 in Appendix E, where we showed that a documentation-style description of the environment variables is sufficient for Eureka to generate effective reward functions. Eureka\u2019s inner-loop does use RL to evaluate its reward candidates, and this inner-loop indeed only accesses the environment with Eureka generated reward function as black-box simulation. We hope that this explanation helps clarify the misunderstanding regarding Eureka\u2019s use of environment as context and the problem setting it tackles.\n\n---\n**Question/Comment 2**:  Strong Assumptions on fitness function F(); This limitation implies that the method is applicable only to tasks that come with known ground-truth reward functions.\n\n**Response 2**: We would like to clarify that the ground-truth reward functions are simply sparse task success criteria, which are easy to define in practice. On all our 29 benchmark tasks, they can be written in one line of code (see Appendix B), which is much shorter and simpler than the human-written shaped reward functions that we compare Eureka to. Therefore, we believe that \u201ctasks that come with known ground-truth reward functions\u201d are not difficult to satisfy in practice, and Eureka readily applies to all such tasks with programmatic success conditions. This is a large space of tasks, and we have extensively evaluated a broad spectrum of robotic tasks in our experiments. Therefore, we believe that our approach\u2019s applicability is already broad.\n\nFurthermore, we do even show results where no fitness function is analytically expressed. In Section 4.4, we have studied cases where the desired behaviors are difficult to express via a mathematical function, such as running in a stable gait. In these cases, we have introduced Eureka from Human Feedback, which allows humans to express the \u201cfitness\u201d feedback in textual form, bypassing an analytical fitness function. We have shown that Eureka is indeed compatible with such a notion of fitness and can effectively generate reward functions that better align with human intention. In our limitation section in the updated manuscript (Section 5), we have also discussed the possibility of using a vision-language model (VLM) to construct automated textual feedback to improve the reward functions, providing a scalable alternative to human-based fitness feedback.\n\nPlease let us know whether our response has sufficiently addressed your concern about the fitness function; we are happy to provide more clarification as we believe that this is an important point that we\u2019d like the reviewer to reach an agreement with. \n\n---\n**Question/Comment 3**:  Clarification on how the pre-trained policy was obtained.\n\n**Response 3**: The pre-training stage is identical to the original Shadow Hand environment in the Isaac Gym benchmark suite we study, except that we swapped out the cube object to the pen object. The application of Eureka for this stage is identical to the benchmark tasks, in which Eureka iteratively discovers better reward functions. The best Eureka reward is then used to train a policy to solve the task; the resulting converged policy from this training is the \u201cpre-trained\u201d policy. \n\n---\n**Question/Comment 4**:  Is the training process \u2013 both pre-training and fine-tuning stages \u2013 guided by the reward functions derived using the LLM-powered reward design method proposed in this paper? Are these reward functions identical?\n\n**Response 4**: Yes, during both stages, the policies are trained using the best Eureka-discovered reward function from the pre-training stages. These reward functions are hence identical."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700008409775,
                "cdate": 1700008409775,
                "tmdate": 1700008409775,
                "mdate": 1700008409775,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RwjLIVHRXy",
                "forum": "IEduRUO55F",
                "replyto": "Ykqgo5SYXN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_XD65"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_XD65"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, \n\nThank you for the detailed response! They have clarified some of my doubts. I'll keep my score for now as the assumptions on your version of \"known environments\" and \"known sparse rewards\" are too strong for the works on reward design."
                    }
                },
                "number": 31,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605374541,
                "cdate": 1700605374541,
                "tmdate": 1700605374541,
                "mdate": 1700605374541,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "M8mKWYcthK",
            "forum": "IEduRUO55F",
            "replyto": "IEduRUO55F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7937/Reviewer_hi7d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7937/Reviewer_hi7d"
            ],
            "content": {
                "summary": {
                    "value": "The main idea of the paper is to have a LLM provide the reward function for RL problems.\nTo achieve this, the LLM gets to see the code of the environment along with the plain text description of the task. The task for the LLM is to provide sample code for reward functions, which are in turn improved using an evolutionary search. The experiments section of the paper goes through a number of environments, where performance is shown to be better than performance using human-designed reward functions. For the examples provided, Eureka does not use task-specific prompts."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I love the idea of using an LLM to provide initial versions of the reward functions, and to then improve it using evolutionary search. Moreover, the evaluation shows that the approach can deal with challenging environments, leading to good solutions or solutions for problems that have not been solved before. The work is also well motivated, and potentially lead to interesting advances in RL itself; it would be quite interesting to see this published and available for further research.\n\nThe work already contains inputs/outputs of the approach in the appendix, and code and prompts are expected to be released."
                },
                "weaknesses": {
                    "value": "While the paper does do a great job in selling the idea, there's a frustrating lack of technical detail in the main part of the paper. One example to illustrate this problem: The subsection on evolutionary search provides no detail on what is the exact input, outputs, or about the specific method being used. This is one core aspect of the proposed approach, and would require more details to be understandable. I understand some parts of this appear in the appendix or will be clear from code release, but the main part should make the core parts of the approach more clear.\n\nIn the Eureka algorithm description, it is a bit unclear to me what the reward reflection part does; for example the algorithm specifies the reward functions and their evaluations are provided as inputs, but the text also talks about the choices of RL method that are important for this part. There's only little information in the text that tells me how it works.\n\nSimilarly, I like the idea of \"environment as context\", but it would be good to know what is considered here as environment (what does it mean), for example to what level of detail does the environment need to be described / down to what level do you have to go. The appendix describes that, due to limited context length, the only the observation context will be used, but for simulators different from Isaac, what information do you expect you need to provide for this to work.\n\nThis could be connected with a missing discussion of limitations of the approach, for example do you expect this approach to be useful when you do not have full access to the code of the environment but maybe just some API, or if the environment is the real world.\n\nMaybe more philosophically but I am also not quite sure about the classification of the generation as \"zero-shot\" as it is unclear what the LLM has actually seen during training, and it would be interesting to see further investigations of this (not necessarily in this work) and capabilities of transferring domain knowledge - the release of benchmarks after the cut-off doesn't necessarily mean there was no environment like this before.\n\nMost of the above weaknesses impact the presentation of the work; while formal presentation of the work is good overall, Figure 2 and Figure 3 could be improved for contrast and font sizes."
                },
                "questions": {
                    "value": "My main questions are around limitations as mentioned above, and the types of problems this is expected to work well for (and where do you see this not working). \nTo what extent the approach makes use of information available at training time (eg about the simulator or environments). More directly about the approach I would find it interesting to hear about approximate run times from beginning to end, for some of the examples."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7937/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807187057,
            "cdate": 1698807187057,
            "tmdate": 1699636974668,
            "mdate": 1699636974668,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Bs7h66i18G",
                "forum": "IEduRUO55F",
                "replyto": "M8mKWYcthK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7937/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7937/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer hi7d,\n\n\nThank you for your thoughtful comments and suggestions! Here, we respond to the specific questions and comments the reviewer raises. Please let us know if you have lingering questions and whether we can provide any additional clarifications during the discussion period to improve your rating of our paper. \n\n---\n**Question/Comment 1**: The main part of the paper lacks technical details. For example, in the subsection on evolutionary search, what is the exact input, outputs, or about the specific method being used? \n\n**Response 1**: We have revised our manuscript to clarify the technical approach in Section 3 and added pointers to the Appendix for specific prompts and examples when relevant. Please let us know whether there are additional places you\u2019d like us to add details.\n\nIn the context of evolutionary search iteration, the inputs are (a) the best reward function from the previous iteration, (b) its reward reflection (Section 3.3), and (c) the prompt instruction for how to mutate the provided reward function given its reward reflection feedback. These inputs will be the prompt to the LLM, which then generates $K$ i.i.d. outputs, corresponding to $K$ reward samples. \n\n---\n**Question/Comment 2**: Clarifications on reward reflection. For example, the algorithm specifies the reward functions and their evaluations are provided as inputs, but the text also talks about the choices of RL method that are important for this part. \n\n\n**Response 2**: Thanks for this suggestion! We have updated our manuscript to point to concrete reward reflection examples in App. G.1. The reviewer is correct in that reward reflection is a textual evaluation of reward function (e.g., summarizing the training dynamics of its components). The subsequent discussion about the choices of RL method is meant to provide intuitive justification as to why reward reflection is helpful. We have improved the writing to more cleanly separate the procedure itself from its motivations. Please let us know if there is any remaining clarification we should include. \n\n---\n**Question/Comment 3**: For \u201cenvironment as context\u201d, what level of detail does the environment need to be described. For simulators different from Isaac, what information do you expect you need to provide for this to work.\n\n**Response 3**: \u201cEnvironment as context\u201d requires only the state and action variables in the environment to be exposed in the source code; in practice, as described in Appendix D, we have an automatic script to extract just the observation function from all raw environment source code. Note that our prompts and the observation source code do not reveal identity of the simulator, so Eureka is already general and we expect the same level of information, namely the names of state and action variables in the environment, to be needed for other simulators. To empirically validate this claim, we have also added an experiment testing Eureka on the Mujoco Humanoid environment; its observation code is included in Appendix E. As seen, the Mujoco variant exposes variables in a comment block whereas the Isaac Gym variant exposes them as class attributes, but since both contain sufficient state and action information, Eureka is able to competently generate effective reward functions in both cases, showcasing its generality. \n\n---\n**Question/Comment 4**: A limitation section should be included. Do you expect the approach to be useful when we do not have full access to the code of the environment, or if the environment is the real world.\n\n**Response 4**: Thanks for this suggestion! We have included a limitation section in our updated manuscript that details Eureka\u2019s current limitations and several future work directions to overcome them. \n\nWe do expect the approach to be useful when we do not have full access to the code of the environment. As discussed in the previous question, Eureka only needs to know the state and action variables of the environment in its context to generate reward functions; this information could be automatically extracted (as in our current approach) or accessed via an API (as the reviewer suggests) without access to the full environment code. \n\nIf the environment is real world, as we expanded in our new limitation section, we believe that Sim2Real approaches are particularly promising. This approach allows Eureka to readily combine with mature Sim2Real techniques to enable real-world transfer of policies learned in simulation. Another approach is to use state-of-art state estimation techniques to construct a symbolic representation of the real-world environment to allow Eureka to define reward functions. State-estimation based solutions have been attempted by many other works targeted at learning and deploying in the real world [1,2]."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700008758843,
                "cdate": 1700008758843,
                "tmdate": 1700008758843,
                "mdate": 1700008758843,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "epkpmQxFIT",
                "forum": "IEduRUO55F",
                "replyto": "PR25Ss6E6h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_hi7d"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_hi7d"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\nThank you for clarifying my questions. I am satisfied the additions address my concerns."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700569095572,
                "cdate": 1700569095572,
                "tmdate": 1700569095572,
                "mdate": 1700569095572,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zpptFKNqGs",
            "forum": "IEduRUO55F",
            "replyto": "IEduRUO55F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7937/Reviewer_DCBe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7937/Reviewer_DCBe"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel approach to leverage LLMs to plan/control complex low-level manipulation tasks in the form of an LLM-powered reward function generator. Its generation algorithm performs iterative improvements over an initially designed reward function without task-specific prompting nor a few short examples, by automatically calculating the fine-grained fitness of a policy in text over a sequence of executions on a target RL environment and using the LLM to improve the code. The presented approach is evaluated in varied RL environments and experimental results show it outperforms RL-expert humans on the majority of tasks. Moreover, the authors provide a case study of how the proposed approach enables higher complexity tasks where manual reward policy design is challenging."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The submitted manuscript is very well written and presents a novel and interesting approach to automatically generate reward functions for simulated RL environments, which seemingly could be applied to different scenarios.\n\nIt presents a clever approach to leveraging recent LLMs' zero-shot code generation ability to both understand a simulation environment and to iteratively improve generated reward functions that would be hard to manually author and tune.\n\nMoreover, the described evolutionary search and reward relection components or the approach, while not groundbreaking, provide interesting insights into the problem and on better interacting with LLMs for code generation."
                },
                "weaknesses": {
                    "value": "One of the main weaknesses of the submitted paper is the lack of a Limitations section/discussion, or such discussion throughout the text. While the authors claim the generality of Eureka, the proposed approach has only been evaluated on a single base simulator (Isaac Gym) and with a fixed RL algorithm. In other words, the claim seems to be overstated. \n\nAnother weakness is the experiment part, while the submitted text showcases different (and relevant) comparisons with human results, the human rewards are zero-shot and not tuned for many RL trials to further improve the performance. Therefore, I believe the comparison may be unfair. If you tune the human rewards in this baseline (e.g. search the weights for different reward terms) and train RL for many trials (same as the cost of the evolutionary search in Eureka ), some claims may not hold.\n\nA specific discussion I missed was about how the proposed approach handles the difference between optimizing for the internals of the simulation vs its sensing/acting interface. The former should be avoided in any general approach. The authors claim to use an \"automatic script to extract just the observation portion of the environment source code\", but this doesn't necessarily guarantee no leaks or that such observation code abstraction level leaks details.\n\nMoreover, as the proposed approach depends on feeding the environment code to the LLM, besides just claiming the \"the observation portion of the environment\", I believe a more in-depth discussion is needed on how Eureka could be adapted to a) more complex environments, which may be too large for the model context windows; and b) scenarios of interaction with the real world (actual robot control).\n\nParticularly for a), this is a critically important discussion. E.g., What would be the impact on the pen spinning demo with more detailed material characteristics and physics (friction, inertia, actuator latencies, etc.)?\n\nThe authors also claim only a \"handful\" of LLM output is enough. However, 16 is hardly a handful (<5). Intuitively the number of samples to obtain one fully executable policy will grow in proportion to simulator and environment complexity. However, again there is no discussion of such limitations of the approach."
                },
                "questions": {
                    "value": "In view of the claim of generality, how to qualify/quantify the impact of different simulation platforms (and level of physics detail) or RL algorithms? Please also comment on the scenario of interfacing with the real world."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7937/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7937/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7937/Reviewer_DCBe"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7937/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699252737311,
            "cdate": 1699252737311,
            "tmdate": 1700642096774,
            "mdate": 1700642096774,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uoAn8ETTkt",
                "forum": "IEduRUO55F",
                "replyto": "zpptFKNqGs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7937/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7937/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer DCBe,\n\n\nThank you for your thoughtful comments and suggestions! Here, we respond to the specific questions and comments the reviewer raises. Please let us know if you have lingering questions and whether we can provide any additional clarifications during the discussion period to improve your assessment and rating of our paper. \n\n**Question/Comment 1**: Human rewards are zero-shot and not tuned for many RL trials to further improve the performance. The comparison to human rewards may be unfair. \n\n**Response 1**: \n\nFirst, there are good reasons to believe that the human reward functions are representative of expert-level human reward functions and are not \u201czero-shot\u201d. Given that they are written by RL researchers who designed the benchmarks, we hypothesize that they are likely carefully tuned to ensure that they are optimizable by their choice of reinforcement learning algorithms \u2013 so that the benchmarks are solvable; we discuss this aspect in Section 4.1. In particular, the authors of the bi-manual Dexterity benchmark wrote the following excerpt in Appendix 2.1 of their paper, confirming judicious and iterative reward shaping, as per common practice: \n\n\u201cDesigning a reward function is very important for an RL task\u2026 because the scenarios of each task are different, the hyperparameters of the reward function will inevitably be different. We have tried our best to avoid manual reward shaping for each task provided that RL can be successfully trained.\u201d \n\nThis excerpt confirms that their reward functions are not zero-shot. Furthermore, their codebase also has commented-out reward snippets (e.g., https://github.com/PKU-MARL/DexterousHands/bidexhands/tasks/shadow_hand_block_stack.py#L1456, \nhttps://github.com/PKU-MARL/DexterousHands/bidexhands/tasks/shadow_hand_kettle.py#L1488), which are likely prior attempted reward functions/components that did not work as well. \n\nGiven this and the fact that we cannot control for how the original human reward engineering took place, we believe that our experiments are a valuable and meaningful comparison of Eureka\u2019s reward generation capability against expert humans.\n\nSecond, we note that the purpose of comparing against the official human reward functions is to be able to ground the quality of Eureka reward functions against some pre-existing reward functions that we know to be competent. It is certainly possible to come up with better reward functions to compare against, but we instead believe that the interesting scientific finding is rather the fact that Eureka is capable of improving reward functions over multiple iterations without manual prescription. It is this improvement capability that enables Eureka reward functions to eventually surpass human reward functions for various tasks despite the inferior quality of Eureka\u2019s initial reward generations (Iteration 1 in Figure 1).\n\n---\n**Question/Comment 2**: A baseline that searches the weights for different reward terms in the human rewards should be considered.\n\n**Response 2**: Thank you for this suggestion. We have implemented this baseline for Humanoid in the Isaac suite. More specifically, we choose 4 components from the respective human reward functions and grid search the best weight combination from having 3 possible values for each weight: 0.1x, 1x, 10x the original value. Therefore, we search over 81 tuned human reward functions (80 new ones), making the comparison fair to Eureka in terms of the total reward candidates searched. The results are below: \n\n|          | Eureka   | Human | Human (Tuned) |\n|----------|----------|-------|---------------|\n| Humanoid | **8.24** | 6.28  | 7.47          |\n\nAs shown, while Human (Tuned) does improve, the relative ordering between Eureka and Human does not change. The relatively small gap between Human and Human (Tuned) also suggests that the official human reward function is reasonably well-tuned and competent. Finally, we note that this is an unfairly advantaged baseline to Eureka because Eureka is meant to design a reward function from scratch (hence a reward design algorithm) whereas this baseline tunes an existing reward function. Even then, given the same reward sample budget, Eureka\u2019s best reward generated from scratch outperforms the best tuned human reward function. \n\n---\n**Question/Comment 3**:  Discussion on whether the proposed approach handles the difference between optimizing for the internals of the simulation vs its sensing/acting interface.\n\n**Response 3**: Thank you for this suggestion. We have confirmed that our observation code does not leak details about the simulation internals; an example of extracted observation code is in Appendix D (Example 1).  In general, we have also revised our manuscript to include a more concrete description of our \u201cenvironment as context\u201d procedure to clarify that observation code needs to expose only usable state variables in the environment for our proposed approach to work well."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700009291198,
                "cdate": 1700009291198,
                "tmdate": 1700009291198,
                "mdate": 1700009291198,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vJvfCVUg9j",
                "forum": "IEduRUO55F",
                "replyto": "9vUk0Hg4mm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_DCBe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_DCBe"
                ],
                "content": {
                    "comment": {
                        "value": "A quick question before I make the final decision. What is the error bar of the Human (Tuned)? Can Eureka in fact produce reward functions that are statistically significantly better than Humans (Tuned)?"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578218694,
                "cdate": 1700578218694,
                "tmdate": 1700578218694,
                "mdate": 1700578218694,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9wBClsRe6O",
                "forum": "IEduRUO55F",
                "replyto": "Ir7coKfNsu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_DCBe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_DCBe"
                ],
                "content": {
                    "comment": {
                        "value": "I am quite hesitant about making a decision because I've noticed in Figure 4 that the gap between EUREKA and human is 55/45, which I believe indicates a statistical difference. However, considering that the performance of human tuning later increases, I suspect that the gap between EUREKA and human (tuned) might not be significant across the 20 tasks, implying that the human reward still has potential. Of course, I am not currently asking the authors to conduct additional experiments, but I hope the authors can provide some extra explanations to address whether my concerns are reasonable. In any case, my stance is neutral at the moment, and I will follow the consensus reached through discussion with the other reviewers regarding the decision."
                    }
                },
                "number": 34,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700628667664,
                "cdate": 1700628667664,
                "tmdate": 1700628667664,
                "mdate": 1700628667664,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LDhQWWzU69",
                "forum": "IEduRUO55F",
                "replyto": "8KqnTIV1eu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_DCBe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7937/Reviewer_DCBe"
                ],
                "content": {
                    "comment": {
                        "value": "I am grateful for the public opinion and the efforts of the author. Although there may still be some issues with this paper, there is no doubt that it is more refined than the original version. By the way, it seems that excessive promotion of articles on Twitter is not necessarily a good thing."
                    }
                },
                "number": 36,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7937/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642405384,
                "cdate": 1700642405384,
                "tmdate": 1700642405384,
                "mdate": 1700642405384,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]