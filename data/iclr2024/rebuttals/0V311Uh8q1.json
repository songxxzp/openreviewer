[
    {
        "title": "Algorithmic Stability Unleashed: Generalization Bounds with Unbounded Losses"
    },
    {
        "review": {
            "id": "T5aXlwf2D3",
            "forum": "0V311Uh8q1",
            "replyto": "0V311Uh8q1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5181/Reviewer_wVug"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5181/Reviewer_wVug"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies high-probability generalization bounds for stable algorithms with unbounded loss functions. In particular, the paper considers a symmetric and totally Lipschitz stable algorithm, where the change of the output model with respect to the perturbation of a single example follows from a subweibull distribution. To this aim, the paper first builds a bound on the p-norm of a summation of independent subweibull random variables, and apply it to develop concentration inequalities for random functions with the increment behaving as a subweibull random variable."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper studies learning problems with unbounded loss functions. The assumption on subweibull distribution is more relaxed than the previous sub-gaussian and sub-exponential distribution. The derived results recover the previous results for both the sub-gaussian and sub-exponential case."
                },
                "weaknesses": {
                    "value": "The high-probability bound established in the paper is of the order $O(\\sqrt{n}\\gamma)$, which can only imply sub-optimal bounds. One would be more interested in bounds of the order $O(1/\\sqrt{n}+\\gamma\\log n)$, which were developed for learning with bounded loss functions.\n\nThe analysis seems to be not clear and there are several issues in the theoretical analysis. Furthermore, the main result is based on Lemma 5, which seems to be a corollary of the results in Lata\u0142a, 1997. The analysis in Lata\u0142a, 1997 gives general bounds on the p-norm of a summation of random variables. Lemma 5 is derived by considering the special subweibull random variables.\n\nThe contribution seems to be a bit incremental. The problem setting follows the concentration in unbounded metric spaces established in Kontorovich (2014). The main contributions are the extension of the analysis in Kontorovich (2014), Maurer & Pontil (2021) from sub-gaussian and sub-exponential distribution to subweibull distribution. It is not clear to me whether the extension is substantial, and the paper does not include enough examples to justify the importance of subweibull distribution in practice."
                },
                "questions": {
                    "value": "- In the proof of Lemma 5, the paper shows that $\\|\\sum_i a_iX_i\\| \\leq \\|\\sum_i a_i Z_i\\|$. It is not clear to me how this inequality holds. Indeed, the random variable $Z_i$ is not clearly defined, and the paper only assumes $P(|Z_i|\\geq t)=\\exp(-t^\\alpha)$. However, it is not clear to me how to use this property to derive $\\|\\sum_ia_iX_i\\|\\leq \\|\\sum_ia_iZ_i\\|$.\n\n- In the proof of Lemma 5, the paper uses the inequality $\\inf\\{t>0:\\sum_i\\max\\{X_i(t),Y_i(t)\\}\\leq p\\}\\leq \\inf\\{t>0:\\sum_iX_i(t)\\leq p\\}+\\inf\\{t>0:\\sum_iY_i(t)\\leq p\\}$. It is not clear to me how this inequality holds. Would you please provide details? Furthermore, you have $p(a_ie^2/t)^2\\|Z_i\\|^2\\leq p$ in the second line, but the last line is $p^2\\Gamma(2/\\alpha+1)e^4/t^2\\|a\\|^2 \\leq p$.\n\n- In the proof of Lemma 5, you have the identity $p^{-1-(p-2)/\\alpha}=p^{-p/\\alpha}p^{(2-\\alpha)/\\alpha}$, which is not correct.\n\n- The generalization bounds have very complicated dependency on the parameter $\\alpha$, which makes the results hard to interpret. How should we understand the effect of $\\alpha$ on generalization?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5181/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698114916310,
            "cdate": 1698114916310,
            "tmdate": 1699636514210,
            "mdate": 1699636514210,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rhAfontDV1",
                "forum": "0V311Uh8q1",
                "replyto": "T5aXlwf2D3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5181/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5181/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part I"
                    },
                    "comment": {
                        "value": "Thanks for your time and thoughts. We will answer all your questions. \n\nQuestion 1: Issue in Weakness ``The high-probability bound established in the paper is ...''\n\nAnswer: Recently, (Bousquet et al. 2020) provided a sharper generalization bound of the order $O(1/\\sqrt{n} + \\gamma \\log n)$ for algorithmic stability. In their proof, they also use the $p$-th moment technique, and the $p$-th moment version of the Bounded differences/McDiarmid\u2019s inequality is an important component. It seems to be straightforward to give better generalization inequality by combining our $p$-th moment inequality and the technique of peeling the dataset in (Bousquet et al. 2020). Since our work mainly follows the work  (Kontorovich, 2014; Maurer \\& Pontil, 2021), we didn't consider giving sharper generalizations in the manuscript.\n We will include this part of the results in the final version.\n\nQuestion 2: Issue in Weakness ``The analysis seems to be not clear and there are several issues ... ''\n\nAnswer: Our work is not simply a corollary of the results in Lata\u0142a, 1997. The analysis in (Lata\u0142a, 1997) gives bounds for the \\emph{symmetric} random variables, which can not be directly used in our analysis. \nOn one hand, we need to carefully construct new random variables to satisfy the symmetry condition, for example, we introduce random variables $Y_i$, $Z_i$ in the proof of Lemma 5. On the other hand, since we study the weighted summation, Khinchin-Kahane inequality is also required to use.\n\nQuestion 3: Issue in Weakness ``The contribution seems to be a bit incremental. The problem setting follows ... ''\n\nAnswer: We believe our work is not incremental. Here, we discuss the technical novelties of Theorem 1. The first challenge to prove Theorem 1 is that we deal with the concentration of the general function $f$. Actually, the related work (Kontorovich, 2014) used the martingale method to decompose $f-\\mathbb{E}f$, while (Maurer and Pontil, 2021) use the sub-additivity of entropy to decompose the general function $f-\\mathbb{E}f$. After the decomposition, the next step of (Kontorovich, 2014) and (Maurer and Pontil, 2021) is to bound the MGF ($\\mathbb{E}e^{\\lambda Z}$) or a variant MGF ($\\mathbb{E}Z^2 e^{\\lambda Z}$), respectively. The second challenge is that the MGF is bounded for sub-Gaussian and sub-exponential random variables, but it is unbounded for subWeibull variables because there is some convexity lost. The standard technique to prove the MGF failed for the heavy-tailed subWeibull random variables, as discussed in the penultimate paragraph of the Introduction and Remark 1. This implies that if we do not study the MGF, we need to consider different decomposition on the general function $f-\\mathbb{E}f$.\n\nTo address the first challenge, we introduce Lemma 4, where we decompose the general function $f-\\mathbb{E}f$ to the sum of independent sub-Weibul variables. In the proof of Lemma 4, a key step is that we need to construct a function $t \\to h(\\epsilon_n F_n(X_n, X'_n) +t)$ to apply our induction assumption. The proof of Lemma 4 may be simple, but Lemma 4 itself is very useful. For example, one can use Lemma 4 to prove more McDiarmid inequalities, e.g., the polynomially decaying random variables, which will enrich the family of McDiarmid inequality.\n\nTo address the second challenge, rather than bounding the MGF, we bound the $p$-th moment of the sum of subWeibull random variables. Thanks to the fact that subWeibull random variables are log-convex for $\\alpha \\leq 1$ and log-concave for $\\alpha \\geq 1$, we can apply Lata{\\l}a's inequality (Lemma 7 and Lemma 8) to bound this $p$-th moment. However, it is not a direct application of the Lata{\\l}a's inequality. Lata{\\l}a's inequality holds for the $p$-th moment of the \\emph{symmetric} random variables. On one hand, we need to carefully construct new random variables to satisfy the symmetry condition, for example, we introduce random variables $Y_i$, $Z_i$ in the proof of Lemma 5. On the other hand, since we study the weighted summation, Khinchin-Kahane inequality is also required to use.\n\nFinally, we would like to quote the comment of Reviewer LoHq to confirm the technical novelties: \"The proof consists of two steps: reduction to sum of independent sub-Weibul variables (Lemma 4), and the corresponding moment bound on a sum of sub-weibull rv's. The first part is done thanks to a conditioning trick with Jensen's inequality. The second part is done thanks to a carful application of Latala's inequality (Lemma 7 in the appendix).\"\n\nProving generalization bounds for heavy-tailed losses is a significant direction in the learning theory community. For the tool of PAC-Bayes, Space Complexity, and Information theory, there are many works that study the heavy-tailed losses, while it is lacking for the tool of stability. Our work is the first attempt to give generalization bounds for the heavy-tailed loss for Algorithmic Stability. Therefore, we believe our extension is substantial."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5181/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678054280,
                "cdate": 1700678054280,
                "tmdate": 1700678054280,
                "mdate": 1700678054280,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CpuRue5nzG",
                "forum": "0V311Uh8q1",
                "replyto": "T5aXlwf2D3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5181/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5181/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part II"
                    },
                    "comment": {
                        "value": "Question 4: Issue in Question 1 ``In the proof of Lemma 5, the paper shows ...''\n\nAnswer: Thanks for this review. Let $\\\\{Z_i\\\\}^n_{i=1}$ be independent symmetric random variables satisfying $|Z_i| \\overset{d}{=} Y_i$ and $\\mathbb{P}(|Z_i| \\ge t) = \\exp(-t^\\alpha)$ for all $t \\ge 0$. The symmetric variable $Z_i$ is introduced to use the Lata{\\l}a's inequality.\n\n $ || \\sum\\_{i=1}^p  a\\_i X\\_i ||\\_p \\leq || \\sum_{i=1}^p a\\_i Z\\_i ||\\_p $? Did the reviewer mean $|| \\sum\\_{i=1}^p a\\_i  \\epsilon\\_i Y\\_i ||\\_p \\leq || \\sum\\_{i=1}^p a\\_i Z\\_i ||\\_p $? \nRigorously, it should be $|| \\sum\\_{i=1}^p a\\_i  \\epsilon\\_i Y\\_i ||\\_p = || \\sum_{i=1}^p a\\_i Z\\_i ||\\_p $. Note that the tail of $Y\\_i$ is $ P(Y\\_i \\geq t) \\leq e^{-t^{\\alpha}}$, the $Y\\_i$ are positive since $Y\\_i = (|X\\_i| - (\\log 2)^{1/\\alpha})\\_+$, and tail of  $Z_i$ is also $\\mathbb{P}(|Z\\_i| \\ge t) = \\exp(-t^\\alpha)$.  According to the equivalence of the tail and the moment, it is clear that  $ || \\sum\\_{i-1}^p a\\_i \\epsilon\\_i Y\\_i ||\\_p = || \\sum\\_{i=1}^p a\\_i Z\\_i ||\\_p $.\n\n\nQuestion 5: Issue in Question 2 ``In the proof of Lemma 5, the paper uses the inequality ...''\n\nAnswer:  Thanks for this review. \n\n(1) Sorry that we missed a constant $2$ in this step. The inequality is that\n\n\\begin{align*} \n\\inf [ t>0 : \\sum\\_i \\max [ X\\_i(t), Y\\_i(t) ]  \\leq p ] \\leq \\inf [ t>0 : \\sum\\_i X\\_i(t) +  \n \\sum\\_i Y\\_i(t)  \\leq p ]\n \\leq \\inf [ t>0 : 2 \\sum\\_i X\\_i(t)  \\leq p ] + \\inf [ t>0: 2 \\sum\\_i X\\_i(t)  \\leq p ].\n\\end{align*}\n\n We have fixed this issue.\n\n(2) For the next question, the inequality holds because $||Z_i||\\_p^p = p \\Gamma(\\frac{p}{\\alpha} + 1)$ and we set $p=2$.\n\n\nQuestion 6: Issue in Question 3 ``In the proof of Lemma 5, you have the identity ...''\n\nAnswer: \n$p^{-1-(p-2)/\\alpha} = p^{-1-\\frac{(p-2)}{\\alpha}}  = p^{-p/\\alpha}p^{(2-\\alpha)/\\alpha}$.\nThis equality is correct, please check it again.\n\nQuestion 7: Issue in Question 4 ``The generalization bounds have very complicated dependency ...''\n\nAnswer: \n(1) We first discuss Theorem 1. \n$c_\\alpha =2 \\sqrt{2} ((\\log 2)^{1/\\alpha}+e^3\\Gamma^{1/2}(\\frac{2}{\\alpha} +1) + e^3 3^{\\frac{2-\\alpha}{3\\alpha}} \\sup_{p\\ge 2}p^{\\frac{-1}{\\alpha}} \\Gamma^{1/p}(\\frac{p}{\\alpha} +1) )$. According to the property of the Gamma function, $\\Gamma(\\frac{2}{\\alpha} +1)$ becomes bigger as $\\alpha$ becomes smaller. As for the the term $\\sup_{p \\geq 2} p^{\\frac{-1}{\\alpha}} \\Gamma^{1/p}(\\frac{p}{\\alpha} +1) )$,  the Stirling formula easily gives a concise form. \nBy the Stirling formula\n\\begin{align*}\nn! = \\sqrt{2\\pi n}n^ne^{-n + \\theta_n}, \\quad |\\theta_n|<\\frac{1}{12 n}, n>1,\n\\end{align*}\nwe get the following result \n\\begin{align*}\n    p^{-\\frac{1}{\\alpha}} \\Gamma^{1/p}(\\frac{p}{\\alpha} + 1) \n    \\leq\n    p^{-\\frac{1}{\\alpha}} (\\sqrt{2\\pi p/\\alpha} (\\frac{p}{e\\alpha})^{\\frac{p}{\\alpha}} e^{\\frac{\\alpha}{12p}} )^{1/p} \n    = p^{-\\frac{1}{\\alpha}} (\\sqrt{\\frac{2\\pi}{\\alpha}})^{1/p} p^{1/2p} (\\frac{p}{\\alpha})^{1/\\alpha} e^{\\alpha/12p^2 -1/\\alpha} \n    \\leq  (\\sqrt{\\frac{2\\pi}{\\alpha}})^{1/p}  e^{1/2e} \\frac{1}{(e\\alpha)^{1/\\alpha}} e^{\\alpha/12p^2} \n    \\leq  (\\sqrt{\\frac{2\\pi}{\\alpha}})^{1/2}  e^{1/2e} \\frac{1}{(e\\alpha)^{1/\\alpha}} e^{\\alpha/48},\n\\end{align*}\nwhere the first inequality uses the Stirling formula and the second inequality uses the fact that $p^{1/p} \\leq e^{1/e}$. Thus we can find that the term $\\sup_{p \\geq 2}p^{\\frac{-1}{\\alpha}} \\Gamma^{1/p}(\\frac{p}{\\alpha} +1) )$ is not involved in $p$ and only depends on $\\alpha$, and the smaller $\\alpha$ is, the bigger this term is.\nAccording to above analysis, we  have the conclusion that the smaller $\\alpha$ is, the constant related to $\\alpha$ (i.e., $c_\\alpha$) in the upper bound is bigger.  We then discuss Theorem 2. As discussed in Section 2.1, the smaller $\\alpha$ is, the heavier tail the random variable has, which means that $\\Delta_\\alpha$ is bigger according to the definition. By the above analysis, we  have the conclusion that the smaller $\\alpha$ is, the terms related to $\\alpha$ ($c_\\alpha$ and $\\Delta_\\alpha$) in the upper bound is bigger. This result is consistent with the intuition: heavier-tailed distribution, i.e., smaller $\\alpha$, will lead to a larger generalization bound.\n\nReferences\n\nOlivier Bousquet and Andre Elisseeff. Stability and Generalization. 2002.\n\nAryeh Kontorovich. Concentration in unbounded metric spaces and algorithmic stability. 2014.\n\nVitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. 2019.\n\nOliver Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable algorithms 2020.\n\nAndreas Maurer and Massimiliano Pontil. Concentration inequalities under sub-gaussian and sub-exponential conditions. 2021"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5181/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678857578,
                "cdate": 1700678857578,
                "tmdate": 1700729740890,
                "mdate": 1700729740890,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Mhu5qkfBUx",
            "forum": "0V311Uh8q1",
            "replyto": "0V311Uh8q1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5181/Reviewer_LoHq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5181/Reviewer_LoHq"
            ],
            "content": {
                "summary": {
                    "value": "The present paper proposes a stability generalization bound for a variant of non-uniform stability condition.\n\nTheir analysis revolves around a new form of McDiarmid inequality, where the differences are bounded by another function, which satisfies certain moment conditions (sub-Weibul). The proof consists of two steps: reduction to sum of independent sub-Weibul variables (Lemma 4), and the corresponding moment bound on a sum of sub-weibull rv's. The first part is done thanks to a conditioning trick with Jensen's inequality. The second part is done thanks to a carful application of Latala's inequality (Lemma 7 in the appendix). This inequality is a direct extension of one from Kontorovich (2014) to an interesting non-trivial case. I particularly like that Theorem 1 does not have any extra logarithms.\n\nIt would be great to include some analysis of how optimal is the bound. Whether there are some cases, where each of the terms is necessary. For example, is it possible to replace the first term with \\sqrt{\\sum_{i = 1} E (F_i)^{2}}, i.e. without the orlizc norm? I understand how to do it if we can sacrifice additional logorithm in the second term (e.g. using Thm 15.10 in [1]), is it possible to do it without this sacrifice?\n\nMy other concern is about sub optimality of the generalization bounds listed on pages 5-6. Since the the original paper Bousquet & Eliseef (2002), there have beed significant improvements in these bounds [2, 3]. The proof in [3] works directly with moment bounds, perhaps it is possible to make it work with their technique and obtain a better generalization inequality. Since you mention generalization bounds in the title, I think it is crucial to make the bound closer to state-of-the-art.\n\nI also want to point out that the condition $|S - S_i| \\leq F_i(X_i, X_i')$ is also to some extent uniform, since it does not depend on $X_1, \\dots, X_{i-1}, X_{i+1}, \\dots, X_{n}$. Does the condition hold e.g. for ridge regression that you mention in the introduction?\n\nTo sum up, although the paper provides interesting results, there is room for immediate improvement.\n\n[1] Boucheron et al (2014) Concentration inequalities.\n\n[2] Feldman and Vondrak (2019). Generalization bounds for uniformly stable algorithms.\n\n[3] Bousquet et al (2020). Sharper bounds for uniformly stable algorithms"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Novel version of bounded differences inequality, where the differences are not really bounded."
                },
                "weaknesses": {
                    "value": "Do not touch question of optimality.\n\nSuboptimal generalization bound."
                },
                "questions": {
                    "value": "Does the condition hold e.g. for ridge regression that you mention in the introduction?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5181/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5181/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5181/Reviewer_LoHq"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5181/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698604069652,
            "cdate": 1698604069652,
            "tmdate": 1699636514098,
            "mdate": 1699636514098,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YbOhUS81IG",
                "forum": "0V311Uh8q1",
                "replyto": "Mhu5qkfBUx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5181/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5181/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review. We will answer all your questions. \n\nQuestion 1: Issue in Summary ``It would be great to include some analysis of how optimal ...''\n\nAnswer:  Yes, the bound provided by Theorem 1 is solely in terms\nof $||F\\_i (X\\_i, X'\\_i)||\\_{\\psi\\_\\alpha}$.  It is possible to replace the orlizc norm with the variance $\\sqrt{\\sum\\_{i=1} \\mathbb{E}(F\\_i)^2} $, as you commented, however, an additional logarithms $\\log^{\\frac{1}{\\alpha}} (n+1)$ appeared in the second term, which is induced by taking the max operator outside the orlicz norm. Our method of using the orlicz norm does not introduce the logarithmic term. But we think this review is very constructive. From the classical central limit theorem (asymptotically\nthe distribution of the sum (properly scaled) is determined by the variance of the sum), we naturally expect the Gaussian part of the tail to depend on the variance $\\sqrt{\\sum_{i=1} \\mathbb{E}(F_i)^2} $ only.  We are working to provide a tighter bound without sacrificing the additional logarithm.\n\nWe now provide some discussions on the optimality of our bounds. In Theorem 1, if $0<\\alpha\\leq1$, our inequality \n\\begin{align*}\n|f(X\\_1,...,X\\_n) - \\mathbb{E} f(X\\_1,...,X\\_n)| \\le c\\_\\alpha \\left ( \\sqrt{\\log(\\frac{1}{\\delta})} \\left(\\sum\\_{i=1}^n ||F\\_i (X\\_i, X'\\_i)||\\_{\\psi_\\alpha}^2\\right)^{\\frac{1}{2}} + \\log^{1/\\alpha}(\\frac{1}{\\delta}) \\max\\_{1\\le i \\le n}||F\\_i (X\\_i, X'\\_i)||_{\\psi\\_\\alpha}\\right)\n\\end{align*}\nshows a mixture of\nsub-gaussian $\\sqrt{\\log(\\frac{1}{\\delta})}$\nand sub-weibull $\\log^{1/\\alpha}(\\frac{1}{\\delta})$ tails. The sub-Gaussian tail  is of course expected from the central limit theorem, and the sub-weibull tail captures the right decaying rate of the sub-weibull random variables. Therefore, our inequality successfully captures the right sub-Gaussian tail for small deviations and the right sub-weibull tail for large deviations, which also means that the convergence rate will be faster for small deviations and will be slower for large deviations.\n\nIf $\\alpha >1$, our inequality\n\\begin{align*}\n|f(X\\_1,...,X\\_n) - \\mathbb{E} f(X\\_1,...,X\\_n)|   \\le&c'\\_\\alpha\\left (\\sqrt{\\log(\\frac{1}{\\delta})} \\left(\\sum\\_{i=1}^n ||F\\_i (X\\_i, X'\\_i)||\\_{\\psi\\_\\alpha}^2\\right)^{\\frac{1}{2}} +  \\log^{1/\\alpha}(\\frac{1}{\\delta}) || (|| F (X, X')||\\_{\\psi_\\alpha}) ||\\_{\\alpha^\\ast}\\right)\n\\end{align*}\nfollows the same spirit and also exhibits a mixture of\nsub-gaussian $\\sqrt{\\log(\\frac{1}{\\delta})}$\nand sub-weibull $\\log^{1/\\alpha}(\\frac{1}{\\delta})$ tails.\n\nQuestion 2: Issue in Summary ``My other concern is about sub optimality of the generalization bounds ...''\n\nAnswer: Recently, (Bousquet et al. 2020) provided a sharper generalization bound for algorithmic stability. In their proof, they also use the $p$-th moment technique, and the $p$-th moment version of the Bounded differences/McDiarmid\u2019s inequality is an important component. It seems to be straightforward to give better generalization inequality by combining our $p$-th moment inequality and the technique of peeling the dataset in (Bousquet et al. 2020). Since our work mainly follows the work  (Kontorovich, 2014; Maurer \\& Pontil, 2021), we didn't consider giving sharper generalizations in the manuscript.\n We will include this part of the results in the final version.\n\nQuestion 3: Issue in Summary ``I also want to point out that the condition ...''\n\n\nAnswer: Yes. This condition holds for ridge regression. Here are the proofs.\n\nLet's consider that the loss function $f(x,y) = (h(x) - y)^2 $, $h: \\mathcal{X} \\to [0,1]$ is a Lipschitz function with Lipschitz constant $L$: $h(x) - h(y) \\leq L d(x,y)$, and $(\\mathcal{Z}, d_2)$ is the metric\nspace where $\\mathcal{Z} = \\mathcal{X} \\times [0,1]$ and $d_2((x,y), (x',y') ) = (d(x,x')^2 + |y-y'|^2)^{1/2}$. Let us take $\\psi[0,1]^2 \\to \\mathbb{R}$ to be $\\psi(h,y) = (h-y)^2$, which satisfies $\\max_{(h,y) \\in [0,1]^2} \\| \\nabla \\psi(h,y) \\|_2 =2^{3/2}$.\nIt follows that \n\\begin{align}\n|f(x,y) - f(x',y')| & = | (h(x) -y)^2 - (h(x') -y')^2 | \n \\leq 2^{3/2} ((h(x) -h(x'))^2 + (y - y')^2)^{1/2} \n \\leq 2^{3/2} (L^2 d(x ,x')^2 + (y - y')^2)^{1/2}  \\leq 2^{3/2} \\max [ 1, L ] d_2((x,y), (x',y')).   \n\\end{align}\nWhen $h(x) = ax $, we have $|f(x,y) - f(x',y')| \\leq 2^{3/2} \\max [ 1, a ] d_2((x,y), (x',y'))$. Corresponding $F_i(X_i,X'_i))$ to $d_2((x,y), (x',y'))$, we can conclude that the condition $|S-S_i| \\leq F_i(X_i,X'_i)$ holds for ridge regression.\n\nReferences\n\nOlivier Bousquet and Andre Elisseeff. Stability and Generalization. 2002\n\nAryeh Kontorovich. Concentration in unbounded metric spaces and algorithmic stability. 2014\n\nVitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. 2019\n\nOliver Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable algorithms 2020\n\nAndreas Maurer and Massimiliano Pontil. Concentration inequalities under sub-gaussian and sub-exponential conditions. 2021"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5181/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677921742,
                "cdate": 1700677921742,
                "tmdate": 1700677937541,
                "mdate": 1700677937541,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tAKZoji5eT",
            "forum": "0V311Uh8q1",
            "replyto": "0V311Uh8q1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5181/Reviewer_K21p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5181/Reviewer_K21p"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies algorithmic stability and generalization bounds for unbounded losses of independent subweibull random variables. Specifically, the concentration inequality for subweibull random variables (which covers subexponential or subgaussian random variables studied in the previous works) is established for general losses for both heavy tails and non-heavy tails variables. Based on this result, the generalization bounds in the order of $O(1/\\sqrt{n})$ for the symmetric and Lipschitz stable algorithms are derived, which recovers the previous results for subexponential or subgaussian distributions up to some constants."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "A more general stability and generalization results for the symmetric and Lipschitz stable algorithms are established by introducing the subweibull diameter of the input space. The concentration inequalities (Theorem 1) might be of independent interest."
                },
                "weaknesses": {
                    "value": "1. Although the paper extends the previous results on subexponential or subgaussian random variables to a more general setting, it seems that it is incremental. There is no technological or conceptual novelty here. The key result of the paper is Theorem 1, while the proofs are standard and straightforward. The generalization bounds can be easily obtained once the connection between the stability and generalization is established.\n\n2. The paper is worse written. Discussions of the main results (Theorems 1 and 2) are insufficient. For example, the bounds in Theorem 1 are related to $c_{\\alpha}$, which are dependent on $\\Gamma^{1/2}(\\alpha)$ and a $sup_{p\\ge 2}$ term.  The impact of the value of $\\alpha$ on the results needs to be discussed since it controls the degree of heavy tails of the random variable. Similarly, the value of $\\Delta_{\\alpha}$ should be discussed in Theorem 2. In addition, I would suggest the authors go through the paper and polish the language, some notations are not introduced and some symbols are inconsistent. For example, Page 4, the learning algorithm is sometimes notated by $A_S$ and sometimes $\\mathcal{A}_S$."
                },
                "questions": {
                    "value": "1.\tAs mentioned in weakness 1, it seems that the proofs of the theorems are standard. Could the authors point out the technique novelty of the paper?\n2.\tThe paper establishes the generalization bounds for the Lipschitz losses. It is possible to remove this assumption, or could we assume that the loss is smooth instead? \n3.\tWhat does \u201c\\mathcal{A} is a symmetric algorithm\u201d mean in Lemma 1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5181/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5181/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5181/Reviewer_K21p"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5181/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830219863,
            "cdate": 1698830219863,
            "tmdate": 1699636513997,
            "mdate": 1699636513997,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v6PWmKWQCn",
                "forum": "0V311Uh8q1",
                "replyto": "tAKZoji5eT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5181/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5181/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part I"
                    },
                    "comment": {
                        "value": "Thank you for your review. Here are the replies for your questions.\n\nQuestion 1: As mentioned in weakness 1, it seems that the proofs ...\n\nAnswer: We believe our work is not incremental. Here, we discuss the technical novelties of Theorem 1. The first challenge to prove Theorem 1 is that we deal with the concentration of the general function $f$. Actually, the related work (Kontorovich, 2014) used the martingale method to decompose $f-\\mathbb{E}f$, while (Maurer and Pontil, 2021) use the sub-additivity of entropy to decompose the general function $f-\\mathbb{E}f$. After the decomposition, the next step of (Kontorovich, 2014) and (Maurer and Pontil, 2021) is to bound the MGF ($\\mathbb{E}e^{\\lambda Z}$) or a variant MGF ($\\mathbb{E}Z^2 e^{\\lambda Z}$), respectively. The second challenge is that the MGF is bounded for sub-Gaussian and sub-exponential random variables, but it is unbounded for subWeibull variables because there is some convexity lost. The standard technique to prove the MGF failed for the heavy-tailed subWeibull random variables, as discussed in the penultimate paragraph of the Introduction and Remark 1. This implies that if we do not study the MGF, we need to consider different decomposition on the general function $f-\\mathbb{E}f$.\n\nTo address the first challenge, we introduce Lemma 4, where we decompose the general function $f-\\mathbb{E}f$ to the sum of independent sub-Weibul variables. In the proof of Lemma 4, a key step is that we need to construct a function $t \\to h(\\epsilon_n F_n(X_n, X'_n) +t)$ to apply our induction assumption. The proof of Lemma 4 may be simple, but Lemma 4 itself is very useful. For example, one can use Lemma 4 to prove more McDiarmid inequalities, e.g., the polynomially decaying random variables, which will enrich the family of McDiarmid inequality.\n\nTo address the second challenge, rather than bounding the MGF, we bound the $p$-th moment of the sum of subWeibull random variables. Thanks to the fact that subWeibull random variables are log-convex for $\\alpha \\leq 1$ and log-concave for $\\alpha \\geq 1$, we can apply Lata{\\l}a's inequality (Lemma 7 and Lemma 8) to bound this $p$-th moment. However, it is not a direct application of the Lata{\\l}a's inequality. Lata{\\l}a's inequality holds for the $p$-th moment of the \\emph{symmetric} random variables. On one hand, we need to carefully construct new random variables to satisfy the symmetry condition, for example, we introduce random variables $Y_i$, $Z_i$ in the proof of Lemma 5. On the other hand, since we study the weighted summation, Khinchin-Kahane inequality is also required to use.\n\nFinally, we would like to quote the comment of Reviewer LoHq to confirm the technical novelties: \"The proof consists of two steps: reduction to sum of independent sub-Weibul variables (Lemma 4), and the corresponding moment bound on a sum of sub-weibull rv's. The first part is done thanks to a conditioning trick with Jensen's inequality. The second part is done thanks to a carful application of Latala's inequality (Lemma 7 in the appendix).\"\n\nEstablishing the connection between the stability and generalization needs to consider different cases: $\\alpha>1$ and $0<\\alpha\\le 1$. Due to the fact that there is some convexity lost in the case $0<\\alpha\\le 1$, the proof excludes the Jensen\u2019s inequality and requires a different analysis.\n\nQuestion 2: The paper establishes the generalization bounds for the Lipschitz losses ...\n\nAnswer: \nOur generalization bound is derived by the established McDiarmid inequality, and a Lipschitz-type assumption is typically required for the McDiarmid inequality. Thus, this paper establishes the generalization bounds for the Lipschitz losses. But we think this review is inspiring.  \nThe smoothness itself is a Lipschitz-type assumption on the gradient. When assuming that the loss is smooth instead, it is possible to establish generalization bounds on the gradient by our concentration inequality. The generalization bound on the gradient is drawing increasing interest in the nonconvex learning of the learning theory community.\n\n\n\n\nQuestion 3: What does ``$\\mathcal{A}$ is a symmetric algorithm'' mean in Lemma 1?\n\nAnswer: This means that the algorithm's outputs remain unchanged when swapping the order of samples in the data set."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5181/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676953369,
                "cdate": 1700676953369,
                "tmdate": 1700676953369,
                "mdate": 1700676953369,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aASxIUDiMr",
                "forum": "0V311Uh8q1",
                "replyto": "tAKZoji5eT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5181/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5181/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part II"
                    },
                    "comment": {
                        "value": "Question 4: Issue in Weakness 2. ``The paper is worse written. Discussions of the main results ...''\n\nAnswer: \n\n(1) We first discuss Theorem 1. \n$c_\\alpha =2 \\sqrt{2} ((\\log 2)^{1/\\alpha}+e^3\\Gamma^{1/2}(\\frac{2}{\\alpha} +1) + e^3 3^{\\frac{2-\\alpha}{3\\alpha}} \\sup_{p\\ge 2}p^{\\frac{-1}{\\alpha}} \\Gamma^{1/p}(\\frac{p}{\\alpha} +1) )$. According to the property of the Gamma function, $\\Gamma(\\frac{2}{\\alpha} +1)$ becomes bigger as $\\alpha$ becomes smaller. As for the the term $\\sup_{p \\geq 2}$,  the Stirling formula easily gives a concise form. \nBy the Stirling formula\n\\begin{align*}\nn! = \\sqrt{2\\pi n}n^ne^{-n + \\theta_n}, \\quad |\\theta_n|<\\frac{1}{12 n}, n>1,\n\\end{align*}\nwe get the following result \n\\begin{align*}\n    p^{-\\frac{1}{\\alpha}} \\Gamma^{1/p}(\\frac{p}{\\alpha} + 1) \n    \\leq\n    p^{-\\frac{1}{\\alpha}} (\\sqrt{2\\pi p/\\alpha} (\\frac{p}{e\\alpha})^{\\frac{p}{\\alpha}} e^{\\frac{\\alpha}{12p}} )^{1/p} \n    &= p^{-\\frac{1}{\\alpha}} (\\sqrt{\\frac{2\\pi}{\\alpha}})^{1/p} p^{1/2p} (\\frac{p}{\\alpha})^{1/\\alpha} e^{\\alpha/12p^2 -1/\\alpha} \n    \\leq  (\\sqrt{\\frac{2\\pi}{\\alpha}})^{1/p}  e^{1/2e} \\frac{1}{(e\\alpha)^{1/\\alpha}} e^{\\alpha/12p^2} \n    \\leq  (\\sqrt{\\frac{2\\pi}{\\alpha}})^{1/2}  e^{1/2e} \\frac{1}{(e\\alpha)^{1/\\alpha}} e^{\\alpha/48},\n\\end{align*}\nwhere the first inequality uses the Stirling formula and the second inequality uses the fact that $p^{1/p} \\leq e^{1/e}$. Thus we can find that the term $\\sup_{p \\geq 2}$ is not involved in $p$ and only deoends on $\\alpha$, and the smaller $\\alpha$ is, the bigger this term is.\nAccording to above analysis, we  have the conclusion that the smaller $\\alpha$ is, the constant related to $\\alpha$ (i.e., $c_\\alpha$) in the upper bound is bigger.  We then discuss Theorem 2. As discussed in Section 2.1, the smaller $\\alpha$ is, the heavier tail the random variable has, which means that $\\Delta_\\alpha$ is bigger according to the definition. By the above analysis, we  have the conclusion that the smaller $\\alpha$ is, the terms related to $\\alpha$ ($c_\\alpha$ and $\\Delta_\\alpha$) in the upper bound is bigger. This result is consistent with the intuition: heavier-tailed distribution, i.e., smaller $\\alpha$, will lead to a larger upper bound.\n\n(2) Thanks for this review. We have revised $A_S$ to $\\mathcal{A}_S$. We will further go through the paper and polish the language in the final version.\n\n\nReferences\n\nOlivier Bousquet and Andre Elisseeff. Stability and Generalization. 2002.\n\nAryeh Kontorovich. Concentration in unbounded metric spaces and algorithmic stability. 2014.\n\nVitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. 2019.\n\nOliver Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable algorithms 2020.\n\nAndreas Maurer and Massimiliano Pontil. Concentration inequalities under sub-gaussian and sub-exponential conditions. 2021"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5181/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677245418,
                "cdate": 1700677245418,
                "tmdate": 1700677245418,
                "mdate": 1700677245418,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cBUksJeqFP",
            "forum": "0V311Uh8q1",
            "replyto": "0V311Uh8q1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5181/Reviewer_wNWA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5181/Reviewer_wNWA"
            ],
            "content": {
                "summary": {
                    "value": "Algorithmic stability has increasingly become an important tool for studying the generalization ability of machine learning algorithms. The general idea is to understand how well algorithms generalize out-of-sample by looking that how insensitive ('stable\") they are to perturbations of the input dataset. Intuitively, algorithms that do not have high dependence on any small subset of the dataset tend to not overfit to the data. This is should be contrasted with uniform convergence where generalization is proven using the simplicity of the class of possible output hypotheses instead. The present paper studies generalization under algorithmic stability for unbounded losses, complementing the standard techniques that works of case of algorithms that satisfy uniform boundedness. \n\nThe paper presents results extending uniform boundedness to a notion they call subWeibull diameter which corresponds to control on the tails of the deviations and show algorithmic stability bounds under this condition. The main technical contribution is a concentration inequality for subWeibull distributions that could be of independent interest."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper studies a natural extension of bounded uniform stability to unbounded losses and notes an interesting concentration inequality for Subweibull distributions. I believe the concentration inequality could be of general use in other applications."
                },
                "weaknesses": {
                    "value": "Though the generalization to unbounded losses is interesting, it would be nice if there are more applications presented justifying the generalization to this case. Also, see author questions section for detailed. \n\nI would be happy to raise my score if these concerns are addressed."
                },
                "questions": {
                    "value": "- One point that would be nice to emphasize the technical novelties in the proof. This is not meant to sound critical but from a first reading it is difficult to fully appreciate the challenge of the proofs As stated below, Lemma 3 and 4 are standard (contraction and symmetrization). So the main lemma seems to be Lemma 5. From my reading it is difficult to figure out why this is substantially different from the standard proof that Orlicz spaces ( for phi = exp(t^p) - 1 for p>1 ) have bounded moments. I understand that there is some convexity lost for p < 1 which is what you study but from the proof as written I fail to see the challenge (other than the fact that one can't go directly through the MGF. For example I am thinking of the proof here (https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf Prop 2.5.2). The challenge seems to be showing sums of subWeibull satisfy subWeibull tails (or rather keeping track of the parameters; ) without using the MGF (if this interpretation is correct maybe it is useful to emphasize). In summary would be nice to have a part explaining the technical novelty\n- I would recommend moving the proofs of Lemma 3 and Lemma 4 to the appendix since they are standard to allow the reader to focus on the new contributions of the work. \n- I might be missing something simple but I don't see how one went from a bound in terms of F to a bound in terms of the Weibull diameter without further assumptions on f/F. A priori F is some arbitrary function of F(X_i , X'_i) right, unrelated to the distance? I guess that in the remark there is some Lipshitzness assumed. It would be nice to make that explicit. \n- Page 4: Population Risk"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5181/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699118183440,
            "cdate": 1699118183440,
            "tmdate": 1699636513922,
            "mdate": 1699636513922,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z2CFdJ153y",
                "forum": "0V311Uh8q1",
                "replyto": "cBUksJeqFP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5181/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5181/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part I"
                    },
                    "comment": {
                        "value": "Thank you for your time and thoughts. Below, we reply to all the issues. \n\nQuestion 1: One point that would be nice to emphasize the technical novelties in the proof...\n\nAnswer: Thanks for this review. The first challenge is that we deal with the concentration of the general function $f$. Actually, the related work (Kontorovich, 2014) used the martingale method to decompose $f-\\mathbb{E}f$, while (Maurer and Pontil, 2021) use the sub-additivity of entropy to decompose the general function $f-\\mathbb{E}f$. After the decomposition, the next step of (Kontorovich, 2014) and (Maurer and Pontil, 2021) is to bound the MGF ($\\mathbb{E}e^{\\lambda Z}$) or a variant MGF ($\\mathbb{E}Z^2 e^{\\lambda Z}$), respectively. The second challenge is that, as you commented, the MGF is bounded for sub-Gaussian and sub-exponential random variables, but it is unbounded for subWeibull variables because there is some convexity lost. The standard technique to prove the MGF failed for the heavy-tailed subWeibull random variables. This implies that if we do not study the MGF, we need to consider different decomposition on the general function $f-\\mathbb{E}f$.\n\nTo address the first challenge, we introduce Lemma 4, where we decompose the general function $f-\\mathbb{E}f$ to the sum of independent sub-Weibul variables. In the proof of Lemma 4, a key step is that we need to construct a function $t \\to h(\\epsilon_n F_n(X_n, X'_n) +t)$ to apply our induction assumption. The proof of Lemma 4 may be simple, but Lemma 4 itself is very useful. For example, one can use Lemma 4 to prove more McDiarmid inequalities, e.g., the polynomially decaying random variables, which will enrich the family of McDiarmid inequality.\n\nTo address the second challenge, rather than bounding the MGF, we bound the $p$-th moment of the sum of subWeibull random variables. Thanks to the fact that subWeibull random variables are log-convex for $\\alpha \\leq 1$ and log-concave for $\\alpha \\geq 1$, we can apply Lata{\\l}a's inequality (Lemma 7 and Lemma 8) to bound this $p$-th moment. However, it is not a direct application of the Lata{\\l}a's inequality. Lata{\\l}a's inequality holds for the $p$-th moment of the \\emph{symmetric} random variables. On one hand, we need to carefully construct new random variables to satisfy the symmetry condition, for example, we introduce random variables $Y_i$, $Z_i$ in the proof of Lemma 5. On the other hand, since we study the weighted summation, Khinchin-Kahane inequality is also required to use.\n\nFinally, we would like to quote the comment of Reviewer LoHq to confirm the technical novelties: \"The proof consists of two steps: reduction to sum of independent sub-Weibul variables (Lemma 4), and the corresponding moment bound on a sum of sub-weibull rv's. The first part is done thanks to a conditioning trick with Jensen's inequality. The second part is done thanks to a carful application of Latala's inequality (Lemma 7 in the appendix).\"\n\nQuestion 2: I would recommend moving the proofs of Lemma 3 and Lemma 4 to the appendix ...\n\nAnswer: Thanks for your suggestion. We have moved the proofs of Lemma 3 and Lemma 4 to the appendix. \n\n\nQuestion 3: I might be missing something simple but I don't see ...\n\nAnswer: In Lemma 4, a key Lemma to prove Theorem 1, we assumed $|S - S_i| \\leq F_i(X_i, X_i')$ for $i=1,..., n$, where $S = f(X_1,...,X_{i-1},X_i,X_{i+1},...,X_n)$ and $S\\_i = f(X\\_1,...,X\\_{i-1},X'\\_i,X\\_{i+1},...,X\\_n)$, which can be analogized to the bounded difference condition $|S - S_i| \\leq c\\_i$ for $i=1...,n$. Here, $F_i$ is a arbitrary function satisfying the condition of subweibull random variable $||F\\_i(X\\_i, X'\\_i)||_{\\psi\\_{\\alpha}} \\leq \\infty$, which is unrelated to the distance. The result in Lemma 4 helped us went from a bound in terms of $f-\\mathbb{E}f$ to a bound in terms of the $F\\_i$. This argument can also refer to the comment of Reviewer LoHq: reduction to sum of independent sub-Weibul variables (Lemma 4).\n\nThe assumption $|S - S_i| \\leq F_i(X_i, X_i')$ can be seen as a Lipschitz condition when $F_i(X_i, X_i')$ is a distance function. For example, in Remark 1, by considering $F_i(X_i, X_i')$ as a distance function, we give concentration inequities in the context of the subweibull diameter.\n\nQuestion 4: Page 4: Population Risk\n\nAnswer: Thanks for this review. We have fixed this typo."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5181/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676768212,
                "cdate": 1700676768212,
                "tmdate": 1700676768212,
                "mdate": 1700676768212,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tpLrCHIzWL",
                "forum": "0V311Uh8q1",
                "replyto": "cBUksJeqFP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5181/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5181/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part II"
                    },
                    "comment": {
                        "value": "Question 5: Issue in Weakness ``Though the generalization to unbounded losses is interesting, it would be nice if there are more applications presented ...''\n\nAnswer: McDiarmid's inequality has been proved to be useful in a number of\napplications, such as algorithmic stability and suprema of empirical\nprocesses. This work extends McDiarmid's inequality to a broad class of heavy-tailed distributions and can motivate more McDiarmid's inequality by Lemma 4. Besides, proving generalization bounds for heavy-tailed losses is a significant direction in the learning theory community. For the tool of PAC-Bayes, Space Complexity, and Information theory, there are many works that study the heavy-tailed losses, while it is lacking for the tool of Algorithmic stability. Our work is the first attempt to give generalization bounds for the heavy-tailed loss for Algorithmic stability. Recently, (Bousquet et al. 2020) provided a sharper generalization bound for algorithmic stability. In their proof, they also use the $p$-th moment technique, and the $p$-th moment version of the Bounded differences/McDiarmid\u2019s inequality is an important component. It seems to be straightforward to give better generalization inequality by combining our $p$-th moment inequality and the technique of peeling the dataset in (Bousquet et al. 2020). Since our work mainly follows the work  (Kontorovich, 2014; Maurer \\& Pontil, 2021), we didn't consider giving sharper generalizations in the manuscript.\n We will include this part of the results in the final version.\n\nReferences\n\nOlivier Bousquet and Andre Elisseeff. Stability and Generalization. 2002.\n\nAryeh Kontorovich. Concentration in unbounded metric spaces and algorithmic stability. 2014.\n\nVitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. 2019.\n\nOliver Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable algorithms 2020.\n\nAndreas Maurer and Massimiliano Pontil. Concentration inequalities under sub-gaussian and sub-exponential conditions. 2021"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5181/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676844519,
                "cdate": 1700676844519,
                "tmdate": 1700677278555,
                "mdate": 1700677278555,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]