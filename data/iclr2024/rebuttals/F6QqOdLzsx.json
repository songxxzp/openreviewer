[
    {
        "title": "Advancing Test-Time Adaptation for Acoustic Foundation Models in Open-World Shifts"
    },
    {
        "review": {
            "id": "PYdF4MP4IH",
            "forum": "F6QqOdLzsx",
            "replyto": "F6QqOdLzsx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4821/Reviewer_hWCQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4821/Reviewer_hWCQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to investigate test-time adaptation tasks for pre-trained acoustic and speech models, considering them foundational in their respective domains.\n\nDuring test-time adaptation that involves frozen pre-trained models, the principal innovations of this work are encapsulated in two proposed methods: `Confidence Enhanced Adaptation` and `Short-Term Consistency Regularization.` \n\n- Both methods release trainable parameters, whether through normalization layers or input-based feature modifications. These methods closely resemble the techniques used in neural speech model reprogramming, as reported in acoustic model reprogramming ICML 2021 [1], addressing similar challenges in the adaptation of frozen models.\n\nOverall, the author seeks to address a critical issue in acoustic modeling and speech processing. However, the paper's effectiveness is compromised by a noticeable gap in the understanding of related work on frozen acoustic model adaptation. This lack, coupled with certain experimental setups and theoretical justifications, undermines the paper's ability to convincingly argue whether the proposed method is both novel and a parameter-efficient approach for the adaptation of frozen acoustic models.\n\nAs a reviewer, I will outline several potential and relevant perspectives for improvement below. Generally, I concur that the direction of open-set test-time adaptation warrants deeper study and is a worthwhile pursuit. \n\nFrom a neutral standpoint, it is necessary for the authors to significantly revise the manuscript, as there are fundamental LaTeX errors that need correction. Currently, the draft falls short of the above-average quality expected of a paper submission.\n\nFor instance, the authors should use `\\citep` for parenthetical references, `\\citet` when referring to the author(s) as part of the narrative, and `\\cite` when mentioning the work as a standalone noun. Further details on this can be found in the strengths and weaknesses sections."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The topic of test-time adaptation is generally interesting to the ml / speech community, although some perspectives on foundation (very large pre-trained) models based on in-context learning are not covered."
                },
                "weaknesses": {
                    "value": "- writing quality could be very improved (perhaps doing in their next version)\n  - citations code / format usages\n  - grammar issues\n  - The literature review on frozen acoustic model-based adaptation lacks depth. The current review primarily applies existing algorithms from the ML community, and the proposed method inadvertently overlaps with reprogramming/prompting due to an inadequate literature survey.\n    - for example, the additional losses based adapters / input-only training has been also proposed in [4]\n- The theoretical connection between test-time adaptation efficacy and frozen pre-trained models is unclear. A well-known recent insight involves latent space alignment via Wasserstein measurements, yet the related discussion is absent here.\n- The related speech processing references are missing\n  - frozen model adaptation via Bayesian adaptation in speech\n  - frozen model adaptation with trainable inputs reprogramming / prompting \n- ablation experiments are missing on \n  - Similar parameter-efficient baselines work on frozen pre-trained adaptation diagrams (e.g., input reprogramming, adapters, prompting), making it difficult to justify the novelty on applying trainable layer norms\n  - In terms of frozen pre-trained models, only Wav2Vec 2.0 Large has been evaluated. The term \"foundation model\" is often used for models with over 1 billion or even 100 billion parameters to showcase emergent abilities, which is not the case here.\n\n***\n\n**References**\n\n1. Voice2series: Reprogramming acoustic models for time series classification, ICML 21\n2. Wavprompt: Towards few-shot spoken language understanding with frozen language models, Interspeech 22\n3. Comparison of Multilingual Self-Supervised and Weakly-Supervised Speech Pre-Training for Adaptation to Unseen Languages, Interspeech 23\n4. Parameter-Efficient Learning for Text-to-Speech Accent Adaptation, Interspeech 23"
                },
                "questions": {
                    "value": "- high level question\n   - the authors mainly highlight their claims on the perspective of foundation acoustic model adaptation, only a single pre-trained model has been discovered. A better takeaway to the community could be how different pre-trained speech and acoustic models in response to the input based adaptation. \n    - If the authors could provide studies different pre-trained acoustic models (e.g., AudioSet pre-trained, WavLM, Whisper, joint supervised and unsupervised w2v2 based methods) to have a deeper look on the (a) training data source and (b) size of model such as [5], this work could be with larger impacts. \n  - how's the proposed methods different with reprogramming and jointly reprogramming plus adapter [6] in the prior works? \n\n- low-level questions\n  - what is the trainable parameters in both settings?\n  - in terms of acoustic modeling, any acoustic classification or speaker-level open set has been studied in the proposed method?\n  - I am curious how the entropy based additional losses training gonna impact the latent space distance measured from w2v2 pre-trained to target open set \n\n\n***\n**References**\n\n5. How to Estimate Model Transferability of Pre-Trained Speech Models? Interspeech 23\n6. From English to More Languages: Parameter-Efficient Model Reprogramming for Cross-Lingual Speech Recognition, ICASSP 23\n\n\n## post-discussion\n\n- I raise my score to six (not argue to reject) and give conditional accept to this work toward checking their statements on\n   - including frozen model adaptation discussion\n   - Bit and P-tuning with revised entropy loss training\n   - total trainable parameter numbers \n\nin their final version. Flagged for AC and SAC double checking if the recommendation is accepted."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4821/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4821/Reviewer_hWCQ",
                        "ICLR.cc/2024/Conference/Submission4821/Senior_Area_Chairs"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no major ethic concerns."
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4821/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698642264913,
            "cdate": 1698642264913,
            "tmdate": 1700763085448,
            "mdate": 1700763085448,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nmNYZHOSfQ",
                "forum": "F6QqOdLzsx",
                "replyto": "PYdF4MP4IH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4821/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the careful review and constructive suggestions. Below are our detailed responses to your questions:\n\n---\n\n**Q1**: \"writing quality could be very improved\"\n\n**A1**: We appreciate your feedback regarding the writing quality. We have carefully revised the citation formats and typos in the new version of our paper.\n\n---\n\n**Q2**: \"The related speech processing references are missing\", \"A well-known recent insight involves latent space alignment via Wasserstein measurements, yet the related discussion is absent here\"\n\n**A2**: Thanks for highlighting these references. We have incorporated a discussion of them within the related work section in the revised version.\n\n---\n\n**Q3**: \"how's the proposed methods different with reprogramming and jointly reprogramming plus adapter in the prior works?\"\n\n**A3**: In summary, the key difference is that our proposed method performs **unsupervised** adaptation-based entropy minimization while reprogramming and jointly reprogramming plus adapter perform **supervised** optimization with access to annotated target data pairs. Furthermore, while reprogramming-based methods focus on training additional small parameter sets for data input parts or adapters within the model, our method focuses on the feature modulation on norm layers without additional parameter introduction.\n\n---\n\n**Q4**: \"...how different pre-trained speech and acoustic models...\", \"provide studies different pre-trained acoustic models to have a deeper look\", \"...only Wav2Vec 2.0 Large has been evaluated. The term \"foundation model\" is often used for models with over 1 billion or even 100 billion parameters to showcase emergent abilities, which is not the case here\"\n\n**A4**: Thanks for the good suggestion. We choose the Wav2vec2 models due to their popularity in the community and we acknowledge the importance of evaluating diverse acoustic models. Hence, we conduct additional experiments using, Hubert-Base, Hubert-Large, WavLM-Base, and WavLM-Large to compare the adaptation performance under different model sizes, and training data sources. The experimental results are discussed in Appendix A.5 in the new version. \n\nFurthermore, we believe that there hasn\u2019t been a clear definition for the \"acoustic foundation models\" in terms of model sizes. This perspective -- \"The term \"foundation model\" is often used for models with over 1 billion or even 100 billion parameters\" -- is often discussed in the context of large language models. We refer to \"acoustic foundation models\" as those large acoustic models that serve fundamental roles and can be extended to various downstream speech tasks.  \n\n--- \n\n**Q5**: \"Similar parameter-efficient baselines work on frozen pre-trained adaptation diagrams, making it difficult to justify the novelty on applying trainable layer norms\"\n\n**A5**: We hope to kindly clarify that parameter efficiency and applying trainable layer norms are not our main contributions even though our method is indeed parameter-efficient. Rather, we investigate TTA within the audio modality and discover that non-silent frames with high entropy contain vital semantic content that cannot be dropped. This motivates us to develop a learning-based adaptation approach to address it. \n\nBesides, there is a key difference in the experimental setting. That is, the mentioned parameter-efficient baselines are trained using supervised signals, while our method performs unsupervised optimization.   \n\n---\n\n**Q6**: \"what is the trainable parameters in both settings?\"\n\n**A6**: In the CEA, trainable parameters include affine parameters associated with layer normalization and the feature extractor. In the STCR, trainable parameters are affine parameters associated with layer normalization in the model, as illustrated in Figure 2.\n\n--- \n\n**Q7**: \"in terms of acoustic modeling, any acoustic classification or speaker-level open set has been studied in the proposed method?\"\n\n**A7**: Thanks for this good suggestion. We mainly evaluate ASR in our work since it is one of the most typical problems in speech processing. We definitely agree that acoustic classification and speaker-related tasks are also interesting and could be extendable tasks in future work.  \n\n\n---\n\n**Q8**: \"I am curious how the entropy based additional losses training gonna impact the latent space distance measured from w2v2 pre-trained to target open set\"\n\n**A8**: We believe such latent space distance might increase as entropy-based additional losses decrease. Basically, it might not be meaningful for a direct comparison of such distances as we cannot correlate larger distances to better adaptation performance.  Instead, it might make sense for the distance comparison among different adaptation methods. This might be an interesting future work.\n\n---\n\nWe hope these responses comprehensively address your queries, and we are open to any further discussion or feedback."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480622896,
                "cdate": 1700480622896,
                "tmdate": 1700480622896,
                "mdate": 1700480622896,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "crwIHHv1Hf",
                "forum": "F6QqOdLzsx",
                "replyto": "PYdF4MP4IH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4821/Reviewer_hWCQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4821/Reviewer_hWCQ"
                ],
                "content": {
                    "title": {
                        "value": "Re: Layer norm is associated with trainable parameters"
                    },
                    "comment": {
                        "value": "Thanks for authors for the responses.\n- I am happy to see the paper draft format (citations latex issues and model size issues) and quality have both improved largely. \n\nBesides, there are issues I confirm the authors would have replied with imprecise information. I provide below for further references / help for improvements. \n\n1. Does layer norm / feature modulation introduce new additional associated with trainable parameters? \n\n- Actually, Layer Normalization (Layer Norm) does have trainable parameters. Specifically, it has two parameters: the scale (denoted as $\\gamma$) and the shift (denoted as $\\beta$). As the cases in this work, applying an additional layer(s) of Layer Normalization to a frozen model can be considered as introducing new parameters for model adaptation. \n\nThis extra norms provides a way to adapt the model to new (open domain) data or tasks without altering the original, pre-trained parameters of the model, which is a strategy often used in transfer learning and fine-tuning scenarios.\n\nAs a frozen model adaptation study, the additional loss based training upon the additional trainable parameter is standard for having adapters baseline as the reviewer\u2019s initial review suggestion.\n\nAlso, the authors\u2019 response of no extra parameter introduction is imprecise (e.g., difference between acoustic model reprogramming and input prompting). \n\nPlease consider to correct it since it will potentially mislead further readers. \n\n2. The wav prompt reference \n\n- The wav prompt reference is not a rigorous format of in-context learning, where ICL is required sequence stacking in previous rounds forward propagations. Referring as prompting learning is much formal. \n\n\n3. Language usage of foundation models\n\n- I disagree with the authors\u2019 viewpoint on non large scale (1B+) statement. General purpose pre-trained model and foundation model in terms of scaling effects benefited representation learning are different, since many frozen model adaptation characteristics are highly related to scales of model and data. This point does not affect my final score. \n\nIn short, this is borderline paper for me. The authors have made largely improvements from a clear reject initial draft. In the latest draft, there is still a gap in terms of deeper understanding of parameter efficient adaptation that missed other multi-loss based parameter efficient adaptation analysis. \n\nI am ok if the paper got accepted but I would keep my recommendation as borderline reject."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670433090,
                "cdate": 1700670433090,
                "tmdate": 1700724474790,
                "mdate": 1700724474790,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T6S1gAxkpQ",
                "forum": "F6QqOdLzsx",
                "replyto": "PYdF4MP4IH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4821/Reviewer_hWCQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4821/Reviewer_hWCQ"
                ],
                "content": {
                    "title": {
                        "value": "Re: partial layer norm training setting"
                    },
                    "comment": {
                        "value": "Thanks for the authors clarified the layer norm has been parts of the frozen pre-trained model instead of adding new layer norms. My original interpretation is indeed with this confusion. \n\nMeanwhile, the partially released setting of layer norm / feature modulation is related to bias-only tuning and P-tuning in parameter efficient adaptation. \n\nIn recent understandings, parameter efficient learning (e.g., like this partially frozen adaptation work) performing better in few-shot and low-resource adaptation is due to the generalization loss over universal approximation without modifying the decision boundaries. The data efficiency is not the main focus on the performance-driven parameter efficient learning. I would like to point out that the connection between existing frozen representation learning is important here. \n\nP-tuning in Q, V would often take longer time to process experiments. If the authors would add some preliminary results on their additional losses training works for bias-only tuning (or commit to add it in their final version), I would consider the disconnection of equivalent techniques have been satisfactorily resolved to increase my score to six. \n\nThe aforementioned Chen et al. [5] and A. Pasad et al. ASRU 21 have discussed this relationship. The author may consider to add some related large pre-trained AM theoretical findings discussion to have broader impacts in their final version.\n\nAgain, thanks the author for the replies. I would avoid to use foundation model for rigor definition and encourage the authors spend some time reading the original foundation model paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725610665,
                "cdate": 1700725610665,
                "tmdate": 1700725891221,
                "mdate": 1700725891221,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uZrkCJPVNx",
                "forum": "F6QqOdLzsx",
                "replyto": "PYdF4MP4IH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4821/Reviewer_hWCQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4821/Reviewer_hWCQ"
                ],
                "content": {
                    "title": {
                        "value": "Re: minor Figure 2 suggestion"
                    },
                    "comment": {
                        "value": "The authors could consider to put trainable color blocks into red and frozen colors blocks into blue / gray for better visualization toward ICLR audiences."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726073612,
                "cdate": 1700726073612,
                "tmdate": 1700726102785,
                "mdate": 1700726102785,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Xij6w2Rofv",
            "forum": "F6QqOdLzsx",
            "replyto": "F6QqOdLzsx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4821/Reviewer_UNGc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4821/Reviewer_UNGc"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new approach to test-time adaptation.\nUnlike previous work, this work explicitly focuses on sequential data (speech), as opposed to static and independent objects such as images. \nThe proposed method is a heuristic-free, learning-based, confidence enhanced approach to adaptation.\nThe authors demonstrate the effectiveness of their method and superiority over existing approaches on synthetic and real-world data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* This paper addresses test-time adaptation for sequential data (speech) and not only for static objects (images).\n* The topic is relevant.\n* The paper proposes a fully learnable approach for test-time adaptation.\n* The experiments show small but consistent gains (up to a few % relative)."
                },
                "weaknesses": {
                    "value": "* One of the paper's goals is heuristic-free test-time adaptation. As for me, the authors mainly move existing heuristics into loss functions, which makes it a learnable approach and gets rid of hyper-parameters that need to be manually set (at least when ignoring optimization hyperparameters such as learning rate, etc.). But the heuristics remain, don't they?\n* Unlike previous work, this paper focuses on sequential data (here, speech) rather than static objects like images. I commend the authors for this. However, the only twist that the authors add is an auxiliary loss based on some heuristic for speech signals' short-term consistency. According to Table 5, the benefits from this loss term are modest. Is this all we can/need to do for sequential data? See also question 1 below.\n\n* I think there are a few issues with the equations:\n  - Eq.(3) & Eq.(4): Are these frame-level (what the text says) or sequence-level (what the equations look like) quantities?\n  - Eq.(4): Please fix subscript - subscript S with i or sum over i? Utterance-level E?\n  - Eq.(5): Cardinalities of z\u2019 don\u2019t match.\n* The text says \"The classification of frames as silent or non-silent was determined based on pseudo labels derived from model predictions.\" For Figure 3, is the classification done once on the initial, non-adapted model or repeatedly on the respective adapted model?\n* How are the hyper-parameters (e.g., loss coefficients) selected?\n* Could you please clarify what LS-C stands for: LibriSpeech Noise Corrupted (Figure 3) vs synthetic dataset (Sec. 5.1)?\n* Something is off with the inline citations.\n* Figure 3: Is percentage in [0,1] or [0,100]? \n* Typo: \"model training _phrase_\""
                },
                "questions": {
                    "value": "1. Why do we need the auxiliary loss in Eq.(5) to improve the consistency of speech signals? Could we get rid of this heuristic by using a proper sequence-level loss (e.g., sequence-level instead of frame-level entropy in Eq.(2)) ?\n\n2. Summary of Table 2, for example: 'Ours' outperforms 'SUTA' 0.2 - 1.1% absolute, but WER for lower SNRs is still a multiple of WER on \"clean\" data (as much as 82.2 vs 17.5). This makes me wonder what the lowest achievable WER (e.g., human WER) is, in particular on the heavily corrupted data sets? This information might be useful to understand the empirical results:\n\n    (i) If bound by information loss of corruption: The proposed approach may be optimal and the small gains highly significant. But the task may be not the best to evaluate the proposed method.\n\n    (ii) If not bound: What is missing to considerably close the WER gap?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4821/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4821/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4821/Reviewer_UNGc"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4821/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827365682,
            "cdate": 1698827365682,
            "tmdate": 1699636465404,
            "mdate": 1699636465404,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BSYfWzAniK",
                "forum": "F6QqOdLzsx",
                "replyto": "Xij6w2Rofv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4821/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your meticulous review and constructive feedback. Below are our responses to your queries:\n\n---\n\n**Q1**: \"One of the paper's goals is heuristic-free test-time adaptation. As for me, the authors mainly move existing heuristics into loss functions, which makes it a learnable approach and gets rid of hyperparameters that need to be manually set (at least when ignoring optimization hyperparameters such as learning rate, etc.). But the heuristics remain, don't they?\"\n\n**A1**: Thanks for this good question. We acknowledge that the heuristics still remain. Our \"heuristic-free\" refers to removing the filter-based method for unreliable data with high entropy. Our original intention is to make this heuristic-based method into a learning-based method. We will clarify our description and revise it in the new version.\n\n---\n\n**Q2**: \"... However, the only twist that the authors add is an auxiliary loss based on some heuristic for speech signals' short-term consistency ...\", \"Why do we need the auxiliary loss in Eq.(5) to improve the consistency of speech signals?\"\n\n**A2**: The rationale behind the short-term consistency regularization is grounded in the inherent characteristics of speech signals. Typically, neighboring speech frames often correspond to the same phoneme or character. As such, this consistency regularization aids in fostering reliable predictions and maintaining coherence in speech frames.\n\n---\n\n**Q3**: \"...Is this all we can/need to do for sequential data?\", \"Could we get rid of this heuristic by using a proper sequence-level loss (e.g., sequence-level instead of frame-level entropy in Eq.(2))?\"\n\n**A3**: Thanks for the great suggestion.  We agree that adopting the sequence-level loss, such as sequence discriminative training loss might further improve the TTA performance. We consider exploring this as a future work.  \n\n---\n\n**Q4**: \"a few issues with the equations\"\n\n**A4**: We are sorry for the confusion. Eq.(3) and Eq.(4) should be frame-level quantities. We have revised Equations (3) and (4), as well as the cardinalities in Equation (5) in the updated version of our paper. Thanks for pointing out these issues.\n\n--- \n\n**Q5**: \"For Figure 3, is the classification done once on the initial, non-adapted model or repeatedly on the respective adapted model?\"\n\n**A5**: The classification is done repeatedly on the respective adapted model. \n\n---\n\n**Q6**: \"How are the hyper-parameters (e.g., loss coefficients) selected?\"\n\n**A6**: Hyper-parameters are selected using standard search techniques. The implementation details of hyper-parameters can be found in Appendix A.2.\n\n--- \n\n**Q7**: \"Could you please clarify what LS-C stands for: LibriSpeech Noise Corrupted (Figure 3) vs synthetic dataset (Sec. 5.1)?\"\n\n**A7**: We are sorry for the confusion. LS-C stands for LibriSpeech test-other set Corrupted by Gaussian noises, as initially mentioned in Sec. 5.1. We have made appropriate revisions in Sec. 5.1 for greater clarity. \n\n---\n\n**Q8**: \"Figure 3: Is percentage in [0,1] or [0,100]?\"\n\n**A8**: The percentage value is in the range [0,1].\n\n--- \n\n**Q9**: \"Something is off with the inline citations.\" \"Typo: \"model training phrase\"\"\n\n**A9**: Thanks for pointing out these issues. We have revised them in the new version.\n\n--- \n\n**Q10**: \"...This makes me wonder what the lowest achievable WER (e.g., human WER) is, in particular on the heavily corrupted data sets? This information might be useful to understand the empirical results.\"\n\n**A10**: This is a good question. We agree that the human WER bound can indeed assist in understanding the experimental results. However, it is difficult for us to obtain such numbers as it requires conducting human subjective studies.  Besides, heavily corrupted speech data may not be common cases in the real world. Our primary aim in this experimental setting is to evaluate the trend of model robustness under varying noise levels, and our results demonstrate that our proposed method outperforms all baselines in this regard.\n\n---\n\nWe hope these responses adequately address your concerns and we are open to any further feedback or discussions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480497664,
                "cdate": 1700480497664,
                "tmdate": 1700480497664,
                "mdate": 1700480497664,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1UMyNVohDn",
                "forum": "F6QqOdLzsx",
                "replyto": "BSYfWzAniK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4821/Reviewer_UNGc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4821/Reviewer_UNGc"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the responses. I recognize the small but systematic gains by the proposed method and the improved\u00a0clarity of writing.\n\n> A10: This is a good question. We agree that the human WER bound can indeed assist in understanding the experimental results. However, it is difficult for us to obtain such numbers as it requires conducting human subjective studies.\u00a0\n\nA minimal, low-effort solution might be to listen to a number of utterances and convince yourself (and the reader) that humans recognize the clean and heavily corrupted utterances with similar accuracy.\n\n> Besides, heavily corrupted speech data may not be common cases in the real world.\u00a0\n\nOkay. If this is the case, I suggest adding a brief comment explaining the artificial nature of the data.\n\n> Our primary aim in this experimental setting is to evaluate the trend of model robustness under varying noise levels, and our results demonstrate that our proposed method outperforms all baselines in this regard.\n\nAgreed. But in my opinion, the quality and contribution of this paper largely depends on a careful choice of the experimental setup and a solid/extensive evaluation/analysis.\u00a0\n\nI personally think the paper in its current state is a weak reject, but I'm happy to be overruled by the other reviewers and see the work accepted."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728589672,
                "cdate": 1700728589672,
                "tmdate": 1700728589672,
                "mdate": 1700728589672,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ytoL0mcQ2C",
            "forum": "F6QqOdLzsx",
            "replyto": "F6QqOdLzsx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4821/Reviewer_SG39"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4821/Reviewer_SG39"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the issue of test-time adaptation (TTA) for Speech Foundation Models, including wav2vec2, hubert, and the like. Conventional TTA methods for visual modality typically aim to minimize label entropy in test utterances. These utterances are chosen based on predefined entropy thresholds to prevent model updates that may lead to mode collapse (Niu et al., 2023).\n\nHowever, applying this traditional approach to speech models doesn't yield the desired results because non-silent frames in speech models often exhibit high entropy and carry valuable information (thus, they shouldn't be pruned). To address this, the paper introduces a confidence-enhanced entropy method that proves effective for speech models.\n\nEmpirical findings from experiments on synthetic and real benchmarks underscore the practicality and effectiveness of this proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper clearly highlights the reason for previous approaches to not work in audio modality. It make sense that non-silent frames have a high entropy as they are the ones where the non-blank labels are to be predicted.\n\n2. The empirical results show the usefulness of the approach on various experimental protocol.\n\n3. The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The contributions seems minimal, as the core idea of TTA (Niu et al., 2023 and other related works cited in the paper) are already available in the literature."
                },
                "questions": {
                    "value": "1. In comparison to different vocabulary sizes, such as employing a BPE model with a larger vocabulary, how does this work perform? Would the entropy for non-silent frames significantly increase under such circumstances?\n\n2. Could this approach be adapted to function with a model architecture similar to RNN-Transducer? What specific modifications would be necessary to enable its compatibility with such an architecture?\n\n3. Wav2vec2 features a self-supervised encoder. Would it be a viable alternative to the proposed TTA approach to fine-tune the wav2vec2 encoder using test utterance audio as a baseline? What advantages or disadvantages might this alternative approach present?\n\n4. Could you provide further clarification on the decoding strategies outlined in Section 5.4 of your paper? Specifically, post-adaptation, how do different inference strategies, such as beam search or greedy decoding, impact the inference process, and what are the key differences in their outcomes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4821/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4821/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4821/Reviewer_SG39"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4821/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698859663875,
            "cdate": 1698859663875,
            "tmdate": 1699695012806,
            "mdate": 1699695012806,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZMIMLnvkkj",
                "forum": "F6QqOdLzsx",
                "replyto": "ytoL0mcQ2C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4821/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your thoughtful review and valuable suggestions. Our responses to your questions are as follows:\n\n---\n\n**Q1**: \"The contributions seems minimal, as the core idea of TTA are already available in the literature\"\n\n**A1**: We recognize the existence of TTA-related works in the literature. However, our work focuses on acoustic modeling, compared to the prevalent focus on vision tasks in previous studies. Specifically, our key contributions include: \n- Identifying that noisy speech frames with high entropy often contain non-silent segments crucial for semantic understanding.\n- Introducing a learning-based adaptation approach, complemented by short-term consistency regularization, to augment the adaptation performance.\n\n---\n\n**Q2**: \"In comparison to different vocabulary sizes, such as employing a BPE model with a larger vocabulary, how does this work perform? Would the entropy for non-silent frames significantly increase under such circumstances?\"\n\n**A2**: Thanks for the question. Our proposed method is generalizable to models with large vocabulary sizes. Theoretically, the maximum entropy for non-silent frames is expected to increase due to the larger number of classes. Practically, this might also depend on the test input and models. We conduct an additional experiment using the Conformer-CTC model with BPE tokenization, observing an increase in entropy for non-silent frames from 59.4\\% to 70.0\\%, as illustrated in Table 2.\n  \nTable 2: Entropy Distribution at Step 0 for models with different vocabulary sizes.\n\n| Vocab Size | Wav2vec2 Base | Conformer-CTC | \n| ---    | ---   | ---    | \n| n-sil-h | 0.594 | 0.700 | \n| n-sil-l | 0.406 | 0.300 |  \n| sil-h   | 0.362 | 0.497 |  \n| sil-l   | 0.638 | 0.503 | \n  \n\n---\n\n**Q3**: \"Could this approach be adapted to function with a model architecture similar to RNN-Transducer? What specific modifications would be necessary to enable its compatibility with such an architecture\"\n\n**A3**: We think that our proposed approach is adaptable to architectures like RNN-Transducer. The primary modification would involve basing the prediction probability and pseudo labels on the decoder outputs.  \n\n---\n\n**Q4**: \"... Would it be a viable alternative to the proposed TTA approach to fine-tune the wav2vec2 encoder using test utterance audio as a baseline? What advantages or disadvantages might this alternative approach present\"\n\n**A4**: Thanks for the question. Self-supervised fine-tuning wav2vec2 encoder could serve as a baseline. However, in our experimental setting, we consider a more practical setting where we only have access to a single utterance for adaptation before test-time inference. In this setting, self-supervised fine-tuning wav2vec2 using such a single utterance poses two major disadvantages:\n- The insufficiency of a single test utterance for effective batch construction, leading to data imbalance issues.\n- Potential distortion of the models' distribution upon fine-tuning the entire wav2vec2 encoder. \n\n---\n\n**Q5**: \"Could you provide further clarification on the decoding strategies outlined in Section 5.4 of your paper? Specifically, post-adaptation, how do different inference strategies, such as beam search or greedy decoding, impact the inference process, and what are the key differences in their outcomes?\"\n\n**A5**: The performance differences between the two decoding strategies are detailed in Table 4, Sec.5.3.2. Beam search incorporates an additional language model in the decoding process, while greedy search does not. This language model, with its inherent text distribution prior, can help eliminate impossible text combinations during decoding. Moreover, the inclusion of a language model provides a mechanism to address text-domain shifts. \n\n---\n\nWe hope these responses comprehensively address your concerns and we remain open to further discussion and feedback."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480381220,
                "cdate": 1700480381220,
                "tmdate": 1700480381220,
                "mdate": 1700480381220,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4cNzVpRx0e",
                "forum": "F6QqOdLzsx",
                "replyto": "ZMIMLnvkkj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4821/Reviewer_SG39"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4821/Reviewer_SG39"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the response. I read all the reviews and corresponding rebuttal and I would like to keep my ratings.\n\nJust for Information, I want to share a paper that do test-time adaptation via in context-learning (COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning). Table 6 of the paper might be similar to what you are proposing."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735362560,
                "cdate": 1700735362560,
                "tmdate": 1700735362560,
                "mdate": 1700735362560,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EUpnBrfbny",
            "forum": "F6QqOdLzsx",
            "replyto": "F6QqOdLzsx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4821/Reviewer_cqyf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4821/Reviewer_cqyf"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates Test Time Adaptation of pre-trained acoustic models. different real world shifts like noise, sining voice and accents are examined. For adaptation two issues are considered one is correlation between speech frames and the other is enhancing the confidence. Experimental results supports the gain of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- An important issue is addressed\n- Paper is written well"
                },
                "weaknesses": {
                    "value": "- Experiments could be extended"
                },
                "questions": {
                    "value": "- In the ablation study the impact of the two terms in equation 5 is explained. I think the conclusion about the impact of components depends on the setups. The importance of the two components could be different in the noise shift than the accent shift. what do you think?\n\n- Do you consider language as a shift? could a foundation model be adapted to a new language?\n\n- what is a practical way to get alpha in equation 5?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4821/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698968698753,
            "cdate": 1698968698753,
            "tmdate": 1699636465236,
            "mdate": 1699636465236,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cssfmK2UBr",
                "forum": "F6QqOdLzsx",
                "replyto": "EUpnBrfbny",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4821/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful for your insightful feedback and offer our responses as follows:\n\n---\n\n**Q1**: \"...The importance of the two components could be different in the noise shift than the accent shift. what do you think?\" \n\n**A1**: Thanks for this question. We agree that the importance of two components may indeed be different in the noise shift and accent shift because these shifts come from distinct sources. Therefore, we conduct additional ablation studies for them respectively. Specifically, we conduct experiments using the dataset with level-3 Gaussian noises in LS-C for noise shift and speech of speaker BWC with Mandarin as L1 in L2-Arctic for accent shift. All experiments are carried out using the Wav2vec2 Base model. The results are detailed in Table 1: \n\nTable 1: Ablation study of core components in the noise shift and accent shift\n\n| Method | Noise Shift | Accent Shift | \n| ---    | ---   | ---    | \n| Ours     | **24.0** | **23.0** | \n| w/o STCR | 25.1 | 23.4 | \n| w/o CEA  | 35.9 | 26.9 | \n| Source | 39.5 | 28.5 |\n \nThese experimental findings indicate a pronounced efficacy of our method in mitigating noise shifts as opposed to accent shifts. We conjecture the reason could be that the shift caused by Gaussian noises for each frame is consistent while other shifts such as accent shift could be different within frames. \n\n---\n\n**Q2**: \"Do you consider language as a shift? could a foundation model be adapted to a new language?\"\n\n**A2**: Thanks for the interesting question. In our current experimental setting, we mainly address acoustic shifts including Gaussian and environmental noises, accents, and sung speech. We consider the text-domain shifts within the same language, reflecting variations in the linguistic content or context of the speech data. Adapting the model to a new language is an important research problem in multilingual acoustic models. We consider it as future work.  \n\n---\n\n**Q3**: \"what is a practical way to get alpha in equation 5?\"\n\n**A3**: Alpha represents the weight for short-term consistency regularization and is a hyperparameter in our experiments. We employ standard grid search techniques to search the value ranging from 0.1 to 1 with 0.1 an interval. The details can be found in Appendix A.2. \n\n---\n\nWe hope that these responses satisfactorily address your questions and look forward to any further feedback or suggestions you may have."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480228543,
                "cdate": 1700480228543,
                "tmdate": 1700480228543,
                "mdate": 1700480228543,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "leh2P2UY9B",
                "forum": "F6QqOdLzsx",
                "replyto": "cssfmK2UBr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4821/Reviewer_cqyf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4821/Reviewer_cqyf"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the response"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650369342,
                "cdate": 1700650369342,
                "tmdate": 1700650369342,
                "mdate": 1700650369342,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]