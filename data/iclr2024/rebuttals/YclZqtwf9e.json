[
    {
        "title": "Slingshot Perturbation to Learning in Monotone Games"
    },
    {
        "review": {
            "id": "LaF4uQmuPz",
            "forum": "YclZqtwf9e",
            "replyto": "YclZqtwf9e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5384/Reviewer_yRC2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5384/Reviewer_yRC2"
            ],
            "content": {
                "summary": {
                    "value": "This paper gives a unified framework to construct uncoupled learning algorithms that can provably converge to Nash equilibria in monotone games, for the full information feedback model and the noisy feedback model. The central idea of this framework is to introduce a slingshot strategy, so when doing (regularized) gradient descent, one does not only consider the gradient of the payoff function but also considers the gradient of the distance $G(\\pi_i, \\sigma_i)$ from the current strategy $\\pi_i$ to the slingshot strategy $\\sigma_i$. When the slingshot strategy is fixed, the algorithm can find an approximate equilibria, depending on how close $\\sigma_i$ is to a Nash equilibrium. By updating the slingshot strategy periodically, the algorithm can then converge to an exact Nash equilibrium. In particular, if $G$ is the sqaured $\\ell^2$-distance, then the convergence rate is $O(1/\\sqrt T)$ with full information feedback and $O(T^{-1/10})$ with noisy feedback."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The problem studied -- last iterate convergence to Nash equilibria via uncoupled learning in games -- is doubtless important.\n\n(2) The framework, techniques, and result of this paper are very general: One can use any function $G$ to measure the distance from a strategy $\\pi_i$ to the slingshot strategy $\\sigma_i$, as long as $G$ is smooth and strongly convex. One can use any standard regularizer $\\psi$. And the result holds for any monotone games. This is a nice combination and abstraction of the techniques in a wide range of previous works.\n\n(3) Experimental results support the theoretical claims.\n\n(4) The writing is really clear."
                },
                "weaknesses": {
                    "value": "(1) The authors didn't give any impossibility results (lower bounds) on the convergence rate. I would suggest the authors to present some lower bound results to contrast with the upper bound results they give, even if the lower bounds can be from previous work.\n\n(2) The $O(T^{-1/10})$ bound for finding exact Nash equilibrium in the noisy feedback case (Theorem 5.2) does not seem to be tight. It would be great if the authors can provide some discussion on the tightness of this result.\n\n(3) While the convergence rates to approximate equilibria (Section 4) are presented for any general $G$ functions, the convergence rates for exact Nash equilibria (Section 5) are only for $G$ being the squared $\\ell^2$-distance. The authors say that Theorem G.1 and G.2 are for other $G$ functions, but they are only asymptotic results, with no quantitative convergence rates. Given that the main contribution of this paper is a unified framework, I feel that a quantitative convergence rate for general $G$ function is needed."
                },
                "questions": {
                    "value": "**Questions:**\n\n(Q1) The cited related work [Cai-Luo-Wei-Zheng, 2023, Uncoupled and Convergent Learning in Two-Player Zero-Sum Markov Games] shows that the Nash equilibria of 2-player zero-sum games can be found by an uncoupled learning algorithm with bandit feedback with $O(T^{-1/8})$ convegence rate. This is better than the $O(T^{-1/10})$ bound in this paper. Of course I'm not saying [Cai-Luo-Wei-Zheng, 2023] outperforms this paper, because this paper's result holds for more general monotone games. But I wonder whether [Cai-Luo-Wei-Zheng, 2023]'s ideas can be used to improve this paper's result? In particular, improving the convergence rate or proving results for the bandit feedback case?\n\n(Q2) Do the authors have any responses to the 3 weaknesses I mentioned above?\n\n**Suggestion:**\n\n(S1) Page 3, \"Other notations\" paragraph: \"$\\langle \\nabla \\psi(\\pi_i'), \\pi - \\pi_i'\\rangle$\" should be \"$\\langle \\nabla \\psi(\\pi_i'), \\pi_i - \\pi_i'\\rangle$\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5384/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698633449367,
            "cdate": 1698633449367,
            "tmdate": 1699636544731,
            "mdate": 1699636544731,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bjZljb2Cck",
                "forum": "YclZqtwf9e",
                "replyto": "LaF4uQmuPz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5384/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5384/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yRC2"
                    },
                    "comment": {
                        "value": "We thank you for your positive feedback and constructive comments. The detailed answers to each of the questions can be found below.\n\n---\n\n### Weakness 1\n\n> (1) The authors didn't give any impossibility results (lower bounds) on the convergence rate. I would suggest the authors to present some lower bound results to contrast with the upper bound results they give, even if the lower bounds can be from previous work.\n\n### Answer\n\nThank you for your valuable comment regarding the tightness of our derived convergence rates. We agree that establishing lower bounds for our algorithm in both full and noisy feedback settings would be a promising future direction. In fact, to the best of our knowledge, no existing research has derived a lower bound for the noisy feedback setting. We will strive to provide a lower bound for our algorithm in full and noisy feedback settings in future work.\n\n---\n\n### Weakness 2\n\n> (2) The $O(T^{-1/10})$ bound for finding exact Nash equilibrium in the noisy feedback case (Theorem 5.2) does not seem to be tight. It would be great if the authors can provide some discussion on the tightness of this result.\n\n### Answer\n\nOur convergence rate in the noisy feedback could be improved because we have roughly derived the upper bound on $\\langle \\nabla_{\\pi_i}v_i(\\pi^{\\mu, \\sigma^k}), \\pi^{\\mu, \\sigma^{k-1}} - \\sigma^k\\rangle$ by using Cauchy\u2013Schwarz inequality in the proof of Lemma F.2. We will mention the potential for improvement in our convergence rate in the subsequent revision.\n\n---\n\n### Weakness 3\n\n> (3) While the convergence rates to approximate equilibria (Section 4) are presented for any general $G$ functions, the convergence rates for exact Nash equilibria (Section 5) are only for $G$ being the squared $\\ell^2$-distance. The authors say that Theorem G.1 and G.2 are for other $G$ functions, but they are only asymptotic results, with no quantitative convergence rates. Given that the main contribution of this paper is a unified framework, I feel that a quantitative convergence rate for general $G$ function is needed.\n\n### Answer\n\nWe would like to emphasize that existing studies have only provided asymptotic last-iterate convergence results for special cases of $G$, such as KL divergence [Perolat et al., 2021] or reverse KL divergence [Abe et al., 2023]. Our work significantly contributes to this field by (i) generalizing these results to a wider class of $G$ functions, and (ii) deriving a non-asymptotic last-iterate convergence rate for a specific $G$ function. We acknowledge the importance and challenge of extending Theorems 5.1 and 5.2 to general $G$ functions, which we view as an intriguing open problem for future work.\n\n---\n\n### Question 1\n\n> The cited related work [Cai-Luo-Wei-Zheng, 2023, Uncoupled and Convergent Learning in Two-Player Zero-Sum Markov Games] shows that the Nash equilibria of 2-player zero-sum games can be found by an uncoupled learning algorithm with bandit feedback with $O(T^{-1/8})$ convegence rate.\n\n### Answer\n\nAs you pointed out, Cai et al. [2023] have analyzed settings that are subtly different from ours, both in terms of the feedback type (bandit feedback vs noisy feedback), and the type of game (two-player zero-sum matrix games or Markov games vs $N$-player monotone games). This distinction makes it challenging to draw a straightforward comparison between the convergence rate of theirs and ours in Theorem 5.2.\n\nMoreover, the algorithm proposed by Cai et al. [2023] involves a decreasing perturbation strength $\\mu_t$ over iteration $t$. However, in our method, as indicated in Theorem 4.2, a smaller $\\mu$ necessitates a correspondingly smaller learning rate $\\eta$. This could potentially slow down the convergence rate and make it difficult to find an appropriate learning rate. Therefore, for the sake of practicality, we have opted to maintain $\\mu$ as a constant independent of $t$ in our algorithm."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287706405,
                "cdate": 1700287706405,
                "tmdate": 1700287706405,
                "mdate": 1700287706405,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FuaLvh0AwC",
                "forum": "YclZqtwf9e",
                "replyto": "bjZljb2Cck",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5384/Reviewer_yRC2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5384/Reviewer_yRC2"
                ],
                "content": {
                    "title": {
                        "value": "Happy with authors' rebuttal and keep rating 6"
                    },
                    "comment": {
                        "value": "I am happy with the authors' response and keep a positive rating 6."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687759373,
                "cdate": 1700687759373,
                "tmdate": 1700687759373,
                "mdate": 1700687759373,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CpFWzwKxih",
            "forum": "YclZqtwf9e",
            "replyto": "YclZqtwf9e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5384/Reviewer_GKmU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5384/Reviewer_GKmU"
            ],
            "content": {
                "summary": {
                    "value": "The authors studied equilibrium computation in monotone games by addressing the cases where the feedback can be contaminated by some noise through the perturbation of payoffs. To this end, they focused on perturbing the payoffs based on the distance from a slingshot strategy. They characterized the convergence rate toward a near equilibrium. They proposed updating the slingshot strategy to converge exact equilibrium with last-iterate convergence guarantees."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The entire paper (including the Appendix) is well-written.\n- The balance between the preliminary information and new content is very good, which makes the paper accessible.\n- Addressing noisy feedback is of interest.\n- Updating the slingshot strategy for exact equilibrium convergence is interesting."
                },
                "weaknesses": {
                    "value": "- In-line mathematical expressions makes the paper difficult to read."
                },
                "questions": {
                    "value": "No clarification is needed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5384/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5384/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5384/Reviewer_GKmU"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5384/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785548375,
            "cdate": 1698785548375,
            "tmdate": 1699636544622,
            "mdate": 1699636544622,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "guxdx5pBUO",
                "forum": "YclZqtwf9e",
                "replyto": "CpFWzwKxih",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5384/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5384/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GKmU"
                    },
                    "comment": {
                        "value": "Many thanks for your encouraging and positive comments on our paper! We appreciate your positive remarks on the writing, the balance of content, and our approach. Your comments greatly encourage our work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700041559220,
                "cdate": 1700041559220,
                "tmdate": 1700041559220,
                "mdate": 1700041559220,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0Aw89l9AlU",
            "forum": "YclZqtwf9e",
            "replyto": "YclZqtwf9e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5384/Reviewer_53rn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5384/Reviewer_53rn"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of computing Nash equilibria in monotone games. Monotone games are of utter importance in the (algorithmic) game theory and algorithms that can be executed in a rather decentralized manner and have provable efficient convergence rates are always sought after.\n\nIn particular, this work unifies pre-existing literature that relies on perturbing agents' payoffs and develops a general meta-algorithm, \"FTRL with Slingshot Perturbation\". This framework can retrieve multiple algorithms proposed in recent works, e.g., [1], [2], [3].\n\nThe algorithm (in the full-information setting) works as follows:\n* A regularization function, $\\psi$, and a regularization parameter, $\\mu$, are selected. Every player initializes a \"slingshot\" strategy.\n* For a fixed number of steps, players run an instance of the FTRL algorithm, with the spin-off of perturbing their utility gradient by adding the gradient of the Bregman distance generated by $\\psi$ times the parameter $\\mu$.\n* Every player updates their \"slingshot strategy\" and returns to the second step.\n\nThe guarantees of this algorithm are:\n* an exponential convergence rate of the sequence of strategies to the unique Nash equilibrium of the perturbed problem for a fixed slingshot strategy \n* the convergence of the slingshot strategies (i.e., the strategies as well) to an approximate equilibrium.\n\nFurther, if the algorithm is run for an infinite number of steps, it is guaranteed to reach an exact Nash equilibrium of the unperturbed game.\n\nThe results are also extended for a meta-algorithm where Mirror Descent is used in place of FTRL.\n\n[1] Tuyls, K., Hoen, P.J.T. and Vanschoenwinkel, B., 2006. An evolutionary dynamical analysis of multi-agent learning in iterated games. \n[2] Perolat, J., Munos, R., Lespiau, J.B., Omidshafiei, S., Rowland, M., Ortega, P., Burch, N., Anthony, T., Balduzzi, D., De Vylder, B. and Piliouras, G., 2021, July. From Poincar\u00e9 recurrence to convergence in imperfect information games: Finding equilibrium via regularization.\n[3] Abe, Kenshi, Sakamoto, Mitsuki, Iwasaki Atsushi, 2022. Mutation-driven follow the regularized leader\nfor last-iterate convergence in zero-sum games."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper unifies existing work and its main claims are quite general in their statement.\n* The idea of the FTRL-slingshot-perturbation algorithm is natural and elegant.\n* The paper is more or less self-contained.\n* The authors establish convergence rates for both the full information and the noisy information feedback setting.\n* The algorithm assets a simplified analysis of convergence."
                },
                "weaknesses": {
                    "value": "* At first glance, the claims about \"exact Nash equilibrium\" computation can seem misleading.\n* The algorithm is not fully decentralized. It could be possible to discuss whether the process would converge when the frequency of update of the slingshot strategy and $\\mu$ varied across players.\n* There already exist algorithms with last-iterate convergence and no-regret guarantees for monotone games. This work does not have any no-regret guarantees nor does it try to consider more decentralized settings (i.e., players have to change their slingshot strategy at the same time, and $\\mu$ is the same for everyone as well as $\\psi$ --- at least according to my understanding)"
                },
                "questions": {
                    "value": "* Do you think that the slingshot perturbation framework achieves no adversarial regret?\n* What would happen if a fixed number of agents did not update their slingshot strategy every time or every player had their own frequency of updating the slingshot strategy?\n* Would it be possible to tune $\\mu$ as well to achieve better convergence rates?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5384/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5384/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5384/Reviewer_53rn"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5384/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796145380,
            "cdate": 1698796145380,
            "tmdate": 1699636544527,
            "mdate": 1699636544527,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XpfMfNrkKu",
                "forum": "YclZqtwf9e",
                "replyto": "0Aw89l9AlU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5384/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5384/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 53rn"
                    },
                    "comment": {
                        "value": "We thank you for your positive feedback and constructive comments. The detailed answers to each of the questions can be found below.\n\n---\n\n### Weakness 1\n\n> At first glance, the claims about \"exact Nash equilibrium\" computation can seem misleading.\n\n### Answer\n\nThanks for pointing this out to us. We have replaced the term \u2018\u2019exact Nash equilibrium\u2019\u2019 with \u2018\u2019Nash equilibrium of the underlying game\u2019\u2019 in our latest version (highlighted in red) to clarify what the term means.\n\n---\n\n### Weakness 2\n\n> The algorithm is not fully decentralized. It could be possible to discuss whether the process would converge when the frequency of update of the slingshot strategy and\u00a0$\\mu$\u00a0varied across players.\n\n### Answer\n\nWhen each player may have a distinct perturbation strength $\\mu_i>0$, convergence results very similar to the theorems in Section 4 are expected to be obtained. This is because strong convexity for every perturbed payoff function is still maintained. Consequently, we expect the behavior to be consistent with these theorems, substituting $\\mu$ with the smallest perturbation strength $\\min_{i\\in [N]}\\mu_i$ across all players.\n\nFor the scenarios where the frequency of updates of the slingshot strategy, $T_{\\sigma}$, varies among players, that is, each player updates the slingshot strategy at his or her own timing, we assume that each player independently adheres to the lower bound on $T_{\\sigma}$ as defined in Theorems 5.1 or 5.2. Under this condition, we are expected to obtain similar convergence rates as in these theorems.\n\nTherefore, we believe that we can fully decentralize our algorithm with ease. We also agree that further analysis of this asymmetric setup would present an intriguing and promising avenue for future research.\n\n---\n\n### Question 1\n\n> Do you think that the slingshot perturbation framework achieves no adversarial regret?\n\n### Answer\n\nAlthough a no-regret guarantee is not crucial for ensuring last-iterate convergence, we guess the FTRL-SP would achieve the no-regret property. This is because FTRL-SP updates the slingshot strategy so that it converges to the best response against the current strategy of the opponent. However, this does not always admit an optimal rate. We will mention this issue in the subsequent revision upon your request.\n\n---\n\n### Question 2\n\n> What would happen if a fixed number of agents did not update their slingshot strategy every time or every player had their own frequency of updating the slingshot strategy?\n\n### Answer\n\nWe anticipate that if some agents do not update their slingshot strategy, or if $T_{\\sigma}=\\infty$ for some agents, it would disrupt the last-iterate convergence property. Specifically, the strategy of the player with $T_{\\sigma}=\\infty$  at least would not reach a Nash equilibrium strategy.\n\nWith regards to the scenario where each player has their own $T_{\\sigma}$, we have addressed this in our response to Weakness 2.\n\n---\n\n### Question 3\n\n> Would it be possible to tune\u00a0$\\mu$\u00a0as well to achieve better convergence rates?\n\n### Answer\n\nWe appreciate your insightful suggestion regarding the tuning of $\\mu$ for better convergence rates. Indeed, setting $\\mu$ to a small value that depends on $T$ could potentially lead to improved convergence rates. However, as we have implied in Theorem 4.2, a smaller $\\mu$ would necessitate a smaller learning rate $\\eta$, which could make the balancing between $\\mu$ and $\\eta$ quite challenging. For the sake of practicality, we have opted to maintain $\\mu$ as a constant independent of $T$."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700036531943,
                "cdate": 1700036531943,
                "tmdate": 1700036531943,
                "mdate": 1700036531943,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DpraGI6269",
            "forum": "YclZqtwf9e",
            "replyto": "YclZqtwf9e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5384/Reviewer_vVKF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5384/Reviewer_vVKF"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies last-iterate convergence to NE in monotone games, under full but noisy feedback. The techniques used focus on regularising the payoffs by a reference strategy named \"slingshot strategy\". The slingshot is then updated as the current maintained strategy at exponential intervals and thus converges last-iterate to the true NE of the game rater then to the perturbed one."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The per studies the problem of finding a NE under perturbed feedbacks in an important subclass of games, which are monotone games. This class includes many important examples and thus is an important achievement. The paper is fairly well written and the results seems correct (although not surprising). The topic of the per is in line with the interests of the ICLR community"
                },
                "weaknesses": {
                    "value": "My first concern is with the motivation for the work. The authors rarely mention way one should consider full but noisy feedback. You only have full feedback when you posses a great understanding of the environment, but then noise wouldn't make much sense. I would get is you considered noisy bandit feedback, but the authors do not seem to discuss the problem.\nAlso, it is not clear if noise ruins the guarantees of the algorithms already present in the literature. Usually no-regret algorithm still converge in full feedback, to some sort of equilibria even in the presence of noisy feedback as long as the loss used are unbiased. The authors should discuss better why existing techniques fail, why now they only fail in terms of experimental evaluation.\n\nMy second, and far more pressing, concern is on the technical innovation of the work. The \"slingshot\" perturbation was studied extensively in recent papers, as also noted by the authors (Sokota et al. 2023, Bernasconi et al. 2022, Perlolat at al. 2021, Liu et al. 2023).\nThe main technical contribution of the paper is the introduction of the noise into the feedback, but it not clearly well explained why this is challenging or important and what technical novelty does the problem require.\n\nMoreover, I'm puzzled about some results about section 5.  In particular how is it possible to have some meaningful guarantees as the gradient of $G(\\pi,\\sigma)$ can be arbitrary large in general. For example if $\\sigma$ is close to the boundary and $G$ is the KL divergence, probably the results of section 5 only require very nice properties of $G$ and thus it limits its applicability. \n\nIs also not clear to me why you do the slingshot update. You give guarantees on the exploitability of the last iterate strategy, but not on the convergence rate to the strategy itself, as you do in theorem 4.2 and 4.10 for approximate nash."
                },
                "questions": {
                    "value": "1) Way should one care about full but noisy feedback?\n2) What are the new techniques introduced here that are designed to help with noisy feedback?\n3) Way don't you consider dynamic $\\mu=\\mu_k$ in section 5? Diminishing it every $T_\\sigma$ turns would help as long as you can bound the distance (with either $G$ of $D$) between $\\pi^{\\mu_k}$ and $\\pi^{\\mu_{k+1}}$. This is a natural question that should give asymptotic rates to exact nash.\n4) Do you require $G$ to have bounded gradient in section 5? Otherwise I think you cannot use some statements such as theorem 4.2 and 4.9 in section 5.\n5) Have you considered using smaller and smaller values of $\\mu=\\mu_k$? This would create a sequence of NE equilibria that converge to the exact one. Similar techniques have been used in Bernasconi et al. 2022 and Liu et al. 2023, and I think should be discussed.\n6) Way MWU does not converge in figure 1, without noise?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5384/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5384/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5384/Reviewer_vVKF"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5384/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698921725970,
            "cdate": 1698921725970,
            "tmdate": 1699636544431,
            "mdate": 1699636544431,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MTyI5Oesk3",
                "forum": "YclZqtwf9e",
                "replyto": "DpraGI6269",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5384/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5384/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vVKF (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our work and for providing your valuable feedback. We appreciate your insights and understand that you may have some concerns regarding certain aspects of our study. Our responses below address these concerns.\n\n---\n\n### Weakness 1\n\n> Usually no-regret algorithm still converge in full feedback, to some sort of equilibria even in the presence of noisy feedback as long as the loss used are unbiased. The authors should discuss better why existing techniques fail, why now they only fail in terms of experimental evaluation.\n\n### Answer\n\nStandard no-regret algorithms like MWU converge to a Nash equilibrium only when one averages strategies over iterations to obtain the exact equilibrium strategy, since the dynamics make an orbit around it. While this property is well-known as average-iterate convergence, we focus on last-iterate convergence, where an algorithm converges to an equilibrium without taking the average of the strategies. We hope this clarifies the difference between the concepts of average-iterate and last-iterate.\n\nPrevious studies [Daskalakis and Panageas, 2019, Mertikopoulos et al., 2019, Wei et al., 2021] have shown that optimistic no-regret algorithms, such as OWMU and OGD, enjoy last-iterate convergence only with full feedback, but fail with noisy feedback [Abe et al. 2023]. Roughly speaking, the success with full feedback is due to the proper bound of the variation of the gradient feedback vectors $\\sum_{i=1}^N\\\\|\\widehat{\\nabla}\\_{\\pi_i}v_i(\\pi^t) -\\widehat{\\nabla}\\_{\\pi_i}v_i(\\pi^{t-1})\\\\|^2$ over iterations. More formally, we define the path length of the gradient feedbacks as $\\sum_{s=1}^t\\sum_{i=1}^N\\\\|\\widehat{\\nabla}\\_{\\pi_i}v_i(\\pi^s) -\\widehat{\\nabla}\\_{\\pi_i}v_i(\\pi^{s-1})\\\\|^2$. Noisy feedback makes the path length large, leading to the failure of optimistic no-regret algorithms to converge to an equilibrium.\n\nThus, achieving last-iterate convergence with noisy feedback is still a significantly challenging task. We will explain the intuition in our paper's Introduction or related literature section.\n\n---\n\n### Weakness 2\n\n> You give guarantees on the exploitability of the last iterate strategy, but not on the convergence rate to the strategy itself, as you do in theorem 4.2 and 4.10 for approximate nash.\n\n### Answer\n\nFirst, we would like to clarify that the convergence of exploitability means that the last-iterate strategy itself converges to an equilibrium. Therefore, our results (Theorems 5.1 and 5.2) inherently indicate that we have provided the convergence rate of the strategy itself. Exploitability is a standard measure of proximity to a Nash equilibrium, as used in several studies [Cai et al., 2022a,b, Abe et al., 2023, Cai and Zheng, 2023].\n\n---\n\n### Question 1\n\n> What are the new techniques introduced here that are designed to help with noisy feedback?\n\n### Answer\n\nThe techniques devised in this paper are manifold. \n\n- We have developed a subtly different perturbation technique from the existing ones you mentioned so that it enables the FTRL or MD dynamics to converge to an approximate equilibrium in underlying games (an equilibrium in perturbed games).\n- The existing studies only show the sort of convergence only with full feedback, and we have been successful with noisy feedback. So, we are the first to establish last-iterate convergence with noisy feedback.\n- Our perturbation technique also provides a comprehensive view of payoff-regularized algorithms such as Perorat et al. [2021] and Abe et al. [2022].\n- Furthermore, the idea of the slingshot strategy update leads us to find a Nash equilibrium in underlying games (not an approximate one). In other words, it is innovative that we mix the perturbation technique with the slingshot strategy update and exhibit last-iterate convergence with both full and noisy feedback, which has not been achieved so far."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699940459118,
                "cdate": 1699940459118,
                "tmdate": 1700110225713,
                "mdate": 1700110225713,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tq33YuAgp2",
                "forum": "YclZqtwf9e",
                "replyto": "DpraGI6269",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5384/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5384/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vVKF (2/2)"
                    },
                    "comment": {
                        "value": "### Question 2\n\n> Do you require\u00a0$G$\u00a0to have bounded gradient in section 5? Otherwise I think you cannot use some statements such as theorem 4.2 and 4.9 in section 5.\n\n### Answer\n\nNo, we do **not** require the gradient of $G$ to be bounded for Theorems 5.1, 5.2, G.1, and G.2. Therefore, we believe that these theorems hold for a wide range of divergence functions.\n\n---\n\n### Question 3\n\n> Way don't you consider dynamic\u00a0$\\mu=\\mu_k$\u00a0in section 5? Diminishing it every\u00a0$T_{\\sigma}$\u00a0turns would help as long as you can bound the distance (with either\u00a0$G$\u00a0of $D$) between\u00a0$\\pi^{\\mu_k}$\u00a0and\u00a0$\\pi_{\\mu_{k+1}}$.\n\n### Answer\n\nThank you for your insightful comment! We would first like to point out that we have already given the **non-asymptotic convergence rates to exact Nash equilibria** in Theorems 5.1 and 5.2 for our approach (of course, they also mean the asymptotic rates). As in Theorem 4.2, when the perturbation strength $\\mu$ is small, a correspondingly small learning rate $\\eta$ should be used. Consequently, the convergence speed would slow down if we were to adopt an approach where $\\mu_k$ decreases as $k$ increases, and finding a proper learning rate would be difficult. On the contrary, our slingshot update approach does not require a decaying $\\mu_k$. Hence, our method does not suffer from the stringent requirement of selecting appropriate $\\mu$ and $\\eta$ and is more robust than decaying perturbation approaches in terms of hyperparameter settings.\n\n---\n\n### Question 4\n\n> Way MWU does not converge in figure 1, without noise?\n\n### Answer\n\nFigure 1 shows the exploitability of the last-iterate strategy of each algorithm. As we previously addressed in our response to Weakness 1, MWU\u2019s last-iterate strategy indeed fails to converge even with full feedback. This phenomenon has been widely owhebserved and discussed in several studies [Perolat et al., 2021, Abe et al., 2023, Liu et al., 2023]. Furthermore, it has even been theoretically substantiated by Mertikopoulos et al. [2018] and Bailey and Piliouras [2018]."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699940484962,
                "cdate": 1699940484962,
                "tmdate": 1699940484962,
                "mdate": 1699940484962,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BUPGGeRMei",
                "forum": "YclZqtwf9e",
                "replyto": "tq33YuAgp2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5384/Reviewer_vVKF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5384/Reviewer_vVKF"
                ],
                "content": {
                    "comment": {
                        "value": "After the authors' response I still have doubts about the paper's contributions.\n1) The algorithm converge last iterate to perturbed equilibrium, not to exact equilibrium. (Convergence to exact eq is given only in section 5 where the authors rely on some strange assumptions about the rate of change of parametric equilibria).\n2) Section 5 (the one in which you actually use slingshot perturbation) only works with $G(a,b)=\\|a-b\\|_2^2$. If the authors think that the procedure could also work for other $G$ (see response to review yRC2, weakness 3) they should prove it or tone down the achievements about that part of the paper.\n3) \"First, we would like to clarify that the convergence of exploitability means that the last-iterate strategy itself converges to an equilibrium\" This is not true. You converge last iterate to an approximate equilibrium, that has comparable exploitability as the true equilibrium, but it could be very far from the actual equilibrium, so last iterate property is less appealing that if the algorithm would converge to the true equilibrium\n4) In Th 5.1 and 5.2 the assumption on the difference $\\pi^{\\mu,\\sigma^k}-\\sigma^{k+1}$ seems to bypass the main difficulty of doing slingshot update, which is that that equilibria can jump from one place to another when the parameter (or losses) are changes, even slightly. It can also be that the assumption are written in a confusing way or that I'm missing something. \nThe authors should definitely discuss if these assumptions are ever met, because they seem pretty strong written as such.\n5) I think it would be more fair to wither consider the average policy in MWU or not to include it at all, when you known that the last iterate convergence would not converge. This is because, the presence of noise could be unknown to the user of the algorithm, but  the user would certainly know that it has to take the average strategy for MWU to work. Also it would be fair to include RM, RM+ and equivalent in the experimental evaluations."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514841740,
                "cdate": 1700514841740,
                "tmdate": 1700514841740,
                "mdate": 1700514841740,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KMODOi2Wc6",
                "forum": "YclZqtwf9e",
                "replyto": "DpraGI6269",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5384/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5384/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the comments of Reviewer vVKF (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your further comments! Firstly, we would like to summarize the key points of our response to your comments as follows:\n\n- **The assumptions of Theorems 5.1 and 5.2 are naturally satisfied by Theorems 4.2 and 4.9, hence they are not strong assumptions.**\n- **Theorems 5.1 and 5.2 guarantee the last-iterate convergence to an actual Nash equilibrium, not a perturbed equilibrium.**\n- **We have also provided the last-iterate convergence results for $G$ other than squared $\\ell^2$-distance in Theorems G.1 and G.2.**\n\nThe details of our responses to each of your comments can be found below.\n\n---\n\n### Comment 1\n\n> The algorithm converge last iterate to perturbed equilibrium, not to exact equilibrium. (Convergence to exact eq is given only in section 5 where the authors rely on some strange assumptions about the rate of change of parametric equilibria).\n\n### Answer\n\nIn Section 5, we have presented Theorems 5.1 and 5.2 that ensure the last-iterate convergence to an actual equilibrium, not to a perturbed one. Your concern might be regarding the assumptions on the difference\u00a0$\\pi^{\\mu,\\sigma^k}-\\sigma^{k+1}$. To clarify, we have addressed this point in our response to Comment 4, where we explain that these assumptions are generally satisfied and thus, are not strong assumptions.\n\n---\n\n### Comment 2\n\n> Section 5 (the one in which you actually use slingshot perturbation) only works with\u00a0$G(a,b)=|a-b|_2^2$. If the authors think that the procedure could also work for other\u00a0$G$\u00a0(see response to review yRC2, weakness 3) they should prove it or tone down the achievements about that part of the paper.\n\n### Answer\n\nWhile our last-iterate convergence rates are indeed provided for the case where $G$ is squared $\\ell^2$-distance, we have also established the asymptotic last-iterate convergence results for more general functions $G$ in Theorems G.1 and G.2 (i.e., Bregman divergence, $\\alpha$-divergence, R\u00e9nyi-divergence, and reverse KL divergence). Notably, these results significantly extend the scope of the asymptotic convergence results previously established for KL divergence and reverse KL divergence in existing studies [Perorat et al., 2021, Abe et al., 2022]."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552741170,
                "cdate": 1700552741170,
                "tmdate": 1700564897988,
                "mdate": 1700564897988,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uyAOFUy2JD",
                "forum": "YclZqtwf9e",
                "replyto": "DpraGI6269",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5384/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5384/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the comments of Reviewer vVKF (2/2)"
                    },
                    "comment": {
                        "value": "### Comment 3\n\n> \"First, we would like to clarify that the convergence of exploitability means that the last-iterate strategy itself converges to an equilibrium\" This is not true. You converge last iterate to an approximate equilibrium, that has comparable exploitability as the true equilibrium, but it could be very far from the actual equilibrium, so last iterate property is less appealing that if the algorithm would converge to the true equilibrium\n\n### Answer\n\nAs indicated by Theorems 5.1 and 5.2, the exploitability of the last-iterate strategy $\\pi^t$ converges $0$. Moreover, according to the definition of exploitability (as we have described in Section 2), the value of exploitability $\\mathrm{exploit}(\\pi)$ of a given strategy $\\pi$ equals $0$ **if and only if $\\pi$ is an actual Nash equilibrium in the original game**. Therefore, Theorems 5.1 and 5.2 mean that the last-iterate strategy $\\pi^t$ updated by our FTRL-SP does indeed converge to an actual Nash equilibrium, not an approximate equilibrium. \n\nWe will be happy if this clarifies your concerns, although we are still uncertain about why you think last-iterate could be far from the actual equilibrium. Let us know if you think the last iterate property established so far in this literature lacks appeal, please. \n\n---\n\n### Comment 4\n\n> In Th 5.1 and 5.2 the assumption on the difference\u00a0$\\pi^{\\mu,\\sigma^k}-\\sigma^{k+1}$\u00a0seems to bypass the main difficulty of doing slingshot update, which is that that equilibria can jump from one place to another when the parameter (or losses) are changes, even slightly. It can also be that the assumption are written in a confusing way or that I'm missing something. The authors should definitely discuss if these assumptions are ever met, because they seem pretty strong written as such.\n\n### Answer\n\nWe would like to emphasize that in our slingshot strategy update framework, the perturbed equilibrium $\\pi^{\\mu,\\sigma^k}$ at $k$-th slingshot update is not far from the next slingshot strategy $\\sigma^{k+1}$. Specifically, the assumption that $\\|\\pi^{\\mu,\\sigma^k}-\\sigma^{k+1}\\|\\leq \\|\\pi^{\\mu,\\sigma^k}-\\sigma^k\\|\\left(\\frac{1}{c}\\right)^{T_{\\sigma}}$ is directly satisfied when Theorem 4.2 holds. This is due to the following reasons:\n\n- Let us set $\\sigma=\\sigma^k$ and $\\pi^0=\\sigma^k$ in Theorem 4.2.\n- From this theorem, we can immediately deduce that $\\|\\pi^{\\mu,\\sigma^k}-\\pi^t\\| \\leq \\|\\pi^{\\mu,\\sigma^k} - \\sigma^k\\|\\left(\\frac{1}{c}\\right)^t$ holds for any $t\\geq 1$.\n- Therefore, by taking $t=T_{\\sigma}$, we obtain $\\|\\pi^{\\mu,\\sigma^k}-\\pi^{T_{\\sigma}}\\| \\leq \\|\\pi^{\\mu,\\sigma^k} - \\sigma^k\\|\\left(\\frac{1}{c}\\right)^{T_{\\sigma}}$.\n- Since we use $\\pi^{T_{\\sigma}}$ as the next slingshot strategy $\\sigma^{k+1}$ in our framework, it follows that $\\|\\pi^{\\mu,\\sigma^k}-\\sigma^{k+1}\\|\\leq \\|\\pi^{\\mu,\\sigma^k}-\\sigma^k\\|\\left(\\frac{1}{c}\\right)^{T_{\\sigma}}$.\n\nHence, FTRL-SP always satisfies this assumption in the full feedback setting, and we believe that the assumption regarding the difference\u00a0$\\pi^{\\mu,\\sigma^k}-\\sigma^{k+1}$\u00a0is not strong. A similar reasoning can be applied to Theorem 5.2 for the noisy feedback setting. \n\n---\n\n### Comment 5\n\n> I think it would be more fair to wither consider the average policy in MWU or not to include it at all, when you known that the last iterate convergence would not converge. This is because, the presence of noise could be unknown to the user of the algorithm, but the user would certainly know that it has to take the average strategy for MWU to work. Also it would be fair to include RM, RM+ and equivalent in the experimental evaluations.\n\n### Answer\n\nThank you for your suggestion! We will remove MWU\u2019s results from Figures 1 and 2. Regarding your suggestion to include RM and RM+ in our experimental evaluations, we are currently conducting these experiments. As soon as the results are available, we will update our paper accordingly to provide a more comprehensive analysis."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552791300,
                "cdate": 1700552791300,
                "tmdate": 1700615517800,
                "mdate": 1700615517800,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UYGqDvkF25",
                "forum": "YclZqtwf9e",
                "replyto": "DpraGI6269",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5384/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5384/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Manuscript Revision - Experimental Comparison Included as Requested"
                    },
                    "comment": {
                        "value": "Dear Reviewer vVKF,\n\n\nAs you requested, we have included an experimental comparison with the averaged strategies of MWU, RM, and RM+ in Section H.4 of our revised manuscript (highlighted in red). \nIf you're interested, feel free to take a look at these results.\n\nSincerely,\n\nThe Authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5384/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646593198,
                "cdate": 1700646593198,
                "tmdate": 1700646621116,
                "mdate": 1700646621116,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]