[
    {
        "title": "Intrinsic Riemannian Classifiers on the Deformed SPD Manifolds: A Unified Framework"
    },
    {
        "review": {
            "id": "WCb1sYBSVi",
            "forum": "EyWKb7Ltcx",
            "replyto": "EyWKb7Ltcx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4035/Reviewer_ur4M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4035/Reviewer_ur4M"
            ],
            "content": {
                "summary": {
                    "value": "Geometric deep learning has gained attention for extending deep learning to non-Euclidean spaces. To improve the classification of non-Euclidean features, researchers have explored intrinsic classifiers based on Riemannian geometry. However, existing approaches are limited due to their reliance on specific geometric properties. This paper introduces a general framework for designing multinomial logistic regression on Riemannian manifolds. This framework requires minimal geometric properties. The focus is on symmetric positive definite (SPD) manifolds, and the study includes five families of parameterized Riemannian metrics to develop diverse SPD classifiers. The versatility and effectiveness of this framework are demonstrated in applications such as radar recognition, human action recognition, and EEG classification."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper addresses the problem of supervised classification on Riemannian manifolds with a focus on the SPD manifold. The latter is used extensively to classify biosignals such as MEG or EEG.\nSeveral applications are considered: classification of radar, human action and EEG data.\nThe deformation $\\theta$ shows promising results in application and can be used in placed in many classical classification algorithms."
                },
                "weaknesses": {
                    "value": "The paper is quite hard to follow.\n\nFirst of all, the authors claim their approach is general in terms of classifiers and Riemannian manifolds. However, they only derive results for multinomial logistic regression on the SPD manifold.\n\nSecond, the contributions are not very clear. For example, the derivation of Theorem 3.4 has already been done in eq 17 of \"Riemannian Multiclass Logistics Regression for SPD Neural Networks\" from Chen et al. Furthermore, it can be directly derived from eq (3) by parametrizing $b_k$ as $\\langle p_k, x \\rangle$ and then interpreting the subtraction as a Riemannian logarithm.\nThe distance $d(S, \\tilde{H}_{A, P})$ is defined twice: in eq (8) and eq (11). One should be a proposition and the other a definition.\nThere is a mistake in $b_k$ in Appendix C.\n\nThird, the section 4 is really hard to understand. Specifically, the first paragraph of sub-section 4.1 discusses metrics that have not been presented so far. An example of how to apply theorem 4.2 and lemma 4.3 to a Riemannian metric should be added to understand their implications better.\n\nForth, the tables in the experiment section are not very clear. For example, in table 4, the authors mention methods [93, 30] and then [93, 70, 30]. What does it mean? The second row of results utilizes the same methods as the first row?"
                },
                "questions": {
                    "value": "1) Can you explain more precisely the contributions of the paper? The one presented in the introduction are too broad.\n2) Can you provide an example of how to apply Theorem 4.2 and Lemma 4.3?\n3) Can you explain the rows in tables of the numerical experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4035/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4035/Reviewer_ur4M",
                        "ICLR.cc/2024/Conference/Submission4035/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4035/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697641808797,
            "cdate": 1697641808797,
            "tmdate": 1700473424645,
            "mdate": 1700473424645,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HZK0IdPvAO",
                "forum": "EyWKb7Ltcx",
                "replyto": "WCb1sYBSVi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4035/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4035/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Reviewer ur4M (R4) (1/2)"
                    },
                    "comment": {
                        "value": "We thank $\\textcolor{blue}{ur4M}$ \uff08$\\textcolor{blue}{R4}$\uff09for the careful review and the suggestive comments. Below, we address the comments in detail. ***Notice that as we have revised our manuscript, references to our paper here always denote its revised version, unless otherwise stated.***\n***\n\n**1. Results of Riemannian multinomial logistic regression (RMLR) on other manifolds.**\n\nRecalling our RMLR in Eq. (14), three operators are involved: Riemannian metric, logarithm, and parallel transportation (or left translation). For specific manifolds, one only needs to put these three operators into Eq. (14). We further implement our RMLR on $\\mathrm{SO}(n)$. Please refer to CQ#2 in the common response.\n\n**2. The distance is defined twice: in Eq (8) and Eq (11).**\n\nThe distance in Eq. (11) is different from Eq. (8). The distance in Eq. (11) is the one used in this paper. For better clarity, we have change the $d(\\cdot)$ in Eq. (8) as $\\tilde{d}(\\cdot)$. \n\n**3. It can be directly derived from Eq (3) by parametrizing $b _k$ as $\\langle p_k, x \\rangle$ and then interpreting the subtraction as a Riemannian logarithm.**\n\nThanks for your insightful comment. We can indeed directly interpret subtraction as the Riemannian logarithm and obtain the RMLR in Eq. (14). However, this direct re-interpretation is only an intuitive re-interpretation and has little theoretical support, as it involves no margin distance or margin hyperplane. However, the margin distance and hyperplane play an important role in designing RMLR, such as the ones on hyperbolic[1] and spherical manifolds [2]. \n\nOur insight is that RMLR in Eq. (14) is derived from the Riemannian margin distance (Def. 3.3) and hyperplane (Eq. (7)). In this way, it is precise to call Eq. (14) as RMLR. \n\nWe assume your concerns might come from our original structure of Sec. 3.2. To obtain RMLR, the margin distance in Def. 3.3 should be first solved. In the original submission, we put the results on margin distance in the appendix and directly presented our RMLR in the main paper. We have added a theorem about the solution of margin distance (Thm. 3.4) between Def. 3.3 and RMLR (Thm. 3.5) in the revised paper.\n\n**4. RMLR has already done in [3].**\n\nOur work differs with [3] both in ***theory*** and ***practice***, and further covers all the results in [3]. \n\n**Theory:** Although our RMLR (Eq. (14)) has a similar expression with [3, Eq. (18)], the underlying theories are significantly different. All the results in [3], including Eq. (18) in [3], are confined within SPD manifolds under the metric pulled back from the Euclidean space. However, numerous metrics are not pullback metrics from the Euclidean space, such as BWM, AIM on SPD manifolds [4], the invariant metric on matrix Lie groups [5], and the metrics on the Grassmann and Stiefel manifolds [6]. [3] fails to construct MLR under these metrics. In contrast, our RMLR is more general and can deal with non-flat metrics. Throughout the proof of Thms. 3.4- 3.5, our RMLR only requires geodesic connectedness. Diverse metrics or manifolds in machine learning satisfy this property. ***Therefore, our RMLR has broader application scenarios.***\n\n**Pratice:** We cover all the results in [3] and extensively study diverse SPD MLRs under deformed metrics. In detail, [3] only implements SPD MLR under $(\\alpha,\\beta)$-LEM, while our work implements five families of deformed metrics. Besides, as we discussed in Rmk 4.4, when $\\theta=1$, our SPD MLR under $(1,\\alpha,\\beta)$-LEM covers the SPD MLR in [3].\n\n**5. More precise contributions of the paper.**\n\nThis work presents a general framework for building Riemannian classifiers on manifolds and specifically showcases our framework on the deformed SPD manifolds. Please refer to CQ#1 in the common response for a detailed explanation."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4035/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700427157598,
                "cdate": 1700427157598,
                "tmdate": 1700427157598,
                "mdate": 1700427157598,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xz6liQ9KnJ",
                "forum": "EyWKb7Ltcx",
                "replyto": "WCb1sYBSVi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4035/Reviewer_ur4M"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4035/Reviewer_ur4M"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for all the answers. I read the new version of the paper and it is now much clearer. I am still a little bit disappointed by the numerical experiments where the effects of the metric could have been shown with figures (heat maps, line plots, ...) instead of tables.\n\nOverall, the paper is original and can bring value to the community of machine learning on Riemannian manifolds. Therefore, I raised my rating to a 6."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4035/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700473384028,
                "cdate": 1700473384028,
                "tmdate": 1700473384028,
                "mdate": 1700473384028,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KxKjsKArlO",
                "forum": "EyWKb7Ltcx",
                "replyto": "WCb1sYBSVi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4035/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4035/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for raising the score and suggestive comments"
                    },
                    "comment": {
                        "value": "We thank $\\textcolor{blue}{ur4M}$  ($\\textcolor{blue}{R4}$)  for raising the score, and we are delighted that you appreciate the value of our work.\ud83d\ude04 \n\n**1. Visualization of experimental results.**\n\nThanks for your suggestive comments. Following your suggestion, we visualize histograms of our experimental results on the HDM05 and Radar datasets. Please check G.4 in the appendix. If our paper gets accepted, we will move some experimental settings into the appendix and put these histograms into the main paper."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4035/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700522277259,
                "cdate": 1700522277259,
                "tmdate": 1700522437258,
                "mdate": 1700522437258,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "01tpHLm7j8",
            "forum": "EyWKb7Ltcx",
            "replyto": "EyWKb7Ltcx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4035/Reviewer_Qd7L"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4035/Reviewer_Qd7L"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies five families of deformed parameterized Riemannian metrics, developing diverse SPD classifiers respecting these metrics. The proposed methods were examined in radar recognition, human action recognition, and electroencephalography (EEG) classification tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper studies different metrics for classification on SPD manifolds.  The theoretical discussions provide nice insights covering different metrics extending the current solutions proposed in the literature."
                },
                "weaknesses": {
                    "value": "The proposal on the \u201cunified framework\u201d is an overclaim.  The paper provides nice results and in detailed theoretical discussions for different metrics. However, there are still rooms for exploration to develop a \u201cunified framework\u201d such as extension of the work for SPD manifolds with different structures, transformations and classifiers. Therefore, I suggest authors revising their claim considering the concrete results given in the paper, i.e. employment of 5 additional metrics for classification on SPD manifolds.\n\nAlthough theoretical discussions on different formulation of the metrics are nice, they should be extended considering their complexity and equivalence properties.  In addition, experimental analyses should be extended with additional datasets and backbones."
                },
                "questions": {
                    "value": "-\tCan you provide a comparative analysis of complexity (memory and running time footprints) of different metrics, both theoretically and experimentally (e.g. even for one task)?\n\t\n-\tThe accuracy of models are sensitive to hyperparameters of the metrics. How can researcher estimate these hyper-parameters in practice?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4035/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698745998543,
            "cdate": 1698745998543,
            "tmdate": 1699636366569,
            "mdate": 1699636366569,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K2nX4LMmqa",
                "forum": "EyWKb7Ltcx",
                "replyto": "01tpHLm7j8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4035/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4035/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank $\\textcolor{green}{Reviwer Qd7L}$ ($\\textcolor{green}{R3}$) for the instant reply and thoughtful comments. We respond to the questions point by point as follows. ***As we have revised our manuscript, references to our paper here always denote its revised version.***\n***\n\n**1.  \u201cunified framework\u201d --> \"general framework\".**\n\nThanks for your suggestive comments. Our RMLR in Eq. (14) is a general framework, as it only requires the existence of Riemannian logarithm $\\mathrm{Log} _P Q$ for any pair $P, Q$ in the manifold $\\mathcal{M}$. We call this property geodesic connectedness, and diverse manifolds in machine learning satisfy this property. Therefore, our framework can be implemented beyond SPD manifolds. We have extended our framework into the Lie group. Please refer to CQ#2 in the common response for more details.\n\nAlthough most of the existing manifolds in machine learning are geodesic connectedness, we do not know if, in the future, there would be a powerful metric with the unknown Riemannian logarithm. Therefore, as you said, \"unified\" might be a bit overly absolute. We have changed the word \"unified\" to \"general\".\n\n**2. Experimental analyses should be extended with additional datasets and backbones.**\n\nOur RMLR in Eq. (14) can readily apply to other manifolds or metrics. Please refer to CQ#2 in the common response for the experiments on $\\mathrm{SO(n)}$.\n\n**3. Comparative analysis of complexity (memory and running time).**\n\n**Memory cost:** Recalling Eq. (14) and Tab. 2, each class $k$ requires an SPD parameter $P _k$ and Euclidean parameter $A _k$ in SPD MLR. Therefore, the memory cost of the SPD MLR under each metric is the same.\n\n**Computational cost:** The key factor of the computational complexity of SPD MLRs under different metrics lies in the number of matrix functions, such as matrix power, logarithm, Lyapunov operator, and Cholesky decomposition. These matrix functions are divided into two categories: one is based on eigendecomposition, and the other is the Cholesky decomposition. The following table summarizes the number of matrix functions in SPD MLR. Therefore, the general efficiency of SPD MLR should be EM>LEM>LCM>AIM>BWM. Note that Cholesky decomposition is more efficient than eigendecomposition. Without deformation, the order should be EM>LCM>LEM>AIM>BWM. We report the average training time per epoch in Tab. 10 in App. G.3. As shown in Tab. 10, EM-, LEM-and LCM-based SPD MLR is more efficient than AIM- and BWM-based MLR.\n\nTable A: Number of matrix functions for each class $k$ in different SPD MLRs. a (b) means the number of matrix functions in the SPD MLR under the deformed (standard) metric.\n| Metric | Eig-based Matrix Functions | Cholesky Decomposition | In Total |\n|:------:|:--------------------------:|:----------------------:|:-----:|\n| LEM    |             1 (1)            |           0 (0)           |  1 (1) |\n| AIM    |             3 (2)            |           0 (0)           |  3 (2) |\n| EM     |             1 (0)            |           0 (0)           |  1 (0) |\n| LCM    |             1 (0)            |           1 (0)           |  2 (1) |\n| BWM    |             5 (4)            |           1 (1)           |  6 (5) |"
                    },
                    "title": {
                        "value": "Response Reviewer Qd7L (R3) (1/2)"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4035/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700426888152,
                "cdate": 1700426888152,
                "tmdate": 1700427063018,
                "mdate": 1700427063018,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0crO2lARiv",
            "forum": "EyWKb7Ltcx",
            "replyto": "EyWKb7Ltcx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4035/Reviewer_fSLb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4035/Reviewer_fSLb"
            ],
            "content": {
                "summary": {
                    "value": "The authors present an approach to build classifiers on Riemannian manifolds. This approach is then applied to SPD manifolds with 5 different Riemannian metrics. The proposed method is validated on radar recognition, action recognition, and electroencephalography (EEG) classification."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Summary of notations and mathematical proofs are provided.\n* The proposed method improves SPDNet on radar recognition and action recognition, and improves SPDDSMBN on EEG classification."
                },
                "weaknesses": {
                    "value": "* The contribution is incremental as it is heavly based on the works of Nguyen & Yang (2023) and Thanwerdas & Pennec (2019a; 2022a).\n* Experimental results are poorly presented (the text size in some tables, e.g. Tabs. 3 et 6 is too small to read).\n* Lack of evaluation to show the benefit of the proposed method. \n* Limitations are not discussed."
                },
                "questions": {
                    "value": "I have several concerns about the paper (please also see the question):\n\nFirst of all, there are definitions and statetements that look strange to me. \n\nAs stated by the authors, the main motivation behind the proposed approach is that it can be applied to Riemannian manifolds that only require geodesic connectedness as opposed to existing works. However, the definition of geodesic connectedness (Definition 3.1) given in the paper does not seem to be corrected. I'm wondering if \"geodesic connectedness\" means \"there exists a unique geodesic line connecting any two points\". As far as I know, the existence of a unique geodesic in Definition 3.1 is too strong. See for instance:\n\nhttps://www.cis.upenn.edu/~cis6100/diffgeom-n.pdf\n\nIt says that a Riemannian manifold is connected iff any two points can be joined by a broken geodesic (a piecewise smooth curve where each curve segment is a geodesic, Proposition 12.10). \n\nAm I wrong ? Please clarify.\n\nThis also leads to another question: What are the requirements for the proposed approach to be applicable ? If the requirement is that there must exist a unique geodesic line between any two points of the manifold, then I'm wondering if the range of applicability of the proposed approach is as limited as the approach in Nguyen & Yang (2023) ? Please clarify.\n\nI also doubt the statement at the end of Section 4.2 \"our work is the first to apply EM and BWM to establish Riemannian neural networks, opening up new possibilities for utilizing these metrics in machine learning applications\". Note that Han et al. (2021) has thoroughly studied the Bures-Wasserstein (BW) geometry for Riemannian optimization on SPD manifolds, where different machine learning applications have been presented. \n\nIt is also claimed in the paper that the proposed method is applicable to a broader class of Riemannian manifolds compared to existing works. However, the derived MLRs are all built on SPD manifolds and it is not clear if the proposed method is also effective in improving existing neural networks on other manifolds, e.g. Huang et al. (2017; 2018).\n\nConcerning the experiments, the authors only present comparisons against SPDNet and SPDDSMBN. I could not find any other comparisons against state-of-the-art methods on the target applications in the supplemental material. This makes it hard to make rigorous judgments about the effectiveness of the proposed approach with respect to other categories of neural networks. Taking action recognition application as an example. Many DNNs have been proven effective in this application on large-scale datasets. Experiments on large-scale datasets are thus important to show the advantage of learning on SPD manifolds over other manifolds (e.g. Euclidean, hyperbolic, Stiefel,...). \n\n\n**Questions:**\n\nIn Remark 3.2, it is not clear if item (a) is an observation made by the authors or it is a well-known result in the literature. In the first case, could the authors give a brief proof for that ? Otherwise, the result should be properly cited. \n\n\n**References**\n\n1. Andi Han, Bamdev Mishra, Pratik Kumar Jawanpuria, Junbin Gao: On Riemannian Optimization over Positive Definite Matrices with the Bures-Wasserstein Geometry. NeurIPS 2021: 8940-8953.\n\n2. Zhiwu Huang, Chengde Wan, Thomas Probst, Luc Van Gool: Deep Learning on Lie Groups for Skeleton-Based Action Recognition. CVPR 2017: 1243-1252.\n\n3. Zhiwu Huang, Jiqing Wu, Luc Van Gool: Building Deep Networks on Grassmann Manifolds. AAAI 2018: 3279-3286"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4035/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4035/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4035/Reviewer_fSLb"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4035/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823187887,
            "cdate": 1698823187887,
            "tmdate": 1699636366494,
            "mdate": 1699636366494,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n6V8RUl5lc",
                "forum": "EyWKb7Ltcx",
                "replyto": "0crO2lARiv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4035/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4035/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Reviewer fSLb (R2) (1/3)"
                    },
                    "comment": {
                        "value": "We thank $\\textcolor{brown}{Reviwer fSLb}$ ($\\textcolor{brown}{R2}$) for the constructive suggestions and insightful comments! In the following, we respond to the concerns in detail. ***Notice that as we have revised our manuscript, references to our paper here always denote its revised version.***\n\n**1. Our work and contributions are significantly different from Nguyen & Yang (2023) [1] and Thanwerdas & Pennec (2019a; 2022a) [2-3].**\n\nWe have briefly discussed the difference in Rmks. 3.6 and 4.4. Here, we present a more detailed explanation.\n\n**Difference with gyro SPD MLR [1]**\n\nOur work differs from gyro SPD MLR [1] both in ***theory*** and ***practice***. \n\n***Theory:*** Our framework uses no gyro structure and is more general than gyro SPD MLR.\n\nFirstly, the gyro MLR is based on gyro structures. However, not all metrics can induce gyro structures. In contrast, our method only requires Riemannian logarithm and parallel transportation (or group translation), which usually exists in manifolds in machine learning. Our method is thus more general and can be applied to more metrics.\n\nSecondly, even if the gyro structure exists, the margin distance to the hyperplane in Gyro MLR is based on gyro distance and gyro trigonometry [1, Def. 2.22]. Note that gyro distance is defined by the tangent space at the identity [1, Def. 2.15]. However, the most natural counterparts on manifolds are geodesic distance and Riemannian trigonometry (Eq. 12). Therefore, we directly use geodesic distance and Riemannian trigonometry to obtain the margin distance Eq. (13).\n\nThirdly, Gyro MLR is obtained case-by-case for a specific gyro space, since the pseudo-gyrodistance [1, Def. 2.22] should be specifically solved for each gyro space. On the contrary, our work solves the margin distance in a general form (Thm. 3.4), and presents a general framework in Thm. 3.5. For a specific metric, one only needs to put the Riemannian logarithm and parallel transport into Eqs. (14-15). These operators exist in most of the manifolds in machine learning. \n\n***Practice:*** Our work implements five families of deformed SPD MLRs (Tab. 2), while gyro SPD MLR only implements gyro SPD MLRs under the standard AIM, LEM, and LCM. Besides, as we discussed in Rmk. 4.4, our SPD MLRs incorporate the results presented in [1]. Moreover, the BWM and EM are geodesically incomplete, while gyro operations require geodesic completeness [1, Eqs. (1-2)]. Therefore, EM and BWM could be problematic to induce gyro structures, as could gyro SPD MLRs. On the contrary, SPD MLRs can be easily obtained by our Thm. 3.5 for these two metrics. The following table briefly summarizes the difference between our SPD MLRs and gyro SPD MLRs. In addition, our RMLR in Thm. 3.5 can also be implemented into other manifolds, such as $\\mathrm{SO}(n)$. Please refer to CQ#2 in the common response for our MLR in Lie groups.\n\nTab. 1 Difference between our SPD MLRs and the gyro SPD MLRs.\n|       SPD MLR      | Metrics Involved | Theoretical Foundations | Geometric Requirement |\n|:------------------:|:----------------:|:-----------------------:|-----------------------|\n|  SPD MLRs  | The standard AIM, LEM, and LCM | Gyro-structures induced by the standard AIM, LEM, and LCM | Gyro-structures, geodesic completeness|\n|  Ours  | $(\\theta,\\alpha,\\beta)$-AIM, $(\\theta,\\alpha,\\beta)$-LEM, $(\\theta,\\alpha,\\beta)$-EM, $\\theta$-LCM, and $2\\theta$-BWM | Riemannian geometry | Geodesic connectedness|\n\n\n\n\n**Difference with [2-3]**\n\nAlthough power deformation is discussed in [2-3], they only cover the deformation of $(\\alpha,\\beta)$-AIM, BWM, and the standard EM. Our paper further generalized $(\\alpha,\\beta)$-LEM, LCM, and $(\\alpha,\\beta)$-EM into deformed metrics (see Sec. 4.1). Besides, we not only discuss the properties of all the deformed metrics in Fig. 1 and Tab. 1, but also validate the efficacy of deformation. Experimental results shown in Tabs. 3-6 indicates the effectiveness of the deformed metrics.\n\n**2. Tabs. 3-6 is too small to read**\n\nThanks for the suggestion. After our work gets accepted, we will divide each table into two lines and move some stuff to supplementary.\n\n**3. Limitations are not discussed.**\n\nThe main limitation of our work is the over-parameterized $P _k$ in our RMLR. Recalling our RMLR in Eq. (14), each class would require an SPD parameter $P _k$ and a Euclidean parameter $A _k$. Consequently, as the number of classes grows, the classification layer would become burdened with excessive parameters. We have added a discussion on this limitation in App. A and will address this problem in our future work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4035/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700426657798,
                "cdate": 1700426657798,
                "tmdate": 1700426765387,
                "mdate": 1700426765387,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C3NdhffoBp",
                "forum": "EyWKb7Ltcx",
                "replyto": "n6V8RUl5lc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4035/Reviewer_fSLb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4035/Reviewer_fSLb"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their responses but these responses haven't addressed my major concerns (at least my first two questions haven't been answered by the authors)."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4035/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661622169,
                "cdate": 1700661622169,
                "tmdate": 1700661622169,
                "mdate": 1700661622169,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ApPlvmOzcj",
                "forum": "EyWKb7Ltcx",
                "replyto": "WNEcQnuvzQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4035/Reviewer_fSLb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4035/Reviewer_fSLb"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for further clarifications. \n\nHowever, I respectfully disagree with the authors answers for my questions. The authors said that \"...we do not find a term in standard Riemannian geometry to characterize this property...\". A quick search shows that the term \"geodesic connectedness\" has already been used in the literature:\n\nhttps://arxiv.org/pdf/math/0005039.pdf\n\nhttps://arxiv.org/pdf/2004.08357.pdf\n\nThe definitions of geodesic connectedness in the above works seem to be in line with Proposition 12.10.   \n\nSo what's the point of giving a new definition for geodesic connectedness ?\n\nThe authors are also vague in saying \"As the existence of $Log_P(Q)$ literally means geodesic connectedness, we define this property as geodesic connectedness.\" Does the two properties are the same ? \n\nI believe that the statement \"In this context, Def. 3.1 can be satisfied by many manifolds and metrics, such as SPD manifolds, Grassmannian manifolds, Stiefel manifolds, and various kinds of Lie groups.\" is not true. Grassmann manifolds are counter-examples because of the existance of cut locus points."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4035/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683562464,
                "cdate": 1700683562464,
                "tmdate": 1700683562464,
                "mdate": 1700683562464,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PHfcKA6ZT2",
                "forum": "EyWKb7Ltcx",
                "replyto": "0crO2lARiv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4035/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4035/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for further clarification and insightful comments."
                    },
                    "comment": {
                        "value": "Thanks for the instant reply and insightful comments! According to your comments, we have revised our paper to ensure our claims are more precise. We address your concerns in the following.\n\n**1. Geodesic connectedness --> The existence of the Riemannian logarithm**\n\nThanks for your insightful comments. We carefully read the literature you present. In the first literature [1], geodesic connectedness refers to whether each two points in the manifold can be joined by a geodesic, which is a bit different from our previous definition (our previous definition is defined by a unique geodesic).\n\nNevertheless, the ultimate reason for presenting this definition is that we want a short name to describe the existence of the Riemannian logarithm. We have changed the \"geodesic connectedness\" into \"the existence of the Riemannian logarithm\" throughout the paper for better clarity.\n\n- **Why we require the existence of the Riemannian logarithm.**\n\nRecall the reformulation of MLR Eq. (6-7):\n$$\np(y=k \\mid S) \\propto \\exp (\\operatorname{sign}(\\langle \\tilde{A} _k, \\mathrm{Log} _{P _k}(S) \\rangle _{P _k})\\|\\tilde{A} _k\\| _{P _k} \\tilde{d} (S, \\tilde{H} _{\\tilde{A} _k, P _k})),\n$$\n$$\n\\tilde{H} _{\\tilde{A} _k, P _k} = \\{S \\in \\mathcal{M}: g _{P _k}( \\mathrm{Log} _{P _k} S, \\tilde{A} _k) = \\langle \\mathrm{Log} _{P _k} S, \\tilde{A} _k \\rangle _{P _k}=0\\}.\n$$\nThe minimal requirement is the existence of Riemannian logarithm $\\mathrm{Log} _{P _k}(S)$ for each $k$. Without the existence of the Riemannian logarithm, even the hyperplane is ill-defined. However, previous work further relies on additional properties to realize MLR in manifolds, such as the generalized law of sines and gyro structures. In contrast, throughout the proof of Thm. 3.3-3.4, we only require the existence of the Riemannian logarithm. \n\n**2. Applicability**\n\nAs we clarified, the only requirement is the existence of the Riemannian logarithm. Many metrics or manifolds on machine learning indeed satisfy this property, including the five families of metrics discussed in this paper on SPD manifolds, the Lorentz model on hyperbolic manifolds [2, Eq. (11)], spherical manifolds [3, Sec. 3.1.1], and Lie groups [4, Tab. 1].\n\nIn our main paper, we have implemented our framework under five families of metrics on SPD manifolds. Besides, we further implement our framework on $\\mathrm{SO}(n)$. These applications should justify the applicability of our approach.\n\n**3. The Grassmann**\n\nThanks for your insightful comments on the cut locus of the Grassmann. On the Grassmann $\\mathrm{Gr} _{n,p}$, for $P, Q \\in \\mathrm{Gr} _{n,p}$, the Riemannian logarithm $\\mathrm{Log} _{P} (Q)$ exists for $Q \\in \\mathrm{Gr} _{n,p} \\backslash \\mathrm{Cut}_P$ [5, Props. 5.6], where $\\mathrm{Cut}_P$ is the cut locus of $P$.\n\nAs we mentioned above, if $S$ is in $\\mathrm{Cut} _{P _k}$, the hyperplane and MLR do not exist. In contrast, assuming $S$ is not in $\\mathrm{Cut} _{P _k}$ for each $k$, our Thm3. 3.3-3.4 still holds. In [6], the author also makes a similar assumption when dealing with the logarithm on the Grassmann [6, Sec. 3.2, P4]. However, how to ensure or transform $S$ outside $\\mathrm{Cut} _{P _k}$ is out of the scope of this work. We will explore this possibility in the future.\n\n> [1] https://arxiv.org/pdf/math/0005039.pdf\n> \n>[2] Liu Q, Nickel M, Kiela D. Hyperbolic graph neural networks.\n>\n>[3] Chakraborty R. Manifoldnorm: Extending normalizations on Riemannian manifolds.\n>\n>[4] Boumal N, Absil P A. A discrete regression method on manifolds and its application to data on SO (n).\n> \n>[5] Bendokat T, Zimmermann R, Absil P A. A Grassmann manifold handbook: Basic geometry and computational aspects.\n>\n> [6] Nguyen X S. The Gyro-Structure of Some Matrix Manifolds."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4035/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697429206,
                "cdate": 1700697429206,
                "tmdate": 1700697592477,
                "mdate": 1700697592477,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uxZeW87yNK",
            "forum": "EyWKb7Ltcx",
            "replyto": "EyWKb7Ltcx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4035/Reviewer_tGbz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4035/Reviewer_tGbz"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a unified framework for designing Riemannian classifiers for geometric deep\nlearning.  In this paper, we presented a  framework for designing intrinsic Riemannian classifiers\nfor matrix manifolds, with a specific focus on SPD manifolds. The paper studies five\nfamilies of deformed parameterized Riemannian metrics. Each of them develops an SPD\nclassifier respecting one of these metrics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Extensive experiments conducted on widely-used SPD benchmarks demonstrate that our proposed SPD classifiers achieve consistent performance gains, outperforming the previous classifiers by about 10% on human action recognition,\nand by 4.46% on electroencephalography (EEG) inter-subject classification."
                },
                "weaknesses": {
                    "value": "- The presentation of the paper doesn't help the reader to understand the main contributions of the paper.\n\n- The novelty is not clear. Using SPD matrices for human action recognition and EEG is not new.\n\n- Using J. Cavazza, A. Zunino, M. San Biagio, and V. Murino, \u201cKernelized covariance for action recognition,\u201d in Pattern Recognition (ICPR), 2016 23rd International Conference on. IEEE, 2016, pp. 408\u2013413.\n Eman A. Abdel-Ghaffar, Yujin Wu, Mohamed Daoudi, Subject-Dependent Emotion Recognition System Based on Multidimensional Electroencephalographic Signals: A Riemannian Geometry Approach. IEEE Access 10: 14993-15006 (2022)"
                },
                "questions": {
                    "value": "The authors should clarify the novelty of the proposed approach and reorganize the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NAN"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4035/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699200228979,
            "cdate": 1699200228979,
            "tmdate": 1699636366404,
            "mdate": 1699636366404,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Rqm1CbY0zg",
                "forum": "EyWKb7Ltcx",
                "replyto": "uxZeW87yNK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4035/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4035/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Reviewer tGbz (R1)"
                    },
                    "comment": {
                        "value": "We thank $\\textcolor{red}{Reviwer tGbz}$ ($\\textcolor{red}{R1}$) for the valuable comments. Below is our detailed response.\n***\n\n**1. Novelty and Contributions**\n\nThis paper presents a general framework for building Riemannian Multinomial Logistic Regression (RMLR) on manifolds, and specifically showcases five implementations on SPD manifolds. Please refer to CQ#1 in the common response for detailed discussions about our contributions and novelty.\n\n**2. Using J. Cavazza, A. Zunino, M. San Biagio, and V. Murino, \u201cKernelized covariance for action recognition,\u201d in Pattern Recognition (ICPR), 2016 23rd International Conference on. IEEE, 2016, pp. 408\u2013413. Eman A. Abdel-Ghaffar, Yujin Wu, Mohamed Daoudi, Subject-Dependent Emotion Recognition System Based on Multidimensional Electroencephalographic Signals: A Riemannian Geometry Approach. IEEE Access 10: 14993-15006 (2022)**\n\nWe have added these two papers in the reference of our revised version. Since our SPD MLRs are independent of network structures, our MLRs should be able to be applied to classify the SPD features in these two papers without any problems. We will explore this possibility in the future."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4035/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700426531126,
                "cdate": 1700426531126,
                "tmdate": 1700426531126,
                "mdate": 1700426531126,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]