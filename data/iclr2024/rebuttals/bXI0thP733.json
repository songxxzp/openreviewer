[
    {
        "title": "Mitigating backdoor attacks with generative modelling and dataset relabelling"
    },
    {
        "review": {
            "id": "4TcDljqNFt",
            "forum": "bXI0thP733",
            "replyto": "bXI0thP733",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9081/Reviewer_y1uP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9081/Reviewer_y1uP"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a sanitization-based defense against backdoor attacks.  Specifically, the authors learn generative models over class conditional feature space representations.  These learned distributions are then used to filter suspect training data, with the final model trained over the sanitized dataset."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Machine learning models are brittle, and as models are deployed in settings critical to human well-being, model failures can lead to real-world harm. This paper proposes a simple, intuitive method to improve model robustness. The paper follows the classic paradigm of using generative models to improve discriminative models' performance (i.e., robustness)."
                },
                "weaknesses": {
                    "value": "Like all other empirical defenses, the authors' method comes with no guarantees on its effectiveness.  In my view, empirical defense papers must meet two necessary conditions to be fit for publication.  Unfortunately, this paper does not meet either.\n1. The paper should explicitly note that their method comes with no guarantees and contrast this weakness against the plethora of papers (e.g., [1]) that provide methods certifiably robust to training data attacks.\n2. The paper should ideally evaluate against an adaptive attacker who is aware of their defense and actively tries to avoid it.  At the very minimum, the paper should include a convincing discussion of why an adaptive attacker is not feasible or reasonable.\n\nThe authors define $D^y_{F}=\\\\{f_{\\theta_F}(x_i) \\vert y_i = y \\\\}$. As I understand it, $D^y_F$ contains the set of features for all training instances whose **true label** is $y$.  (Note at the bottom of page 3, I believe $y_i$ is defined as the true labels).  Of course, $y_i$ for the poisoned data is unknown (otherwise, the problem is trivial).  I cannot determine whether there is a problem with the notation or method, but it seems ${D}_F^y$ is not known as defined.  Perhaps the authors are assuming a clean validation set to learn these generative models (as in other work), but I do not see a discussion of that.  This is a major concern and one reason I rate the soundness as 1.\n\nIn the \"Questions\" section below, I detail a concern about overstating the paper's novel contributions.  I will wait for the authors' response before categorically defining it as a weakness.\n\nI **strongly** recommend either removing or redesigning Figure 2. The flow of the figure is very non-linear and non-intuitive. Best I can determine, the figure could have a linear progression starting at the initial poisoned dataset and terminating with the final trained model. Perhaps the authors chose this non-linear progression to save space, but I would view this as an especially poor choice.\n* One potential way to solve this problem entirely is to change Figure 2 to an algorithm.\n\nSeveral typographical issues exist in the paper.  I provide a non-exhaustive list below. These did not affect my overall score.\n* Page 1: \"...stealthines...\"\n* The authors repeatedly use `\\citet{...}` in place of `\\citep{...}`. See for example the two citations on page 1 in the paragraph that begins \"In this work, ...\".\n* Page 7: \"...fllowing...:\n* Page 7: \", Sample specific\" (\"S\" should not be capitalized here)\n* Page 7: \"sample0specific\"\n* Page 13: In multiple places, the letter \"x\" is used in math mode when specifying dimensions resulting in the \"x\" being italics.  Either do not place the x in math mode or better use `\\times` instead of \"x\".\n\nTable 4 should show the minimum poisoning rate where either the attacks or the defenses start to fail.  For example, does your defense still work at 0.1% poisoning rate.\n* Poisoning rate is also only one dimension of an attack's strength. Perturbation strength is an orthogonal dimension of attack strength against which the authors' defense is surely highly susceptible but is not explored in the empirical evaluation.\n\nThe empirical evaluation's main results use either a 10% or 25% poisoning rate.  In my view, those attack rates are wholly unrealistic for any marginally plausible real-world scenario.  I would go so far as to consider those poisoning rates not meaningful to study since I cannot see a case where an attacker is inserting 25% poisoned data.\n\nThe proposed method is studied only in the vision context. Other modalities are not explored or discussed.\n\n### References\n\n[1] Levine et al. \"Deep Partition Aggregation: Provable Defenses Against General Poisoning Attacks\" ICLR'20201."
                },
                "questions": {
                    "value": "On page 2, you summarize the paper's second contribution writing, \"*We propose the first backdoor defense based on generative modeling*.\"  This is a very broad claim that I suspect is not true. For example, [1] uses backdoor modeling for a backdoor defense back at NeurIPS 2019.   Please speak more to the basis of this claim.\n\nAt the beginning of Section 3.1, the authors write, \"*We consider backdoor defenses that avoid standard supervised learning due to its sensitivity to poisoned labels and susceptibility to overfitting*.\" When I encountered this sentence when reading through the paper the first time, I did not understand what the authors meant, and after completing the paper, I am not sure I understood it.\n\n### References\n\n[1] Qiao et al. \"Defending Neural Backdoors via Generative Distribution Modeling\" NeurIPS'2019."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This paper proposes a defense against backdoor attacks. Better defenses can always theoretically facilitate better attacks in the future, but that risk is unavoidable and in this case remote."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9081/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698384091777,
            "cdate": 1698384091777,
            "tmdate": 1699637143819,
            "mdate": 1699637143819,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bUKAwEmf6b",
                "forum": "bXI0thP733",
                "replyto": "4TcDljqNFt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9081/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9081/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Weaknesses:**\n\n>The authors define $D^y_{F}=\\{f_{\\theta_F}(x_i) \\vert y_i = y \\}$. As I understand it, $D^y_F$ contains the set of features for all training instances whose **true label** is $y$. [...]\n\nWe apologize, we have made an error in the definition of $D^y_\\text{F}$ by omitting $(x_i,y_i) \\in \\widetilde{\\mathcal D}$. \nThe original labels are not known and the definition should have been $D^y_\\text{F}=\\{f_{\\theta_F}(x_i) \\mid (x_i,y_i) \\in \\widetilde{\\mathcal D}, y_i = y \\}$.\n\n>I **strongly** recommend either removing or redesigning Figure 2. [...]\n>One potential way to solve this problem entirely is to change Figure 2 to an algorithm.\n\nWould you consider a description like this as an improvement over Figure 2?\n\n1. Train a self-supervised feature extractor $f_{\\theta_{\\text{E}}}$ on images from the potentially poisoned dataset $\\tilde D$.\n2. Learn per-class densities $p_{\\theta_{y}}(z)$ self-supervised features $z=f_{\\theta_{\\text{E}}}(x)$.\n3. Identify disruptively and non-disruptively poisoned classes.\n4. Assign poisoning scores $s_y(z)=\\frac{p_{\\theta_y}(z)}{\\arg\\max_{y'\\neq y} p_{\\theta_{y'}}(z)}$ to examples from classes $y$ identified as poisonous.\n5. Split $\\tilde D$ into subsets based on poisoning scores $s_y(z)$:\n\t* $\\hat D_\\text{P}$: poisoned examples,\n\t* $\\hat D_\\text{O}$: uncertain examples,\n\t* $\\hat D_\\text{C}$: clean examples from classes identified as targets and all examples from classes identified as clean.\n6. Produce the cleansed poisoned subset $D_\\text{P}'$ by assigning pseudo-labels to the examples from $\\hat D_\\text{P}$ according to $\\arg\\max_{y'\\neq y}p_{\\theta_{y'}}(z)$.\n7. Produce the cleansed dataset as $\\hat D' = \\hat D_C \\cup D_P'$.\n8. Train a discriminative model on $\\hat D'$.\n\n>Several typographical issues\n\nThank you!\n\n>Table 4 should show the minimum poisoning rate where either the attacks or the defenses start to fail. For example, does your defense still work at 0.1% poisoning rate.\n\nThis is a valuable suggestion. In our recent experiments,\nwe discovered that a poisoning rate of 0.1% is indeed \nsufficient to compromise our defense. The scarcity of poisoned samples results in the failure to accurately identify the target class, consequently preventing the elimination of these poisoned samples.\n\n>Poisoning rate is also only one dimension of an attack's strength. Perturbation strength is an orthogonal dimension of attack strength against which the authors' defense is surely highly susceptible but is not explored in the empirical evaluation.\n\nWe have noticed that some attacks that are non-disruptive become distruptive above some poisoning rate threshold.\nIt also seems intuitive that stronger perturbations might make the attack more likely to be disruptive because the poisoned examples then bear less resemblance to their original class. \nIn all our experiments, our defense has worked well in both cases.\n\n>The empirical evaluation's main results use either a 10% or 25% poisoning rate. In my view, those attack rates are wholly unrealistic for any marginally plausible real-world scenario. I would go so far as to consider those poisoning rates not meaningful to study since I cannot see a case where an attacker is inserting 25% poisoned data.\n\nWe apologize if it was not clear: the 25% denotes that only 25% of the poisoned class consists of poisoned examples when the clean-label attack (LC) is used.\nWe will run additional experiments with the clean-label attacks suggested by reviewer U1ob.\n\nWe have considered attacks that poison 10% examples in order to be in line with previous work [1, 2].\nDo you have any suggestions about what to read to get a better idea of the attacker capabilities in important real-world scenarios?\n\n**Questions:**\n\n>At the beginning of Section 3.1, the authors write, \"_We consider backdoor defenses that avoid standard supervised learning due to its sensitivity to poisoned labels and susceptibility to overfitting_.\" When I encountered this sentence when reading through the paper the first time, I did not understand what the authors meant, and after completing the paper, I am not sure I understood it.\n\nWe aim for our defense to rely on self-supervised training, without labels. \nThis approach is preferred because it is less likely to treat the trigger as a significant feature. \nIn contrast, supervised training is likely to consider the trigger as the primary feature for identifying the label of poisoned examples.\nWe will make this more clear in future revisions of our work.\n\nPlease find the answers to your other comments in our response to all reviewers.\n\n[1] Li et al. - Neural Attention Distillation, https://arxiv.org/abs/2101.05930  \n[2] Li et al. - Anti-Backdoor Learning, https://arxiv.org/abs/2110.11571"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9081/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574481178,
                "cdate": 1700574481178,
                "tmdate": 1700574506374,
                "mdate": 1700574506374,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WdaL9c9tV9",
            "forum": "bXI0thP733",
            "replyto": "bXI0thP733",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9081/Reviewer_U1ob"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9081/Reviewer_U1ob"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel method to detect and mitigate data poisoning attacks by means of per-class generative modeling. Instead of training the generative models on image space, which is claimed ineffective, the paper proposes to model the latent embeddings extracted by a self-supervised-learning feature extractor, using per-class normalizing flows. Then, it detects backdoor classes based on the average log-density over all foreign examples. Next, it computes a poisoning score to split samples of the identified target classes into clean, poisoned, and uncertain sets. Finally, the samples in the poisoned set are relabeled and combined with the clean set to produce a cleansed dataset for training. The method effectively mitigates common dirty-label attacks and one clean-label method on CIFAR-10, ImageNet, and VGGFace2."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper explores a new approach for poisoning attack mitigation using generative modeling. While training the generative models on image space is ineffective, the paper proposes to use model the latent embeddings extracted by a self-supervised-learning feature extractor.\n- The method effectively mitigates common dirty-label attacks and one clean-label method on CIFAR-10, ImageNet, and VGGFace2."
                },
                "weaknesses": {
                    "value": "- The last two sentences in Section 4.5 are confusing. Although the numerator is extremely low, why is the score significantly higher? Also, if the poisoned samples score significantly higher than clean samples, aren't those poisoned samples mislabeled into the clean set? Finally, the arguments may be invalid by ignoring the change of the denominator.\n\n- The paper experiments with only one clean-label attack, which injects adversarial noises into the poisoned data. Hence, it assumes that with clean-label attacks, the poisoned samples are completely distinct from the rest of the dataset in the self-supervised feature\nspace. That assumption may be incorrect with other clean-label attacks such as SIG [1] and Refool [2].\n\n- The paper should discuss some adaptive attacks. For instance, the attacker can tune the backdoor trigger to fool a surrogate SimCLR model trained on clean data.\n\n- The proposed method depends on too many hyper-parameters (\\alpha, \\lambda, \\beta_{ND}, \\beta_D). I cannot see how the selected values of these hyper-parameters are general and can work for all scenarios. The authors should ablate the choice of these hyper-parameters, particularly under different poisoning rates.\n\n- \\alpha is set as 0.15. \n  - First, it means 70% of samples of the identified target class are uncertain and will be removed, which is a lot. It will weaken the cleansed dataset significantly, particularly when multiple (or all) classes are poisoned.\n  - Also, all samples in the D_p are relabeled to a different class (Eq. 7), which is problematic if the percentage of the poisoned examples is less than 15% of the number of samples in the target class. For instance, in the case of CIFAR-10 with a 1% poisoning rate, the poisoned examples cover less than 9% of the samples in the target class, meaning more than 6% of the clean images in the target class are relabeled wrongly.\n  - 15% of the samples in the target class are set as clean. It becomes problematic if the poisoned examples cover more than 85% of the samples of the target class. That situation can happen when the number of classes is high, e.g., a classification task with 100 classes and a poisoning rate of 10%.\n\n- How does the algorithm behave in case the dataset is clean? And how does it behave under all2all attacks?\n\n[1]. Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set corruption without label poisoning. In ICIP 2019.\n[2]. Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection backdoor: A natural backdoor attack on deep neural networks. In ECCV 2020."
                },
                "questions": {
                    "value": "- The last two sentences in Section 4.5 are confusing. Although the numerator is extremely low, why is the score significantly higher? Also, if the poisoned samples score significantly higher than clean samples, aren't those poisoned samples mislabeled into the clean set? Finally, the arguments may be invalid by ignoring the change of the denominator.\n- The authors should run the analysis in Fig. 1 and the experiments in Table 1 using SIG and Refool attacks?\n- The authors should define and examine some potential adaptive attacks?\n- The authors should ablate the choice of the hyper-parameters (\\alpha, \\lambda, \\beta_{ND}, \\beta_D), particularly under different poisoning rates.\n- A more in-depth discussion on the impact of the value choice for \\alpha.\n- How does the algorithm behave in case the dataset is clean? And how does it behave under all2all attacks?\n\n-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9081/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698641658315,
            "cdate": 1698641658315,
            "tmdate": 1699637143712,
            "mdate": 1699637143712,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A5hq7seCDu",
                "forum": "bXI0thP733",
                "replyto": "WdaL9c9tV9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9081/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9081/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback!\n\n>The last two sentences in Section 4.5 are confusing. Although the numerator is extremely low, why is the score significantly higher? Also, if the poisoned samples score significantly higher than clean samples, aren't those poisoned samples mislabeled into the clean set? Finally, the arguments may be invalid by ignoring the change of the denominator.\n\nWe apologize. The penultimate sentence should have said *denominator* instead of *numerator*. \nThe numerator will be higher because the generative model of the poisoned class is trained on the poisoned examples.\n\nIf the above explanation does not resolve your doubts, please clarify \"the change of the denominator\".\n\nPlease find the answers to your other comments in our response to all reviewers."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9081/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574469400,
                "cdate": 1700574469400,
                "tmdate": 1700574469400,
                "mdate": 1700574469400,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "t7Z9N8zXf1",
            "forum": "bXI0thP733",
            "replyto": "bXI0thP733",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9081/Reviewer_gUtc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9081/Reviewer_gUtc"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method for robust training of neural network classifiers against backdoor data poisonings. In short, backdoor attacks aim to create hidden associations between a trigger and a target class by poisoning a small portion of the training data. This can be done via attaching small triggers to the image and optionally changing the labels associated with each image. If the label is changed, the attack is called poisoned-label attacks, while attacks that do not change the labels are called clean label attacks.\n\nIn this paper, the authors proposes a three stage process for purifying the poisoned dataset and training a robust model that is free of backdoors. In the first step, they use a self-supervised method such as SimCLR to train a feature extractor on the poisoned dataset. Using this feature extractor, they then get the feature representations of all the training samples. In the second step, they train a generative model (here normalizing flows) for each class representation. Using these normalizing flows, they then defined a likelihood-based score function to identify poisonous samples from clean ones. This step is motivated by earlier observations on the feature space representation of backdoor attacks using self-supervised models. In particular, for samples that are likely to be poisoned-label attacks, the proposed method can identify the target class and correctify their labels. Some samples are also removed from the training dataset if they do not belong to any of the previous categories. Once this step is done, a neural network is finally trained over the purified dataset.\n\nExperimental results over CIFAR-10, ImageNet-30, and VGGFace indicate the effectiveness of the proposed method against poisoned-label and clean-label backdoor attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method is novel and interesting. It is based on an empirical observation around the feature space representation of poisoned data in the feature space of models trained with self-supervised learning. The use of normalizing flows to model the per-class latent space distribution is also novel.\n\n- Empirical results indicate that this three stage method can mitigate the effect of backdoor attacks. Even more interestingly, it can revive the poisoned-label samples and re-use them in the training process."
                },
                "weaknesses": {
                    "value": "- Even though the proposed method is working well, it is highly inefficient and requires lots of compute. In particular, the proposed method starts with self-supervised pre-training of a feature extractor using SimCLR. Then, it trains _one_ normalizing flow _per each class_ to finally be able to get rid of poisonous samples and start training a robust model. Such extensive use of resources is quite intensive, and frankly speaking, might be redundant. The field of backdoor defense has came up with alternative solutions such as [1-4] that are far more efficient than the proposed solution where some of them just take one training round to give robust models. Apart from the expensive self-supervised training at the beginning, the proposed solution requires one flow-based model per class which means that its resources grows linearly with the number of classes. \n\n- In lieu of the previous issue, first the paper needs to include more recent baselines [1-3], and second, it is important to include the total training time (from start to delivering a robust model) for all of the methods. This way, the readers can have a better understanding of the computational efficiency of current methods.\n\n- There are certain parts in the paper that might cause confusion. For instance, the explanations given in Sections 4.5-4.6 are seem contrasting. On the one hand, the paper says that for clean samples the score $s\\_{y}(\\boldsymbol{z})$ is higher. On the other hand, the same score is also higher for disruptive poisoning. Figure 4 also shows the same trend for both the clean samples as well as disruptive attacks. I think that these two sections should be re-written (see below for questions), because currently it seems that some of the clean samples can also initially be removed by this method. If this is the case, it should be explained. Optionally, adding a diagram of step-by-step poisoned sample removal might also be helpful.\n\n[1] Liu, Min, et al. \"Beating Backdoor Attack at Its Own Game.\" _ICCV_, 2023.\n\n[2] Huang, Hanxun, et al. \"Distilling Cognitive Backdoor Patterns within an Image.\" _ICLR_, 2023.\n\n[3] Dolatabadi, Hadi, et al. \"Collider: A robust training framework for backdoor data.\" _ACCV_, 2022.\n\n[4] Hayase, Jonathan, et al. \"Spectre: Defending against backdoor attacks using robust statistics.\" _ICML_, 2021."
                },
                "questions": {
                    "value": "- Claiming that this work is \"the first backdoor defense based on generative modelling\" is inaccurate. For one, MESA [5] has also used generative modelling as a solution to neural backdoors.\n\n- Why do we need to identify/remove poisonous samples using class conditional normalizing flows and then train another neural network of our task? In other words, can't we just use the per-class normalizing flow for classification as well? Running experiments on this scenario is highly encouraged.\n\n- Can you repeat the same process for generating Figure 1 for other attacks?\n\n- Based on the Figure 1 (left), the proposed method heavily relies on the fact that the poisoned samples in the target class are scarce. What happens if the number of poisoned samples (those with triggers) that use the same trigger are abundant? Experiments on this scenario is highly encouraged.\n\n- Section 4.5 and 4.6 are rather confusing. Can you please elaborate on the filtration procedure? The paper currently says that \"We include the samples with $\\alpha$ highest poisoning scores in $\\hat{\\mathcal{D}}\\_{\\mathrm{P}}$, and include the samples with the $\\alpha$ lowest poisoning scores together with samples from identified clean classes in $\\hat{\\mathcal{D}}\\_{\\mathrm{C}}$.\" Do these two steps done on the same score graph? Does this mean that some of the clean samples are also removed? Potentially, is this the reason for the under-performance of the proposed method in the case of high poison rate (Table 4)?\n\n- What experimental settings (number of epochs, etc.) are used for SimCLR? What is the architecture of normalizing flows?\n\n- As mentioned above, add the mentioned baselines and report the total training time for all of the methods to see the computational efficiency.\n\n- Why so many number of epochs (200) is used for training models? Usually, 120 epochs is enough to train ResNet models with SGD on CIFAR-10.\n \n[5] Qiao, Ximing, Yukun Yang, and Hai Li. \"Defending neural backdoors via generative distribution modeling.\" _NeurIPS_, 2019."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9081/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698893478462,
            "cdate": 1698893478462,
            "tmdate": 1699637143617,
            "mdate": 1699637143617,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "34NsGR7IhK",
                "forum": "bXI0thP733",
                "replyto": "t7Z9N8zXf1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9081/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9081/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback!\n\n**Weaknesses**\n\nIn case of a dataset with input size $224\\times224$, 10 classes and ~5000 example per class, \nself-supervised representation learning takes 10000 s,\nsupervised training (200 epochs) takes 2750 s, \nwhile the training of generative classifiers takes about 90 s per class regardless of image size. \nWe observe that our generative setups involve a neglectable processing overhead. \nMoreover, this can be further improved by replacing C per-class models with 1 model of joint density $p[z, y]$. \nSelf-supervised learning indeed is computationally expensive, however that seems as an acceptable price for a substantial reduction of vulnerability to cybernetic attacks.\n\nWe will try to address the last point by improving the explanations.\n\n**Questions**\n\n>Why do we need to identify/remove poisonous samples using class conditional normalizing flows and then train another neural network of our task? In other words, can't we just use the per-class normalizing flow for classification as well? Running experiments on this scenario is highly encouraged.\n\nWe agree that it would make sense to test the normalizing flows as the final classifier or test such a classifier re-trained on the cleansed dataset.\nGiven our assumptions, we expect the following:\n- In case of non-disruptive attacks, we expect such a classifier to correctly classify some, but still be ambiguous and misclassify many of the poisoned examples with respect to the original class. In cases when it is ambiguous on two classes, those classes might often be the original (poisoned) class and the target class.\n- In case of disruptive attacks, we would expect such a classifier to often classify the poisoned examples into the target class because they are not similar to their original class in the self-supervised feature space.\nWe will test this empirically.\n\nMost practical setups will prefer to re-train the discriminative model on cleansed data since i) discriminative models perform better on discriminative tasks. \nNote that the resulting computational overhead is not large compared to feature extraction, and that it would be an acceptable price for the delivered cleansing performance. Recent comparison of generative and discriminative classification performance can be found in [1].\n\n\n>Can you repeat the same process for generating Figure 1 for other attacks?\n\nYes. We will generate Figure 1 for other attacks and include it in the future revisions of our work.\n\n>What experimental settings (number of epochs, etc.) are used for SimCLR? What is the architecture of normalizing flows?\n\nWe perform SimCLR training for 100 epochs with a batch size of 256 and \nAdam optimizer with \na fixed learning rate of $3\\cdot10^{-4}$. \nOur per-class normalizing flow consists of two steps which consist of actnorm [4] and an affine coupling [5].\nEach coupling module computes the affine parameters with a pair of fully-connected layers with ReLU between them.\nWe shall include these pieces of information in the next revision of our work.\n\n>Why so many number of epochs (200) is used for training models? Usually, 120 epochs is enough to train ResNet models with SGD on CIFAR-10.\n\nWe were following the training configurations from previous work [2, 3].\n\nPlease find the answers to your other comments in our response to all reviewers.\n\n[1] Mackowiak et al. - Generative Classifiers as a Basis for Trustworthy Image Classification, https://arxiv.org/abs/2007.15036  \n[2] Huang et al. - Backdoor Defense via Decoupling the Training Process, https://arxiv.org/abs/2202.03423  \n[3] Gao et al. - Backdoor Defense via Adaptively Splitting Poisoned Dataset, https://arxiv.org/abs/2303.12993  \n[4] Kingma et al. - Glow: Generative Flow with Invertible 1x1 Convolutions, https://arxiv.org/abs/1807.03039  \n[5] Dinh et al. - Density estimation using Real NVP, https://arxiv.org/abs/1605.08803"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9081/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574449913,
                "cdate": 1700574449913,
                "tmdate": 1700574559082,
                "mdate": 1700574559082,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5eej93JkwT",
                "forum": "bXI0thP733",
                "replyto": "34NsGR7IhK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9081/Reviewer_gUtc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9081/Reviewer_gUtc"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors"
                    },
                    "comment": {
                        "value": "I want to thank the authors for their response. Unfortunately, the authors' response did not address my concerns that I mentioned in my initial review. In particular, additional comparisons with newer existing baselines in terms of both the attack success rate as well as total wall-time is required to demonstrate whether the proposed method is making an efficient use of resources or not. Also, as evident in Table 1, the proposed method is getting rid of some clean samples and the benign accuracy (ACC) is usually lower than that of other methods. To evade this, the paper has use the notion of (ACC-ASR) to determine which method is most successful, but in my view, both ACC and ASR should be considered separately. The issue of filtering clean data might get even worse as we increase the filtration threshold which in turn can hurt the clean accuracy.\n\nOverall, I believe that my initial assessment is a fair evaluation of this paper. That being said, I am open to discussion, especially from my peers."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9081/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718625076,
                "cdate": 1700718625076,
                "tmdate": 1700718625076,
                "mdate": 1700718625076,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qZplFIaMi3",
            "forum": "bXI0thP733",
            "replyto": "bXI0thP733",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9081/Reviewer_rmHx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9081/Reviewer_rmHx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to model the per-class distribution with a generative model and uses it to sanitise the data against backdoors. The approach operates in a latent dimension as opposed to the input space. Under the assumption and empirical observation (Figure 3, Figure 4) that the poisoned samples will exhibit different density scores for their target classes, they use a threshold to identify the poisoned samples in two scenarios"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper builds on an empirical observation that comparing per-class densities over extracted features can reveal poisoning behaviour. \n\n- The ablation study shows that choice of feature representation doesn\u2019t significantly vary the performance with two models resulting in marginal differences in attack success rate and accuracy"
                },
                "weaknesses": {
                    "value": "- I think the paper can improve with investigation of what makes a generative model stand out? Perhaps an investigation of how adaptive attacks can circumvent the threshold based detector?\n\n- Similarly, the paper can also benefit from investigation of the choice of threshold \\beta_ND and \\beta_D. \n\n- Figure 1 and its legend are a bit small to read"
                },
                "questions": {
                    "value": "- How sensitive is this method with respect to choice of thresholds?\n\n- It is unclear what the limits of this approach are? Are there scenarios where this detector will fail to detect?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9081/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699331933145,
            "cdate": 1699331933145,
            "tmdate": 1699637143497,
            "mdate": 1699637143497,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9hpaU75rCL",
                "forum": "bXI0thP733",
                "replyto": "qZplFIaMi3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9081/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9081/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback!\n\n>I think the paper can improve with investigation of what makes a generative model stand out?\n\nIf we understand correctly, we agree that we should test how important the conditional generative model $p[z\\mid y]$ is by comparing its feature densities with class probabilities of a suitable discriminative model $p[y\\mid z]$.\n\nPlease find the answers to your other comments in our response to all reviewers."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9081/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574432787,
                "cdate": 1700574432787,
                "tmdate": 1700574432787,
                "mdate": 1700574432787,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]