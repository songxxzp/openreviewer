[
    {
        "title": "Denoising Diffusion Step-aware Models"
    },
    {
        "review": {
            "id": "mEC6SDSO0M",
            "forum": "c43FGk8Pcg",
            "replyto": "c43FGk8Pcg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission535/Reviewer_buDR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission535/Reviewer_buDR"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel approach to accelerating sampling from denoising diffusion models without sacrificing image quality. This approach is based on the idea that not every generation step is equally difficult nor equally important for the quality of the final image. The authors propose using *slimmable networks* to prune the network weights at less-important generation steps, and combine this with an evolutionary search method to identify which steps can be safely pruned without sacrificing image quality. The authors demonstrate that their method reduces the FLOPs required for multiple datasets while achieving comparable or better sample quality (FID) than the unpruned methods, and further show that it can be combined with the adaptive step-skipping of DDIM or with latent diffusion for further improvements. They also visualize the learned pruning schedules for different datasets, and show that the set of safely-prunable generation steps differs depending on dataset structure."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**[S1]** The idea of pruning the network adaptively based on the difficult of each generation step is insightful, and well-motivated by the introduction and the pilot study in section 3.1. This approach also seems quite novel. I'm not aware of any previous work that has considered adaptively using different network sizes at different timesteps.\n\n**[S2]** The authors show that their approach is actually complementary to existing acceleration methods like DDIM, and yields additional improvements when combined. I expect that the approach could also be combined with some of the more recent ODE-based acceleration methods like DPM-Solver, since the proposed DDSM technique is agnostic to the exact mathematical sampling process and instead adaptively shrinks the network itself.\n\n**[S3]** The empirical results are quite strong across multiple datasets. The authors show significant cost savings relative to sampling from the full model, and actually show slight *improvements* in sample quality for all but one of the datasets (LSUN-bedroom takes a small hit).\n\n**[S4]** The approach is easy to understand and seems like it would not be too difficult to implement. The authors also plan to release their code.\n\n**[S5]** I found the results of the pruning schedule search to be quite insightful. Different datasets show different amounts of pruning, and this pruning occurs at different times, which seems to match whether the dataset has more information in its high-frequency or low-frequency components."
                },
                "weaknesses": {
                    "value": "**[W1]** The approach relies on \"slimmable networks\" and \"slimmable counterparts\" of standard convolution and normalization. However, these aren't discussed in much detail. I think the paper could benefit from some additional background on what slimmable networks are and how they work, and more details on the particular slimmable network architecture used for this work. (In particular, how many slimmable switches were used, and how do they fit into the U-Net architecture?)\n\n**[W2]** The objective for the evolutionary search algorithm is also presented at a fairly high level and could use more details. The authors mention using NSGA-II to \"balance conflicting objectives\", but it's not clear to me what objectives were used. Algorithm 2 suggests that the search objective was a linear combination of FID and FLOPS, but the details of the linear combination weights are not specified.\n\n**[W3]** Although the contributions of this work do seem orthogonal to some of the more recent work in accelerating diffusion model sampling, I think the experimental results would be more impressive if they could also be demonstrated for these more recent accelerated sampling approaches like DPM-Solver, [DPM-Solver++](https://arxiv.org/abs/2211.01095) or the recent [UniPC](https://arxiv.org/abs/2302.04867). My guess is that the methods could be combined, but it would be useful to see how much FLOPs can be saved when combined with these more recent samplers."
                },
                "questions": {
                    "value": "Could you provide more details on the slimmable network architecture and on the configuration of the search algorithm, as I discuss in [W1] and [W2]? For the search algorithm, what was the value of $w_M$ used, and how critical is this choice? (Or, are the FID and FLOPs measurements automatically balanced by NGSA-II somehow?)\n\nFigure 4a and Figure A are quite interesting, but they also look somewhat blurry. I can sort of see some fuzzy \"bands\" at different points of the trajectory; are these actual changes in the slimmed model size or are these some sort of compression artifact in the figure image? Also, what is the resolution of the X axis here, e.g. where are the boundaries between different steps? It's hard to understand exactly what the plot is showing, and I think this plot would be more readable if it were presented as a line graph (instead of as a heatmap), since its only 1 dimensional.\n\nI found some very recent related work [\"Structural Pruning for Diffusion Models\" (Fang et al. 2023)](https://arxiv.org/abs/2305.10924) which was just accepted at NeurIPS. That work also considers pruning in order to speed up diffusion model inference, although I believe they only consider pruning the entire network rather than adaptively pruning at different timesteps. It might make sense to discuss this related work in your paper, and I'd also be curious how your approach compares to theirs in terms of FLOPs savings.\n\nHave you explored combining your DDSM approach with some of the more recent accelerated sampling approaches? And do you still observe runtime improvements for those methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission535/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697554370151,
            "cdate": 1697554370151,
            "tmdate": 1699635980759,
            "mdate": 1699635980759,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3ZJrHeiOPC",
                "forum": "c43FGk8Pcg",
                "replyto": "mEC6SDSO0M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission535/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission535/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Background of slimmable networks**\n\nOur implementation of slimmable networks draws inspiration from the US-Net [15]. These networks are characterized by their ability to operate at various widths, providing a flexible and universal solution for network scalability. During the training of slimmable networks, we focus on optimizing the smallest, largest, and a selection of randomly sampled middle-sized sub-networks. This approach implicitly enhances the performance of all potential networks within the supernet.\n\nIn practical terms, our slimmable UNet adheres to the structure of ADM, but with a significant modification: all convolution layers are replaced by slimmable convolutions. These specialized convolutions are capable of adaptively processing tensors with a varying number of input channels. To accommodate a broader range of sub-networks, we adjusted the group number in the group normalization layer from 32 to 16. Additionally, we chose to omit the Batch Normalization (BN) calibration stage, as proposed in [11], since our diffusion UNet exclusively utilizes GroupNorm. Regarding the sizes of the sub-networks, we offer seven different options, corresponding to 2/8, 3/8, 4/8, 5/8, 6/8, 7/8, and 8/8 of the original ADM's width. This results in a total strategy space of 7^(num_timesteps).\n\n**Details of the search algorithms**\n\nFor the search parameters, the process encompasses a total of 10 iterations, with each iteration involving a population of 50, and maintaining a mutation rate of 0.001. The initial generation crucially includes a mix of uniform non-step-aware strategies and some random strategies. This specific approach to initialization and mutation has been empirically found to facilitate easier convergence of the search algorithm. Furthermore, a weight parameter is incorporated, which multiplies the GFLOPs to strike a balance between image quality and computational efficiency. For CIFAR-10, we set the weight parameter to 0.1 to favor higher image quality, while for CelebA, the FLOPs weight parameter is adjusted to 0.25. These parameters were manually selected to ensure that there are no compromises in generation quality.\n\n**Elaboration on the search result**\n\nFor visual propose, I smoothed the color binds to present a general trend instead of the details. The fuzzy area are some mutation points of the search result. They are actual changes in model size. I think your suggestion of drawing line graph is very useful. Therefore, I plot them in our modified version in the **Appendix Section D**.\n\n**Discussion on \u201cStructural Pruning for Diffusion Models\u201d**\n\nPlease see the comment to all reviewers.\n\n**Compatible with recent fast solvers**\n\nPlease see the comment to all reviewers.\n\n\nFinally, I sincerely appreciate the valuable insights provided by the reviewers and warmly welcome any further criticisms or suggestions. Your timely feedback is crucial for the enhancement of this paper. I look forward to your valuable comments.\n\n\n[15] Universally Slimmable Networks and Improved Training Techniques, Yu et al.."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission535/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362264785,
                "cdate": 1700362264785,
                "tmdate": 1700362264785,
                "mdate": 1700362264785,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wa1l1es1Pu",
                "forum": "c43FGk8Pcg",
                "replyto": "3ZJrHeiOPC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission535/Reviewer_buDR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission535/Reviewer_buDR"
                ],
                "content": {
                    "title": {
                        "value": "Discussion"
                    },
                    "comment": {
                        "value": "**Details on slimmable networks and search algorithms:** Thanks for the additional details. I think you should also add these details to the paper so that your results can be reproduced, it doesn't look like they are currently present.\n\n**Visualization of search results:** I see. I don't think you should smooth the color bands in Figure 4, since that does not accurately represent what your method does. The plots in the appendix are much more readable and I would suggest replacing the color band visualizations with these line plots in the main paper.\n\nI am surprised that your search results have such sudden jumps back and forth between sizes. It seems unlikely to me that this represents the best schedule. How much variability did you observe between the search results over different search attempts?\n\n**Diff-Pruning / EDM results:** Thanks for providing these new results. It doesn't look like these results have actually been added to the paper yet, are you planning to add them?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission535/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700449660593,
                "cdate": 1700449660593,
                "tmdate": 1700449660593,
                "mdate": 1700449660593,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SOwpqNKRB3",
            "forum": "c43FGk8Pcg",
            "replyto": "c43FGk8Pcg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission535/Reviewer_X3mN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission535/Reviewer_X3mN"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Denoising Diffusion Step-aware Models (DDSM) to improve efficiency of denoising diffusion probabilistic models (DDPMs) for image generation. Previously, DDPMs require compute-intensive iterative sampling, using the full model each step. DDSM hypothesizes different steps have varying importance, and uses a spectrum of networks with adapted sizes for each step, determined via evolutionary search. This avoids redundant computation in less critical steps.\nDDSM integrates with slimmable networks - trained simultaneously on sub-networks to enable execution at arbitrary sizes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "DDSM accelerates diffusion models by avoiding uniform computation for all steps. The step-aware network design is shown to be efficient and effective across datasets."
                },
                "weaknesses": {
                    "value": "The major concern is the pathway of denoising acceleration.  It's evident that the prevalent approach to enhancing diffusion speed, in the context of network compression, hinges on the application of post-training quantization techniques. These techniques enable the compression of neural networks in a manner that circumvents additional training [1,2]. However, I observe that your method necessitates training and, notably, falls short in performance when compared to methods that forego training. To illustrate, empirical assessments show that DDSM facilitates computational reductions of 49% for CIFAR-10 and 76% for ImageNet. In contrast, the techniques in [1,2] manage to achieve 4 or 8-bit quantization (surpassing DDSM in speed) without compromising the FID score. It's crucial to underscore that these methods achieve this efficiency entirely without the need for further training. Consequently, the approach employed by DDSM for accelerating denoising doesn't appear to be robust enough.\n\n\n[1] PTQ4DM, CVPR 2023\n\n[2] Q-dfiifusion, ICCV 2023"
                },
                "questions": {
                    "value": "Refer to weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission535/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission535/Reviewer_X3mN",
                        "ICLR.cc/2024/Conference/Submission535/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission535/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697729123809,
            "cdate": 1697729123809,
            "tmdate": 1700715418553,
            "mdate": 1700715418553,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nd3raFwbBY",
                "forum": "c43FGk8Pcg",
                "replyto": "SOwpqNKRB3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission535/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission535/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Model Compression Techniques: Pruning and Quantization**\n\nThank you for your insightful comments and constructive criticism regarding our paper. We appreciate the opportunity to clarify and further elaborate on our work, especially in relation to the concerns raised about the pathway of denoising acceleration and the comparison with post-training quantization techniques. In the field of model compression, discerning the roles of model pruning and weight quantization is crucial, as these are two distinct yet synergistic approaches to accelerating model performance. Model pruning is primarily concerned with eliminating superfluous weights in a network. Conversely, quantization involves converting the data type of weights and activations into lower precision formats, such as int8 or int4, to reduce computational demands.\n\nAlthough these methods are individually powerful, their combined application, typically applied sequentially on a pretrained model, can further enhance the performance. For comprehensive insights, [10] presents a detailed survey on the collaborative use of these techniques for optimal model efficiency. Additionally, [11] and [12] showcase practical implementations where both weight pruning and quantization are simultaneously applied to a model.\n\nIt's noteworthy that pruning typically necessitates additional training or fine-tuning, as opposed to quantization which generally only requires calibration. However, the unique contributions of pruning, due to its orthogonal nature to quantization, should not be underestimated.\n\n**Distinctive Aspects of DDSM and Quantization-Based Methods**\n\nWe would like to emphasize the unique aspects of DDSM when compared to the quantization-based methods mentioned, such as PTQ4DM [13] and Q-diffusion [14]. The key point of our approach is the step-awareness in diffusion models. Unlike these methods, which apply a uniform model across all steps, DDSM tailors a spectrum of network sizes adapted for each diffusion step. This is a significant departure from PTQ4DM and Q-diffusion\u2019s approach, which leads to a uniformly quantized model for all steps. \n\nIn conclusion, we acknowledge the points raised and appreciate the comparison with other methods and we will discuss it in our modified version. We believe that DDSM contributes a unique perspective to the field of diffusion model acceleration. We also see potential for future work, possibly exploring how DDSM might be combined with quantization techniques to further enhance its performance. I sincerely appreciate the valuable insights provided by the reviewers and warmly welcome any further criticisms or suggestions. Your timely feedback is crucial for the enhancement of this paper. I look forward to your valuable comments.\n\n[10] Pruning and Quantization for Deep Neural Network Acceleration: A Survey, Liang et al., *Elsevier.*\n\n[11] PQK: Model Compression via Pruning, Quantization, and Knowledge Distillation, Kim et al., INTERSPEECH 2021.\n\n[12] Once Quantization-Aware Training: High Performance Extremely Low-Bit Architecture Search, Shen et al., ICCV2021.\n\n[13] Post-training Quantization on Diffusion Models, Shang et al., CVPR 2023.\n\n[14] Q-Diffusion: Quantizing Diffusion Models, Li et al., ICCV 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission535/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362167192,
                "cdate": 1700362167192,
                "tmdate": 1700362167192,
                "mdate": 1700362167192,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uyDdAkstdS",
                "forum": "c43FGk8Pcg",
                "replyto": "SOwpqNKRB3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission535/Reviewer_X3mN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission535/Reviewer_X3mN"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "I appreciate the authors' efforts in discussing the distinctions between quantization and pruning. However, I'd like to **address some misconceptions regarding quantization in the authors' responses**. The authors state that \"pruning typically necessitates additional training or fine-tuning, as opposed to quantization which generally only requires calibration\". This statement overlooks the complexity of quantization techniques. Specifically, quantization can be divided into two main categories: post-training quantization and quantization-aware training. The latter, contrary to the statement, does require training. For instance, the method used in \"SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds\" (presented at NeurIPS 2023, with the code and paper released in July) is a clear example of a quantization-aware training approach for speeding up diffusion models.\n\nAdditionally, the authors' claim that quantization and pruning are orthogonal techniques warrants further scrutiny. There is substantial research suggesting that due to the inherent trade-off between a network's representational capacity and its degree of over-parameterization, quantization and pruning might not be effectively employed simultaneously. This aspect of interaction between the two techniques seems to be overlooked in the current discussion.\n\nGiven these considerations, I believe that the role of quantization, particularly in diffusion acceleration tasks, should be revisited. Its simplicity and straightforward application make it a well-studied technique to consider in the context of the discussed research.\nOverall, I will not change my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission535/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546054737,
                "cdate": 1700546054737,
                "tmdate": 1700546105767,
                "mdate": 1700546105767,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h6p35Sker6",
                "forum": "c43FGk8Pcg",
                "replyto": "nMVHQgvSt7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission535/Reviewer_X3mN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission535/Reviewer_X3mN"
                ],
                "content": {
                    "title": {
                        "value": "Quantization and pruning are not simply orthogonal."
                    },
                    "comment": {
                        "value": "The pruning-quantization strategy is more complex than either pruning or quantization alone. In other words, it is almost impossible for manually tuning the pruning ratios and quantization codebooks at fine-grained levels [1]. To address this issue, recent methods adopt certain optimization techniques to optimize this problem. Thus, quantization and pruning are not simply orthogonal as the authors claimed. If one wants to realize quantization and pruning simultaneously, one needs to optimize this problem very carefully.\n\n\nOPQ: Compressing Deep Neural Networks with One-shot Pruning-Quantization, AAAI 2021"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission535/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579955585,
                "cdate": 1700579955585,
                "tmdate": 1700579955585,
                "mdate": 1700579955585,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dc92mwvKIY",
                "forum": "c43FGk8Pcg",
                "replyto": "zDrgBzEk5d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission535/Reviewer_X3mN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission535/Reviewer_X3mN"
                ],
                "content": {
                    "comment": {
                        "value": "The authors addressed most of my concerns. I raised my score to 6."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission535/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715402576,
                "cdate": 1700715402576,
                "tmdate": 1700715402576,
                "mdate": 1700715402576,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TXZNKiatUn",
            "forum": "c43FGk8Pcg",
            "replyto": "c43FGk8Pcg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission535/Reviewer_339s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission535/Reviewer_339s"
            ],
            "content": {
                "summary": {
                    "value": "This paper...\n- proposes to accelerate diffusion sampling by using diffusion models of different size at each time-step,\n- proposes to use evolutionary search to find a best step-aware strategy,\n- shows the effectiveness of the proposed approach on CIFAR-10, CelebA-HQ, and ImageNet sampling."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Paper is easy to follow.\n- Using networks of different size (slimmable network) for each-time step is an unexplored approach in diffusion acceleration.\n- DDSM shows promising performance gains on a variety of generation tasks."
                },
                "weaknesses": {
                    "value": "While the idea behind DDSM is interesting, I am inclined to give \"marginal reject\" due to weak experimental validation.\n\n- The paper lacks comparison with [1], which I think is a very relevant work.\n- The paper lacks experiments on higher resolution data. Can the authors provide results on $\\geq 128$ resolution images?\n\n[1] Structural Pruning for Diffusion Models, Fang et al., NeurIPS, 2023."
                },
                "questions": {
                    "value": "- Figure 3 is unclear. What do networks with mixed colors (blue and green) mean?\n- In Table 3, why do DDSM outperform ADM-large? Shouldn't the performance of DDSM be bounded by the performance of the largest model?\n- Is DDSM compatible with guided diffusion? Can the authors provide some demonstrations?\n- Is DDSM compatible with recent fast solvers, such as EDM [2]?\n\n[2] Elucidating the Design Space of Diffusion-Based Generative Models, Karras et al., NeurIPS, 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission535/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission535/Reviewer_339s",
                        "ICLR.cc/2024/Conference/Submission535/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission535/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698637325691,
            "cdate": 1698637325691,
            "tmdate": 1700552682444,
            "mdate": 1700552682444,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aO1HUpKvvV",
                "forum": "c43FGk8Pcg",
                "replyto": "TXZNKiatUn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission535/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission535/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Discussion on \u201cStructural Pruning for Diffusion Models\u201d**\n\nPlease see the comment to all reviewers.\n\n**Compatible with recent fast solvers**\n\nPlease see the comment to all reviewers.\n\n**Explanation of Figure 3**\n\nWe apologize for any confusion caused and have now provided a more detailed description in the caption of Figure 3 for clarity. In this figure, the color green represents the activated weights within the network, whereas blue represents the inactive weights. The varying proportions of blue and green in different networks indicate networks of various sizes. The supernet, which has all of its weights activated, is depicted entirely in green.\n\nDuring the training phase, our focus is on the supernet, which we train by optimizing its various sub-networks. This step is crucial for establishing a foundation of diverse network sizes and capabilities. In the subsequent searching stage, we creatively combine these sub-networks into different step-aware strategies. The objective here is to explore and identify the most optimal strategy, one that effectively balances performance and efficiency. \n\n**Explanation of DDSM outperforming ADM-large**\n\nIn Table 4, our DDSM model demonstrates a marginal improvement over ADM-large, reflected in a FID score enhancement of 0.161. We believe this improvement is largely due to the inherent randomness in the image generation process. To substantiate this claim, we conducted additional experiments using three distinct seeds for image regeneration. The results, detailed in the table below, reveal that the FID scores of DDSM are closely aligned with those of ADM-large, suggesting comparable performance between the two models.\n\n| Seed | ADM-Large FID | DDSM FID |\n| --- | --- | --- |\n| 1 | 3.713 | 3.552 |\n| 2 | 3.917 | 3.878 |\n| 3 | 3.332 | 3.639 |\n| Mean | 3.654 | 3.690 |\n\nDDSM's exceptional performance on ImageNet, surpassing ADM-large by a margin of 2.286, illustrates that pruned models can indeed outperform their unpruned counterparts. This is not a new concept and aligns with findings in prior research. For example, [7] improves 0.9 accuracy with 40% pruned DenseNet, [8] and [9] improves MobileNetV2 by 3.4 and 3.2 with less parameters. The success of DDSM is due to effective pruning strategies that eliminate unnecessary parameters, leading to a more efficient model focused on key features. Additionally, pruning acts as a regularizer that helps in enhancing the model's generalization ability, allowing it to perform better on diverse datasets like ImageNet.\n\nFinally, I sincerely appreciate the valuable insights provided by the reviewers and warmly welcome any further criticisms or suggestions. Your timely feedback is crucial for the enhancement of this paper. I look forward to your valuable comments.\n\n[7] \"Learning Efficient Convolutional Networks through Network Slimming,\" Liu et al., ICCV, 2017.\n\n[8] \"Once for All: Train One Network and Specialize it for Efficient Deployment,\" Cai et al., ICLR, 2020.\n\n[9] \"AutoSlim: Towards One-Shot Architecture Search for Channel Numbers\", Yu et al., NIPS workshop, 2019.\n\n**Compatibility with more diffusion settings**\n\nThe compatibility of our DDSM is rooted in our core design: adaptively allocate computational resources in a step-aware manner. This step-awareness make our method orthogonal to these advanced applications.\n\nWe recognize your concerns regarding the need for more experimental validation. To address this, we have initiated experiments with CelebA at a resolution of 128x128 and classifier-guided diffusion on CIFAR-10. These experiments are time-intensive, we are not sure whether we can obtain a result by the time of rebuttal due date. But we commit to sharing the results as soon as they are available.\n\nSo far, our work has undergone validation across five different datasets in various domains. With the addition of these newly added experiments, our aim is to potentially broaden the scope of our research\u2019s validation, hoping to encompass a wider array of applications."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission535/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700361986147,
                "cdate": 1700361986147,
                "tmdate": 1700362371379,
                "mdate": 1700362371379,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FevubV4jxg",
                "forum": "c43FGk8Pcg",
                "replyto": "aO1HUpKvvV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission535/Reviewer_339s"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission535/Reviewer_339s"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' reply. While this work has interesting practical implications, there are no theoretical contributions. So, I believe I must apply a higher standard to experimental validation, and I think $64 \\times 64$ resolution results are not sufficient for ICLR. I will keep my initial rating, and wait for $\\geq 128 \\times 128$ resolution results."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission535/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700445388480,
                "cdate": 1700445388480,
                "tmdate": 1700445388480,
                "mdate": 1700445388480,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DXhDBsF3f2",
                "forum": "c43FGk8Pcg",
                "replyto": "YDEj1fIf1y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission535/Reviewer_339s"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission535/Reviewer_339s"
                ],
                "content": {
                    "comment": {
                        "value": "I apologize for my vague remark. As this work is purely empirical, the significance of this work is decided by the practicality of DDSM. There are two reasons why I believe results on $\\geq 128 \\times 128$ resolution images are important in order to accurately gauge the practicality of DDSM.\n\n- First, compared to previous work on making diffusion models efficient such as pruning [1] / distillation [2] / better integrators [3] **which are applicable to any pre-trained diffusion models**, this paper requires **both the training of a new diffusion model and sampling strategy search through evolutionary search**. This makes this technique more expensive compared to [1,2,3], which naturally raises the question, is DDSM really worth using over works such as [1,2,3]? Specifically, it is well known that diffusion model training becomes extremely expensive (both in terms of computation and time) as image resolution increases. DDSM requires additional evolutionary search as well. Thus, for me to accurately compare the drawback of DDSM (training cost + evolutionary search cost) against the contribution of DDSM (sampling acceleration), I need additional results on $\\geq 128 \\times 128$ resolution images.\n\n- Second, previous works on efficient sampling of diffusion models [1,2,3] provide at least one result on $\\geq 128 \\times 128$ resolution data. For instance, on LSUN bedroom $256 \\times 256$, [2] achieves 5.22 FID with 2 NFE, and  [3] achieves $\\leq 3$ FID under 20 NFEs. For me to gauge the significance of this work compared to previous works [1,2,3] accurately under all settings, I need results on higher resolution data.\n\n[1] Structural Pruning for Diffusion Models, Fang et al., NeurIPS, 2023.\n\n[2] Consistency Models, Song et al., ICML, 2023.\n\n[3] DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps, Lu et al., NeurIPS, 2022."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission535/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503748273,
                "cdate": 1700503748273,
                "tmdate": 1700503748273,
                "mdate": 1700503748273,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4K8YOOdKgX",
                "forum": "c43FGk8Pcg",
                "replyto": "TXZNKiatUn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission535/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission535/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Hi. Thanks for your comments. We would like to update our empirical results on CelebA-HQ 128x128 and conditional CIFAR-10. \n\nOur experiments with CelebA-HQ at a resolution of 128x128 have demonstrated the efficacy of our DDSM with higher resolution data. In this study, we trained a new slimmable supernet for 128x128 resolution data from scratch. We then applied the strategy initially searched on the 64x64 CelebA-HQ dataset to this supernet. The results were promising: DDSM achieved a 61% reduction in FLOPs while preserving the quality of generation. \n\nSimilarly, our experiments on conditional CIFAR-10 showcased DDSM's adaptability to guided diffusion. Here, we constructed a novel slimmable supernet, incorporating class labels as conditions. During inference, we leveraged classifier guidance. By applying the strategy devised for the unconditional CIFAR-10 to this supernet, we maintained the same level of FLOPs efficiency and preserved the FID score.\n\nThese experiments underscore the robustness and versatility of our DDSM. They also show the good transferability of DDSM across different settings of the same dataset. To save computational cost, one could search on a small proxy dataset (for example, low-resolution data) and then apply the result to high-resolution data. Moreover, it's worth noting that the performance of DDSM on each dataset could potentially be further enhanced by restarting the search process individually for them.\n\n|  | CelebA-HQ 128x128 |  | conditional CIFAR-10 |  |\n| --- | --- | --- | --- | --- |\n|  | ADM | +DDSM | ADM | +DDSM |\n| FID | 7.53 | 7.71 | 2.48 | 2.52 |\n| GLOPs | 194.00 | 76.18 | 12.14 | 6.20 |"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission535/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528927056,
                "cdate": 1700528927056,
                "tmdate": 1700528979269,
                "mdate": 1700528979269,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x64POab6PQ",
                "forum": "c43FGk8Pcg",
                "replyto": "4K8YOOdKgX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission535/Reviewer_339s"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission535/Reviewer_339s"
                ],
                "content": {
                    "comment": {
                        "value": "While there is slight degradation in FID after applying DDSM, I think it's acceptable given the improved efficiency. In particular, I like how the step strategy on lower resolution data transfer well to higher resolution data. Perhaps, it is possible to warm-start the strategy search for higher resolution data from the step strategy on lower resolution data. In any case, this addresses all my concerns, and I have raised the score to marginal accept."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission535/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552667057,
                "cdate": 1700552667057,
                "tmdate": 1700552667057,
                "mdate": 1700552667057,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6AD1Pn1GHj",
            "forum": "c43FGk8Pcg",
            "replyto": "c43FGk8Pcg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission535/Reviewer_GL7V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission535/Reviewer_GL7V"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Denoising Diffusion Step-aware Models (DDSM), which utilize variable-sized neural networks for different steps of the diffusion generative process. The key insight is that diffusion steps have varying importance, so uniformly allocating computational resources is inefficient. The method trains a slimmable UNet that can be flexibly pruned to different capacities. An evolutionary search then determines the optimal per-step network size to balance efficiency and performance. Experiments demonstrate substantial computational savings on CIFAR-10, CelebA-HQ, LSUN-bedroom, AFHQ, and ImageNet versus conventional diffusion models, with minimal quality loss."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The core ideas are technically sound and offer a unique perspective on accelerating diffusion models.\n- Using evolutionary search to determine step-wise network requirements. \n- Compatibility with existing methods like DDIM."
                },
                "weaknesses": {
                    "value": "- Using a different network at different timestep has been explored before, such as in e-diff-i. \n- The compatibility claims with DDIM and latent diffusion are fairly cursory. More detailed experiments showing accelerated performance combining these methods could better showcase modularity.\n- The evolutionary search itself requires non-trivial compute resources. Analysis of the search costs and scalability could be insightful."
                },
                "questions": {
                    "value": "You claim compatibility with methods like DDIM and latent diffusion, but details are limited. Could you provide in-depth quantitative experiments demonstrating accelerated performance when combining DDSM with these existing diffusion acceleration techniques?\n\nThe search cost and scalability of the evolutionary algorithm is unclear. Could you analyze the computational requirements of the search procedure and discuss how it scales with factors like dataset size?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission535/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820193487,
            "cdate": 1698820193487,
            "tmdate": 1699635980526,
            "mdate": 1699635980526,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x66RglpaCK",
                "forum": "c43FGk8Pcg",
                "replyto": "6AD1Pn1GHj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission535/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission535/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Discussion on EDiff-I**\n\nWhile eDiff-I [6] adopts multiple expert models at each step to improve image quality, it does not enhance efficiency, as all these expert models are uniformly sized. Our method distinguishes itself by utilizing models of varying sizes for acceleration purposes. Moreover, unlike eDiff-I, which manually assigns models to each step, we introduce an evolutionary search approach. This innovation not only automates model allocation but also optimizes the process, potentially yielding more effective outcomes.\n\n[6] eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers, Balaji et al..\n\n**Details of combining DDIM and Latent Diffusion**\n\nFor the DDIM experiment, we adopted the weights from the 1000-step ADM model and applied the DDIM sampling schedule. As indicated in Table 4, our method significantly boosts DDIM's speed by 48%, 45%, 53%, and 62% for 10, 50, 100, and 1000 steps, respectively. This suggests that while additional steps enhance generation quality, they also introduce excess computational load. Our approach proves increasingly beneficial as the number of steps increases.\n\nIn our latent diffusion experiment, we employed the AutoEncoderKL of SD1.4 to transform images into latent vectors, upon which our DDSM was trained. To adapt to the reduced spatial size, we modified the U-Net downsampling from 4 to 2. Our DDSM achieved a 60% acceleration in latent diffusion methods.\n\n**Analysis of the evolutionary search**\n\nA detailed discussion of search costs can be found in **Section B** of our appendix. In deep learning, models like StableDiffusion and Imagen are usually trained once and then used repeatedly. Though these models require substantial training time, they efficiently generate a large number of images upon completion. Our DDSM aims to reduce the inference costs in diffusion models by employing a step-aware pruning strategy, achieving up to 76% faster performance. Despite DDSM incurring higher initial training and search costs (about 2-3 times the training cost of standard models, with search costs similar to training a conventional model), these are one-time expenses. The specific time costs are detailed in Table B.\n\nFinally, I sincerely appreciate the valuable insights provided by the reviewers and warmly welcome any further criticisms or suggestions. Your timely feedback is crucial for the enhancement of this paper. I look forward to your valuable comments."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission535/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700361723381,
                "cdate": 1700361723381,
                "tmdate": 1700361723381,
                "mdate": 1700361723381,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]