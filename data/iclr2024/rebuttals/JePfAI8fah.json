[
    {
        "title": "iTransformer: Inverted Transformers Are Effective for Time Series Forecasting"
    },
    {
        "review": {
            "id": "bRklD21RqZ",
            "forum": "JePfAI8fah",
            "replyto": "JePfAI8fah",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission632/Reviewer_b5RW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission632/Reviewer_b5RW"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of time series forecasting.\nThe authors propose a model that embeds univariate\nchannels as a whole and then uses attention between\nembedded channels. In experiments on several datasets\nthey show that their model outperforms current models\nand established new state of the art results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "s1. very simple idea and model.\ns2. very good results, establishing new state of the art results.\ns3. interesting additional finding that longer lookback windows\n  now are mostly beneficial (fig. 6)."
                },
                "weaknesses": {
                    "value": "w1. experiments only on selected datasets compared to some\n  major baselines.\nw2. experimental results for close baseline PatchTST varies from\n  published results.\nw3. no standard deviations reported."
                },
                "questions": {
                    "value": "The paper proposes a very simple idea, but the experiments\nto the best of my knowledge are establishing a new state of the\nart, making it an important contribution like an indepth study.\nI also liked the ablation study with growing observation horizons,\nas they now are more plausible than in the related work: longer\nobservation horizons usually pay off for your model.\n\nSome points should be discussed:\nw1. experiments only on selected datasets compared to some\n  major baselines.\n- You do not report on datasets Exchange, ILI and ETTm1, ETTm2,\n  ETTh1 and ETTh2, different from the experiments reported in TimesNet.\n  This way it is hard to see if the proposed method really outperforms\n  the baselines consistently or just on the selected datasets.\n\nw2. experimental results for close baseline PatchTST varies from\n  published results.\n- PatchTST consistently reports better results, e.g., for Electricity\n  with horizon 96 they report an MSE of 0.129, you report 0.195.\n  Where does the difference come from? \n\nw3. no standard deviations reported.\n- Standard deviations will help to assess which differences might be\n  significant and which spurious. \n\nSome minor language issues:\n- abstract, \"However, Transformer is challenged\": missing \"a\".\n- p. 2 \"irrationality\": sounds a little bit too strong to me."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission632/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698424388481,
            "cdate": 1698424388481,
            "tmdate": 1699635991054,
            "mdate": 1699635991054,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o68lv1cZ2g",
                "forum": "JePfAI8fah",
                "replyto": "bRklD21RqZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission632/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission632/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer b5RW"
                    },
                    "comment": {
                        "value": "Many thanks to Reviewer b5RW for providing thorough detailed comments.\n\n**Q1:** Robustness and completeness of the experiments.\n\nThanks for your scientific rigor. We provide the additional experiments in our revised paper:\n\n  * Report the standard deviations of iTransformer performance in $\\underline{\\text{Table 5 of the revised paper}}$ with five random seeds,  **exhibiting stable performance**. \n  * Update the main results in $\\underline{\\text{Figure 1 and Table 1 of the revised paper}}$. We report the averaged results from four subsets of ETT and PEMS. We also add the Exchange dataset. The detailed results are updated in  $\\underline{\\text{Table 9 and 10 of the revised paper}}$.\n  * Full results of the inverted Transformers on more datasets in $\\underline{\\text{Appendix E and F of the revised paper}}$ to further support the conclusions of our model analysis.\n\n**Q2:** About the different PatchTST results from its original paper.\n\nWe carefully read through the forecasting settings of the PatchTST paper and its official repo. There's a lot of difference here:\n\n  * **Enlarged lookback window**: PatchTST paper adopts tunable lookback length in $\\{336, 512\\}$, while ours adopts the unified length of $96$, following the unified long-term forecasting protocol of TimesNet.\n  * **More epochs to train**: PatchTST paper trains the PatchTST model with $100$ epochs, while we train all models with $10$ epochs.\n  * **Learning rate**: PatchTST paper adapts a carefully designed learning rate strategy.\n\n**Q3:** About the writing issues.\n\nThanks for your valuable suggestions. We conduct the following revisions of our paper:\n\n  * Abstract: \"Transformer is challenged\" -> \"Transformers are challenged\".\n  * Introduction: \"Concerning irrationality of embedding multivariate points\" -> \"Concerning the potential risks of embedding multivariate points\"."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission632/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301397765,
                "cdate": 1700301397765,
                "tmdate": 1700301397765,
                "mdate": 1700301397765,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vMNBdFMZl3",
            "forum": "JePfAI8fah",
            "replyto": "JePfAI8fah",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission632/Reviewer_HTy3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission632/Reviewer_HTy3"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, Authors propose to investigate why Transformer-based models do not seem to be as efficient as Linear-based models for Multivariate Time Series Forecasting (MTSF), while they are predominant in other AI domains. They suggest that the way Transformer is implemented for MTSF is inappropriate. To better benefit from Transformer architecture, they propose to tokenize dataset based on variate and not on timestep. It ends up modifying the input and the FFN. Their proposal also uses only an encoder compared to the vanilla Transformer architecture.\n\nAuthors conduct extensive experiments with several Linear-based and Transformer-based baselines to determine the performance of their proposal. These experiments are performed over 6 usual MTSF datasets along with 6 market-based datasets. Their proposal, iTransformer, appears to achieve best predictions for all datasets despite the selected prediction horizon."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "In my opinion, this paper will have a high impact on MTSF research and community. They back-up their proposal with extensive experiments on several datasets and compared to multiple baselines. \nThe results and ablation study are discussed even though I feel it could have gone further, but the page limitation was probably the issue."
                },
                "weaknesses": {
                    "value": "The biggest weaknesses in my opinion are first, the writing style, which sometimes is not going to the point (could be improved to better convey the idea and have greater impact) and second that Authors fail to include some important SOTA baselines and properly synthetize/discuss CD/CI/distribution shift comparing all the baselines (including the missing ones) and iTransformer."
                },
                "questions": {
                    "value": "# Paper as is\nAs far as I am concern, the paper in its current version with some proof-reading and the following revisions can be Accepted.\n * In abstract \u201cthe duties of the attention mechanism\u201d should be \u201cthe following duties of the [\u2026]\u201d to make reading smoother and avoid readers from questioning on what duties means here.\n * The following claim \u201cwith potentially unaligned timestamps\u201d need to be referenced or proven. Which datasets have unaligned timestamps, or by unaligned does Authors means that there are delays between values from variates? In other terms, are physical measurements unaligned (which is not the case in the considered dataset)? or do values that correspond to the same \u201cevent\u201d appears at different timesteps for different variates (which means that there are delays, which might be the case for Traffic, but for the others, in my opinion, Authors would need to demonstrate it. For instance, in Weather (which are data from the same weather station), it is clearly not the case).\n * In my opinion, it is more appropriate to call Electricity dataset as ECL.\n * Reproducibility\n   * Avoid confusion and precise which ETT is used. It looks like it is ETTm according to appendix (15min frequency), but is it m1 or m2? Informer results looks like ETTh though\u2026\n\n   * Also, precise which PEMS you use PEMS-Bay (occupancy ratio or speed in San Francisco Bay?) or other?\n * In my opinion, Figure 1 is focusing too much on iTransformer results. Why not having each axis of the web going from 1 (in the center) to 0? For better fairness in the visualization.\n * Figure 7 (b?) right part is difficult to understand without explanation of x and y axis, and what is the differences between Score Map and Correlations. What is important in this figure? How should reader interpret these lines. Why is it better than usual Transformer?\n * Figure 8 needs label of x axis.\n * \u201cwhich can be attributed to the extremely fluctuating series of the dataset, and the patching mechanism of PatchTST may lose focus on specific locality to handle rapid fluctuation. By contrast, our proposed method aggregating the whole series variations for series representations can better cope with this situation\u201d I would need proof for such a claim (even in appendix). First, showing this situation and second showing learning weight for iTransformer that shows it cope with the situation.\n\nProof-read is required for instance:\n * \u201cthis goal can be hardy achieved\u201d I guess there is a typo here and Authors was aiming to write hardly?\n * \u201cSoloar-Energy\u201d- > Solar Energy\n\n\n# Toward a bigger impact\n\n## Additional baselines and larger scope\nNonetheless, I feel that Authors are missing the opportunity to have an even greater impact in the MTSF community (And having my contribution score going from Good to Excellent). Indeed, the proposal is very promising and is on a very important topic, i.e., how to consider variate in MTSF. Are they channel dependent (CD) or channel independent (CI), and if CI are projection perform commonly or individually? However, in my humble opinion, despite doing a good job to present their proposal and results with ablation study and visualization, Authors fail to really position their proposal in the landscape of the above question. It is true that they compare they work to PatchTST, and the CD/CI discussion, but RLinear or RMLP (depending on the dataset) [2] appears to also beat PatchTST. And especially, RLinear with individual projection (one linear layer per variate) similarly to NLinear or DLinear performs better. In addition, RLinear or RMLP use RevIN that was proposed in [1]. The latter show that RevIN helps to handle distribution shift and could be applied to Transformer-based models such as Informer to improve their performance.\n\nTherefore, in my opinion, the paper will have a greater impact if Authors compare their results to these following baselines: revInformer, RLinear and RMLP (and so corresponding papers). And discuss the results and impact of inverse versus CI versus distribution shift handling. This extra step would really be significant for the community in order to have a better understanding of the big picture and what is happening here.\n\nIn addition, the abstract and intro emphasis that Transformer superiority is \u201cshaken\u201d by Linear-based models. However, Authors have only few of such Linear models as baselines. Therefore, adding RLinear and RMLP, especially if iTransformer can beat them, will emphasis such a claim. \n\nAuthors cited [2] in their paper, so they are aware of this work and in my opinion should have included it.\n\nFinally, for the experiments where only P% of the variates are used for training, as the variates are selected randomly, it would be good to perform the experiment with different set of random variates and plot an error plot or box plot. This will further highlight that the set selected randomly is not a specific case. For instance, in Figure 8 (a? left one), we could have for each % of variate the min, max, average among the different sets. Figure 5 could be a boxplot. In addition, Authors should make clear that the set of variates use with iTransformer is the same set used in CI transformer (Figure 5) to avoid any misunderstanding from Reader.\n\n## More results in appendix to ensure proper reproducibility\nFurthermore, in order to target greater impact on the community, I would suggest Authors to add results and visualizations for the other datasets (similar to Table2, Figure 5, and following) in appendix. This would help to make sure results showed applied to all datasets. Indeed, Authors mentioned that PEMS is more difficult so the nature/type/characteristics of the dataset may impact the results and it would be important to mention it and discuss it (even though this point might what Authors expect to do as future work by saying \u201cexplore iTransformers for extensive time series analysis tasks\u201d).\n\nEspecially, I also expect Figure 6 for dataset like Solar energy to not be as good as the others. Because seasonality of Solar energy is high and it strongly depend on the weather, so increasing the lookback window might not be that beneficial.\n\n\n\n[1] https://openreview.net/pdf?id=cGDAkQo1C0p\n\n[2] https://arxiv.org/pdf/2305.10721.pdf"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission632/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698656860346,
            "cdate": 1698656860346,
            "tmdate": 1699635990972,
            "mdate": 1699635990972,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZtknUtoX03",
                "forum": "JePfAI8fah",
                "replyto": "vMNBdFMZl3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission632/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission632/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HTy3 (Part 1)"
                    },
                    "comment": {
                        "value": "We would like to sincerely thank Reviewer HTy3 for providing a detailed review and insightful suggestions.\n\n**Q1:** About the writing and figure revisions.\n\nThanks for your valuable suggestions. We conduct the following revisions of our paper:\n\n  * Misspelling: hardy -> hardly; Soloar-Energy -> Solar Energy.\n  * Abstract: \"inverts the duties of the attention mechanism and the feed-forward network\" -> \"applies the attention and feed-forward network on the inverted dimensions\". The detailed duties are elaborated in $\\underline{\\text{Section 3.2}}$. \n  * Figure 8: add the label of the x-axis.\n  * Abbreviate the Electricity dataset as ECL.\n\nAs for the fairness of $\\underline{\\text{Figure 1}}$, the results corresponds strictly to $\\underline{\\text{Table 1}}$. We set the maximum value of the radar as the best result of the benchmark for aesthetic reasons.\n\n**Q2:** Avoid dataset confusion and be precise on ETT and PEMS datasets.\n\nThanks for your scientific rigor. To avoid confusion, we extensively test the performance of all four ETT subsets and the same four PEMS subsets as the SCINet. **We report the averaged results of these subsets** in $\\underline{\\text{Figure 1 and Table 1 of the revised paper}}$ to  The detailed dataset descriptions are updated in $\\underline{\\text{Appendix A.1 of the revised paper}}$.\n\nTo be more precise and reproducible, we also **report the standard deviations of iTransformer performance in five runs, indicating stable results**, which are updated in $\\underline{\\text{Table 5 of the revised paper}}$. \n\n**Q3:** The proof supports the potential risks of unaligned timestamps and measurements on specific data sets.\n\nWe provide a detailed analysis of the Traffic dataset in $\\underline{\\text{Appendix E.3 of the revised paper}}$. We find **there are systematical delays in the road occupancy that each series describes**, because the sensors are installed in different areas (imagine how a traffic jam influences the road occupancy of different areas). \n\nDelay detection is the essential work that the MTSF model should cope with. However, it is because the vanilla Transformer embeds the localized time points that the obvious decay will be fused with noise.\n\nAnother support comes from the declined Traffic performance of the second and third rows in $\\underline{\\text{Table 3}}$, where attention is applied to the tokens of simultaneous time points. Since they do not reflect the same event, the performance can degrade a lot because of meaningless attention maps, unless the model has an enlarged respective field to capture the decay.\n\nAs for the unaligned measurements, it is common in time series forecasting, such as organizing together the different meteorological indicators (temperature and rainfall in Weather), or the quantity and proportion of the same observation (ILI dataset).\n\n**Q4:** More detailed explanations about $\\underline{\\text{Figure 7(b)}}$ about the multivariate correlations.\n\nWe newly add detailed explanations about multivariate correlations in $\\underline{\\text{Appendix E.1 of the revised paper}}$. In a nutshell, the x- and y-axis place the variates, and **the attention map can thus exhibit the Pearson Correlations among variates**. We want to show that the attention maps can reflect the correlations of raw series (comparing the subplots of the same column), benefiting Transformer with enhanced interpretability. Besides, the correlations, which vary from the lookback window to forecast window, will be reflected by the attention maps that change accordingly (comparing the subplots of the same row).\n\n**Q5:** The proof supports the fluctuation of PEMS.\n\nWe provide PEMS prediction visualization in $\\underline{\\text{Appendix E.2 of the revised paper}}$. **It shows that PEMS has more fluctuating series variations compared with others**. We highlight \"PatchTST may lose focus on specific locality to handle rapid fluctuation\" for that the patch length should be adaptively tuned for the series (such as frequency). Instead, iTransformer embeds the whole series as a token to describe the underlying process globally."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission632/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301319443,
                "cdate": 1700301319443,
                "tmdate": 1700301319443,
                "mdate": 1700301319443,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nkV915sxwp",
                "forum": "JePfAI8fah",
                "replyto": "vMNBdFMZl3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission632/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission632/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HTy3 (Part 2)"
                    },
                    "comment": {
                        "value": "**Q6:** Discussions on the Channal-Independence/Dependence, Instance Normalization, and the linear models.\n\nThanks for your valuable suggestion. **We newly added RLinear to our baseline**, which is very competitive on several datasets, such as ETT and Weather.\n\nAs the reviewer insightfully mentioned the CI/CD and RevIN[1], it is important to discuss on these techiques: \n\n  - Previous works[2]\\[3] have discussed the CI/CD tradeoff on model capacity and robustness. For example, CI can greatly increase the performance when data is scarce, while CD benefits from a higher capacity ideally. In the context of our work, **we think it is essential to make the variates independent**, especially when we know there are potential risks of delayed series and loosely organized measurements. Still, no matter CI/CD, **multivariate correlations** can not be explicitly utilized. Perpendicular to these works, iTransformer repurposes an architecture with the native Transformer modules to cope with it.\n  - RevIN or Series Stationarization[4] has been widely applied for the distribution shift (or non-stationarity) for real-world time series. They can boost the performance of both linear models and Transformers. There are Transformers in our baseline already equipped with this technique (PatchTST and Stationary), and we newly include the linear one, RLinear. These works strive for modeling **temporal dependencies**. In iTransformer, it is modeled naturally by the FFN and layernorm, and still leaves further improvement for us to tackle the distribution shift.\n  - Compared with linear forecasters, **iTransformer is good at forecasting high-dimensional time series** (that is, numerous variates, and complicated correlations). Under the univariate forecasting scenarios, iTransformer becomes in fact a stackable linear forecaster (FFN with the laynorm). For variate correlating, iTransformer keeps the variate independent and the attention module is on duty.\n\n[1] RevIN: Reversible Instance Normalization For Accurate Time-series Forecasting Against Distribution Shift.\n\n[2] The Capacity and Robustness Trade-off: Revisiting the Channel Independent Strategy for Multivariate Time Series Forecasting.\n\n[3] Is Channel Independent strategy optimal for Time Series Forecasting?\n\n[4] Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting.\n\n**Q7:** Complete experiments to enhance the robustness of the results and reproducibility.\n\nWe extensively include the following experiments to our paper:\n\n  * To support $\\underline{\\text{Table 2}}$: promotion of the inverted Transformer on all datasets in $\\underline{\\text{Appendix F.1 of the revised paper}}$.\n\n  * To support $\\underline{\\text{Figure 5}}$: additional results of PEMS to show iTransformer can generalize on unseen variates in $\\underline{\\text{Appendix F.3 of the revised paper}}$.\n\nAdditional results of increasing lookback on Solar-Energy: increasing the lookback length lead to an U-curve of performance (change point in $336$ and $720$) on this dataset. We also provide the explanations in $\\underline{\\text{Q4 of Reviewer yopU}}$.\n\n**Q8:** Clarify the protocol for the experiments trained with partial variates. \n\nDifferent from the efficiency training strategy, which randomly chooses part of the variates in each batch, there is no randomness in variate selection in $\\underline{\\text{Figure 5}}$. We conventionally take **the first $20\\%$ variate** of each dataset for training and use all for inference. So **CI-Transformer and iTransformer use the same set of variates during the whole process of training**. We have revised the part with a more rigorous description.\n\nWe also elaborate more on the the efficiency training strategy with more datasets added, and comprehensively test the model efficiency and performance in $\\underline{\\text{Appendix D of the revised paper}}$."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission632/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301361575,
                "cdate": 1700301361575,
                "tmdate": 1700463123543,
                "mdate": 1700463123543,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lHITGrmunH",
                "forum": "JePfAI8fah",
                "replyto": "vMNBdFMZl3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission632/Reviewer_HTy3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission632/Reviewer_HTy3"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the revision of the paper and replies to my review.\nAs I already said in my review, the paper has enough material to be accepted and Authors applied some of my comment as well as other reviewers' comments, which is, in my humble opinion, \"bullet-proofing\" the paper even more.\n\nBut here are few comments based on the new version:\n\n## Data unaligned\nI would just notify Authors that in the paper they mentioned dataset with \"unaligned timestamps\". However, in all the dataset Authors are using, timestamps are not unaligned. It is just the effect of specific events that is delayed, and it is only due to spatial nature of some dataset. Therefore, they should not say there are dataset with unaligned timestamp (because aligning variates for these specific events will results in misalignment for other timestamps) but rather talk about delayed or unaligned event.\n\n## Market Dataset\nFYI, Authors did not provide the results of RLinear.\n\n## Protocol for experiments with partial variates\n> We conventionally take the first 20% variates of each dataset [...]\n\nThen, my comment still stands. What would have been the results if Authors had taken the last 20% variates? or any other set of 20% variates.\nThere is a risk that taking the first 20% variates is a specific scenario for which the transfer works correctly. Therefore, to be rigorous it would have been better to try various configurations and check if the performances vary a lot or if it is stable.\n\n## Regarding CD/CI/DistributionShift discussion\n>  Discussions on the Channel-Independence/Dependence, Instance Normalization, and the linear models.\n\nIn my opinion, the comments of the Authors (and their opinion on this important topic) should be in the paper, even a condensed version. However, right now I can't see it in the revised version. But I do understand that the paper \"format\" (and page limitation) prevents to fully discuss and further investigate this topic.\n\nAnyway, I'm convinced that the proposal of iTransformer will be an important milestone for Multivariate Time Series Forecasting, therefore no problem on my side for the new version of the paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission632/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700557601120,
                "cdate": 1700557601120,
                "tmdate": 1700558281893,
                "mdate": 1700558281893,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3gpiaCUyNX",
                "forum": "JePfAI8fah",
                "replyto": "rVBwekFYwh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission632/Reviewer_HTy3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission632/Reviewer_HTy3"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for these additional experiments and results. \nI really think this way your paper will help other researchers advance on TSF with transformer-based architecture. These results open up to interesting questions that I think will be tackled in the future.\nI look forward to reading the final version."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission632/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649252757,
                "cdate": 1700649252757,
                "tmdate": 1700649252757,
                "mdate": 1700649252757,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FEsTt6kN2T",
            "forum": "JePfAI8fah",
            "replyto": "JePfAI8fah",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission632/Reviewer_yopU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission632/Reviewer_yopU"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a simple variant of transformer for time series forecasting, where the embedding is applied on each time series and the attention is across each variate. The idea is simple and effective. The improved performance is shown on various real-world datasets. The paper is well-written and easy to read. The idea can be viewed as a principal way to be adopted on various transformer-based architectures. The numerical experiments on different architectures and analysis of representations/correlations greatly enhance the importance of this work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors propose a principal way to apply the transformer-based model for time series forecasting. The idea is simple and effective, and the effectiveness is demonstrated via extensive experiments and ablation studies."
                },
                "weaknesses": {
                    "value": "The paper is relatively short of explanation/justification about the effectiveness of such an approach."
                },
                "questions": {
                    "value": "1. It would be better to describe the train-validation-test split in experiments, like training in past years and predicting in the next year, as it could be tricky for data pre-processing in time series forecasting and cause data leakage issues.\n\n2. After reading this paper, does the author implicitly assume the heterogeneity of variates is more important than temporal dependency in terms of forecasting? \n\n3. There are some interesting results in Table 3. Could the author comment on the not-so-good performance of the second row (both attention)?\n\n4. A minor one: in the left panel of Figure 6, there is a little jump on the red line from 336 to 720. Any reason why? Do multiple runs help?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission632/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission632/Reviewer_yopU",
                        "ICLR.cc/2024/Conference/Submission632/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission632/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826495303,
            "cdate": 1698826495303,
            "tmdate": 1700552897280,
            "mdate": 1700552897280,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jz05CsdWHO",
                "forum": "JePfAI8fah",
                "replyto": "FEsTt6kN2T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission632/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission632/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yopU"
                    },
                    "comment": {
                        "value": "Many thanks to Reviewer yopU for providing a detailed review and insightful questions.\n\n**Q1:** Describe the train-validation-test split in experiments.\n\nWe adopt the same train-validation-test split protocol as previous works, where the train, validation, and test datasets are **strictly divided according to chronological order to make sure there are no data leakage issues**. The data processing and split protocols are also included in $\\underline{\\text{Appendix A.1 of the revised paper}}$.\n\n**Q2:** Do we implicitly assume that variate heterogeneity is more important than temporal dependency for forecasting?\n\nWe think they are both important to achieve good MTSF performance. **However, the problem is that the heterogeneity of variates can hardly be considered in the vanilla Transformer**. After embedding, the variates are projected into the channels of embedding. It ignores the problem of inconsistent physical measurements and can fail to maintain the independence of variates, let alone capture and utilize the multivariate correlation, which is essential for forecasting with numerous variates, and complicated systems driven by the latent physical process (such as meteorological systems). \n\nIn addition, even if the FFN and layernorm seem simpler than attention blocks, **they are efficient and competent in learning the temporal dependency of a series**, which can be traced back to statistical forecasters such as ARIMA and Holt-Winter. They also have no problems with inconsistent measurements since they work on the time points of the same variate, and have an enlarged respective field as the whole lookback series can be embedded as the variate token.\n\n**Q3:** The reason for the not-so-good performance when applying attention to both temporal and variate dimensions.\n\nAs we observed the same declined performance of Traffic in the third row, the reason can be that **the designs both apply the attention module to the temporal dimension**. In our opinion, capturing temporal dependencies by attention is not a big problem. But it is **based on the fact that the time points of each timestamp are essentially aligned**.\n\nWe analyze the Traffic dataset in $\\underline{\\text{Appendix E.3 of the revised paper}}$, and we find there are potential risks of systematical delay among the road occupancy that each series describes, since the sensors are installed in different areas of the highway.\n\nConsequently, applying attention to the time points of the same timestamp on such a dataset can make the model learn meaningless attention maps, unless the model has an enlarged respective field to capture the decay or causal process. It is one of the reasons that we advocate embedding the whole series as the variate token.\n\n**Q4:** Explanations about the error jump when the input length increases from $336$ to $720$ in $\\underline{\\text{Figure 6}}$.\n\nWe have also noticed the problem that our approach embedding a large lookback series simply by MLP can be simple and coarse-grained. As we increase the lookback length by a large step, the input series inevitably contains more nonbeneficial information for future prediction, and the model capacity should also increase accordingly. It is instructive for us to develop fine-grained variate tokenization and well-designed embedding mechanisms.\n\n**Q5:** Explanations about the effectiveness of our approach.\n\nWe add new sections to analyze the effects of the inverting:\n\n  * Analysis of multivariate correlations in $\\underline{\\text{Appendix E.1 of the revised paper}}$.\n  * Potential risks of the vanilla Transformer embedding in $\\underline{\\text{Appendix E.3 of the revised paper}}$.\n  * Full results of the iTransformers in $\\underline{\\text{Appendix F of the revised paper}}$ to support that our method can boost the Transformers' performance, and generalize on unseen variates."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission632/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301252594,
                "cdate": 1700301252594,
                "tmdate": 1700301252594,
                "mdate": 1700301252594,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FjfR8twbhN",
                "forum": "JePfAI8fah",
                "replyto": "jz05CsdWHO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission632/Reviewer_yopU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission632/Reviewer_yopU"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' effort in answering my questions. I raised my score from 6 to 8, as the authors have addressed most of my concerns."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission632/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552952995,
                "cdate": 1700552952995,
                "tmdate": 1700552952995,
                "mdate": 1700552952995,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u92XrFEZgp",
            "forum": "JePfAI8fah",
            "replyto": "JePfAI8fah",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission632/Reviewer_hg6h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission632/Reviewer_hg6h"
            ],
            "content": {
                "summary": {
                    "value": "This paper explored a new angle to apply Transformer model to the multivariate time-series forecasting problem. Without the modification of the original transformer component, the proposed iTransformer inverted the duties of the self-attention mechanism and the feed-forward network. In iTransformer, the feed-forward network was used for series encoding, while the self-attention mechanism captured the correlation among different variates. The authors conducted experiments on six real-world datasets to evaluate the proposed model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThis paper provided a simple and effective inverted view to improve transformer-based multivariate time-series forecasters.\n\n2.\tCompared with the previous use of Transformer structure (without invert), the iTransformer showed some advantages, including better generalization on unseen variates, and the desired performance improvement over enlarged historical information.  \n\n3.\tExtensive experiments on different multivariate time-series forecasting tasks were conducted for evaluation. The author compared the proposed model with various baselines, along with a comprehensive modal analysis."
                },
                "weaknesses": {
                    "value": "1.\tAccording to Table 3 and Table 7, most of the result values are relatively small. This suggests that some marginal improvement may be susceptible to random factors (e.g., iTransformer v.s. PatchTST on ETT and Weather dataset, iTransformer v.s. SCINet on PEMS dataset). Therefore, I recommend reporting the standard deviation under different random runs and adding a significance test to provide further insights.\n\n\n2.\tAlthough the proposed efficient training strategy can reduce the required memory, it would still be better to compare its efficiency with linear models, since recent studies have indicated their advantages in both performance and efficiency."
                },
                "questions": {
                    "value": "1.\tCan you please explain why the TiDE results are so different from those reported in their original paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission632/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698843269433,
            "cdate": 1698843269433,
            "tmdate": 1699635990812,
            "mdate": 1699635990812,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TZrF0wRVjk",
                "forum": "JePfAI8fah",
                "replyto": "u92XrFEZgp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission632/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission632/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hg6h"
                    },
                    "comment": {
                        "value": "Many thanks to Reviewer hg6h for providing thorough insightful comments. \n\n**Q1:** The standard deviation of the results and the significance tests.\n\nThanks for your scientific rigor. We repeat each experiment five times with different random seeds. The standard deviations of iTransformer performance are updated in $\\underline{\\text{Table 5 of the revised paper}}$, which shows **the performance is stable**. \n\nWe also conducted the significance tests to compare iTransformer and the mentioned previous SOTA. We include all subsets of ETT and PEMS. The performance is also averaged from four prediction lengths with each containing five runs of random seeds. The standard deviations and test statistics are listed below, showing that **the performance is on par with the previous SOTA PatchTST and SCINet within the margin of error**.\n\n| MSE     | iTransformer | Previous SOTA | test statistic |\n| ------- | ------------ | ------------- | -------------- |\n| ETT     | 0.383\u00b10.001  | 0.381\u00b10.001   | 2.124          |\n| Weather | 0.258\u00b10.001  | 0.259\u00b10.001   | -2.913         |\n| PEMS    | 0.119\u00b10.001  | 0.121\u00b10.002   | -0.419         |\n\nTo our best knowledge, the benchmark of time series forecasting has been excavated for a long time. And PatchTST and SCINet as the SOTA on specific datasets has surpassed concurrent works by a large margin, **iTransformer can be on par with on their advantaged datasets while further go ahead on other chanllenging datasets** (such as ECL and Traffic), and thus achieves the comprehensive SOTA.\n\n**Q2:** Compare the model efficiency with linear models.\n\nWe newly measure the model efficiency in $\\underline{\\text{Figure 10 of the revised paper}}$, which comprehensively compares the training speed, memory footprint, and performance of the following models: iTransformer, iTransformer with our efficient training strategy and iTransformer with an efficient attention module; linear forecasters: DLinear and TiDE; Transformers: Transformer, PatchTST, and Crossformer. We have the following observations:\n\n  * The efficiency of iTransformer exceeds other Transformers in datasets with a small number of variates (such as Weather with $21$ variates). In datasets with numerous variates (such as Traffic with $862$ variates), the memory footprints are basically the same, but iTransformer can be trained faster.\n  * **iTransformer achieves particularly better performance on the dataset with numerous variates**, since the multivariate correlations can be explicitly utilized in our model. \n  * By adopting an efficient attention module or our proposed efficient training strategy on partial variates, **iTransformer can enjoy the same level of speed and memory footprint as linear forecasters**.\n\n**Q3:** About the different TiDE results from its original paper.\n\nWe carefully read through the forecasting settings of the TiDE paper and its official repo. The differences are listed here:\n\n  * **Enlarged lookback window**: TiDE paper adopts the lookback length of $720$, while ours uses the length of $96$, following the unified long-term forecasting protocol of TimesNet.\n\n  * **More epochs to train**: TiDE paper trains the TiDE model with $100$ Epochs, while we reproduce the results of all models with $10$ epochs.\n\nWe try our best to keep hyperparameters comparable for the fairness of experiments, such as adopting the same hidden dimension and the number of layers. **We also compare their performance under the same lookback length of TiDE, where iTransformer still achieves better results**, since our model can also benefit from an enlarged lookback window.\n\n| MSE          | ECL       | ETTh2     | Exchange  | Traffic   | Weather   | Solar-Energy | PEMS03    |\n| ------------ | :-------- | :-------- | :-------- | :-------- | :-------- | :----------- | :-------- |\n| TiDE         | 0.160     | 0.309     | 0.092     | 0.445     | 0.171     | 0.303        | 0.216     |\n| iTransformer | **0.129** | **0.301** | **0.086** | **0.349** | **0.158** | **0.191**    | **0.071** |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission632/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700301199606,
                "cdate": 1700301199606,
                "tmdate": 1700301199606,
                "mdate": 1700301199606,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DS4TEpFecQ",
                "forum": "JePfAI8fah",
                "replyto": "u92XrFEZgp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission632/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission632/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer hg6h,\n\nThanks for your valuable and rigorous review, which has inspired us to improve our paper further substantially. Following your suggestions, we have enhanced the robustness of experiments with the standard deviations of all datasets reported, newly tested the model efficiency comprehensively, and compared the training protocol with TiDE.\n\nDuring the rebuttal, we received insightful reviews and valuable comments to improve our paper further. $\\underline{\\text{Reviewer HTy3}}$ highlighted our contributions and gave high comments. And $\\underline{\\text{Reviewers yopU}}$ raised the score as we have addressed the concerns. We hope that this new version can address your concerns to your satisfaction and notice our contributions. We eagerly await your reply and are happy to answer any further questions. We kindly remind you that the reviewer-author discussion phase will end soon. After that, we may not have a chance to respond to your comments.\n\nThanks again for your valuable review. Looking forward to your reply.\n\nAuthors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission632/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645054870,
                "cdate": 1700645054870,
                "tmdate": 1700645054870,
                "mdate": 1700645054870,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]