[
    {
        "title": "LegoMT2: Non-Blocking Federated Learning for Massive Multilingual Machine Translation"
    },
    {
        "review": {
            "id": "XXrLLNaT0x",
            "forum": "Kg0IDQF3wp",
            "replyto": "Kg0IDQF3wp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4754/Reviewer_x6EA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4754/Reviewer_x6EA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an efficient approach with a tailored model architecture for massive multilingual neural machine translation. LegoMT2\norganizes 435 languages into 8 language-centric groups and attributes one local encoder-decoder for each group and a global encoder-decoder for all languages. LegoMT2 then trains each local and global encoder-decoder on a group-dedicated set of clients through asynchronous updating of parameters."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- federated learning used in MNMT to solve the parameter interference problem is somewhat novel\n- This paper is well-written, and experiments show their improvements over baselines."
                },
                "weaknesses": {
                    "value": "- The authors should present the key features of the traditional federated learning methods in the related works. The authors claim an efficient approach with a tailored model architecture for massive multilingual neural machine translation. What are the key attributes of the tailored model? In other words, what is the key difference between the federated learning used in this paper compared to the traditional federated method? \n- The experimental results are somewhat less convincing. Actually, the model size of the model should be viewed as 10.4B rather than 1.6B. And the final model used in inference is the averaged version of the 8 local models. Therefore, the model should be compared to the same-size finetuned model.\n- Why the model is finetuned from the pre-trained model? Why not training from scratch?"
                },
                "questions": {
                    "value": "- See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4754/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4754/Reviewer_x6EA",
                        "ICLR.cc/2024/Conference/Submission4754/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4754/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697880986431,
            "cdate": 1697880986431,
            "tmdate": 1700710863776,
            "mdate": 1700710863776,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BWP69dWk4D",
                "forum": "Kg0IDQF3wp",
                "replyto": "XXrLLNaT0x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4754/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4754/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the valuable comments and positive feedback on the novelty and effectiveness of our work. Please find our point-to-point responses below, which we hope will address any questions or concerns you may have.\n\n>**Question 1: What is the key difference between the federated learning used in this paper compared to the traditional federated method?**\n\nThe key difference between the traditional federated method and our method is stated in Appendix A.  The main difference includes three aspects:  the data construction, the model architecture, and the communication method.\n\n|  | Traditional Federated Method | LegoMT2 |\n| --- | --- | --- |\n| Data Construction | Domain [1] | Language-centric grouping |\n| Model Architecture | Centralized  | Detachable |\n| Communication Method* | Synchronous | Asynchronous |\n\n\nOne key factor in recent advanced FL algorithms is communication compression [2,3,4,5,6,7]. The traditional implementation of FL requires that each client send a full model (or a full model update) back to the server in each round. For large models, this step is likely to be the bottleneck of FL for a few reasons.\n\n**Reason 1:** internet speeds are usually faster for downloading than uploading. For example, Xfinity offers 125Mbps for downloads and only 15Mbps for uploads.\n\n**Reason 2:**  The size of the model is quite large. Therefore, there are numerous strategies available to compress these models or to reduce the bandwidth required for downloading the current model.\n\n**Reason 3:** Different clients may experience a considerable amount of idle time while waiting for the server to collect all models and generate a new global model.\n\n[1] Training mixed domain translation models via federated learning\n[2] Federated learning with compression: Unified analysis and sharp guarantees.\n[3] Federated learning: Strategies for improving communication efficiency\n[4] Communication-efficient adaptive federated learning\n[5] AutoFedNLP: An efficient FedNLP framework \n[6] FS-Real: Towards Real-World Cross-Device Federated Learning\n[7] FedED: Federated Learning via Ensemble Distillation for Medical Relation Extraction\n\n>**Question 2-1:  The final model used in inference is the averaged version of the 8 local models.**\n\nThere seems to be a misunderstanding here: during inference (the results obtained in Table 1), we utilize only a single flow (mix-flow, 1.6B), not the averaged version of the 8 local models.\n\n>**Question 2-2:  The model size of the model should be viewed as 10.4B rather than 1.6B. Therefore, the model should be compared to the same-size finetuned model.**\nYes,  we agree with your point. A fair comparing base model would be training 10.4B model and distilling into 1.6B model.\n\n**1) another valid comparison**\n\nWe\u2019ve given careful consideration to your suggestion and believe that, in addition to the model of the same size already listed in the table, there is indeed another model that could serve as a valid comparison:\n\n+ LegoMT2: In training, the model of the whole system is 10.4B; but in inference, each flow can work independently, and the model size is only 1.6B.\n\n+ Valid-Comparion:  We train a 10.4B model with centralized method, and then distilled this model to 1.6B.\n\n**2) It can be anticipated that this method will lag behind LegoMT2 in terms of both speed and performance.**\n\n+ Time: As evident from Table 3, training a 10.4B model is significantly slower, specifically 16.2 times slower, than LegoMT2. Furthermore, this pipeline also necessitates the distillation of the 10.4B model to a 1.6B model. This process requires the inference of the 10.4B model to obtain the output of each sample, which significantly increases the time requirement.\n\n|  | Lego-MT2 | Valid-Comparison |\n| --- | --- | --- |\n| Training Speed | 16.2$\\times$ | 1.0$\\times$ |\n| distillation | - | Inference 10.4 to get the output of each sample |\n\n+ Performance:  The 54.5B model\u2019s average performance surpasses LegoMT-2 by one point. Therefore, the upper limit of the 10.4B model has been established, and it\u2019s important to note that the distillation process inevitably leads to performance losses.\n\nHence, when compared to the solution of training a 10.4B model and then distilling it, LegoMT2 proves to be both faster and more efficient."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4754/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645155045,
                "cdate": 1700645155045,
                "tmdate": 1700645155045,
                "mdate": 1700645155045,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ChIITp1E3X",
                "forum": "Kg0IDQF3wp",
                "replyto": "cd7XTBIqNr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4754/Reviewer_x6EA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4754/Reviewer_x6EA"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttar"
                    },
                    "comment": {
                        "value": "Hi, authors,\n   Thanks for your rebuttal. I have read your rebuttals to all reviewers and I re-read your paper carefully.\n   1  Actually, I have reviewed this paper two times (the last review in NIPS) and I noticed the improvement of your paper. However, I still cannot find some shining scientific points in your paper. \n   2 Can I say that you just apply the FL into multi-lingual NMT with little modification? As your rebuttal, it seems that little improvement have you made about FL you used in this paper.  Is there any detailed design for NMT.\n  3 The fair comparison is very important for a scientific research paper. We cannot just see the final report score, and more importantly, we need to care about why the improvement is achieved.\n  4 My question \"why not train from scratch\" shows my concern that your method may rely on a pre-trained NMT model."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4754/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710829058,
                "cdate": 1700710829058,
                "tmdate": 1700710829058,
                "mdate": 1700710829058,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SnJPQavdAH",
                "forum": "Kg0IDQF3wp",
                "replyto": "XXrLLNaT0x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4754/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4754/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**The fair comparison is very important for a scientific research paper. We cannot just see the final report score, and more importantly, we need to care about why the improvement is achieved.** \n\n1. We didn't merely report the final score, but also carried out extensive analysis experiments focusing on the key design elements of this training framework.\n+ Table 4: model architecture analysis\n+ Table 5: the language group strategy analysis \n+ Figure 2: the effect of asynchronous learning algorithm on system performance.\n+ Figure 3: the effect of hyperparameters in the training algorithm\n\n2. Extensive experiments, coupled with multi-faced evaluations, have underscored the efficacy of the framework.\n+ effectively supports translation between over 400 languages\n+ evaluation of existing benchmark (Table 1 & Table 2)\n+ human evaluation (Page 7-page 8 & Appendix D)\n+ Compare with ChatGPT (Page 20)\n   \n3. This task involves pre-training, which inherently requires substantial computational and time resources. \n+ It\u2019s not that we are unwilling to make a fair comparison, but rather, we are unable to do so due to these constraints.\n|  | Lego-MT2 | Valid-Comparison |\n| --- | --- | --- |\n| Training Speed | 16.2$\\times$ | 1.0$\\times$ |\n| Training Time | 15 days on 64 A100 GPUs  | **240 days  on 64 A100 GPUs**  |\n| distillation | - | Inference 10.4 to get the output of each sample |\n| Overall Time | 15 days |  **240+ days** |\n\n+ We can expect the performance of the solution (training a 10.4B model and then distilling to 1.6B) to be worse than LegoMT2.\n+ The upper limit of the 10.4B model has been established. The 54.5B model\u2019s average performance surpasses LegoMT-2 by one point. Therefore\n+ It\u2019s important to note that the distillation process inevitably leads to performance losses.\n\nHence, when compared to the solution of training a 10.4B model and then distilling it, LegoMT2 proves to be both faster and more efficient.\n\n>**My question \"why not train from scratch\" shows my concern that your method may rely on a pre-trained NMT model.**\n \n1) Utilizing NLLB-200-1.3B as an initial step is important, but it is not enough for handling 400+ languages.\n\n+ Fine-tuning NLLB-200-1.3B directly on the data of over 400 languages actually results in a decrease in model performance, as shown in Table 1 of the paper.\n+ One of our contributions to utilizing NLLB includes effort in extending the vocabulary from 200 languages to 435 languages. The tokens for an additional 235 languages are initiated randomly, resulting 0.3B extra parameters compared to NLLB. In this way, we can best utilize the pre-trained parameters in NLLB while largely expanding the language support in LegoMT2, in a cost-effective way.\n\n 2) The success of NLLB-200-1.3B is not trivial.   As shown below (which is from the NLLB paper[5]),  they devote considerable energy to optimizing language performance. On average, enhancing the performance of a language takes 119 days, with the most time-consuming language requiring as long as 287 days for optimization.\n|  |  |\n| --- | --- |\n| # of Languages requiring Re-translation | 10 |\n| Avg # of Re-translations | 1 |\n| Max # of Re-translations | 2 |\n| Avg # of Days to Translate 1 language | 42 |\n| Avg # of Days to align | 28 |\n| **Avg # of Days for 1 language** | **119** |\n| Shortest Turnaround (days) for 1 language | 70 |\n| **Longest Turnaround (days) for 1 language** | **287** |\n|  |  |\n\n[5] No Language Left Behind: Scaling Human-Centered Machine Translation\n\n 3) It is too costly to try full training of LegoMT2 without NLLB initialization.  \n\n+ Plan 1: Spending 287 days to replicate the performance of NLLB, and then adding more than 200+ languages does not seem to be an efficient solution.\n+ Plan 2: Training each language from scratch and optimizing it, which takes over 200 days, is also costly.\n\nConsidering limited computation resources, we want to utilize them to conduct more impactful research."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4754/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736128717,
                "cdate": 1700736128717,
                "tmdate": 1700740639448,
                "mdate": 1700740639448,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lrYxEicplG",
            "forum": "Kg0IDQF3wp",
            "replyto": "Kg0IDQF3wp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4754/Reviewer_s6KB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4754/Reviewer_s6KB"
            ],
            "content": {
                "summary": {
                    "value": "To train a single model for massive languages is known for a challenging problem. This paper tackles the problem of how to efficiently train a neural machine translation for massive multilingual languages and proposed LegoMT2 that consists of local encoder-decoder models for language groups and a global encoder-decoder for all languages, where 435 languages are grouped into 8 language-centric category. The experimental results show the training efficiency and translation accuracy improvement, achieving 16.2x faster than the distributed training method for the same-size NLLLB and improving the translation accuracy by 2.2 BLEU on Flores-101 dataset averagely."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The idea of asynchronous model parameter update that are language-group dependent is straightforward. Extensive experiments show that the proposed approach yields improvements in translation accuracy across languages. The proposed approach also helps the multi-way model to get trained faster."
                },
                "weaknesses": {
                    "value": "- Extensive experimental results and analyses are not fit in 9 pages. There are some description overlaps in Section 1 and 3 so the authors can move the contents from Appendix to the main pages."
                },
                "questions": {
                    "value": "- Reg Section 3.3; how helpful is the parameter initialization with NLLB-200-1.3B? Have you ever looked into this effect, without having the NLLB initialization?\n- Have you ever tried with different language grouping? \n- Why do you think Dec-Flows is better in the low-resource language groups?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4754/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4754/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4754/Reviewer_s6KB"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4754/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814277210,
            "cdate": 1698814277210,
            "tmdate": 1699636457747,
            "mdate": 1699636457747,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dIfYuyDcek",
                "forum": "Kg0IDQF3wp",
                "replyto": "lrYxEicplG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4754/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4754/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your helpful suggestions. We will make the necessary changes.\n\n>**Weakness: Extensive experimental results and analyses are not fit in 9 pages. There are some description overlaps in Section 1 and 3 so the authors can move the contents from Appendix to the main pages.**\n\nThanks for your suggestion! We will move the analysis of the difference between of traditional Federated Learning and comparison with LLM from the Appendix to the main part.\n\n>**Question 1: Reg Section 3.3; how helpful is the parameter initialization with NLLB-200-1.3B? Have you ever looked into this effect, without having the NLLB initialization?**\n\nWe have a reasonable belief that NLLB initialization is important. However, it is too costly to try full training of LegoMT2 without NLLB initialization. Therefore, we did not conduct alternative experiments. According to NLLB paper[1] (Table 2), it would take over 200 days to replicate the alignment process of NLLB  to extend from NLLB's 200 languages to LegoMT2's 435 languages. \n\n|   | |\n|-------------------------------------------|-----|\n| # of Languages requiring Re-translation   | 10  |\n| Avg # of Re-translations                  | 1   |\n| Max # of Re-translations                  | 2   |\n| Avg # of Days to Translate 1 language     | 42  |\n| Avg # of Days to align                    | 28  |\n| Avg # of Days for 1 language              | 119 |\n| Shortest Turnaround (days) for 1 language | 70  |\n| Longest Turnaround (days) for 1 language  | 287 |\n|   | |\n\n\n[1] No Language Left Behind: Scaling Human-Centered Machine Translation\n\n\n\nNevertheless, we would like to underscore that NLLB-200-1.3B only supports 200 languages. Our contribution to utilizing NLLB includes effort in extending the vocabulary from 200 languages to 435 languages. The tokens for an additional 235 languages are initiated randomly, resulting 0.3B extra parameters compared to NLLB. In this way, we can best utilize the pre-trained parameters in NLLB while largely expanding the language support in LegoMT2, in a cost-effective way."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4754/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634403229,
                "cdate": 1700634403229,
                "tmdate": 1700634940059,
                "mdate": 1700634940059,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AXJzAHHcaO",
            "forum": "Kg0IDQF3wp",
            "replyto": "Kg0IDQF3wp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4754/Reviewer_fThx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4754/Reviewer_fThx"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel approach called LegoMT2 for multilingual neural machine translation. It addresses the challenge of learning a single model for a large number of languages by organizing languages into groups and using a multi-way model that includes multiple encoder-decoders \u2013 each for a certain language group and another global encoder-decoder. LegoMT2 trains these encoder-decoder pairs on dedicated server clients using asynchronous updating of parameters."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed LegoMT2 supports over 400 languages for machine translation with one single encoder-decoder model, doubling the number of NLLB while significantly faster in training."
                },
                "weaknesses": {
                    "value": "The paper did not conduct specific verification experiments on parameter interference to demonstrate that the performance improvement of LegoMT2 over finetuned NLLB-200-1.3B indeed stems from the alleviation of parameter interference phenomena."
                },
                "questions": {
                    "value": "1. Which of Single-FT or Single-FT + MoE in Table 3 is used for the experiments in Table 1 and Table 2? Have the translation performance of both been evaluated?\n2. Have any other methods for MERGE operation of non-blocking federated learning, apart from simple averaging, been tried and evaluated?\n3. How about LLMs for Multilingual Machine Translation\uff1f"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4754/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698972793073,
            "cdate": 1698972793073,
            "tmdate": 1699636457653,
            "mdate": 1699636457653,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v5OYKJ2sra",
                "forum": "Kg0IDQF3wp",
                "replyto": "AXJzAHHcaO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4754/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4754/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your insightful feedback and suggestions. Please see our responses to each of your comments listed below.\n\n>**Weakness: The paper did not conduct specific verification experiments on parameter interference to demonstrate that the performance improvement of LegoMT2 over finetuned NLLB-200-1.3B indeed stems from the alleviation of parameter interference phenomena.**\n\nParameter interference is a fundamental problem in multilingual machine translation. \n\nIt refers to the competition between different languages for the limited parameters of a model when we hope to use a single model to handle all translation directions. This can result in good translation results for some languages, while the translation results for other languages may be less satisfactory.\n\nAs illustrated in Table 1, directly tuning NLLB-200-1.6B yields results that are inferior to those of NLLB-200-1.3B. However, LegoMT2 effectively enhances performance across over 400 languages, which suggests that LegoMT2 successfully mitigates parameter interference.\n\n>**Question 1: Which of Single-FT or Single-FT + MoE in Table 3 is used for the experiments in Table 1 and Table 2? Have the translation performance of both been evaluated?**\n\nThe Single-FT is used for experiments in Table 1 and Table 2. We evaluated a pretrained 12B model (M2M-100-12B) but did not fine-tune the 12B model, because the training 12B model is too slow (16x slower in Table 3). It would require 240 days on 64 A100 GPUs to complete the training, which is prohibitively costly. \n\n>**Question 2\uff1aHave any other methods for MERGE operation of non-blocking federated learning, apart from simple averaging, been tried and evaluated?**\n\nNo, we do not try other methods for MEREG. \n\n1)  At present, this MERGE operation is adequate for LegoMT2.  The existing MERGE operation, which involves replacing the global module on the client side with the average of the parameters obtained from the server, is the standard merge operation in federated learning. \n\n2) Testing different merge operations is a very resource- and time-consuming operation.  While we could explore more variations, it\u2019s important to note that LegoMT2 represents a substantial pre-training effort.  LegoMT2's training takes 15 days on  64 80G A100 GPUs.\n\n3) We sincerely hope that you reconsider the value of our paper. In this paper, we proposed a novel training framework, LegoMT2,  designed for massive massively multilingual machine translation (MNMT) systems. \n\n + We introduced an efficient training framework for MNMT that supports 435 languages, which is more than any other system.\n + LegoMT2 partitions the model and data together and employs an efficient non-blocking algorithm to accelerate training.\n + LegoMT2 is small but powerful, which works only with 1.6B parameters and achieves better results than other models at the same or larger size (only behind NLLB-54B which is 30x larger). \n\n>**Question 3\uff1a How about LLMs for Multilingual Machine Translation\uff1f**\n\nThe comparison with ChatGPT is in the Appendix. Both in the En\u2192X and X\u2192En direction, ChatGPT falls behind LegoMT2 even with eight-shot. \n\n| Model              | X$\\rightarrow$En | En$\\rightarrow$X | AVG. |\n|--------------------|------------------|------------------|------|\n| ChatGPT zero-shot  | 27.9             | 23.9             | 25.9 |\n| ChatGPT eight-shot | 31.9             | 24.7             | 28.3 |\n| LegoMT2            | 38.3             | 31.6             | 35.0 |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4754/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675611256,
                "cdate": 1700675611256,
                "tmdate": 1700675611256,
                "mdate": 1700675611256,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]