[
    {
        "title": "Harnessing Attention Prior for Reference-based Multi-view Image Synthesis"
    },
    {
        "review": {
            "id": "QYsA0d5ruR",
            "forum": "wyHCt1P7SR",
            "replyto": "wyHCt1P7SR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission666/Reviewer_RjKx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission666/Reviewer_RjKx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an approach for reference-based multi-view synthesis that supports both image inpainting from reference samples and novel view synthesis of the reference image. They are all formulated as contextual inpainting tasks. The proposed ARCI enhances attention mechanisms in T2I models by learning correlations across different reference view with self attention and control novel view synthesis with cross attention. Both qualitative and quantitative experiments have been conducted to demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper regards multi-view image synthesis as reference-based inpainting task, which is novel and provides an interesting direction for realizing NVS.\n    \n2. The results of novel view synthesis are excellent.\n    \n3. The proposed Block Casual Masking bridges the gap of converting a diffusion model to a AR-based generative model."
                },
                "weaknesses": {
                    "value": "The paper needs to improve readability by structuring the content more logically."
                },
                "questions": {
                    "value": "1. Do view embeddings require retraining for each image?\n\n2. For multi-view NVS task, does the LDM need fintuning for each new image tested?\n\n3. How to control the view direction of multi-view synthesis?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission666/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission666/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission666/Reviewer_RjKx"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission666/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698469041827,
            "cdate": 1698469041827,
            "tmdate": 1699635993854,
            "mdate": 1699635993854,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EychG8aMC0",
                "forum": "wyHCt1P7SR",
                "replyto": "QYsA0d5ruR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission666/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission666/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RjKx"
                    },
                    "comment": {
                        "value": "Thanks for your valuable feedback. We would further improve the readability in our revision.\n\n**1. Do view embeddings require retraining for each image?**\n\nThanks. Our ARCI is a generalized model without any test-time fine-tuning for both Ref-inpainting and NVS.\n\n**2. For multi-view NVS task, does the LDM need fintuning for each new image tested?**\n\nThanks. Benefiting from the autoregressive training of ARCI, all views can be generalized with the same model without test-time fine-tuning.\n\n**3. How to control the view direction of multi-view synthesis?**\n\nThanks. For multi-view NVS, we provide a relative pose for each view (based on the first view) to pose FC as in Fig.2(c)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission666/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107230488,
                "cdate": 1700107230488,
                "tmdate": 1700107230488,
                "mdate": 1700107230488,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5dc9FWJl4t",
            "forum": "wyHCt1P7SR",
            "replyto": "wyHCt1P7SR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission666/Reviewer_d1up"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission666/Reviewer_d1up"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the task of novel view image synthesis. The authors introduce Attention Reactivated Contextual Inpainting (ARCI) technique for both reference-based inpainting and novel view synthesis. The authors show comparison results on MegaDepth, ETH3D and Objaverse."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper conducts many experiments and shows extensive quantitative and qualitative comparisons for different applications in novel view synthesis.\n2. The authors show some good results in reference-based image inpainting."
                },
                "weaknesses": {
                    "value": "1. The definition is a bit confusing. Why do we need the concept of local synthesis and global synthesis for novel view synthesis? What\u2019s the core challenge? Besides, the claimed local synthesis is actually the reference-guided inpainting task, and the proposed ARCI is built upon an inpainting model, which makes the target more like inpainting. \u00a0\n    \n2. The presentation is not clear enough. for example, in Figure 1, the authors should explain what the purple mask means. What is the input of the model, the green bounding box or the purple one? The inputs and outputs look very different in (a)-(d).\n    \n3. The authors claim in the introduction that they can use efficient task and view prompt tuning for novel view synthesis with frozen SD, However, in the experiments, the authors finetuned the whole stable diffusion and reported some results with Lora and fully finetuned results. I think the authors should grandly review and improve their presentation and make them clearer.\n    \n4. The comparison in Figure 5 is not fair. The authors should report the original results of Zero123 instead of re-training it with a suboptimal training setting. Besides, zero123 is able to take specific camera poses to generate the novel view, what\u2019s the input used here for both zero123 and ARCI? The authors should make sure all the models take appropriate inputs.\n    \n5. Some important experiments are missing. The authors should conduct an ablation study on adaptive masking. For comparison with zero-123 for novel view synthesis, the authors should show comparisons on GSO and RTMV following zero-123. For reference-based inpainting, the authors should also show comparison results by using some very different references following the setting of Paint-by-Example to verify the effectiveness of the ref-inpainting technique for open-domain cases.\n    \n6. What\u2019s the used task and view prompt? Specifically, do the task prompts indicate local or global? How to set the view prompts? Rotation angles or front/side/back view? If the model tasks as input an image beyond these views, then how does the ARCI work?"
                },
                "questions": {
                    "value": "It is a bit hard for me to fully understand this paper. It looks like there are two different tasks (without much analysis and relations) in the same paper, i.e., reference-based inpainting and novel view synthesis. Besides, instead of following existing standard benchmarks, the authors design new comparison settings for different tasks in this paper. Many details are missing especially about the detailed setting of different tasks and view prompts. For example, how to correlate the concrete prompt with concrete views in the training dataset?  Please find more details in the Weaknesses and address my concerns."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission666/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698546085558,
            "cdate": 1698546085558,
            "tmdate": 1699635993774,
            "mdate": 1699635993774,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OYGVKoRbka",
                "forum": "wyHCt1P7SR",
                "replyto": "5dc9FWJl4t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission666/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission666/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer d1up"
                    },
                    "comment": {
                        "value": "Thanks for your valuable feedback. We recognize that the pivotal contribution of our work has not been adequately emphasized and will make a major revision to our paper to rectify this issue.\n\n**1. Confused definition for two tasks.**\n\nThanks. We would definitely re-write our paper to improve the presentation and definition.\n\n**2. Unclear presentation for Fig.1**\n\nThanks. We would improve the presentation and redraw Fig.1.\n\n**3. Why does ARCI with prompt tuning need fine-tuning?**\n\nThanks for this good point! We have to clarify that ARCI based on **Ref-inpainting does not require any fine-tuning for LDM**, while NVS requires fine-tuning for superior results, which have been illustrated in Fig.2 and Sec.3.\nMoreover, our method also enjoys much faster convergence compared to previous methods like Zero123 as we verified in Fig9 of the Appendix.\nBesides, both ARCIs for Ref-inpainting and NVS are generalized models, without requiring any test-time fine-tuning.\n\n**4. Unfair comparison of Fig5. What's the input of Zero123 and ARCI?**\n\nThanks. We would add the results of official zero123 to Fig5 in the revision. Moreover, inputs for ARCI and Zero123 are the same, including a reference image and relative poses.\n\n**5. Missing experiments for NVS and very different references for Ref-inpainting.**\n\nThanks. We have compared our method to Zero123 in GSO dataset in the Appendix. For the very different references in Ref-inpainting, we have to clarify that this is not our goal in reference-guided inpainting, which has been clearly defined in Sec.2.3 (all reference and target views should be taken from the same object from different viewpoints).\n\n**6. Details about the prompts used in ARCI**\n\nFor  NVS, we follow Zero123 to encode polar, azimuth, and radius distance which are detailed in the Appendix.\nWe use shared (task) and unshared (view) trainable embeddings as task and view prompt, which are encoded by CLIP-H and further infused into LDM through cross-attention (Fig.2). We will further claim these in our revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission666/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700106963015,
                "cdate": 1700106963015,
                "tmdate": 1700106963015,
                "mdate": 1700106963015,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Cv5DGw6QHc",
            "forum": "wyHCt1P7SR",
            "replyto": "wyHCt1P7SR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission666/Reviewer_frhM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission666/Reviewer_frhM"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method to generate multi-view images. The major novelty is a combination of local synthesis and global synthesis."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The results seem to be good. The numerical results indicate the method to achieve state-of-to-the-art performance.\n2. The proposed methods are general. It can be applied to multiple tasks, e.g. single-view NVS, multi-view NVS."
                },
                "weaknesses": {
                    "value": "1. The writing is too bad. The paper has many confused words, unclear explanation, and unconvincing arguments. Here are some examples:\n    * Confused words. In abstract, \"Our contributions of ARCI, built upon the Stable Diffusion fine-tuned for text-guided inpainting, include skillfully handling difficult multi-view synthesis tasks with off-the-shelf T2I models, introducing task and view-specific prompt tuning for generative control, achieving end-to-end Ref-inpainting, and implementing block causal masking for autoregressive NVS. \" The sentences indicates there are 3 contribution: 1) handling difficult multi-view synthesis tasks with off-the-shelf T2I models, 2) introducing task and view-specific prompt tuning for generative control, achieving end-to-end Ref-inpainting, 3) Implementing block causal masking for autoregressive NVS. The phrase 'block causal masking' is first introduced here, which confuses readers. In addition, why implementing this can be counted as a contribution? I suggest the author explain what the block causal masking is first.\n    * Unclear explanation. In introduction, \"This task can be broadly categorized into two facets: local and global multi-view image synthesis from reference images. \" If I understand Figure 1 correctly, there seems to be 2 tasks, ref-impainting and novel view synthesis here, but the author write one task with 2 facets. I think this is a confusing definition.\n   * Unconvincing arguments. In introduction, 'They struggle to capture fine-grained correlations, including object orientations and precise object locations, between reference and target images. These nuanced details are pivotal for tasks such as multi-view generation, as exemplified by Ref-inpainting.' Is there any reference paper or experimental results supporting this arguments? \nThe above shows 3 typical examples and there are many more here and there. The paper writing does not satifiy ICLR bar and requires major re-writing.\n2. What's the motivation of this paper? 'However, adapting them for multi-view synthesis is challenging due to the intricate correlations between reference and target images.' This sentence is correct, this is not the problem of current methods, e.g. Zero-1-to-3. Why does this proposed method better than zero-1-to-3. I suggest the author to write the motivation clearly in abstract. \n3. In related work, 'Compared with these aforementioned manners, the proposed ARCI enjoys both spatial modeling capability and computational efficiency.' What is spatial modeling capability? Please define spatial modeling capability.\n4. In method, please define and formulate the two tasks first. \n5. ATTENTION REACTIVATED CONTEXTUAL INPAINTING is not intuitive. It's hard to image the technique by the name. I recommentd to use more intuitive method name.\n6. The method design seems to be complicated or the method writing is bad, which make the method design look complicated."
                },
                "questions": {
                    "value": "I have many questions, but many of them are due to unclear writing. I recommend the author to re-write the entire paper and I can re-rate the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission666/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission666/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission666/Reviewer_frhM"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission666/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698554013456,
            "cdate": 1698554013456,
            "tmdate": 1699662267958,
            "mdate": 1699662267958,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OknK8gnwy4",
                "forum": "wyHCt1P7SR",
                "replyto": "Cv5DGw6QHc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission666/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission666/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer frhM"
                    },
                    "comment": {
                        "value": "Thanks for your valuable feedback. We recognize that the pivotal contribution of our work has not been adequately emphasized and will make a major revision to our paper to rectify this issue.\n\n**1. Confused and unclear explanation and motivation**\n\nThanks. We would definitely re-write our paper to improve the presentation.\n\n**2. Unconvincing arguments: \"They struggle to capture fine-grained correlations\" and \"spatial modeling capability\"**\n\nThanks for this good point. We have to clarify that we have proved these in the qualitative and quantitative experiments in Fig.4 and Tab.1.\nFor Ref-inpainting, which needs to learn spatial relations, previous methods like ControlNet fail to address it properly. \nThus our claims that ARCI enjoys good fine-grained correlations and spatial modeling capability are reasonable.\n\n**3. Please define and formulate the two tasks first in method**\n\nThanks. We would clearly define two tasks at the start of the method (Sec.3).\n\n**4. ATTENTION REACTIVATED CONTEXTUAL INPAINTING is not intuitive**\n\nThanks. We would change to a new name for our method."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission666/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700105790953,
                "cdate": 1700105790953,
                "tmdate": 1700105790953,
                "mdate": 1700105790953,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "biY0IsteNW",
            "forum": "wyHCt1P7SR",
            "replyto": "wyHCt1P7SR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission666/Reviewer_a7pP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission666/Reviewer_a7pP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Attention Reactivated Contextual Inpainting (ARCI), a unified method to leverage pre-trained text-to-image diffusion models (e.g., Stable Diffusion) to complete inpainting tasks. The key ideas of ARCI are i) using the attention layers in T2I diffusion models to learn correlations across different views and ii) using cross-attention layers to inject extra control via prompt tuning. Experiments are conducted on Ref-inpainting (local inpainting) and novel view synthesis (global inpainting). ARCI shows superior performance on both tasks compared to existing methods and requires fewer extra parameters and training costs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper proposes a unified framework for solving two challenging tasks. Although reusing the attention layers of pre-trained T2I diffusion models for various purposes has been common in recent works, such a general framework applicable to different generation tasks is still interesting.\n\n- Extensive experimental results are presented for both tasks, and the proposed ARCI is resource-friendly, requiring only a tiny amount of extra parameters and fewer training costs than other approaches. \n\n- The Ref-inpainting results are promising. The model is lightweight and also achieves state-of-the-art inpainting quality. The attention visualization is convincing and shows that the model learns to look at the correct locations in the reference images."
                },
                "weaknesses": {
                    "value": "- The biggest issue of the paper is the writing quality, which makes the paper very hard to follow. Details are listed below.\n\n    1. The introduction is extremely long and poorly organized. Many points are made, but I cannot find a precise sentence that emphasizes the essential contribution of the paper. The two applications (Ref-inpainting and NVS) seem to stem from the unified ARCI approach, but the introduction always tries to separate them when discussing their challenges and claiming improvements.\n\n    2. While the introduction makes separate claims for the two tasks, the descriptions of the two tasks in Sec.3 are heavily entangled, making it hard to clearly understand how the model works for each task.\n\n    3.  Fig.1 to Fig.3 are very difficult to parse. The texts in the figures are too small. The inputs and outputs for each task are not clearly explained. The captions are not self-contained, and it is also very hard to link them to certain parts of the main text.  \n\n\n- There seems to be no *quantitative evaluation* for multiview image generation -- Table 2 of the main paper only provides results with a single target view. Since the paper claims improvements in multiview image generation, it is important to formally evaluate the consistency of the generated multiview images. \n\n- The proposed ARCI is limited by the autoregressive generation design and cannot produce many multiview images. While a potential tradeoff is to constrain the length of the condition, it would definitely sacrifice the quality/consistency of the generated images. Note that an important goal of multiview image generation is to extract the 3D object/geometry. Both the number of views and the multiview consistency are important when exporting a 3D model from the generated images. The proposed designs are suboptimal compared to recent works such as [SyncDreamer](https://arxiv.org/abs/2309.03453) and [MVDiffusion](https://arxiv.org/abs/2307.01097), which can generate 16 or more views in parallel.\n\n- Although the authors claim that ARCI outperforms Zero123 in novel view synthesis (NVS), Zero123 itself is not merely an NVS model. Zero123 can serve as a general diffusion model backbone with 3D shape/global priors, which facilitates many other approaches via fine-tuning or score distillation (e.g., [Magic123](https://arxiv.org/abs/2306.17843), [One-2-3-45](https://arxiv.org/abs/2306.16928), [SyncDreamer](https://arxiv.org/abs/2309.03453), etc). It is true that ARCI outperforms Zero123 in NVS, especially when the training budget is limited, but the scope of ARCI is much narrower than Zero123 given the more complicated designs."
                },
                "questions": {
                    "value": "As detailed in the weakness section, I believe the poor writing organization significantly impairs the quality of the paper. At the same time, the value of the proposed ARCI for multiview NVS is not convincing -- there are no quantitative evaluations for multiview image generation, and the method design seems to be suboptimal when we need to generate a large number of consistent views."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission666/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698745988830,
            "cdate": 1698745988830,
            "tmdate": 1699635993563,
            "mdate": 1699635993563,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sbPbbnfTU8",
                "forum": "wyHCt1P7SR",
                "replyto": "biY0IsteNW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission666/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission666/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer a7pP"
                    },
                    "comment": {
                        "value": "Thanks for your valuable feedback. We recognize that the pivotal contribution of our work has not been adequately emphasized and will make a major revision to our paper to rectify this issue.\n\n**1. Entangled descriptions of the two tasks in Sec.3**\n\nThanks for this comment. We should clarify that ARCI is a generalized model to address these two tasks.\nFor the single reference-based synthesis, Ref-inpainting and NVS work very similarly in ARCI (just stitching reference and masked target). The three differences are 1) NVS needs to fine-tune the whole SD, 2) NVS needs another pose FC to encode relative pose information, 3) NVS needs to add positional encoding before the self-attention modules. We will further claim these in Sec.3.1 and Sec.3.2.\nFor the multi-view synthesis, the only difference is the autoregressive generation for NVS, as detailed in Sec.3.2.\n\n**2. No quantitative evaluation for multiview image generation**\n\nThanks. We add the multi-view quantitative evaluation as follows:\n|         | Ref view | PSNR   | SSIM  | LPIPS  | CLIP   | P-CLIP |\n|---------|----------|--------|-------|--------|--------|--------|\n| Zero123 | first    | 19.265 | 0.855 | 0.1366 | 0.7723 | 0.7756 |\n| Zero123 | last     | 14.621 | 0.767 | 0.2569 | 0.6921 | 0.7667 |\n| ours    | first    | 21.573 | 0.883 | 0.1143 | 0.7964 | 0.7709 |\n| ours    | AR       | 21.271 | 0.882 | 0.1195 | 0.7882 | 0.7958 |\n\nWe introduce the pairwise CLIP score (P-CLIP) to verify the consistency of all generated samples. \nLeftRefill outperforms Zero123 in most metrics, while AR could prominently improve the consistency with just a little quality degradation.\n\n**3. ARCI is limited by the autoregressive generation design and cannot produce many multiview images**\n\nThanks. We have provided results of consistent multi-view synthesis in our supplementary.\n\n**4. ARCI is much narrower than Zero123 given the more complicated designs**\n\nThanks. ARCI is a simple model design that can also be used for image-to-3D prior learning. Because ARCI could generate consistent multi-view images for methods like NERF to learn. Since we do not focus on 3D generation, these works can be seen as interesting future work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission666/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700104801349,
                "cdate": 1700104801349,
                "tmdate": 1700105183653,
                "mdate": 1700105183653,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]