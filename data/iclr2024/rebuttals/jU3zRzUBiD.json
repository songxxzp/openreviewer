[
    {
        "title": "Compensating for Nonlinear Reduction with Linear Computations in Private Inference"
    },
    {
        "review": {
            "id": "lLRlcyqVxO",
            "forum": "jU3zRzUBiD",
            "replyto": "jU3zRzUBiD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5216/Reviewer_SDAb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5216/Reviewer_SDAb"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces approaches to improve the performance of cryptographically secure private inference by improving the network's ReLU efficiency. The authors leverage neural architecture search to compensate for the reduced number of ReLU activations by augmenting the network's linear operations (FLOPs). The achieved performance is on par with state-of-the-art ReLU-pruning methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "$\\bullet$  Experimental results on Imagnet dataset. \n\n$\\bullet$ The ReLU-Accuracy performance of the proposed baseline model is on par with existing ReLU-pruning methods. \n\n$\\bullet$  Ablation studies are presented well to show the efficacy of the pruning method and the merits of reusing ReLUs.\n\n\n$\\bullet$ Proposed methods are very well presented and easy to understand."
                },
                "weaknesses": {
                    "value": "$\\bullet$ **Novelty of the method:** The proposed approaches for improving ReLU efficiency at the expense of FLOPs count are not novel at all. Previous work, such as CryptoNAS and Sphynx, has already explored this concept by maintaining a constant ReLU count per layer, which in turn increases the FLOPs count (however, these studies did not provide detailed FLOPs count information). Furthermore, this paper fails to introduce any fresh insights or observations that shed light on how networks can trade ReLUs for FLOPs. The insights presented in Section 4.4 of the paper are already well-established.\n\n$\\bullet$ **FLOPs-cost are ignore:** This is the core-issue with the paper. The authors presented the results with online latency and ignored their impact on end-to-end latency, a trend also seen in works like SENet, SNL, Sphynx, DeepReDuce, and CryptoNAS. Nonetheless, these prior work on PI improved the ReLU efficiency of the given baseline networks (mostly ResNets and WideResNets) *without increasing their FLOPs counts* (Although WidResNets have 4x to 5x higher FLOPs than ResNets). \n\nDelphi assumes that no matter how many FLOPs are there in networks, they can be processed offline. However, in real-world scenarios, private inference requests arrive at non-zero rates. Even at low arrival rates, processing the entire FLOPs offline becomes impractical due to limited computing resources, storage, communication bandwidth between server and client, and time constraints arising from the non-zero request-arrival rate. Consequently, offline costs are no longer truly offline, and FLOPs start affecting real-time performance, as illustrated in Figure 7 of the paper [1]. This effect can be exacerbated by networks with higher FLOP counts, as proposed by the authors.\n\n\n**The argument of online vs. offline latency is only valid if the optimization is performed for a single private inference in isolation.** FLOP penalties can only be disregarded when there are no inference arrivals or when an accelerator offering more than 1000x speedup is employed. Even with complete FLOP parallelization, such as using LPHE in [1], end-to-end performance improves but does not eliminate FLOP costs.\n\nThe authors cite [1] to support the claim that ReLU is 300x costlier than FLOPs, but this argument is valid only for online overhead. When considering end-to-end latency, FLOPs are shown to be 4.8x more expensive than ReLUs, as demonstrated in Table 1 of [1]. Therefore, the authors should have provided a FLOPs comparison with ResNet18, WRN22x8, and CryptoNAS.\n\nIn summary, *this paper does not contribute any new perspectives on private inference and fails to advance the understanding of current gaps in the field, rendering it less relevant for the ICLR audience.*\n\n\n[1] Garimella et al., \"Characterizing and Optimizing End-to-End Systems for Private Inference,\" ASPLOS 2023."
                },
                "questions": {
                    "value": "See the points in weakness.\n\nAdditionally, in line with the proposed method, increasing the network's width has been shown to  [2] [3] (however,  at the expense of FLOPs). How does the proposed method's trade-off between ReLUs and FLOPs counts compare to the straightforward approach of widening the baseline networks, such as ResNet18? \n\n\n\n[2] Doll\u00e1r et al., Fast and accurate model scaling. CVPR'21. \n\n\n[3] Lee et al.,  Wide neural networks of any depth evolve as linear models under gradient descent. NeurIPS'19."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5216/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5216/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5216/Reviewer_SDAb"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5216/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698683896656,
            "cdate": 1698683896656,
            "tmdate": 1699636519411,
            "mdate": 1699636519411,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JJJVixNbXR",
                "forum": "jU3zRzUBiD",
                "replyto": "lLRlcyqVxO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5216/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**1. Novelty of the method. **\n\nThanks for raising the concern. We agree that additional experiments to explore the tradeoff between FLOPs and ReLU counts can help provide more novel insights. However, we believe that our paper **already offers significant novelty and contributions**, and **the current experiments also shed some light on the above tradeoff**. \n\nFor CryptoNAS and Sphynx, their approach deviates from the PPML setting we adopt, and reduces the amount of ReLU using NAS at the expense of accuracy loss. Although they can introduce more linear operators, they will suffer from more ReLU counts with **one-to-one building blocks** instead of **many-to-one building blocks**. In contrast, we employ additional linear computation and fully reuse nonlinear operators to compensate for the accuracy loss, and achieve even better performance.\n\n\n**2. FLOPs-cost are ignored.**\n\nThank you for pointing out the ASPLOS\u201923 paper and the issue of offline HE computation cost for generating secret share triples. We do admit that in our design, more linear operators will incur more offline HE computations, and at high request arrival rates, it may be a bottleneck in the online throughput. However, the offline HE computations can be accelerated using layer-parallel HE (LPHE) by Garimella et al. or hardware accelerators.\n\nAs shown in Figures 5 and 6, even with the extra cost of computing more linear operators, Seesaw can still achieve better accuracy-latency tradeoffs compared to the baselines on real execution performance.\n\n\n**3. The argument of online vs. offline latency is only valid if the optimization is performed for a single private inference in isolation**\n\nAs exactly in the same ASPLOS\u201923 paper by Garimella et al., the offline latency can be greatly reduced by using more server computing resources to compute in parallel, even without expensive HE hardware accelerators. \n\nIn the table below, we compare our Seesaw models with the Delphi ResNet-32-300K baseline (300K ReLUs), completely considering both online and offline cost. The Delphi offline phase uses the ASPLOS\u201923 LPHE approach, and needs 14.44 seconds.\n\nBecause Seesaw proposes a spectrum of models with different tradeoffs, we choose two models: Seesaw-16K (16K ReLUs) that has **similar linear FLOPs (and thus similar offline HE cost) and similar accuracy** to the baseline, and Seesaw-45K that has **2x linear FLOPs and higher accuracy**. We assume the Seesaw models use the same amount of offline computing resources as the baseline above, so the offline HE time is proportional to the linear FLOPs.\n\n|                                 | Offline HE | Garbled Circuits (online) | Secret Sharing (online) | Communication (online) | Total |\n| ------------------------------- | ---------- | ---------------- | -------------- | ------------- | ----- |\n| Delphi (69% CIFAR-100 accuracy) | 14.44      | 24.87            | 0.65           | 5.33          | 45.29 |\n| Seesaw-16K (69% CIFAR-100 accuracy) | 16.12      | 0.96       | 0.73          | 0.84          | 18.65 |\n| Seesaw-45K (72% CIFAR-100 accuracy) | 26.33    | 2.01\t|0.99\t        |1.28\t|30.61    |\n\nWe see that even when limited to similar FLOPs as the baseline, Seesaw-16K can still save online ReLU cost while maintaining accuracy, due to a shallower network topology. It achieves **over 2.4x end-to-end (offline + online) speedup**.\n\nFor Seesaw-45K, even the offline HE cost is 2x more, the online save is even more significant to compensate for it, enabling end-to-end speedups. Although the speedup here is not very high, it is actually a conservative result, because **we can easily accelerate the offline part by using more servers, while the online part is difficult to accelerate**.\n\n\n**4. How does the proposed method's trade-off between ReLUs and FLOPs counts compare to the straightforward approach of widening the baseline networks, such as ResNet18?**\n\nThanks for your references. We may add such experiments for comparison in the final version. Wider networks can be helpful, as shown in SNL [1]. However, we also need to argue that widening the networks in Seesaw is not applicable because more channels will increase the consumption of ReLU budgets. We organize the parallel linear operators at some linear cost, but widening the networks results in both linear and nonlinear cost. \n\n\n[1]: Minsu Cho, Ameya Joshi, Brandon Reagen, Siddharth Garg, and Chinmay Hegde. Selective network linearization for efficient private inference. In the International Conference on Machine Learning, pp. 3947\u20133961. PMLR, 2022b."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700369997250,
                "cdate": 1700369997250,
                "tmdate": 1700369997250,
                "mdate": 1700369997250,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5zyfdBTeEn",
                "forum": "jU3zRzUBiD",
                "replyto": "JJJVixNbXR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5216/Reviewer_SDAb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5216/Reviewer_SDAb"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the Authors' Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the rebuttal. \n\n\n\nThe data presented in the rebuttal is cherry-picked (seesaw-16K and seesaw-45K). Those comparisons should be presented over a broad spectrum of  ReLU counts (till 150K to 200K on CIFAR-100). Most of my initial concerns are still not addressed, and I will keep my score. \n\nI hope the authors will include a detailed discussion and comparison of FLOPs with the previous methods, even when comparing only the online latency. Also, the argument of  **one-to-one building blocks vs. many-to-one building blocks** is unjustified; these are just different implementations to improve the ReLU efficiency at the expense of higher FLOPs count, not a point to show the novelty."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686378695,
                "cdate": 1700686378695,
                "tmdate": 1700686378695,
                "mdate": 1700686378695,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "50uvitknf4",
            "forum": "jU3zRzUBiD",
            "replyto": "jU3zRzUBiD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5216/Reviewer_68ze"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5216/Reviewer_68ze"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the issue of accuracy loss due to approximating the activation function during the implementation of a privacy-preserving artificial intelligence model. It proposes a method for modifying the model to add convolution operation blocks or reuse activation results to restore accuracy. In the process of modifying this model, it suggests utilizing Neural Architecture Search to automate the model adjustments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "When implementing information security artificial intelligence models using homomorphic encryption or multi-party computation, the issue of accuracy loss due to activation function approximation has been extensively discussed in many papers and is one of the fundamental problems to address. The approach of solving this problem by designing a Neural Architecture Search (NAS) with meaningful exploration directions, such as the addition of linear operations or activation reuse, is considered a novel method. While papers using NAS algorithms have existed before, I believe there is a novelty in setting the exploration directions. I consider this a valuable method that can be effectively used in subsequent papers to design information security artificial intelligence models."
                },
                "weaknesses": {
                    "value": "When reviewing the experimental results using this technology, I found it ambiguous whether the corresponding privacy-preserving machine learning model were actually implemented with the homomorphic encryption and the multiparty computation. If homomorphic encryption and multi-party computation were used, specific encryption parameters and communication amounts should be provided, but such information was not clearly presented. While the NAS algorithm itself is novel, the lack of thorough discussion on security during its implementation and validation makes this paper appear incomplete as a paper on privacy-preserving AI. Simply presenting a good algorithm may not be sufficient for approval if it's not clear whether actual cryptographic algorithms were used in the implementation."
                },
                "questions": {
                    "value": "1) Please all of the details about the cryptographic parameters in your implementation.\n2) Please give the communication costs for each result.\n3) Did you implement your models with homomorphic encryption and multiparty computation? or did you only compute the expected runtime for your results without the implementation of the privacy-preserving machine learning system?\n\nMy rating will be changed with these questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5216/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5216/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5216/Reviewer_68ze"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5216/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833623307,
            "cdate": 1698833623307,
            "tmdate": 1699636519334,
            "mdate": 1699636519334,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lSUnOnvfA2",
                "forum": "jU3zRzUBiD",
                "replyto": "50uvitknf4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5216/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**1. Please list all the details about the cryptographic parameters in your implementation.**\n\nThank you for pointing that out. We leverage the DELPHI framework to perform real performance experiments. We inherit its settings on our machines and get the real latency in Figure 5 and Figure 6, whose implementation works over the 41-bit prime finite field defined by the prime 2061584302081, and uses a 11-bit fixed-point representation (see https://github.com/mc2-project/delphi/tree/master). \n\n\n\n**2. Please give the communication costs for each result.**\n\nWe break the communication cost into offline and online phases and show them as follows:\n\n| Model               | Offline Communication/s | Online Communication/s | Accuracy |\n| ------------------- | ----------------------- | ---------------------- | -------- |\n| DELPHI (ResNet-32)  | 73.42                   | 3.86                   | 69       |\n| model-1             | 4.01                    | 0.21                   | 67.26    |\n| model-2             | 9.02                    | 0.47                   | 72.5299  |\n| model-3             | 11.03                   | 0.58                   | 72.95    |\n| model-4             | 19.05                   | 1                      | 73.31    |\n| model-5             | 25.06                   | 1.32                   | 74.54    |\n| model-6             | 35.09                   | 1.85                   | 74.659   |\n| model-7             | 65.16                   | 3.43                   | 75.439   |\n| model-8             | 75.18                   | 3.96                   | 76.66    |\n\nTable: Communication latency breakdown of Seesaw on CIFAR100.\n\n| model | offline communication/s | online communication/s |\n|-------|------------------------|-------------------------|\n| model-1 | 26.18 | 1.45 |\n| model-2 | 87.25 | 4.85 |\n| model-3 | 130.88 | 7.27 |\n| model-4 | 258.85 | 14.38 |\n| model-5 | 340.29 | 18.90 |\n\nTable: Communication latency breakdown of Seesaw on ImageNet.\n\nFrom the tables, we see that the reduction of ReLU can save a lot of online and offline communication cost compared to DELPHI with 69% accuracy.\n\n\n**3. Did you implement your models with homomorphic encryption and multiparty computation? **\n\nWe do not implement a new PPML system, but adopt the design of DELPHI (see https://github.com/mc2-project/delphi/tree/master), which is a state-of-the-art implementation of PPML using HE and MPC."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700369915035,
                "cdate": 1700369915035,
                "tmdate": 1700369915035,
                "mdate": 1700369915035,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "a29DSgtN1M",
            "forum": "jU3zRzUBiD",
            "replyto": "jU3zRzUBiD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5216/Reviewer_gfp9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5216/Reviewer_gfp9"
            ],
            "content": {
                "summary": {
                    "value": "The paper Seesaw presents a new approach in Privacy-preserving machine learning which overcomes state-of-the-art issues on the decrease of accuracy when reducing the amount of non linear operators for a given architecture. The authors designed a new Neural architecture search where they consider to use more linear computations and to reuse the results from non linear operators and thus compensate from having less of them. The introduced design also enables more representational capability of the model with the increase on the linear operators. The proposed design outperforms SOTA especially on Imagenet with better accuracy with fewer non linear operations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well presented with understandable figures on the design of Seesaw\n- The proposed approach gives competitive results compared to SOTA. Especially on Imagenet, we observe a big improvement"
                },
                "weaknesses": {
                    "value": "- The authors do not provide an analysis on the weighting parameters used in the loss function regarding the pruning of the linear branches and non linear operators.\n- The results on CIFA100 are less significant compared to the one on Imagenet\n- It would have been nice to have an additional dataset for the evaluation to see the result tendency compared to CIFAR100 and Imagenet"
                },
                "questions": {
                    "value": "- Is there a reason which explains why the results on accuracies on Imagenet and CIFAR100 are different with respect to SENet? We observe better results of Seesaw compared to SENet on Imagenet\n- Section 4.1, you say that with abundant ReLu budget, Seesaw can outperform Resnet models, do you prove this statement somewhere? or It is just the assumption knowing that you have more linear operators."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5216/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5216/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5216/Reviewer_gfp9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5216/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699459528511,
            "cdate": 1699459528511,
            "tmdate": 1699636519251,
            "mdate": 1699636519251,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vRPCvW5vI3",
                "forum": "jU3zRzUBiD",
                "replyto": "a29DSgtN1M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5216/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**1. Analysis on the weighting parameters used in the loss function regarding the pruning of the linear branches and nonlinear operators.**\n\nThank you for your question. We use Eq (1) to capture the loss from the variation of linear branch parameters, and Eq (2) to capture the nonlinear operator loss as the difference between the target ReLU budget and the current ReLU number. Combining these two losses, we introduce our optimization targets: finding the important linear operators and removing unnecessary nonlinear operators. We adjust $\\lambda_{lin}$ and $\\lambda_{nonlin}$ in Eq (3) to get the best network architecture (with dynamic learning rate from 0.05 to 0 for $\\lambda_{lin}$ and 0.001 and 0.1 for $\\lambda_{nonlin}$).\n\nFigure 8 and Figure 9 provide simple analysis on weight parameters. Figure 8 shows that the variance of sampling block branch weights is likely higher towards the backend of the network, reflecting that more linear operators are pruned under the threshold. Figure 9 shows the sampling blocks at the latter stage of the network tend to have higher ReLU weights and would keep the ReLU operators.\n\nTherefore, we get an observation that an optimized PPML network architecture needs to preserve sufficient nonlinearity in the latter blocks of the model, while at the earlier stage, it can instead increase the linear computations to increase the representation capacity.\n\n\n**2. The results on CIFAR100 are less significant compared to the one on Imagenet.**\n\nImageNet requires more expressive networks for higher resolution pictures, and Seesaw provides more linear operators to achieve that. However, on CIFAR100 with low resolution, the benefits of adding linear operators are relatively low after obtaining good feature extraction capabilities. At this point, the ReLU budget is the real determinant of accuracy.\n\n\n**3. Is there a reason which explains why the results on accuracies on Imagenet and CIFAR100 are different with respect to SENet? We observe better results of Seesaw compared to SENet on Imagenet**\n\nThe reason is similar to that of Question 2 above. \nSENet proposes fine-grained ReLU pruning algorithms, which can efficiently reduce the ReLU counts of existing networks. They choose ResNet18 and ResNet34 as targets, which already has enough expression ability for CIFAR100. However, linear operators of shallow ResNet are not enough for ImageNet.\nIn summary, simple networks can achieve good results on small datasets, but not on large datasets.\n\n\n**4. Section 4.1, you say that with abundant ReLU budgets, Seesaw can outperform Resnet models, do you prove this statement somewhere? or It is just the assumption knowing that you have more linear operators.**\n\nWe compare Seesaw with ResNet on Figure 3 and Figure 4, finding that with the same ReLU budget, Seesaw can outperform ResNet. However, we must admit that Seesaw achieves such performance with more linear operators."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700369871882,
                "cdate": 1700369871882,
                "tmdate": 1700369871882,
                "mdate": 1700369871882,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6x0BE61HDU",
                "forum": "jU3zRzUBiD",
                "replyto": "vRPCvW5vI3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5216/Reviewer_gfp9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5216/Reviewer_gfp9"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your answers. You provided the details I was expecting."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667048970,
                "cdate": 1700667048970,
                "tmdate": 1700667048970,
                "mdate": 1700667048970,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]