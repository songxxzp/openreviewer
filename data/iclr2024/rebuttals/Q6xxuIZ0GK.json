[
    {
        "title": "Gradient-norm Constrained Algorithm on Offline and Online Learning"
    },
    {
        "review": {
            "id": "CIbzQJiyTZ",
            "forum": "Q6xxuIZ0GK",
            "replyto": "Q6xxuIZ0GK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2893/Reviewer_daqm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2893/Reviewer_daqm"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a regularization method for training value and policy networks in offline & online RL settings. First, the paper incorporates a gradient-norm penalty to the target value that encourage the policy network to avoid choosing actions with high sharpness in the Q-function. Second, the paper introduces a regularizer based of the Q-function gradient that encourages the policy network to avoid the stationary points in the parameter space. The paper claims that the proposed method outperforms previous method in both offline and online settings."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Introducing a Q-function gradient norm regularizer to penalize overestimate Q-values is novel."
                },
                "weaknesses": {
                    "value": "- The proof of Proposition 1 within Appendix B appears to be wrong and necessitates a thorough review. Specifically, there is ambiguity regarding the representation of \"$a$\", which requires clarification: is it to be interpreted as a distribution or a vector? Also, since $\\pi(a \\mid s)$ is multivariate, its inverse cannot be defined. Moreover, the third line in Equation 21 seems wrong since the left matrix is of dimension $1 \\times |\\mathcal{A}|$ but the right matrix is of dimension $1 \\times \\theta$, which cannot be multiplied.\n- The paper omits several important baselines in both online and offline settings, including REDQ [1], EDAC [2], and PBRL [3].\n- The performance improvement in the offline setting seems very marginal.\n\n[1] Xinyue Chen et al., Randomized Ensembled Double Q-Learning: Learning Fast Without a Model, ICLR 2021\\\n[2] Gaon An et al., Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble, NeurIPS 2021\\\n[3] Chenjia Bai et al., Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning, ICLR 2022"
                },
                "questions": {
                    "value": "See the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2893/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2893/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2893/Reviewer_daqm"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2893/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697691610079,
            "cdate": 1697691610079,
            "tmdate": 1699636232681,
            "mdate": 1699636232681,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "h1bFLnu4cW",
            "forum": "Q6xxuIZ0GK",
            "replyto": "Q6xxuIZ0GK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2893/Reviewer_FbbA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2893/Reviewer_FbbA"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces gradient norm regularization for off-policy actor-critic methods for deep reinforcement learning, addressing key issues like the saddle point problem during parameter updates and distribution mismatches in off-policy methods. Additionally, the method is combined with behavior cloning for offline reinforcement learning applications."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper targets significant issues in RL, such as the saddle point problem and distribution mismatch in off-policy methods.\n2. The proposed policy regularization and value penalty provide some potentially new perspectives to address these challenges with theoretical intuitions.\n3. Empirical results show some benefits of the proposed regularization methods."
                },
                "weaknesses": {
                    "value": "1. In the experimental section, only DDPG, TD3, and SAC are included, and no other regularization-based approaches are included.  As introduced in the related work, there are many other approaches with regularizers and those could be missing baselines.\n2. Introducing gradient norm regularization would result in extra computation overhead since it needs to calculate the second-order derivative, and there is no discussion on the comparison of the compute. See Q2. \n3. There is no ablation of how sensitive the proposed method is with respect to the extra hyperparameter \\beta."
                },
                "questions": {
                    "value": "Q1. How is the sum over action space and the size of the action space in eq. (15) calculated in the continuous domain? I could imagine some discretization tricks but there is no reference for that in the paper.\n\nQ2. There could be some trade-off between computation overhead and performance. I wonder whether other simpler regularization methods, like the ones mentioned in the related works without second-order derivatives, could achieve similar performances."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2893/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2893/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2893/Reviewer_FbbA"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2893/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822564818,
            "cdate": 1698822564818,
            "tmdate": 1699636232573,
            "mdate": 1699636232573,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "uNIKvO6kMy",
            "forum": "Q6xxuIZ0GK",
            "replyto": "Q6xxuIZ0GK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2893/Reviewer_ar2S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2893/Reviewer_ar2S"
            ],
            "content": {
                "summary": {
                    "value": "This paper identifies two pertinent problems with off-policy RL algorithms, namely: the saddle point problem and value function exploitation problem. The authors propose to tackle them with 2 regularization objects around the Q-value gradients.\n\nTo mitigate the value function exploitation problem, the authors introduce a penalty term in the value target when performing policy iteration. Specifically, the penalty is a scaled norm of the Q-value's gradient with respect to actions. The hypothesis is that sharp Q-value landscapes could push the policy into OOD regions. To tackle the saddle point problem, the authors add an extra term when computing policy updates to encourage actions with non-negative gradients in the Q-value function. The rationale behind this is a pessimistic view where the action produced by the current policy is always treated as a saddle point.\n\nThe authors experiment with the proposed method in both online and offline RL settings on a few Mujoco locomotion tasks. The results show that the policies trained by the proposed GNC method generally converge to higher performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This work studies two fundamental challenges with off-policy RL with function approximators. A working solution has the potential for high impact.\n- The authors provide mathematic derivations to justify the penalty terms.\n- The experiment results support that GNC is outperforming SOTA RL algorithms."
                },
                "weaknesses": {
                    "value": "The main weakness of this paper is in the way it is presented.\n- Typos, wrong references, and dubious claims: There is a typo in the first sentence of the paper, which is not a good look. References to equations (10 vs. 22) (proposition 1 vs. 2) are intertwined, making it hard to read. Finally, the paper says RL is mainly limited to video games but in reality, RL has been applied to robot learning, real robot hardware, and even the training of large language models.\n- Poorly organized figures: In Figure 2, the text is not legible. The ordering of the experiments is inconsistent between the main results and the ablation studies. The plots of the ablation study should also contain the base GNC method.\n- The offline RL setting is potentially a good selling point of this paper, but the results are in the appendix.\n- The derivation of policy regularization might need some clarification (see questions)."
                },
                "questions": {
                    "value": "- GNC-VP seems to be completely identical to GNC in the humanoid environment. Maybe I missed the experiment details, but what would be the cause?\n- I don't fully understand the derivation of the objective for policy regularization. In equation (8), the expected gradient w.r.t. policy parameters $\\theta$ is the same shape as $\\theta$. Is it greater than or equal to zero element-wise? Also, the first paragraph of section 4.2 states that the gradient of Q w.r.t. actions should be non-negative, but in equation (10), the update rule seems to be penalizing positive gradients. Am I missing something here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2893/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2893/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2893/Reviewer_ar2S"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2893/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698950189713,
            "cdate": 1698950189713,
            "tmdate": 1699636232493,
            "mdate": 1699636232493,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]