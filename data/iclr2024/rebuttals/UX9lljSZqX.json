[
    {
        "title": "Unified Static and Dynamic: Temporal Filtering Network for Efficient Video Grounding"
    },
    {
        "review": {
            "id": "1L3ZND615P",
            "forum": "UX9lljSZqX",
            "replyto": "UX9lljSZqX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1579/Reviewer_GE2s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1579/Reviewer_GE2s"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles two video grounding problems based on natural and spoken language queries. Inspired by the human biology, the authors propose a novel framework, called UniSDNet, that enables both static and dynamic interactions to facilitate the learning of video grounding. The static interaction is implanted by a series of ResMLP layers, while the dynamic interaction is conducted by graph convolution with Gaussian Radial filters. Experimental results on three benchmarks for each task confirm the effectiveness of the proposed framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The manuscript is overall well-organized and easy to follow.\n+ The motivation behind the static and dynamic interactions based on the brain activity is clear and compelling.\n+ The two-stage information aggregation methods are shown to be effective, where each component is appropriately designed.\n+ The experimental results are very strong, clearly outperforming the existing approaches for both grounding tasks.\n+ The collected spoken language grounding datasets will significantly benefit the research community. The authors are encouraged to publish the code and data after the review process."
                },
                "weaknesses": {
                    "value": "I did not find major weaknesses in this paper, yet summarize some questions about the method below.\n\n- What is the motivation behind the implementation of Static Semantic Supplement Network? I am wondering how the cross-modal interaction is performed through the MLP layers. To my understanding, the shared weights across different modalities would extract some common features spanning different modalities. Some analytical experiments on this would be beneficial. Also, the architecture design seems similar to that of Transformer blocks except for the self-attention. What happens if we use the conventional Transformer layers?\n- The proposed architecture exploits multiple queries at once, to facilitate the model learning. However, how the number of queries affects the performance is not diagnosed. An ablative study on the number of queries regarding performance and cost would be helpful.\n- In Figure 5, the effectiveness of the proposed filtering GCN is clearly verified. On the other hand, there are some interesting tendency differences between NLVG and SLVG. That is, the graph convolution layer itself is important, yet different layer modeling brings insignificant performance gaps on NLVG. In contrast, on SLVG, the graph modeling brings negligible gains alone, but the proposed filtering mechanism shows substantial improvements. How can one interpret this phenomenon? If you have, please share some insights.\n- The proposed method is well validated in the datasets with one-to-one matching between queries and moments. How would it perform for one-to-many matching datasets, such as QVHighlights [1]?\n\n[1] Lei et al. \"QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries\", Neurips, 2021.\n\n(Minor)\n\nThe manuscript contains some formatting errors due to the excessively small margins between captions and the main text. They should be handled appropriately to raise the quality of the paper."
                },
                "questions": {
                    "value": "Please refer to the Weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1579/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1579/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1579/Reviewer_GE2s"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1579/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831720028,
            "cdate": 1698831720028,
            "tmdate": 1699636086630,
            "mdate": 1699636086630,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7mTt6Fg3Qm",
                "forum": "UX9lljSZqX",
                "replyto": "1L3ZND615P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1579/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1579/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The Response to Reviewer GE2s (Part 1/3)"
                    },
                    "comment": {
                        "value": "We are so encouraged and cheerful to receive your constructive comments, your positive reception of UniSDNet's biological motivation and recognition of our two-stage information aggregation methods are both encouraging and deeply appreciated. \n\n> **Q1: What is the motivation behind the implementation of Static Semantic Supplement Network? I am wondering how the cross-modal interaction is performed through the MLP layers. To my understanding, the shared weights across different modalities would extract some common features spanning different modalities. Some analytical experiments on this would be beneficial. Also, the architecture design seems similar to that of Transformer blocks except for the self-attention. What happens if we use the conventional Transformer layers?**\n\n**A1:**\nThank you for raising the concerns about the motivation of static network. We would like to explain the implementation of our static network from the perspectives of motivation, technology, and experiment.\n\n- **Motivation-wise,** the implementation of Static Semantic Supplement Network is strongly inspired by the **global neural workspace (GNW)** theory in biology [R15], that is, for video understanding, the brain will perform 'global broadcast communication' (interaction) of the multi-source information in the early stage of visual perception for the video, while the cognitive consciousness of the brain will **strengthen perceptual conditions** and **delay conditioned reflexes**, enabling the human brain to understand new conditioned reflexes brought about by perceptual information updates.\n\n    On the premise of the above theory, this inspires us to utilize residual networks structure to update our understanding of video sequences in the delaying conditioned reflex states by aggregating the perceptual information. **Multilayer perceptron (MLP)** is proven to be effective and has strong interpretability in modeling multi-level cognitive processing [R19], thus we use MLP to aggregate the perceptual information. The visual consciousness process is modeled as follows\uff1a$x^{n+1}$ = $\\text{ResMLP}(x^n)$ = $x^n$+$\\text{MLP}(x^n)$, where $x^n$ represents the perception condition in the current state, $x^{n+1}$ denotes the reflection result, and $\\text{ResMLP}(x^n)$ can enhance the information of the perception condition $x^n$.\n\n- **Technology-wise**,  we concatenate multiple queries and video clips together as a sequence input into the network for fully connected interaction of all tokens, its main function is to supplement and associate semantics between modalities before the latter graph filtering, provide more video descriptions information and significantly fill the gap between vision-language modalities, aiding in understanding video content.\n\n- **Experiment-wise**, as per your valuable suggestion, we have tested the effect of Transformer as a static network as shown in the table below (**Table R6**), and as you are concerned, Reviewer 'TT77' has also focused on the ablation study of the static module. From the results, in terms of performance and efficiency, Transformer is close to our method, but our results are better. We speculate that the reason is that our network also includes the second stage of graph filtering. The static network uses a lightweight and stable network, which is more conducive to model training. Using Transformer as a static network increases the weight and instability factors [R20] of the network. \n\n**Table R6: Results of different static networks on the ActivityNet Captions.**\n\n|Static | Infer. Speed (s/query) | R@1, IoU@0.3 | R@1, IoU@0.5 | R@1, IoU@0.7  |  R@5, IoU@0.3 | R@5, IoU@0.5  | R@5, IoU@0.7 | mIoU  | \n| :---------- | :----------: | :----------:  | :----------: | :----------:  | :----------:  | :----------:  | :----------:  | :----------: | \n| Transformer | 0.024 | 75.17 | 59.98 | 38.38 | 91.26 | 85.77 | 74.25 | 54.97| \n|**Our ResMLP** | **0.009** | **75.85**\t| **60.75**\t| **38.88**\t| **91.16** | **85.34** | **74.01**\t| **55.47** |\n\n**References:**\n\n[R15] Volzhenin et al., Multilevel development of cognitive abilities in an artificial neural network. Proceedings of the National Academy of Sciences, 119(39), pp. e2201304119, 2022.\n\n[R19] Chavlis et al., Drawing inspiration from biological dendrites to empower artificial neural networks. Current opinion in neurobiology. 70, pp. 1-10, 2021. \n\n[R20] Hugo et al., Resmlp: Feedforward networks for image classification with data-efficient training. TPAMI, 45(4):5314\u20135321, 2022."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584541506,
                "cdate": 1700584541506,
                "tmdate": 1700584541506,
                "mdate": 1700584541506,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wmunCJg35S",
                "forum": "UX9lljSZqX",
                "replyto": "1L3ZND615P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1579/Reviewer_GE2s"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1579/Reviewer_GE2s"
                ],
                "content": {
                    "title": {
                        "value": "Post-rebuttal Comment"
                    },
                    "comment": {
                        "value": "Thanks to the authors for the careful response.\n\nI believe most of my concerns are appropriately addressed.\n\nEspecially, the ablative experiments regarding the number of queries as well as the results on QVHighlights are substantially valuable.\n\nI will keep my rating unchanged with the hope that the authors will reflect on the discussion made during the rebuttal in the manuscript."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705940677,
                "cdate": 1700705940677,
                "tmdate": 1700705956737,
                "mdate": 1700705956737,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pi86s7onrw",
            "forum": "UX9lljSZqX",
            "replyto": "UX9lljSZqX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1579/Reviewer_Z5of"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1579/Reviewer_Z5of"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new approach to natural language-based (NLVG) and spoken language-based video grounding (SLVG). It first uses a MLP with residual connection to model the interaction between video feature and queries. Next, it proposes a graph network to model the short-term dynamics. The proposed model achieves great improvements on both NLVG and SLVG benchmarks and runs faster than the multi-query benchmark."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Good performance on both NLVG and SLVG benchmarks.\n\n2. It is nice to see an extension from NLVG to SLVG with a newly proposed benchmark. The proposed method proves effective on both tasks.\n\n3. Detailed implementation details and prediction analysis in the appendix."
                },
                "weaknesses": {
                    "value": "1. The inspiration from human visual perception biology is not very motivating. Specifically, it is hard to see why a MLP with residual connection is the way to achieve the \u201cglobal broadcast communication\u201d of the brain. Either bridge the gap or Simply drop the bio-inspiration and go straight into the technical method.\n\n2. When expanding a single gaussian kernel to multi-kernel Gaussian, it seems that only the bias $z_i$ is sweeping? Have you tried different $\\gamma$?\n\n3. Ablation in Fig 5 shows mostly similar results especially on NLVG, indicating that the designs in Dynamic Filter Graph actually do not quite matter."
                },
                "questions": {
                    "value": "1. Template\n\n(1) The first page is missing a header.\n\n(2) Please change `\\cite{..}` to `\\citep{..}` for clarity."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1579/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699075092855,
            "cdate": 1699075092855,
            "tmdate": 1699636086569,
            "mdate": 1699636086569,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0JhTBLqsBU",
                "forum": "UX9lljSZqX",
                "replyto": "pi86s7onrw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1579/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1579/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The Response to Reviewer Z5of (Part 1/3)"
                    },
                    "comment": {
                        "value": "Thank you very much for your recognition of our contributions to NLVG and SLVG tasks. Your constructive comments are exceedingly helpful for us to improve our paper. \n\n> **Q1: The inspiration from human visual perception biology is not very motivating. Specifically, it is hard to see why a MLP with residual connection is the way to achieve the 'global broadcast communication' of the brain. Either bridge the gap or Simply drop the bio-inspiration and go straight into the technical method.**\n\n**A1:**\nThank you very much for your attention to the design motivation of static networks (MLP with residual connection, ResMLP). Here, we clarify the motivation to bridge the gap between the biological theory 'global broadcast communication' and the technical design of static network ResMLP. \nSincerely, our work has been strongly inspired by the latest theories of human visual perception [R14, R15]. The **global neuronal workspace (GNW)** theory mentioned in [R15] is the foundation of conscious work in visual understanding [R15, R16, R17], it can achieve the fusion process of multi-source information in the early stage of visual event recognition, that is, the **\"global broadcast communication\"** of the brain. GNW theory indicates that cognitive consciousness has the function of **strengthening perceptual conditions** and **delaying conditioned reflexes** [R15, R18], enabling the human brain to understand new conditioned reflexes brought about by perceptual information updates [R15, R16].\n\nOn the premise of the above theory, this inspires us to utilize residual network structure to update our understanding of video sequences in the delaying conditioned reflex states by aggregating the perceptual information. **Multilayer perceptron (MLP)** is proven to be effective and has strong interpretability in modeling multi-level cognitive processing [R19], thus we use MLP to aggregate the perceptual information. The visual consciousness process is modeled as follows: $x^{n+1} = \\text{ResMLP}(x^n) = x^n+\\text{MLP}(x^n),$ where $x^n$ represents the perception condition in the current state, $x^{n+1}$ denotes the reflection result, and $\\text{ResMLP}(x^n)$ can enhance the information of the perception condition $x^n$.\n\nTechnically, in this work, we adopt a unified framework of static (ResMLP) and dynamic (graph filtering) structures for video grounding. In the early stage, this global broadcast mode of ResMLP first perceives all multimodal information, while the latter graph filter purifies key visual information. \nThis pre-broadcast learning mode of ResMLP is very necessary as a global biological perception mechanism and has also verified its effectiveness via extensive experiments, as discussed in the paper. We will add the explanation on the rationality of our static network design from a **technical perspective** and incorporate your suggestion into the paper writing by considering both motivation and technical implements for better clarification. \n\n**References:**\n\n[R14] Barbosa et al., Interplay between persistent activity and activity-silent dynamics in the prefrontal cortex underlies serial biases in working memory. Nature neuroscience, 23(8), pp. 1016-1024, 2020. \n\n[R15] Volzhenin et al., Multilevel development of cognitive abilities in an artificial neural network. Proceedings of the National Academy of Sciences, 119(39), pp. e2201304119, 2022.\n\n[R16] Cleeremans et al., Consciousness matters: Phenomenal experience has functional value. Neuroscience of consciousness. 2022, pp. niac007, 2022.\n\n[R17] Richards et al., A deep learning framework for neuroscience. Nature neuroscience, 22, pp. 1761-1770, 2019.\n\n[R18] Grover et al., Differential mechanisms underlie trace and delay conditioning in Drosophila. Nature, 603, pp. 302-308, 2022. \n\n[R19] Chavlis et al., Drawing inspiration from biological dendrites to empower artificial neural networks. Current opinion in neurobiology. 70, pp. 1-10, 2021."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582552852,
                "cdate": 1700582552852,
                "tmdate": 1700582552852,
                "mdate": 1700582552852,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OFRQYAs5FY",
            "forum": "UX9lljSZqX",
            "replyto": "UX9lljSZqX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1579/Reviewer_TT77"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1579/Reviewer_TT77"
            ],
            "content": {
                "summary": {
                    "value": "To address the Natural Language Video Grounding (NLVG) and Spoken Language Video Grounding (SLVG) problems, this paper introduces a Unified Static and Dynamic Network (UniSDNet). In which, the Static Network utilizes ResMLP layer to model global context while the Dynamic Network leverages the multi-kernel Temporal Gaussian Filter to build graph, where the gaussian filter leverages the temporal distance and semantic similarity. Extensive experiments demonstrate promising results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Most parts of the paper is well-written, clearly demonstrating the motivation,  methodology and experiments. The methodology part is kind of easy to follow. \n2. The idea is motivated from the human visual perception biology, which formulates an interesting story for this paper.\n3. Extensive experiments successfully demonstrate the effectiveness of each proposed component of this work, which is good.\n4. The visualization and figures are plus to show more intuitions.\n5.  The final results of this paper achieves the state-of-the-art from both efficiency and effectiveness perspectives."
                },
                "weaknesses": {
                    "value": "1. The introduction reads like a related work. It will be great to make more comparison between this work and previous work. Answering what is wrong with previous works? and where the efficiency and performance gain come from in this paper?\n2. This paper introduces some new/confusing terminologies with their own definition, which hurts the reading experience. For example, 'static semantic supplement network' and 'activity-silent mechanism' are actually the global context interaction.  \n3. Although the motivation of static and dynamic network is demonstrated, the justification of specific design is not enough. For example, in the static network, transformer architecture or the recent S4[1] architecture can also be used as long-range filter. Some ablation studies regarding either the performance or efficiency would be great to include.\n4. In the dynamic network, not sure why use Gaussian filter on the distance (d_{ij}). Can you provide more insights? why not directly use the distance.\n5. No notation for the 'FNN'. Is this the feedforward network?\n6. In the Figure 5, no notation/description for 'D'.\n\n[1]  Efficiently modeling long sequences with structured state spaces. ICLR 2021"
                },
                "questions": {
                    "value": "Is there any chance also leverage the audio signal into this work, formulating a multi-model graph?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1579/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699122467606,
            "cdate": 1699122467606,
            "tmdate": 1699636086496,
            "mdate": 1699636086496,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "p6s20YKIxu",
                "forum": "UX9lljSZqX",
                "replyto": "OFRQYAs5FY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1579/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1579/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The Response to Reviewer TT77 (Part 1/3)"
                    },
                    "comment": {
                        "value": "Thanks for your insightful and constructive feedback on our manuscript. Your positive reception of our visual perception biological motivation to guide network design and recognition of our method and experiments are both encouraging and deeply appreciated. We will make some clarifications in accordance with your suggestions.\n\n  \n> **Q1: The introduction reads like a related work. It will be great to make more comparison between this work and previous work. Answering what is wrong with previous works? and where the efficiency and performance gain come from in this paper?**\n\n**A1:** Thanks for your valuable suggestion. Technology-wise, previous methods mostly focus on solving a certain aspect of the Temporal Video Grounding (TVG) task, such as representation learning of language and video self-modality [R1, R2], multimodal fusion [R3, R4], cross-modal interaction [R5, R6], proposal candidate generation [R7, R8], proposal-based cross-modal matching [R9, R10], target moment boundary regression [R11, R12], and so on.\nOur work actually proposes a new paradigm to establish a two-stage unified static and dynamic semantic complementary new architecture. Its unique characteristics are that\n- Processing multimodal signals in a unified ResMLP network, while many previous works are independently encoding the language modality and video modality [R1-R7, R9-R12].\n- After the implementation of the above static ResMLP, we introduce a Gaussian nonlinear filtering method to learn the semantic associations within the video and combine it with the back-end proposal generation to promote cross-modal semantic alignment, further developing the proposal-based TVG method.\n\nOur work is inspired by visual perception biology. This unified static and dynamic two-stage architecture performs excellent joint learning of language and video, which achieves state-of-the-art performance on NLVG and SLVG tasks. \nThe ablation experiments in **Table 4** of our manuscript demonstrate the effectiveness of each proposed component of this work. We will improve our manuscript based on your constructive suggestions.\n\n**E\ufb00iciency and performance:** Technology-wise, the inference efficiency of our model comes from a streamlined architecture design, which allows for parallel inference of video and multiple queries in a unified ResMLP, saving time and expenses.\nOur promising model performance mainly comes from our model's comprehensive information interaction with query and video (ResMLP, static network), as well as our detailed inference of video content (DTFNet, dynamic graph filtering network), which was confirmed in the first ablation experiment in **Table 4** of the manuscript.\n\n**References:**\n\n[R1] Xia et al., Video-guided curriculum learning for spoken video grounding. ACM MM, pp. 5191\u20135200, 2022.\n\n[R2] Rodriguez et al., Memory-efficient temporal moment localization in long videos. ECAL, pp. 1901\u20131916, 2023.\n\n[R3] Li et al., Proposal-free video grounding with contextual pyramid network. AAAI, pp, 1902\u20131910, 2021. \n\n[R4] Liu et al., Exploring optical-flow-guided motion and detection-based appearance for temporal sentence grounding. IEEE TMM, 2023.\n\n[R5] Liu et al., Jointly cross-and self-modal graph attention network for query-based moment localization. ACM MM, pp. 4070\u20134078, 2020.\n\n[R6] Sun et al., Video moment retrieval via comprehensive relation-aware network. IEEE TCSVT, 2023.\n\n[R7] Zhang et al., Learning 2d temporal adjacent networks for moment localization with natural language. AAAI, pp. 12870\u201312877, 2020.\n\n[R8] Zhang et al., Multi-stage aggregated transformer network for temporal language localization in videos. CVPR, pp. 12669\u201312678, 2021. \n\n[R9] Gao et al., Fast video moment retrieval. ICCV, pp. 1523\u20131532, 2021.\n\n[R10] Zheng et al., Phrase-level temporal relationship mining for temporal sentence localization. AAAI, pp. 3669\u20133677, 2023.\n\n[R11] Zhang et al., Natural language video localization: A revisit in span-based question answering framework. IEEE TPAMI, 44(8), pp. 4252\u20134266, 2021.\n\n[R12] Liu et al., Skimming, locating, then perusing: A human-like framework for natural language video localization. ACM MM, pp. 4536\u20134545, 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581597020,
                "cdate": 1700581597020,
                "tmdate": 1700581597020,
                "mdate": 1700581597020,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uiqfpxjIRP",
                "forum": "UX9lljSZqX",
                "replyto": "OFRQYAs5FY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1579/Reviewer_TT77"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1579/Reviewer_TT77"
                ],
                "content": {
                    "title": {
                        "value": "Post rebuttal comment"
                    },
                    "comment": {
                        "value": "Thanks all authors for their effort in the rebuttal.\n\nThe rebuttal address most of my concerns, so I keep my rating suggesting acceptance of this work.\n\nThanks"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591087622,
                "cdate": 1700591087622,
                "tmdate": 1700591087622,
                "mdate": 1700591087622,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "q6tRFCz9qI",
            "forum": "UX9lljSZqX",
            "replyto": "UX9lljSZqX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1579/Reviewer_1gED"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1579/Reviewer_1gED"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes the Unified Static and Dynamic Network (UniSDNet) for video grounding, which establishes semantic associations between multiple text/audio queries and video content in a cross-modal environment. UniSDNet combines static and dynamic modeling techniques. For static modeling, it employs an MLP within a residual structure (ResMLP) to facilitate comprehensive interactions between video content and multiple queries, enhancing mutual semantic understanding. For dynamic modeling, the paper draws inspiration from human visual perception mechanisms and constructs a diffusive connected video clip graph to represent short-term relationships and employs a multi-kernel Temporal Gaussian Filter for complex visual perception simulation. UniSDNet achieves state-of-the-art performance on various NLVG and SLVG datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed Dynamic Temporal Filter Network captures more fine-grained context correlations between video clips based on a well-desgined graph network.\n2. The proposed method achieves state-of-the-art performance on NLVG and SLVG tasks.\n3. In this work, two new SLVG datasets are collected based on existing NLVG datasets.\n4. Compared with previous multi-queried methods, the proposed UniSDNet has less model parameters and is more efficient according to the average inference time per query."
                },
                "weaknesses": {
                    "value": "1. In ResMLP, visual features and multiple query features are concatenated and fed into the network, largely leveraging the information leakage between different queries (because the features incorporate more accurate textual information that describes the video content). If each query is individually input into the network, would this method exhibit a significant performance degradation?\n2. In the ablation study, individually employing the static network and DTFNet yields significant improvements compared to the baseline. However, the combination of both modules does not exhibit a notably large improvement compared to using either single module. Is there a specific explanation for this phenomenon? The authors should provide more details about the baseline models."
                },
                "questions": {
                    "value": "I have listed my major concerns and questions in the weaknesses. I hope the authors can provide more details of baseline models in the ablation study and some experimental results about comparing multi-query input and single-query input."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1579/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1579/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1579/Reviewer_1gED"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1579/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699311902072,
            "cdate": 1699311902072,
            "tmdate": 1699636086395,
            "mdate": 1699636086395,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LbgBp0FgIv",
                "forum": "UX9lljSZqX",
                "replyto": "q6tRFCz9qI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1579/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1579/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your recognition of our proposed Dynamic Temporal Filter Network, and your insightful reply is very helpful. We follow your advice and add the new experimental results about comparing multi-query input and single-query input.\n\n> **Q1: In ResMLP, visual features and multiple query features are concatenated and fed into the network, largely leveraging the information leakage between different queries (because the features incorporate more accurate textual information that describes the video content). If each query is individually input into the network, would this method exhibit a significant performance degradation?**\n\n**A1:** Thank you for your valuable feedback. Here, we attempt to clarify our conclusion from the following two aspects. On one hand, regardless multi-query or single-query modes, the role of Multilayer perceptron with residual design (ResMLP) is designed to capture the associations between query and video, simulating the brain's processing of multi-source information in the early stage of visual perception for the video. On the other hand, yes, we agree with your opinion that multiple queries can indeed provide more semantics to the video and significantly fill the gap between vision-language modalities, aiding in understanding video content.  \nTo eliminate your concern, we have supplemented the relevant experiments and found two facts in **Table R1**:\n\n- Our model still performs best in single-query input mode, compared to other single query methods. For example, our R@1, IoU@0.7 is 32.25, exceeding the current SOTA methods.\n\n- At present, there are some multi-query methods, but their advantages over the single-query methods are not obvious. How to better utilize multiple queries to assist in video grounding is also a challenge. Our method has advantages in handling both single-query and multi-query input modes. \n\n**Table R1: Model size vs. Infer. Speed vs. Performance comparison in different query input modes.**\n\n|(#Query) | Method | Model Size | Infer. Speed (s/Query) | R@1, IoU@0.3 | R@1, IoU@0.5 | R@1, IoU@0.7  |  R@5, IoU@0.3 | R@5, IoU@0.5  | R@5, IoU@0.7 | mIoU  |\n| :---------- | :---------- | :---------- | :----------: | :----------: | :----------: | :----------: | :----------: | :----------: | :----------: | :----------:| \n| Single | 2D-TAN  |  21.62M  |  0.061 | 59.45 | 44.51 | 26.54 | 85.53 | 77.13 | 61.96 | - |\n| Single | MS-2D-TAN  | 479.46M  |  0.141 | 61.04 | 46.16 | 29.21 | 87.30 | 78.80 | 60.85 | - |\n| Single | MSAT       |  37.19M  |  0.042 |   -   | 48.02 | 31.78 |   -   | 78.02 | 63.18 | - |\n| **Single** |**UNiSDNet**|**76.52M**|**0.009** |**68.66**|**52.35**|**32.25**|**89.74**|**83.35**|**70.61**|**50.22**|\n| Multi  | MMN       | 152.22M | 0.014 | 65.05 | 48.59 | 29.26 | 87.25 | 79.50 | 64.76 | - |\n| Multi  | PTRM      | 152.25M | 0.038 | 66.41 | 50.44 | 31.18 |   -   |   -   |   -   | 47.68 |\n| **Multi**  | **UNiSDNet**  |**76.52M**|**0.009**|**75.85**|**60.75**|**38.88**|**91.16**|**85.34**|**74.01**|**55.47**|"
                    },
                    "title": {
                        "value": "The Response to Reviewer 1gED (Part 1/2)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580903314,
                "cdate": 1700580903314,
                "tmdate": 1700581526044,
                "mdate": 1700581526044,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mEDnfevVwa",
                "forum": "UX9lljSZqX",
                "replyto": "q6tRFCz9qI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1579/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1579/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **Q2: In the ablation study, individually employing the static network and DTFNet yields significant improvements compared to the baseline. However, the combination of both modules does not exhibit a notably large improvement compared to using either single module. Is there a specific explanation for this phenomenon? The authors should provide more details about the baseline models.**\n\n**A2:** Thank you for raising the concerns about the main network modules. \nIn our manuscript, we propose a method that starts with static global review queries and video content (ResMLP), and then dynamically filters video content to extract important information (DTFNet). \nOn the one hand, observing the experimental results of our static module alone (ResMLP) compared with existing works (in Tables 1 and 4 of our manuscript), previous works have overlooked and performed insufficiently in extracting information from global static reviews.\nOn the other hand, even if the static review information is missed and only basic cross-modal information is obtained, we achieve good video grounding results using the dynamic module (DTFNet) alone. **Effective information filtering method (DTFNet)** can still extract useful information.\n\nFor convenience, we restate Table 4 of our manuscript in the following **Table R2**. Relatively speaking, the large performance improvement of a single module through the respective exploration of multimodal information has reached **saturation** (for NLVG task, the R@1, IoU@0.3 of ResMLP and DTFNet are 73.57 and 74.56, respectively.). \nIn this case, the R@1, IoU@0.3 of a single module increases from 73.57/74.56 to the final static-dynamic combination of 75.85, which is not a small improvement. \nThat's why the effect of combining ResMLP and DTFNet is not as much improved as employing them alone.\nThis further demonstrates the effectiveness of our static and dynamic modules. \n\n**Table R2: Ablation studies on the static and dynamic modules on the ActivityNet Captions and ActivityNet Speech datasets for both NLVG and SLVG.**\n|Task | Static Module | Dynamic Module | R@1, IoU@0.3 | R@1, IoU@0.5 | R@1, IoU@0.7  |  R@5, IoU@0.3 | R@5, IoU@0.5  | R@5, IoU@0.7 | mIoU  |\n| :----------: | :----------: | :----------: | :----------: | :----------: | :----------: | :----------: | :----------: | :----------: | :----------: |\n| NVLG | N | N | 61.22 | 44.46 | 26.76 | 87.19 | 78.63 | 63.60 | 43.98 |\n| NVLG | Y | N | 73.57 | 58.70 | 37.07 | 91.17 | 85.55 | 73.98 | 54.06 |\n| NVLG | N | Y | 74.56 | 59.45 | 37.44 | 90.98 | 85.43 | 73.60 | 54.43 |\n| **NVLG** | **Y** | **Y** | **75.85** | **60.75** | **38.88** | **91.16** | **85.34** | **74.01** | **55.47** |\n| SLVG | N | N | 53.63 | 35.91 | 20.51 | 84.71 | 74.21 | 55.95 | 38.23 |\n| SLVG | Y | N | 69.71 | 53.75 | 31.26 | 90.42 | 84.11 | 70.82 | 50.69 |\n| SLVG | N | Y | 71.34 | 54.03 | 31.51 | 89.75 | 82.62 | 68.12 | 50.97 |\n|**SLVG** | **Y** | **Y** | **72.27** | **56.29** | **33.29** | **90.41** | **84.28** | **72.42** | **52.22** |\n\n\n**About the baseline model**, we belong to the scope of the 2D proposal-based method. Specifically, we adopt the same backend decoding method as MMN (including the 2D proposal generation module). For training losses, we use the cross entropy loss between the predicted 2D proposal score map and groundtruth which is consistent with 2D-TAN, and add the contrastive loss proposed in MMN as an auxiliary constraint. That is to say, our method is based on the typical 2D temporal map's backend decoding architecture (2D-TAN and MMN), and by incorporating a static-dynamic module design as the fore-end implementation. \nOur work further develops the 2D proposal-based method in this field. \n\nWe hope our response addresses your concerns. If there are any further questions, please let us know. Thank you for your time and valuable feedback."
                    },
                    "title": {
                        "value": "The Response to Reviewer 1gED (Part 2/2)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580984188,
                "cdate": 1700580984188,
                "tmdate": 1700581554135,
                "mdate": 1700581554135,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rk6K0qBgpd",
                "forum": "UX9lljSZqX",
                "replyto": "mEDnfevVwa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1579/Reviewer_1gED"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1579/Reviewer_1gED"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks all authors for their effort in rebuttal. I think that Most of my concerns have been addressed. I would keep my rating and recommend the acceptance of this work."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706728494,
                "cdate": 1700706728494,
                "tmdate": 1700706728494,
                "mdate": 1700706728494,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]