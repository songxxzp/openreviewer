[
    {
        "title": "Defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics"
    },
    {
        "review": {
            "id": "tXGsa6mMrI",
            "forum": "RLhS1TrjK3",
            "replyto": "RLhS1TrjK3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1079/Reviewer_EDJD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1079/Reviewer_EDJD"
            ],
            "content": {
                "summary": {
                    "value": "The paper integrated previous public datasets and designed an annotation tool for fast annotation, thereby adding rich semantic annotations to the dataset. To further increase the diversity of defective images, the paper proposed a two-stage Diffusion Model to generate additional abnormal data. In addition, the paper conducted extensive experiments on segmentation methods based on CNN and ViT, establishing a reliable benchmark. Finally, the paper validated that the proposed data augmentation method improves the performance of segmentation methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper first proposes to add rich semantic information to pixel-level annotation. As far as I know, this is the first work in defect detection that performs semantic annotation at the pixel level.\n+ To achieve the goal of adding semantic-level pixel annotation, the paper designed a tool for defect annotation using SAM and Focal-Click.\n+ To enrich defective images, the paper designed a two-stage Defect Synthesis Diffusion Model.\n+ The paper conducted segmentation experiments on various segmentation models and established a reliable benchmark."
                },
                "weaknesses": {
                    "value": "+ The paper focused on semantic annotation of defects, but completely ignored the utilization of normal data. As a result, the dataset for defect detection is not much different from semantic segmentation datasets, which leads to the task of defect detection becoming more similar to semantic segmentation of natural images. This needs to be considered.\n+ Although the paper added rich semantic information to the dataset, its detection performance was not very good. Existing unsupervised defect detection methods have already surpassed an AUPR score of 0.7 at the pixel level, which means that existing defect detection methods can achieve an IoU of over 70 without using defect data. Some methods that use a large amount of normal data and a small amount of abnormal data have even higher performance."
                },
                "questions": {
                    "value": "+ I think the motivation behind the paper is very valuable, but perhaps the authors need to consider the differences from existing unsupervised anomaly detection methods.\n+ Although there are few methods that use abnormal data for detection, some methods still use a large amount of normal data and a small amount of abnormal data. Their main weakness is the inability to determine the type of defect, which may be a problem that the paper can solve. However, the paper may need to do some additional work beyond the data to design a solution for this problem."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1079/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1079/Reviewer_EDJD",
                        "ICLR.cc/2024/Conference/Submission1079/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1079/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698479252673,
            "cdate": 1698479252673,
            "tmdate": 1700656512564,
            "mdate": 1700656512564,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zi5KtDpRfA",
                "forum": "RLhS1TrjK3",
                "replyto": "tXGsa6mMrI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1079/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1079/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough and insightful feedback on our paper. Your expertise in unsupervised anomaly detection is evident, and we appreciate the depth of your review. Firstly, we\u2019d like to revisit our work. This paper primarily aims to pose a new question in defect detection research, rather than provide a definitive solution. We observed that current benchmarks in defect inspection research are approaching performance saturation. However, in real-world scenarios, defect inspection remains a challenging problem. This gap between academic research and industry applications prompted us to propose a new, more challenging benchmark. In this paper, we invite the research community to explore algorithms that can effectively address this benchmark.\nAs for your concerns. we fully understand and would like to address them as follows:\n\n**Inclusion of Anomaly Detection (AD) Methods**: In our benchmark, we focused on evaluating semantic segmentation methods, intentionally excluding unsupervised AD methods. The reason is that current AD methods are not equipped to provide masks for different defect types, which is crucial in our approach. We compute the mean Intersection over Union (mIoU) by averaging across different defect types, a metric where AD methods fall short as they cannot differentiate between these types.\nRegarding the concern about 'poor detection performance,' our use of mIoU for multi-class segmentation sets a higher standard than the binary classification task's Average Precision at a Recall (AUPR). Although our mIoU values are lower than AUPR scores reported in AD papers, they reflect a more robust defect inspection capability, as demonstrated in **Appendix Section. E** of our updated paper. Additionally, AD methods struggle with unaligned images, such as those in the VISION dataset, where variance is high.\n\n**Utilizing Normal Data**: We agree with your observation regarding the utilization of normal data. Normal data are indeed part of our defect spectrum dataset, initially included in our four subsets without requiring additional annotation. To address your concern about performance benchmarking, we have decided to incorporate normal data into our training set and re-evaluate our candidate methods. Due to time constraints in the rebuttal period, we have re-conducted a portion of the experiments from Table 3. Preliminary results shown in the table below (also see **Appendix Section. F** of our updated paper) demonstrate improved performance with the inclusion of a small amount of normal data by addressing the issue\nof over-penalizing non-defective areas. We plan to revise Table 3 accordingly, incorporating these findings.\n\nCombining different percentages of normal(defect-free) data. The source indicates the re-fined MVTec training set without any normal data\n|      | Source | +20% Normal       | +100% Normal  | +200% Normal  | +300% Normal  |\n|--------|--------|-------------------|---------------|---------------|---------------|\n| **Mean** | 51.58  | **53.87**        | 53.06         | 53.38         | 53.04         |\n\n**Differences from Semantic Segmentation**: While our task may appear similar to semantic segmentation, there are distinct challenges in handling industrial data. These include issues such as few-shot learning and the small area problem, which differentiate our task from standard semantic segmentation in natural images.\n\n**Future Directions and Community Involvement**: Our work aims to define a more complex defect inspection task in a practical manner and present augmented datasets for exploration. We leave the design of specific methods to the broader research community. Our paper's goal is to establish a foundation for researchers to adapt and leverage state-of-the-art methods within our proposed benchmark.\n\nFinally, I sincerely appreciate the valuable insights provided by the reviewers and warmly welcome any further criticisms or suggestions. Your timely feedback is crucial for the enhancement of this paper. I look forward to your valuable comments."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700309834384,
                "cdate": 1700309834384,
                "tmdate": 1700318750134,
                "mdate": 1700318750134,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xjcl9RiGoL",
                "forum": "RLhS1TrjK3",
                "replyto": "tXGsa6mMrI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1079/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1079/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer EDJD,\n\nAs the window for reviewer-author interaction is closing soon, on November 21st, I wanted to extend my sincerest gratitude for the invaluable time and effort you have dedicated to reviewing our work. To ensure that we have met your expectations, may I kindly ask if you find our responses satisfactory and if there are any remaining issues that need further clarification or improvement?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549267377,
                "cdate": 1700549267377,
                "tmdate": 1700549267377,
                "mdate": 1700549267377,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4sWLzUJbRd",
                "forum": "RLhS1TrjK3",
                "replyto": "tXGsa6mMrI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1079/Reviewer_EDJD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1079/Reviewer_EDJD"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the author for his detailed answer. Although there is a large gap between this work and existing research, I believe it is a valuable attempt. The experimental results that the author added using more normal samples are in line with expectations, but what I want more is some insight into how to better utilize normal samples. Taken together, I think the paper is a meaningful attempt. However, it's might suitable for some dataset track or workshop."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700569097483,
                "cdate": 1700569097483,
                "tmdate": 1700656780801,
                "mdate": 1700656780801,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "84ezTKGydI",
                "forum": "RLhS1TrjK3",
                "replyto": "tXGsa6mMrI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1079/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1079/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer EDJD,\n\nThank you for your invaluable comment. I noticed that you change the rating from 6 to 5 and added a comment of \u201cHowever, it's might suitable for some dataset track or workshop.\u201d\n\nI would like to clarify a key point regarding the submission category of our paper. We have submitted our paper under the **\"Primary Area: datasets and benchmarks\"** track. This submission choice aligns with the guidelines of ICLR, which, to our understanding, accepts papers focused on datasets.\n\nGiven this alignment with the submission track requirements, I kindly request you to reconsider the rating of our paper. We believe that our submission is well-placed in the datasets and benchmarks track, and we hope this clarification might positively influence your assessment."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691018133,
                "cdate": 1700691018133,
                "tmdate": 1700706325785,
                "mdate": 1700706325785,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NA5f0ZAYak",
                "forum": "RLhS1TrjK3",
                "replyto": "84ezTKGydI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1079/Reviewer_EDJD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1079/Reviewer_EDJD"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I changed my score primarily based on the opinion of **Reviewer VeNd**. The paper focuses mainly on datasets and benchmarks, but additional annotation work was done on other people's datasets. Besides, these captions and semantic annotations do not demonstrate an advantage over unlabeled data. The proposed annotation tool and generation method in the paper will contribute to the development of the field, but as a new dataset, the paper cannot complete the closed loop of problem formulation and solution."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710300839,
                "cdate": 1700710300839,
                "tmdate": 1700710300839,
                "mdate": 1700710300839,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wOMaVvUlSc",
            "forum": "RLhS1TrjK3",
            "replyto": "RLhS1TrjK3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1079/Reviewer_VeNd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1079/Reviewer_VeNd"
            ],
            "content": {
                "summary": {
                    "value": "This paper refines the existing defect inspection datasets by introducing more detailed annotations and additioinal synthetic data from the diffusion generator. The experimental results demonstrate partial effectiveness of the refined datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "+ This paper provides more detailed annotations for the existing defect inspection datasets, which can partially be benefit to the development of defect inspection."
                },
                "weaknesses": {
                    "value": "+ This paper is more suitable for a demos track, which only displays its rough progression of dataset refinement, but lacks technique detail presentation. The important details about auxiliary annotation tool and diffusion generator for defect image generation are not clearly clarified. If this refined dataset and annotations are private (not publicly available), the contributions to the development of defect inspection are very limited. \n\n+ The comparative experiments between the refined dataset and the baseline dataset are missing in Tab.3. The results in Tab.3 can not illustrate the advantages of the refined dataset. \n\n+ The results of VISION dataset in Tab.4 is somewhat trival. The authors should analyze these results. The improvements of refined dataset should be clearly clarified, such as how many synthetic data are supplemented and the performance improvement relative to the increment of dataset size."
                },
                "questions": {
                    "value": "See in Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1079/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1079/Reviewer_VeNd",
                        "ICLR.cc/2024/Conference/Submission1079/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1079/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646897433,
            "cdate": 1698646897433,
            "tmdate": 1700791189103,
            "mdate": 1700791189103,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H48yPk3y6Y",
                "forum": "RLhS1TrjK3",
                "replyto": "wOMaVvUlSc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1079/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1079/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful observations. We address each of your concerns as follows: \n\n**Open Source Issue**: We completely understand your concern regarding the open-source of our dataset. We have now made these resources publicly through this anonymous link: https://defect-spectrum-authors.github.io/defect-spectrum/, ensuring that our contributions to the field of defect inspection are substantial and accessible to the research community.\n\n**Comparison between Refined and Baseline Dataset**: To address your concern about the comparative experiments. Since both the training set and testing set were being refined, it is hard to compare the baseline-trained model quantitatively using the refined testing set or the source testing set. Thus, we qualitatively compared the segmentation model trained on the baseline dataset and our refined dataset, as shown in the **Appendix section. D** of our updated paper. The results with our annotation demonstrate 1) a more granular segmentation mask; 2) a more comprehensive detection (including those that remain unlabeled in the baseline dataset); 3) captures a wide range of defective types with better granularity while having rich semantics (different defective classes). \n\n**Clarification on Table 4 and Synthetic Data Utilization**: We appreciate your comments on Table 4 and the need for clarification regarding synthetic data. Regarding the results in Table 4, we would like to clarify that we have selected the best baseline to perform our performance boost experiment. We think although the improvement is comparatively smaller than the increase in the MVTec dataset, it still demonstrates the effectiveness of the synthetic data. We also extended the experiment to compare using three other baselines: M2F, MiT-B0, and DeepLabV3+, the experiments can be found in the table below (also see **Appendix Section. A** in the updated paper). \n\nPerformance (mIoU) comparison between models trained with and without synthetic data. The bolded text indicates results with synthetic data.\n| Model        | MVTec                     | VISION                    | Cotton                     |\n|--------------|---------------------------|---------------------------|----------------------------|\n| DeepLabV3+   | 51.58/**55.55**           | 52.33/**53.46**           | 48.73/**58.58**            |\n| Mask2Former  | 45.70/**50.16**           | 54.12/**55.47**           | 64.09/**65.39**            |\n| MiT-B0       | 46.45/**56.21**           | 49.62/**50.75**           | 50.52/**55.86**            |\n\n**Generation Data Ratio Experiment**: Further, as shown in Figure. 8, our experiments include a detailed analysis of the generation data ratio, where we examine the impact of varying proportions of synthetic data on model performance. This provides a comprehensive view of how synthetic data supplementation affects defect detection capabilities. We are sorry for the confusion it brought and will adjust it by extending the experiment as we mentioned in the response to reviewer 2(qXne), the results for the extended synthetic data ratio experiment can be found in **Appendix Section. B** of our updated paper.\n\nFinally, I sincerely appreciate the valuable insights provided by the reviewers and warmly welcome any further criticisms or suggestions. Your timely feedback is crucial for the enhancement of this paper. I look forward to your valuable comments."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700309710775,
                "cdate": 1700309710775,
                "tmdate": 1700319759598,
                "mdate": 1700319759598,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x4uOO62VfG",
                "forum": "RLhS1TrjK3",
                "replyto": "wOMaVvUlSc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1079/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1079/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer VeNd,\n\nAs the window for reviewer-author interaction is closing soon, on November 21st, I wanted to extend my sincerest gratitude for the invaluable time and effort you have dedicated to reviewing our work. To ensure that we have met your expectations, may I kindly ask if you find our responses satisfactory and if there are any remaining issues that need further clarification or improvement?"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549249948,
                "cdate": 1700549249948,
                "tmdate": 1700549249948,
                "mdate": 1700549249948,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TqwI74vA2d",
                "forum": "RLhS1TrjK3",
                "replyto": "wOMaVvUlSc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1079/Reviewer_VeNd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1079/Reviewer_VeNd"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. My concens are not well addressed.\n+ The publication of captions and synthetic data. The authors only publish the refined annotation about 4 public datasets. The most important captions and synthetic data are totally unavailable. The method for data generation is the most important part for this paper. However, the solution and synthetic data are not well clarified.\n+ The source code only provides the diffusion generator for configuration file of cotton fabric dataset. The configurations for other datasets are unavailable. \n+ The important details about auxiliary annotation tool and diffusion generator for defect image generation are not clearly clarified. \n+ The visualization results cannot effectively support the superority of the proposed dataset. I would like to see more quantitative results.\n+ I still think the results of VISION dataset in Tab.4 is somewhat trival.\n\nHence, I keep the previous score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574227492,
                "cdate": 1700574227492,
                "tmdate": 1700574279291,
                "mdate": 1700574279291,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gQ7JtzHVs1",
                "forum": "RLhS1TrjK3",
                "replyto": "Hs2PA0rMVh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1079/Reviewer_VeNd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1079/Reviewer_VeNd"
                ],
                "content": {
                    "comment": {
                        "value": "Additionally, I suggest that the authors should change the \"Large-Scale Defect Datasets\" in the paper title. \nThe size of dataset is below 10K.  The description of  \"Large-Scale Defect Datasets\" is overclaimed."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711354412,
                "cdate": 1700711354412,
                "tmdate": 1700711354412,
                "mdate": 1700711354412,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XCPoUp2SeN",
            "forum": "RLhS1TrjK3",
            "replyto": "RLhS1TrjK3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1079/Reviewer_qXne"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1079/Reviewer_qXne"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents the Defect Spectrum, a novel dataset that provides rich and accurate defect annotations for industrial defect inspection. The main advantages are two folds. First, it contains very accurate annotations, and very fine-grained semantics, that are critical in modern industrial manufacturing pipelines. Second, it presents a data generation solution that can generate high fidelity samples with annotations, which further increases the number of samples for training."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. This work addresses the accuracy of masks and richness of defect annotations, which are overlooked in by previous works. The mask annotation is much more accurate than previous approaches. And to the best of my knowledge, the number of defect type is much more than previous works. This is quite important for practical industrial manufacturing. \n\n2. The data generation approach, albeit simple, seems quite interesting. By restraining the receptive field, the proposed approach models global and patch-level distribution in a structured way, allowing for learning from few defect samples. The effectiveness is also demonstrated by experiments like Fig. 8 and Table 4."
                },
                "weaknesses": {
                    "value": "1. Although the dataset contains descriptive captions, they are not used in the defect inspection baselines. It is not clear with me why these descriptive captions are essential and how they are used in practical systems? To me, it seems that captions are not as accurate as the provided semantic masks. And in what way can these captions improve the defect inspection system?\n\n2. Figure 8 looks quite interesting, but it seems a bit incomplete. In Fig. 8, there is a clear trend that when adding more synthetic samples, the performance of both approaches is boosted. Thus, It would be more interesting if results of further increasing the ratio of synthetic data can be shown. \n\n3. It seems the the sampling strategy for generated samples could be improved. In the paper, it seems that the quantity of generated samples is at a fixed ratio to the real samples. In this sense, if there is a lot of real data for certain object, then model would generate a lot of synthetic data, and vice versa. This is a bit counter-intuitive to me. It seems more reasonable that for those objects with limited number of real data, we should generate more synthetic data to mitigate the overfitting problem."
                },
                "questions": {
                    "value": "Overall it is a good paper. Please address my concerns in the weakness section to make it stronger, e.g., make it make it clearer hof ow the captions benefit practical models, and provide more in-depth analysis to the sampling strategy of generated samples."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1079/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764307010,
            "cdate": 1698764307010,
            "tmdate": 1699636034197,
            "mdate": 1699636034197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EXpiC17HJN",
                "forum": "RLhS1TrjK3",
                "replyto": "XCPoUp2SeN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1079/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1079/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful observations. We address each of your concerns as follows: \n\n**Importance of Image-Text Pair Datasets**: Image-text pair datasets are crucial for training Multimodal Large Language Models(MLLMs), which have shown their dominant capabilities in a large range of tasks. Our descriptive captions offer additional contextual information that can be leveraged by these models. We look forward to empowering MLLMs with the ability of defect inspection through this effort.\n\n**Extended Synthetic Data Ratio Experiments**: To address your concerns, we have extended our experiments to include scenarios with higher ratios of synthetic data, shown in the **Appendix Section. B**(Please refer to the updated paper). Specifically, we have now included results for systems trained with 200% and 300% synthetic data ratios. When using synthetic data that is 200% of the size of the original training set, there is an enhancement in the performance, but results in greater variance. Additionally, the performance starts to decrease after reaching 300%. This is due to the overly-used synthetic data disrupting the original data distribution. \n\n**Sampling Strategy**: As an immediate response to your suggestion, we conduct an experiment where we employ a uniform sample strategy \u2013 using a fixed number (e.g., 150) of samples in the training set for each object. So those with limited data gain more synthetic data and vice versa. As shown in the table below (also see **Appendix Section. C**), with this sampling strategy, baselines such as MiT-B0 show a great leap in performance.\n\n|       | Baseline: **DeepLabV3+** | With Synthetic | Baseline: **MiT-B0** | With Synthetic |\n|--------|--------------------------|----------------|---------------------|----------------|\n| **Mean** | 51.58             | **54.87**      | 46.45           | **56.21**      |\n\nFinally, I sincerely appreciate the valuable insights provided by the reviewers and warmly welcome any further criticisms or suggestions. Your timely feedback is crucial for the enhancement of this paper. I look forward to your valuable comments."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700309636742,
                "cdate": 1700309636742,
                "tmdate": 1700318702261,
                "mdate": 1700318702261,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "I3bvu8tHoV",
                "forum": "RLhS1TrjK3",
                "replyto": "XCPoUp2SeN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1079/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1079/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer qXne, \n\nAs the window for reviewer-author interaction is closing soon, on November 21st, I wanted to extend my sincerest gratitude for the invaluable time and effort you have dedicated to reviewing our work. To ensure that we have met your expectations, may I kindly ask if you find our responses satisfactory and if there are any remaining issues that need further clarification or improvement?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549223382,
                "cdate": 1700549223382,
                "tmdate": 1700549223382,
                "mdate": 1700549223382,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "z8gckPY14R",
                "forum": "RLhS1TrjK3",
                "replyto": "I3bvu8tHoV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1079/Reviewer_qXne"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1079/Reviewer_qXne"
                ],
                "content": {
                    "comment": {
                        "value": "Thank for the detailed responds. The authors have addressed my concerns and I keep my original rating."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632705494,
                "cdate": 1700632705494,
                "tmdate": 1700632705494,
                "mdate": 1700632705494,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "22p19jNUoe",
            "forum": "RLhS1TrjK3",
            "replyto": "RLhS1TrjK3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1079/Reviewer_J4Kk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1079/Reviewer_J4Kk"
            ],
            "content": {
                "summary": {
                    "value": "Based on four industrial defect detection benchmarks, namely MVTec, VISION V1, Cotton-Fabric, and DAGM2007, this paper creates a new hybrid dataset by refining the existing annotations and differentiating multiple defect types within each image. Additionally, this paper propose Defect-Gen, a two-stage diffusion-based generator meant to generate a diverse set of defective synthetic images when dealing with limited datasets. The authors conduct experiments to evaluate the diversity of the proposed dataset, as well as the quality of the synthetic data and its potential to enhance the inference performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- It 's generally an interesting and comprehensive study. \n\nThe authors provide an integrated defect benchmark with fine-grained annotations, as well as a diffusion-based defect generation method to augment the dataset. \n- The methodology is generally clarified and easy to follow.\n\nBoth the dataset reconstruction process and the technical details of the data generation method are clearly explained.   \n\n- An integrated and refined dataset is beneficial for the defect segmentation research community."
                },
                "weaknesses": {
                    "value": "- unclear motivation of the choice of the base benchmarks\n- insufficient experiments"
                },
                "questions": {
                    "value": "My primary concerns are focused on the experiment section.\n\nFirstly, I have some questions regarding the quantitative evaluation results of multiple defect segmentation methods across the different Defect Spectrum reannotated datasets presented in Table 3. The table shows that the average performance of these methods is quite similar, despite their varying performance on individual defect image types. This raises doubts about the suitability of this benchmark for assessing the discrimination ability of different models.\n\nSecondly, there are limitations to the quantitative evaluation of the generation quality of the proposed data generation method. In Table 4 of section 4.4, only the performance of the best baseline is reported, which may not be sufficiently persuasive.\n\nFurthermore, I am curious about the reasons behind choosing to integrate and reannotate those four datasets from the diverse benchmarks listed in Table 1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1079/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765122481,
            "cdate": 1698765122481,
            "tmdate": 1699636034113,
            "mdate": 1699636034113,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yYON4jbCjX",
                "forum": "RLhS1TrjK3",
                "replyto": "22p19jNUoe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1079/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1079/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful query regarding our paper, we will address your concerns one by one: \n\n**Concerns on the quantitative evaluation result**: The concern about the similar average performance of various methods across different datasets, despite their varying performance on individual defect image types, is a valid point. However, if we look at the mean IOU report in Table 3, the min and max MIOU reported are 45.40 and 51.58. The segmentation results on the CityScapes dataset across various baselines also demonstrated similar MIOU results between 76.9 to 81.6, except for the traditional UNet(69.1).  As for the varying performance on individual classes, this is due to the different granularity that exists in different defect types, making some of the defective regions comparatively smaller than other types.  \n\n**Extended experiments on other baselines**: The decision to select the best baseline aims to demonstrate Defect-Gen's capability in the most demanding scenarios, particularly where baseline performance is already high. This approach highlights the robustness and effectiveness of our method, even under challenging conditions. To further underscore the efficacy of Defect-Gen, we conducted comprehensive experiments using three distinct segmentation models: Mask2Former, MIT-B0, and DeepLabV3+, as detailed in the table below (also see **Appendix Section A** in the updated paper). The results from these models reinforce our findings, demonstrating that Defect-Gen not only excels with top-performing methods but also significantly enhances the performance of less advanced models. This broad applicability underscores the versatility and practical utility of our approach.\n\nPerformance (mIoU) comparison between models trained with and without synthetic data. The bolded text indicates results with synthetic data.\n| Model        | MVTec                     | VISION                    | Cotton                     |\n|--------------|---------------------------|---------------------------|----------------------------|\n| DeepLabV3+   | 51.58/**55.55**           | 52.33/**53.46**           | 48.73/**58.58**            |\n| Mask2Former  | 45.70/**50.16**           | 54.12/**55.47**           | 64.09/**65.39**            |\n| MiT-B0       | 46.45/**56.21**           | 49.62/**50.75**           | 50.52/**55.86**            |\n\n**The reason to choose MVTEC, VISION, DAGM, and Cotton datasets**:\n**MVTEC Dataset**: Renowned for its high quality, the MVTEC dataset is a well-established benchmark in the field. Its high-resolution images and large-scale nature provide a robust testbed for models, enabling detailed defect detection. \n\n**VISION Dataset**: as the largest scale dataset among those we evaluated, VISION has high variance and high-resolution images. This dataset challenges models with a wide array of defect types and imaging conditions, making it an excellent benchmark for assessing a model's versatility. \n\n**DAGM Dataset**: Unique in its use of special imaging techniques, DAGM provides images that are far from natural representations. This characteristic is crucial for testing a model's ability to generalize and perform in less conventional or more challenging imaging environments. \n\n**Cotton Dataset**: As a real-world industry dataset produced directly in factory settings, the Cotton dataset offers high variance and practical relevance. It simulates real-world industrial conditions, thereby providing a realistic assessment of a model's performance in actual deployment scenarios.\n\nFinally, I sincerely appreciate the valuable insights provided by the reviewers and warmly welcome any further criticisms or suggestions. Your timely feedback is crucial for the enhancement of this paper. I look forward to your valuable comments."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700309530853,
                "cdate": 1700309530853,
                "tmdate": 1700319678387,
                "mdate": 1700319678387,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LsEOsTZTnx",
                "forum": "RLhS1TrjK3",
                "replyto": "22p19jNUoe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1079/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1079/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer J4Kk, \n\nAs the window for reviewer-author interaction is closing soon, on November 21st, I wanted to extend my sincerest gratitude for the invaluable time and effort you have dedicated to reviewing our work. To ensure that we have met your expectations, may I kindly ask if you find our responses satisfactory and if there are any remaining issues that need further clarification or improvement?"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549199559,
                "cdate": 1700549199559,
                "tmdate": 1700549199559,
                "mdate": 1700549199559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rcRPaF2mbI",
                "forum": "RLhS1TrjK3",
                "replyto": "LsEOsTZTnx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1079/Reviewer_J4Kk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1079/Reviewer_J4Kk"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the detailed responses. They have addressed my concerns, and as a result, I have decided to maintain my initial rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1079/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570146295,
                "cdate": 1700570146295,
                "tmdate": 1700570146295,
                "mdate": 1700570146295,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]