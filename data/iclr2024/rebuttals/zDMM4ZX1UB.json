[
    {
        "title": "Exploiting Code Symmetries for Learning Program Semantics"
    },
    {
        "review": {
            "id": "ACm5w7GwRr",
            "forum": "zDMM4ZX1UB",
            "replyto": "zDMM4ZX1UB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8801/Reviewer_3XTL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8801/Reviewer_3XTL"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes SymC, an approach to training a Transformer architecture that is invariant/equivariant to dependence-preserving reordering of code. SymC's formal foundation is a group theoretic definition of invariance and equivariance. The paper defines this symmetry group over program interpretation graphs, graphs whose nodes are program instructions and whose edges indicate whether there is any execution path in which there is a direct dependence between data computed by the two instructions. The paper then relaxes this to the sound overapproximation of dependence graphs, and shows an implementation of self-attention that is equivariant to actions of this symmetry group. The paper evaluates the proposed model on a range of invariant tasks, evaluating on code transformations that fall within and outside the scope of the invariance. The paper shows that SymC is competitive or surpasses baseline models on nearly all metrics of interest."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* Broadly, the paper is very well written. The paper provides a clear description of all the background knowledge of symmetry groups, clearly grounds the theory in the task (symmetry groups of code ordering), explains the implementation reasonably well (invariant/equivariant self-attention), and has a clear evaluation.\n* The problem domain itself is interesting and important, and the proposed solution is novel\n* The evaluation is quite extensive, with strong results on all metrics of interest"
                },
                "weaknesses": {
                    "value": "* The one part of the paper that could be more clear is in the precise discussion of the implementation of SymC in a Transformer. Specifically:\n  * I found the definition of the Aut(PDG) distance matrix to be a bit hard to reason about\n  * I also wasn't sure what the relationship between this distance matrix and the standard use of positional encodings is.\n* I would appreciate some discussion of why the F1/AUC results in Table 2 are not monotonic in the percentage of semantics-preserving permutations.\n* The evaluation also lacks any quantification of variance in the results (e.g., standard error across different training trials).\n* Minor typo: Section 3.3: \"tofuture\""
                },
                "questions": {
                    "value": "* Does SymC use positional embeddings?\n* Why are the F1/AUC results in Table 2 not monotonic in the percentage of semantics-preserving permutations?\n* Could the authors provide examples of code where the relaxation of the interpretation graph to the program dependence graph is too conservative?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8801/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698347836977,
            "cdate": 1698347836977,
            "tmdate": 1699637106633,
            "mdate": 1699637106633,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xbctvyEt2a",
                "forum": "zDMM4ZX1UB",
                "replyto": "ACm5w7GwRr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We really appreciate your constructive comments. We address each of your points below.\n\n**Weakness 1.1: The definition of the Aut(PDG) distance matrix to be a bit hard to reason about**\n\nWe will clarify the definition of the distance matrix used in our paper. In general, there can be various distance matrices to represent the graph in self-attention layers and preserve automorphisms as long as it satisfies the two properties of Lemma 4 & 5 (see proofs in Lemma 6 in Appendix). We chose to represent the distance between two nodes using their distance to their lowest common ancestor, based on the intuition that this can potentially represent how they are related based on program dependencies, e.g., branch condition, same source of dataflow, etc.\n\n**Q1 & Weakness 1.2: Relationship between this distance matrix and the standard use of positional encodings**\n\nSymC does not use absolute positional embeddings fed to the input sequences. Instead, it follows a similar setting as relative positional embedding, which biases the attention computation, i.e., the product of query and key, with the program structure. Unlike regular relative positional embedding, where the distance between the pair of nodes is still computed based on their relative distance in the linear sequence, e.g., the distance between token 2 and token 5 is (5-2=3), we compute the relative distance based on the customized distance matrix using PDG, and we prove the self-attention biased by this distance matrix preserves the automorphism of PDG, thus the semantics-preserving code permutations."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541683889,
                "cdate": 1700541683889,
                "tmdate": 1700544496273,
                "mdate": 1700544496273,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JpfS2R2YpV",
                "forum": "zDMM4ZX1UB",
                "replyto": "ACm5w7GwRr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q4: Illustrate cases where the relaxation of the interpretation graph to the program dependence graph is too conservative**\n\nAs described in our response to Reviewer 6HuJ, consider the case in binary analysis:\n\n```asm\n\u2026\n1       mov rcx, [rbx]\n2       mov [rax], rdx\n\u2026\n```\nWe have 2 nodes in PDG, i.e., {1,2} denoting 2 instructions, and the edge {1->2} because we can not easily eliminate the possibility that `[rbx]` and `[rax]` would point to the same memory location. This can lead to the spurious edge, rendering the permutation of 1 and 2 an invalid automorphism. In memory-bound applications, memory-accessing instructions can be very frequent. For these programs, the size of the automorphism group can indeed be very small (not many valid semantics-preserving permutations), restricting the number of possible permutations."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541772053,
                "cdate": 1700541772053,
                "tmdate": 1700542240498,
                "mdate": 1700542240498,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PIQ06LvDmV",
            "forum": "zDMM4ZX1UB",
            "replyto": "zDMM4ZX1UB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8801/Reviewer_Srm2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8801/Reviewer_Srm2"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces SymC, a novel approach that leverages code symmetries, defined through group-theoretic principles, to enhance large language models (LLMs) for program analysis. By embedding a group-equivariant self-attention mechanism within the Transformer architecture, SymC achieves significant improvements in understanding program semantics. The method demonstrates strong generalizability across various code transformations, outperforming existing state-of-the-art LLMs, including WizardCoder and GPT-4, by substantial margins in four specific tasks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper presents a unique and innovative approach to harnessing code symmetry, grounded in group theory, which stands out from previous methods that rely on ad-hoc heuristics. Instead of using these transformations for data augmentation, as is common in prior work, SymC ingeniously incorporates them into the attention layers of Transformers, showcasing a novel application.\n\n2. SymC's performance is noteworthy, as it surpasses the baselines across the various tasks presented in the paper, sometimes by a large margin."
                },
                "weaknesses": {
                    "value": "1. The paper could benefit from a more comprehensive comparison with related works, such as DOBF (https://arxiv.org/abs/2102.07492), which exploits code symmetry in pretraining through a deobfuscation objective, and CodeT5 (https://arxiv.org/abs/2109.00859), which leverages code symmetry in pretraining with identifier-aware data augmentation. These related works were not discussed or compared to the proposed method in the paper.\n\n2. The evaluation framework relies heavily on four artificial tasks created by the authors, omitting well-established, practical benchmarks used commonly in the field. For instance, important code generation tasks like OpenAI HumanEval  (https://huggingface.co/datasets/openai_humaneval) and MBPP (https://huggingface.co/datasets/mbpp), as well as code translation, clone detection, defect detection, and code repair tasks from CodeXGLUE (https://github.com/microsoft/CodeXGLUE), are all relevant to the domain but were not considered. This absence of evaluation on existing benchmarks and comparison with related works raises questions about the paper's soundness and the model's real-world applicability.\n\n3. The paper does not discuss potential limitations of the SymC model, such as its requirement for input code to be processed by a parser and a static analysis tool. This requirement may limit the model's applicability when dealing with incomplete or syntactically incorrect code, such as in code completion tasks or when faced with an empty Python block. While it is acceptable to establish certain assumptions for input code, these assumptions must be explicitly discussed rather than overlooked."
                },
                "questions": {
                    "value": "1. Can the authors provide a comparative analysis of SymC with related works such as DOBF and CodeT5 that also leverage code symmetry?\n2. Why were the evaluation tasks limited to four artificial tasks created by the authors?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8801/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698510975814,
            "cdate": 1698510975814,
            "tmdate": 1699637106480,
            "mdate": 1699637106480,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "frWMO7dLmX",
                "forum": "zDMM4ZX1UB",
                "replyto": "PIQ06LvDmV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We really appreciate your constructive comments. We address each of your points below, with additional experiments. We think that this clarification might significantly affect your view of the paper. We hope that you will take this into consideration.\n\n**Q1 & Weakness 1: The paper could benefit from a more comprehensive comparison with related works, such as DOBF (https://arxiv.org/abs/2102.07492), which exploits code symmetry in pretraining through a deobfuscation objective, and CodeT5 (https://arxiv.org/abs/2109.00859), which leverages code symmetry in pretraining with identifier-aware data augmentation. These related works were not discussed or compared to the proposed method in the paper.**\n\nThanks for introducing the related works to us. As shown in our response to Reviewer nM8R, Q2, we include the new results comparing SymC to DOBF, CodeT5, and GraphCodeBERT. These baselines are pre-trained, while SymC is not. Therefore, to enable a more fair comparison, we consider both pre-trained and non-pre-trained baselines and also a pre-trained version of SymC (similar to the experiment setup in Section 6.3, Figure 4(b)). In particular, the pre-trained version of SymC was pre-trained using masked language modeling on the functions provided by code2seq (java-small in the paper). \n\nNot pre-trained:\n\n|               | Original Sample (F1) | Permuted Samples (F1) | Invariance Violation Rate (the lower the better) |\n|---------------|-----------------|-------------------------|----------------------|\n| SymC          | 36.3            | 36.3                    | 0%                   |\n| GraphCodeBERT | 20.83           | 20.62                   | 31%                  |\n| DOBF          | 16.34           | 20.07                   | 41%                  |\n| CodeT5        | 25.35           | 25.42                   | 1%                |\n\n\nPre-trained: \n\n|               | Original Sample (F1) | Permuted Samples (F1) | Invariance Violation Rate (the lower the better) |\n|---------------|-----------------|-------------------------|----------------------|\n| SymC          | 37.1            | 37.1                    | 0%                   |\n| GraphCodeBERT | 28.65           | 30.27                   | 13%                  |\n| DOBF          | 34.64           | 33.71                   | 16%                  |\n| CodeT5        | 41.06           | 41.73                   | 10.7%                |\n\n\nThe results in the above tables show that, without pre-training, SymC outperforms the second-best model, CodeT5, by 43.2% and 42.8%, in original and permuted test samples, respectively. The baselines have 24.3% violation rate on average when the samples are permuted, while SymC enjoys provable invariance by construction. In addition, we observe that pre-trained CodeT5 has 10.7x higher violation rate than that when it is not pre-trained. This is likely due to that the model learns spurious positional biases not robust to statement permutations during pre-training.\n\nWe will add more discussion to the related work, and complete Table 2."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541182518,
                "cdate": 1700541182518,
                "tmdate": 1700544520478,
                "mdate": 1700544520478,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b49f24OX6x",
                "forum": "zDMM4ZX1UB",
                "replyto": "PIQ06LvDmV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "* New results\n\nHowever, we agree with the reviewer that there are many ML4code tasks, like defect prediction, that can benefit from code symmetry. To demonstrate that effect, we follow the reviewer\u2019s suggestion by incorporating the defect prediction and include our preliminary results in the following. \n\nParticularly, we consider defects4j dataset [7] and follow CodeXGLUE to formulate the task as a binary classification (buggy/non-buggy) given the function. We ensure the number of buggy and non-buggy samples are the same, and measure the prediction accuracy on the original and permuted samples. Our baselines include GraphCodeBERT [8], DOBF [9], CodeT5 [10], UnixCoder [11], and CodeBert [12].\n\n|  | Original Sample | Permuted Sample |\n|---|---|---|\n| SymC | 68.8$\\pm$1.6 | 68.8$\\pm$1.6 |\n| GraphCodeBERT | 61.7$\\pm$1.7 | 61.7$\\pm$2.2 |\n| DOBF | 62.4$\\pm$1.03 | 61.5$\\pm$1.2 |\n| CodeT5 | 63.3$\\pm$2.1 | 60$\\pm$7.5 |\n| UnixCoder | 67.1$\\pm$2.2  | 67.1$\\pm$1.6 |\n| CodeBERT | 62.2$\\pm$1.2 | 61.7$\\pm$2.1 |\n| GraphCodeBERT-not-pre-trained | 60.6$\\pm$2.4 | 61$\\pm$1.7 |\n| DOBF-not-pre-trained | 59.2$\\pm$1  | 59.9$\\pm$1.4 |\n| CodeT5-not-pre-trained | 59.2$\\pm$2.7 | 59.2$\\pm$2.7 |\n| UnixCoder-not-pre-trained | 63.1$\\pm$2.6 | 63.1$\\pm$2.6 |\n| CodeBERT-not-pre-trained | 62.2$\\pm$4.2 | 62.4$\\pm$4.6 |\n\nThe above table shows that SymC outperforms all the baselines, even when they are pre-trained on large-scale code datasets, by 11.4% on average. For example, SymC outperforms the second-best model, UnixCoder, by 2.5% in accuracy on permuted samples. Similar to function name prediction, all the baselines are susceptible to simple statement permutations. We will add more discussions to the related works and experiments in the paper.\n\n[7] Just, Ren\u00e9, Darioush Jalali, and Michael D. Ernst. \"Defects4J: A database of existing faults to enable controlled testing studies for Java programs.\" In Proceedings of the 2014 International Symposium on Software Testing and Analysis, pp. 437-440. 2014.\n\n[8] Guo, Daya, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou et al. \"GraphCodeBERT: Pre-training code representations with data flow.\" arXiv preprint arXiv:2009.08366 (2020).\n\n[9] Roziere, Baptiste, Marie-Anne Lachaux, Marc Szafraniec, and Guillaume Lample. \"Dobf: A deobfuscation pre-training objective for programming languages.\" arXiv preprint arXiv:2102.07492 (2021).\n\n[10] Wang, Yue, Weishi Wang, Shafiq Joty, and Steven CH Hoi. \"Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation.\" arXiv preprint arXiv:2109.00859 (2021).\n\n[11] Guo, Daya, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. \"Unixcoder: Unified cross-modal pre-training for code representation.\" arXiv preprint arXiv:2203.03850 (2022).\n\n[12] Feng, Zhangyin, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou et al. \"Codebert: A pre-trained model for programming and natural languages.\" arXiv preprint arXiv:2002.08155 (2020)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541345833,
                "cdate": 1700541345833,
                "tmdate": 1700542573301,
                "mdate": 1700542573301,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yJaLFrK252",
                "forum": "zDMM4ZX1UB",
                "replyto": "PIQ06LvDmV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Weakness 3: The paper does not discuss potential limitations of the SymC model, such as its requirement for input code to be processed by a parser and a static analysis tool. This requirement may limit the model's applicability when dealing with incomplete or syntactically incorrect code, such as in code completion tasks or when faced with an empty Python block. While it is acceptable to establish certain assumptions for input code, these assumptions must be explicitly discussed rather than overlooked.**\n\nAs shown in our response to Reviewer 6HuJ, statically analyzing the input code incurs additional overhead, e.g., 250.2ms per sample, but still much less than the existing model. That being said, we agree that efficiently analyzing incomplete or syntactically incorrect code on the fly can be an interesting future direction. We will add the discussion of limitations in the paper."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541453629,
                "cdate": 1700541453629,
                "tmdate": 1700698209670,
                "mdate": 1700698209670,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LeQlJiHLHR",
                "forum": "zDMM4ZX1UB",
                "replyto": "PIQ06LvDmV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer Srm2,\n\nThanks so much again for your constructive comments. We would be grateful if you could let us know whether our responses, i.e., the experiments on comparing to code baselines, e.g., DOBF, CodeT5, GraphCoderBERT, CodeBERT, and UnixCoder, and the evaluation on a new task, i.e., defect prediction, satisfactorily answer your questions and if so, would you like to increase the score. Thanks!"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698315913,
                "cdate": 1700698315913,
                "tmdate": 1700701226204,
                "mdate": 1700701226204,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lxPosBP7P5",
            "forum": "zDMM4ZX1UB",
            "replyto": "zDMM4ZX1UB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8801/Reviewer_nM8R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8801/Reviewer_nM8R"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a group-theoretic framework that defines code symmetries as semantics-preserving transformations, enabling precise reasoning within LLMs. SYMC employs a novel variant of group-equivariant self-attention that is provably equivariant to code symmetries.\nThe evaluation results show that SYMC generalizes to unseen code transformations, outperforming the state-of-the-art code models by 30.7%."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of defining code symmetries as semantics-preserving transformations, enabling precise reasoning within LLMs is somewhat interesting. \nTo evaluate the approach, four analysis tasks that require a deep understanding of code behavior such that they are expected to stay invariant to code symmetries were considered. Also a set of real-world semantics-preserving transformations\nbeyond PDG automorphisms to evaluate SYMC\u2019s generalization in the experiments."
                },
                "weaknesses": {
                    "value": "The paper needs more evaluations, e.g. an evaluation of the robustness of SYMC using the adversarial attack methods based on code transformations.\nSome contents are not well presented/stated."
                },
                "questions": {
                    "value": "Q: As stated in the paper, current code LLMs struggle with generalization to new code. Have you tried to evaluate the robustness of SYMC using the adversarial attack methods based on code transformations? The evaluation may make your method more convincing. \n\nQ: Have you tried to compare with \"Graphcodebert: Pre-training code representations with data flow\", which is a state-of-the-art method considering the inherent structure of code, in your evaluation? \n\nQ: Page 5, \"PDG (VPDG,EPDG) is a super graph of IG, sharing the same vertices but having a superset of edges (EPDG \u2287 EIG), because we consider all memory accesses as aliasing, making PDG a conservative construction of IG\",\nIf you \"consider all memory accesses as aliasing\", which is apparently a very weak encoding of the program semantics, it seems there would be too many aliases in the programs, making most statements unexchangeable to accomplish semantics-preserving statement permutations?\n\nQ: Page 6, \"Each entry dij is a 2-value tuple (pij , nij), indicating the shortest path from the lowest common ancestor of Vi and Vj , denoted as Tij , to Vi and Vj , respectively\", is pij the positive distances and nij the negative distances as denoted in the next paragraph? Also, what do you mean by positive distances and negative distances?\n\nQ: Page 2, \"SYMC enforces its output to stay invariant via keeping its learned representation G-equivariant, where the code representation (e1, e2, e3, e4) is transformed into (e2, e3, e4, e1) xxx\", should \"(e2, e3, e4, e1)\" be \"(e2, e1, e3, e4)\" as shown in Figure 1a?\n\nQ: Page 9, the lines labeled 2nd, 4th, 6th in Figure 4a. Which lines are for Aut(PDG)-equivariant self-attention layers and which are for the Aut(PDG)-invariant ones?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8801/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698566347212,
            "cdate": 1698566347212,
            "tmdate": 1699637106366,
            "mdate": 1699637106366,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pTXrHBuEob",
                "forum": "zDMM4ZX1UB",
                "replyto": "lxPosBP7P5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your time and effort in reviewing the paper. We address each of your points below.\n\n**Q1: As stated in the paper, current code LLMs struggle with generalization to new code. Have you tried to evaluate the robustness of SYMC using the adversarial attack methods based on code transformations? The evaluation may make your method more convincing.**\n\nThanks for bringing up the robustness angle against adversarial attacks. The key approach we started off from this paper is the development of a code model with **provable robustness by construction** against semantics-preserving code permutations. Guided by this framework, we proved that our approach is verifiably robust against any possible adversarial permutations, and thus not bound by minimal perturbations assumed in many adversarial attacks [1]. We leave exploring other code symmetry groups and their robustness guarantees as future work.\n\n[1] Pierazzi, Fabio, Feargus Pendlebury, Jacopo Cortellazzi, and Lorenzo Cavallaro. \"Intriguing properties of adversarial ML attacks in the problem space.\" In 2020 IEEE Symposium on Security and Privacy (S&P), pp. 1332-1349. IEEE, 2020.\n\nIndeed, when evaluating *other code transformations beyond permutations*, we agree that adversarial attacks can potentially lead to a greater number of violations when they are guided properly, e.g., by the input gradient. Therefore, we evaluate SymC and the newly suggested code models against the adversarial attack for function name prediction, i.e., Averloc [2]. The adversarial transformations implemented in Averloc are comprehensive, and include the adversarial code transformations proposed in prior work by Yefet et al. [3], e.g., variable renaming, dead code insertion, etc., with additional transformations such as loop unrolling. We follow the setting in Averloc by computing the adversarial attacks against a seq2seq model trained by the Averloc authors, and evaluate SymC and the baselines (GrahCodeBERT as suggested by the reviewer, and DOBF and CodeT5 as suggested by reviewer Srm2) on the generated adversarial examples. This ensures a fair comparison by evaluating all the models on the same set of adversarial examples. The baselines are trained on the same exact training set as SymC.\n\n|               | Original Sample (F1) | Adversarial Sample (F1) | Invariance Violation (the lower the better) |\n|---------------|-----------------|--------------------|----------------------|\n| SymC          | 52.9            | 47.5               | 26%                   |\n| GraphCodeBERT | 52.56           | 42.89              | 51%                  |\n| DOBF          | 51.59          | 39.68              | 51%                  |\n| CodeT5        | 44.21           | 36.66              | 47%                |\n\nThe table shows that SymC outperforms the second-best baseline, GraphCodeBERT, by 10.7% and 49%, in F1 and violation rate on the adversarial examples, respectively, against adversarial transformations (beyond semantics-preserving permutations). This indicates the strong robustness of SymC against adversarial code transformations, even though the attacks are not statement permutations. \n\n[2] Henkel, Jordan, Goutham Ramakrishnan, Zi Wang, Aws Albarghouthi, Somesh Jha, and Thomas Reps. \"Semantic robustness of models of source code.\" In 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER), pp. 526-537. IEEE, 2022.\n\n[3] Yefet, Noam, Uri Alon, and Eran Yahav. \"Adversarial examples for models of code.\" Proceedings of the ACM on Programming Languages 4, no. OOPSLA (2020): 1-30."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540477625,
                "cdate": 1700540477625,
                "tmdate": 1700544319940,
                "mdate": 1700544319940,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3vX5ADXJp5",
                "forum": "zDMM4ZX1UB",
                "replyto": "lxPosBP7P5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer nM8R,\n\nThanks again for making the effort to review our paper. We would be grateful if you could let us know whether our responses, i.e., the experiments on adversarial attacks and GraphCodeBERT, satisfactorily answer your questions and if so, would you like to increase the score. Thanks!"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698108487,
                "cdate": 1700698108487,
                "tmdate": 1700701283183,
                "mdate": 1700701283183,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "n5iubmHjds",
            "forum": "zDMM4ZX1UB",
            "replyto": "zDMM4ZX1UB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8801/Reviewer_6HuJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8801/Reviewer_6HuJ"
            ],
            "content": {
                "summary": {
                    "value": "This work explores invariance to symmetries in code that do not change the semantics of the code. This notion is formalized via automorphisms of program interpretation graphs. To achieve equivariance (and invariance) to these automorphisms, the authors use a self-attention based model with pairwise features based on an invariant distance matrix."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Formalization of code symmetries as automorphisms of graphs is nice and seems like the correct formalism.\n2. SymC model achieves equivariance to the code symmetries under consideration in a natural way, which is not too different from existing Transformer-based models.\n3. Empirical results show that SymC outperforms strong baselines, while being small and robust to code symmetries."
                },
                "weaknesses": {
                    "value": "1. Hard to understand exactly what program interpretation graphs and program dependence graphs look like, which is crucial to the paper.\n2. Experimental details are lacking. What is the training procedure for SymC, is it just supervised training on the downstream task? How about for the other models? For Function Name prediction, do the LLMs take in just the text as input, and what exactly does SymC take as input there?\n3. Computation of graphs associated to code may be costly and restrictive."
                },
                "questions": {
                    "value": "1. Could you illustrate example program interpretation graph and program dependence graphs? This would be quite helpful for understanding.\n2. How costly is obtaining the code graphs?\n3. Why does SymC require \"40,162 lines of code\"? I'm curious as to what makes it require so much."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8801/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8801/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8801/Reviewer_6HuJ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8801/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698898398505,
            "cdate": 1698898398505,
            "tmdate": 1699637106265,
            "mdate": 1699637106265,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eyctivA1ML",
                "forum": "zDMM4ZX1UB",
                "replyto": "n5iubmHjds",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We really appreciate your constructive comments. We address each of your points below.\n\n**Q1 & Weakness 1: Could you illustrate an example program interpretation graph and program dependence graphs? This would be quite helpful for understanding.**\n\nConsider the following Java example:\n\n```java\n\u2026\u2026\n1       int a = 1;\n2       int b = 2;\n3       if (a > b) {\n4           return 1;\n5       } else {\n6           return 0;\n7       }\n\u2026\u2026\n```\nWe will have 5 nodes in IG, i.e., {1,2,3,4,6} denoting 5 statements, and the edges {1->3, 2->3, 3->4, 3->6}. In such a case, IG is equivalent to PDG.\n\nConsider the case in binary analysis:\n\n```asm\n\u2026\n1       mov rcx, [rbx]\n2       mov [rax], rdx\n\u2026\n```\nWe have 2 nodes in IG, i.e., {1,2} denoting 2 instructions, and there is no edge in actual IG, as there is no dataflow from 1 to 2. However, in PDG, we conservatively consider the edge {1->2} by marking them as a write-after-read dependence. This is because we can not easily eliminate the possibility that `[rbx]` and `[rax]` would point to the same memory location.\n\n**Weakness 2: Experimental details are lacking. What is the training procedure for SymC, is it just supervised training on the downstream task? How about the other models? For Function Name prediction, do the LLMs take in just the text as input, and what exactly does SymC take as input there?**\n\nSymC adopts only supervised training and trains the model from scratch *without any pre-training*. While Figure 4(b) demonstrates that adding pre-training brings extra improvement, we aim to demonstrate having symmetry-preserving architecture alone leads to better performance than many pre-trained models.\n\nFor other baselines, we respect their own training paradigm. In particular, PalmTree, GPT4, and WizardCoder are pre-trained on programs in the wild. code2vec, code2seq, and GGNN use supervised training. The new models that we have experimented with during the rebuttal, i.e., CodeBert, GraphCodeBERT, UnixCoder, DOBF, and CodeT5 (see our responses to reviewer Srm2), are pre-trained with large-scale code datasets. \n\nFor function name prediction, both LLMs and SymC take as input the function (function body and function signature) with the function name stripped."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540307191,
                "cdate": 1700540307191,
                "tmdate": 1700544547494,
                "mdate": 1700544547494,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XZHBlFYy0z",
                "forum": "zDMM4ZX1UB",
                "replyto": "n5iubmHjds",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q2 & Weakness 3: How costly is obtaining the code graphs? Computation of graphs associated to code may be costly and restrictive.**\n\nTo incorporate the inductive bias of code, many code models involve extracting and encoding code structures, including our baselines, as computing PDGs statically is not overly expensive. Below, we add the runtime performance (in milliseconds) per code sample of SymC and PalmTree on binary analysis, training, and inference, using the same exact hardware (an AMD EPYC 7502 processor, 128 virtual cores, 256GB RAM, Nvidia RTX 3090 GPUs). \n\n| | Graph Construction | Train | Inference |\n| ----------|----------|----------|----------|\n| SymC | 250.2ms | 15.4ms | 2ms |\n| PalmTree | 4460ms | 36ms | 17ms |\n\nSymC\u2019s cheap computation of PDG incurs 17.8x less runtime overhead. It is also an interesting research problem to incorporate optimization, e.g., caching, to improve the efficiency of the PDG computation during inference. We will update the paper to include both source and binary analysis tasks.\n\n\n**Q3: Why does SymC require \"40,162 lines of code\"? I'm curious as to what makes it require so much.**\n\nBesides implementing the model and algorithm (~4.5k lines), we include the lines of code for doing program analysis (both source and binary code) for obtaining PDGs (2.4k lines) and generating ground truths for the analysis tasks (3k lines). We also include the code for setting up the baselines (4.3k lines), implementing program transformations (2.8k lines), data processing (1k), and all the scripts for experiments and result analysis (22k lines). We will clarify in the paper by providing a breakdown for each of these components."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540332915,
                "cdate": 1700540332915,
                "tmdate": 1700566795887,
                "mdate": 1700566795887,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JZrYjIs5lk",
                "forum": "zDMM4ZX1UB",
                "replyto": "XZHBlFYy0z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8801/Reviewer_6HuJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8801/Reviewer_6HuJ"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThanks for the comments. The example and runtimes are helpful, I definitely recommend putting them in the new versions of the paper.\n\nOne issue I have with the paper is the framing of SymC as an LLM (this is why I asked the question about the training procedure). The model is a Transformer with graph information included via relative positional encodings. But it cannot be trained or used for next-token prediction, but it can be used for supervised tasks on parts of code.\n\nRelatedly, models like GGNN may be a better comparison. Notably that model has lower invariance violation than LLMs. Given that GGNN also works on graphs associated to code, could you explain why it is not invariant to the code symmetry transformations you consider?"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666928993,
                "cdate": 1700666928993,
                "tmdate": 1700666928993,
                "mdate": 1700666928993,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FmZmNVdI3a",
                "forum": "zDMM4ZX1UB",
                "replyto": "n5iubmHjds",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1: The example and runtimes are helpful, I definitely recommend putting them in the new versions of the paper.**\n\nWe are grateful for your prompt response. We will definitely include them in the paper - we are working on drafting the figure/table and adding the discussion. We will update you as soon as we finish the draft. \n\n**Q2: One issue I have with the paper is the framing of SymC as an LLM (this is why I asked the question about the training procedure). The model is a Transformer with graph information included via relative positional encodings. But it cannot be trained or used for next-token prediction, but it can be used for supervised tasks on parts of code.**\n\nWe appreciate you pointing out this issue. We agree that our focus in this work is primarily on the Transformer encoder with bidirectional self-attention, without explicitly demonstrating its application in the context of a Transformer decoder, as commonly used in most LLMs today.\n\nHowever, we would like to highlight Section 3.5, \u201cToken-level predictor\u201d and Lemma 3, and the proof in Appendix B in the paper. We demonstrate how SymC preserves $Aut(\\mathcal{IG})$-invariance at the individual token level. This ensures end-to-end $Aut(\\mathcal{IG})$-invariant property in a Transformer decoder. As the autoregressive decoder generates a single token one at a time, attending to all previous tokens is in the same manner as the token-level predictor, and it is $Aut(\\mathcal{IG})$-invariant according to Lemma 3.\n\nWe will edit the framing of the paper, by focusing more on how SymC augments the Transformer architecture, and leave a detailed evaluation of LLM (autoregressive decoder) as future work. We will edit Section 3.5 to make the discussion of Transformer decoder more explicit. Thanks for helping us improve the clarity of our paper."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682563875,
                "cdate": 1700682563875,
                "tmdate": 1700682642449,
                "mdate": 1700682642449,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E4YIWtqQ49",
                "forum": "zDMM4ZX1UB",
                "replyto": "n5iubmHjds",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8801/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q3: Relatedly, models like GGNN may be a better comparison. Notably that model has lower invariance violation than LLMs. Given that GGNN also works on graphs associated to code, could you explain why it is not invariant to the code symmetry transformations you consider?**\n\nWe will answer this question from both conceptual and experimental perspectives.\n\n**Conceptually**, while GGNN incorporates the data and control flow graph, its particular dataflow construction *does not preserve* invariance to semantics-preserving permutations. For example, GGNN maintains edges for all reads and writes to the same variable token, e.g., using LastRead and LastWrite edge types [1]. This will prevent valid permutation between two statements that only read the same variable (Read-after-Read). Moreover, the permuted nodes can potentially lead to changed edge types. Such designs can result in the changed model\u2019s output while its input transformation is semantics-preserving, breaking the robustness guarantees that SymC offers.\n\n[1] Allamanis, Miltiadis, Marc Brockschmidt, and Mahmoud Khademi. \"Learning to represent programs with graphs.\" arXiv preprint arXiv:1711.00740 (2017).\n\nHowever, we agree that GNNs based strictly on our PDG will naturally preserve the Aut(PDG)-invariance. We aim to study how such a GNN architecture compares to SymC in our future work. The line of works on the graph Transformer suggests that Transformer with graph structures outperforms both the vanilla Transformer and GNN [2,3], but it remains interesting to study their robustness guarantees to code [3].\n\n[2] Ying, Chengxuan, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. \"Do transformers really perform badly for graph representation?.\" Advances in Neural Information Processing Systems 34 (2021): 28877-28888.\n\n[3] Hellendoorn, Vincent J., Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. \"Global relational models of source code.\" In International conference on learning representations. 2019.\n\n**Experimentally**, we observe the particular low-invariance-violation on GGNN in our experiments is because it keeps predicting the same meaningless label, i.e., its F1 score remains at 0.016 across all the permutation percentages (Table 2). For example, in more than 60% of the samples, GGNN implementation provided by [4] (the original GGNN paper [1] did not perform function name prediction) keeps predicting a meaningless name \u201cSPR9486\u201d on disparate samples. \n\n[4] Rabin, Md Rafiqul Islam, Nghi DQ Bui, Ke Wang, Yijun Yu, Lingxiao Jiang, and Mohammad Amin Alipour. \"On the generalizability of Neural Program Models with respect to semantic-preserving program transformations.\" Information and Software Technology 135 (2021): 106552."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8801/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682621505,
                "cdate": 1700682621505,
                "tmdate": 1700696009776,
                "mdate": 1700696009776,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]