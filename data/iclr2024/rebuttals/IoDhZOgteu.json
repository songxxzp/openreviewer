[
    {
        "title": "DEXR: A Unified Approach Towards Environment Agnostic Exploration"
    },
    {
        "review": {
            "id": "5wJKsKwogF",
            "forum": "IoDhZOgteu",
            "replyto": "IoDhZOgteu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8228/Reviewer_NQDk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8228/Reviewer_NQDk"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a framework for exploration in which the task policy is rolled out first and the exploration policy continues after. This is compared to related work such as rolling exploration first or optimizing a joint objective. There are experiments on continuous control mazes and mujoco tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper proposes a straightforward framework that intuitively should work well\n- The results are promising"
                },
                "weaknesses": {
                    "value": "- The method is incremental compared to work like Go-Explore.\n- There are missing baselines. Specifically, the method is never compared to the same method but without exploration, i.e. the TD3 baseline."
                },
                "questions": {
                    "value": "Comparing the results to the original TD3 paper seems that the proposed method does not outperform TD3. Furthermore, the baseline exploration methods reported all perform worse than TD3. Why does this happen? Since these methods are implemented on top of TD3 it would be strange if they had worse performance than the backbone RL method without any extrinsic rewards."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8228/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8228/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8228/Reviewer_NQDk"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8228/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698103968089,
            "cdate": 1698103968089,
            "tmdate": 1700608231498,
            "mdate": 1700608231498,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T7xYsBSvw2",
                "forum": "IoDhZOgteu",
                "replyto": "5wJKsKwogF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8228/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8228/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We want to thank the reviewer NQDk for the effort in reviewing the paper. We address the questions and respond to the concerns in the following.\n\nQ1: \"Comparing the results ... does not outperform TD3.\"\n\nA1: In our PointMaze experiments, DEXR consistently outperforms TD3 across various layouts, achieving significantly better results. Specifically, we observed that TD3 requires at least twice as long to reach the goal point compared to DEXR. This demonstrates the superior efficiency and effectiveness of DEXR in these scenarios.\n\nRegarding the MuJoCo Locomotion tasks, our results indicate a more nuanced performance comparison. While DEXR does not show a substantial advantage over TD3 in environments with dense rewards, it notably surpasses TD3's performance in most environments characterized by sparse rewards. This distinction highlights DEXR's enhanced adaptability and proficiency in more challenging settings with limited feedback.\n\nQ2: Furthermore, the baseline exploration methods reported all perform worse than TD3. Why does this happen? Since these methods are implemented on top of TD3 it would be strange if they had worse performance than the backbone RL method without any extrinsic rewards.\n\nA2: As discussed extensively in Sections 1 and 2, methods like Exp, which rely on intrinsic rewards, tend to be distracted and exhibit unstable performance, failing to exploit efficiently when the hyperparameter is not carefully tuned. This is the precise challenge DEXR aims to address.\n\nAs detailed in Section 5, all algorithms, including TD3, are trained using extrinsic rewards. Notably, the TD3 agent is trained without the addition of intrinsic rewards.\n\nConcern 1: The method is incremental compared to work like Go-Explore.\n\nResponse 1: We regret any misunderstanding regarding the relationship between DEXR and Go-Explore. While there is a superficial similarity, DEXR embodies a fundamentally different design philosophy and operates within a markedly distinct setting.\n\nFirstly, Go-Explore's methodology hinges on the environment being deterministic or the availability of a local model (resettable environment) that can relocate the agent to a previously visited state. Contrarily, DEXR operates independently of these requirements.\n\nSecondly, Go-Explore relocates the agent to regions that are rarely visited previously, whereas DEXR relocates the agent to regions that are believed to be fruitful according to the extra exploitation policy. Despite these two paradigms being similar in form, DEXR and Go-Explore hold drastically different ideas and intuition. To be specific, Go-Explore enables more efficient **over-exploration**, by leveraging the special assumptions and privilege accesses to accurately relocate the agent to \u201cregions not well covered\u201d, so that the agent can fully cover the whole MDP, including regions that might not be worth exploring. In contrast, DEXR prevents **over-exploration** and does not need any special assumptions, by employing an extra exploitation policy to relocate the agent to regions that are \u201cworth exploring\u201d, which largely mitigates the hyperparameter sensitivity, as discussed in Section 1 and Section 4 in detail. \n\nThe effectiveness of DEXR, while straightforward and intuitive, is underscored by our comprehensive empirical analysis. This analysis examines the behaviors of the exploitation and exploration policies in navigation and evaluates the capacity of the exploitation policy to effectively guide the exploration policy.\n\nWe have already put a discussion on the relationship between DEXR and the Go-Explore algorithm in the related work section, the reviewer can refer to the revised manuscript for a clearer context.\n\nConcern 2: \"There are missing baselines ... TD3 baseline.\"\n\nWeakness 2: We acknowledge the inquiry regarding the comparison with TD3. For your reference, this comparison can be found in the context of the PointMaze experiment. Additionally, we provide visual representations of the behavior of various algorithms, including TD3, within the PointMaze environment, detailed on page 8 of our document.\n\nWe apologize for the omission of the split plots and the comparative plot against TD3 in the appendix. We have since rectified this oversight, and these plots are now included in the updated version of the manuscript and can be found in Appendix A."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700464013582,
                "cdate": 1700464013582,
                "tmdate": 1700464013582,
                "mdate": 1700464013582,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u6ffRjMzLe",
                "forum": "IoDhZOgteu",
                "replyto": "T7xYsBSvw2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8228/Reviewer_NQDk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8228/Reviewer_NQDk"
                ],
                "content": {
                    "title": {
                        "value": "The experimental evidence insufficient"
                    },
                    "comment": {
                        "value": "- I thank the authors for the updated results and I increase my score accordingly. However, I find the experimental evidence for the benefits of using the algorithm insufficient, which prevents me from accepting the paper\n  - A key claim of the paper is that DEXR is more robust to different values of the balance hyperparameter. However, setting the balance to 1 yields good results for the baselines evaluated in this paper. While the proposed method works better with much higher values, it is unclear why that is notable. Balance term of 1 seems like a reasonable default value. \n  - With beta=1, the proposed method produces mixed results relatively to the baselines and does not definitively outperform them.\n  - The robustness results are also mixed. DEXR's performance degrades with larger balance terms, and in the aggregate setting DEXR performs somewhat similarly to baselines on 3 out of 5 sparse reward tasks, and clearly outperforms the baselines on only 2 tasks. \n- I regret any misunderstanding regarding Go-Explore. While the major result in the go-explore paper requires the ability to reset the agent to an arbitrary state, the paper also proposes a version of the method that employs goal-conditioned policies to reach the desired state. This is further extended in Ecoffet'20. I find the discussion of the reset assumption confusing in light of these results. \n\nEcoffet'20, First return, then explore"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608206370,
                "cdate": 1700608206370,
                "tmdate": 1700608206370,
                "mdate": 1700608206370,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9gpSejXbzd",
            "forum": "IoDhZOgteu",
            "replyto": "IoDhZOgteu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8228/Reviewer_CdTY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8228/Reviewer_CdTY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new exploration scheme to reduce the sensitivity of exploration hyperparameters across different environments. The new scheme is a two-phased process, starting with a pure exploitative policy and then switching to an exploratory policy at each episode. Specifically, the exploratory policy will try to take over the episode according to a Bernoulli event at each time step. Empirical results on some navigation tasks and MuJoCo controls tasks show consistent performance improvement over baseline algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strengths of the paper are its originality and significance. To the best of my knowledge, the idea of the paper is novel. The exploration-exploitation trade-off has been an outstanding issue in reinforcement learning, and the idea of this paper provides a simple but effective way to handle the trade-off. It\u2019s interesting to see how an extra exploitation phase at the beginning of each episode helps deeper exploration, which may be of interest to the broad RL community. \n\nIn addition, the theoretical analysis of DEXR-UCB (LSVI-UCB with DEXR) is somewhat interesting. I suggest the author(s) lay out the condition on the intrinsic motivation reward under which DEXR is theoretically sound, which would make the theoretical results more appealing."
                },
                "weaknesses": {
                    "value": "In terms of weaknesses, the paper can be improved in its soundness and quality:\n1. First of all, Appendix B is still missing, which contains important empirical results to validate the paper's central claim. The paper claims to find an algorithm that has a reduced sensitivity to the exploration hyperparameter. Then, how the algorithm (and baselines) reacts to different values of the exploration hyperparameter should be shown. Also, I think it would be better if it\u2019s in the main text.\n2. The effect of the extra hyperparameter, the truncation probability $p$, is not discussed adequately. The paper only mentions that it is set to $1-\\gamma$. In addition, there is an annealing of this parameter, which is not mentioned in the main text nor investigated thoroughly.  Are there any justifications for these design choices? \n3. There are still a lot of typos and reference format issues in the paper. Please carefully fix them. See Questions for an unexhausted list.\n\nI will reevaluate the paper if the authors address the above concerns."
                },
                "questions": {
                    "value": "1. How does $\\beta$ influence the performance of each algorithm?\n2. Are there any justifications for setting $p$ to $1-\\gamma$? Also, why is $p$ annealed to $\\frac{1}{H}$ during the course of the training?\n\nTypos and minor suggestions:\n1. At the top of page 4, the two $\\pi$s seem to be missing a superscript $^{ext}$.\n2. In Eq. 3 and Eq. 4 on page 4, $V(s, a)$ should be $V(s)$.\n3. It may be a good idea to also define $b$ in the input of Algorithm 1. Otherwise, the reader may be confused if they just read the pseudocode.\n4. At the bottom of page 8, \u201cfor for\u201d should be \u201cfor.\u201d\n5. In the first paragraph of page 13, a verb is missing between \u201cthen $\\frac{1}{1-\\gamma}.\u201d\n6. There are incorrect or inconsistent reference formats scattered throughout. Please fix them."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8228/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8228/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8228/Reviewer_CdTY"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8228/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698729884558,
            "cdate": 1698729884558,
            "tmdate": 1700642698691,
            "mdate": 1700642698691,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dq5akJl1Yh",
                "forum": "IoDhZOgteu",
                "replyto": "9gpSejXbzd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8228/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8228/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We want to thank the reviewer CdTY for the effort in reviewing our paper, we address the questions and concerns in the following.\n\nQ1: How does $\\beta$ influence the performance of each algorithm?\n\nA1: We apologize for the oversight concerning the absence of split plots in the appendix of our manuscript. We have already put the split results in Appendix A, which describes the influence of $\\beta$ on each algorithm.\n\nQ2: Are there any justifications for setting $p$ to $1-\\gamma$? Also, why is $p$ annealed to $\\frac{1}{H}$ during the course of the training?\n\nA2: Let me walk through the key choices in detail:\n\nUsing $p=1-\\gamma$ for truncation probability initially:\n\n- The discount factor $\\gamma$ determines the effective horizon $H^e=1/(1-\\gamma)$\n- This horizon suggests the steps needed to propagate rewards back to early states\n- Therefore, letting the exploitation policy run for H steps on average via p=1-\u03b3 allows it to reach promising states\n- Formally, if $T \\sim Geom(p)$, then $\\mathbb E[T]=1/p=1/(1-\u03b3)=H^e$\n\nDecaying $p$ over time:\n\n- As exploitation policy improves, less exploration is needed near the initial point\n- Reducing p increases $\\mathbb E[T]$ since $\\mathbb E[T]=1/p$\n- This lets exploitation run longer to fine-tune on higher-value regions\n- $p$ is decayed gradually to $1/H$, to gradually increase $\\mathbb E[T]$ to $H$\n\nAvoiding excessive pointless exploitation:\n\n- Despite higher $\\mathbb E[T]$ later, $T$ still follows a geometric distribution\n- So there are always probabilistic chances of truncating early\n- Formally, $\\mathbb P(T=t)=(1-p)^{t-1}p$ still enables occasional exploration in region near the initial point\n- Experiments confirm this prevents issues of over-exploitation\n\nPer your feedback, we have already added some discussion on this design choice in Section 4 for more information. \n\nQ3: Typos and minor suggestions.\n\nA3: We extend our apologies for any inconvenience caused by these errors. Additionally, we want to clarify that in Equations 3 and 4, the term $\\mathbb{P}V(s, a)$ is defined as $\\mathbb{E}_{s\u2019 \\sim \\mathbb{P}(\\cdot|s, a)} V(s')$, as elaborated in the paragraph subsequent to Equation 2. \n\nConcern 1: First of all, Appendix B is still missing, which contains important empirical results to validate the paper's central claim. The paper claims to find an algorithm that has a reduced sensitivity to the exploration hyperparameter. Then, how the algorithm (and baselines) reacts to different values of the exploration hyperparameter should be shown. Also, I think it would be better if it\u2019s in the main text.\n\nResponse 1: We again apologize for the oversight concerning the absence of split plots in the appendix of our manuscript. To rectify this, we have now included a separate plot in Appendix A.\n\nConcern 2: The effect of the extra hyperparameter, the truncation probability, is not discussed adequately. The paper only mentions that it is set to\u00a0$1-\\gamma$. In addition, there is an annealing of this parameter, which is not mentioned in the main text nor investigated thoroughly. Are there any justifications for these design choices?\n\nResponse 2:  We have addressed this concern in Q2, the reviewer can refer to the details in our answer A2. \n\nConcern 3: There are still a lot of typos and reference format issues in the paper. Please carefully fix them. See Questions for an unexhausted list.\n\nResponse 3: We appreciate your suggestion and would like to inform you that the necessary corrections have been made."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463497356,
                "cdate": 1700463497356,
                "tmdate": 1700463497356,
                "mdate": 1700463497356,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ep51QJcuyD",
                "forum": "IoDhZOgteu",
                "replyto": "dq5akJl1Yh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8228/Reviewer_CdTY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8228/Reviewer_CdTY"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed reply. The updated version addresses some of my concerns about the missing results and presentation. I've updated my assessment accordingly. Here are some follow-ups:\n> We have already put the split results in Appendix A, which describes the influence of $\\beta$\n on each algorithm.\n\nThe added learning curve plots are very useful for inspecting the behavior of algorithms with specific parameter values. However, there are too many plots, and it\u2019s not very obvious to see whether the proposed method is less sensitive to $\\beta$. Maybe it's a good idea to summarize them with sensitivity plots. For example, use the x-axis to show different values of $\\beta$ and the y-axis to show a corresponding performance metric.\n\n> This horizon suggests the steps needed to propagate rewards back to early states\n\nI know the expectation of the termination time step of the exploitation policy would be $1/p$. But what do you mean by \u201cpropagate rewards back to early states\u201d? Could you elaborate on this?\n\nAlso, I agree with Reviewer Nqr6 that results on hard exploration tasks may provide further insights into the effect of incorporating DEXR into existing intrinsic motivation methods."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642662831,
                "cdate": 1700642662831,
                "tmdate": 1700642662831,
                "mdate": 1700642662831,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8WaIQcPK2S",
            "forum": "IoDhZOgteu",
            "replyto": "IoDhZOgteu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8228/Reviewer_Nqr6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8228/Reviewer_Nqr6"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to address the problem that existing approaches to exploration with intrinsic bonuses are highly sensitive to hyperparameters that are used to balance exploration and exploitation.\n\nThis paper proposes a framework called DEXR, which can be applied to explore using intrinsic rewards in off-policy settings. Concretely, separate exploration and exploitation policies are maintained, and actions are selected through the exploitative policy till a stochastic truncation point, after which the exploratory policy takes over.\n\nExperiments show that the proposed approach enables learning good policies in fewer samples for a wide range of exploration factors/intrinsic reward coefficients."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**S1.** Using intrinsic rewards is a popular choice for exploring environments with sparse rewards and high-dimensional state spaces. Thus, improving the hyperparameter sensitivity of these approaches and enabling applicability even in dense reward settings would interest RL practitioners.\n\n**S2.** The proposed approach is conceptually simple and easy to integrate with various bonuses and RL algorithms."
                },
                "weaknesses": {
                    "value": "**W1.** While the proposed approach shows robustness to choices of the intrinsic reward coefficient ($\\beta$) in the considered environments, it is currently unclear if this approach would hinder existing curiosity-based approaches in the settings where they have shown significant benefits.\n\nThe environments are not the best representation of when curiosity-based approaches succeed. The environments considered are low-dimensional, without a hard-exploration challenge.\n\nWhile I agree that the maze navigation environment is technically a sparse-reward setting, the considered mazes are \u2018simple\u2019 to explore. Even TD-3, without any intrinsic bonus, reliably achieves rewards in three of the four settings and reaches the goal sometimes in the largest maze. Many intrinsic bonus approaches have enabled success in settings where naive exploration (as in standard TD-3)  is far slower and (almost) never succeeds in the considered interactions (e.g., mazes in MiniGrid [1] or hard-exploration atari environments [2]).\n\n**W2.** To properly evaluate the sensitivity of approaches to the intrinsic reward coefficient, further details are needed for how the range for  $\\beta$ was selected. For the navigation tasks, only two values of $\\beta_s=1$ and $\\beta_l=10000$ were selected, and they are extremely wide apart. I have similar concerns for the Mujoco experiments, which use five values of $\\beta$ in the same range. Is there a reason why lower values of $\\beta$ were not considered?\n\n**W3.** It would also be useful to understand DEXR\u2019s sensitivity to truncation probability p. No results are presented regarding this. \nFurther, is there a reason why p is chosen as 1- gamma? \n\nIt would also be better to mention that p is decayed in the main text (not just in the appendix).\n\n\n**W4.** The paper misses discussions regarding Bayesian RL approaches (see [3] and references within), which can elegantly balance exploration and exploitation without ugly coefficients to balance intrinsic and extrinsic rewards. There also exist some scalable variants (e.g., [4]). \n\nOn a separate note regarding presentation, the paper could be improved by incorporating a separate section for the theoretical results. I also felt that the introduction and the related work section (that immediately follows the introduction) could jointly be compressed as there is quite a bit of overlap between them.\n\n\u2014------------------\u2014------------------\u2014------------------\u2014------------------\u2014------------------\n\n### References\n\n[1] Chevalier-Boisvert, M., Dai, B., Towers, M., de Lazcano, R., Willems, L., Lahlou, S., ... & Terry, J. (2023). Minigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks. arXiv preprint arXiv:2306.13831.\n\n[2] Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., & Munos, R. (2016). Unifying count-based exploration and intrinsic motivation. Advances in neural information processing systems, 29.\n\n\n[3] Ghavamzadeh, M., Mannor, S., Pineau, J., & Tamar, A. (2015). Bayesian reinforcement learning: A survey. Foundations and Trends\u00ae in Machine Learning, 8(5-6), 359-483.\n\n[4] Osband, I., Blundell, C., Pritzel, A., & Van Roy, B. (2016). Deep exploration via bootstrapped DQN. Advances in neural information processing systems, 29."
                },
                "questions": {
                    "value": "Q1. Regarding Figure 2, could the authors explain why DEXR\u2019s exploration is preferable to standard intrinsic motivation? It seems like DEXR would not help in reward-free exploration.\n\nQ2. Since off-policy approaches can be more sample-efficient, I would like to understand if it is possible to use TD-3 + EIPO (instead of PPO) with a similar motivation, i.e., alternating interactions with exploratory and exploitative policies. Or are there other reasons why PPO needs to be used with EIPO?\n\nQ3. Regarding the results presented in Figure 6, are similar figures available for the case where performance is not aggregated across choices of intrinsic reward coefficients ($\\beta$)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8228/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8228/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8228/Reviewer_Nqr6"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8228/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698953186219,
            "cdate": 1698953186219,
            "tmdate": 1699637022025,
            "mdate": 1699637022025,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u561AxaMVq",
                "forum": "IoDhZOgteu",
                "replyto": "8WaIQcPK2S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8228/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8228/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer Nqr6 for the suggestions and the questions. We address the concerns and the questions in the following:\n\nQ1: \"Regarding Figure 2 ... not help in reward-free exploration.\"\n\nA1: Thanks for raising this point. We would like to clarify that DEXR is primarily designed to enhance stability across hyperparameters,\u00a0 and thus enable efficient exploration in an\u00a0environment-agnostic\u00a0manner, rather than to maximize exploration in environments without reward. Nevertheless, despite it is not designed for the reward-free setting, our theoretical analysis shows that it would perform on par with standard intrinsic reward exploration algorithms, which indicates that DEXR can also explore efficiently under the reward-free setting per the findings in [1]. \n\nQ2: \"Since off-policy approaches can be more sample-efficient ... PPO need to be used with EIPO?\"\n\nA2: Thank you for raising the thoughtful question about using TD3 as an alternative backbone for EIPO. You make an excellent point that off-policy algorithms could provide advantages. There were two key factors in preserving the PPO backbone in EIPO:\n1. EIPO relies on accurate policy value estimates to switch between exploration/exploitation. As an on-policy method, PPO better fits this need.\n2. Significantly changing the original PPO-based EIPO risks invalidating the baseline.\n\nQ3: \"Regarding the results presented in Figure 6 ... reward coefficients $\\beta$?\"\n\nA3: We apologize for missing the split plots, we have already restored the split plots in Appendix A to provide more detailed information on the experimental results.\n\nConcern 1: \"While the proposed approach shows robustness ... curiosity-based approaches succeed.\"\n\nResponse 1: We would like to acknowledge that harder exploration problems exist, such as Montezuma\u2019s Revenge. However, we believe our experiment, along with our theoretical analysis, supports our technical claim about DEXR well, as our focus is to enhance to stability of curiosity-driven exploration methods under different reward sparsity settings. \n\nConcern 2: \"The environments considered ... the goal sometimes in the largest maze.\"\n\nResponse 2: The reason why we chose to work with MuJoCo is two-fold:\n1. The aim of DEXR is not to outperform the state-of-the-art in the most popular benchmark but to enhance the stability of existing curiosity-driven exploration algorithms.\n2. As Suggested in [3], vision representation can affect performance heavily in pixel-based environments. We evaluate DEXR on MuJoCo to eliminate other factors.\n\nPointMaze is a good testbed for exploration:\n1. Its observation space contains clear location information, which allows for monitoring visualizing, and analyzing the behavior of the agents.\n2. The hardness of an environment should not be defined by whether the random exploration can get the reward, but defined as \u201chow many samples are needed to find the near-optimal policy\u201d. For reference, without structured exploration, PPO can obtain a considerable amount of reward in Montezuma\u2019s Revenge.\n\nDespite TD3 reaching the goal location a few times in the training process in Large-Maze, still fails to escape the suboptimality. And TD3 gets stuck with its sub-optimal policy, it takes at least 2x longer for TD3 to reach the goal compared to DEXR in all tasks. \n\nConcern 3: \"To properly evaluate ... why lower values of $\\beta$ were not considered?\"\n\nResponse 3: For PointMaze, the intention was to investigate the impact of small v.s. large intrinsic reward coefficients on the performance and the behaviors of different algorithms.\n\nFor MuJoCo locomotion, to evaluate the robustness of DEXR in MuJoCo locomotion tasks, we test various hyperparameters. The exploration & exploitation balance with a higher intrinsic reward coefficient is crucial: higher values encourage exploration in unknown environments but can lead to distractions and unstable optimization. We assess DEXR's effectiveness with coefficients ranging from the most natural choice 1.0 and increase up to 1000.0, examining its ability to handle these challenges.\n\nConcern 4: \"It would also be useful ... why p is chosen as 1- gamma?\"\n\nResponse 4: We have also added some discussion on this design choice in Section 4 for more information, and provide a set of experiments on evaluating how DEXR would respond to different initial truncation probability $p$ and different decay schedules, which can be found in Appendix A.\n\nConcern 5: \"The paper misses .. variants (e.g., [4])\".\n\nResponse 5: We have already added some discussion on the Bayesian RL approach in the related work section.   \n\nReference: \n\n[1]Wang et al. (2020) On Reward-Free Reinforcement Learning with Linear Function Approximation.\n\n[2] Bellemare et al. (2016) Unifying count-based exploration and intrinsic motivation. \n\n[3]Kostrikov et al. (2020) Image augmentation is all you need: Regularizing deep reinforcement learning from pixels."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462310147,
                "cdate": 1700462310147,
                "tmdate": 1700464433879,
                "mdate": 1700464433879,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OeUlUoBZkm",
                "forum": "IoDhZOgteu",
                "replyto": "8WaIQcPK2S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8228/Reviewer_Nqr6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8228/Reviewer_Nqr6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your answers/clarifications. I appreciate the time and effort spent in crafting the reponse. I will maintain my score as some of my main concerns still remain open.\n\n- Thanks for the clarification regarding Q1, it was slightly confusing since I was trying to see how DEXR better explored in that setting.\n\n- I still think applying the idea of EIPO with TD3 could be useful, but I can accept that it is probably not essential due to the reasons you mention.\n\n- Regarding responses 1 and 2, I still feel that harder exploration environments should also have been considered to understand DEXR comprehensively. It is important to know how much of a potential downside (or not) there could be from DEXR in such environments. Harder environments have been used by the considered baselines for stably training with intrinsic rewards. EIPO evaluates in the atari suite, DERL studies DeepSea. About the large point-maze, I agree that obtaining rewards should not be a measure of hardness in general, but I think it is not unreasonable for a sparse reward navigation task. Even if we consider it a relatively hard exploration task, it is still only one example on the simpler side.\n\n- Regarding response 3, could the authors clarify why $\\beta=1$ is a natural choice on the lower side? Many (if not most) approaches to intrinsic exploration often use a far smaller value of $\\beta$, for eg. see appendix A2 of RIDE (https://arxiv.org/pdf/2002.12292.pdf), which uses coefficients of 0.005 in some cases. The range over which the approaches are evaluated should definitely contain the best values of the intrinsic bonus coefficient. Is that the case in these experiments? Of course, the best value could differ across environments, and that would be a strong motivation for DEXR."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584979384,
                "cdate": 1700584979384,
                "tmdate": 1700652373832,
                "mdate": 1700652373832,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6ng35oK7pN",
            "forum": "IoDhZOgteu",
            "replyto": "IoDhZOgteu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8228/Reviewer_CCrp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8228/Reviewer_CCrp"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a framework for handling the exploration-exploitation problem in reinforcement learning. This framework involves an exploitative and exploratory policy, each of which is learnt using off-policy RL using data collected by exploiting in the first few steps of an episode, followed by exploration. The paper suggests that this helps them avoid massively out-of-distribution samples that arise in off-policy RL with random exploration, and leads to superior performance compared to other intrinsic-reward based RL approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The lower dependence on the success of this approach on the proportion of intrinsic reward (exploration factor $\\beta$) demonstrates that the approach is robust to this hyperparameter, which is a key claim made in this paper.\n- The results show that the approach works in both sparse and dense reward settings, which is also a key claim made in this paper.\n- Figure 8 provides very good intuitive qualitative evidence of how DEXR is more agnostic to $\\beta$ than the other baselines; I like how this paper goes beyond the normal learning curves to provide visualisations for their central claims."
                },
                "weaknesses": {
                    "value": "- The related work section covers important papers, but I felt a key section was missing, the Go-explore family of algorithms. [1] proposed Go-Explore, which I think should be discussed in related work. [2] actually has quite a similar proposition to this paper; there are two policies learnt, one that learns only to exploit and one that learns only to explore. While that is in the meta-RL setup, I think it should be mentioned that there is existing work that has suggested similar ideas, but not from an intrinsic motivation perspective.\n- It may also be worth covering temporal abstraction for exploration in RL in related work. For instance, [3] performs exploration at the level of options i.e. at a higher level of temporal abstraction.\n- In the results, the paper mentions that DEXR enjoys notably smaller variances compared to DeRL and Exp but that does not seem to be the case (or at least does not seem to be obvious) in Figure 6. Could the authors provide cleaner plots (with only one DEXR, one Exp, and one DeRL variant, instead of the full set of plots)?\n- Minor points:\n    - Figure 1 has a grey line at the right; it seems to be a screenshot that didn't crop properly?\n    - Algorithm 1: DXER -> DEXR\n    - Page 6 \u2013 board -> broad\n\n\nReferences:\n\n[1] First return, then explore. Ecoffet et al, 2021.\n\n[2] First-Explore, then Exploit: Meta-Learning Intelligent Exploration. Normal and Clune, 2023.\n\n[3] Temporally-Extended $\\epsilon$-Greedy Exploration. Dabney et al, 2020."
                },
                "questions": {
                    "value": "- If you have the truncation probability, why do you need the horizon $H$ in the algorithm? Is it an implementation detail to ensure that the exploitation phase does not cover a majority of the episode?\n- In Figure 6, the results are averaged over 5 random seeds and 5 values of $\\beta$. It is not standard practice to average over hyperparameters, so I am curious to know why the authors went with this particular decision. Also, the paper mentions that the split over $\\beta$ is in the appendix but I was unable to find it. Could the authors clarify where I can find these results?\n- Bonus (not a request for experiments): I'm curious to know if the authors experimented with iterative exploitation-exploration within an episode."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8228/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699142246939,
            "cdate": 1699142246939,
            "tmdate": 1699637021921,
            "mdate": 1699637021921,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UTMXvzSWwx",
                "forum": "IoDhZOgteu",
                "replyto": "6ng35oK7pN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8228/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8228/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer CCrp for the suggestions and for bringing up insightful questions. We address the concerns and the questions in the following: \n\nQ1: If you have the truncation probability, why do you need the horizon in the algorithm? Is it an implementation detail to ensure that the exploitation phase does not cover a majority of the episode?\n\nA1: Note that H is an environment parameter of the finite horizon setting. The algorithm takes this parameter and splits the horizon into two parts randomly (via the truncation probability). The algorithm performs exploitation in the first part and exploration in the second part. Indeed, if the environment is infinite-horizon, we can set another truncation probability $1-\\gamma$ for the exploration policy.\n\nQ2: In Figure 6, the results are averaged over 5 random seeds and 5 values of\u00a0$\\beta$. It is not standard practice to average over hyperparameters, so I am curious to know why the authors went with this particular decision. Also, the paper mentions that the split over is in the appendix but I was unable to find it. Could the authors clarify where I can find these results? \n\nA2: We have already restored the split plots in Appendix A to provide more detailed information on the experimental results, we apologize for missing these plots in the previous version. Regarding the averaging performance across both hyperparameters and seeds, our rationale was to use such averaged results over the hyperparameters to show the robustness of our method, as the performance is promising, and the standard deviation is much lower than Exp and DeRL. Per your feedback, we move the aggregated plots to Appendix A, and in the main text, we only keep the results with Disagreement intrinsic reward with $\\beta=1.0$ and $\\beta=1000.0$ for better clarity.\n\nQ3: Bonus (not a request for experiments): I'm curious to know if the authors experimented with iterative exploitation-exploration within an episode.\n\nA3: The paradigm of alternating exploitation and exploitation within every episode is precisely the approach taken by EIPO: EIPO utilizes PPO as the backbone and toggles between exploration and exploitation based on state-value estimates. However, as evidenced by our visualizations in Section 5, this approach is suboptimal. As we have discussed in Section 5 of the paper, the alternative policy switching makes the exploitation policy unable to identify successful trajectories and hence hampers overall performance. \n\nConcern 1: The related work section covers important papers, but I felt a key section was missing, the Go-explore family of algorithms.\n\nResponse 1: Thanks for your suggestion to include a discussion on the Go-Explore work in the Related Work section. In response, we have added a discussion on the connection between our method and the Go-Explore method in the related work section.\n\nConcern 2: (First-Explore, then Exploit: Meta-Learning Intelligent Exploration) actually has quite a similar proposition to this paper; there are two policies learned, one that learns only to exploit and one that learns only to explore. While that is in the meta-RL setup, I think it should be mentioned that there is existing work that has suggested similar ideas, but not from an intrinsic motivation perspective.\n\nResponse 2: Thank you for the insightful point. Indeed this is quite relevant to our paper, and it shares some similar approach with DEXR. We have added a discussion concerning this work to our related work section. \n\nConcern 3: In the results, the paper mentions that DEXR enjoys notably smaller variances compared to DeRL and Exp but that does not seem to be the case (or at least does not seem to be obvious) in Figure 6. Could the authors provide cleaner plots (with only one DEXR, one Exp, and one DeRL variant, instead of the full set of plots)?\n\nResponse 3: We have put cleaner results in the main texts, and added the detailed split plots in Appendix A."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460218847,
                "cdate": 1700460218847,
                "tmdate": 1700460622690,
                "mdate": 1700460622690,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1UGmBwJHHJ",
                "forum": "IoDhZOgteu",
                "replyto": "6ng35oK7pN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8228/Reviewer_CCrp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8228/Reviewer_CCrp"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for answering the questions I had and addressing some of the concerns particularly in the related work section. I have gone through the revised version and also concur with the concerns raised by other reviewers regarding the inclusion of another hard exploration task, and I am thus keeping my current score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8228/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650822612,
                "cdate": 1700650822612,
                "tmdate": 1700674297423,
                "mdate": 1700674297423,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]