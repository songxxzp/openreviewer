[
    {
        "title": "GeoLLM: Extracting Geospatial Knowledge from Large Language Models"
    },
    {
        "review": {
            "id": "aLr8uJtKCN",
            "forum": "TqL2xBwXP3",
            "replyto": "TqL2xBwXP3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission776/Reviewer_92yi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission776/Reviewer_92yi"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores possible ways to extract and tease out the geospatial information embedded inside the knowledge bases like LLMs. Specifically, they fine-tune LLMs such as chatGPT and Llama-2 with training data curated using OpenMaps with labels derived from various sources for geospatial tasks. On a variety of tasks, they show that fine-tuning LLMs on such data can effectively outperform several baselines. They further establish the geographical consistency of their results as well as ablation on suitable prompts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper reveals an innovative way to use LLMs towards a expert tasks such as computing geospatial metrics without access to various kinds of data. \n\n- The access to various covariates, as mentioned, might not always be available - so the motivation of this work is sound and valid.\n\n- The experiments cover a wide variety of tasks and settings, indicating the power of the proposed approach.\n\n- The experiments involve comparison between multiple LMs like RoBERTa, GPT3.5 and LLama-2, with several useful observations pertaining to performance as well as the data efficiency of each of those."
                },
                "weaknesses": {
                    "value": "- There are no experiments which really delineate where the performance improvements of the final model are coming from. The chosen baselines are not that strong. To delineate the knowledge offered by the LLMs compared to that of the training data, can the authors also conduct an experiment where they fine-tune a normal neural network (not LLM) on the created training data? For example, we can first pass the prompt to a sentence encoder, and then train an MLP/Neural Network which uses this embedding to predict the output as a regression or classification task  (using the same training data used during fine-tuning). This would separate the contributions offered by the LLM compared to the training data which was used in fine-tuning. \n\n-  Adding to the above, can the authors also provide results with using zeroshot capabilities of the LLMs? This would help to separate the importance of fine-tuning on custom prompts. \n\n- Assuming the covariates are indeed available, does the current method offer complimentary benefits to further improve the performance?\n\n- The authors, if possible, should also make comparisons with fine-tuning several other geo-spatial LLM type models, like K2 (Deng et. al. 2023) through fine-tuning on the same set of data."
                },
                "questions": {
                    "value": "Overall, I have several requests to further delineate the performance improvements due to various aspects, while also using stronger baselines as detailed above. I am ready to raise my rating upon satisfactory rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission776/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697854081317,
            "cdate": 1697854081317,
            "tmdate": 1699636005182,
            "mdate": 1699636005182,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "smQtXKEDzb",
                "forum": "TqL2xBwXP3",
                "replyto": "aLr8uJtKCN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 92yi"
                    },
                    "comment": {
                        "value": "Thank you for acknowledging the novelty of GeoLLM, the clear use cases and motivation, and constructive criticism. Your suggestions have provided valuable directions for further strengthening our work. Please find our responses to each of your points below:\n\n**Q1: Use of Stronger Baselines**\n\n**A1:** Please see \u201cStronger baselines\u201d in the overall response. We think that \u201cFull set of ablations\u201d in the overall response may also be helpful.\n\n**Q2: Zero-shot Capabilities of LLMs**\n\n**A2:** Please see \u201cZero-shot and few-shot experiments\u201d in the overall response.\n\n**Q3: Complementarity with Available Covariates**\n\n**A3:** We added an additional baseline \u201cNighlight\u201d in Table 1 and Table 3. Please refer to \u201cStronger baselines\u201d in our general response for the description and discussion of this baseline. The Nightlight baseline provides insights into the baseline performance attainable using a well-established satellite image based method. Notably, LLMs seem to be far more robust on a wide variety of tasks when compared to this baseline. Somewhat predictably, Nightlight is only particularly strong on the population tasks. This supports the idea that LLMs are a richer source of knowledge and a better covariate compared to the well-established nightlight satellite imagery. In practice, GeoLLM, satellite imagery, and other covariates would be used together for the best predictions.\n\n**Q4: Finetuning Other Geo-spatial LLM Models**\n\n**A4:** The main contribution of the paper is to present a novel method to extract geospatial knowledge from LLMs. It would be very interesting to determine whether or not the additional domain specific pre-training and finetuning of geospatial LLMs such as K-2 embeds additional knowledge that can be leveraged by GeoLLM. However, we believe that this investigation would be orthogonal from the main purpose of GeoLLM, which was simply to show that LLMs are new covariates and to present a method to use them as such. We leave this investigation to future work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission776/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513720821,
                "cdate": 1700513720821,
                "tmdate": 1700513720821,
                "mdate": 1700513720821,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "owuXojLMGP",
                "forum": "TqL2xBwXP3",
                "replyto": "aLr8uJtKCN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder for Reviewer 92yi"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nAs we are approaching the discussion deadline, we hope that your concerns are resolved and that you can reconsider our scores. Thank you very much for your time."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission776/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633482674,
                "cdate": 1700633482674,
                "tmdate": 1700633482674,
                "mdate": 1700633482674,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WXyNXhlg5X",
            "forum": "TqL2xBwXP3",
            "replyto": "TqL2xBwXP3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission776/Reviewer_Gcfb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission776/Reviewer_Gcfb"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new method called GEOLLM, which can effectively extract spatial knowledge from large language models and fine tune it by using auxiliary map data from OpenStreetMap. Experiments on various tasks of several large-scale real-world sets show that the method is practical and effective. In the experiment, the author found that the pre-training language model has rich geospatial knowledge, and their method can unlock this knowledge. In addition, the author also discusses how to construct appropriate hints to extract geospatial knowledge, and how to find a balance between knowledge extraction and sample efficiency.\nThe contributions of the paper include demonstrating the quantity, quality, and scalable nature of geospatial knowledge contained in LLMs and presenting a simple and efficient method for extracting this knowledge."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1: This paper delves into the internal structure of large-scale language models in extracting geographical knowledge. Utilizing auxiliary map data from OpenStreetMap to extract geospatial knowledge from large language models represents a new approach that has been sparingly explored in previous research. By fine-tuning basic models of different structures and scales and employing prompt templates containing addresses and nearby places, geographical knowledge within pre-trained models is effectively extracted. \nS2: In terms of significance, this paper addresses a challenge in natural language processing by exploring how to extract geospatial knowledge from large-scale language models. The paper proposes a new approach to knowledge extraction based on fine-tuning and prompt strategies, establishing a new concept of \"geospatial covariates.\""
                },
                "weaknesses": {
                    "value": "W1: Lack of clear prompt templates and answer pairs design. The paper mentions the use of multiple data sources and various tasks; however, there are significant deficiencies in the theoretical approach. Specifically, the paper does not clearly specify whether the prompt formats are the same for each task and lacks detailed descriptions of how prompt templates and answer pairs were designed for different tasks. This lack of clarity makes it challenging for readers to understand the experimental design and poses obstacles for replication and further research.\nW2: Lack of clear experimental design details. Furthermore, in terms of experimental design, there are shortcomings in this paper. It does not provide a clear overview of the experimental training process, including the selection of data, preprocessing steps, and hyperparameter tuning. This omission makes it difficult to evaluate the detailed experimental procedure, hindering the reproducibility of the experiments.\nW3: Absence of comparison with ground-truth. Additionally, the paper does not thoroughly explore the comparison between experimental results and real-world geospatial data (Ground Truth). Accuracy of geospatial knowledge is crucial in practical applications; however, the paper lacks comparative analysis with actual geographical data."
                },
                "questions": {
                    "value": "Besides what is mentioned in weaknesses, what does \u201cthe prompts\u2019 mean in \u201cThis suggests that the model is less prone to overfitting on the prediction tasks compared to the prompts\u201d on page 4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission776/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission776/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission776/Reviewer_Gcfb"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission776/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698635351028,
            "cdate": 1698635351028,
            "tmdate": 1699636005086,
            "mdate": 1699636005086,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UVyTra3HGt",
                "forum": "TqL2xBwXP3",
                "replyto": "WXyNXhlg5X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Gcfb"
                    },
                    "comment": {
                        "value": "Thank you for your insightful review and the recognition of the novel approach and significance of GeoLLM in extracting geographical knowledge from large language models using OpenStreetMap data. We appreciate your constructive feedback on the areas needing clarification and improvement. Below, we address your concerns and questions:\n\n**Q1: Lack of Clear Prompt Templates and Answer Pairs Design**\n\n**A1:** We added Appendix A.1 to provide details on the prompt template that we used across all tasks as well as an additional prompt example that we use in the asset wealth task. You can find details for the source of the auxiliary data used to construct the prompts in section 3.2. We hope this aids in the replicability of our results.\n\n**Q2: Lack of Detailed Experimental Design**\n\n**A2:** We added Appendix A.2 to describe the finetuning process and the exact hyperparameters used. Details on the preprocessing of the data for prompts are provided in the last paragraph of section 3.1. Details on the ground truth data for tasks, such as their sources or any task-specific preprocessing that we do, are provided in section 4.1.\n\n**Q3: Absence of Comparison with Ground-Truth Data**\n\n**A3:** Pearson\u2019s $r^2$, which is the correlation squared, is shown for every model across all tasks in table 1. These are calculated using the ground truth and the predictions. We also show the mean absolute errors in an identically structured table in Appendix A.3. We clarify the fine-tuning process of the LLMs in Appendix A.2 where we describe how we use unsupervised learning on the prompts and labels (ground truth) concatenated. We also try to use the word \u201clabels\u201d more to be more clear about the fine-tuning process and the fact that we use ground truth. Please let us know if this is a sufficient clarification or if we are misunderstanding your point.\n\n**Q4: Clarification on the Statement Regarding Overfitting**\n\n**A4:** We clarify in section 3.3 and Appendix A.2 that we are using unsupervised learning with the prompts and labels concatenated. While the model may overfit (increase in loss on test set) with this unsupervised learning at the last few epochs, it does not seem to be overfitting on the actual \u201clabels\u201d or ground truth that were concatenated which is indicated by the continued improvement in performance on the test sets.\n\nWe encourage you to see our overall response to see how we have strengthened our paper further or to answer any more questions you have."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission776/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513545501,
                "cdate": 1700513545501,
                "tmdate": 1700513545501,
                "mdate": 1700513545501,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "exv6DClJTL",
                "forum": "TqL2xBwXP3",
                "replyto": "WXyNXhlg5X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder for Reviewer Gcfb"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nAs we are approaching the discussion deadline, we hope that your concerns are resolved and that you can reconsider our scores. Thank you very much for your time."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission776/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633430786,
                "cdate": 1700633430786,
                "tmdate": 1700633430786,
                "mdate": 1700633430786,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sbi9HCKfrX",
            "forum": "TqL2xBwXP3",
            "replyto": "TqL2xBwXP3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission776/Reviewer_tFFk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission776/Reviewer_tFFk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes GeoLLM, a prompt method that enriches geographic coordinates with auxiliary map data for geospatial prediction task (e.g., population density, economic livelihoods). This prompt approach outperforms some baseline methods in several tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.  This paper is well structured and good at the clarity of presentation.\n\n2.  The significance of the problem is high, potentially impacting a wide range of geospatial applications and offering a new way to view large language model capabilities.\n\n3. The paper provides a detailed discussion of how large language models can be linked to geospatial applications. This can potentially bring new research opportunities to the field of spatial data mining."
                },
                "weaknesses": {
                    "value": "1.  The method presented seems to build incrementally on existing methodologies to generate auxiliary texts for prompt engineering, which may not represent a significance in technical novelty.\n\n2.  The scope of experiments could be further expanded to support the conclusion. Please consider the Questions below for details."
                },
                "questions": {
                    "value": "1.  While Peason's r^2 is used for comparisons with previous literature, can I see more comparisons with baselines on other evaluation metrics like MAE or MSE?\n    \n2.  Could the paper present the label distribution to evaluate performance variations between urban and rural areas, or densely versus sparsely populated regions?\n    \n3.  It would be beneficial to include a wider range of baselines, particularly \"weaker\" text embedding models like Doc2Vec, ELMo, and BERT, paired with basic predictors, for a holistic performance comparison.\n    \n4.  I tried to use GPT 4.0 for the example in Figure 1, and it looks like it can solve this question pretty well with only coordinates:  \n    \"\"\"User: Coordinates:  \n    (40.76208, -73.98042)  \n    Population Density (On a Scale from 0.0 to 9.9):\n    \n    ChatGPT:  \n    Given the coordinates (40.76208, -73.98042), this location is in New York City, specifically in Manhattan. Manhattan has one of the highest population densities of any area in the United States.  \n    On a scale from 0.0 to 9.9, where 9.9 would represent the most densely populated, Manhattan would be close to the top of that scale. While I can't provide an exact number without more specific criteria for each point on the scale, it would be reasonable to rate Manhattan, particularly Midtown which is where these coordinates roughly point to, above 9.0 on a density scale capped at 9.9, considering its population density surpasses 10,000 people per square kilometer.\n    \n    User:  \n    Just return a number\n    \n    ChatGPT:  \n    9.5  \n    \"\"\"  \n    The paper could enhance its soundness by examining the performance with more powerful LLMs. Performance insights using LLama 13b and 70b, as well as the latest GPT-4 model, would be valuable.\n\n\n\n\n5.  Can I see the performance on LLMs without finetuning? It would be necessary to see the impact of fine-tuning in geospatial predictions.\n    \n6.  Few-shot learning capabilities are an intriguing aspect of LLMs. An analysis of the model\u2019s few-shot performance, both pre and post fine-tuning, could significantly add to our understanding of the method's efficiency and flexibility.\n    \n7.  The current ablation study does not fully clarify the independent contribution of coordinates to the model's performance. Results from experiments that exclude coordinates from prompts would help demonstrate that the enhanced performance is not solely attributable to the auxiliary map data."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission776/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698636574936,
            "cdate": 1698636574936,
            "tmdate": 1699636005028,
            "mdate": 1699636005028,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zCbxrpr0RL",
                "forum": "TqL2xBwXP3",
                "replyto": "sbi9HCKfrX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tFFk"
                    },
                    "comment": {
                        "value": "Thank you for your positive evaluation of our paper, recognition of the clear presentation, and acknowledgment of the potential impact of our work in geospatial applications. We appreciate your constructive feedback and have addressed your concerns and questions as follows:\n\n**Q1: Incremental nature of methodology**\n\n**A1:** In addition to showing the importance of comprehensive prompts constructed from auxiliary map data, we revise the paper to show the importance of fine-tuning for overall performance. In particular we find that unsupervised learning (used for the pretraining of the LLMs) for fine-tuning works very well. The details of this finetuning for LLMs are included in section 3.3. Experiments comparing few-shot and finetuning are included in Appendix A.4.\n\n**Q2: Expansion of the scope of experiments**\n\n**A2:** With the revision, we add a sentence embeddings baseline, satellite imagery baseline, results for GPT-2 with GeoLLM, 3 new ablations, few-shot performance with GPT-3.5 and GPT-4, and a preliminary evaluation of the biases of LLMs. Please see \u201cZero-shot and few-shot experiments\u201d, \u201cFull set of ablations\u201d, and \u201cPreliminary investigation of bias\u201d in the overall response.\n\n**Q3: Additional evaluation metrics (MAE, MSE)**\n\n**A3:** As a potentially more interpretable metric, we present the mean absolute error (MAE) for all models across all tasks and training sample sizes in Table 3 in Appendix A.3. One can observe that the same conclusions that are made with Pearson's $r^2$ can be made when comparing the MAE of the various models. In particular, the relative ranking in performance of the models within tasks is consistent with Pearson's $r^2$.\n\n**Q4: Performance variation in different geographical contexts**\n\n**A4:** While there does not appear to be any obvious signs of performance bias across countries or continents as seen in fig. 2, the existence of biases in performance is inevitable as the internet training corpora of LLMs are inherently biased towards developed and densely populated areas. We show preliminary signs of this bias in table 5 in Appendix A.5. We find that there is a moderate increase in mean absolute error between densely populated and sparsely populated areas and another moderate increase between the areas with above median asset wealth and below median asset wealth. As we discussed in section 5, this demonstrates that GeoLLM has the potential to be used as a tool to reveal LLM biases on a geographical scale. However, substantial additional research is required for a comprehensive analysis of the geospatial biases in LLMs and their training corpora.\n\n**Q5: Comparison with \"weaker\" text embedding models and a wider variety of baselines**\n\n**A5:** Please see \u201cStronger baselines\u201d in the overall response.\n\n**Q6: Performance of more powerful LLMs**\n\n**A6:** Please see \u201cZero-shot and few-shot experiments\u201d in the overall response.\n\n**Q7: Impact of fine-tuning on performance**\n\n**A7:** Please see \u201cZero-shot and few-shot experiments\u201d in the overall response.\n\n**Q8: Few-shot learning capabilities**\n\n**A8:** Please see \u201cZero-shot and few-shot experiments\u201d in the overall response.\n\n**Q9: Independent contribution of coordinates**\n\n**A9:** Please see \u201cFull set of ablations\u201d in the overall response."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission776/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513388431,
                "cdate": 1700513388431,
                "tmdate": 1700513388431,
                "mdate": 1700513388431,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XqOpqOnTal",
                "forum": "TqL2xBwXP3",
                "replyto": "sbi9HCKfrX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder for Reviewer tFFk"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nAs we are approaching the discussion deadline, we hope that your concerns are resolved and that you can confirm that this is the case. Thank you very much for your time."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission776/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633385739,
                "cdate": 1700633385739,
                "tmdate": 1700633385739,
                "mdate": 1700633385739,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Hqj5SHk7RV",
            "forum": "TqL2xBwXP3",
            "replyto": "TqL2xBwXP3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission776/Reviewer_5MUc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission776/Reviewer_5MUc"
            ],
            "content": {
                "summary": {
                    "value": "This study explores leveraging large language models (LLMs) for geospatial prediction tasks, addressing limitations of traditional covariates like satellite imagery. The authors introduce GeoLLM, an approach that effectively extracts geospatial knowledge from LLMs with auxiliary map data. The proposed method demonstrates a 70% improvement in performance compared to baselines, rivaling satellite-based benchmarks. GPT-3.5 outperforms other models, highlighting the scalability of the proposed approach. This research underscores LLMs' efficiency, global robustness, and potential to enhance geospatial analysis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It proposed a novel method for efficiently extracting geospatial knowledge from large language models.\n2. The paper outlined experiments to evaluate extracting geospatial knowledge from large language models, which included constructing a comprehensive benchmark, developing a robust set of baselines, and presenting results and an ablation study.\n3. The paper revealed that GeoLLMs are sample-efficient, rich in geospatial information, and robust across the globe."
                },
                "weaknesses": {
                    "value": "1. The paper does not provide a detailed analysis of the potential biases of LLMs and their training corpora. \n2. It would be better to compare the GeoLLM\u2019s performance with the results from satellite images."
                },
                "questions": {
                    "value": "Have you tried the zero-shot or few-shot performance of various LLMs on the presented tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission776/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission776/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission776/Reviewer_5MUc"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission776/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727384428,
            "cdate": 1698727384428,
            "tmdate": 1699636004954,
            "mdate": 1699636004954,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iRuhJQj27T",
                "forum": "TqL2xBwXP3",
                "replyto": "Hqj5SHk7RV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5MUc"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful feedback, recognition of GeoLLM\u2019s novelty and of our broad analysis across multiple relevant benchmark datasets with a comprehensive list of baselines and ablations. We appreciate your suggestions on investigating potential biases, few-shot or zero-shot performance, and on comparing with satellite images. You may find responses to your questions below:\n\n**Q1: Lack of a detailed analysis on biases of LLMs and training datasets**\n\n**A1:** While there does not appear to be any obvious signs of performance bias across countries or continents as seen in fig. 2, the existence of biases in performance is inevitable as the internet training corpora of LLMs are inherently biased towards developed and densely populated areas. We show preliminary signs of this bias in Table 5 in Appendix A.5. We find that there is a moderate increase in mean absolute error between densely populated and sparsely populated areas and another moderate increase between the areas with above median asset wealth and below median asset wealth. As we discussed in section 5, this demonstrates that GeoLLM has the potential to be used as a tool to reveal LLM biases on a geographical scale. However, substantial additional research is required for a comprehensive analysis of the geospatial biases in LLMs and their training corpora.\n\n**Q2: Comparison with methods that use satellite imagery**\n\n**A2:** Please see the Nightlight baseline in the \u201cStronger baselines\u201d section of the overall response.\n\n**Q3: Including results from zero-shot or few-shot prompting**\n\n**A3:** Please see \u201cZero-shot and few-shot experiments\u201d in the overall response."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission776/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513229157,
                "cdate": 1700513229157,
                "tmdate": 1700513229157,
                "mdate": 1700513229157,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1zkOB77CLy",
                "forum": "TqL2xBwXP3",
                "replyto": "Hqj5SHk7RV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder for Reviewer 5MUc"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nAs we are approaching the discussion deadline, we hope that your concerns are resolved and that you can reconsider our scores. Thank you very much for your time."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission776/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633294678,
                "cdate": 1700633294678,
                "tmdate": 1700633294678,
                "mdate": 1700633294678,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2m1HJE2dOK",
            "forum": "TqL2xBwXP3",
            "replyto": "TqL2xBwXP3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission776/Reviewer_xypS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission776/Reviewer_xypS"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method to extract prior geospatial knowledge from pretrained LLM. \nThe queried knowledge includes spatial demographics data, census data, and survey data. \nThe findings in the paper are interesting, however seem very preliminary. And the experiments are limited."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is well presented, and the experiments cover several different geospatial datasets and tasks in relation to census and demographic data."
                },
                "weaknesses": {
                    "value": "\u2022\tIt seems that the proposed GeoLLM can only perform one specific task. After fine-tuning, is the fine-tuned LLMs (e.g., GPT-3.5) able to retain the ability to answer general questions that is not related to the specific task? Including some discussions about the generalization part could be useful.\n\n\u2022\tThe proposed model can only handle static/tabular geo information. It does not handle other types of spatial data, or spatiotemporal data and tasks.\n\n\u2022\tBaselines are too simple. Considering some more recent deep-learning-based or Transformer-based baselines could be more convincing.\n\n\u2022\tThe experiments are limited."
                },
                "questions": {
                    "value": "\u2022\tIs there any specific tokenization process introduced for GPS coordinates? Normally, the direct tokenizers with LLMs could split the GPS point into several different tokens, which might undermine the ability to understand GPS coordinates correctly.\n\n\u2022\tWhy using classification setting? The current loss functions that LLMs are using is a kind of binary, where only predictions that exactly match the masked word are considered correct and rewarded, while all other predictions are considered incorrect and penalized. So, there is no has no awareness and no sense of being close to or far away the correct answer. Normally, for the task described in this paper, this kind of sense should be valuable in getting better performance.\n\n\u2022\tWhat about the deployment cost of the proposed GeoLLM? For example, compared to the baselines, what are the fine-tuning costs and inference costs of using GPT-3.5 or other LLMs. From my experience, fine-tuning GPT-3.5 can be very expensive."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission776/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756039090,
            "cdate": 1698756039090,
            "tmdate": 1699636004862,
            "mdate": 1699636004862,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e2xLn0sjZl",
                "forum": "TqL2xBwXP3",
                "replyto": "2m1HJE2dOK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xypS"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful feedback, recognition of GeoLLM\u2019s evaluation across a broad set of relevant geospatial datasets and tasks, and suggestions for clarifying our design choices. You may find responses to your concerns below:\n\n**Q1: GeoLLM performs a specific task/ does it answer general questions?**\n\n**A1:** GeoLLM is not intended to retain natural language or general-purpose response capabilities. It is a method to introduce a novel set of covariates that can enhance various downstream geospatial ML tasks that currently depend on traditional covariates such as satellite images. These tasks include poverty estimation, food security, biodiversity preservation, and environmental conservation. Thus, we can quickly adapt powerful LLMs to a variety of geospatial tasks using our novel prompting strategy. Moreover, we demonstrate that a high level of performance is achievable with relatively cheap finetuning using methods such as QLoRA. \n\n**Q2: Can GeoLLM handle different types of spatial/spatio-temporal data**\n\n**A2:** As LLMs have not yet been shown to be effective covariates for geospatial ML tasks, we focus on providing a method that enables this by efficiently extracting their spatial knowledge. In fact, there are two general settings for spatial prediction: location modeling and spatial contextual modeling [1]. The former aims at predicting the attributes/characteristics of a place given its current location. The latter focuses on predicting the attributes/characteristics of a place given its location as well as its spatial context. Our GeoLLM is able to handle both types of tasks. The spatial context modeling corresponds to our current GeoLLM setting since we give the location and description of the current location as well as its nearby places extracted from OpenStreetMap (See Figure 1 (b) bottom). The location modeling corresponds to the setting in which we delete the \u201cNearby place\u201d in our prompt. We believe we are the first one that shows LLMs\u2019 abilities on a wide range of geo-tasks. We acknowledge that there are other geo-tasks such as spatial relation prediction between two places, or questions requiring spatial footprints more than points (e.g., polylines and polygons). We treat them as future work.\n\n**Q3: Simplicity of baselines**\n\n**A3:** Please see \u201cStronger baselines\u201d in the overall response.\n\n**Q4: Limited experiments**\n\n**A4:** Please see \u201cStronger baselines\u201d,  \u201cZero-shot and few-shot experiments\u201d, and \u201cFull set of ablations\u201d in the overall response.**\n\n**Q5: Tokenizer for geospatial coordinates**\n\n**A5:** We recognize the tokenization problem of the current LLM for geo-coordinates.  Current LLMs break up the GPS coordinates into multiple tokens and makes understanding them more difficult, especially for smaller LLMs like GPT-2. While larger models such as GPT-3.5 are better able to utilize the numeric value of the coordinates, as shown in the \u201conly coordinates'' ablation in Table 2, the coordinates do not affect the final performance of GPT-3.5 significantly (bump from 0.72 to 0.73 Pearson\u2019s $r^2$) as shown in the \u201cno coordinates'' ablation in table 2.  In fact, GeoLLM is motivated by this observation and is developed to overcome this limitation. How to develop a tokenizer that makes LLM better understand geo-coordinate is a very interesting research direction but is orthogonal to our GeoLLM framework. We leave it as one of our future work.\n\n**Q6: Relevance of framing as a classification problem**\n\n**A6:** Making geospatial predictions via the generation of tokens is significantly more difficult than regression. As one would expect, GPT-2 struggles to perform well within this paradigm, even doing much worse than MLP-BERT that uses regression. For this very reason, we deliberately introduced decimals in the labels (0.0 to 9.9 as opposed to 0 to 99) to add consistency to the predictions as they always require exactly 3 tokens to generate (e.g. \u201c5\u201d, \u201c.\u201d, \u201c3\u201d). However, to our surprise, the autoregressive nature of LLMs not seem to hinder GPT-3.5 and Llama 2 significantly, with GPT-3.5 exceeding the performance of all models that use regression, including RoBERTa and the newly introduced MLP-BERT and Nightlight baselines. In addition, as GPT-3.5 only allows for fine-tuning through OpenAI\u2019s API, we cannot make any modifications to the tokenization or architecture of the best performing model by far.\n\n**Q7: Fine-tuning and inference costs**\n\n**A7:** We present the costs of the finetuning of the LLMs we use in Appendix A.2. In short, fine-tuning GPT-3.5 with 10,000 samples costs 33 dollars. Thanks to its sample efficiency, one can finetune it with 1,000 samples for 5 dollars without much sacrifice to the overall performance. As a set of predictions only needs to be made once or a small number of times for a specific application, the deployment cost is not as significant or as much of a burden as with general-purpose LLMs."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission776/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513035686,
                "cdate": 1700513035686,
                "tmdate": 1700513035686,
                "mdate": 1700513035686,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8fPCWprb8c",
                "forum": "TqL2xBwXP3",
                "replyto": "2m1HJE2dOK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xypS (Continued)"
                    },
                    "comment": {
                        "value": "References:\n\n[1] Mai, Gengchen, Krzysztof Janowicz, Bo Yan, Rui Zhu, Ling Cai, and Ni Lao. \"Multi-Scale Representation Learning for Spatial Feature Distributions using Grid Cells.\" In International Conference on Learning Representations. 2020."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission776/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700513140435,
                "cdate": 1700513140435,
                "tmdate": 1700513140435,
                "mdate": 1700513140435,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m5RpLA8M5n",
                "forum": "TqL2xBwXP3",
                "replyto": "2m1HJE2dOK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission776/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder for Reviewer xypS"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nAs we are approaching the discussion deadline, we hope that your concerns are resolved and that you can reconsider our scores. Thank you very much for your time."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission776/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633236716,
                "cdate": 1700633236716,
                "tmdate": 1700633236716,
                "mdate": 1700633236716,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]