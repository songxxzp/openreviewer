[
    {
        "title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets"
    },
    {
        "review": {
            "id": "VC9qQQQ5cR",
            "forum": "fW7DOHDQvF",
            "replyto": "fW7DOHDQvF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4492/Reviewer_XSqD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4492/Reviewer_XSqD"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel approach to weakly supervised learning, specifically targeting multi-class classification from multiple unlabeled datasets with class priors. It addresses limitations identified in Tang et al., 2022, and offers two distinct approaches: a classifier-consistent method using a probability transition matrix and a risk-consistent approach employing importance weighting. However, certain areas require further clarification and empirical investigation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper successfully addresses limitations observed in the MCMU method and presents an alternative solution, expanding the landscape of weakly supervised learning.\n- The introduction of a novel approach focusing on empirical risk minimization (ERM) at the instance level, treating individual data points, stands out as a significant strength."
                },
                "weaknesses": {
                    "value": "- The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points. Moreover, presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.\n- The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as m (the number of unlabeled sets) and n_i (the number of data points in each set), which are crucial for practical applicability.\n- Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels."
                },
                "questions": {
                    "value": "1. The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to m and n_i.\n   \n2. A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.\n\n3. Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.\n\n4. A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.\n\n5. The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.\n\nOther suggestions:\n\n\"In MCMU, given the class priors, the data points in the same set are independent from each other, while in LLP, given the label proportions, the data points in\nthe same set are dependent from each other.\"\n\nThe second part of the sentence should clearly state whether data points are \"independent from each other\" or \"dependent on each other.\" If it implies independence, it raises a significant ambiguity regarding how MCMU and LLP fundamentally differ, as class priors and label proportions are practically equivalent concepts. Moreover, there appears to be a conceptual misuse in the paragraph. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches. \n\nI think using the term \"label proportions\" when referring to class priors within each set would be more appropriate. In fact what the paper denotes as the j-th class prior of the i-th unlabeled set can be denoted as the label proportion of class j in dataset i. This would distinguish label proportions from class priors estimated using data from all available unlabeled sets.\n\n\nIn Section 4.1. \"The ni data points contained in i-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\".  Data already exists... nothing is supposed to be generated.\n\nIt would greatly enhance the paper if, within the related work section, an early explanation were provided regarding the reasons behind the possibility of negative empirical risk in these methods (Lu et al., 2019; Tsai & Lin, 2020; Tang et al., 2022). This proactive approach would help readers anticipate and better understand the issues related to negative empirical risk that the paper seeks to address.\n\nIn page 4 i is used for indexing both the sets and data points. It may cause confusions. \n\nIn Theorem 3.6 the paper defines this probability p(y=j|\\bar{y},x)as the probability of x in \\bar{y}-th unlabeled set whose ground-truth label is j. Isn't this supposed to be the probability of x in \\bar{y}-th unlabeled set belonging to class j? x may or may not belong to class j. x's ground truth label is not necessarily j. \n\nTheorem 3.6 also considers p(x|y=j)=p(x|y=j,\\bar{y}) as a fact. However, this is not necessarily true. The first one is the conditional distribution of class j estimated using all data belonging to class j whereas the second one is the conditional distribution of class j estimated using only data from unlabeled set \\bar{y}. So, from an empirical standpoint these distributions are not equal. Please clarify.\n\n\nAfter Rebuttal:\n\nThanks for taking the time to do those extra comparisons I mentioned. It\u2019s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4492/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4492/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4492/Reviewer_XSqD"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4492/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698070637844,
            "cdate": 1698070637844,
            "tmdate": 1700451462602,
            "mdate": 1700451462602,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YT97zkEPmO",
                "forum": "fW7DOHDQvF",
                "replyto": "VC9qQQQ5cR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer XSqD  (Part 1/4)"
                    },
                    "comment": {
                        "value": "We sincerely thank you for spending so much time reviewing our paper! We are really touched by your strong sense of responsibility and meticulosity! We also appreciate the valuable suggestions you provided for our paper, which can definitely improve the quality of our paper! Our point-by-point responses are provided below.\n\n**Weakness 1: The experimental comparison is somewhat limited, primarily featuring methods from Tang et al., 2022. While justified due to MCMU's novelty, including additional weakly supervised methods could provide valuable reference points.**\n\nMCMU is a relatively new research area that was introduced very recently. To the best of our knowledge, there is only one work [1] that has studied this problem and this work proposes a number of effective methods to solve this MCMU problem. Therefore, we adopted all the methods in [1] as our baselines. We believe that the number of compared methods is sufficient, as we have already compared all the existing methods designed for MCMU. We would be very grateful if you could provide us with more weakly supervised learning methods that can be used for MCMU, and we will definitely include such methods in experimental comparisons.\n\n[1] Tang Y, Lu N, Zhang T, et al. Learning from Multiple Unlabeled Datasets with Partial Risk Regularization. ACML 2022.\n\n**Presenting accuracies for fully supervised learning on the same train/test splits would enhance the evaluation.**\n\nThank you for your advice. We have conducted experiments with fully supervised data. The experimental results have been incorporated into Table 2 and Table 3 for comprehensive evaluations.\n\n**Weakness 2: The paper lacks a comprehensive study of the method's limitations, particularly with regards to constraints on parameters such as $m$ (the number of unlabeled sets) and $n_i$ (the number of data points in each set), which are crucial for practical applicability.**\n\nWe conducted experiments on variants $m$ and $n_i$ and showed the experimental results in Appendix E. Here we provide a detailed description of the additional experimental results.\n\nWe conducted additional experiments with different numbers of sets with fixed data points (i.e. with fixed set number times the set size) on the MNIST dataset. Random matrix and asymmetric matrix are adapted as the class prior matrix. The experimental results are shown in Table 1, Table 2, and Table 3. Our proposed RCM and CCM demonstrate stable performance with the increase in the number of sets and still outperform the baseline methods. When the number of sets becomes excessively large, some baseline methods may achieve poorer performance. This is because in such a case, the distribution $p(x)$ on training data might shift from testing data. Some papers [2,3] refer to this case as a covariate shift, which would lead to degraded performance.\n\nTable 1: Classification performance of each method on the MNIST dataset using an asymmetric prior matrix with variant set numbers.\n\n| Methods   | m=30         | m=50         | m=100        | m=200        | m=300         | m=500         | m=1000       |\n| --------- | ------------ | ------------ | ------------ | ------------ | ------------- | ------------- | ------------ |\n| Unbiased  | 84.30\u00b12.55% | 86.85\u00b11.06% | 86.74\u00b10.88% | 87.75\u00b10.47% | 88.17\u00b10.68%  | 87.46\u00b10.41%  | 62.78\u00b15.07% |\n| U-Stop    | 89.69\u00b11.21% | 89.79\u00b11.59% | 91.05\u00b10.42% | 89.47\u00b10.20% | 89.71\u00b10.58%  | 88.37\u00b10.21%  | 85.23\u00b10.65% |\n| U-Correct | 94.07\u00b10.18% | 94.16\u00b10.23% | 87.00\u00b10.30% | 78.94\u00b11.86% | 63.33\u00b116.18% | 27.04\u00b18.39%  | 26.80\u00b17.59% |\n| U-Flood   | 93.42\u00b10.56% | 93.29\u00b10.34% | 90.55\u00b10.53% | 85.29\u00b10.86% | 79.46\u00b10.83%  | 68.43\u00b110.75% | 56.70\u00b13.33% |\n| Prop      | 82.98\u00b11.55% | 79.85\u00b11.01% | 75.46\u00b11.23% | 72.77\u00b11.60% | 72.62\u00b10.67%  | 71.61\u00b10.51%  | 71.90\u00b10.32% |\n| U-PRR     | 91.11\u00b10.29% | 89.32\u00b10.89% | 79.28\u00b11.34% | 70.88\u00b12.36% | 67.80\u00b10.69%  | 62.76\u00b12.53%  | 67.28\u00b12.11% |\n| CCM       | 95.77\u00b10.12% | 95.66\u00b10.17% | 95.66\u00b10.08% | 95.40\u00b10.14% | 95.69\u00b10.14%  | 95.57\u00b10.20%  | **95.91\u00b10.17%** |\n| RCM       | **95.94\u00b10.23%** | **96.02\u00b10.08%** | **95.95\u00b10.15%** | **95.70\u00b10.23%** | **95.88\u00b10.08%**  | **95.82\u00b10.07%**  | 95.77\u00b10.22% |\n\nContinue with the next post"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700385766760,
                "cdate": 1700385766760,
                "tmdate": 1700389807679,
                "mdate": 1700389807679,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6WA1nkg1Ys",
                "forum": "fW7DOHDQvF",
                "replyto": "VC9qQQQ5cR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer XSqD  (Part 2/4)"
                    },
                    "comment": {
                        "value": "Table 2: Classification performance of each method on the MNIST dataset using a random prior matrix with variant set numbers.\n| Methods   | m=30         | m=50         | m=100         | m=200         | m=300        | m=500        | m=1000       |\n| --------- | ------------ | ------------ | ------------- | ------------- | ------------ | ------------ | ------------ |\n| Unbiased  | 14.14\u00b11.15% | 15.64\u00b11.48% | 13.90\u00b10.70%  | 11.18\u00b12.30%  | 15.12\u00b12.64% | 16.36\u00b11.53% | 13.31\u00b12.95% |\n| U-Stop    | 53.12\u00b13.76% | 71.61\u00b10.52% | 38.41\u00b118.44% | 40.62\u00b124.60% | 18.33\u00b16.65% | 18.27\u00b12.32% | 25.89\u00b13.77% |\n| U-Correct | 63.84\u00b14.61% | 66.64\u00b11.21% | 75.00\u00b10.50%  | 64.24\u00b10.34%  | 64.85\u00b11.51% | 71.59\u00b10.20% | 77.01\u00b10.83% |\n| U-Flood   | 89.14\u00b10.96% | 88.91\u00b10.92% | 85.87\u00b10.42%  | 75.59\u00b10.72%  | 69.92\u00b11.47% | 59.82\u00b12.38% | 50.54\u00b15.46% |\n| Prop      | 81.84\u00b11.36% | 77.29\u00b11.51% | 66.61\u00b12.35%  | 57.96\u00b11.47%  | 54.38\u00b11.12% | 53.38\u00b11.26% | 51.44\u00b11.16% |\n| U-PRR     | 23.92\u00b10.56% | 19.62\u00b11.15% | 19.30\u00b11.30%  | 18.15\u00b11.22%  | 14.97\u00b10.47% | 17.98\u00b11.16% | 13.69\u00b10.53% |\n| CCM       | 92.90\u00b10.29% | 92.49\u00b10.14% | 92.86\u00b10.39%  | 92.37\u00b10.20%  | 93.23\u00b10.37% | 93.00\u00b10.08% | 93.06\u00b10.20% |\n| RCM       | **95.69\u00b10.25%** | **95.63\u00b10.12%** | **95.49\u00b10.17%**  | **95.38\u00b10.18%**  | **95.49\u00b10.07%** | **95.46\u00b10.15%** | **95.78\u00b10.14%** |\n\nTable 3: Classification performance of each method on the MNIST dataset using a random prior matrix with variant set numbers. ($m \\geq k$)\n\n| Methods | Unbiased        | U-Stop           | U-Correct       | U-Flood         | Prop            | U-PRR           | CCM             | RCM             |\n| ------- | --------------- | ---------------- | --------------- | --------------- | --------------- | --------------- | --------------- | --------------- |\n| m=5     | 18.23$\\pm$8.08% | 45.88$\\pm$12.29% | 34.71$\\pm$7.20% | 58.97$\\pm$4.25% | 72.48$\\pm$2.37% | 26.34$\\pm$6.46% | 88.80$\\pm$2.43% | **89.16$\\pm$4.29%** |\n| m=6     | 14.61$\\pm$6.72% | 42.02$\\pm$22.68% | 40.06$\\pm$8.00% | 68.00$\\pm$3.39% | 82.06$\\pm$4.49% | 28.45$\\pm$5.02% | 89.80$\\pm$3.59% | **92.71$\\pm$0.73%** |\n| m=7     | 15.56$\\pm$2.38% | 42.39$\\pm$6.74%  | 49.72$\\pm$8.40% | 77.53$\\pm$7.95% | 89.32$\\pm$3.43% | 32.78$\\pm$5.33% | 90.66$\\pm$2.19% | **94.53$\\pm$1.06%** |\n| m=8     | 15.13$\\pm$0.87% | 37.08$\\pm$16.58% | 32.65$\\pm$2.24% | 79.89$\\pm$4.45% | 88.57$\\pm$3.11% | 27.72$\\pm$0.64% | 89.99$\\pm$2.75% | **94.49$\\pm$1.06%** |\n| m=9     | 14.06$\\pm$1.79% | 27.77$\\pm$8.25%  | 33.80$\\pm$1.88% | 82.83$\\pm$2.78% | 88.31$\\pm$3.68% | 29.79$\\pm$4.22% | 91.42$\\pm$1.26% | **94.44$\\pm$0.91%** |\n\n[2] Yiyang Zhang, Feng Liu, Zhen Fang, Bo Yuan, Guangquan Zhang, and Jie Lu. Clarinet: A one-step approach towards budget-friendly unsupervised domain adaptation. IJCAI 2020.\n\n[3] Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of Statistical Planning and Inference, 90(2):227\u2013244, 2000.\n\n**Weakness 3: Theoretical results like Theorem 3.5, which establishes an upper bound for the difference between true and empirical risk, would benefit from discussion regarding their practical implications and their relationship to the actual classification task involving class labels.**\n\nThe practical implication of Theorem 3.5 lies in the fact that the performance of our method can be improved, by increasing the training data of MCMU. We can also find that the optimal classifier obtained by our method will converge to the optimal classifier (learned by minimizing the actual classification risk involving class labels), as the number of training data of MCMU increases.\n\n**Q1: The paper could further investigate the impact of small m (few unlabeled sets) and significant variations in label proportions across sets on the generalizability of its results with respect to $m$ and $n_i$.**\n\nThank you for your valuable suggestion. We have conducted experiments specifically designed for the case of small values of $m$. The experimental results are reported in Table 3. Please refer to our response to Weakness 2.\n\n**Q2: A clarification regarding the difference between Lemma 3.1 in the paper and Theorem 1 in Lu et al., ICML 2021, would be valuable for readers.**\n\nLemma 3.1 can be considered as a multi-class extension of Theorem 1 (only for binary classification) in Lu et al., ICML 2021. As we indicated in our paper, CCM is a multi-class extension of Lu et al., ICML 2021. We would like to explain that the CCM is not the main contribution of this paper. The main contribution of our paper lies in the proposed RCM.\n\n**Q3: Given that label proportions in real-world scenarios may only slightly differ, the paper should outline constraints on parameters m and n for practical significance, especially considering that a large m may be necessary for the method's effectiveness.**\n\nPlease refer to our response to Weakness 2, where we have conducted experiments on various values of $m$ and $n$.\n\nContinue with the next post"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700385988108,
                "cdate": 1700385988108,
                "tmdate": 1700388568637,
                "mdate": 1700388568637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mF7jk9GnBK",
                "forum": "fW7DOHDQvF",
                "replyto": "VC9qQQQ5cR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer XSqD (3/4)"
                    },
                    "comment": {
                        "value": "**Q4: A discussion on the practical relevance of the derived upper bound in Theorem 3.5 and its connection to the actual classification task involving class labels would provide valuable insights.**\n\nPlease check our response to Weakness 3.\n\n**Q5: The observation that the proposed approach performs significantly worse under certain settings, such as a random class prior matrix and non-constrained m, should be supported by clearer details on how test accuracies were computed, whether test data were balanced, and if class size variations in datasets played a role.**\n\nWe computed the test accuracies on the test set of widely used benchmark datasets, including MNIST, Fashion, Kuzushiji, CIFAR-10, and SVHN. We simply used the given test set and did not make modifications to the class size of the provided test set.\n\nWe also conducted additional experiments on the MNIST, Kuzushiji, and Fashion datasets with imbalanced classes. We constructed the class-imbalanced prior matrix by following a similar procedure to [1]. Firstly, we generate a random prior matrix $\\Theta$. Secondly, we generate a geometric progression $\\boldsymbol{l}$ according to the imbalance ratio $\\rho$, where $\\rho$ denotes the ratio between the largest and smallest elements in $\\boldsymbol{l}$, i.e., $\\boldsymbol{l}=\\{1,\\rho^{1/k-1}\\,\\rho^{2/k-1}\\,\\dots,\\rho\\}$. Thirdly, we divide each row of the prior matrix by this geometric progression, i.e., $\\Theta_{ij}=\\Theta_{ij}/$ $\\boldsymbol{l}{j}$. Finally, we normalize each row of $\\theta$ by $\\Theta_{ij}=\\Theta_{ij}/\\sum_{j=1}^k\\Theta_{ij}$ where $k$ is the class number.\n\nThe experimental results are reported in Table 4. As can be seen from Table 4, our proposed methods RCM and CCM can still achieve satisfactory performance even under extreme class imbalance conditions. In MNIST and Kuzushiji datasets, RCM exhibited \nstronger robustness against class imbalance compared with CCM, as it was less affected as $\\rho$ rises.\n\nTable 4: Classification performance of RCM and CCM trained on imbalanced classes.\n\n|   $\\rho$    | Methods |      MNIST       |     Fashion      |    Kuzushiji    |\n| :---------: | :-----: | :--------------: | :--------------: | :-------------: |\n| 1 (balance) |   CCM   | 91.46$\\pm$1.22%  | 76.09$\\pm$07.57% | 72.85$\\pm$2.52% |\n|             |   RCM   | **94.80$\\pm$0.64%**  | **80.49$\\pm$2.50%**  | **78.36$\\pm$2.89%** |\n|     10      |   CCM   | 89.76$\\pm$0.82%  | 73.21$\\pm$7.96%  | 68.57$\\pm$3.14% |\n|             |   RCM   | **93.27$\\pm$1.05%**  | **75.36$\\pm$3.53%**  | **74.03$\\pm$2.67%** |\n|     100     |   CCM   | 72.49$\\pm$14.81% | 64.77$\\pm$2.46%  | 53.54$\\pm$3.43% |\n|             |   RCM   | **85.32$\\pm$2.75%**  | **65.43$\\pm$5.26%**  | **62.69$\\pm$2.31%** |\n\n[1] Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss, NeurIPS 2019.\n\n**Other suggestions.**\n\nWe have incorporated the necessary changes according to your suggestions into our revised paper. Below we provide our responses to your suggestions.\n\n***Clearly state whether data points are \"independent from each other\" or \"dependent on each other for MCMU. Data points within each class are typically assumed to be independent, conditioned on their class labels. The statement in question uses \"class priors\" in place of \"class labels\", which could introduce confusion and should be clarified. Overall, given that label proportions and class priors are practically the same thing in the context studied by the papers it is hard to see any difference from a dependence perspective between the two approaches.***\n\nThank you for your valuable suggestion! We have revised the description in our paper. In the LLP problem, data points are dependent on each other. \nThe label proportions and class priors are two different notions in our paper, and we describe the differences between them in the following response.\n\n***I think using the term \"label proportions\" when referring to class priors within each set would be more appropriate.***\n\nThank you for your thoughtful consideration of our paper.\nIn the MCMU problem, the unlabeled set is sampled according to class priors. In our humble opinion, we think that using class priors is more appropriate for MCMU. Although class priors and label proportions seem to be practically equivalent concepts, they actually describe two different sampling processes. In LLP, the notion of \"label proportions\" means that we calculate the proportion of instances belonging to each class after generating the data according to the underlying data distribution $p(x,y)$.\n\nContinue with the next post"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700387777025,
                "cdate": 1700387777025,
                "tmdate": 1700390867595,
                "mdate": 1700390867595,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GQcnc2Es2A",
                "forum": "fW7DOHDQvF",
                "replyto": "VC9qQQQ5cR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer XSqD  (Part 4/4)"
                    },
                    "comment": {
                        "value": "***In Section 4.1. \"The $n_i$ data points contained in $i$-th unlabeled set were randomly generated according to ...\" Isn't this supposed to be \"randomly sampled\" or \"drawn\". Data already exists... nothing is supposed to be generated.***\n\nThank you for pointing out this issue and we have corrected it in our paper.\n\n***It would greatly enhance the paper if, within the related work section, an early explanation was provided regarding the reasons behind the possibility of negative empirical risk.***\n\nThank you for this suggestion. There are two important reasons that we did not provide a detailed explanation/analysis of the negative empirical risk of previous work. Firstly, many previous studies on various weakly supervised learning problems have exhibited the negative empirical risk shared by common methods, such as positive-unlabeled learning [2], unlabeled-unlabeled learning [3], complementary-label learning [4], and so on. Thus we consider that this issue was commonly known and may not be necessary to be shown in this paper. Secondly, the page limit will also prevent us from adding a detailed explanation/analysis of the negative empirical risk of previous work. We really appreciate this suggestion and we will consider incorporating this part if our paper is accepted, because we are only allowed to use one more page for the camera-ready version. \n\n[2] Positive-unlabeled learning with non-negative risk estimator. NeurIPS 2017.\n\n[3] Mitigating Overfitting in Supervised Classification from Two Unlabeled Datasets: A Consistent Risk Correction Approach. AISTATS 2020.\n\n[4] Complementary-label learning for arbitrary losses and models. ICML 2019.\n\n***On page 4 $i$ is used for indexing both the sets and data points. It may cause confusion.***\n\nThank you for pointing out this issue and we have corrected it in our paper.\n\n***In Theorem 3.6 the paper defines this probability $p(y=j|\\overline{y},x)$ as the probability of $x$ in $\\overline{y}$-th unlabeled set whose ground-truth label is $j$. Isn't this supposed to be the probability of $x$ in $\\overline{y}$-th unlabeled set belonging to class $j$? $x$ may or may not belong to class $j$. $x$'s ground truth label is not necessarily $j$.***\n\nYes, Your understanding is correct. $p(y=j|\\overline{y},x)$ denotes the probability of $x$ in $\\overline{y}$-th unlabeled set belonging to class $j$.\n\n***Theorem 3.6 also considers $p(x|y=j)=p(x|y=j,\\overline{y})$ as a fact. However, this is not necessarily true. The first one is the conditional distribution of class $j$ estimated using all data belonging to class $j$ whereas the second one is the conditional distribution of class $j$ estimated using only data from unlabeled set $\\overline{y}$. So, from an empirical viewpoint, these distributions are not equal. Please clarify.***\n\nFor the data generation of the MCMU problem, $p(x|y=j)=p(x|y=j,\\overline{y})$ always holds. This is because, for MCMU, we need to first sample the information of true labels according to the class priors of each unlabeled set (i.e., $\\overline{y} \\rightarrow y$) and then sample the instances of each class according to the class-conditional density $p(x|y=j)$ (i.e., $y\\rightarrow x$). Hence the whole data generation process is $\\overline{y}\\rightarrow y\\rightarrow x$, which means, given the true label $y$, $x$ is independent of $\\overline{y}$, i.e., $p(x|y=j)=p(x|y=j,\\overline{y})$.\n\n---------------------\n\nFinally, thank you again for your huge efforts in our paper. We sincerely hope that everything goes well with you!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700387972601,
                "cdate": 1700387972601,
                "tmdate": 1700391342587,
                "mdate": 1700391342587,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cnoScPKpv9",
                "forum": "fW7DOHDQvF",
                "replyto": "GQcnc2Es2A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4492/Reviewer_XSqD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4492/Reviewer_XSqD"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for taking the time to do those extra comparisons I mentioned. It\u2019s helpful to see how your method holds up with different 'm' values and varying levels of class imbalance. Based on these new results,  I'm happy to bump up my score by one."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700451429429,
                "cdate": 1700451429429,
                "tmdate": 1700451429429,
                "mdate": 1700451429429,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "w0B0QPfT1n",
            "forum": "fW7DOHDQvF",
            "replyto": "fW7DOHDQvF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4492/Reviewer_S73G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4492/Reviewer_S73G"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates an interesting weakly supervised learning problem called multi-class\nclassification from multiple unlabeled datasets, where only multiple sets of unlabeled data and their\nclass priors (i.e., the proportions of each class) are provided for training the classifier. To tackle this\nproblem, this paper first gives a multi-class extension of a previous work on binary classification from\nmultiple unlabeled datasets. However, this paper says that such a method still has several\ndisadvantages that limit the performance. So this paper further proposes a risk-consistent method that\ncan avoid those disadvantages and maintain theoretical guarantees. Experimental results support the\nclaim of this paper."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Problem. This paper investigates the problem of multi-class classification from multiple unlabeled\ndatasets, which is interesting problem.\n- Method. To solve the problem, this paper proposes two methods. The first one is a classifierconsistent method, which is a multi-class extension of a previous work on binary classification. The\nsecond one is a risk-consistent method that can address the shortcomings of the first one.\n- Theory. This paper gives theoretical analysis for the two methods proposed in this paper.\n- Performance. From the experimental results, we can find that the classifier-consistent method can\nachieve good performance compared with previous methods, and the risk-consistent method\noutperforms all the methods. Some ablation studies also support the risk-consistent method."
                },
                "weaknesses": {
                    "value": "- This paper has proposed two methods (CCM and RCM) to solve the problem and showed that the\nsecond method is better than the first method. I think that there lacks a separate paragraph that is\nspecially for describing the difference between RCM and CCM in detail.\n- This paper should give more explanations for the theoretical findings. For example, there are no\ndiscussions or descriptions on Theorem 3.5 and Theorem 3.7."
                },
                "questions": {
                    "value": "- What can we learn from Theorem 3.5 and Theorem 3.7?\n- What is the relationship between the studied problem and the unlabeled-unlabeled learning problem\n[Lu et al. (2019)]?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4492/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4492/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4492/Reviewer_S73G"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4492/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698584537481,
            "cdate": 1698584537481,
            "tmdate": 1699636425170,
            "mdate": 1699636425170,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VlLVOivchx",
                "forum": "fW7DOHDQvF",
                "replyto": "w0B0QPfT1n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer S73G"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback. We sincerely appreciate your insightful comments.  Below is our response.\n\n**Weakness 1: This paper has proposed two methods (CCM and RCM) to solve the problem and showed that the second method is better than the first method. I think that there lacks a separate paragraph that is specially for describing the difference between RCM and CCM in detail.**\n\nThank you for your valuable suggestion. We have adopted your suggestion to provide a separate paragraph to describe the distinctions between RCM and CCM in our revised paper.\n\nFrom a theoretical perspective, RCM could approximate or simulate the distribution of real clean data by utilizing the data distribution from the unlabeled set (by the importance-weighting schema). This means that RCM attempts to infer latent distribution patterns similar to those of real clean data from the unlabeled data. In contrast, CCM aims to better fit the distribution of the unlabeled set by maximizing a log-likelihood object. With a sufficient number of samples, RCM is more accurate in predicting the labels of unseen samples because it considers the restoration of the distribution of real clean data when modeling unlabeled data. This enables RCM to exhibit better generalization performance when facing unknown data, making more precise predictions for unseen samples.\n\n**Weakness 2: This paper should give more explanations for the theoretical findings. For example, there are no discussions or descriptions on Theorem 3.5 and Theorem 3.7.**\n\nGenerally, the Rademacher complexity of the hypothesis space $H$ can be upper bounded by $C_{H}/\\sqrt{n}$ for a positive constant $C_{H}$ [1,2]. Theorem 3.5 demonstrates that the empirical minimizer $\\hat{f}_{\\text{ccm}}$ would coverage to the minimizer $f^*$ on clean data as the training sample size approximates infinity. Theorem 3.7 shows that the empirical risk would converge to the expected risk as the sample size approaches infinity. \n\n[1] N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks. COLT 2018.\n\n[2] N. Lu, T.-Y. Zhang, G. Niu, and M. Sugiyama. Mitigating overfitting in supervised classification from two unlabeled datasets: A consistent risk correction approach. AISTATS, 2020.\n\n**Q1: What can we learn from Theorem 3.5 and Theorem 3.7?**\n\nPlease check our response to Weakness 2.\n\n**Q2: What is the relationship between the studied problem and the unlabeled-unlabeled learning problem [Lu et al. (2019)]?**\n\nBoth MCMU and the unlabeled-unlabeled learning aim to learn an instance-level classifier with the unlabeled sets. There are more constraints imposed on the problem setting of unlabeled-unlabeled learning,i.e., the number of classes is restricted to 2 (binary classification) and the number of unlabeled sets is also limited to 2. Our studied MCMU is more general and thus may have more potential applications."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700385491728,
                "cdate": 1700385491728,
                "tmdate": 1700390250859,
                "mdate": 1700390250859,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QuGkgFd7Yh",
            "forum": "fW7DOHDQvF",
            "replyto": "fW7DOHDQvF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4492/Reviewer_3iq5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4492/Reviewer_3iq5"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on a newly proposed weakly supervised learning problem called multi-class classification from multiple unlabeled datasets (MCMU), where only multiple sets of unlabeled data and their class priors are provided in the training process. To solve this problem, this paper proposes two methods, including a classifier-consistent method (CCM) based on a probability transition function and a risk-consistent method (RCM) based on importance weighting. Additionally, theoretical analyses of the proposed methods and experimental results on multiple benchmark datasets are provided."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. This paper studies a newly proposed weakly supervised learning problem called multi-class classification from multiple unlabeled datasets (MCMU), and proposes two effective methods, which could avoid the negative risk issue commonly encountered by previous unbiased risk estimator methods.\n2. Theoretical analyses are provided to show the theoretical guarantees of the proposed methods.\n3. Comprehensive experimental results on multiple benchmark datasets across various settings demonstrate the effectiveness of the proposed methods.\n4. The paper is well organized and well written, which makes it easy to follow."
                },
                "weaknesses": {
                    "value": "1. There is a lack of descriptions of Theorem 3.5 and Theorem 3.7. Additionally, could we compare RCM and CCM from a theoretical perspective?\n2. The analysis in section 4.3 is interesting. However, there is a lack of discussion about the observation from Fig. 1. So, could the authors provide more details of why CCM is more robust when few data points are provided?\n3. I noticed that in all experimental settings, the number of sets is greater than or equal to the number of classes. Could the authors provide a discussion where the number of sets is less than the number of classes?"
                },
                "questions": {
                    "value": "I have listed the questions in the weaknesses above. Please address them."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4492/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4492/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4492/Reviewer_3iq5"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4492/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699252962128,
            "cdate": 1699252962128,
            "tmdate": 1700654429402,
            "mdate": 1700654429402,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bbH1cibPIH",
                "forum": "fW7DOHDQvF",
                "replyto": "QuGkgFd7Yh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 3iq5"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback. We really appreciate your insightful comments. Our responses to your concerns are provided as follows.\n\n**Q1: There is a lack of descriptions of Theorem 3.5 and Theorem 3.7.**\n\nThank you for your advice, we have provided additional descriptions of Theorem 3.5 and Theorem 3.7 in our revised paper.\n\nGenerally, the Rademacher complexity of the hypothesis space $H$ is upper bounded by $C_H/\\sqrt{n}$ [1,2] for a constant $C_H$. Theorem 3.5 demonstrates that the empirical minimizer $\\hat{f}_{\\text{ccm}}$ would coverage to the minimizer $f^*$ on clean data as the training sample size approximates infinity. Theorem 3.7 shows that the empirical risk would converge to the expected risk as the sample size approaches infinity. \n\n[1] N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks. COLT 2017.\n\n[2] N. Lu, T.-Y. Zhang, G. Niu, and M. Sugiyama. Mitigating overfitting in supervised classification from two unlabeled datasets: A consistent risk correction approach. In AISTATS, 2020.\n\n**Additionally, could we compare RCM and CCM from a theoretical perspective?**\n\nFrom a theoretical perspective, RCM tries to approximate or simulate the distribution of real clean data by utilizing the data distribution from the unlabeled set (by the importance-weighting schema). This means that RCM attempts to infer latent distribution patterns of real clean data from the unlabeled data. In contrast, CCM aims to better fit the distribution of the unlabeled set by maximizing a log-likelihood object. With a sufficient number of samples, RCM is more accurate in predicting the labels of unseen samples because it considers the restoration of the distribution of real clean data when modeling unlabeled data. This enables RCM to exhibit better generalization performance when facing unknown data, making more precise predictions on unseen samples.\n\n**Q2: The analysis in section 4.3 is interesting. However, there is a lack of discussion about the observation from Fig. 1. So, could the authors provide more details of why CCM is more robust when few data points are provided?**\n\nWe think it is because RCM is based on importance weighting, which requires a certain number of samples to accurately estimate the weights for reliably optimizing the objective. Hence, with a small sample size, CCM may outperform RCM.\n\n**Q3: I noticed that in all experimental settings, the number of sets is greater than or equal to the number of classes. Could the authors provide a discussion where the number of sets is less than the number of classes?**\n\nThank you for your suggestion. We have additionally conducted experiments for the mentioned scenario where the number of sets is less than the number of classes, given a fixed number of data points. \n\nTable 1: Classification performance of each method on the MNIST dataset using a random prior matrix with variant set numbers. ($m \\geq k$)\n\n| Methods | Unbiased        | U-Stop           | U-Correct       | U-Flood         | Prop            | U-PRR           | CCM             | RCM             |\n| ------- | --------------- | ---------------- | --------------- | --------------- | --------------- | --------------- | --------------- | --------------- |\n| m=5     | 18.23$\\pm$8.08% | 45.88$\\pm$12.29% | 34.71$\\pm$7.20% | 58.97$\\pm$4.25% | 72.48$\\pm$2.37% | 26.34$\\pm$6.46% | 88.80$\\pm$2.43% | **89.16$\\pm$4.29%** |\n| m=6     | 14.61$\\pm$6.72% | 42.02$\\pm$22.68% | 40.06$\\pm$8.00% | 68.00$\\pm$3.39% | 82.06$\\pm$4.49% | 28.45$\\pm$5.02% | 89.80$\\pm$3.59% | **92.71$\\pm$0.73%** |\n| m=7     | 15.56$\\pm$2.38% | 42.39$\\pm$6.74%  | 49.72$\\pm$8.40% | 77.53$\\pm$7.95% | 89.32$\\pm$3.43% | 32.78$\\pm$5.33% | 90.66$\\pm$2.19% | **94.53$\\pm$1.06%** |\n| m=8     | 15.13$\\pm$0.87% | 37.08$\\pm$16.58% | 32.65$\\pm$2.24% | 79.89$\\pm$4.45% | 88.57$\\pm$3.11% | 27.72$\\pm$0.64% | 89.99$\\pm$2.75% | **94.49$\\pm$1.06%** |\n| m=9     | 14.06$\\pm$1.79% | 27.77$\\pm$8.25%  | 33.80$\\pm$1.88% | 82.83$\\pm$2.78% | 88.31$\\pm$3.68% | 29.79$\\pm$4.22% | 91.42$\\pm$1.26% | **94.44$\\pm$0.91%** |\n\nThe experimental results are reported in Table 1. In the experiments where $m$ is less than $k$, our proposed RCM and CCM consistently outperform other methods. Notably, RCM exhibits superior performance compared with CCM, and this performance difference becomes larger as $m$ increases."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700385339433,
                "cdate": 1700385339433,
                "tmdate": 1700389730739,
                "mdate": 1700389730739,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4v1xzzhToB",
                "forum": "fW7DOHDQvF",
                "replyto": "aqnrcpeG3e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4492/Reviewer_3iq5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4492/Reviewer_3iq5"
                ],
                "content": {
                    "title": {
                        "value": "I am happy to see your response, and I will raise the scores."
                    },
                    "comment": {
                        "value": "Thanks for the rebuttal. I am satisfied with it. I will update my score to credit the authors' efforts in extra experiments that demonstrate when the number of sets is small their method can still work well and showing this relaxation case can strengthen the work."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654387325,
                "cdate": 1700654387325,
                "tmdate": 1700654387325,
                "mdate": 1700654387325,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nexTVP9arW",
            "forum": "fW7DOHDQvF",
            "replyto": "fW7DOHDQvF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4492/Reviewer_vMrL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4492/Reviewer_vMrL"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes two algorithms (CCM and RCM) to classify multiple classes from multiple datasets using only class proportion information. They provide theoretical guarantees on the accuracy of their methods and show some experimental results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I am not familiar with the related work. e.g. LLP, but the overall proposed method seems interesting and sufficiently novel. The proofs in the appendix seem reasonable and the additional experiments there exhaustive enough."
                },
                "weaknesses": {
                    "value": "- See questions\n-\tThere is typo / the sentence got broken up here: \u201cwhere d is a positive integer denotes the input dimension. [k] = \u201c\n-\tThis is not grammatically correct / does not make sense \u201cmultiple-instance learning (Zhou et al., 2009)) could usually have access to weakly supervised labels.\u201d\n\nHowever, the paper is badly written (there is barely any explanation of anything; the appendix in some ways is more clear than the main paper!) and very difficult to understand. This is the biggest flaw of the paper, and makes it difficult to evaluate the paper accurately."
                },
                "questions": {
                    "value": "-\tThe author\u2019s state that Section 2.2 that in MCMU has the data generating process P(X|y) P(y|c) P(c) which seems reasonable, but that the generating process of LLP is P(y|X)? That would make sense as the modelling distribution (learning a discriminative function), but not as a data generating process? Also where is the class priors in the data generating process of LLP?\n-\tWhat are the class priors? The proportion of y_i for each of the k classes?\n-\tSection 2.4 is kind of randomly there without any introduction and it could be \u201ccleaned up\u201d. \n-\tWhat is WSL?\n-\tThe proposed method is extremely unclear. The paper needs to explain more of its proposed approach instead of spending large parts of the text comparing to other papers e.g. Lu et al. and describing their approach as an extension / variation of other papers. (Note: this also unintentionally make it sound less novel)\n-\tThe main idea of CCM seems to be that by converting the problem from classifying multiple classes in multiple datasets, it can be simplified into classifying which dataset the X sample comes from? And this equivalency is due to a deterministic transition function T() which is based on \\rho the probability a datapoint belongs to a dataset, \\theta the proportions of each class for each dataset, and \\pi the proportion of each \u201cclass\u201d over all datasets? \n-\tThe problem setup seems very restrictive / artificial. There has to be the same number of classes for each dataset and the classes are \u201cordered\u201d / aligned across multiple datasets? Also while the problem of classifying which dataset a sample belongs intuitively is \u201ceasier\u201d, it is not clear how that solves the problem of which class within which dataset a sample belongs to. Multiplying by the proportions of dataset size and class size will \u201con average\u201d get you probabilities on the latter problem, but this is just a math trick to achieve a bound. It is not clear how this works practically. The experiments seems to indicate it works, it is just extremely unintuitively how / explained badly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4492/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699310877596,
            "cdate": 1699310877596,
            "tmdate": 1699636425003,
            "mdate": 1699636425003,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "etALAMaGio",
                "forum": "fW7DOHDQvF",
                "replyto": "nexTVP9arW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer vMrL (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments. We have corrected the identified typos in the manuscript. Below, we provide a detailed response to further enhance the clarity and quality of our work.\n\n**Q1: The author\u2019s state that Section 2.2 that in MCMU has the data generating process P(X|y) P(y|c) P(c) which seems reasonable, but that the generating process of LLP is P(y|X)? That would make sense as the modelling distribution (learning a discriminative function), but not as a data generating process? Also where is the class priors in the data generating process of LLP?**\n\nThe generation process in LLP can also be simplified by first generating a group of {$(x_{i},y_{i})$}$^n_{i=1}$ pairs randomly, and then calculating the corresponding class proportions based on the group of labels {$y_i$}$^n_{i=1}$. Thus the \"class priors\" in LLP are calculated by the group {$y_i$}$^n_{i=1}$ and we usually use the term \"label proportions\" (instead of \"class priors\") in LLP. In conclusion, for LLP, we first sample data pairs {$(x_i,y_i)$}$_{i=1}^n$ randomly according to probability density $p(x,y)$ then we could calculate the \"label proportions\".\n\nIn contrast, for MCMU, we first set the \"class priors\", and generate data pairs {$(x_i,y_i)$}$_{i=1}^n$ according to the \"class priors\" as we described in Section 2.4.\n\n**Q2: What are the class priors? The proportion of $y_i$ for each of the $k$ classes?**\n\nIn our paper, the class priors in MCMU denote the generative probabilities of classes, which means, we would like to generate training instances in each class according to the class priors. It is worth noting that the class priors used in MCMU are different from the label proportions used in LLP, where we calculate the proportion of instances belonging to each class after generating the data according to the underlying data distribution $p(x,y)$.\n\n**Q3: Section 2.4 is kind of randomly there without any introduction and it could be \u201ccleaned up\u201d.**\n\nThanks for your suggestion. We have cleaned up it in Section 2.4.\n\n**Q4:  What is WSL?**\n\nIn our paper, WSL refers to Weakly Supervised Learning. We have provided an explanation of WSL in the revised paper.\n\n**Q5: The proposed method is extremely unclear. The paper needs to explain more of its proposed approach instead of spending large parts of the text comparing to other papers e.g. Lu et al. and describing their approach as an extension / variation of other papers. (Note: this also unintentionally make it sound less novel)**\n\nThanks so much for this wonderful suggestion. We have provided more detailed descriptions/explanations of our proposed methods in Appendix F.\n\n**Q6: The main idea of CCM seems to be that by converting the problem from classifying multiple classes in multiple datasets, it can be simplified into classifying which dataset the X sample comes from? And this equivalency is due to a deterministic transition function T() which is based on $\\rho$ the probability a datapoint belongs to a dataset, $\\theta$ the proportions of each class for each dataset, and $\\pi$ the proportion of each \"class\" over all datasets?**\n\nThe main idea of CCM is to design a loss function by converting the problem of classifying multiple classes into multiple unlabeled datasets.\nThis conversion is based on our built connection relationship between $p(y|x)$ (the probability an instance $x$ belongs to a label) and $p(\\overline{y}|x)$ (the probability an instance $x$ belongs to an unlabeled set) in Eq. (2), which is further represented as a transition function $T$ (i.e., $\\overline{\\eta}(x) = T(\\eta(x))$ where $\\overline{\\eta}(x)=p(\\overline{y}|x)$ and $\\eta(x)=p(y|x)$).\nAs we aim to approximate $p(\\overline{y}|x)$ by $T(g(x))$, we can infer that $p(y|x)$ can be approximated by $g(x)$ (where we use $g(x)$ to denote the to represent the Softmax output of the model), because $T$ is an injective function. The detailed proof can be found in Appendix B.3.\n$\\rho$ is the probability that a data point belongs to an unlabeled set, $\\theta$ represents the priors of each class for each unlabeled set, and $\\pi$ is the proportion of each \"class\" overall unlabeled sets. \n\nContinue with the next post"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700384236364,
                "cdate": 1700384236364,
                "tmdate": 1700388835540,
                "mdate": 1700388835540,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XwfZ9jqjxX",
                "forum": "fW7DOHDQvF",
                "replyto": "nexTVP9arW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer vMrL (Part 2/2)"
                    },
                    "comment": {
                        "value": "**Q7: The problem setup seems very restrictive / artificial. There has to be the same number of classes for each dataset and the classes are \u201cordered\u201d / aligned across multiple datasets? Also while the problem of classifying which dataset a sample belongs intuitively is \u201ceasier\u201d, it is not clear how that solves the problem of which class within which dataset a sample belongs to. Multiplying by the proportions of dataset size and class size will \u201con average\u201d get you probabilities on the latter problem, but this is just a math trick to achieve a bound. It is not clear how this works practically. The experiments seem to indicate it works, it is just extremely unintuitively how / explained badly.**\n\nWe would like to clarify that our setting does not have the constraint that the number of classes in each unlabeled set must be the same. There could be different numbers of classes in different unlabeled sets. For example, if there is no class $j$ exists in unlabeled set $i$. We can simply set $\\theta_{ij}$ (the $j$-th class prior of the $i$-th unlabeled set) to 0, then we can still handle this setting by our proposed methods.\n\nWe admit that the classes should be ordered/aligned across multiple unlabeled sets. This is actually the basic requirement to learn a single classifier from multiple datasets simultaneously. Besides, in our humble opinion, it is also easy to satisfy this requirement during the data collection process, by simply assigning each collected class label a unique index. \n\nIn our humble opinion, the MCMU problem is not artificial but can be encountered in various real-world scenarios. For example, predicting the demographic information of users in social networks is crucial for practical policy-making [1]. However, the collection of individual user information might be constrained by data privacy concerns. Fortunately, acquiring multiple unlabeled datasets is more feasible, and their class priors can be derived from existing census data.\n\n[1] Culotta A, Kumar N, Cutler J. Predicting the demographics of twitter users from website traffic data. AAAI 2015.\n\n**Q8: Solves the problem of which class within which dataset a sample belongs to.**\n\nPlease check our response to Q6."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700384515997,
                "cdate": 1700384515997,
                "tmdate": 1700384639264,
                "mdate": 1700384639264,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lRLaDXGb3D",
            "forum": "fW7DOHDQvF",
            "replyto": "fW7DOHDQvF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4492/Reviewer_x6yh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4492/Reviewer_x6yh"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the challenge of weakly supervised learning with a focus on multi-class classification from multiple unlabeled datasets. They propose two methods: the Classifier-Consistent Method (CCM), which utilizes class priors and a probability transition function for training, and the Risk-Consistent Method (RCM), which aims to enhance the CCM by ensuring risk consistency through importance weighting to refine supervision during training. The study claims the superiority of these methods, supporting them with comprehensive theoretical analyses for statistical consistency and positive experimental results across multiple benchmark datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper presents a novel approach based on statistical learning theory to solve the problem of multi-class classification from multiple unlabeled datasets and the theoretical guarantees of the estimation error bounds strengthen the claims regarding its effectiveness.\n2. The authors provide a clear and comprehensive description of their methodology, which enables other researchers to reproduce their results."
                },
                "weaknesses": {
                    "value": "1. The methods and the corresponding theory presented in the paper are sound and relevant, but the lack of sufficient originality and novelty compared to a previously published work [1,2] could be a limitation. Therefore, the authors should carefully consider these points and take the necessary steps to distinguish their work and demonstrate its original contributions.\n\n   [1] Feng, L., Lv, J., Han, B., Xu, M., Niu, G., Geng, X., ... & Sugiyama, M. (2020). Provably consistent partial-label learning. Advances in neural information processing systems, 33, 10948-10960.\n\n   [2] Kobayashi, R., Mukuta, Y., & Harada, T. (2022). Learning from Label Proportions with Instance-wise Consistency. *arXiv preprint arXiv:2203.12836*. \n\n2. Employing the direct outputs of a network to estimate the posterior probabilities $p(y=j\u2223x)$ can be imprecise, particularly under the extreme weak supervision scenario where only class prior probabilities serve as supervisory signals. In such a setting, the network predictions may not align well with the true posterior distributions, leading to suboptimal performance.\n\n3. In Theorems 3.5 and 3.7, the assumptions regarding the Lipschitz continuity of the loss function present an indeterminable strength which could affect the assessment of the model's robustness. Furthermore, the generalization error bound incorporates a summation over $k$ categories of the Rademacher complexity, which may result in a rather loose bound. \n\n4. The use of subscripts $i$ and $j$ appears to be somewhat confusing and may lead to difficulty in understanding for the reader. Specifically, the subscripts switch between $i$ and $j$, which could introduce ambiguity in distinguishing between the elements associated with the k-dimensional vector $\\eta(x)$ and the m-dimensional vector $\\bar{\\eta}(x)$. Clarity in the mathematical notation is crucial for the precise communication of such theoretical results."
                },
                "questions": {
                    "value": "Seen weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4492/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4492/Reviewer_x6yh",
                        "ICLR.cc/2024/Conference/Submission4492/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4492/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699366759737,
            "cdate": 1699366759737,
            "tmdate": 1700452008917,
            "mdate": 1700452008917,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "afbibtTE44",
                "forum": "fW7DOHDQvF",
                "replyto": "lRLaDXGb3D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer x6yh (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thanks for the reviewer's valuable comments. Our point-by-point responses are provided as follows.\n\n**Q1: The methods and the corresponding theory presented in the paper are sound and relevant, but the lack of sufficient originality and novelty compared to a previously published work [1,2] could be a limitation. Therefore, the authors should carefully consider these points and take the necessary steps to distinguish their work and demonstrate its original contributions.**\n\nWe would like to admit that our proposed RCM shares a similar high-level idea, i.e., the importance-weighting (IW) strategy, compared with [1]. However, in our humble opinion, the originality/novelty of our work is sufficient, because of the following reasons:\n\n1) This is the first time that the IW strategy is used in the MCMU problem. It is worth noting that the IW strategy has been widely used in various weakly supervised learning problems, including Partial Label Learning [1], Noisy Label Learning [3,4,5], Learning from Label Proportions [2] (which is different from our studied MCMU, as explained in our paper), and so on. Our paper provides the first attempt to apply the IW strategy to the MCMU problem. In addition, the computational complexity of the method in [2] gets significantly larger as the size of unlabeled sets increases. Therefore, [2] introduces an approximation method, however, this approximation method does not have theoretical guarantees (i.e., the risk equivalence). In contrast, the computational complexity of our method will not be influenced by the increased size of unlabeled sets, and thus our method does not require approximation, hence our method always holds the risk equivalence.\n\n2) It is not straightforward to derive our risk of MCMU that is equivalent to the fully supervised classification risk. This is because we need to consider some special properties of the MCMU problem to derive the desired risk. Hence, the derivation process can also reflect the originality/novelty of our work. Concretely, compared with [1], applying the IW strategy to the MCMU problem has the following difficulties. (a) In Partial Label Learning, the partial label $Z$ is assigned to a single instance, while in MCMU, the class priors $\\theta$ is assigned to a group of instance. (b) The probability dependency between the true label $y$ and the class priors $\\theta$ is much more complex compared with the dependency between the true label $y$ and the partial label $Z$. $p(y|\\theta)$ is a real number between 0 and 1, while $p(Z|y)$ takes value 0 when $y\\in Z$ or take the value $1/(2^{k-1}-1)$ when $y\\notin Z$.\n\n**Q2: Employing the direct outputs of a network to estimate the posterior probabilities $p(y|x)$ can be imprecise, particularly under the extreme weak supervision scenario where only class prior probabilities serve as supervisory signals. In such a setting, the network predictions may not align well with the true posterior distributions, leading to suboptimal performance.**\n\nWe agree with the reviewer to some degree that the Softmax outputs may not precisely estimate the posterior probabilities. However, from the theoretical perspective, our provided Lemma 3.3 demonstrates that if certain loss functions are used (e.g., cross-entropy loss), the Softmax outputs of the model can ideally approximate the true posterior distributions. This theoretical result motivates us to use the Softmax outputs to estimate the posterior probabilities.\n\nFrom the empirical perspective, it is true that the Softmax outputs may not precisely estimate the posterior probabilities. However, there are still many previous methods that have applied the Softmax outputs to approximate the posterior probabilities [1,3,4,5] and achieved excellent performance on various tasks. Hence, the rationale for employing Softmax outputs to approximate posterior probabilities has been widely acknowledged in previous studies. Actually, how to precisely estimate the posterior probabilities is another important task called model calibration [6], which is definitely out of the scope of this paper. In the current stage, we cannot expect there exists a perfect estimation strategy that has no estimation error. That is why we simply used the Softmax outputs to estimate the posterior probabilities, in our paper.\n\nContinue with the next post."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700382922445,
                "cdate": 1700382922445,
                "tmdate": 1700387888472,
                "mdate": 1700387888472,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z8trX92lYW",
                "forum": "fW7DOHDQvF",
                "replyto": "lRLaDXGb3D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4492/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer x6yh (Part 2/2)"
                    },
                    "comment": {
                        "value": "**Q3: In Theorems 3.5 and 3.7, the assumptions regarding the Lipschitz continuity of the loss function present an indeterminable strength which could affect the assessment of the model's robustness. Furthermore, the generalization error bound incorporates a summation over $k$ categories of the Rademacher complexity, which may result in a rather loose bound.**\n\nThe Lipschitz continuity assumption was widely accepted in the machine learning community [1,3,4,5] for establishing generalization or estimation error bounds. This property has been chosen for its stability-enhancing characteristics in optimization and theoretical analyses, contributing to the theoretical underpinning of our work. Importantly, our use of Lipschitz continuity does not compromise the robustness assessment of the model under investigation.\n\nFurthermore, it is noteworthy that Lipschitz continuity has become a fundamental property of many loss functions, such as cross-entropy loss, mean absolute error, square loss, and so on. The widespread applications of these loss functions have demonstrated the rationality of Lipschitz continuity.\n\nFor the generalization error bound and the summation over $k$ categories of the Rademacher complexity. The summation of $k$ Rademacher complexities is theoretically supported by the Rademacher vector contraction inequality [7], with the fact that MCMU is a $k$-class classification problem. According to [7], the summation of $k$ Rademacher complexities will not result in a loose bound.\nIt is also worth noting that similar bounds have been established in many previous studies on weakly supervised learning, e.g., [1,3,5].\n\n**Q4: The use of subscripts $i$ and $j$ appears to be somewhat confusing and may lead to difficulty in understanding for the reader. Specifically, the subscripts switch between $i$ and $j$, which could introduce ambiguity in distinguishing between the elements associated with the k-dimensional vector $\\eta(x)$ and the m-dimensional vector $\\overline{\\eta}(x)$. Clarity in the mathematical notation is crucial for the precise communication of such theoretical results.**\n\nThank you very much for your valuable suggestions. We have made some necessary revisions accordingly, to improve the clarity of mathematical notations in our paper.\n\n**References:**\n\n[1] Feng L, Lv J, Han B, et al. Provably consistent partial-label learning. NeurIPS 2020.\n\n[2] Kobayashi R, Mukuta Y, Harada T. Learning from Label Proportions with Instance-wise Consistency. arXiv preprint arXiv:2203.12836, 2022.\n\n[3] Wu S, Xia X, Liu T, et al. Class2simi: A noise reduction perspective on learning with noisy labels. ICML 2021.\n\n[4] Liu T, Tao D. Classification with noisy labels by importance reweighting. TPAMI 2016.\n\n[5] Xia X, Liu T, Wang N, et al. Are anchor points really indispensable in label-noise learning? NeurIPS 2019.\n\n[6] Guo C, Pleiss G, Sun Y, et al. On calibration of modern neural networks. ICML 2017.\n\n[7] Zatarain-Vera O. A vector-contraction inequality for Rademacher complexities using $p$-stable variables. ALT 2016."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383005318,
                "cdate": 1700383005318,
                "tmdate": 1700387958628,
                "mdate": 1700387958628,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HmPV4zTBOJ",
                "forum": "fW7DOHDQvF",
                "replyto": "Z8trX92lYW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4492/Reviewer_x6yh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4492/Reviewer_x6yh"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comprehensive response. Your clarifications and revisions have addressed all of my concerns.  I have decided to increase my rating."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4492/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700451974423,
                "cdate": 1700451974423,
                "tmdate": 1700451974423,
                "mdate": 1700451974423,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]