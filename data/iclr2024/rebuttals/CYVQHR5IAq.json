[
    {
        "title": "Enhancing Personal Decentralized Federated Learning through Model Decoupling"
    },
    {
        "review": {
            "id": "kKHZwdmIzD",
            "forum": "CYVQHR5IAq",
            "replyto": "CYVQHR5IAq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4664/Reviewer_7fx1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4664/Reviewer_7fx1"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a personalized federated learning (PFL) framework called DFEedMDC, which pursues robust communication and better model performance with a convergence guarantee. Besides, to promote the shared parameters aggregation process, the authors propose DFedSMDC via integrating the local Sharpness Aware Minimization (SAM) optimizer to update the shared parameters."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This work designs personalized local models and training schemes for decentralized federated learning. The authors present theoretical analyses for the convergence, which shows the negative influence of the statistical heterogeneity and the communication topology. Extensive experiments are conducted to evaluate the effectiveness of the proposed methods."
                },
                "weaknesses": {
                    "value": "What do the \"Grid\" and \"Exp\" represent in Fig. 3? It would be easier for the readers to understand different communication topologies by visualizing them in the main test or in the Appendix.\n\nIn light of Theorem 1 and Theorem 2, the communication topology (i.e., the eigenvalue $\\lambda$) has an impact on the DFedMDC and DFedSMDC methods. The reviewer suggests the authors report the $\\lambda$ values of different communication topologies in Fig. 3 and discuss the influence of $\\lambda$ on the test accuracy."
                },
                "questions": {
                    "value": "The proposed DFedSMDC method, a variant of DFedMDC, achieves better performance by integrating the SAM optimizer into the local iteration update of shared parameters. The reviewer is curious if the incorporation of this optimizer could similarly enhance the performance of other baseline methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4664/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4664/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4664/Reviewer_7fx1"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4664/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698328674806,
            "cdate": 1698328674806,
            "tmdate": 1699636447153,
            "mdate": 1699636447153,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hQ7hhcajGP",
                "forum": "CYVQHR5IAq",
                "replyto": "kKHZwdmIzD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewers",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 7fx1."
                    },
                    "comment": {
                        "value": "We greatly appreciate your suggestions and comments, which will make our work better than this version. Thanks for your positive comments, and we address the weaknesses and minor comments below.\n\n## W1:  What do the \"Grid\" and \"Exp\" represent?\nThank you for your suggestion and we added the visualization of topologies in Appendix B.3 in the revision.  The explanations of Grid and Exp are as follows:\n- __Grid__: Grid topology refers to a network configuration where clients are arranged in a two-dimensional grid-like pattern. That means clients are organized in rows and columns, forming a rectangular or square grid structure. In a 100-client network, the exponential neighbors of client 10 are client 11(10+1), client 9(10-1), client 19(10+9), client 1(10-9).\n- __Exp__: Exponential(Exp) topology refers to a network configuration where clients are arranged in an exponential manner. In a 100-client network, the exponential neighbors of client 10 are client 11(10+$2^0$), client 12(10+$2^1$), client 14(10+$2^2$), client 18(10+$2^3$), client 26(10+$2^4$), client 42(10+$2^5$), client 74(10+$2^6$).\n\n## W2:  The value and the influence of $\\lambda$ in Fig.3.\nThank you for your suggestion and we have added more analysis of the spectral gap $(1-\\lambda)$ of different communication topologies to the revision.  From the relationship between the spectral gap $(1-\\lambda)$ and the participation number of clients $m$ ($m=100$ in our experiment), as follows, we can see that the convergence bounds of different topologies are ranked as follows: Fully-connected > Exponential > Grid > Ring, which is consistent with our experiment results.\nGraph Topology | Spectral Gap $1-\\lambda$ \n--- | :--:|\nFully-connected|1\nDisconnected|0\nRing| $\\approx 16\\pi^2/3m^2$ \nGrid| $\\mathcal{O}(1/(mlog_2(m)))$\nExponential Graph|$2/(1 + log_2(m))$\n\n## Q1:  Incorporation of SAM optimizer with other baseline methods.\nWe add more experiments about the incorporation of SAM optimizer with other baseline methods on CIFAR-10 as follows.\nAlgorithm| FedAvg| FedPer | FedRep  | FedBABU | FedRod |Ditto |  DFedAvgM| Dis-PFL |DFedMDC\n--- | :--:| :--: | :--: | :--:| :--: | :--: | :--: | :--: | :--: \nDir0.3|79.66|84.06|84.50|83.26|85.68|73.51|82.60 |82.71|86.50\nDir0.3+SAM| 80.02 |86.81|87.19|85.61|85.33|71.07|77.36 |82.66|88.32\nPat2|85.04|90.94|91.16|91.17|90.10|84.96|90.72|88.19|91.26\nPat2+SAM|84.99|91.73|91.80|91.67|89.52|83.36|90.14|87.89|92.21\n\nFrom the experimental results, SAM could similarly enhance the performance of the partial model personalized methods, but whether it could enhance the full model personalized methods is doubtful. For the partial model personalized methods FedPer, FedRep, FedBABU and DFedMDC, SAM adds proper perturbation in the direction of the shared models\u2019 gradient to make the shared part more robust and flat, which will benefit the average progress for each client. For the full model personalized methods FedAvg, FedRod, Ditto and DFedAvgM, pursuing the full model flatteness will decrease the classifier\u2019s adaptation to local distribution."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4664/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664967696,
                "cdate": 1700664967696,
                "tmdate": 1700664967696,
                "mdate": 1700664967696,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qden7U0diU",
                "forum": "CYVQHR5IAq",
                "replyto": "hQ7hhcajGP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4664/Reviewer_7fx1"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewers",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewer_7fx1"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the responses. After careful consideration, I decided to keep my initial score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4664/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707562117,
                "cdate": 1700707562117,
                "tmdate": 1700707562117,
                "mdate": 1700707562117,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "50MW9erjfl",
            "forum": "CYVQHR5IAq",
            "replyto": "CYVQHR5IAq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4664/Reviewer_z1Hn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4664/Reviewer_z1Hn"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the personalized federated learning problem under fully decentralized setting. The framework of the considered personalized learning is the commonly used model decoupling with a globally shared model and personalized local models. DFedSMDC, an algorithm via integrating the local Sharpness Aware Minimization (SAM) optimizer to update the shared parameters, is proposed. Theoretical convergence results and numerical experimental results are both presented."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper studies the personalized federated learning problem under fully decentralized setting, and proposed DFedSMDC, an algorithm via integrating the local Sharpness Aware Minimization (SAM) optimizer to update the shared parameters."
                },
                "weaknesses": {
                    "value": "1. The reviewer is quite doubt about the final results as shown in Theorem 1 and Theorem 2. I\u2019ve checked the theoretical proof in the appendix and do not find the exact expressions for the final convergence results, but only the $\\mathcal{O}$ expression. The first questionable part is that the right-hand side of Eqs. (3)-(4) will goes to 0 as the number of rounds $T$ goes to infinity, while in reality, this is not true for non-i.i.d scenarios. There will exists some constant terms related with heterogeneity that are irrelevant to $T$. Please explain this. \n\n2. The second part that may not be true in the theoretical results is that the convergence speed is monotonically related with the spectral gap $\\lambda$. If this is true, it solves the challenging topology design problem of decentralized federated learning, since a fully-connected topology is the optimal topology according to the theoretical results in this paper. There is no discussion about this point in current manuscript and this leads to a doubtful result.  \n\n3. Why is the convergence results not related with the number of workers? This is also a weird part. \n\n4. Why Theorem 1 is related with the cross Lipschitz constant $L_{vu}$, and Theorem 2 is related with $L_{vu}$? How about $L_{uv}$?\n\n5. The results in Fig.3 are questionable according to the second comment. The reviewer is not sure if a fully-connected topology is the best.\n\n6. What is the meaning of Fig. 4? Are multiple local epochs good or bad? How is it related with the theoretical results?"
                },
                "questions": {
                    "value": "See the weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4664/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4664/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4664/Reviewer_z1Hn"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4664/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698638102150,
            "cdate": 1698638102150,
            "tmdate": 1699636447025,
            "mdate": 1699636447025,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZqdTpG56G3",
                "forum": "CYVQHR5IAq",
                "replyto": "50MW9erjfl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewers",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer z1Hn(1)."
                    },
                    "comment": {
                        "value": "Thanks for your comments and we reply to them below. Please reconsider the contributions of our work.\n## W1: The doubts about Theorem 1 and Theorem 2.\n- __Only the $\\mathcal{O}$ expression.__ $\\mathcal{O}$ is the final results after assuming the learning rates $\\eta_u$ and $\\eta_v$. Similar results can be found in [1-3]. Due to the limited space, the initial convergence analysis has been presented in Formula (19) and Formula (33). \n- __The right-hand side of Eqs. (3)-(4) will goes to 0 as the number of rounds $T$ goes to infinity.__ \n    - The gradient that goes to 0 is one of the optimality conditions in non-convex optimization.  As an optimization problem, we consider minimizing problem: $\\min _{x \\in \\mathbb{R}^d} f(x)$ and find the global minima $x^*$, that satisfies $f(x) \\geq f(x^*)$. In non-convex optimization, the minima can be defined by  $\\nabla f\\left(x^{\\star}\\right)=0$.  So we prove the convergence from the gradients going to 0, which satisfies $\\| \\nabla f(x^T)\\| \\leq \\epsilon$. More details about non-convex optimization can be referred to [4,5].\n    - We assume that $\\eta_u= \\mathcal{O}(1/L_uK_u\\sqrt{T})$ and $\\eta_v= \\mathcal{O}(1/L_vK_v\\sqrt{T})$, which means that the learning rate $\\eta_u$ and $\\eta_v$ tend to 0 as the number of rounds $T$ goes to infinity, and the gradients eventually tend to 0 correspondently. We deduce that the reviewer wants to say that the proposed methods will converge to a variance under the constant learning rate. Actually, our methods converge to a variance domain with the sublinear order convergence speed.\n    -  Moreover, in a non-iid scenario, each client will achieve convergence and their gradient will approximate to 0 after sufficient large communication rounds. However, this does not mean that all clients have the same solution. For PFL, every client will own its unique solution $(u^*,v_i^*)$, the $v_i^*$ of which is different from each other.\n\n## W2: The convergence speed is monotonically related to the spectral gap.\n- __The convergence speed is monotonically related to the spectral gap.__ The convergence speed is related to the statistical heterogeneity $\\delta$, the smoothness $L_u$, $L_v$, $L_{vu}$ of loss functions, the local epochs $K_u$ and the communication topology (1 \u2212 $\\lambda$) in the theoretical results. Due to the limited space, we only present the final results in Formula (3) and Formula (4). Please check the details in Formula (20-21) and Formula (34-36). \n- __Whether Fully-connected topology is the optimal topology according to the theoretical results.__ \n    - Theoretically, Fully-connected topology is the optimal topology among the comparison topologies and the comparison of the spectral gap of each topology is as follows ($m$ refers to the partition number of clients):\n        Graph Topology | Spectral Gap $1-\\lambda$ \n        --- | :--:|\n        Fully-connected|1\n        Disconnected|0\n        Ring| $\\approx 16\\pi^2/3m^2$ \n        Grid| $\\mathcal{O}(1/(mlog_2(m)))$\n        Exponential|$2/(1 + log_2(m))$\n    \n        It is clear to see that the convergence bounds of different topologies are ranked as follows: Fully-connected > Exponential > Grid > Ring. __But it contains more communication cost compared with other topologies.__ So it is a trade-off between the convergence and communication cost in the real world. We have added the comparison to the revision.\n    - Empirically, we conduct experiments on different topologies (i.e. Ring, Grid, Exponential and Fully-connected\uff09in Fig 3, which suggests the personalized performance of different topologies is ranked as follows: Fully-connected > Exponential > Grid > Ring.\n\n## W3: The convergence results are not related to the number of workers.\nRefer to the table above, the effects of the participation workers $m$ have been involved in the spectral gap $(1-\\lambda)$ of communication topologies. We have added it to the revision.\n\n[1] Decentralized federated averaging, ICML2022.\n\n[2] Improving the Model Consistency of Decentralized Federated Learning, ICML2023.\n\n[3]Federated Learning with Partial Model Personalization, ICML2022.\n\n[4] Non-convex Optimization for Machine Learning.\n\n[5] First-order methods in optimization."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4664/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663559933,
                "cdate": 1700663559933,
                "tmdate": 1700663559933,
                "mdate": 1700663559933,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k6dY0ZVKy7",
                "forum": "CYVQHR5IAq",
                "replyto": "Y50hSOJMZg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4664/Reviewer_z1Hn"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewers",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewer_z1Hn"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response."
                    },
                    "comment": {
                        "value": "I read the authors' response and will keep my original score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4664/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699510917,
                "cdate": 1700699510917,
                "tmdate": 1700699510917,
                "mdate": 1700699510917,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tkgjATbopq",
            "forum": "CYVQHR5IAq",
            "replyto": "CYVQHR5IAq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4664/Reviewer_iEM9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4664/Reviewer_iEM9"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors present an innovative framework known as DFedMDC, which leverages model decoupling to address these issues and aims to provide robust communication and superior model performance while guaranteeing convergence. DFedMDC achieves this by personalizing the \"right\" components within modern deep models through alternate updates of shared and personal parameters, facilitating the training of partially personalized models in a peer-to-peer manner. To enhance the shared parameters aggregation process, the authors introduce DFedSMDC, which incorporates the local Sharpness Aware Minimization (SAM) optimizer to update shared parameters. SAM optimizer introduces proper perturbations in the gradient direction to mitigate inconsistencies in the shared model across clients.\n\nThe paper provides a thorough theoretical foundation, offering a convergence analysis of both algorithms in a general non-convex setting with partial personalization and SAM optimizer for the shared model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and exhibits a high degree of clarity, making it accessible and easy to comprehend.\n2. The paper's strength is further underscored by its meticulous convergence analysis, enhancing its overall robustness.\n3. The paper substantiates its claims with an exhaustive array of experimental results, effectively confirming the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. A significant concern revolves around the novelty of the proposed method. The concept of model decoupling in personalized federated learning [1] and the application of Sharpness Aware Minimization (SAM) [2] to address model inconsistencies in decentralized federated learning have both been extensively explored in the literature. As such, the proposed method may appear to be a fusion of existing ideas (resembling an 'A+B' approach). It is essential for the authors to underscore their distinctive contributions in a more prominent manner.\n\n2. In terms of experimental baselines, it is recommended that the authors include the most recent decentralized federated learning method ([2]) for a comprehensive comparison. This will enhance the paper's completeness and relevance in the context of the current state of the field.\n\n3. Regarding the convergence analysis, it would be valuable to incorporate a discussion that compares the proposed method's convergence rate with the state-of-the-art (SOTA) approaches. \n\n[1] Exploiting Shared Representations for Personalized Federated Learning\n[2] Improving the Model Consistency of Decentralized Federated Learning"
                },
                "questions": {
                    "value": "See weaknesses section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4664/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698682339837,
            "cdate": 1698682339837,
            "tmdate": 1699636446930,
            "mdate": 1699636446930,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wcEjuQ3Hl2",
                "forum": "CYVQHR5IAq",
                "replyto": "tkgjATbopq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewers",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer iEM9."
                    },
                    "comment": {
                        "value": "Thanks for your comments and we reply to them below. Please reconsider the contributions of our work.\n\n## W1: The novelty of our work.\nWe give more details about our background, motivations and contributions. Please check them in the top official comment box.\n\n## W2: Add the most recent decentralized federated learning method for a comprehensive comparison.\nThanks for your suggestion. We provide a comparison with DFedSAM in the following and we have added this comparison in the revision. \n\n| **Dataset** \u00a0 | \u00a0  | **Cifar-10** |  | \u00a0 | \u00a0  | **Cifar-100** | | \u00a0 | \u00a0  \n|:-------------:|:-------|:------------:|:-----:|:-----:|:-------:|:-------------:|:-----|:------|  \n| **Algorithm** | Dir-0.1 | Dir-0.3 \u00a0 \u00a0  | Pat-2 | Pat-5 | Dir-0.1 | Dir-0.3 \u00a0 \u00a0 \u00a0 | Pat-5 | Pat-10 | \u00a0 \u00a0 \u00a0  \n| **DFedSAM**|84.96|\u00a077.36|90.14|83.05|58.21|47.80|74.25|67.34|\n| **DFedAvgM**  | 87.39 \u00a0 | 82.60 \u00a0 \u00a0 \u00a0  | 90.72 | 84.69 | 59.76 \u00a0 | 54.98 \u00a0 \u00a0 \u00a0 \u00a0 | 76.70 | 71.08  | \n| **DFedMDC**| 88.85 \u00a0 | 86.50 \u00a0 \u00a0 \u00a0  | 91.26 | 86.85 | 66.26 \u00a0 | 57.66 \u00a0 \u00a0 \u00a0 \u00a0 | 78.78 | 72.19  | \u00a0\u00a0 \u00a0 \u00a0  \n| **DFedSMDC**\u00a0 | 91.08 | 87.67|92.20|88.34|67.03|58.73|80.82|74.50|\n\nFrom the comparison between DFedAvgM and DFedSAM we can see that adding the gradient perturbation to the full model will increase the model consistency but hurt the personalized performance. On the contrary, adding the perturbation to the shared representation parts in DFedSMDC outperforms DFedMDC clearly, which shows that both improving the shared representation parts consistency and keeping the private classifiers locally are both the key points to facilitate the personalized performance in decentralized PFL.\n## W3: compares the proposed method's convergence rate with the state-of-the-art (SOTA) approaches.\nCompared with the SOTA bounds $\\mathcal{O}\\Big(\\frac{1}{\\sqrt{T}}+\\frac{\\sigma_l^2 + K \\sigma_g^2}{K \\sqrt{T}}+\\frac{\\sigma_l^2 + K\\sigma_g^2+ KB^2}{K(1-\\lambda)^2T^{3/2}}\\Big)$  ( is the upper bound of the gradient) of existing work DFedAvg and $\\mathcal{O}\\Big(\\frac{1}{\\sqrt{KT}}+\\frac{K(\\sigma_g^2+\\sigma_l^2)}{T}+\\frac{\\sigma_g^2+\\sigma_l^2}{K^{1/2}(1-\\lambda)^2T^{3/2}}\\Big)$ of DFedSAM in decentralized works, our algorithms reflect the impact of the L-smoothness and the gradient variance of the shared model $u$ and personalized model $v$ on convergence rate. On the other hand, compared with the SOTA bound of FedAlt in personalized works, our algorithms reflect the impact of the communication topology $\\lambda$ (the value of  that increases when connectivity is more sparse, which has been studied by existing work [1].) \n\nTherefore, the main contribution of our convergence analysis is the first to analyze the impact of model decoupling and the decentralized networks on the convergence rate instead of delivering the SOTA bound compared with others. Meanwhile, this theoretical analysis can be mutually verified with experimental results.\n\n[1] Topology-aware Generalization of Decentralized SGD. ICML2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4664/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663151847,
                "cdate": 1700663151847,
                "tmdate": 1700664708732,
                "mdate": 1700664708732,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p3VYyrxP8h",
                "forum": "CYVQHR5IAq",
                "replyto": "tkgjATbopq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewers",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Last Time Reminder & Sincerely Reply to Reviewer iEM9."
                    },
                    "comment": {
                        "value": "Dear Reviewer iEM9,\n\nWe sincerely appreciate the time and effort you have invested in reviewing our work. As the deadline for the discussion period is approaching, we kindly request that you inform us of any remaining questions you may have.\n\nWe are confident that our response has adequately addressed your concerns, and we would be grateful for your feedback. Should you require further clarification, we would be delighted to answer any additional questions and provide more information.\n\nBest wishes,\n\nThe Authors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4664/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725698750,
                "cdate": 1700725698750,
                "tmdate": 1700725859808,
                "mdate": 1700725859808,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4HdIMmNqF2",
            "forum": "CYVQHR5IAq",
            "replyto": "CYVQHR5IAq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4664/Reviewer_A4M3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4664/Reviewer_A4M3"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes interesting methods DFedMDC and DFedSMDC for PFL, which simultaneously guarantee robust communication and better model performance with convergence guarantee via adopting decentralized partial model personalization based on model decoupling."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The study of personalized FL on decentralized FL is meaningful.\n2. The experiments demonstrate that the proposed method is useful."
                },
                "weaknesses": {
                    "value": "1. The proposed algorithm seems trivial and common in PFL. It seems its idea is the adoption of the method in DFL. Can you clarify what is the main novelty of this method?\n2. Why introduce the SAM? It is unclear about the advantage of introducing this optimizer. Can you elaborate on it intuitively and theoretically?\n3. In the theorem, why is it $V^{t+1}$, instead of $V^{t}$, and what does it mean?\n4. The experiment results are a bit weird, in Table 1. Why do all baselines achieve better performance under larger heterogeneity? As I know, larger heterogeneity will usually lead to worse performance [1].\n5. Regarding ``The test performance will get a great margin with the participation of clients decreasing\u2019\u2019: What will happen when the client number is less than 10, even 1? Does it mean no collaboration is the best?\n\n[1]Karimireddy S P, Kale S, Mohri M, et al. Scaffold: Stochastic controlled averaging for federated learning[C]//International conference on machine learning. PMLR, 2020: 5132-5143.\n\nMinors:\n\n1.\tIt seems the hyperparameters of the proposed methods are finetuned (like $rho$ and local epoch for the personal part). Are the baselines\u2019 results well finetuned? What's the used hyperparameter for baselines?\n2.\tWhat is the definition of $\\sigma$ in Theorem 2?"
                },
                "questions": {
                    "value": "1. Could you give more explanation on Theorem 2? What is the difference/advantage compared with Theorem 1 as you introduce SAM into shared parameters?\n2. Can you provide baseline results with more hyperparameter settings?\n3. Could the authors provide more details about the experiment settings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4664/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766544380,
            "cdate": 1698766544380,
            "tmdate": 1699636446849,
            "mdate": 1699636446849,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lhCr0vQ2Fz",
                "forum": "CYVQHR5IAq",
                "replyto": "4HdIMmNqF2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewers",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer A4M3(1)."
                    },
                    "comment": {
                        "value": "Thanks for your comments and we reply to them below. Please reconsider the contributions of our work.\n## W1: Clarify the main novelty of this method.\nWe clarify more details about our background, motivation and contribution. Please check them in the top official comment.\n## W2: Why introduce the SAM? \n- Intuitively, SAM adds proper perturbation in the gradient direction to make the shared part become more robust and flatter, which will benefit the model average between clients.\n- Theoretically, we give the effect on bound after adding the operation of SAM in Remark 2, where assuming $\\rho = \\mathcal{O}(\\frac{1}{\\sqrt{T}})$. Due to the limited space, we have  presented the bound before assuming $\\rho = \\mathcal{O}(\\frac{1}{\\sqrt{T}})$ in Formula (34) in Appendix C.2.2 on Page 26 as follows:\n    - before using $\\rho = \\mathcal{O}(\\frac{1}{\\sqrt{T}})$:\n    $\\begin{aligned} &\\frac{1}{T} \\sum_{i=1}^T\\left(\\frac{1}{L_u} \\mathbb{E}\\left[\\Delta_{\\bar{u}}^t\\right]+\\frac{1}{L_v} \\mathbb{E}\\left[\\Delta_v^t\\right]\\right)  \\leq \\mathcal{O}\\left(\\frac{F\\left(\\bar{u}^1, V^1\\right)-F^*}{\\sqrt{T}}+\\frac{\\sigma_v^2\\left(L_v+1\\right)}{L_v^2 \\sqrt{T}}+\\frac{L_u^2 \\rho^2+\\sigma_u^2+\\delta^2}{L_u K_u \\sqrt{T}}\\right.  \\left.+\\frac{L_{v u}^2}{(1-\\lambda)^2 \\sqrt{T}}\\left(\\frac{\\rho^2}{K_u}+\\frac{\\sigma_u^2+\\delta^2}{L_u^2}\\right)+\\frac{L_u}{(1-\\lambda)^2 T}\\left(\\frac{\\rho^2}{K_u}+\\frac{\\sigma_u^2+\\delta^2}{L_u^2}\\right)\\right) .\\end{aligned}$\n    - after using $\\rho = \\mathcal{O}(\\frac{1}{\\sqrt{T}})$:\n    $\\begin{aligned} &\\frac{1}{T} \\sum_{i=1}^T\\left(\\frac{1}{L_u} \\mathbb{E}\\left[\\Delta_{\\bar{u}}^t\\right]+\\frac{1}{L_v} \\mathbb{E}\\left[\\Delta_v^t\\right]\\right)  \\leq \\mathcal{O}\\left(\\frac{F\\left(\\bar{u}^1, V^1\\right)-F^*}{\\sqrt{T}}\n    +\\frac{\\sigma_v^2\\left(L_v+1\\right)}{L_v^2 \\sqrt{T}}\n    +\\frac{\\sigma_u^2+\\delta^2}{L_u K_u \\sqrt{T}}\\right. \\left.+\\frac{L_{v u}^2(\\sigma_u^2+\\delta^2)}{L_u^2(1-\\lambda)^2 \\sqrt{T}}\n    +\\frac{\\sigma_u^2+\\delta^2}{L_u(1-\\lambda)^2 T}+ \\frac{L_u}{K_u\\sqrt{T^3}}+\\frac{L_{vu}^2}{K_u(1-\\lambda)^2\\sqrt{T^3}}+\\frac{L_u}{K_u(1-\\lambda)^2T^2}\\right) .\\end{aligned}$\n- Empirically, the comparison between DFedSMDC and DFedMDC in different heterogeneity and different topologies can significantly demonstrate the effectiveness in enhancing the consistency of the shared models in decentralized PFL.\n## W3: $V^{t}$ not $V^{t+1}$ in the theorem.\nThanks for your suggestion and we have polished it in the revision paper.  \n## W4: \"Why do all baselines achieve better performance under larger heterogeneity? \"\nWhat we focus on are the PFL problems, not the FL problems. In PFL tasks, the higher heterogeneity of data distribution means it owns fewer classes of data locally, which makes the classification task easier and clients will achieve better performance. For example, in the Pathological-2 setting, the local task is a binary classification task, which is easier than the five classification tasks in the Pathological-5 setting, so the average test performance in Pathological-2 is better than that in Pathological-5. The same phenomenon can be seen in most PFL works [1, 2, 3, 4].\n\n## W5:  \"The test performance will get a great margin with the participation of clients decreasing.\u2019\u2019\nWe have updated more experiments when the client number is 10 and 5 in Fig 5 in the revision paper. The test performance still improves with the participation of clients decreasing. And we clarify our opinion theoretically and empirically as follows:\n- Theoretically, the convergence bound is related to the topology, which is associated with the participation of clients. The relationship between the topology and the participation clients $m$ is as below:\n    Graph Topology | Spectral Gap 1-$\\lambda$ \n    --- | :--:|\n    Fully-connected|1\n    Disconnected|0\n    Ring| $\\approx 16\\pi^2/3m^2$ \n    Grid| $\\mathcal{O}(1/(mlog_2(m)))$\n    Exponential Graph|$2/(1 + log_2(m))$\n\n    When the participation clients $m$ decrease, the spectral gap increases and the convergence bound is tighter.\n- Empirically, with the participation of clients decreasing, the number of local data increases. More data will help to achieve a better performance. \n- When the client number is 1, the problem will become a classical ML problem based on a centralized data assumption, not an FL problem based on a decentralized data setting. We aim to collaboratively train the personalized models without transmitting data.\n\n[1]On bridging generic and personalized federated learning for image classification. ICLR2022.\n\n[2]Fedbabu: Towards enhanced representation for federated image classification. ICLR2022.\n\n[3]Personalized federated learning with feature alignment and classifier collaboration. ICLR2023.\n\n[4]FedProto: federated prototype learning across heterogeneous clients AAAI2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4664/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662051617,
                "cdate": 1700662051617,
                "tmdate": 1700662967575,
                "mdate": 1700662967575,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "12dltqfaIi",
                "forum": "CYVQHR5IAq",
                "replyto": "4HdIMmNqF2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewers",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer A4M3(2)."
                    },
                    "comment": {
                        "value": "## M1: What's the used hyperparameter for baselines? Are the baselines\u2019 results well fine-tuned? \nThe baselines\u2019 results have been well-finetuned and the hyperparameter of the baselines can be found in Appendix B.3 (MORE DETAILS ABOUT BASELINES).  We show the finetuned processing as follows.\n- __Setting__: For all methods, we keep the experiment setting as the same as follows. We perform 500 communication rounds with 100 clients. The client sampling radio is 0.1 in CFL, while each client communicates with 10 neighbors in DFL. The batch size is 128. The learning rate is 0.1 with 0.99 exponential decay. The local epochs are set to 5 for full model personalized methods and the shared models of the partial model personalized methods. The local optimization is SGD with momentum 0.9 and 5e-4 weighted decay. \n- __Finetuning__: For FedAvg, FedPer, FedBABU, Fed-RoD, DFedAvgM, there is no more hyperparameters. We show the finetuning progress for FedRep, FedSAM, Ditto, Dis-PFL on CIFAR-10 as follow:\n    - __FedRep__: We finetune the local epochs for the personalized parts $K_v$ and set $K_v=10$ in our experiments. \n         $K_v$ | 1|4|7|10|15|\n        --- | :--:|:--:|:--:|:--:|:--:|\n        Dir-0.3 |88.50|90.71|90.81|91.09|90.79\n        Pat-2 |80.85|83.93|84.17|84.50|84.27\n\n        Theoretically, multiple local epochs will speed up convergence but may be prone to over-fitting. It is a trade-off between convergence and generalization. The difference of the best epochs for the personal part demonstrates the difference between CFL and DFL.\n    - __FedSAM__: The extra hyperparameter in FedSAM is the perturbation radius $\\rho$ and we set $\\rho = 0.7$ in our experiments.\n        $\\rho$ | 0.01|0.1|0.3|0.5|0.7|0.9\n        --- | :--:|:--:|:--:|:--:|:--:|:--:|\n        Dir-0.3 |78.78|79.08|79.20|79.61|80.02|79.52\n        Pat-2 |83.26|83.69|84.11|84.57|84.99|83.48\n\n        With a larger $\\rho$, the flatter models that benefit the aggregation average can increase the global generalization. But for PFL, it is a trade-off between the personalization and the generalization.\n    - __Ditto__: The extra hyperparameter in Ditto is the  interpolated coefficient $\\lambda$ and we set $\\lambda = 0.75$ in our experiments.\n        $\\lambda$ | 0.1|0.4|0.75|1.25|2|\n        --- | :--:|:--:|:--:|:--:|:--:|\n        Dir-0.3 |64.77|70.63|73.51|72.73|69.50\n        Pat-2 |80.10|83.64|84.96|84.78|76.71\n\n       $\\lambda$ offers a trade-off between the personalization and the generalization. When $\\lambda$ is set to 0, Ditto is reduced to training local models; as $\\lambda$ grows large, it recovers the global model objective.\n    - __Dis-PFL__:We finetuned the sparsity ratio in Dis-PFL and set it as 0.5 in our experiments.  \n        Sparsity Ratio | 0.2|0.4|0.5|0.6|0.8|\n        --- | :--:|:--:|:--:|:--:|:--:|\n        Dir-0.3 |80.17|82.63|82.72|82.56|81.94\n        Pat-2 |87.77|88.07|88.19|87.99|87.82\n\n      The sparsity ratio is a trade-off between generalization and personalization. A higher sparsity ratio may bring more generalization benefits when the sparsity is small. But with the sparsity ratio increasing, less information exchange will lead to performance degradation.\n## M2: What is the definition of $\\sigma$ in Theorem 2?\n$\\sigma$ is a variate associate with the gradient perturbation $\\rho$ in SAM. We define $\\sigma^2 =\\frac{\\rho^2}{K_u}+\\frac{\\sigma^2_u+\\delta^2}{L_u^2}$ in Formula (24-26). And when assuming $\\rho = \\mathcal{O}(\\frac{1}{\\sqrt{T}})$, $\\mathcal{O}(\\sigma^2)= \\mathcal{O}(\\frac{1}{K_uT}+\\frac{\\sigma^2_u+\\delta^2}{L_u^2})$."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4664/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662651449,
                "cdate": 1700662651449,
                "tmdate": 1700662936295,
                "mdate": 1700662936295,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uDbodTae8Q",
                "forum": "CYVQHR5IAq",
                "replyto": "4HdIMmNqF2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewers",
                    "ICLR.cc/2024/Conference/Submission4664/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4664/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Last Time Reminder & Sincerely Reply to Reviewer A4M3."
                    },
                    "comment": {
                        "value": "Dear Reviewer A4M3,\n\nWe sincerely appreciate the time and effort you have invested in reviewing our work. As the deadline for the discussion period is approaching, we kindly request that you inform us of any remaining questions you may have.\n\nWe are confident that our response has adequately addressed your concerns, and we would be grateful for your feedback. Should you require further clarification, we would be delighted to answer any additional questions and provide more information.\n\nBest wishes,\n\nThe Authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4664/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725624104,
                "cdate": 1700725624104,
                "tmdate": 1700725838402,
                "mdate": 1700725838402,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]