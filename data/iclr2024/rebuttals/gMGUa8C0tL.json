[
    {
        "title": "TaCA: Hot-Plugging Upgrades for Foundation Model with Task-agnostic Compatible Adapter"
    },
    {
        "review": {
            "id": "Yb8DadUOCm",
            "forum": "gMGUa8C0tL",
            "replyto": "gMGUa8C0tL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1059/Reviewer_N62r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1059/Reviewer_N62r"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new task, Hot-Plugging Upgrades for visual foundation models. The aim is \nto seamlessly integrate superior-performing foundation models into downstream applications without adjusting the downstream modules. To realize this objective, The authors introduce a parameter-efficient and Task-agnostic Compatible Adapter, referred to as TaCA, which promotes compatibility across distinct foundation models while concurrently enhancing performance for the new models. The authors conduct extensive experimental validation of TaCA using different scales of models with up to one billion parameters on various tasks such as video-text retrieval, video recognition, and visual question answering."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The authors propose a hot-plugging upgrading module, which is interesting.  \n- The experiments have been conducted to illustrate the superiority of the proposed method."
                },
                "weaknesses": {
                    "value": "- The authors should validate the flexibility of the proposed TaCA module. How about the performance when the TaCA is aligned with the other LLMs? \n- The qualitative analysis and visualization in experiments are missing."
                },
                "questions": {
                    "value": "- The authors should validate the flexibility of the proposed TaCA module. How about the performance when the TaCA is aligned with the other LLMs? \n- The qualitative analysis and visualization in experiments are missing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1059/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1059/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1059/Reviewer_N62r"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1059/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698401680726,
            "cdate": 1698401680726,
            "tmdate": 1699636032251,
            "mdate": 1699636032251,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CCEF1os0mV",
                "forum": "gMGUa8C0tL",
                "replyto": "Yb8DadUOCm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1059/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1059/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1: The authors should validate the flexibility of the proposed TaCA module. How about the performance when the TaCA is aligned with the other LLMs?**\n\n**A1:** The proposed TaCA demonstrates exceptional flexibility by seamlessly integrating with a range of downstream tasks, eliminating the need for additional fine-tuning costs. Its versatility has been validated across tasks such as video-text retrieval, video classification, visual question answering, zero-shot image classification, and zero-shot image-text retrieval. Table 3 further illustrates TaCA's compatibility with the BLIP-2 LLM, confirming its robust performance when paired with different language models.\n\n\n\n**Q2: Addressing the Lack of Qualitative Analysis and Visualizations**\n\n**A2:** We recognize the importance of qualitative analysis and visualization in our experimental assessment. The manuscript currently discusses qualitative aspects, and visualizations are provided through the inference framework and image generation samples in Appendix C. We will enhance our manuscript with a more comprehensive qualitative analysis and additional visualizations in line with your feedback."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1059/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664475565,
                "cdate": 1700664475565,
                "tmdate": 1700664475565,
                "mdate": 1700664475565,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u63WFj3NUo",
            "forum": "gMGUa8C0tL",
            "replyto": "gMGUa8C0tL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1059/Reviewer_REM4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1059/Reviewer_REM4"
            ],
            "content": {
                "summary": {
                    "value": "This paper introducing the task of hot-plugging upgrades for visual foundation models. And it proposes TaCA, which aims at effectively replacing visual foundation model without any downstream adaptation. Extensive experiments on video-related tasks indicate the effectiveness of TaCA."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper introduces the task of hot-plugging upgrades for visual foundation models, which aims at effectively replacing upstream visual fundation model.\n\n2. The experimental results prove TaCA can upgrade the visual foundation models without requiring the training data from downstream video-related tasks."
                },
                "weaknesses": {
                    "value": "TaCA forces large-scale visual model to align with relatively small-scale visual model using adapter. It defeats the purpose of changing the visual model in my opinion. This approach restricts the transferability of the large-scale visual model, which may limit its potential benefits. Additionally, according to the results presented in Table 2, TaCA provides marginal improvements on downstream video classification tasks comparing to directly using large-scale visual model.\n\nThis paper evaluates the effectiveness of TaCA on video-related tasks. However, there are a number of works transfer CLIP to image-based tasks, e.g. image-text retrieval, image segmetation, and few-shot image classification. The absence of experiments on image-related tasks in this paper leaves a gap in evaluating TaCA's capability."
                },
                "questions": {
                    "value": "What is the training overhead in terms of time?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1059/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759925333,
            "cdate": 1698759925333,
            "tmdate": 1699636032152,
            "mdate": 1699636032152,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G0OBSNXYqW",
                "forum": "gMGUa8C0tL",
                "replyto": "u63WFj3NUo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1059/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1059/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1: TaCA forces large-scale visual model to align with relatively small-scale visual model using adapter. in Table 2, TaCA provides marginal improvements on downstream video classification tasks comparing to directly using large-scale visual model.**\n\n**A1:** **The purpose of TaCA is to efficiently align the old model with a large-scale or powerful visual model, thereby enhancing downstream task performance.** In Table 2 in the main paper, the de-emphasized lines represent the results achieved by fine-tuning all downstream heads, which are task-specific and inefficient. Conversely, the emphasized lines depict the performance obtained by directly deploying the TaCA model without making any modifications to the downstream modules.\n\n\n\n\n\n**Q2: Add experiments on image-related tasks like image-text retrieval, image segmentation, and few-shot image classification.**\n\n**A2:** We have broadened the scope of our experiments to include fundamental image classification and retrieval tasks. The results, detailed in Tables 1 and 2, demonstrate TaCA's consistent performance improvements across these basic image tasks.\n\n\n\n| **Model**         | **ImageNet** | **CIFAR100** |\n| ----------------- | ------------ | ------------ |\n| **ViT-B/16**      | 68.6         | 68.7         |\n| **ViT-L/14**      | 75.3         | 77.9         |\n| **TaCA-ViT-L/14** | 74.3(+5.7)   | 76.5(+7.8)   |\n\nTable1: Zero-shot image classification on ImageNet and CIFAR100 datasets. We report the accuracy.\n\n| **Model**         | **MSCOCO**   | **Flickr**  |\n| ----------------- | ------------ | ----------- |\n| **ViT-B/16**      | 42.75        | 72.18       |\n| **ViT-L/14**      | 46.42        | 75.08       |\n| **TaCA-ViT-L/14** | 44.71(+3.67) | 73.97(+2.9) |\n\nTable2: Zero-shot image-text retrieval on MSCOCO and Flickr datasets. We report the Recall@1.\n\n\n\n**Q3: What is the training overhead in terms of time?**\n\n**A3:** The fine-tuning schedules and inference details for these paradigms are presented in Appendix C.  In the video-text retrieval task, we fine-tune three distinct sets of downstream heads on the MSRVTT, MSVD, and DiDeMo datasets, resulting in the training of a total of 42M parameters. For the video classification task, fine-tuning the classification head requires nearly twice as many parameters as the TaCA model. However, TaCA requires training just once, with no additional fine-tuning costs, yielding a more parameter-efficient and training-free application in downstream tasks. This discussion will be incorporated into the final paper.\n\n\n\n| **Strategy**   | **Learnable params.** | **Finetune data**      | **MSRVTT** | **MSVD** | **DiDeMo** |\n| -------------- | --------------------- | ---------------------- | ---------- | -------- | ---------- |\n| **Finetuning** | 14M                   | WIT+MSRVTT/MSVD/DiDeMo | 44.8       | 46.7     | 38.1       |\n| **TaCA**       | 17M                   | No need                | 44.5       | 45.6     | 36.6       |\n\nTable3: Comparison on Video-Text Retrieval datasets. The new visual fundation model (VFM) is ViT-L/14, and the old VFM is ViT-B/16.\n\n| **Strategy**   | **Learnable params.** | **Finetune data** | **K400** | **UCF-101** |\n| -------------- | --------------------- | ----------------- | -------- | ----------- |\n| **Finetuning** | 29M                   | Kinetics-400      | 87       | 85.7        |\n| **TaCA**       | 17M                   | No need           | 83.6     | 83.1        |\n\nTable4: Comparison on Video Classification datasets. The new visual fundation model (VFM) is ViT-L/14, and the old VFM is ViT-B/16."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1059/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664444036,
                "cdate": 1700664444036,
                "tmdate": 1700664444036,
                "mdate": 1700664444036,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7fcRZtxCPY",
            "forum": "gMGUa8C0tL",
            "replyto": "gMGUa8C0tL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1059/Reviewer_dvkW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1059/Reviewer_dvkW"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Hot-Plugging Upgrades for visual foundation models. The aim is to seamlessly integrate superior-performing foundation models into downstream applications without adjusting the downstream modules. To realize this objective, this paper introduces a parameter-efficient and task-agnostic Compatible Adapter, referred to as TaCA, which promotes compatibility across distinct foundation models while concurrently enhancing performance for the new models. The paper is written well and easy to follow."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. This paper spearheads the exploration into the scenario of upgrading large-scale foundation models and introduces hot-plugging upgrades of visual foundation models in modular frameworks.\n2. This paper introduces a parameter-efficient upgrading strategy using a Task-agnostic Compatible Adapter (TaCA)\n3. The paper is written well and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The approach is incremental, and the techniques employed are all verified strategies. Specifically, it utilizes a combination of distillation methods and contrastive learning, forming a hybrid approach.\n\n2. Why not conduct experiments on more basic image classification and retrieval datasets (e.g., MSCOCO and imagenet)? If the effectiveness of this method can be verified on a more basic dataset, I am willing to increase my score\n\n3. In my opinion, TaCA, which utilizes an adapter to align a large-scale visual model with a smaller-scale visual model, undermines the purpose of changing the visual model. This approach hampers the transferability of the large-scale visual model, potentially limiting its advantages. Moreover, based on the results presented in Table 2, TaCA only shows marginal enhancements in downstream video classification tasks compared to directly employing a large-scale visual model.\n\n4. while this paper assesses the effectiveness of TaCA in video-related tasks, it overlooks numerous studies that apply CLIP to image-based tasks such as image-text retrieval, image segmentation, and few-shot image classification. The absence of experiments on image-related tasks in this paper creates a gap in evaluating TaCA.\n\n5. What would happen if Old VFM and New VFM were different (e.g., VIT-B to ResNet-50)? Can we distill and transfer knowledge between VFMs of different architectures to each other? For example, distilling knowledge from miniGPT or LLAMA (decoder only architecture) to CLIP?"
                },
                "questions": {
                    "value": "1. The method is incremental, and the methods used are all validated schemes, which is actually distillation method combined with contrastive learning.\n2. Why not conduct experiments on more basic image classification and retrieval datasets"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1059/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1059/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1059/Reviewer_dvkW"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1059/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775673793,
            "cdate": 1698775673793,
            "tmdate": 1699699431241,
            "mdate": 1699699431241,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1JEpBUCLGI",
                "forum": "gMGUa8C0tL",
                "replyto": "7fcRZtxCPY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1059/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1059/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1: The approach is incremental.**\n\n**A1:** We reclaim that our key contributions are: \n\n(1) For the **first time**, we introduce \"hot-plugging upgrades\", which directly deploys the new foundation models **without retraining cost**. Specifically, we introduce the **task-agnostic** compatible adapter (TaCA), which enhances performance across a variety of downstream tasks. \n\n(2) Our method not only demonstrates compelling practical value but also boasts remarkable industrial applications. Unlike the traditional approach of fine-tuning specific downstream heads, which often incurs prohibitively high training costs and exhibits suboptimal transferability, our approach shines. To illustrate, the process of retraining a Q-Former in BLIP-2 demands approximately 9 days utilizing 16 A100 GPUs. TaCA, in contrast, offers a versatile task-agnostic capability, enabling seamless adaptation to various downstream tasks without incurring any finetuning costs.\n\n(3) The significance and novelty of our contributions have been duly acknowledged by Reviewer #TD34 and #N62r, further attesting to the groundbreaking nature of our work.\n\n\n\n\n\n**Q2: Add experiments on more basic image classification and retrieval datasets (e.g., MSCOCO and imagenet)**\n\n**A2:** We have broadened the scope of our experiments to include fundamental image classification and retrieval tasks. The results, detailed in Tables 1 and 2, demonstrate TaCA's consistent performance improvements across these basic image tasks.\n\n\n\n| **Model**         | **ImageNet** | **CIFAR100** |\n| ----------------- | ------------ | ------------ |\n| **ViT-B/16**      | 68.6         | 68.7         |\n| **ViT-L/14**      | 75.3         | 77.9         |\n| **TaCA-ViT-L/14** | 74.3(+5.7)   | 76.5(+7.8)   |\n\nTable1: Zero-shot image classification on ImageNet and CIFAR100 datasets. We report the accuracy.\n\n| **Model**         | **MSCOCO**   | **Flickr**  |\n| ----------------- | ------------ | ----------- |\n| **ViT-B/16**      | 42.75        | 72.18       |\n| **ViT-L/14**      | 46.42        | 75.08       |\n| **TaCA-ViT-L/14** | 44.71(+3.67) | 73.97(+2.9) |\n\nTable2: Zero-shot image-text retrieval on MSCOCO and Flickr datasets. We report the Recall@1.\n\n\n\n\n\n**Q3: TaCA utilizes an adapter to align a large-scale visual model with a smaller-scale visual model. In Table 2, TaCA only shows marginal enhancements in downstream video classification tasks compared to directly employing a large-scale visual model**\n\nA3: We acknowledge the confusion in our initial explanation. **The purpose of TaCA is to efficiently align the old model with a large-scale or powerful visual model, thereby enhancing downstream task performance.** In Table 2, the de-emphasized lines represent the results achieved by fine-tuning all downstream heads, which are task-specific and inefficient. Conversely, the emphasized lines depict the performance obtained by directly deploying the TaCA model without making any modifications to the downstream modules.\n\n\n\n**Q4: Add experiments on image-related tasks like image-text retrieval, image segmentation, and few-shot image classification.**\n\n**A4:** For further details on the experiments encompassing image-text retrieval, image segmentation, and few-shot image classification, please refer to the response to Question 2.\n\n\n\n**Q5: What would happen if Old VFM and New VFM were different? Can knowledge of different architectures be distilled? For example, distilling knowledge from miniGPT or LLAMA (decoder only architecture) to CLIP?**\n\n**A5:** (1) Our research in Table 9 establishes that TaCA can generalize across various foundational models, allowing for knowledge distillation between different architectures. However, the performance may be less optimal if the new model is not as capable as the previous one (e.g., replacing ViT-B with ResNet-50). (2) While this paper confirms TaCA's efficiency with visual models, its applicability to language models, such as distilling knowledge from decoder-only architectures like miniGPT or LLAMA to CLIP, remains an area for future exploration."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1059/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664350970,
                "cdate": 1700664350970,
                "tmdate": 1700664350970,
                "mdate": 1700664350970,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ISIyRf9Suf",
            "forum": "gMGUa8C0tL",
            "replyto": "gMGUa8C0tL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1059/Reviewer_TD34"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1059/Reviewer_TD34"
            ],
            "content": {
                "summary": {
                    "value": "- This manuscript introduces a new hot-plugging adapter, with which the task-specific model's foundation backbone can be replaced without re-training for both the backbone and the task-specific head. \n- The proposed method is tested on the CLIP series foundation models and evaluated on various vision-language tasks. The results validate the proposed method's effectiveness. Specifically, the performance of downstream tasks is improved when the backbone networks are replaced with more powerful ones."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of hot-plugging adapters is interesting. It could have a good impact on future research and other applications. \n2. The proposed method is technically sound. \n3. The manuscript is well-written and easy to follow. \n4. The comprehensive experiments validate the model's effectiveness on various tasks."
                },
                "weaknesses": {
                    "value": "1. While the idea of hot-plugging adapters is intuitively sound at first glance, this paper lacks quantitative evidence to support this motivation. Specifically, the motivation of this paper is: *When replacing the visual backbones, fine-tuning the proposed adapter is better than fine-tuning the downstream task-specific head*. Therefore, a comparison between these two fine-tuning methods should be presented, and such a comparison should be in a fair enough setting because, in my opinion, it should be the most important experiment for the whole manuscript. Specifically, the author should compare 1) the trainable parameter amounts, 2) training FLOPs, 3) The data amounts needed for fine-tuning, and 4) the fine-tuning schedule of these two fine-tuning paradigms. In addition, the results in Table 1 show that the *TACA-BetterModel*'s performance is inferior to directly fine-tuning the task-specific head with a *BetterModel*, i.g., ViT-H, which also shows the necessity of such a comparison. \n2. The symmetric adapter is a very interesting point of this manuscript, as it outperforms the standard adapter. It would be better to include some experiments to study its effectiveness on other tasks, e.g., image generation or some NLP tasks. \n3. The manuscript should include more experiments to show its generalization ability to other non-CLIP models. For example, can the proposed method work on classic vision tasks, like detection or segmentation? \n4. I am also curious if the method can be applied to replacing *different* foundation models. For example, can we use it to replace a DINO ViT with an ImageNet-22k Supervised ViT? \n5. The proposed head-fixing strategy makes me think about the head-inheriting trick that is commonly used in knowledge distillation [a][b]. Therefore, discussing it in the related work section will increase the comprehensiveness of this work.\n\nTo conclude, I like the motivation of this work, and I also acknowledge that the provided experiments do validate its effectiveness to some extent. If the authors can address my concerns, especially the first point, I am happy to raise my rating.\n\n\n[a] Yang, Chenhongyi, et al. \"Prediction-guided distillation for dense object detection.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n[b] Yang, Zhendong, et al. \"Focal and global knowledge distillation for detectors.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1059/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1059/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1059/Reviewer_TD34"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1059/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801197555,
            "cdate": 1698801197555,
            "tmdate": 1699636032007,
            "mdate": 1699636032007,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lEgKthZxBY",
                "forum": "gMGUa8C0tL",
                "replyto": "ISIyRf9Suf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1059/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1059/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1: About the motivation of hot-plugging adapters.** \n\n**A1:** (1) We acknowledge the confusion in our initial explanation. Our aim is to highlight **not** the superiority of adapter fine-tuning over task-specific head fine-tuning, but rather the **inefficiencies and inflexibility** of the latter. Hot-plugging adapters offer a **task-agnostic, generalized**, and **efficient** alternative, enhancing the performance of various downstream tasks without incurring training costs.\n\n(2) As shown in Table 1, direct fine-tuning of downstream heads can outperform TaCA, but it remains task-specific. The fine-tuning schedules and inference details for these paradigms are presented in Appendix C.  In the video-text retrieval task, we fine-tune three distinct sets of downstream heads on the MSRVTT, MSVD, and DiDeMo datasets, resulting in the training of a total of 42M parameters. For the video classification task, fine-tuning the classification head requires nearly twice as many parameters as the TaCA model. However, TaCA requires training just once, with no additional fine-tuning costs, yielding a more parameter-efficient and training-free application in downstream tasks. This discussion will be incorporated into the final paper.\n\n\n\n| **Strategy**   | **Learnable params.** | **Finetune data**      | **MSRVTT** | **MSVD** | **DiDeMo** |\n| -------------- | --------------------- | ---------------------- | ---------- | -------- | ---------- |\n| **Finetuning** | 14M                   | WIT+MSRVTT/MSVD/DiDeMo | 44.8       | 46.7     | 38.1       |\n| **TaCA**       | 17M                   | No need                | 44.5       | 45.6     | 36.6       |\n\nTable1: Comparison on Video-Text Retrieval datasets. The new visual fundation model (VFM) is ViT-L/14, and the old VFM is ViT-B/16.\n\n| **Strategy**   | **Learnable params.** | **Finetune data** | **K400** | **UCF-101** |\n| -------------- | --------------------- | ----------------- | -------- | ----------- |\n| **Finetuning** | 29M                   | Kinetics-400      | 87       | 85.7        |\n| **TaCA**       | 17M                   | No need           | 83.6     | 83.1        |\n\nTable2: Comparison on Video Classification datasets. The new visual fundation model (VFM) is ViT-L/14, and the old VFM is ViT-B/16.\n\n\n\n**Q2: Expanding Experiments to Additional Tasks (e.g., image generation or some NLP tasks).**\n\n**A2:** We appreciate your suggestion. Our extension to other downstream tasks, such as image generation, is documented in Appendix C.4, page 17. Although the styles of generated images are subtly differentiated, our method is adept at maintaining detailed semantics. This represents the early phase of our research; future work will aim at refining these solutions.\n\n\n\n**Q3: Generalization to Non-CLIP Models and Classic Vision Tasks. Can the proposed method work on classic vision tasks, like detection or segmentation?** \n\n**A3:** We have validated TaCA's effectiveness on non-CLIP models, such as Dinov2 and BEiT, in Table 9 (page 9). These models show slight improvements and meet the compatibility criterion for various tasks.\n\nWe have also applied our method to classical vision tasks, such as zero-shot image classification and zero-shot image-text retrieval, with results presented in Tables 1 and 2 demonstrating consistent improvements. The potential application of TaCA to detection and segmentation will be explored in forthcoming studies.\n\n\n\n\n\n| **Model**         | **ImageNet** | **CIFAR100** |\n| ----------------- | ------------ | ------------ |\n| **ViT-B/16**      | 68.6         | 68.7         |\n| **ViT-L/14**      | 75.3         | 77.9         |\n| **TaCA-ViT-L/14** | 74.3(+5.7)   | 76.5(+7.8)   |\n\nTable1: Zero-shot image classification on ImageNet and CIFAR100 datasets. We report the accuracy.\n\n| **Model**         | **MSCOCO**   | **Flickr**  |\n| ----------------- | ------------ | ----------- |\n| **ViT-B/16**      | 42.75        | 72.18       |\n| **ViT-L/14**      | 46.42        | 75.08       |\n| **TaCA-ViT-L/14** | 44.71(+3.67) | 73.97(+2.9) |\n\nTable2: Zero-shot image-text retrieval on MSCOCO and Flickr datasets. We report the Recall@1.\n\n\n\n**Q4: Does TaCA work well when replacing *different* foundation models? Can we use it to replace a DINO ViT with an ImageNet-22k Supervised ViT?** \n\n**A4:** TaCA's adaptability across diverse foundational backbones is discussed in Table 9 of the main paper. While it is possible to replace a DINO ViT with an ImageNet-22k Supervised ViT using TaCA, the performance may be suboptimal if the new foundation model's representational capabilities are inferior. The efficacy of downstream tasks is contingent upon the foundational models' improved representational quality.\n\n**Q5: Incorporating Knowledge Distillation in Related Work**\n\n**A5:** We have included a new subsection on knowledge distillation in the related works of our revised manuscript, which can be found highlighted on page 18."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1059/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664302749,
                "cdate": 1700664302749,
                "tmdate": 1700664302749,
                "mdate": 1700664302749,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]