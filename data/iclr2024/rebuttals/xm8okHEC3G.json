[
    {
        "title": "Boosting Dataset Distillation with the Assistance of Crucial Samples"
    },
    {
        "review": {
            "id": "10ukxUeR40",
            "forum": "xm8okHEC3G",
            "replyto": "xm8okHEC3G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4449/Reviewer_dBFm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4449/Reviewer_dBFm"
            ],
            "content": {
                "summary": {
                    "value": "The authors delve into the task of dataset distillation from the perspective of sample cruciality, they argue that hard samples in the original dataset contain more information. To this end, they discard some easier samples and enrich harder ones in the semantic space through continuously interpolating between two target feature vectors during data distillation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors delve into the task of dataset distillation from the perspective of sample cruciality and propose the idea of adjusting the proportion of difficult and easy samples in the data distillation process; few papers considered this aspect before.\nThey put forward an infinite semantic augmentation method by continuously interpolating between two target feature vectors, requiring no extra computational costs while being effective.\nThe applicability of distilled data is considered, They demonstrated that their distilled data is capable of providing benefits to continual learning and membership inference defense."
                },
                "weaknesses": {
                    "value": "The author only demonstrated through some simple experiments that in data distillation, the importance of difficult samples is stronger than that of simple samples. However, this conclusion cannot adequately explain that, in Figure 5 of the appendix, it can be observed that discarding difficult samples still allows the distilled data to achieve the comparable performance as the original distillation method, or even better.\n             \nCan the author provide more profound and solid explanations for the point mentioned above?\n\nWhen comparing with baseline methods, the author did not compare with the state-of-the-art methods like FTD and TESLA. Nevertheless, this method only achieved state-of-the-art performance in 8 out of the 14 experimental setups.\n\nCan the author complete the relevant comparative experiments and provide an objective analysis of the experimental results\uff1f\n\n\nDirectly using MSELoss as a criterion to discard a substantial proportion of samples.  Could this lead to a bias in distilled data?"
                },
                "questions": {
                    "value": "Please refer to the Weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4449/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4449/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4449/Reviewer_dBFm"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4449/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698724026980,
            "cdate": 1698724026980,
            "tmdate": 1700544706225,
            "mdate": 1700544706225,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6eiHEW6SaR",
                "forum": "xm8okHEC3G",
                "replyto": "10ukxUeR40",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dBFm  (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for the time and questions. Here we answer your questions point-by-point as follows.\n\n>**Q1**: More explanations of discarding samples.\n\n**A1**: Thank you for the question. Actually, we try to explore \"whether we should make a compromise to focus on the majority of data points around the center to ensure good performances on these data and what would happen if we discard these samples\" (the last line in the first paragraph of Section 2.2). Motivated by this curiosity,  we conduct extensive experiments on discarding easy samples or hard samples in Section2.2 (both the Figure 1 and Figure 5). The results on three datasets and four settings all suggest that dropping easy samples helps dataset distillation in all cases. And when the IPC is small, dropping hard ones can also help sometimes, which is consistent with your guess. Here, we provide two explanations and hope it can solve your puzzles.\n - Recall that the goal for dataset distillation is to condense the large dataset into a smaller one such that a model trained on it can achieve a comparable test performance as one trained on the original dataset (see Introduction). Namely, we need to depict a good decision boundary for classification based on the distilled data. However,  relying on easy samples solely may result in short-cut learning[a]. In contrast, hard samples are hard-to-be-distinguished samples (Eq.4) that usually exist along the decision boundary and thus can help to support a more explicit decision boundary. We think that's why hard samples can help to improve the dataset distillation performances. \n- We have also explored it from both the perspective of data manifold and information in the third paragraph of Section 2.2. In short, we observed an increased manifold overlap between the generated images and the original dataset after discarding some easy samples (Figure 1 middle two images), thereby depicting a better representation of the manifold. We also find that making precise predictions for simple samples with hard samples is easy but the reverse is not. This indicates that harder samples are more informative than easier ones.\n \nWe have included this discussion in the Section B of the revision. We think this is a really interesting finding that makes our work unique and distinct. We hope it can bring more insights to scholars in dataset distillation.\n\n***\n\n>**Q2**: Performance analysis.\n\n**A2**: In Table 1 and 3, we have conducted experiments on 8 datasets under 20 settings. Compared to our baseline, our proposed method improves the baseline with an average relative improvement of 6.6% and an absolute improvement of 2.0% on 16 settings. When compared to SOTA approaches, our method has achieved the SOTA performance under 14 settings. Besides, the performances can be further boosted with more advanced studies, as shown in Table 4 and 8. For example, when combined with RFAD, our performance on CIFAR10, IPC=1 can achieve performance up to 54.8\\%, which is a new SOTA in Table 1. \n\nWhat's more, we find the proposed module only fails to show superiority on small datasets like MNIST and F-MNIST but still holds a comparable performance. In contrast, it holds a superior performance on larger datasets such as T-ImageNet and ImageNet as shown in Table 1 and 3. Since the dataset distillation aims to solve the difficulty in processing large datasets, we believe performing better on large datasets is more important nowadays."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124461662,
                "cdate": 1700124461662,
                "tmdate": 1700124695740,
                "mdate": 1700124695740,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "msMKPBd7FM",
                "forum": "xm8okHEC3G",
                "replyto": "10ukxUeR40",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dBFm  (Part 2/2)"
                    },
                    "comment": {
                        "value": ">**Q3**: Comparison with SOTA methods like FTD and TESLA.\n\n**A3**: The comparisons with FTD and TESLA are listed as follows. While these two methods are novel and provide good performances, it can be observed that our method holds a better performance in most cases. We have included this in Appendix G.\n\n| Method | CIFAR10      |              |              | CIFAR-100    |              |              | T-ImageNet   |              | ImageNette   |              | ImageWoof    |              | ImageNet    |              |\n|--------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|-------------|--------------|\n|        | IPC=1        | IPC=10       | IPC=50       | IPC=1        | IPC=10       | IPC=50       | IPC=1        | IPC=10       | IPC=1        | IPC=10       | IPC=1        | IPC=10       | IPC=1       | IPC=2        |\n| FTD    | 46.8\u00b10.3     | 66.6\u00b10.3     | 73.8\u00b10.2     | 25.2\u00b10.2     | 43.4\u00b10.3     | **50.7\u00b10.3** | 10.4\u00b10.3     | 24.5\u00b10.2     | **52.2\u00b11.0** | 67.7\u00b10.7     | 30.1\u00b11.0     | 38.8\u00b11.4     | -           | -            |\n| TESLA  | **48.5\u00b10.8** | 66.4\u00b10.8     | 72.6\u00b10.7     | 24.8\u00b10.4     | 41.7\u00b10.3     | 47.9\u00b10.3     | -            | -            | -            | -            | -            | -            | 7.7\u00b10.2     | 10.5\u00b10.3     |\n| Ours   | 48.4\u00b10.4     | **67.2\u00b10.4** | **73.8\u00b10.0** | **31.2\u00b10.2** | **46.4\u00b10.5** | 49.4\u00b10.3     | **19.8\u00b10.1** | **27.0\u00b10.3** | 49.6\u00b10.6     | **67.8\u00b10.3** | **30.8\u00b10.5** | **43.8\u00b10.6** | **8.0\u00b10.2** | **10.7\u00b10.1** |\n\n***\n\n>**Q4**: Could MSELoss lead to a bias in distilled data?\n\n**A4**: Discarding a substantial proportion of sample losses in Eq.4 can help the optimization focus more on the hard-to-be-distinguished samples, making the distilled data helpful/biased in depicting a better decision boundary so that a model trained on these distilled samples can show comparable performances to the one trained on original data. This is consistent with the goal of dataset distillation described in the Introduction: Dataset distillation aims to learn a small set of synthetic examples from a large dataset such that a model trained on it can achieve a comparable test performance as one trained on the original dataset."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124521484,
                "cdate": 1700124521484,
                "tmdate": 1700124718472,
                "mdate": 1700124718472,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pDuEyFRVYJ",
                "forum": "xm8okHEC3G",
                "replyto": "10ukxUeR40",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind reminder to look at the authors' reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer dBFm:\n\nWe thank you for the precious time and efforts in reviewing this paper. We have provided corresponding responses with elaborate discussions and extensive experiments on why dropping easy samples can help in dataset distillation, together with more discussion and comparisons with more studies. We have also include these in our revision (highlighted in blue). We hope to further discuss with you whether or not your concerns have been addressed appropriately. Please let us know if you have additional questions or ideas for improvement.\n\nLooking forward to your reply.\n\nAuthors."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448780157,
                "cdate": 1700448780157,
                "tmdate": 1700448780157,
                "mdate": 1700448780157,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Mz4ZoV4fDI",
                "forum": "xm8okHEC3G",
                "replyto": "6eiHEW6SaR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4449/Reviewer_dBFm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4449/Reviewer_dBFm"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply. I have the further questions about this submission.\n\n1. what are the key differences between the proposed crucial samples and the matching samples in the DREAM paper? How about comparing this method to DREAM? \n2. Discarding easy and hard, is there need to set a threshold? if so, hope the authors can show more details about it and explain whether this operation influences the generality of this method.\n\n\n\n\n\nDREAM: Efficient Dataset Distillation by Representative Matching. ICCV2023"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700457537809,
                "cdate": 1700457537809,
                "tmdate": 1700457537809,
                "mdate": 1700457537809,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VQ9Q35zQyk",
                "forum": "xm8okHEC3G",
                "replyto": "rvjJ6nqPW4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4449/Reviewer_dBFm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4449/Reviewer_dBFm"
                ],
                "content": {
                    "comment": {
                        "value": "DREAM is also an online sampling strategy, and the clustering process is conducted through the entire distillation process. DREAM's clustering process also utilizes the complete information of the class.\nUnder the binary classification, your concern makes sense. But if it is extended to multi-class classification, can it still describe better decision boundaries?  I am concerned that decision boundaries will appear in multiple directions, making it more difficult for the model to converge and lose its generalization ability."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538855302,
                "cdate": 1700538855302,
                "tmdate": 1700538855302,
                "mdate": 1700538855302,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9MZxND3kzK",
                "forum": "xm8okHEC3G",
                "replyto": "L5yqqYVYeq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4449/Reviewer_dBFm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4449/Reviewer_dBFm"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your clarification. I will increase the score to 6."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544677222,
                "cdate": 1700544677222,
                "tmdate": 1700544677222,
                "mdate": 1700544677222,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "E1ykiueO1K",
            "forum": "xm8okHEC3G",
            "replyto": "xm8okHEC3G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4449/Reviewer_LMPF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4449/Reviewer_LMPF"
            ],
            "content": {
                "summary": {
                    "value": "The paper innovatively tackles the challenge of Dataset Distillation (DD) with a focus on sample cruciality in the outer loop of the bi-level learning problem. Building upon the neural Feature Regression (FRePo) framework, the authors introduce the Infinite Semantic Augmentation (ISA) algorithm. This algorithm enriches harder-to-represent samples in the semantic space through a process of continuous interpolation between two target feature vectors. Importantly, the algorithm is highly efficient as it formulates the joint contribution to training loss as an analytical closed-form integral solution. The method is rigorously evaluated on five benchmark datasets including MNIST, Fashion-MNIST, CIFAR10, CIFAR100, and Tiny-ImageNet. It is also compared against six baseline dataset distillation algorithms: DSA, DM, MTT, KIP, RFAD and FRePo. The experimental results demonstrate that the proposed ISA method effectively reduces dataset size while maintaining or even enhancing model accuracy. The distilled data also proves to be beneficial for downstream applications such as continual learning and privacy protection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- **Originality**: The paper's focus on optimizing the outer loop in the bi-level optimization problem for Dataset Distillation is original.\n- **Quality**: The paper is methodologically sound, demonstrated by a comprehensive set of experiments across five benchmark datasets. It also includes an ablation study that pinpoints the contributions of different components. The derivation of the integral into an analytical closed-form solution makes the algorithm an efficient solution.\n- **Clarity**: The paper is well-organized and the algorithmic steps are outlined in detail.\n- **Significance**: The proposed method is efficient, achieving state-of-the-art results in dataset size reduction while maintaining or even improving model performance."
                },
                "weaknesses": {
                    "value": "1. **Inconsistent and Marginal Gains in Test Accuracy**: The test accuracy of the proposed method doesn't consistently outperform existing techniques. When it does show an improvement, the margin is sometimes minimal.\n2. **Incomplete Review of Related Work**: The paper falls short in its coverage of existing literature. The need for a more comprehensive review is also detailed in the \"Questions\" section below.\n3. **Lack of Comparative Analysis with Data Selection Algorithms**: The experiments in the paper do not include comparisons with data selection algorithms, leaving a gap in understanding how the proposed method stacks up against these approaches."
                },
                "questions": {
                    "value": "1. **Clarification on \"Data Extension\" Terminology**: The term \"data extension\" is unfamiliar and appears to be non-standard. Is it synonymous with commonly used terms like \"data augmentation\" or \"data interpolation\"? If not, what differentiates it, and why opt for this term?\n2. **Major Revisions in Related Work Section Needed**: The section on related work requires substantial updates for completeness and context.\n    1. **Coresets**: The paper cited is neither the seminal work nor the most recent in the field of coresets. It would be beneficial to include at least these two papers [1*] and [2*], and consider citing earlier foundational works they mention, perhaps in an appendix.\n    2. **Dataset Distillation**: In addition to [2*], works like [3*] and [4*] are missing from both the discussion and comparison tables. Also, MTT, which is covered in the experiments, lacks mention in the related work section. Please include these papers in both the textual discussion and comparative evaluations.\n\n[1*] Yang, Y., Kang, H. & Mirzasoleiman, B.. (2023). Towards Sustainable Learning: Coresets for Data-efficient Deep Learning. *Proceedings of the 40th International Conference on Machine Learning*, in *Proceedings of Machine Learning Research* 202:39314-39330 Available from https://proceedings.mlr.press/v202/yang23g.html.\n\n[2*] Shin, S., Bae, H., Shin, D., Joo, W. & Moon, I.. (2023). Loss-Curvature Matching for Dataset Selection and Condensation. *Proceedings of The 26th International Conference on Artificial Intelligence and Statistics*, in *Proceedings of Machine Learning Research* 206:8606-8628 Available from https://proceedings.mlr.press/v206/shin23a.html.\n\n[3*] Kim, J., Kim, J., Oh, S.J., Yun, S., Song, H., Jeong, J., Ha, J. & Song, H.O.. (2022). Dataset Condensation via Efficient Synthetic-Data Parameterization. *Proceedings of the 39th International Conference on Machine Learning*, in *Proceedings of Machine Learning Research* 162:11102-11118 Available from https://proceedings.mlr.press/v162/kim22c.html.\n\n[4*] Wang, K., Zhao, B., Peng, X., Zhu, Z., Yang, S., Wang, S., ... & You, Y. (2022). Cafe: Learning to condense dataset by aligning features. In\u00a0*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*\u00a0(pp. 12196-12205)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4449/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730865294,
            "cdate": 1698730865294,
            "tmdate": 1699636420020,
            "mdate": 1699636420020,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ftntpas6Fh",
                "forum": "xm8okHEC3G",
                "replyto": "E1ykiueO1K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LMPF (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback and questions. Here we answer your questions point-by-point as follows.\n\n>**Q1**: The test accuracy of the proposed method doesn't consistently outperform existing techniques. When it does show an improvement, the margin is sometimes minimal.\n\n**A1**: In Table 1 and 3, we have conducted experiments on $8$ datasets under $20$ settings. *Compared to our baseline*, our proposed method improves the baseline with an average relative improvement of $6.6$\\% and an absolute improvement of $2.0$\\% on $16$ settings, which is not marginal in dataset distillation field (can be inferred by comparisons with SOTA methods RFAD, MTT and FRePo in Table 1). *When compared to SOTA approaches*, our method has achieved the SOTA performance under $14$ settings. Besides, the performances can be further boosted with more advanced studies, as shown in Table 4 and 8. For example, when combined with RFAD, our performance on CIFAR10, IPC=1 can achieve performance up to $54.8$\\%, which is a new SOTA in Table 1.\n\nWhat's more, we find the proposed module only fails to show superiority on small datasets like MNIST and F-MNIST but still shows to be comparable to other methods. In contrast, it holds a superior performance on larger datasets such as TinyImageNet and ImageNet as shown in Table 1 and 3. Since the dataset distillation aims to solve the difficulty in processing large datasets, we believe performing better on large datasets is more important nowadays.\n\n***\n\n> **Q2**: Incomplete Review of Related Work\n\n**A2**: Thank you for the suggestion. In this paper, we mainly discussed studies like MTT(CVPR2022), FRePo(NeuraIPS2022), RCIG(ICML2023). We have also included the discussion of the suggested work [1*][2*][3*][4*] and experimental comparison with the dataset distillation methods [3*] (IDC) [4*] (CAFE) in Section G of the revision. CAFE proposes to conduct feature alignment for a better dataset distillation performance while IDC is a novel method that analyzes the shortcomings of the existing gradient matching-based condensation methods and develops an effective optimization technique for improving the condensation of training data information. The comparisons are listed in the table below (IDC only provides its mean accuracy in its Table 10, here we only show the mean accuracy without standard deviation value). More results can be found in the revision. Besides, IDC also proposes to divide images into several parts to make full use of the storage budget (IDC+M). We also include this strategy (Ours+M) to make comparisons with IDC in table below.\n\nTable a. Test accuracy (\\%) comparison.\n| Method | CIFAR10  |          |          |\n|--------|----------|----------|----------|\n|        | IPC=1    | IPC=10   | IPC=50   |\n| CAFE   | 31.6     | 50.9     | 62.3     |\n| IDC    | 36.7     | 58.3     | 69.5     |\n| Ours   | **48.4** | **67.2** | **73.8** |\n| IDC+M    | 50.6     | 67.5     | **74.5**     |\n| Ours+M   | **58.6** | **71.1** | 74.2 |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122368022,
                "cdate": 1700122368022,
                "tmdate": 1700705022347,
                "mdate": 1700705022347,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B7qKaaLd0t",
                "forum": "xm8okHEC3G",
                "replyto": "E1ykiueO1K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LMPF (Part 2/2)"
                    },
                    "comment": {
                        "value": ">**Q3**: Lack of Comparative analysis with data selection algorithms\n\n**A3**: Thank you for the question. In this paper, we try to explore what kind of data is more critical during dataset distillation, \"whether we should make a compromise to focus on the majority of data points around the center to ensure good performances on these data and what would happen if we discard these samples\" (the last line in the first paragraph of Section 2.2). Motivated by this curiosity,  we conducted extensive experiments on discarding easy samples or hard samples during training. \n\nCompared to traditional data selection algorithms that find crucial samples first, requiring extra processing time and may lose some information in removed samples, our online method ranks losses within batches during dataset distillation. It is after the samples can be represented by the learned samples (loss in Eq.4 is small) that these samples will be discarded. In this way, the information in discarded samples can be maintained. Besides, the time cost for sorting losses is negligible, making our method more efficient than offline data selection methods. We have also compared our method with 'prune then distill'[a] which prunes the dataset first and then conducts dataset distillation. The experimental results show that when both are combined with MTT, we can achieve a higher performance. Besides, we have also compared our method with DREAM, who is also online method and proposes to conduct clustering (K-Means) to fetch representative samples before dataset distillation every certain iterations. After combining the proposed sample strategy with DREAM's baseline method for a fair comparison, our method can achieve $58.3$\\%(CIFAR10, IPC=1), $71.1$\\%(CIFAR10, IPC=10), $35.0$\\%(CIFAR100, IPC=1) while they are $51.1$\\%, $69.4$\\%, $29.5$\\% for DREAM. We also provide a figure illustrating the sampling differences between DREAM and Ours in Figure 9 of the revision. We hope it can solve your concerns. More detailed discussions can be found in Section G of the revision.\n\nTable b. Test accuracy (\\%) comparison.\n| Method             | CIFAR10      |              |              |\n|--------------------|--------------|--------------|--------------|\n|                    | IPC=1        | IPC=10       | IPC=50       |\n| Prune then distill | 44.7\u00b11.5     | 63.1\u00b10.7     | 69.7\u00b10.4     |\n| Ours+MTT               | **57.9\u00b10.6** | **65.4\u00b10.6** | **72.9\u00b10.2** |\n\n\n***\n\n>**Q4**: Why Data extension and not data augmentation/interpolation\n\n**A4**: Thank you for the question. Both data augmentation and interpolation are techniques designed for dataset extension and differ from each other regarding the processing method. In other words, 'data extension' is the goal while 'data augmentation' and 'interpolation' are methods. In this paper, we named the title of Section 2.3 as 'Crucial sample extension with semantic augmentation' since we propose a method to \"implicitly enrich harder ones in the semantic space through continuously\ninterpolating between two target feature vectors\" (the last paragraph of Introduction). We use semantic augmentation instead of data augmentation due to the reason that data augmentation typically refers to techniques that generate new training samples by applying various transformations or modifications to existing data, such as rotation, flipping, or adding noise. It is usually done before putting samples into models. In contrast, our proposed method conducts augmentation in semantic space. Experimental comparisons are also provided in Figure3.c, which indicates the superiority of the proposed semantic augmentation in the dataset distillation task.\n\n***\n**References**:\n\n[a] Prune then distill: Dataset distillation with importance sampling, ICASSP 2023.\n\n[b] DREAM: Efficient Dataset Distillation by Representative Matching. ICCV2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122543708,
                "cdate": 1700122543708,
                "tmdate": 1700709456160,
                "mdate": 1700709456160,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4FFqF9qleS",
                "forum": "xm8okHEC3G",
                "replyto": "E1ykiueO1K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind reminder to look at the authors' reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer LMPF:\n\nThank you for the precious time and valuable suggestions to our paper. We have updated our revision to include your mentioned works and make comparisons with them, including more data selection methods and dataset distillation methods. And we also give the explanations on our performance gain in our last response. We hope these can address your concerns. As the rebuttal stage is drawing to a close,  we hope to further discuss with you whether or not your concerns have been addressed appropriately. Please let us know if you have additional questions or ideas for improvement.\n\nLooking forward to your reply.\n\nAuthors."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661577672,
                "cdate": 1700661577672,
                "tmdate": 1700661577672,
                "mdate": 1700661577672,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HnEmD9ri9X",
                "forum": "xm8okHEC3G",
                "replyto": "E1ykiueO1K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Another kind reminder to look at the authors' reply"
                    },
                    "comment": {
                        "value": "Dear reviewer LMPF:\n\nSorry for bothering you again. Since there is **only a few hours left until the end of the discussion**, we hope to further discuss with you whether or not your concerns have been addressed appropriately by our responses. Please let us know if you have additional questions or ideas for improvement.\n\nLooking forward to your reply.\n\nAuthors."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709713505,
                "cdate": 1700709713505,
                "tmdate": 1700709713505,
                "mdate": 1700709713505,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iKyWnPxjSk",
                "forum": "xm8okHEC3G",
                "replyto": "HnEmD9ri9X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4449/Reviewer_LMPF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4449/Reviewer_LMPF"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "I want to thank the authors for their timely response. I'm still concerned about the inconsistency of the improvements, but all my other questions have been addressed by the authors. I will keep my initial rating of 6."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734564469,
                "cdate": 1700734564469,
                "tmdate": 1700734564469,
                "mdate": 1700734564469,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8lCfO0of4b",
            "forum": "xm8okHEC3G",
            "replyto": "xm8okHEC3G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4449/Reviewer_R7gb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4449/Reviewer_R7gb"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes two techniques to boost the performance of kernel-based dataset distillation methods: discarding easy samples and infinite semantic augmentation. Experiments on several benchmarks demonstrate the effectiveness of the proposed methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed infinite semantic augmentation technique is mathematically elegant and effective.\n2. The experimental evaluations are comprehensive enough to validate the effectiveness.\n3. The writing is coherent and easy to follow."
                },
                "weaknesses": {
                    "value": "My major concern is on discarding easy samples. \n1. On the one hand, this step requires computing the NFR loss in FRePo twice, which would require longer running time for dataset distillation and make the baseline complex. \n2. On the other hand, this technique is heuristic and seems counterfactual. Intuitively, easy samples should contain some common patterns that can reflect what a class of objects looks like in general. These samples should be more effective than hard samples to capture the major features of each class. In dataset distillation, major information is expected to be stored while other unusual patterns are discarded. It seems strange to me that discarding easy samples leads to better performance, especially when IPC is small. \n3. This strategy drops some samples, which destroys the original data distribution. However, according to Fig. 1, it makes the distilled data better follow the original distribution, which seems strange to me.\n4. Moreover, there seems to be a paper using a similar technique [a].\n5. I would like to see separate results of only using this strategy without ISA, such as in Tab. 2, 3, 4, and 8.\n6. I suggest the authors compare qualitative samples of the baseline, with selection, and with ISA together to better reflect the functionality of each part. Currently it seems that the qualitative results are not different from the original FRePo too much.\n\n[a] Prune then distill: Dataset distillation with importance sampling, ICASSP 2023."
                },
                "questions": {
                    "value": "Please refer to Weaknesses for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4449/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4449/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4449/Reviewer_R7gb"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4449/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764804792,
            "cdate": 1698764804792,
            "tmdate": 1700625698851,
            "mdate": 1700625698851,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vkDAEiomUj",
                "forum": "xm8okHEC3G",
                "replyto": "8lCfO0of4b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer R7gb (Part 1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your time and valuable questions. We answer your questions point-by-point as follows.\n\n>**Q1** : Discarding easy samples requires computing the NFR loss in FRePo twice, which would require longer running time for dataset distillation and make the baseline complex.\n\n**A1**: *The proposed method only requires to compute the 'NFR' loss once*. To be specific, we first calculate the 'NFR' loss with Eq.4 to get the loss of each sample. Afterward, we sort the losses and average those with greater losses to obtain the final loss. Hence, compared to the baseline, we only added a step of sorting, whose time complexity is $\\mathcal{O}(B \\log B)$. $B$ is the batch size. This time cost for ranking is negligible compared to the forward and backward time costs. The python-like pseudo-code relative to the 'NFR loss' is listed below.  Baseline is `    kernel_loss = mean_squared_loss(preds, labels).mean()  `, and ours is as follows:\n\n`   kernel_loss = mean_squared_loss(preds, labels)  `  \n`   _, idx_ranked = top_k(kernel_loss, len(kernel_loss))  `    \n`  kernel_loss = (kernel_loss[idx_picked[:int((1-discarding_rate))*len(idx_ranked))]]).mean()     `  \n\nFurthermore, we have also provided the time cost comparison with our baseline in Appendix D, which writes \"As for the training time cost, it is 2.5 hours (training time for 500,000 steps in total) under CIFAR10, IPC=10 setting while it is 2.4 hours for our baseline\". This indicates that the proposed module introduces negligible extra computational and time costs.\n\n***\n\n>**Q2** : Discarding easy samples is heuristic and seems counterfactual.  \n\n**A2**: Thank you for the question . Actually, we hold the same curiosity with you on \"whether we should make a compromise to focus on the majority of data points around the center to ensure good performances on these data and what would happen if we discard these samples\" (the last line in the first paragraph of Section 2.2). Motivated by this curiosity,  we conduct extensive experiments on discarding easy samples or hard samples in Section2.2 (both the Figure 1 and Figure 5). The results on three datasets and four settings all suggest that dropping easy samples helps dataset distillation in all cases. Here we provide some explanations below.\n- The goal for dataset distillation is to condense the large dataset into a smaller one such that a model trained on it can  achieve a comparable test performance as one trained on original dataset (see Introduction). In other words, we need to depict a good decision boundary for classification based on the distilled data. However,  relying solely on easy samples may result in short-cut learning[a]. In contrast, hard samples are difficult-to-be-distinguished samples (Eq.4) that usually exist along the decision boundary thus can help to support a more explicit decision boundary. We think that's why hard samples can help to improve the dataset distillation performances. \n- We have also explored it from both the perspective of data manifold and information in the third paragraph of Section 2.2. In short, we observed an increased manifold overlap between the generated images and the original dataset after discarding some easy samples (Figure 1 middle two images), thereby depicting a better representation of the manifold. We also find that making precise predictions for simple samples with hard samples is easy but the reverse is not. This indicates that harder samples are more informative than easier ones.\n\nThe above discussion has been included in Appendix B of the revision. We think this is a really interesting finding and it is the 'counterfactual nature' of our discovery that makes our work unique and distinct. We hope it can bring more insights to scholars in dataset distillation.\n\n***\n\n>**Q3**: The data distribution in  Fig. 1.  \n\n**A3**: In fact, we found that compared to the distilled samples generated by the baseline, the samples distilled by our method tend to be distributed not only in high-density spaces (central regions) but also in low-density spaces. This helps to increase the data manifold overlap with the original dataset."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700123818633,
                "cdate": 1700123818633,
                "tmdate": 1700123818633,
                "mdate": 1700123818633,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GS4vrIUbpX",
                "forum": "xm8okHEC3G",
                "replyto": "8lCfO0of4b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer R7gb (Part 2/3)"
                    },
                    "comment": {
                        "value": ">**Q5**: Separate results of only using dropping strategy in Tab2, 3, 4, 8\n\n**A5**: We list the results of using dropping/discarding strategy of Table 2, 3, 4, 8 in the following:\n-  Table 2 evaluates the cross-architecture generalization ability of the distilled data. This evaluation with only discarding operation can be found in Figure 3.d. To be short, removing easy samples can result in a small generalization ability degradation. However, this can be alleviated by our proposed ISA, as shown in Table 2 and Figure 3.d.  \n\n- Table 3 provides the evaluation of a larger dataset. The ablation studies of each proposed module on three datasets (CIFAR10, CIFAR100, Tiny-ImageNet) can be found in Figure 3.a. All the results validate the effectiveness of the proposed discarding strategy and ISA. We have also conducted experiments to provide discarding ablations on ImageNet datasets below. The results are consistent with ablation results in Figure 3.a that discarding some easy samples can help in dataset distillation.  \n\nTable b: Test accuracy (\\%) of adding discarding only (corresponding to Table 3 in main text)\n| Method      | ImageNette(128x128) |              | ImageWoof(128x128) |              | ImageNet(64x64) |              |\n|-------------|---------------------|--------------|--------------------|--------------|-----------------|--------------|\n|             | 1                   | 10           | 1                  | 10           | 1               | 2            |\n| Baseline    | 48.1\u00b10.7            | 66.5\u00b10.8     | **29.7\u00b10.6**       | 42.2\u00b10.9     | 7.5\u00b10.3         | 9.7\u00b10.2      |\n| +Discarding | **49.1\u00b10.7**        | **66.6\u00b10.4** | 28.0-0.4           | **43.0\u00b10.7** | **8.6\u00b10.2**     | **10.6\u00b10.2** |\n\n- Table 4 and  Table 8 provide experimental results of combining the proposed modules with other methods. We list the required results of adding discarding operation only to these methods below. Discarding can boost the performances in most scenarios. This is consistent with our findings. \n\nTable c: Test accuracy (\\%) of adding discarding only (corresponding to Table 4 in main text)\n| Method | CIFAR10  |              |              |              |          |              |\n|--------|----------|--------------|--------------|--------------|----------|--------------|\n|        | IPC=1    |              | IPC=10       |              | IPC=50   |              |\n|        | Baseline | +Discarding  | Normal       | +Discarding  | Normal   | +Discarding  |\n| RFAD   | 52.1\u00b10.1 | **54.3\u00b10.1** | 65.3\u00b10.1     | **66.5\u00b10.1** | 69.8\u00b10.2 | **70.1\u00b10.1** |\n| FRePo  | 46.8\u00b10.7 | **48.0\u00b10.4** | 65.5\u00b10.4     | **66.8\u00b10.4** | 71.7\u00b10.2 | **72.9\u00b10.1** |\n| RCIG   | 53.9\u00b10.5 | **53.9\u00b10.3** | 67.3\u00b10.3     | **67.7\u00b10.4** | 73.5\u00b10.2 | **73.7\u00b10.4** |\n\nTable d: Test accuracy (\\%) of adding discarding only (corresponding to Table 8 in main text)\n| Method | CIFAR10  |              |              |              |          |              |\n|--------|----------|--------------|--------------|--------------|----------|--------------|\n|        | IPC=1    |              | IPC=10       |              | IPC=50   |              |\n|        | Baseline | +Discarding  | Normal       | +Discarding  | Normal   | +Discarding  |\n| DM     | 25.9\u00b10.8 | **26.1\u00b10.3** | **48.9\u00b10.6** | 48.4\u00b10.6     | 62.7\u00b10.5 | **62.8\u00b10.2** |\n| MTT    | 46.3\u00b10.8 | **52.9\u00b10.7** | 65.2\u00b10.5     | **65.2\u00b10.5** | 71.6\u00b10.2 | **72.1\u00b10.2** |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700123971158,
                "cdate": 1700123971158,
                "tmdate": 1700124959981,
                "mdate": 1700124959981,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QOM6Ah6AWV",
                "forum": "xm8okHEC3G",
                "replyto": "8lCfO0of4b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer R7gb (Part 3/3)"
                    },
                    "comment": {
                        "value": ">**Q4**: Comparison to 'Prune then distill'.\n\n**A4**: 'Prune then distill' is a method that proposes to prune the dataset first and then conduct dataset distillation. Our method differs from it from two perspective: \n- Compared to the vallina dataset distillation methods, the offline pruning will introduce extra time costs that you mentioned in the first question. In contrast, our method is an online one which just needs to rank the losses and average the topk losses, introducing almost no time costs as illustrated in the Answer to the fisrt question.  \n- The offline pruning may result in information loss by dropping samples before dataset distillation. Our online method ranks losses within batches during dataset distillation, it is after the samples can be represented by the learned samples (*a.k.a* 'NFR' loss is small) that these samples will be discarded. In this way, the information in easy samples can be maintained. The experimental comparisons are listed as follows. Note that since the 'prune then distill' adopts the distillation method MTT as their distillation method, here we provide MTT+Ours for a fair comparison in the table below, which indicates that our proposed method has a better performace. Detailed information can be found in Section G in the revision.\n\nTable a: Test accuracy (\\%) comparison\n| Method             | CIFAR10      |              |              |\n|--------------------|--------------|--------------|--------------|\n|                    | IPC=1        | IPC=10       | IPC=50       |\n| Prune then distill | 44.7\u00b11.5     | 63.1\u00b10.7     | 69.7\u00b10.4     |\n| Ours               | **57.9\u00b10.6** | **65.4\u00b10.6** | **72.9\u00b10.2** |\n\n***\n\n>**Q6**: Ablation study and the performance gain.\n\n**A6**: The ablation study of each component is provided in the first paragraph (**Ablation Studies on Each Proposed Module.**) of Section 3.2 (ABLATION STUDIES). It is carried out on three datasets (Figure 3.a) and all the results indicate the effectiveness of both the proposed modules. \n\nAs for the performance gain, in Table 1 and 3, we have conducted experiments on $8$ datasets under $20$ settings. Compared to the baseline, our proposed method improves the baseline with an average relative improvement of $6.6$\\% and an absolute improvement of $2.0$\\% on $16$ settings, which is not marginal in the dataset distillation field (can be inferred by comparisons with SOTA methods RFAD, MTT and FRePo in Table 1)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124040099,
                "cdate": 1700124040099,
                "tmdate": 1700124040099,
                "mdate": 1700124040099,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8budXZzxKG",
                "forum": "xm8okHEC3G",
                "replyto": "8lCfO0of4b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind reminder to look at the authors' reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer R7gb:\n\nWe thank you for the precious review time and valuable comments. We have provided corresponding responses with elaborate discussions and extensive experiments on why dropping easy samples can help in dataset distillation,  which we hope to address your concerns. We hope to further discuss with you whether or not your concerns have been addressed appropriately. Please let us know if you have additional questions or ideas for improvement.\n\nLooking forward to your reply.\n\nAuthors."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448290270,
                "cdate": 1700448290270,
                "tmdate": 1700448290270,
                "mdate": 1700448290270,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E8wCowhl95",
                "forum": "xm8okHEC3G",
                "replyto": "8lCfO0of4b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Another kind reminder to look at the authors' reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer R7gb:\n\nSorry to bother you again. As the rebuttal stage is drawing to a close (only 1-2 days left), we wanted to bring to your attention that we have not yet received any feedback from you yet. We have provided corresponding responses with elaborate discussions and extensive experiments on why dropping easy samples can help in dataset distillation a few days ago, which we hope to address your concerns. We hope to further discuss with you whether or not your concerns have been addressed appropriately. Please let us know if you have additional questions or ideas for improvement.\n\nLooking forward to your reply.\n\nAuthors."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534911795,
                "cdate": 1700534911795,
                "tmdate": 1700534962144,
                "mdate": 1700534962144,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OujZQbhwAJ",
                "forum": "xm8okHEC3G",
                "replyto": "vkDAEiomUj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4449/Reviewer_R7gb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4449/Reviewer_R7gb"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for the detailed response. I have understood that discarding easy samples may help DD generate more samples near decision boundaries, which can further help the learning of these boundaries. It would be better if there could be more theoretical analysis on this part. \n\nOverall, my concerns are alleviated and I choose to raise my score to 6."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625682403,
                "cdate": 1700625682403,
                "tmdate": 1700625682403,
                "mdate": 1700625682403,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]