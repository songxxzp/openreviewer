[
    {
        "title": "The Fundamental Limits of Least-Privilege Learning"
    },
    {
        "review": {
            "id": "iKWZ6owVmV",
            "forum": "jGuXGNcK6O",
            "replyto": "jGuXGNcK6O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5214/Reviewer_qV6w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5214/Reviewer_qV6w"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the data attribute leakage problem in machine learning models. The core idea of this paper is that any representation that provides utility for prediction always leaks information about properties of the data other than the task. This work gives the definition of the least-privilege principle (LPP) and gives the theory of the trade-off between LPP and utility. The work also demonstrates this trade-off experimentally in an image classification setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This work theoretically analyzes the relationship between data attribute leakage and utility, which is important for the security of machine learning. This work also reveals the inherent properties of machine learning models."
                },
                "weaknesses": {
                    "value": "1. The study of data attribute leakage is an active research field. Some work attempts to mitigate attribute leakage by designing sophisticated algorithms. This paper does not mention and compare cutting-edge defense schemes in the empirical experiment part.\n\n2. More extensive experiments should be used to evaluate the proposed theory. At present, this work is only conducted on a dataset and a neural network model."
                },
                "questions": {
                    "value": "1.\tAdvanced defense strategies should be discussed. If advanced defense strategies can solve the problem of data attribute leakage, then the value of this work will be limited. Therefore, could the author provide a more adequate overview of the research area discussed? It\u2019s not just data attribute leaks, it should also include cutting-edge defense methods.\n2.\tCould the authors conduct empirical experiments on a wider range of datasets and models? For example, conduct experiments on some NLP tasks. More extensive experimental results can more fully verify the proposed theory.\n3.\tDoes the complexity of the feature extractor affect the extent of data attribute leakage?\n4.\tThere are some typographical errors in the paper. For example, formulas (12), (14), and (19) in Appendix A lack punctuation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5214/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5214/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5214/Reviewer_qV6w"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5214/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698680590798,
            "cdate": 1698680590798,
            "tmdate": 1699636519059,
            "mdate": 1699636519059,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fTtzSS7NGx",
                "forum": "jGuXGNcK6O",
                "replyto": "iKWZ6owVmV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for taking the time to read our paper and the valuable suggestions. Below, we try to address your comments.\n\n**Comparison to other learning techniques:** The review rightly notes that the literature suggests a wide range of learning techniques to address attribute leakage in deep network models. As we show in _Section 3.2, Theorem 2,_ however, the trade-off between a representation\u2019s utility and achieving the LPP applies \u201cregardless of the way the feature representations are obtained\u201d (_Section 3.2, page 6_). In Section 4, we provide an empirical evaluation that supports our theoretical results: Regardless of a model's learning task and even under censoring, the LPP cannot be fulfilled while simultaneously providing high utility for the prediction task.\n\nWe furthermore note that to the best of our knowledge none of the existing techniques in the literature can actually achieve the desired goal of LPL. To do so, the features learned by the model would need to simultaneously restrict leakage for any sensitive attribute $S$. Techniques to defend against attribute inference are mostly limited to one or two fixed sensitive attributes and hence cannot achieve this goal.\n\n**Complexity of feature extractor:** Our theoretical results imply that the trade-off applies not only regardless of the learning technique, but also regardless of a model\u2019s architecture. We have run additional experiments using a different model architecture (ResNet-18) on the same image dataset to confirm that, as predicted by Theorem 2, the trade-off applies regardless of the exact architecture choices. We have uploaded these results as supplementary material, and we them to the Appendix.\n\n**Additional dataset:** In the supplementary material, we provide the results of an additional experiment which demonstrates the strict trade-off between model utility and the LPP on a very different type of data and model. As for tabular data, together with image data, sharing feature encodings instead of raw data is often suggested as a solution to limit harmful inferences, we chose the Texas Hospital dataset and the TabNet model architecture (Arik & Pfister, 2021) for these experiments.\n\nThe results show that, as predicted by Theorem 2 (Section 3.2), for any learning task there always exists a sensitive attribute that violates the LPP."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5214/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700071182398,
                "cdate": 1700071182398,
                "tmdate": 1700071182398,
                "mdate": 1700071182398,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PNkOixxRNq",
            "forum": "jGuXGNcK6O",
            "replyto": "jGuXGNcK6O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5214/Reviewer_KceC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5214/Reviewer_KceC"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies unintended privacy leakage in collaborative learning. Specifically, the paper proposes to  formalize of the least-privilege principle for machine learning. Via information theory, the paper observes that every task comes with fundamental leakage\u2014a representation shared for a particular task must reveal the information that can be inferred from the task label itself. Such fundamental leakage is also testified in a real-world dataset"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+The paper is well-written and easy to follow\n\n+Understanding the fundamental information leakage in collaborative learning is important"
                },
                "weaknesses": {
                    "value": "-The key differences with the existing information-theoretic privacy is unclear\n\n-The observations are only shown on a single dataset"
                },
                "questions": {
                    "value": "While the paper uses least-privilege principle to formalize information leakage, I do not know how this largely differentiate other works that use the similar idea (though those works assume a fixed sensitive attribute), e.g., Zhao et al., 2020; Brown et al., 2022 and Salamatian et al., Privacy-Utility Tradeoff and Privacy Funnel. What is the key technical challenge when we do not assume a fixed sensitive attribute, but assume it is a superset of the input?\n\nTheoretically, the paper shows the fundamental leakage and observes this on a dataset. I am curious how common such observation is in more datasets. \n\nThe evaluation is only tested on a single attribute inference. How generalizable it is to more attribute inference (e.g., data reconstruction attack)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5214/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698847394404,
            "cdate": 1698847394404,
            "tmdate": 1699636518981,
            "mdate": 1699636518981,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wjSw7ymevS",
                "forum": "jGuXGNcK6O",
                "replyto": "PNkOixxRNq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the review and your questions which we try to clarify below.\n\n**Key difference to existing information-theoretic approaches:** \n\n_Connection to CEB:_ Our formalisation of the (previously only stated informally) problem of least-privilege learning can be thought of as a version of the generalised Conditional Entropy Bottleneck (CEB) problem (Fischer, 2020), as we explain in Section 3.2, page 5. In contrast to standard CEB, we consider a stronger notion of leakage, i.e., maximal leakage rather than mutual information, between the representations $Z$ and the sensitive raw data $X$. Weaker notions of leakage are not adequate in our case where we want to assess the least-privilege claim.\n\n_Connection to Privacy Funnels:_ The Privacy Funnel (PF) problem is different from the least-privilege problem that we consider in this work; as Asoodeh & Calmon, 2020 point out, PF and bottleneck-type problems like CEB are \"duals\" of each other.\n\nAs the review notes, privacy funnels operate in a different setting. Most crucially, because they consider only a single fixed attribute. As we discuss in Section 3 (\u201cAttribute Inference\u201d), we have to treat any attribute other than the learning task as sensitive to capture the _least-privilege_ principle: We want the representation to leak _nothing_ else other than the fundamental leakage implied by the learning task.\n\n**Limits of empirical evaluation:** In the Supplementary Material, we provide the results of an additional experiment which demonstrates that the strict trade-off between model utility and the LPP also holds on a very different type of dataset and model. As for tabular data, together with image data, sharing feature encodings instead of raw data is often suggested as a solution to limit harmful inferences, we chose the Texas Hospital dataset and the TabNet model architecture (Arik & Pfister, 2021) for these experiments.\n\nThe results show that, as predicted by Theorem 2, Section 3.2, for any learning task there always exists a sensitive attribute that violates the LPL.\n\nWe limit the empirical evaluation to single attribute inference attacks as, despite their simplicity, they are sufficient to support our theoretical result: As soon as the adversary\u2019s inference gain on a single attribute exceeds the features\u2019 utility for the learning task, the features cannot be claimed to fulfil the LPL claim. We note, however, that Theorem 2 covers any type of inference attack, incl. data reconstruction attacks for the case where $S=X$."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5214/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700071028636,
                "cdate": 1700071028636,
                "tmdate": 1700071028636,
                "mdate": 1700071028636,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cDpghuKmTX",
                "forum": "jGuXGNcK6O",
                "replyto": "PNkOixxRNq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Revised version"
                    },
                    "comment": {
                        "value": "We have uploaded a revised version of our paper that\n- clarifies the connection to related information theoretic problems, such as CEB and Privacy Funnel (see Section 3.2 page 5).\n- contains additional experiment results that confirm that, as predicted by our theoretical results, the trade-off also applies to other types of data (see Appendix C.3, Figure 9).\n\nWe hope that these revisions address your comments in a satisfactory way."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5214/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490958917,
                "cdate": 1700490958917,
                "tmdate": 1700490958917,
                "mdate": 1700490958917,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Lkn1HBbOR2",
                "forum": "jGuXGNcK6O",
                "replyto": "wjSw7ymevS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5214/Reviewer_KceC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5214/Reviewer_KceC"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors' Response"
                    },
                    "comment": {
                        "value": "Thanks for the comments! \n\nHowever, I am still unclear to the novelty of the paper and the rationality of the assumption. \n\nFirst, as pointed out by the authors, the survey paper [a] already well clarifies the terminology, such as information bottleneck and privacy funnel, which are introduced in the form of mutual information, entropy.  In addition, [b] has theoretically analyzed the Fundamental limits of perfect privacy, while the maximal leakage concept and formulation are from [c]. Compared with these prior work, whats the key differences of the proposed theoretical results? What are the challenges? I checked the proof, but did not see new results? Please correct me if I am wrong. \n\nSecond, the authors also assume that the space of inputs is a subset of the space of sensitive attributes. What real-world scenarios make this assumption feasible? Is this only for the purpose of deriving the theoretical results (e.g., inspired by Theorem 1 in [b])? Note that the evaluation only tests on a single private attribute, which did not satisfy the assumption. \n\n[a] Asoodeh and Calmon, Bottleneck Problems: Information and Estimation-Theoretic View\n\n[b] Calmon and Me \u0301dard, Fundamental limits of perfect privacy, 2015. \n\n[c] Issa et al. An Operational Approach to Information Leakage. IEEE TRANSACTIONS ON INFORMATION THEORY,  2020"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5214/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535229348,
                "cdate": 1700535229348,
                "tmdate": 1700535229348,
                "mdate": 1700535229348,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UquCpd0I9g",
            "forum": "jGuXGNcK6O",
            "replyto": "jGuXGNcK6O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5214/Reviewer_kXjd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5214/Reviewer_kXjd"
            ],
            "content": {
                "summary": {
                    "value": "This contribution addresses the concerns of data misuse when offloading model training and inference to a service provider. Collaborative learning and model partitioning are proposed as solutions, where clients share representations of their data instead of the raw data. The principle of least privilege is introduced, which states that the shared representations should only include information relevant to the task at hand. The authors provide the first formalization of the least-privilege principle for machine learning. They prove that there is a trade-off between the utility of the representations and the leakage of information beyond the task. Experiments on image classification demonstrate that representations with good utility also leak more information about the original data than the task label itself. As a result, censoring techniques that hide specific data attributes cannot achieve the goal of least-privilege learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper reveals the fundamental limits of Least-Privilege learning."
                },
                "weaknesses": {
                    "value": "1. The presentaion needs improvement.\n\n2. The contribution is limited."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5214/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698884248219,
            "cdate": 1698884248219,
            "tmdate": 1699636518896,
            "mdate": 1699636518896,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9ZpjSudW5z",
                "forum": "jGuXGNcK6O",
                "replyto": "UquCpd0I9g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the review. As the review has pointed out in the summary, our paper provides the first formalisation and a generic characterization of the trade-offs involved in least-privilege learning, which was proposed in the prior work (Melis et al., 2019; Brown et al., 2022). Our characterization applies to any method of obtaining feature representations, covering the model partitioning and collaborative learning settings. Please let us know if there are any questions we can answer to change the assessment of the contribution significance, or to clarify any concrete issues with the presentation."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5214/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700070366209,
                "cdate": 1700070366209,
                "tmdate": 1700070366209,
                "mdate": 1700070366209,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PeNUEa5fWO",
            "forum": "jGuXGNcK6O",
            "replyto": "jGuXGNcK6O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5214/Reviewer_3Rfy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5214/Reviewer_3Rfy"
            ],
            "content": {
                "summary": {
                    "value": "This work investigates the limits to information leakage by considering the least privilege principle. It formalises this notion under strict condition where any attribute other than the task label is deemed as sensitive for leakage. They propose a formal definition of LPP and under strictly positive posterior and the assumption that the label needs to be shared with the service provider, they theorise that there does not exists a a feature map such that both LPP and utility (as defined by mutual information between labels and feature representation) hold simultaneously.  They further support this with pairwise empirical evaluations across 12 attributes where one is considered as label and the other as the adversary\u2019s targeted sensitive attribute. The analysis also compares inference gain under standard and censoring models. \n\nIn my opinion the paper relies on some fundamental assumptions which have been clearly stated\n- First, it\u2019s a worst case analysis. This has also been highlighted in the Problem Setup (Section 2) and the text following Corollary 1.\n- I think the assumption for label information to be made available is pretty strong from a practical perspective as users may not necessarily need to provide labels to the service provider.\n- Unlike unconditional LPP, the LPP is defined with respect to what the authors consider as fundamental limit of leakage from the label."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I found the paper to be generally well written and easy to follow. While I am not familiar with all the current literature in this field, I found that the paper clearly states its worst-case assumptions and conveys its theoretical results with insights."
                },
                "weaknesses": {
                    "value": "I found the discussion on how users can manage the trade-offs to be quite limited. There is some discussion on DP and its usefulness for training but not test time inference but I think some more discussion on what this theoretical analysis would mean for a user would be useful."
                },
                "questions": {
                    "value": "Although I am not fully familiar with the related literature on this, I found the paper to be self-contained with insightful discussion. \nI voted for marginal acceptance because of the following, and would appreciate if authors can help clarify\n\n1. Practical implications for users and service providers because of the noted limits\n2. Arguments for why sharing label information is a practical worst-case assumption\n3. How the work relates to privacy-preserving techniques other than censoring, like de-anonymisation or sharing the information under encryption etc? While I understand this may not be within the scope and/or page limit constraints, I think even qualitative arguments can help position the paper for discussion within the privacy and ML community."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5214/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699257163712,
            "cdate": 1699257163712,
            "tmdate": 1699636518806,
            "mdate": 1699636518806,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xlZi2MW6x7",
                "forum": "jGuXGNcK6O",
                "replyto": "PeNUEa5fWO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank you for the thoughtful review and the questions which we will try to address as best as possible next.\n\n**Practical implications for users and service providers:** This is an excellent question which we tried to partially address in Sections 4.1 and 4.2 (\u201cTakeaways\u201d). The primary conclusion we draw from our results is that service providers should carefully evaluate the trade-off between utility and unintended information leakage for a given learning task and more openly communicate their results to increase transparency and avoid unexpected inferences, with potentially harmful consequences, on the user side. In particular, service providers should evaluate the fundamental leakage of a task and inform users that \u201cany data attribute that is correlated with the chosen task\u201d (Section 4.1, page 8) might also be revealed. This would enable users to make informed choices about the risk they might take by the use of a prediction service or by contributing their data for model training.\n\nSecond, going forward, we encourage both researchers and practitioners to more rigorously evaluate broad claims, such as, the claim that sharing feature representations instead of raw data records limits information leakage to \u201conly the features relevant to a given task\u201d (Melis et al., 2019). We hope that our formal definitions serve as a good basis to evaluate such claims and any future proposals.\n\n**Strong assumption on sharing label information:** The assumption that the service provider has access to the correct label information of a particular target record is primarily motivated by the observation that it is impossible to achieve the strict LPP definition given in Section 3.1, Definition 1. To allow for any meaningful learning, we have to at least allow for the fundamental leakage through the task label (see _Section 3.2, page 5_), which means we have to assume that the label is known to the adversary. We note that a weaker assumption on the fundamental leakage adversary would result in an even worse trade-off between unintended feature leakage and a model\u2019s utility.\n\nIn some preliminary experiments, we did assess the success of an adversary that only has access to a record\u2019s predicted task label instead of the ground truth. We observed that, as expected, the adversary\u2019s success in predicting a sensitive attribute from the predicted task label increased with increasing model utility. In our analysis and final experiments, we decided to only consider the worst-case adversary that has access to the correct task label as this gives us a lower bound on the adversary\u2019s multiplicative gain and hence the best method to analyse the trade-off.\n\n**Relation to other privacy-preserving techniques:** ln short, alternative solutions to this problem, such as encryption or complete data isolation and local processing are orthogonal to the idea of sharing feature representations instead of the raw data to prevent harmful inferences. As Osia et al., 2017 and 2018, discuss, these methods are often not efficient enough for big data and deep learning.\n\nData anonymisation can indeed be thought of as an instance of sharing a representation of the data rather than the data itself (see [Makhdoumi, et al., 2014](https://arxiv.org/abs/1402.1774)); with the crucial difference that the set of sensitive attributes is fixed and limited and that the learning task is not defined upfront. This hence leads to a different trade-off between preserving and hiding information than in our setting."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5214/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700070313231,
                "cdate": 1700070313231,
                "tmdate": 1700070313231,
                "mdate": 1700070313231,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vHicvsjpyJ",
            "forum": "jGuXGNcK6O",
            "replyto": "jGuXGNcK6O",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5214/Reviewer_orQ6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5214/Reviewer_orQ6"
            ],
            "content": {
                "summary": {
                    "value": "The paper analyzes inference-time information leakage due to releasing the representations of sensitive input data (compared to only releasing the predicted label). It investigates how this leakage relates to the quality of the representation. For modeling the quality of representation (concerning a downstream prediction task), the authors analyze the mutual information between the representation and the label. For modeling the information leakage from the representation about the sensitive attributes, the authors analyze the Bayesian optimal adversary for attribute inference. The authors prove that whenever the representation enables non-negligible performance for downstream prediction tasks, there must exist attributes that are significantly more leaked through the additional release of representation (compared to only releasing the label) of the sensitive input data. \n\nTo interpret and validate this inherent trade-off, the authors further perform experiments on tabular datasets to quantify the empirical information leakage and model utility. Specifically, information leakage is measured against an empirically instantiated Bayesian optimal adversary using auxiliary data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors prove an interesting inherent trade-off between the model's utility and the information leakage due to releasing representation (compared to only releasing labels). As an interesting baseline, the authors also discussed the fundamental information leakage of how much the label of input data reveals sensitive attributes.\n\n- Experiments support the proved trade-off, as the authors observe a positive correlation between the number of attributes that incur high information leakage (due to releasing representations) and the model's utility. Interestingly, when the most leaked attributes are censored during the model training and inference phase, the authors observe that the leakage about other attributes increases."
                },
                "weaknesses": {
                    "value": "- The model splitting between server and clients analyzed in this paper is counter-intuitive. Namely, the authors assume that clients use only the representation layers of the model, while the server uses only a classification head of the model. Since the classification heads are usually small and easy to train, I do not see the incentive for clients to share input data representations to the server in this setting (for prediction tasks). For example, the clients may download the classification head weights or tune a classification head on local data and compute the labels locally at inference time. This also seems different from Melis et al. 2019, where the clients only compute one embedding layer (rather than a large part of the whole model).\n\n- The definitions and notations lack clarity at times. Most importantly, the information leakage is defined by successful inference of *any* attribute. This may be overly strong as many attributes are less sensitive. I'm wondering whether the proved trade-off in this paper is largely a result of this overly strong definition of information leakage."
                },
                "questions": {
                    "value": "- Could the authors discuss more about the effect of the splitting method, i.e., what parts are deemed as representations, on the theoretical and empirical conclusions? For example, when the client only has access to a small part of the model, would the leakage still be unbalanced on different attributes?\n\n- Could the authors discuss how the theoretical and empirical trade-off (between information leakage and model utility) might change if we only consider a smaller set of attributes (rather than the whole input feature space) as sensitive?\n\nMinor comments regarding clarity:\n- What are the transition orders between random variables in the Markov chain $Y - X - W$ defined in Section 2?\n- In Figure 4, bottom right plot, what is the meaning of color? Why is the third row from the top colored blue despite its negative value? What does a negative model utility $\\tilde{I}_{\\infty}(Y, Z)$ mean?\n- Assumption A requires a positive posterior but does not require any lower bound for the posterior density. Does it mean the posterior could also be arbitrarily close to a point distribution?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5214/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699550819000,
            "cdate": 1699550819000,
            "tmdate": 1699636518726,
            "mdate": 1699636518726,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7GlKszuCaF",
                "forum": "jGuXGNcK6O",
                "replyto": "vHicvsjpyJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to the major comments and questions"
                    },
                    "comment": {
                        "value": "First, thank you for taking the time to review our work and the insightful comments which we try to address below.\n\n**Model splitting between server and clients is counter-intuitive:** The model partitioning setup, as we describe it in this paper, is a common paradigm in the area of privacy-preserving cloud computing and in the context of privacy for MLaaS (see, for instance, Osia et al., 2018, Wang et al., 2018, Chi et al., 2018, Li et al., 2021, and Brown et al., 2022). The main motivation, cited in all works and also the focus of our work, is to prevent harmful inferences from raw data records. Other reasons to not share the full model might include intellectual property concerns or scalability considerations (see Osia et al., 2018 for a more detailed discussion).\n\n**Effect of the splitting method on theoretical and empirical conclusions:** In _Section 3.2, Theorem 2,_ we show that the trade-off between a representation\u2019s utility and achieving the LPP applies to any feature representation regardless of the exact splitting method, model architecture or \u201cthe way the feature representations are obtained\u201d (_Section 3.2, page 6_).\n\nIn our experiments, we evaluate the trade-off between utility and unintended inferences on the last layer feature representations of a CNN model. We choose the last layer because it is where a record\u2019s feature activations are expected to be most learning task specific (Melis et al., 2019 and Mo et al. 2021) and hence the claim that these representations meet the LPP is the strongest. Our results demonstrate that, even at the last layer and under censoring of particular attributes, the least-privilege claim does not hold true.\n\nWe have run additional experiments: (1) using a different model architecture (ResNet-18) on the same image dataset, and (2) using a different model (TabNet) on an additional tabular dataset (Texas Hospital dataset) to confirm that, as predicted by Theorem 2, the trade-off applies regardless of the exact model architecture choices. We have uploaded these empirical results as supplementary material, and will add add them to the Appendix.\n\n**Difference to Melis et al.:** The review points out correctly that the setting which we use to motivate our work and in our empirical evaluation differs from the setting described by Melis et al., 2019 who model unintended feature leakage in a collaborative learning setting. Yet, as we state at the beginning of Section 2: \u201call of our formal results apply to any setting in which feature representations are used as a means to limit the data revealed to untrusted third parties, such as the collaborative learning setting\u201d (_Section 2, page 2_). The gradients shared during collaborative learning are a noisy representation of the features learned by the model, and hence are also captured by our setting $Z = f_E(X)$ in the same way as feature representations in the model partitioning scenario.\n\n\n**Worst-case assumption on attributes considered sensitive and how it affects the trade-off:** As mentioned in Section 3 (\"Attribute Inference\"), we want to model an adversary that captures the _least-privilege_ principle which implies that the representations shared with the service provider leak _nothing else_ other than the fundamental leakage implied by the learning task. To adequately capture this principle, we have to consider any attribute, other than the learning task, to be sensitive.\n\nIf we were to restrict the set of attributes considered sensitive, the problem would become similar to a version of the Privacy Funnel (PF) problem (Asoodeh & Calmon\u201920). PF is different from the least-privilege learning problem (a form of generalized Conditional Entropy Bottleneck (CEB) problem; see next) that we consider in this work. As Asoodeh & Calmon point out, PF and bottleneck-type problems like CEB  are \"duals\" of each other.\n\n**Trade-off as a result of strong assumptions:** As we discuss in Section 3.2, the strict trade-off between utility and the LPP exists because the LPP restricts the adversary\u2019s inference gain in terms of maximal leakage. In the standard CEB problem (Fischer, 2020), which considers a weaker notion of unintended leakage, the mutual information between the representations $Z$ and the sensitive raw data $X$, there is hence no such trade-off (see _Section 3.2, page 6_). Such a weak notion of leakage, however, is not adequate to formalise and assess the original claim from many papers that it is possible to learn feature representations that protect against any harmful inferences and fulfil the least-privilege principle."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5214/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700070123550,
                "cdate": 1700070123550,
                "tmdate": 1700070123550,
                "mdate": 1700070123550,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6J7cE8O1LB",
                "forum": "jGuXGNcK6O",
                "replyto": "vHicvsjpyJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to minor comments"
                    },
                    "comment": {
                        "value": "**Response to minor comments:**\n\n- The transition kernels between Y - X - W can be arbitrary so long as $Y \\bot W \\mid X$.\n\n- The positive posterior requirement implies both upper and lower bound as the notation is a shorthand for $P(Y = y \\mid X = x) > 0$ for all $y \\in \\mathbb{Y}$. Indeed, the assumption allows distributions that are arbitrarily close to a point distribution, but not exactly point distributions.\n\n- The bar in the third row should indeed be coloured red. We apologise for this data plotting error. A negative value indicates that the performance of the classifier has dropped below the majority class baseline guess, i.e., that the features learned by the model do not provide any utility for the prediction task. This is a common side effect of censoring techniques which often lead to a degradation in model performance (see, for instance, Song & Shmatikov, 2019 and Raff & Sylvester, 2018)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5214/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700128217059,
                "cdate": 1700128217059,
                "tmdate": 1700128217059,
                "mdate": 1700128217059,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "80T7xWxI8u",
                "forum": "jGuXGNcK6O",
                "replyto": "vHicvsjpyJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "content": {
                    "title": {
                        "value": "New revision"
                    },
                    "comment": {
                        "value": "We have uploaded a revised version of our paper that\n- better justifies our theoretical assumptions to consider all attributes as sensitive and to analyse the trade-off in terms of maximal leakage (see Section 3.2 page 5).\n- better explains how these theoretical assumptions affect the strict trade-off between the least-privilege principle and utility (see Section 3.2 page 6)\n- contains additional experiment results that confirm that, as predicted by our theoretical results, the trade-off applies to any feature representation regardless of the feature mapping (see Appendix C.1 and C.3, Figures 6 and 9).\n- contains an updated version of Figure 4 that fixes the data plotting error that this reviewer thankfully pointed out.\n\nWe hope that these revisions address your comments in a satisfactory way."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5214/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490626368,
                "cdate": 1700490626368,
                "tmdate": 1700490626368,
                "mdate": 1700490626368,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sgHUWMEyKU",
                "forum": "jGuXGNcK6O",
                "replyto": "vHicvsjpyJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5214/Reviewer_orQ6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5214/Reviewer_orQ6"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the response and clarifications. The evaluated partitioning scheme (where the server only has a small part of the model) and the overly strong privacy definition still lack significance to me. \n\nRe the model partitioning scheme, let me rephrase the discrepancy. The authors only evaluate settings with a **small** classifier on the server side. By contrast, the model on the server side is **large** in most (if not all) cited works, especially in Melis et al. \n\nRe the significance of the trade-off, as the authors acknowledge, the results largely result from the strong privacy definition and do not easily extend to settings with a subset of sensitive attributes. This makes the results less interesting/useful, as they only reinforce a common belief in the early work of Melis et al. below.\n> Of course, the purpose of ML is to discover new information about the data. Any useful ML model reveals something about the population from which the training data was drawn. For example, in addition to accurately classifying its inputs, a classifier model may reveal the features that characterize a given class or help construct data points that belong to this class. In this paper, we focus on inferring \"unintended\" features, i.e., properties that hold for certain subsets of the training data, but not generically for all class members.\n\nThis belief is the **whole motivation** of Melis et al., and follow-up works to focus on unintended features rather than all features. Therefore, why should we care about all features again in this paper?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5214/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500319654,
                "cdate": 1700500319654,
                "tmdate": 1700500319654,
                "mdate": 1700500319654,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z7tI1dj9yS",
                "forum": "jGuXGNcK6O",
                "replyto": "vHicvsjpyJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5214/Reviewer_orQ6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5214/Reviewer_orQ6"
                ],
                "content": {
                    "title": {
                        "value": "Additional question regarding Theorem 2"
                    },
                    "comment": {
                        "value": "Additionally, I'm puzzled by whether/how the trade-off under LPP (Theorem 2) does not hold under the conceptual representation $Z=\\arg\\max_{y}P(Y=y|X)$. Specifically, this representation does not reveal any more information than the class of the input data $X$, yet it yields Bayes optimal classification success. \n- Is it because it violates certain assumptions required for Theorem 2?\n- If so, does it imply Theorem 2 is too limited to cover representations that are similarly good, i.e., achieve Bayes-optimal classification success?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5214/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700504643649,
                "cdate": 1700504643649,
                "tmdate": 1700538084307,
                "mdate": 1700538084307,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iyfwiRm1Ta",
                "forum": "jGuXGNcK6O",
                "replyto": "vHicvsjpyJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Regarding the model partitioning scheme:** To clarify this point, we would like to distinguish between two questions  (1) In what form are data representations made available to the service provider/adversary? (2) What is the architecture of the model and how is it split into an encoding $f_E$ and classification part $f_C$?\n \nOn (1), as we describe at the start of Section 2, some works evaluate unintended feature leakage in a collaborative learning setting (Melis et al., 2019), others in  a model partitioning setting (Brown et al., 2020, Song & Shmatikov, 2020, Zhao et al., 2020 or Osia et al., 2018).\n\nMelis et al., 2019 assume that the adversary/service provider has access to _the full set of gradient updates across all layers of the model_. They state that these \"gradient updates can [...] be used to infer feature values, which are in turn based on [...] private training data\". In our notation, the data representations $Z = f_E(X)$ are hence the (noisy) feature values of a (batch of) training examples across _all layers of the model_.\nWorks such as Brown et al., 2020, Song & Shmatikov, 2020 or Zhao et al., 2020 assume that the service provider is given direct access to a record's representation at a _particular output layer of the model_. \n\nWhile we have empirically evaluated the model partitioning setting, we highlight that **our theoretical results equally apply to both settings.**\n\nOn (2), this question only becomes relevant in the model partitioning setting. Here, we decide at which layer we split a model into its encoding and classification parts. This decision determines the output of the encoder $Z = f_E(X)$, which in turn determines what information is made available to the service provider/adversary to make a prediction about $Y = f_C(Z)$ and to makes a guess about $S = g(Z)$. \n\nThe main result of our work (Theorem 2) shows that the trade-off between limiting the inference about $S$ and preserving information about $Y$ holds for _any feature mapping $f_E$_; and equally for any classifier $f_C(Z)$. That is, **these results hold independently of the size of the classification part on the server side**.\n\nTo provide evidence of this fact, in Appendix C.2.2, we added an experiment in which we used the code by Zhao et al., 2020 to reproduce our results under their model architecture and training method (maximum entropy adversarial representation learning). In Figure 8, we show that the trade-off in Theorem 2 also applies to this scenario, which has a more extensive classification part than our original experiments.\n\n\n**Regarding the assumptions and significance of the results**: The goal of this paper is to \"provide [a] formalisation of the least-privilege principle for machine learning\" (Section 1, page 2). The least-privilege concept is repeatedly mentioned in previous works, including Melis et al., as a promising avenue to solve the unintended leakage problem:\n\n\"We [...] showed that [...] defenses [...] are not effective. This should motivate future work on better defenses. Techniques that learn only the features relevant to a given task can potentially serve as the basis of \"least-privilege\" collaboratively trained models.\" (Melis et al., 2019, Section XI \"Conclusions\").\n\nEven though many works point to  the least-privilege concept as a solution, \"so far this idea has not been formalised. Our goal is to formalise this principle and characterise its feasibility.\" (Section 2, page 3). \nOur formalisation allows us to show, for the first time, that the least-privilege principle turns out to be elusive in representation learning. Our results demonstrate that, as the reviewer suspects, the assumption that it is possible to achieve least-privilege learning, \"learn only the features relevant to a given task\" (Melis et al., 2019),  is \u201ctoo strong\u201d. However, **this was not at all obvious prior to our result.** Instead, prior work repeatedly called for least-privilege learning as a promising way to solve the problem of overlearning (see, for instance, Melis et al., 2019, Brown et al., 2020). \n\nAs least-privilege learning comes with a stringent trade off, we agree with the reviewer\u2019s sentiment that the only feasible goal is to limit inference with respect to a specific set of attributes. At that point, however, we have to abandon the concept of least-privilege learning as in \u201clearning representations that contain information that is only relevant to the task itself\u201d and move towards attribute obfuscation and formalisations such as the privacy funnel (see Zhao et al., 2020, Asoodeh and Calmon, 2020 for trade-offs for these problems). How to do that and what are its fundamental limits, is simply a different problem than the one we aimed to formalise and characterise."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5214/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656623781,
                "cdate": 1700656623781,
                "tmdate": 1700657801744,
                "mdate": 1700657801744,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p1zk0KCEB5",
                "forum": "jGuXGNcK6O",
                "replyto": "vHicvsjpyJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5214/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to question about Theorem 2"
                    },
                    "comment": {
                        "value": "We agree that this instance of $Z$ can seem counter-intuitive, but it follows the trade-off just like any other feature representation. To see this, observe that this instance of $Z$ can actually reveal more information about $X$ than only the class label $Y$. It is easiest to see with an example. Consider a binary input space $\\mathbb{X} = \\{a, b\\}$, and binary label space $\\mathbb{Y} = \\{0, 1\\}$, both uniformly distributed: $P(Y = 1) = \\frac{1}{2}$ and $P(X = a) = \\frac{1}{2}$. Assume the data distribution is as follows:\n\n|   $X$   | $P(Y = 0 \\mid X)$ | $P(Y = 1 \\mid X)$ | $P(Z = 0 \\mid X)$ | $P(Z = 1 \\mid X)$ |\n|:-------:|:----------:|:----------:|:----------:|:----------:|\n|   $a$   |     0.6    |     0.4    |     1      |     0      |\n|   $b$   |     0.4    |     0.6    |     0      |     1      |\n\n\n\nConsider a full data reconstruction attack, so the sensitive attribute is $S = X$. Observe that optimal accuracy of predicting $X$ from only class label $Y$ is $P(X = \\hat X(Y)) = \\frac{1}{2} \\cdot 0.6 + \\frac{1}{2} \\cdot 0.6 = 0.6$, so 60%. At the same time, observe that $Z$ and $X$ in this case are one-to-one, as $Z$ is a deterministic function of $X$. So, accuracy of predicting $X$ from $(Z, Y)$ is $P(X = \\hat X(Z, Y)) \\geq P(X = \\hat X(Z)) = 1$, so 100%! Therefore, multiplicative leakage is quite high, $\\log \\frac{1}{0.6} \\approx 0.74$. This is because $Y$ is random, so predicting $X$ from $Y$ carries uncertainty, whereas this instance of $Z$ always reveals $X$ exactly."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5214/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657016327,
                "cdate": 1700657016327,
                "tmdate": 1700658050742,
                "mdate": 1700658050742,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GoCrhJlJRJ",
                "forum": "jGuXGNcK6O",
                "replyto": "p1zk0KCEB5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5214/Reviewer_orQ6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5214/Reviewer_orQ6"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I am not quite convinced that the trade-offs still hold under the conceptual representation example. The authors gave specific construction where releasing this representation Z leaks more information about input X than the leakage for only releasing label Y. However, this construction seems more like an artifact because the mapping between X and Z is a bijection. What if multiple values of X map to the same representation value Z (e.g., many inputs X have the same label distribution)? What if the label distributions are closer to point distributions (under which this conceptual representation's performance seems to improve and its induced information leakage decreases simultaneously)? I do not see how the current trade-off Theorem 2 could hold without change under all these cases. \n\nEven if Theorem 2 holds under this conceptual representation, this quite counterintuitive example raises additional doubts about how the leakage metric in Theorem 2 is meaningful and how serious the consequence of the proved trade-off is."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5214/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659842762,
                "cdate": 1700659842762,
                "tmdate": 1700659842762,
                "mdate": 1700659842762,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]