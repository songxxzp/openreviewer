[
    {
        "title": "Meta-Guided Diffusion Models for Zero-Shot Medical Imaging Inverse Problems"
    },
    {
        "review": {
            "id": "20fPtFeig1",
            "forum": "mbPvdO2dxb",
            "replyto": "mbPvdO2dxb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7428/Reviewer_dvv8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7428/Reviewer_dvv8"
            ],
            "content": {
                "summary": {
                    "value": "The paper titled: META-GUIDED DIFFUSION MODELS FOR ZERO-SHOT MEDICAL IMAGING INVERSE PROBLEMS presents a novel sampling strategy (MGDM) for solving inverse problems in medical imaging using diffusion models. Adopting a pre-trained unconditioned diffusion model to conform to the measurement constrain has been an open problem, though there have been quite a few attempts, its still not a well-solved problem. This work introduce an effective bi-level guidance strategy, that acts as a stronger regularizer. The authors evaluated the proposed algorithm on 2 MRI benchmarks and 1 CT benchmark, showing superior performance compared to existing SOTAs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper introduces a novel yet simple sampling strategy (MGDM) for zero-shot medical imaging inverse problems. I like the core idea of range-null space analysis and using closed-form least square to better conform to the measurements. From the results, the improvements are significant with this simple clean design. \n\n2. The paper is well-written the theory, proof, figures. The core idea is clearly delivered. I truly enjoy reading it, very comprehensive. \n\n3. The authors evaluated the proposed approach on various benchmarks (2 MRI and 1 CT), the ablation studies are well-designed."
                },
                "weaknesses": {
                    "value": "1. Miss the baseline comparisons with supervised method and [Robust compressed sensing MRI with deep generative priors]. I think the current benchmark comparisons are fair, but would curious on how MGDM compare to supervised methods [like MODL: https://arxiv.org/abs/1712.02862] and one of the earlier generative model for MRI recon using Langevin dynamics:  [Robust compressed sensing MRI with deep generative priors]. \n\n2. Regarding BraTS dataset: In general, I'm not a big fan of BraTS dataset, since its real-valued images. Therefore, one big problem is that if you perform Fourier Transform, the k-space is conjugate symmetric, and the undersampling factor is not what it is. For example, ACR = 8 effectively represents ACR = 4, this can be miss leading. Please refer and consider citing the paper: https://www.pnas.org/doi/10.1073/pnas.2117203119 Implicit data crimes: Machine learning bias arising from misuse of public data, that discussed this problem. I won't against using BraTS (its a great dataset), but should mention this issue.\n\nMeanwhile, I would appreciate more results on FastMRI dataset, I only see one visual example - Figure 5, without the undersampling pattern, and much descriptions.\n\n3. From results in Figure 2, I am amazed but also confused on the third column ACR=24, despite the effective undersampling rate, with only a few lines, I don't expect the model accurately predicting the tumor. Could you please elaborate on this results? I would like to learn more on what you think?\n\nMinor problems:\n\n1. The figures are not of high-resolution and some of them are compressed. For example, Figure 5, I can actually see jpeg artifacts in all images, which is not acceptable for imaging related papers. Please fix this problem. Also the knee orientation is up-side-down, please fix it.\n\n2. Wrong citation: In the Dataset Section, ESPIRiT is not cited correctly, should be [ESPIRiT \u2014 An Eigenvalue Approach to Autocalibrating Parallel MRI: Where SENSE meets GRAPPA], please fix it and check other citations."
                },
                "questions": {
                    "value": "1. Please provide some comprehensive explanation on why MGDM can recover the brain tumor given such a high undersample rate.\n2. Could you add some visual results for your ablation studies? I would want to see how the reconstruction results look like with different ablations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7428/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7428/Reviewer_dvv8",
                        "ICLR.cc/2024/Conference/Submission7428/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7428/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698263578475,
            "cdate": 1698263578475,
            "tmdate": 1700772846408,
            "mdate": 1700772846408,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kveQa46Nzd",
                "forum": "mbPvdO2dxb",
                "replyto": "20fPtFeig1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7428/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7428/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response of Submission7428"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their constructive comments. Below are our point-by-point responses to Weaknesses and Questions:\n\n**Weakness 1:** We acknowledge that \u201cRobust Compressed Sensing MRI\u201d is one of the first pioneer papers in the field, cited in our introduction section. Our decision not to test this method was driven by two primary reasons. Firstly, according to Chung et al. (2022) [1], the approach described in \u201cRobust Compressed Sensing MRI\u201d addresses linear inverse problems by employing an approximation $\\nabla_{x_t} \\log p_t(y|x) \\approx \\frac{A^H(y - Ax)}{\\sigma^2}$, which is valid when ${n}$ is considered Gaussian noise with variance $\\sigma^2$. However, this approximation only holds $t=0$ and fails at other noise levels relevant to the generative process. Secondly, while state-of-the-art (SOTA) methods did not benchmark against this approach, Song's method (Score-Med) [2] did include it as a baseline and outperformed it. Moreover, our results have surpassed those of Song's method.\n\nRegarding the inclusion of supervised methods, we faced uncertainty. The challenge lies in the lack of fair comparative studies, as supervised methods have different architectures and hyper-parameters. Despite that, we have now reported the quantitative comparisons with SOTA supervised methods in Appendix Table 4. Consistent with other zero-shot inverse problem solvers, such as those discussed in Chung et al. (2022) [1], Wang et al. (2022) [3], and Kawar et al. (2022) [4], our MGDM model demonstrates superiority over existing supervised methods.\n\n**Weakness 2:** We recognize your concerns about the conjugate symmetry in k-space when applying Fourier Transform to its real-valued images like BraTS, which may affect the effective undersampling factor, potentially halving the apparent ACR. I addressed this issue in my work and included a citation to the paper you have recommended, which discusses the biases that can arise from the misuse of public data. While I continue to utilize the BraTS dataset for its merits, I ensure to mention this potential pitfall for clarity and accuracy.\n\nQuoted from our limitation part: \u201cAdditionally, the BraTS dataset, employed both in our study and by the baseline methods, has been indicated in a recent paper (Shimron et al., 2022) to have an overestimated undersampling factor, which arises from the conjugate symmetry of k-space inherent in real-valued images.\u201d\n\nAdditional results for the FastMRI dataset, including undersampling patterns and detailed descriptions, are provided in the main paper and Appendix.\n\n**Weakness 3:** We first want to acknowledge that other baseline methods have also reported the capability to reconstruct MRI BraTs datasets under x24 acceleration factor. For example, in Song\u2019s paper [4], they reported a similar PSNR of 29.42; ours is slightly higher, with a PSNR of 30.04. We believe that learning informative priors can significantly aid in signal recovery. The training and testing datasets in Brats come from the same distribution, so we expect to achieve good results and effectively recover brain tumors. However, if tested on a dataset outside this distribution, our method may not perform as well, which would require additional adaptations. Our reasoning is further supported by our robust data consistency step.\n\n**Minor Problem 1:** We apologize for the oversight and thank you for bringing it to my attention. Now, all figures, including Figure 5, are provided in high resolution without compression artifacts in the revised manuscript. Additionally, the orientation of the knee images is corrected to the standard presentation.\n\n**Minor Problem 2:** The citation for ESPIRiT is now corrected, and a thorough review of all other references is conducted to ensure their accuracy. Thank you for pointing out this discrepancy.\n\n**Question 1:** Please refer to our response to Weakness 3.\n\n**Question 2:** We have now added Figure 6 to show how the reconstruction results look like with different ablations.\n\n[1] Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687, 2022a.\n\n[2] Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging with score-based generative models. arXiv preprint arXiv:2111.08005, 2021.\n\n[3] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. arXiv preprint arXiv:2212.00490, 2022.\n\n[4] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. Advances in Neural Information Processing Systems, 35:23593\u201323606, 2022."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637377711,
                "cdate": 1700637377711,
                "tmdate": 1700637377711,
                "mdate": 1700637377711,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "a1ph6RrBDn",
            "forum": "mbPvdO2dxb",
            "replyto": "mbPvdO2dxb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7428/Reviewer_TLju"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7428/Reviewer_TLju"
            ],
            "content": {
                "summary": {
                    "value": "This paper is tasked with using diffusion models to solve medical imaging inverse problems.\nIt proposes a new method based on an introduced optimization problem to derive the sampling of images from a diffusion model conditioned on measurements (typically the k-space or sinogram).\nA series of experiments is then presented to showcase the effectiveness of the method"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- the experiments are diverse and a ablation study is presented to get more insights\n- the problem tackled is interesting and in a very thriving area of research"
                },
                "weaknesses": {
                    "value": "- **Presentation**: the presentation needs to be reworked: for example one page is dedicated to introducing diffusion models, which I think is unnecessary; a few lines would suffice to introduce relevant notations and point to relevant references. There are a lot of typos which can be checked using grammarly or LTex (https://valentjn.github.io/ltex/vscode-ltex/installation-usage-vscode-ltex.html) or weird formulation (why zero-shot?). \nIn addition, there are too many confusing notations, and it's difficult to piece how they interact together even with the figure or the algorithm (which would benefit from comments): for example when the ablation study is conducted, I don't what equations the different labels mentioned refer to.\n- **Method**: the presentation of the method is very handwavy : there is no derivation of why this sampling is supposed to work even with very strong assumptions. The only \"theoretical\" grounding is proposition 3.1 which to me amounts a bit to mathiness given its simplicity. Another example is the introduction of the discrepancy gradient which is not discussed.\n- **Results**: given how good the results are, it's important to question why the improvement is so big. If I focus on knee MRI reconstruction, there is a +4dB improvement in PSNR: this is absolutely huge but it isn't discussed. In particular, it should be viewed in comparison with fully supervised methods like the ones presented in the 2019 fastmri challenge which are nowhere near the performance reported here. \n- **Code**: while some code is provided, the README has not been updated from the Wang et al. repo, which makes it difficult to know how to look at the relevant code, i.e. where the new sampling method is introduced.\n\n\nNitpicks:\nThe paper is 10 page-long rather than 9, but it's just due to a figure that slipped into the 10th page."
                },
                "questions": {
                    "value": "- How can the new sampling be derived using a bayesian formulation?\n- How can you explain the gap between this work and previous works?\n- What is (pr) i.e. proximal optimization and refinement?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7428/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698428245334,
            "cdate": 1698428245334,
            "tmdate": 1699636891541,
            "mdate": 1699636891541,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mHaLd58n7i",
                "forum": "mbPvdO2dxb",
                "replyto": "a1ph6RrBDn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7428/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7428/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response of Submission7428 (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their constructive comments. We highly invite the Reviewer to read our paper again, we have significantly improved our representation of the work, including typos, notations, concepts, and formulations.\n\nBelow are our point-by-point responses to Weaknesses and Questions:\n\n**Weakness 1 (Presentation):** We have now reworked our presentation of the paper with major changes highlighted in blue. \n\n* **Diffusion Models:** We had initially aimed to make the paper as comprehensive as possible. Following your suggestion, we have now updated that section by removing the continuous part and rendering the discrete section more concise.\n\n* **Typos:** Thank you for your feedback. We have now thoroughly checked grammar and polished our writing. If you notice any further discrepancies, please know they will be addressed in our continuous efforts to improve. \n\n* **Zero-Shot:** Zero-shot learning is one of the several learning paradigms aimed at out-of-distribution generalization, where the algorithm is trained to categorize objects or concepts that it has not been exposed to during training. In this paradigm, the set of classes during training, denoted as $\\mathcal{Y}^{train}$, is distinct and non-overlapping with the set of classes during testing, denoted as $\\mathcal{Y}^{test}$. The challenge is for the model to generalize from the training classes to the test classes, a significant leap as it must make accurate predictions for classes without having any direct prior examples to learn from. For inverse problems, where the distribution of measurements $\\mathcal{Y}$ can change based on the undersampling pattern, the term 'zero-shot solver' is sometimes applied. It also follows the precedent set by prior works that have employed this terminology for plug-and-play approaches, such as DDNM and SSD [1]. This usage is intended to reflect the solver\u2019s ability to adapt to different undersampling patterns without retraining, similar to how zero-shot learning generalizes to new classes without prior exposure.\n\n* **Confusing notation:** Regarding your concerns on the confusing notations, we've reworked our notations in all mathematical equations and algorithms to make them consistent and easy to follow. Also, we would happily address any, if you could kindly point us to specific places where confusion might arise. \n\n**Weakness 2 (Method):** \n* **Derivation:** We understand that although we did not explicitly state that our approach is derived from Bayesian principles, some terminology used in the previous introductory presentation could have led to confusion that our approach is based on Bayesian inference.  Therefore, we have decided to revise the presentation accordingly. The modified texts are now highlighted in blue in the revised manuscript and as following: \n\n  > \"To mitigate the ill-posedness, it is essential to incorporate an additional assumption based on _prior_ knowledge to constrain the space of possible solutions. In this manner, the inverse problem then can be addressed by optimizing or sampling a function that integrates this prior or regularization term with a data consistency or likelihood term {ongie2020deep}. A prevalent approach for prior imposition is to employ pre-trained deep generative models {bora2017compressed, jalal2021robust}.\"\n\n* **Theory:** We respectfully disagree with the reviewer\u2019s sentiment of the theoretical foundation in our work and we find their characterization of our results to be, rather, unfair. We have designed an algorithm that is to a great extend theoretically motivated. Proposition 3.1, as the name suggest, only small piece of our work. Another important part of our work is the practical side, which clearly shows that our findings and analysis are on the right track.\n\n* **Discrepancy Gradient step:**\nRegarding, discrepancy gradient step, we added some information in the paper and our reasoning.\nDiscrepancy gradient guides $\\mathbf{x}_{t-1}$ towards an equilibrium between two values x_hat_0|t and x_tilde_0|t. This step serves to add additional corrections based on the specific characteristics of the problem at hand, such as incorporating more detailed information from the measurement $\\mathbf{y}$ or adjusting for errors of approximations made in previous steps. This adjustment can mitigate the effects of any errors introduced in earlier steps, thus ensuring a more consistent and robust convergence toward an optimal solution. Empirical evidence suggests that this step can moderately enhance the reconstruction results. We have now explained this clearly in the text.\n\n[1] Gongye Liu et al. Accelerating Diffusion Models for Inverse Problems through Shortcut Sampling. 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644525389,
                "cdate": 1700644525389,
                "tmdate": 1700644549329,
                "mdate": 1700644549329,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3Pi39hMITP",
                "forum": "mbPvdO2dxb",
                "replyto": "yP7H3T4EAM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7428/Reviewer_TLju"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7428/Reviewer_TLju"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for engaging in the discussion respectfully.\n\nI think the 2 main weaknesses are not satisfactorily addressed:\n- the derivation of the algorithm is still handwavy. Typically, for eq (13) it says in the revised paper:\n> . In practice, we assume that $p_t(y|x_t) \\approx \\mathcal{N}(y; Ax_{0|t},\\sigma^2_t, I)$, and then an approximation of the\nexpectation in Eq. (12) is : $\\tilde{x}_{0|t} \\approx \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} [x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_\\theta (x_t, t) \\zeta \\nabla_{x_t} \\|y - Ax_{0|t}\\|_2^2]$ \n\nWhy do make this assumption? What exactly is this assumption : $\\approx$ could mean several different things and it's not clear what here (and could be different in the 2 equations)? What are the assumptions needed for this result to hold? Are they relevant/realistic?\nSimilarly when justifying the use of the discrepancy gradient, it says in the revised paper:\n> This adjustment can mitigate the effects of any errors introduced in earlier steps, thus ensuring a more consistent and robust convergence toward an optimal solution.\n\nBut there is no characterization of said errors, and how in principle this discrepancy gradient could mitigate that formally and under which assumptions.\n\nTo me these are all huge problems, and this comes in addition to what I again think is a \"mathy\" proposition 3.1, which I understand is not the core of the paper. Still the theoretical foundations for such a work claiming extraordinary empirical results need to be very clear.\n\n- The numbers reported by Chung and Ye are the ones reported in Table 4. Those numbers are overly pessimistic when it comes to comparing against fully-supervised methods. Indeed they specify in this paper: \"We use the official implementation5\n, with 4 recurrent blocks and default parameters [...] For all the deep learning comparison studies, we train the network with Gaussian 1D random sampling masks.\". This means they used parameters that were not tailored to this specific mask, which is unrealistic (it's much better to use a cartesian mask to be closer to clinical practice). The fact that retraining happened makes also the reported numbers much less valid. The ideal experiment would be to test these diffusion methods against fully-supervised methods pre-trained using their actual masking/undersampling methods. \nFinally, the numbers reported in Table 4 are not those of DuDorNet as mentionned, but those of E2E-VarNet which is multi-coil and not single-coil as what is done in this paper.\nI think it would generally be very surprising if for a given problem setting (i.e. fixed modality and undersampling pattern), an unsupervised method using the same amount of data as a supervised method performed better. Here the fully-supervised method can adapt to the specific aliasing patterns in the data. \nStill, even not considering fully-supervised methods, the gap between the introduced method and the previous state of the art is so huge it needs a better in-depth explanation as to the failure modes of the previous methods and why this new one succeeds.\n\n\nAs another point, code is still not ready for review. I couldn't find a clear explanation of how to reproduce the results."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661390359,
                "cdate": 1700661390359,
                "tmdate": 1700661390359,
                "mdate": 1700661390359,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qnLYDy9637",
                "forum": "mbPvdO2dxb",
                "replyto": "a1ph6RrBDn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7428/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7428/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response on comparison with supervised approach"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe appreciate your feedback and would like to first provide some response about your concern on comparison with supervised approach:\n\n**Sampling Mask Clarification:** We would like to clarify that the term 'Gaussian 1D random sampling mask' actually refers to a 1D Cartesian mask. We want to make sure the Reviewer has not confused this with 'Gaussian 2D or Possoin Disk mask', which are commonly used for accelerating 3D MRI. The current Gaussian 1D random sampling mask features a full-acquisition in the readout dimension and pseudo-randomly skipped sampling steps in the phase encoding dimension. Importantly, this approach aligns with clinical practice, as many 2D MRI acquisitions employ similar 1D randomly-distributed sampling masks for compressed-sensing-based image acceleration. It's worth noting that this 'random' 1D Cartesian sampling mask pattern is applied deterministically during the pulse sequence design. We want to highlight that our last author possesses a substantial background in MRI, encompassing pulse sequence design and image reconstruction, ensuring the clinical relevance of our work.\n\n**Comparison with Fully-Supervised Methods:** We acknowledge your point about the potential disadvantage of the 'random' sampling mask approach in the context of training the supervised methods. Since training data containing a mix of different sampling mask effects can be challenging for supervised approaches. Although we believe that the Gaussian 1D distribution underpinning these masks may mitigate the mixture effect to some extent, as these masks are very similar to some degree. However, conducting a comprehensive comparison against fully-supervised methods falls beyond the scope of our current study. Implementing various state-of-the-art supervised methods within our revision timeline is a complex undertaking. Hence, we opted to refer to previous publications and quote relevant numbers. We have made efforts to adjust and clarify these comparisons in Appendix Table 4 to highlight the disadvantages that the 'Gaussian 1D random sampling mask' presents to supervised learning approaches. If you strongly believe that the supervised performance reported in the Chung and Ye paper is biased, we are open to removing Table 4 from our Appendix. After reevaluating the tables in the Chung and Ye paper, we can confirm that we are reporting the DuDorNet numbers, not those of E2E-VarNet. We kindly request your review of this clarification.\n\nAs mentioned in our response to 'Weakness 3 (Results),' we have rectified the numbers we previously misreported. The corrected results indicate an improvement of approximately 1dB over the state-of-the-art, which we believe represents a reasonable advancement. We apologize for any confusion arising from our previous use of a different sampling mask.\n\nThank you for your continued feedback and consideration. We are currently working on writing down the Equations deriving E. 13 from Eq 12, which will be posted soon."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708914436,
                "cdate": 1700708914436,
                "tmdate": 1700711380106,
                "mdate": 1700711380106,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DSRWagXRKH",
                "forum": "mbPvdO2dxb",
                "replyto": "qnLYDy9637",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7428/Reviewer_TLju"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7428/Reviewer_TLju"
                ],
                "content": {
                    "comment": {
                        "value": "**Comparison with fully-supervised**: I was wrong on the sampling mask indeed. I was also wrong on the reported results and for this I want to apologize especially since these mistakes happen so late in the discussion period.\n\nI think I was confused when it comes to the sampling mask by both the denomination and the fact that Chung and Ye retrained the network: if the same sampling mask is used, then retraining the network shouldn't be necessary, except for different data, in which case the argument for bad hyperparameter setting holds. \nI agree that conducting a full comparison is beyond the scope of this study, nonetheless, it's worth noting the following discrepancy: how can unsupervised models (in the sense that they don't know the undersampling mechanism) with access to the same amount of data and compute, outperform supervised models (in the sense that they are aware of the undersampling mechanism)? This to me indicates rather a problem in evaluation than anything else.\n\n**Results update**: the fact that the results changed from a margin of 4dB to now a margin of 1dB is a red flag: this means that there are problems in the evaluation, and the fact that code was so difficult to put together in a state where it's reviewable is another evidence of that. I think because the empirical results have so substantially changed, this paper needs in any case to undergo another round of review. I also think it's important to highlight this change in the results in the revision in blue, like what was done for the rest of the paper."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730893379,
                "cdate": 1700730893379,
                "tmdate": 1700730893379,
                "mdate": 1700730893379,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q80xdkuotD",
                "forum": "mbPvdO2dxb",
                "replyto": "ggCUPZF6sV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7428/Reviewer_TLju"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7428/Reviewer_TLju"
                ],
                "content": {
                    "comment": {
                        "value": "**Equations derivation**: the equation derivation in Appendix 4.2. still uses $\\approx$ without specifying when the approximation becomes true, i.e. under which assumptions, and whether these assumptions are close to something reasonable.\nI want to highlight that it is not the only problem when it comes to mathematical derivations in the paper and it was more of an example. I think the whole theoretical section needs to be reworked in that sense.\n\n**Discrepancy gradient**: I think it's fair to have empirical findings and they should just be described as such.\nHowever, I see that most of the discussion at the moment happens in the appendix, which is also a problem for the paper: limitations and discussion should be put forth in the main text."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731183776,
                "cdate": 1700731183776,
                "tmdate": 1700731183776,
                "mdate": 1700731183776,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w0lTKSJ7Qu",
                "forum": "mbPvdO2dxb",
                "replyto": "Efu2CtN1dH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7428/Reviewer_TLju"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7428/Reviewer_TLju"
                ],
                "content": {
                    "comment": {
                        "value": "Regarding code, I think it's normal to have a size limit in the ICLR submission.\nFor the next round of revision or for the next paper, I invite the authors to follow the guidelines written here: https://nips.cc/Conferences/2020/PaperInformation/CodeSubmissionPolicy\n\nBasically, source code should be zipped along with small datasets, but large datasets should just be downloaded from the internet."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731510880,
                "cdate": 1700731510880,
                "tmdate": 1700731510880,
                "mdate": 1700731510880,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zBwTpzBQAf",
                "forum": "mbPvdO2dxb",
                "replyto": "w0lTKSJ7Qu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7428/Reviewer_TLju"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7428/Reviewer_TLju"
                ],
                "content": {
                    "comment": {
                        "value": "I tried running the code just now, and I have too many errors to fix while just setting up the environment: I think the instructions for the setup have bugs (for example there are references to local files in the `requirements.txt`, some packages like the `mkl` ones are not available through `pip`)."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732698497,
                "cdate": 1700732698497,
                "tmdate": 1700732698497,
                "mdate": 1700732698497,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0wwTs92bHN",
                "forum": "mbPvdO2dxb",
                "replyto": "4S72pixS2u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7428/Reviewer_TLju"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7428/Reviewer_TLju"
                ],
                "content": {
                    "comment": {
                        "value": "I think my main point at this stage in the discussion is the following: there were too many problems with evaluation in the original version, the results have therefore changed (i.e. the ones reported for Wang et al. in Table 1, that had not been previously reported in the literature if I understand correctly) and therefore I think the paper needs to go through another round of reviews.\n\nFurthermore, the theoretical part needs to be rewritten to adhere to the standards of the community in terms of rigor for the derivations."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734925434,
                "cdate": 1700734925434,
                "tmdate": 1700734925434,
                "mdate": 1700734925434,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pSIWObQvvC",
                "forum": "mbPvdO2dxb",
                "replyto": "a1ph6RrBDn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7428/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7428/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official response"
                    },
                    "comment": {
                        "value": "We are grateful for the Reviewer's constructive feedback regarding the presentation and clarity of our equations. We acknowledge that some of the derivation steps may not have been immediately clear, especially to readers who are not intimately familiar with the foundational works of DPS (Chung et al., 2022a) [1] and Ravula et al. (2023) [2].\n\nIn response to your critique, we have refined these derivations to enhance their readability and accessibility. This includes incorporating relevant assumptions directly following each equation in the Appendix 4.2, rather than grouping them together in the main document prior to Equation 13. We believe this approach has provided a clearer and more intuitive understanding of our methodology. \n\nThe only assumption made here is measurement noise is Gaussian, which is a standard assumption in most MRI simulation studies. All other \\simeq are approximations derived in DPS (Chung et al., 2022a) [1] and Ravula et al. (2023) [2]. These are now clearly explained together with the Equations both in the main document and in Appendix."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735193086,
                "cdate": 1700735193086,
                "tmdate": 1700741731089,
                "mdate": 1700741731089,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JIxlwPqn8Y",
            "forum": "mbPvdO2dxb",
            "replyto": "mbPvdO2dxb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7428/Reviewer_Bg7d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7428/Reviewer_Bg7d"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new method for inverse problem in medical imaging. The goal is to apply Diffusion Models in medical imaging to produce high-quality images using incomplete and noisy measurements, aiming to reduce costs and risks to patients. To this end, a model named Meta-Guided Diffusion Model (MGDM) is introduced to address the challenge of guiding unconditional predictions to align with measurement information through a bi-level guidance strategy (an outer level and an inner level ). The outer level optimizes for measurement consistency, while the inner level approximates the measurement-conditioned posterior mean as the initial prediction. Empirical results on medical datasets in MRI and CT demonstrate that MGDM outperforms existing methods by generating high-fidelity medical images that closely match measurements and reduce the occurrence of hallucinatory images."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-  This paper provides an effective strategy for addressing medical imaging inverse problems in a zero-shot setting.\n-  Empirical results show a clear improvement, consistently overcoming the state-of-the-art benchmarks, and exhibiting robustness across diverse acceleration rates, projection counts, and anatomical variation."
                },
                "weaknesses": {
                    "value": "- Seems the method is a combination of DPS (Chung et al., 2022a) and DDNM (Wang et al., 2022), which somehow limits the contribution of this paper.\n- The 3D volumes are divided into 2D slices. How can the method ensure the consistency of the volume from other views, like sigital and coronal?"
                },
                "questions": {
                    "value": "- What do you mean by 'a novel class of \"fully\" probabilistic Deep Learning Models (DLMs)'?\n- Seems in section 2.1 that there is no need to include both Discrete-time formulation and Continuous-time formulation, just introduce the one related to this paper.\n- the proximity term penalizes deviations from the initial estimate, which seems like the data consistency in MRI acceleration?\n- Is the pre-trained Guided diffusion model (from natural images) used here or the model is trained from scratch?\n- Why did not test on the brain data from fastMRI? Can we share the same model for BraTS and brain from fastMRI?\n- I am not quite sure what the mentioned 'zero-shot setting' is? For MRI accereration, we do have the fully sampled k-sapce data as the reference to train the neural network. What is the 'zero-shot setting' here?\n- Can we share the model for different acceleration rates?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7428/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7428/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7428/Reviewer_Bg7d"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7428/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775967810,
            "cdate": 1698775967810,
            "tmdate": 1699636891423,
            "mdate": 1699636891423,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hFFujCcAgH",
                "forum": "mbPvdO2dxb",
                "replyto": "JIxlwPqn8Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7428/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7428/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response of Submission7428 (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their constructive comments. Below are our point-by-point responses to Weaknesses and Questions:\n\n**Weakness 1:** \n\nWe respectfully disagree with the comment that our proposed method \u201cis a combination of DPS and DDNM\u201d. DDNM is given by the expression $$ \n\\hat{\\mathbf{x}} = {\\mathcal{A}^{\\dagger}\\mathbf{y}}+{(\\mathbf{I}-\\mathcal{A}^{\\dagger}\\mathcal{A})}{\\bar{\\mathbf{x}}}\n$$, representing a direct analytical approach, which utilizes a straightforward formula involving matrix operations. The solution is a constructive combination of the least-squares fit and the null space component, aiming to be as faithful to the data measurement as possible while respecting the initial prediction. This is a deterministic and direct method, which clearly follows the path dictated by the data and initial conditions.\n\nOn the other hand, MGDM has a closed-form solution derived from balancing two terms in the objective function. \n$$\n\\hat{\\mathbf{x}}_{t} = \\underset{\\mathbf{x}}{\\mathrm{argmin}}  \\frac{1}{2} ||\\mathbf{y} -\\mathcal{A}\\mathbf{x}||_2^2 + \\frac{\\lambda}{2} ||\\mathbf{x}-\\mathbf{x}_0t||_2^2\n$$\n\n MGDM comes from the closed-form of the optimization problem, reflecting an optimization-based approach. It aims to find the best route that respects closeness to the path. The closed-form solution embodies a compromise that includes fitting the model to the data and adhering to a prior reference x0|t, which is introduced through the regularization term \u03bb. The solution represents an equilibrium between minimizing the least-squares error and the regularization term, even when expressed in closed form. This method represents an iterative approach to balance, between fitting the data and incorporating prior knowledge, reflecting a philosophical stance even in the presence of a closed-form solution.\n\nSecondly, our MGDM method differs from the DPS method's purpose and application of gradient-based updates. While both methods involve gradient-based updates, they serve distinct roles. In Algorithm 2 (step 8) of our MGDM, the gradient-descent update is used to refine our clean image initial prediction with the measurement y as a conditioning factor (i.e., approximated measurement-conditioned posterior mean). This contrasts DPS, where gradient-based updates are applied to the noisy image after each reverse sampling process.\n\nIn response to your valuable feedback, we have revised our text to ensure that these distinctions between our method, DDNM, and DPS are clearer.  \n\n \n**Weakness 2:** We appreciate the Reviewer's inquiry and understand the confusion regarding the differences in acquisition modes, particularly from a non-medical-imaging perspective. For instance, in MRI, acquiring only a few 2D slices with anatomical gaps between them is common practice to save scan time. In such cases, stacking them into a continuous 3D volume is not feasible.\n\nIn our approach, we focus on reconstructing 2D images from individual 2D slices obtained from multi-slice 2D acquisitions. Each slice is treated as an independent entity, and our primary goal is to reconstruct the best possible 2D image from the raw 2D slice measurements. This is consistent with other SOTA methods we are comparing against in our paper.\n\nHowever, suppose one decides to acquire multi-slice 2D images covering the 3D slab without gaps between the slices, as the Reviewer suspected. In that case, potential discontinuities may indeed arise when stacking these 2D images to form a 3D volume in other views, such as coronal or sagittal. It's important to note that these discontinuities are inherent limitations of multi-slice 2D acquisitions and are not specific to our reconstruction methods.\n\nTo address this limitation and ensure smoother transitions in the 3D volume, one viable solution is presented in a recent paper [1]. This approach introduces Total Variation regularization in the Z-dimension, effectively enforcing a smooth transition between slices in the 3D volume.\n\nWe highly value this valuable feedback and have revised our text to emphasize the distinction between multi-slice 2D acquisitions and full-slab 3D acquisitions. Additionally, we have referenced the mentioned paper to provide readers with further insights into addressing the challenges associated with 3D reconstruction from 2D diffusion models.\n\n[1] Lee, Suhyeon, et al. \"Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models.\" arXiv preprint arXiv:2303.08440 (2023)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639866991,
                "cdate": 1700639866991,
                "tmdate": 1700639866991,
                "mdate": 1700639866991,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cRb9DWZdd5",
                "forum": "mbPvdO2dxb",
                "replyto": "hFFujCcAgH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7428/Reviewer_Bg7d"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7428/Reviewer_Bg7d"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the explanation and response. I would like to keep my score, also based on comments from other reviewers."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691199937,
                "cdate": 1700691199937,
                "tmdate": 1700691199937,
                "mdate": 1700691199937,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1pVh7gsaNy",
            "forum": "mbPvdO2dxb",
            "replyto": "mbPvdO2dxb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7428/Reviewer_sAeb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7428/Reviewer_sAeb"
            ],
            "content": {
                "summary": {
                    "value": "Sparse-data reconstruction in CT and MR imaging is modeled as an ill-posed linear inverse problems subject to noise.\nSimilar to the DDNM (Denosing Diffusion Null-space models) algorithm for MRI reconstruction, the authors use diffusion denoising,\nbut replace the backprojections in DDNM with a bi-level MGDM approach relying upon a regularized outer objective and an inner expectation approximation.\nSimulated comparisons with competing approaches demonstrate the efficacy of the proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors demonstrate improvements in terms of pSNR and SSIM (an image quality metric) over competing algorithms for both fastMRI datasets via simulation and for CT simulations using real LIDC CT reconstructions as digital phantoms.\n\n2. The ablation study in Sec. 4.4 and Table 3 clearly demonstrates that the regularized proximal optimization plays the most substantial role in the proposed MGDM method."
                },
                "weaknesses": {
                    "value": "1. Two extra parameters $\\zeta$ and $\\rho$ are introduced in the proposed Algorithm 2 (MGDM sampling) in comparison to Algorithm 1 (DDNM sampling).\nIt is clear not how much of the improvements over DDNM were obtained by painstakingly tuning these two new parameters.\n\n2.\nWhile a slice-wise 2D imaging simulation is appropriate for demonstrating the practical efficacy for MR reconstruction and\nthe MRI simulation, as in the fastMRI paper, appears to be realistic, the 1989 ESPIRIT reference for estimate the parallel multi-coil sensitivities appears to be incorrect.\nThe cited 1989 paper doesn't consider any special considerations for the parallel MR imaging problem and the correct reference appears to be:\nESPIRiT \u2014 An Eigenvalue Approach to Autocalibrating Parallel MRI: Where SENSE meets GRAPPA, by Uecker, et al., 2014.\nThis is the reference from the fastMRI paper by Zbontar et al.\n\n3.\nIn the case of CT imaging, 3D cone-beam CT or helical CT are the common practical data acquisition techniques and 2-D simulations are not particularly convincing.\nPlease refer to the following paper for a reasonable 3-D simulation as well as results on real sinogram data:\nKim, Donghwan, Sathish Ramani, and Jeffrey A. Fessler. \"Combining ordered subsets and momentum for accelerated X-ray CT image reconstruction.\" IEEE transactions on medical imaging 34, no. 1 (2014): 167-178.\nIt may be acceptable that the authors do not simulate practically important effects such as beam hardening, but a 2-D simulation and an FBP baseline can be misleading.\nIt is also common practice in CT reconstruction to use an anthropomorphic digital phantom during simulation in order to uncover reconstruction artifacts that might get hidden\nwhen using a reconstructed CT image, itself full of noise and other artifacts.\n\n4.\nSome typos exist in the paper, e.g. \"fidility\" at the bottom of page 5."
                },
                "questions": {
                    "value": "Please note the inherent question underlying weakness 1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7428/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819374978,
            "cdate": 1698819374978,
            "tmdate": 1699636891258,
            "mdate": 1699636891258,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fHzt1XV2jA",
                "forum": "mbPvdO2dxb",
                "replyto": "1pVh7gsaNy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7428/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7428/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response of Submission7428"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for their constructive comments. Below are our point-by-point responses to Weaknesses and Questions:\n\n**Weakness 1:** We appreciate the Reviewer's observation regarding introducing two additional parameters, \u03b6 and \u03c1, in our algorithm (MGDM sampling) compared to DDNM. In response to this concern, we have conducted additional experiments to investigate the impact of these parameters on the algorithm's performance. Our findings indicate the following: (a) Once the parameters \u03b6 and \u03c1 are appropriately tuned within specific ranges, the performance of our MGDM becomes stable, making the parameter tuning process easier. (b) Moreover, we have observed that the parameters once tuned on a small number of specific datasets, exhibit the ability to generalize well to unseen datasets. This suggests that it is not always necessary to perform parameter tuning for each subject, as the tuned values can provide satisfactory performance across different datasets. It's worth noting that hyperparameter tuning is a universal challenge in machine learning and not unique to our method. Our revised manuscript provides detailed insights into our parameter selection process, allowing readers to replicate it in their experiments. Moreover, our ablation study of Table 3 showed that even without the two additional steps involving \u03b6 and \u03c1 (i.e., Ours_{no_ir}), our new proximal projection step alone can already marginally outperform DDNM. \n\n**Weakness 2:** We appreciate the Reviewer's keen attention to detail. We acknowledge the error in our citation of the 'ESPIRIT' paper and apologize for this oversight. We have promptly replaced it with the correct reference, the paper by Zbontar et al. Furthermore, we have thoroughly reviewed all other references to ensure their accuracy.\n\n**Weakness 3:** We appreciate the Reviewer's feedback regarding the limitations of our 2D digital simulation compared to practical 3D cone-beam or helical CT data acquisitions. It's important to clarify that our choice of 2D simulation using parallel-beam geometry aligns with the methodology adopted by several existing methods, including diffusion models such as MCG [1], SIN-4c-PRN [2], and ScoreMed [3]. This allows for a direct comparison of algorithm performances.\n\nWe acknowledge the limitations of this simplified 2D simulation and the absence of practical effects like beam hardening. To address these concerns more comprehensively, we have referenced the paper 'Combining ordered subsets and momentum for accelerated X-ray CT image reconstruction' for further discussion, as suggested by the Reviewer. In light of the short timeframe for revision, we understand that a 3D realistic CT phantom simulation is a valuable avenue for future work, and we appreciate the Reviewer's input in this regard.\n\nQuoted from our limitation part: \u201cIt should be noted that our CT simulation adheres to the 2D parallel beam geometry assumption, aligning with the baseline models used in other studies for direct comparison. This differs from the more complex and realistic 3D cone-beam CT or helical CT simulations (Kim et al., 2014).\u201d\n\n**Weakness 4:** Thank you for pointing out the typographical errors in our manuscript. We have carefully reviewed the document and corrected the typo \"fidility\" to \"fidelity\" at the bottom of page 5, along with a thorough check for any additional typos throughout the text.\n\n\n[1] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for inverse problems using manifold constraints. Advances in Neural Information Processing Systems, 35:25683\u201325696, 2022b.\n\n[2] Haoyu Wei, Florian Schiffers, Tobias Wu \u0308rfl, Daming Shen, Daniel Kim, Aggelos K Katsaggelos, and Oliver Cossairt. 2-step sparse-view ct reconstruction with a domain-specific perceptual net- work. arXiv preprint arXiv:2012.04743, 2020.\n\n[3] Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging with score-based generative models. arXiv preprint arXiv:2111.08005, 2021."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634563825,
                "cdate": 1700634563825,
                "tmdate": 1700634563825,
                "mdate": 1700634563825,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tkpfR6PtXt",
                "forum": "mbPvdO2dxb",
                "replyto": "fHzt1XV2jA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7428/Reviewer_sAeb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7428/Reviewer_sAeb"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your rebuttal !"
                    },
                    "comment": {
                        "value": "I wish to thank the authors for their detailed response to my comments as well as those from other reviewers.\nI especially appreciate the additional experiments regarding the tuning of additional hyper-parameters \\zeta and \\rho. Those additional results are not very surprising.\nI would like to retain my original (positive) rating, although the concerns raised by one reviewer regarding the code and reproducibility of the results do appear to be valid."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706783406,
                "cdate": 1700706783406,
                "tmdate": 1700706783406,
                "mdate": 1700706783406,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]