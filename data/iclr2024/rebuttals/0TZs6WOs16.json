[
    {
        "title": "Hyperbolic Embeddings in Sequential Self-Attention for Improved Next-Item Recommendations"
    },
    {
        "review": {
            "id": "KhL24ud2Ze",
            "forum": "0TZs6WOs16",
            "replyto": "0TZs6WOs16",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7960/Reviewer_hnY1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7960/Reviewer_hnY1"
            ],
            "content": {
                "summary": {
                    "value": "The paper makes the following main contributions:\n- The paper proposes a new approach for the next-item recommendation that combines sequential self-attention with hyperbolic geometry. The base architecture is SASRec, with modifications only to the final prediction layer.\n\n- The prediction layer is adapted to learn how to separate hyperplanes in the Poincar\u00e9 ball, enabling a linear classifier in this non-linear hyperbolic space. This allows the model to leverage the benefits of hyperbolic geometry, like hierarchical representations and dimensionality reduction.\n\n- An approach to estimate the hyperbolicity of datasets using Gromov delta-hyperbolicity is presented. Datasets are categorized as \"good\" or \"bad\" for hyperbolic modeling based on this."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed approach is straightforward to implement, requiring only changes to the prediction layer of SASRec. This makes adoption more practical.\n\nAnalysis of dataset hyperbolicity provides insights into when these models can be expected to work well or not. The categorization into \"good\" vs \"bad\" datasets is useful."
                },
                "weaknesses": {
                    "value": "There is no ablation study on the effects of different space curvature values. Varying the curvature and linking performance to estimated dataset hyperbolicity could provide better insights.\n\nThe negative sampling analysis seems incomplete. Different sampling strategies besides uniform should be evaluated before concluding their effects.\n\nThe approach for estimating dataset hyperbolicity lacks analysis of computational complexity and scalability. This could limit practical applications."
                },
                "questions": {
                    "value": "Please solve the weakness listed above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7960/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785279076,
            "cdate": 1698785279076,
            "tmdate": 1699636978866,
            "mdate": 1699636978866,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8mHEsYp6z6",
                "forum": "0TZs6WOs16",
                "replyto": "KhL24ud2Ze",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7960/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7960/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hnY1"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to thoroughly assess our work and provide valuable feedback. Below we address the concerns related to the listed weakness points. We quote the original point and then provide the details that should address it. We will separate the points into several subsequent comments due to the character limit per single comment.\n\n### Point 1\n> There is no ablation study on the effects of different space curvature values. Varying the curvature and linking performance to estimated dataset hyperbolicity could provide better insights.\n\nWhile we do not dedicate a separate section to such analysis, we do show the effects of varying the space curvature on the performance. It is depicted in Figure 4. The graph on the left side illustrates the relationship between curvature values and machine precision. It demonstrates that the estimated space curvature values exhibit a considerable range of variation across different machine precision settings. The right part of the graph links the performance of the hyperbolic model to the same machine precision settings. Combining both parts indicates the following trend: higher precision aligns with lower values of $c$ and subsequently yields improved recommendation quality. While this figure was meant to demonstrate the importance of accurately estimating the values of $c$ (which is one of our contributions), it also demonstrates that overestimating these values may degrade the performance and provides some insights into the overall performance of hyperbolic models on different datasets.\n\nIn this context, it is also important to note that some prior works, e.g., Khrulkov et al. (2020), did not use the estimated values of $c$ in the final experiments, as they found lower values to provide better results (see the last paragraph of Section 3.1 in their work). According to our calculations, using a better estimation procedure proposed in our work would provide a much better estimate of $c$ that would be much closer to the optimal one (which is an order of magnitude lower) used by the authors in their final experiments."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7960/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502464659,
                "cdate": 1700502464659,
                "tmdate": 1700502464659,
                "mdate": 1700502464659,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "N1gZ4Qbd2y",
            "forum": "0TZs6WOs16",
            "replyto": "0TZs6WOs16",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7960/Reviewer_WdM2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7960/Reviewer_WdM2"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends SASRec [1] in hyperbolic space by adjusting the output prediction layer for next-item recommendation task. \n\n[1] Self-attentive sequential recommendation. ICDM 2018.\n[2] HME: A Hyperbolic Metric Embedding Approach for Next-POI Recommendation. SIGIR 2020."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n\n-\tThe approach is straightforward and well-explained\n-\tThe writing is clear and on point\n-\tThe authors conducted experiments on various datasets for benchmarking"
                },
                "weaknesses": {
                    "value": "Weaknesses:\n\n-\tIn my opinion, the contribution is marginal while the only change is the prediction layer (Eqn (6))\n-\tIn Section 4.1, the authors mentioned that \u201cwe limit the allowed embedding size values to (32, 64, 128), while the Euclidean models explore higher values from (256, 512, 728)\u201d. More deeper analyses would make the paper stronger\n-\tIn Section 6.2, Table 1 shows that hyperbolic based solutions do not always show remarkable performance. It would be better if the authors also dive deeper into the details of the datasets / models, and evaluate about which scenarios can make hyperbolic based methods perform the best.\n-\tAblation studies should be further given. For example, can we generate visualization for user / item embeddings to observe the \u2018before\u2019 and \u2018after\u2019 changing the prediction layer? \n-\tMissing citations / baselines such as [2]\n\nOverall, more works need to be done.\n\n[1] Self-attentive sequential recommendation. ICDM 2018.\n[2] HME: A Hyperbolic Metric Embedding Approach for Next-POI Recommendation. SIGIR 2020."
                },
                "questions": {
                    "value": "Please see my comments above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7960/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698891078873,
            "cdate": 1698891078873,
            "tmdate": 1699636978762,
            "mdate": 1699636978762,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E4kPSSSNid",
                "forum": "0TZs6WOs16",
                "replyto": "N1gZ4Qbd2y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7960/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7960/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WdM2"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's dedicated efforts in evaluating our work and providing actionable suggestions and feedback. We will address the concerns by quoting them and expanding the explanation below the quote. Due to space constraints, our response will be split into several comments below.\n\n### Point 1\n> In my opinion, the contribution is marginal while the only change is the prediction layer (Eqn (6))\n\nWhile we do understand this concern, we respectfully emphasize that the final results are easier to judge once they're obtained. However, even seemingly simple solutions do not necessarily have an easy path to them. In designing our approach, we experimented with several very different architectures, none of which resulted in any improvement, expect the one presented in the paper.  The possibility to boost models performance be simply replacing the final layer with a linear classifier in a non-linear space provides a straightforward practical advantage. However, this is not the only contribution.\n\nWe also provide several insights on the conceptual side. We highlight the non-trivial aspects of understanding and modeling the geometry of data, which remain underexplored in the machine learning literature. We showcase the compatibility issue of data with the hyperbolic space geometry and propose a new view on the problem of estimating the curvature of the hyperbolic space. The latter may become helpful in early indication of possible compatibility issues, replacing the need for running expensive experiments. We also expose the non-trivial effects related to the negative sampling in the case of hyperbolic models, which leads to a significant degradation of quality even with respect to the baseline Euclidean model (SASRec). To the best of our knowledge, such effects remain unknown to the community. We believe it layouts a new direction of research that may spark new theoretical results and lead to better understanding of the limitations and applicability of geometric approaches to real-world problems.\n\n### Point 2\n> In Section 4.1, the authors mentioned that \u201cwe limit the allowed embedding size values to (32, 64, 128), while the Euclidean models explore higher values from (256, 512, 728)\u201d. More deeper analyses would make the paper stronger\n\nWe understand that the choice of ranges of the embedding size may seem non-standard for the typical experiments in the Euclidean space. However, one of the main promises of the Hyperbolic space is its higher representation capacity, starting from the known theoretical fact that any tree can be embedded into the two-dimensional hyperbolic space (Poincar\u00e9 disk). While this theoretical bound is hard to achieve in pratice due to hardware's finite arithmetic precision, one may still expect to see a representation capacity improvement (lower embedding sizes) in the hyperbolic models over their Euclidean counterparts. We, however, show in our work that it only happens if data is geometrically-compatible. The choice of the reduced embedding sizes helps demonstrating that effect. If we lift the restriction on the smaller embedding size, then even on \"bad\" datasets the hyperbolic models can be trivially made to perform at the same level as their Euclidean counterparts. It simply takes setting higher values of $\\delta$ (lower curvature $c$ respectively) -- in this case the hyperbolic models become practically indistinguishable from the Euclidean ones. This result, however, would prevent us from demonstrating the true behavior of hyperbolic models on problematic datasets, while making the formal results (quality metrics) look much better in comparison. We believe that this result would be too trivial to show and instead focus on the discrepancies in learning abilities depending on the geometric properties of data. We believe that our approach offers a more equitable perspective, helping to more effectively unveil the underlying intricacies associated with training hyperbolic models."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7960/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509837968,
                "cdate": 1700509837968,
                "tmdate": 1700509837968,
                "mdate": 1700509837968,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TyQiulW8uK",
            "forum": "0TZs6WOs16",
            "replyto": "0TZs6WOs16",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7960/Reviewer_9PVb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7960/Reviewer_9PVb"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a hyperbolic architecture for sequential self-attention next-item recommendation applied to SASRec. Specifically, they replace the output prediction layer in SASRec with predictors in hyperbolic space. Besides, they adjust the machine precision setting to obtain a more accurate estimation of hyperbolic space curvature. Experimental results demonstrate that HSASRecCE can outperform SASRec with small embeddings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper is the first work to extend the sequential self-attention next-item recommendation architecture by the hyperbolic prediction output layer.\n2. The paper reveals that negative sampling harms the performance of hyperbolic models.\n3. The paper adjusts the machine precision setting to obtain a more accurate estimation of hyperbolic space curvature, which can measure the hyperbolicity of a dataset and improve the performance of the hyperbolic model."
                },
                "weaknesses": {
                    "value": "1. Limited novelty. The work simply replaces the output layer of SASRec with hyperbolic prediction layers. Besides, the used hyperbolic prediction layers(hyperbolic hyperplane and MLR) have been widely applied in other works. The contribution of this article needs to be re-condensed.\n2. The paper measures the hyperbolicity of the dataset simply by \u03b4-hyperbolicity but does not clarify the inherent hyperbolicity of the recommendation dataset. The utilization of hyperbolic space in this setting is questionable. \n3. The paper does not explain and analyze why negative sampling harms the performance of the hyperbolic model, which does not occur in the Euclidean model.\n4. The paper does not optimize the model with the compatibility of data with hyperbolic space, though it tries to obtain a more accurate estimation of the curvature of hyperbolic space. The role of curvature is not fully used in the recommendation design.\n5. The authors state to apply hyperbolic geometry in the sequence learning settings. However, the applicability of the proposed strategy to the state-of-the-art recommendation models is questionable.\n6. The experimental results are not convincing. The comparison seems to focus on PureSVD-N and EASEr from 2019 (4 years ago). It is a bit misleading since many works have emerged in the past 4 years (see [1, 2] for example).\n\n[1] Zhou, Kun, et al. \"Filter-enhanced MLP is all you need for sequential recommendation.\" Proceedings of the ACM web conference 2022. 2022.\n\n[2] Xie, Xu, et al. \"Contrastive learning for sequential recommendation.\" 2022 IEEE 38th international conference on data engineering (ICDE). IEEE, 2022."
                },
                "questions": {
                    "value": "Please refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethical issues."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7960/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698914629858,
            "cdate": 1698914629858,
            "tmdate": 1699636978651,
            "mdate": 1699636978651,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SwnGkqoC26",
                "forum": "0TZs6WOs16",
                "replyto": "TyQiulW8uK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7960/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7960/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9PVb"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thorough assessment and extensive feedback on our work. Below we address the outlined weakness points by quoting them and expanding the explanation below the quote. Due to space constraints, our response will be split into several comments below.\n\n### Point 1\n> Limited novelty. The work simply replaces the output layer of SASRec with hyperbolic prediction layers. Besides, the used hyperbolic prediction layers(hyperbolic hyperplane and MLR) have been widely applied in other works. The contribution of this article needs to be re-condensed.\n\nWe understand the concern and acknowledge that assessing novelty can be subjective. We believe it's important to note that the journey toward what might appear as a straightforward solution often involves intricate paths and complex problem-solving. While our final results may seem straightforward, achieving them involved rigorous exploration, refinement, and iterative processes that might not be immediately evident.\n\nIt's also important to highlight the contextual specificity and the absence of a universally applicable framework for employing hyperbolic models. The application of the hyperbolic geometry in different domains often demands tailored adaptations and nuanced implementations. What might appear as a widely utilized approach in certain studies may not seamlessly translate into another problem set due to the intricacies and contextual nuances inherent in each domain. For example, we conducted several experiments on text-related tasks with the same approach and no immediate improvement was obtained there. \n\nMoreover, our contribution does not limit to the architecture design only. We provide additional experimentally-verified insights into hyperbolic geometry properties with respect to both models and data. We therefore are confident that the approach and methodology we applied in this task provide a nuanced perspective that adds value to the field.\n\n### Point 2\n\n> The paper measures the hyperbolicity of the dataset simply by \u03b4-hyperbolicity but does not clarify the inherent hyperbolicity of the recommendation dataset. The utilization of hyperbolic space in this setting is questionable.\n\nWe totally agree with this viewpoint. Questioning the applicability of hyperbolic geometry is totally reasonable. Which is exactly why we focus a lot on providing additional context on the curvature estimation and compatibility of datasets. This is something that is largely absent in the majority of prior works. This void in understanding and analysis makes it harder to advance the field.\n\nWe are not sure what the reviewer meant by the \"inherent hyperbolicity of the recommendation dataset\". To clarify our approach, measuring Gromov's $\\delta$-hyperbolicity is the commonly used standard for understanding geometry of data in practical applications. By definition, it measures the extent to which a manifold deviates from the Euclidean geometry in favor of hyperbolicity. When applied to the recommendation dataset, one can say that it is supposed to uncover the inherent hyperbolicity of data.\n\nOn a higher level, the question on whether the dataset is plausible for analysis via the hyperbolic geometry was answered in the seminal work by Krioukov et al. (2010). In that sense, we rely on the common understanding. We acknowledge though, that at least in our experiments the practice diverges from the theory. The emergence of \"bad\" examples may indicate a blind-spot in the conceptualization of the task via geometric lens or be an artifact of some undiscovered factors. One of our working hypotheses (yet to be tested) on the difference between \"good\" and \"bad\" datasets in terms of their hyperbolicity estimation is that the latter ones exhibit more heterogeneous structure. For example, the _Movielens-1M_ dataset, on which the hyperbolic models achieve excellent results, is more homogeneous. This is a \"pure\" domain of movies, and the items are more likely to coherently gather onto a well-defined manifold. Conversely, the _Amazon Arts Crafts and Sewing_ dataset seem to be more heterogenous with possibly many substructures, ingraining several different manifolds of different properties (e.g., different space curvatures), which prevents the models from learning good data representations. Verifying this hypothesis requires more elaborate analysis, which goes beyond the scope of the current work and presents an important direction for further research."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7960/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645079802,
                "cdate": 1700645079802,
                "tmdate": 1700645079802,
                "mdate": 1700645079802,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Cj5eaDSLdh",
                "forum": "0TZs6WOs16",
                "replyto": "TyQiulW8uK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7960/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7960/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9PVb (continued)"
                    },
                    "comment": {
                        "value": "### Point 6\n\n> The experimental results are not convincing. The comparison seems to focus on PureSVD-N and EASEr from 2019 (4 years ago). It is a bit misleading since many works have emerged in the past 4 years (see [1, 2] for example).\n\nWe partially addressed this concern in the answer to p.5. Our main comparison target is the SASRec extension with CE loss that shows state-of-the-art performance. The referenced by the reviewer MLP model [1] compares to both SASRec (with BCE) and BERT4Rec. As we already stated above, the CE version of SASRec outperforms both of these baselines, and our hyperbolic variant of CE-based SASRec improves the metrics further. \n\nWe acknowledge that comparing  with MLP model can be interesting. However, the design of this model relies on pairwise ranking loss that also utilizes the negative sampling. As we previously discussed, this sampling may not play well with hyperbolic geometry and we'd argue that final comparison with such models should be performed after the comprehensive analysis of the root cause of the negative sampling impact on hyperbolic models. It links us back to p.3, which we also addressed.\n\nThe contrastive learning model from [2] compares only to the weaker BCE-based SASRec and no comparison is made with CE-based BERT4Rec. Overall, the relative improvement in this work looks weaker than the improvement of the full CE-based SASRec studied by Klenitskiy & Vasilev (2023). For example, on the Movielens-1M dataset in [2] one has:\n- SASRec's HR@10 = 0.1902,\n- CL3SRec's HR@10 = 0.1975,\n- the **relative improvement 3.8%**.\n\nThe same dataset in Klenitskiy & Vasilev (2023):\n- BCE SASRec HR@10 = 0.2500,\n- CE SASRec HR@10 = 0.3152,\n- the **relative improvement 26%**.\n\nThe experimental setup differs a bit in these works. However, the resulting difference in the absolute values of the metrics is not significant, while the relative improvement over the original BCE-based SASRec baseline is much more pronounced in Klenitskiy & Vasilev (2023). This allows us to conclude that [2] provides a weaker baseline than the CE-based SASRec.\n\nAdditionally, the same remark on negative sampling that we did w.r.t MLP model holds for [2] as well. We envision that CL3SRec won't gain much from the hyperbolic geometry due to the negative sampling utilized in part of its loss. The effects of switching to full CE loss in this case are more intricate, as CL3SRec uses a composite multi-task learning. Adopting hyperbolic geometry would require additional efforts to properly combine all parts of the loss. Performing such work would significantly extend our current approach and make it hard to fit into a single conference paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7960/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645816053,
                "cdate": 1700645816053,
                "tmdate": 1700645892386,
                "mdate": 1700645892386,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0yQQUIAveQ",
            "forum": "0TZs6WOs16",
            "replyto": "0TZs6WOs16",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7960/Reviewer_vqza"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7960/Reviewer_vqza"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an approach that leverages hyperbolic geometry for extending recommendation systems. The idea is that the hyperbolic layer will capture structural properties of the approach. The authors are based on one base model, SASRec, which they extend. They perform experiments on multiple datasets with mixed results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is generally well written which I greatly appreciate. It contains some interesting ideas of how to extend recommendation systems to take advantage of hyperbolic geometries. There is quite good coverage of related work even though I would like to see references from practical work. The authors could check recent papers in recommender systems from Pinterest, Airbnb, etc. where there are also real use-cases."
                },
                "weaknesses": {
                    "value": "My first comment is that the originality of this work is quite limited as other works proposed hyperbolic recommenders. The addition of this paper is the extension of SASRec with a hyperbolic layer as stated at the last sentence Section 5. I am not sure how much originality is there.\n\nAs mentioned previously, it would be great if the authors could add some references from recent works of recommender systems on real use-cases just to contrast it with the current SOTA in real applications. Do we expect that such an approach would be viable or would give better online results in a real system?\n\nIn Section 3.2 authors mention that negative sampling is typically implemented using uniform sampling which is rarely the case for real use cases where one employs heuristics in order to sample hard negatives and learn more robust models.\n\nI do not understand the statement \"distributional properties of items based on the popularity hierarchy\". What exactly someone would like to capture here? And why popularity cannot be captured with non-hyperbolic geometry?  I cannot see how this would help a recommendation system.\n\nIn the experimental part I would add the following baselines:\n- Most popular\n- Co-occurence baseline, where one would recommend the property that co-occurs more often with the last item in the sequence. This is often a very strong baseline.\n\nAdding challenging datasets also would be great. In recent years many datasets have been released that can be used for sequential recommendation. For example:\n- https://xmrec.github.io/wsdmcup/\n- https://github.com/ExpediaGroup/pkdd22-challenge-expediagroup\n\nGenerally the results are no convincing/great. It seems in some cases there is some slight improvement (third digit). And the classification in good and bad datasets is arbitrary. What is the hypothesis that in one dataset of Amazon data the hyperbolic version is better and in another one (like Products) is not better? This seems random."
                },
                "questions": {
                    "value": "Please see weaknesses section for the questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7960/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699186129409,
            "cdate": 1699186129409,
            "tmdate": 1699636978539,
            "mdate": 1699636978539,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zGsKC9eHJQ",
                "forum": "0TZs6WOs16",
                "replyto": "0yQQUIAveQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7960/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7960/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vqza"
                    },
                    "comment": {
                        "value": "We are grateful to the reviewer for their careful consideration of our work and for the provided suggestions on possible improvements. Below we address the outlined concerns by quoting them and expanding the explanation below the quote. Due to space constraints, our response will be split into several comments below.\n\n### Point 1\n> My first comment is that the originality of this work is quite limited as other works proposed hyperbolic recommenders. The addition of this paper is the extension of SASRec with a hyperbolic layer as stated at the last sentence Section 5. I am not sure how much originality is there.\n\nWe recognize that at a superficial glance, incorporating a hyperbolic layer into the SASRec framework may appear as an incremental addition to the existing body of work. However, it is imperative to emphasize the intricate and meticulous process involved in this extension. Our approach involved a nuanced exploration of hyperbolic geometry properties with respect to integration within the sequential learning architectures. The final solution was selected among several competing one and required extensive experimentation. Moreover, our contribution does not limit to the architecture design only. We provide additional empirical insights into hyperbolic geometry properties with respect to both models and data. We therefore are confident that the approach and methodology we applied in this task provide a nuanced perspective that adds value to the field and highlights important issues that inform new directions for further research.\n\n### Point 2\n> As mentioned previously, it would be great if the authors could add some references from recent works of recommender systems on real use-cases just to contrast it with the current SOTA in real applications. Do we expect that such an approach would be viable or would give better online results in a real system?\n\nWe appreciate the reviewer's suggestion and recognize the value in contrasting our approach with recent works on real-world recommender systems. We note that our work builds upon the research by Klenitskiy and Vasilev (2023). According to the stated affiliations, the research is conducted within an AI Lab of a large company and provides insights into the ways of significantly improving an already strong sequential learning model. Consequently, our work suggests further improvements to the demonstrated state-of-the-art results. Our approach offers a better quality of recommendations with more compact representation at the same time. We believe, it therefore holds significant practical implications worth sharing with the community. \n\nOur paper delineates the strengths and limitations of our proposed approach's applicability. Furthermore, we offer the complete codebase, enabling straightforward reproduction and adaptation of our solution to specific real-world applications. This comprehensive sharing empowers practitioners to evaluate the practicality of our method, considering its merits and shortcomings in their individual contexts.\n\nIt's crucial to underscore that the realm of recommender systems lacks a universal \"silver bullet\" solution applicable across all domains, and there's an overall agreement in the community that developing a silver bullet is not the end goal [1]. Assessing the viability of a particular model within a specific use case traverses subjective terrain, limiting a scientific approach and potentially harming the field's overall progress. \n\n[1] Ferrari Dacrema, Maurizio, Simone Boglio, Paolo Cremonesi, and Dietmar Jannach. \"A troubling analysis of reproducibility and progress in recommender systems research.\"\u00a0_ACM Transactions on Information Systems (TOIS)_\u00a039, no. 2 (2021): 1-49."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7960/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677447572,
                "cdate": 1700677447572,
                "tmdate": 1700677482706,
                "mdate": 1700677482706,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DgRqo8VTEO",
                "forum": "0TZs6WOs16",
                "replyto": "0yQQUIAveQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7960/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7960/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vqza (continued)"
                    },
                    "comment": {
                        "value": "### Point 5\n> In the experimental part I would add the following baselines:\nMost popular\nCo-occurence baseline, where one would recommend the property that co-occurs more often with the last item in the sequence. This is often a very strong baseline.\n\nWe thank the reviewer for the suggestion of additional baselines. In the case of the last-item-cooccurrence model, we agree that it may provide a reasonable quality. It would however add no additional understanding into the difference between Euclidean and hyperbolic sequential learning models, which is one of the main targets in our work. \n\nWe do have results for the popularity-based model, though, listed below. One can see that the performance is very poor, which is not surprising.\n- Movielens-1M\n\t- NDCG@10 \u00a00.0214\n\t- HR@10 \u00a0 \u00a00.0427\n\t- MRR@10 \u00a0 0.0151\n\t- COV@10 \u00a0 0.0529\n- Grocery and Gourmet Food\n\t- NDCG@10 \u00a00.0256\n\t- HR@10 \u00a0 \u00a00.0336\n\t- MRR@10 \u00a0 0.0230\n\t- COV@10 \u00a0 0.0005\n- Office Products\n\t- HR@10 \u00a0 \u00a00.0074\n\t- MRR@10 \u00a0 0.0024\n\t- NDCG@10 \u00a00.0036\n\t- COV@10 \u00a0 0.0006\n- Arts Crafts and Sewing\n\t- NDCG@10 \u00a00.0096\n\t- HR@10 \u00a0 \u00a00.0150\n\t- MRR@10 \u00a0 0.0080\n\t- COV@10 \u00a0 0.0007\n\nWe do not report these results to save space and avoid distracting the reader from the main effects.\n\n### Point 6\n> Adding challenging datasets also would be great. In recent years many datasets have been released that can be used for sequential recommendation.\n\nWe thank the reviewer for the additional references to rich datasets. This can be a great addition for the deeper analysis of the data compatibility, once the framework for such analysis is developed."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7960/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677747725,
                "cdate": 1700677747725,
                "tmdate": 1700677818948,
                "mdate": 1700677818948,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NpxK6QZGZb",
                "forum": "0TZs6WOs16",
                "replyto": "0yQQUIAveQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7960/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7960/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vqza (continued)"
                    },
                    "comment": {
                        "value": "### Point 7\n> Generally the results are no convincing/great. It seems in some cases there is some slight improvement (third digit). And the classification in good and bad datasets is arbitrary. What is the hypothesis that in one dataset of Amazon data the hyperbolic version is better and in another one (like Products) is not better? This seems random.\n\nWe respectfully disagree with the assessment regarding the improvement observed. Notably, the improvements were achieved utilizing  _smaller embedding sizes_. Please, see Table 4 in the Appendix for more detailed breakdown. Our results showcase that even if the metrics were to remain at par without any explicit improvement over the baseline, our approach would still offer an advantage in terms of _more compact representation_ while maintaining the state-of-the-art quality. This comparison underscores the practical advantage inherent in our approach.\n\nIn terms of the categorization of the datasets, we also recognize the importance of further in-depth analysis to understand what aspects render datasets more compatible with hyperbolic geometry. However, as we discussed in the response to p.2 and p.3 of the Reviewer 9PVb comments, there are several considerable challenges associated with the task.  In the absence of better analysis tools that are yet to be invented, we provide an empirical observation and link it to the most visible differences in the $\\delta$-hyperbolicity estimation. It is indeed not suitable for uncovering the deep underlying effects, but it is not random.\n\nThere seem to be no established theoretical framework for a deeper analysis of data compatibility. We acknowledge that the practice diverges from the theory in our experiments (a circumstance often encountered in general). The theory prescribes that hyperbolic geometry is suitable for any instance of complex networks family, which collaborative filtering datasets  belongs to. The emergence of \"bad\" practical examples (for which the estimation of $\\delta$-hyperbolicity is not reliable) may indicate a blind-spot in the conceptualization of the task via geometric lens or be an artifact of some undiscovered factors. \n\nOne of our working hypotheses (pending further validation) regarding the distinction between \"good\" and \"bad\" datasets in their hyperbolicity estimation is that the latter tends to display a more heterogeneous structure. For example, the\u00a0_Movielens-1M_\u00a0dataset, on which the hyperbolic models achieve excellent results, is more homogeneous. This is a \"pure\" domain of movies, and the items are more likely to coherently gather onto a well-defined manifold. Conversely, the\u00a0_Amazon Arts Crafts and Sewing_\u00a0dataset seem to be more heterogenous with possibly many substructures encompassing different types of products or consumption patterns, therefore ingraining several different manifolds of different properties (e.g., different space curvatures). This in turn may prevent hyperbolic  models from learning better data representations. Verifying this hypothesis requires more elaborate analysis and presents an important direction for further research. \n\nIt is worth adding an additional remark on the quality improvement here, that we also made in response to p.2 of the Reviewer WdM2.  We deliberately restrict hyperbolic models to the reduced embedding sizes.  If we lift this restriction, even on \"bad\" datasets the hyperbolic models can be trivially made to perform at the same level as their Euclidean counterparts. It simply takes setting higher values of\u00a0$\\delta$\u00a0(lower curvature\u00a0$c$\u00a0respectively). In this case the hyperbolic models become practically indistinguishable from the Euclidean ones. This result, however, would prevent us from demonstrating the true behavior of hyperbolic models on problematic datasets, while making the formal results (quality metrics) look much better in comparison. We believe that this result would be too trivial to show and instead focus on the discrepancies in learning abilities depending on the geometric properties of data. We believe that our approach offers a more equitable perspective, helping to more effectively unveil the underlying intricacies associated with training hyperbolic models."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7960/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677785765,
                "cdate": 1700677785765,
                "tmdate": 1700682888833,
                "mdate": 1700682888833,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]